This paper conducts an empirical investigation into the invariance, equivariance, and equivalence properties of representations learned by convolutional networks under various data augmentation strategies. The authors propose additional loss terms aimed at enhancing the invariance or equivariance of these representations.
While the concept of measuring invariance, equivariance, and equivalence in representations is not novel (e.g., Lenc & Vedaldi), the authors are the first to systematically analyze the impact of data augmentation on these properties. However, the results presented lack clarity in terms of their novelty, significance, or practical utility. It is fairly intuitive that data augmentation would increase invariance, and that training with identical augmentations would yield more similar representations compared to training with differing augmentations.
Concerning the proposed method for increasing invariance and equivariance: although it is plausible that more invariant or equivariant representations could generalize better, the paper does not provide a compelling rationale for why one would want to explicitly optimize for these properties if they do not demonstrably improve performance. Furthermore, the paper does not offer evidence that training for increased invariance or equivariance results in meaningful performance gains. Given that the proposed loss function (eq. 6) would significantly increase computational overhead, the practical utility of this technique appears limited.
Minor Comments:
- The notation \( R^{nxn} \) should be corrected to \( R^{n \times n} \).
- In eq. 2, there is a typo: "equivaraince" should be corrected.
- In Section 3.3, the formatting of "argmax" is incorrect.
- The claim that data augmentation became essential with Krizhevsky et al. is inaccurate; data augmentation was already considered important prior to their work.
- The statement, "This is related to the idea of whether CNNs collapse (invariance) or linearize (equivariance) view manifolds of 3D objects," is problematic. The notion that equivariance implies linearization of the manifold (orbit) is incorrect. A linear representation \( M_g \) can produce nonlinear manifolds. For instance, a 2D rotation matrix (a linear transformation) generates a nonlinear manifold (a circle).
- In eq. 2, the term "equivariance" should be replaced with "non-equivariance," as a low value indicates equivariance, while a high value indicates non-equivariance.
- The phrase "Eq. 2 also uses the paradigm that" employs the term "paradigm" in an awkward and unclear manner.
- In the definition of \( x'{ij} \), it seems one of the \( gj \) terms should be inverted. Without this, the transformation appears to be applied twice rather than undone.