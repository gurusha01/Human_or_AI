This paper presents a 'GPU-friendly' adaptation of the A3C algorithm, which relaxes certain synchronicity constraints inherent in the original A3C to better align with the high-throughput capabilities of GPU devices. The authors provide a thorough analysis of the impact of the introduced latency and conduct an extensive systems-level evaluation of the algorithm.
However, one limitation is that the performance results in Table 3 are difficult to compare due to significant variations in experimental protocols. While I understand that DeepMind did not release reproducible code for A3C, it appears that the authors have implemented their own version of vanilla A3C. If so, it would be beneficial to include results for this reimplementation under the same conditions used by DeepMind, as well as under the experimental setup used for GA3C (1-day training). Additionally, the text should explicitly state that the experimental protocols differ (e.g., top 5 out of 50 runs versus a single run) and provide an explanation for this discrepancy, even if the reason is simply a lack of time or resources to replicate the original protocol. A more careful presentation of these details would help reinforce the claim that the approximations introduced do not incur a performance penalty.
I commend the authors for open-sourcing their code, particularly given the scarcity of rigorously tested open-source implementations in this domain. Developing correct implementations of these algorithms is a non-trivial task, and their contribution is valuable to the community.
Finally, a disclaimer: as someone who has not personally implemented A3C, I have limited confidence in my ability to fully evaluate the algorithmic aspects of this work.