The paper introduces an architecture to parallelize the optimization of nested functions using the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This approach breaks the optimization process into two steps: training individual layers and updating the auxiliary coordinates. The work specifically targets binary autoencoders and suggests partitioning the data across multiple machines, allowing parameter updates to migrate between them. The authors report relatively strong speedup factors, particularly on larger datasets, and provide a theoretical performance model that aligns well with the experimental results.
My primary concern is that, although the method is framed as a general framework for optimizing nested functions, the experiments are limited to a narrow class of models—namely, binary autoencoders with linear or kernel encoders and linear decoders—consisting of only two components. While the reported speedup factors are promising, their broader significance is unclear, as the binary autoencoder model examined is not widely studied or commonly used in the research community. I recommend that the authors extend their framework to more general architectures and problem domains to enhance its impact.
Questions for the authors:
1. Does this framework extend to generic multi-layer neural networks? If so, providing experimental results for such architectures would be valuable.
2. What challenges or implications arise when applying this framework to models with more than two components (e.g., an encoder and a decoder) or to architectures with non-linear components?
3. It would be helpful to include a performance plot as a function of time for different configurations to illustrate the speedup after convergence. The current focus appears to be on speedup factors per iteration. For instance, while increasing the mini-batch size might improve iteration speed, it could negatively affect convergence speed.
4. Have you considered scenarios where the dataset is too large to store both the data and auxiliary variables across multiple machines simultaneously?
Additionally, the paper frequently references an ArXiv manuscript with the same title by the authors. To ensure the paper is self-contained, please include any necessary supplementary material in the appendix.
In its current form, without demonstrating the applicability of this framework to more general architectures beyond binary autoencoders, the paper may not appeal to a broad audience at ICLR. Therefore, I recommend a weak reject.