The proposed method is both straightforward and sophisticated; it leverages the remarkable success of gradient-based optimization for deep non-linear function approximators and integrates it with well-established (linear) many-view CCA techniques. A key contribution of this work is the derivation of gradients with respect to the non-linear encoding networks, which map the various views into a shared latent space. The derivation appears to be correct. Overall, this approach is highly intriguing, and I believe it could be applicable to a wide range of similarly structured problems.
The paper is well-written; however, it would benefit from an explicit description of the complete algorithm, particularly emphasizing how the joint embeddings G and U are updated.
I do not have prior experience with many-view CCA-style techniques, which makes it challenging for me to fully assess the practical and empirical advancements presented in this work. That said, the experiments appear reasonably convincing, though they are mostly conducted on small to medium-sized datasets.
Detailed comments:
- The colors or the sign of the x-axis in Figure 3b seem to be inverted compared to Figure 4.
  
- It would be helpful to include a continuous (rainbow-colored) version of Figures 2, 3, and 4 to better visualize neighboring datapoints. More importantly, I would like to see how the average reconstruction error between the individual network outputs and the learned representation evolves during training. Could the mismatch between different views on a validation/test set serve as a meaningful metric for cross-validation? 
- In general, the method seems sensitive to regularization and hyperparameter tuning, likely due to its larger number of parameters compared to GCCA. Additionally, different regularization parameters are used for different views. I wonder if there is a clear, well-defined metric to optimize these parameters effectively.