This paper conducts an empirical investigation into the impact of various data augmentation techniques on CNN performance. Additionally, it introduces supplementary loss functions aimed at promoting approximate invariance or equivariance and demonstrates modest benefits from their inclusion.
The manuscript is well-written, and the objectives are clearly articulated. The exploration of invariances in CNNs is a critical and valuable area of research, and progress in this domain is highly commendable. The paper is structured into two distinct components: an empirical analysis of equivariances in existing CNNs and the introduction of equivariance-promoting objectives. However, each of these components could be more thoroughly developed if considered independently.
Regarding the empirical study, its scope is somewhat narrow, limiting the generalizability of its findings:
- The study is confined to a single network architecture; including at least one additional architecture would enhance the robustness of the conclusions.
- The analysis is restricted to a single layer (fc7), which is problematic as the final layer tends to exhibit the greatest invariance. Examining at least one convolutional layer, if not more, would provide a more comprehensive understanding.
- The reliance on the scanned text dataset is a limitation, though the results on ImageNet are notably promising.
The visualization of performance degradation under varying transformation degrees is a valuable contribution, and the authors do provide some interpretation of the results. However, the analysis could be expanded. The insights derived from evaluating networks with jittered data are inherently limited. Proposing alternative and innovative methods to assess invariance and equivariance could yield more meaningful and actionable conclusions.
As for the proposed loss function, its treatment is brief and cursory (Section 4, half a page). The approach bears similarities to established invariance/equivariance objectives in prior literature, such as Decoste and Scholkopf's "Training Invariant Support Vector Machines" (Machine Learning, 2002).
Dividing the paper into these two distinct contributions may not be the most effective strategy, as both components feel somewhat underdeveloped. A more focused and comprehensive exploration of either the empirical study or the proposed loss function would likely result in a stronger and more cohesive paper.