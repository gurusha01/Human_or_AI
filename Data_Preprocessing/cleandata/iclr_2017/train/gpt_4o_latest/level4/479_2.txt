This paper introduces an iterative memory updating model tailored for cloze-style question-answering tasks. The proposed approach is intriguing, and the results are promising. However, I have several comments regarding the paper:
1. The paper does not propose a single model but rather two distinct models. The first model comprises "reading," "writing," "adaptive computation," and "Answer module 2," while the second includes "reading," "composing," "writing," "gate querying," and "Answer module 1." Based on the methodology and experimental results, the "adaptive computation" model appears to be simpler and achieves better performance. Moreover, without the two-time memory update in a single iteration and the composing module, the model bears similarities to the neural Turing machine.
2. What are the specific MLP settings used in the composing module?
3. The paper evaluates different hidden state sizes: [256, 368, 436, 512]. However, there seems to be no clear relationship between these values. How was the specific value of 436 determined? Were there any particular strategies or heuristics employed to identify these numbers?
4. The paper would benefit from additional ablation studies, particularly exploring the use of different values for T, such as T=1, 2, etc.
5. Based on my understanding, for the adaptive computation mechanism, the process halts when P_T < 0. What is the distribution of T in the testing dataset?