This work is highly novel and an enjoyable read.
My concerns about the paper largely align with my pre-review questions. I agree that the learned variable computation mechanism is clearly performing something interesting. However, the empirical results need to be contextualized with respect to the state of the art, and the role of LSTMs remains a significant point of consideration. (To clarify, I do not view outperforming LSTMs, GRUs, or any specific method as a requirement for acceptance, but such comparisons should still be included.)
In their pre-review responses, the authors noted that LSTMs involve more computation per timestep compared to Elman networks. While this is accurate, it is still an axis along which comparisons can be made, with this factor controlled (at least approximately, by varying the number of LSTM cells), and so on. Including a brief discussion of the proposed gating mechanism in relation to currently popular ones would enhance the clarity and impact of the presentation.
---
2017/1/20: Given that my concerns have been addressed, I am revising my review to a score of 7, with the expectation that the manuscript will be updated to incorporate the new comparisons shared as a comment.