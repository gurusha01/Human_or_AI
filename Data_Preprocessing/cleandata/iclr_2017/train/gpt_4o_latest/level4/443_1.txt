This paper introduces the use of an SSNT model for p(x|y), enabling a noisy channel framework for conditional generation while still supporting incremental generation of y. Additionally, the authors propose an approximate decoding strategy and conduct a thorough empirical evaluation.
PROs: The paper is generally well-written, with the SSNT model being both intriguing and well-justified in its application. Moreover, the empirical evaluation is comprehensive, and the results achieved by the authors are strong.
CONs: A potential concern lies in whether the added complexity in training and decoding is justified. For example, similar benefits might be achievable by reranking complete outputs from a standard seq2seq model using a combined score of p(y|x), p(x|y), and p(y). (Notably, Li et al. (NAACL 2016) employ a comparable approach for conversation modeling.) That said, the ability to rerank during the search process could be advantageous, and it would be valuable to see experiments exploring this aspect.
Other Comments:  
- Since the primary focus of the paper is modeling p(x|y), it might enhance clarity if Section 2 were framed from this perspective rather than reverting to p(y|x) as in the original work by Yu et al.  
- Initially, it might seem counterintuitive to propose a noisy-channel model to address the "explaining away" issue, given the introduction of an explicit, uncalibrated p(y) term. However, considering that seq2seq models inherently perform a significant amount of target-side language modeling, incorporating an explicit p(x|y) term appears to be a clever approach.