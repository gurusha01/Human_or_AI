This paper tackles the challenge of evaluating automatic dialogue responses, a critical issue given that existing automatic evaluation methods (e.g., BLEU, which relies on N-gram overlap) show poor correlation with the desired quality as judged by human annotations. The authors propose an approach that leverages an LSTM-based encoding of the dialogue context, reference response, and model response, combined with appropriate scoring. The core idea involves training one dialogue model to evaluate another. However, the effectiveness of the proposed solution hinges on the availability of a sufficiently robust dialogue model as a starting point, which is not guaranteed, potentially undermining the reliability of the new metric.