This paper builds upon the work of Lenc and Vedaldi (2015), demonstrating that CNN representations at the FC7 layer exhibit a certain degree of equivariance to various classes of transformations. The authors further show that training with specific groups of transformations enhances the equivariance of these representations.
The authors conducted an extensive set of experiments, training over 30 networks with different forms of jitter, which is commendable. However, the main message of the paper is somewhat unclear. While the authors examine properties at a different layer than Lenc and Vedaldi (2015), the insights presented do not appear to extend beyond the well-known observation that jitter aids in achieving invariance. Although the evaluation seems largely correct, the paper does not effectively address the problem stated in its title.
There are significant concerns regarding the experiments on representation distances:
* The exclusive focus on the FC7 layer is debatable, as it is followed by only a single classification layer (FC8), which maps to class likelihoods via linear projection. The equivalence map essentially re-projects the FC8 weights of the attached network onto the weights of the original network. Conducting similar experiments across multiple layers, given that the networks are already trained, might yield more valuable insights.
* The experiment on representation distance lacks an evaluation of classification error on the test dataset. This would clarify whether the representations are indeed compatible up to a linear transformation.
* For the K-NN experiment, it is unclear whether the metric is computed per test set example after training the equivalence map. A more informative approach would be to demonstrate that networks trained on similar groups of jitter transformations exhibit greater compatibility on the target task.
* The proposed method does not consistently improve equivariance across all tasks. With \(\lambda1\) and \(\lambda2\) being so small, the loss essentially reduces to simple data jitter, as it merely combines the losses of the original and transformed images. This issue may stem from the choice of the FC7 layer.
Overall, while the paper presents intriguing results on FC7 equivariance, it fails to derive substantial new insights from these experiments. Due to the limitations in the equivalence experiments and the fine-tuning of equivariance, I cannot recommend this manuscript for acceptance. However, refining the experiments on pre-trained networks and restructuring the manuscript to adopt a more investigative approach could yield a meaningful contribution to the field.
There are also several minor issues:
* The new criterion for equivariance mapping has not been experimentally validated to demonstrate improved results.
* Units (e.g., degrees) are missing for the angles mentioned on pages 1 and 5.
* On page 3, the statement "In practice, it is difficult..." incorrectly refers to \(Mg\) as being maximized/minimized, whereas it is the loss over \(Mg\) that is optimized.
* On page 4, footnote 2, halving the activations is inaccurately described as dropout, as this constant factor can be absorbed by the preceding or following weights.
* It is unclear whether the network used for RVL-CDIP follows the same architecture as AlexNet.
* On page 7, Figures 3a and 3b, turning the diagonal elements white is misleading and likely incorrect, as the distance between identical representations should be zero. This discrepancy also serves as a potential check for the correctness of the experiments.