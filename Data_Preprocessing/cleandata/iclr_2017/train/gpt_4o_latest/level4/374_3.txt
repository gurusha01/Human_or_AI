SUMMARY  
The paper introduces a gating mechanism designed to integrate word embeddings with character-level word representations.  
This mechanism leverages features associated with a word to determine the most effective word representation.  
The fine-grained gating is incorporated into systems aimed at addressing cloze-style reading comprehension question answering and Twitter hashtag prediction tasks.  
For the question answering task, the authors propose a fine-grained adaptation of gated attention to combine document words with questions.  
In both tasks, the fine-grained gating mechanism improves accuracy, surpassing state-of-the-art methods on the CBT dataset and achieving performance comparable to state-of-the-art approaches on the SQuAD dataset.  
OVERALL JUDGMENT  
The paper presents an innovative fine-grained extension of a scalar gating mechanism for combining word representations.  
It is well-written, clear, and provides a thorough review of prior work while benchmarking the proposed method against existing models.  
I appreciated the ablation study, which effectively demonstrates the contribution of individual components.  
Additionally, I found the use of (shallow) linguistic prior knowledge, such as POS tags, NER tags, and word frequency, to be both thoughtful and effective.  
Exploring whether syntactic features could further enhance the approach would be an intriguing direction for future work.