The paper introduces a modified variational autoencoder (VAE) that incorporates a discrete latent variable to mask the activation of the latent code, ensuring that only a subset (referred to as an "epitome") of the latent variables is active for any given sample. The rationale behind this design is that by activating different latent variables for different samples, the model is encouraged to utilize more of the latent code compared to a standard VAE.
While addressing the important issue of latent variable over-pruning, which has been previously discussed in the context of variational inference, the proposed approach does not appear to offer a significant improvement over alternatives such as a mixture of VAEs. A mixture of VAEs would have been a valuable baseline for comparison, as it employs a categorical variable (representing the mixture component) alongside multiple VAEs. The primary distinction between a mixture of VAEs and the epitomic VAE lies in the parameter sharing among the "mixture components" in the epitomic VAE.
The experimental results presented in the paper are problematic for several reasons:
1. The log-likelihood of the proposed models is estimated using a Parzen window estimator, which is known to be less accurate. A more reliable lower bound on the likelihood, which is readily available for VAEs, is not reported. Based on the reviewer's experience, a continuous MNIST likelihood exceeding 900 nats can be achieved with a moderately sized VAE.
2. The paper alternates between experiments on binary MNIST and continuous MNIST without clear exposition, which is confusing. These two dataset variants pose distinct challenges for likelihood-based models. Continuous MNIST is particularly challenging because the data resides in a subspace of the 784-dimensional space (with some pixels consistently or nearly always equal to 0), allowing probability density to become arbitrarily large on this subspace. Models that optimize likelihood often exploit this property, concentrating probability on the subspace rather than genuinely modeling the data. In contrast, well-tuned VAEs trained on binary MNIST (or continuous MNIST with appropriately added noise) typically produce samples of higher quality than those shown in the paper.
3. The claim that the VAE "overfits" to the training data is unsubstantiated. The paper provides no evidence that the reconstruction likelihood on the training data is significantly higher than on the test data. Using the term "overfitting" in this context is misleading, as it does not align with its standard technical meaning.
4. The use of dropout in the dropout VAE is not clearly specified. It is unclear whether dropout is applied to the latent variables or to the hidden layers of the encoder/decoder. These two approaches would result in markedly different behaviors.
5. The MNIST eVAE samples and reconstructions resemble a more diverse version of 2D VAE samples/reconstructions, appearing blurry and failing to encode precise stroke positions. This aligns with an interpretation of the eVAE as a mixture of smaller VAEs rather than a higher-dimensional VAE. Consequently, it is misleading to claim that the eVAE outperforms a high-dimensional VAE based on this evidence.
In the reviewer's assessment, the paper is not yet suitable for publication. A stronger baseline VAE, evaluated using the evidence lower bound or another reliable metric, is crucial for a meaningful comparison between the proposed eVAE and standard VAEs.