This paper introduces a method for incorporating unsupervised auxiliary tasks into a deep RL agent, such as A3C. The authors present a variety of auxiliary control and reward tasks and evaluate their approach on Labyrinth and Atari environments. The proposed UNREAL agent demonstrates notable improvements over A3C in terms of both performance and learning speed. This represents a valuable contribution to the conference. However, the results are not particularly surprising, as incorporating relevant auxiliary tasks is expected to enhance and accelerate feature learning. This work serves as a proof of concept for this principle.
The paper is well-written and accessible to readers with expertise in deep RL.
Could the authors provide insights into the computational resources required to train the UNREAL agent?
The overall architecture appears quite complex. Are the authors planning to release the source code for their model?
--------------------------------------------------------
After rebuttal:
No change in the review.