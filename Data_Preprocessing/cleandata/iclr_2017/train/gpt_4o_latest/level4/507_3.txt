First, I would like to commend the authors for their efforts in creating and releasing an NLP data set, which is a task of significant social value.
The authors employ an algorithm to generate a data set consisting of 500 clusters per language, aimed at capturing semantic similarity. This raises several important questions:
1. If scalability is a key motivation for using the algorithm, why is the released data set so limited in size? Its scale is comparable to the data sets released for SemEval tasks in recent years. A truly scalable approach would likely result in a data set that is orders of magnitude larger, which I had anticipated.
2. The authors manually verified a small subset of the clusters and identified at least one ambiguous cluster that arguably should have been excluded. Given that Mechanical Turk is highly scalable, why not use it to filter all clusters post hoc? This approach, akin to the methodology used for creating ImageNet (which contains millions of items), could significantly enhance the quality of the data set.
3. Evaluating data set papers is inherently challenging. What criteria make a data set "good" or worthy of publication? Many medium-sized NLP data sets are released annually (e.g., through SemEval), typically designed to address specific NLP tasks of interest. While I am not aware of a data set that directly addresses the task proposed by the authors, the task itself—focused on semantic similarity—has been explored extensively, with multiple data sets introduced since SemEval 2012. I would have appreciated comparisons in the paper to demonstrate that the proposed task and data set are better suited for analyzing semantic similarity than existing alternatives.
Two additional observations:
A. This paper does not seem particularly well-aligned with ICLR. While new NLP data sets can indirectly aid in evaluating word embeddings (and, by extension, representations), I found the insights provided by the paper to be limited. For instance, the claim that GloVe performs worse than other embeddings for semantic similarity is intriguing, but the paper does not explore why this might be the case. Such an analysis would have been valuable.
B. The task of "placing a word into a cluster and evaluating whether it stands out" was first proposed in the context of human evaluation of topic models in the following paper, which I believe warrants a citation:
Jonathan Chang, Jordan Boyd-Graber, Chong Wang, Sean Gerrish, and David M. Blei. Reading Tea Leaves: How Humans Interpret Topic Models. Neural Information Processing Systems, 2009.