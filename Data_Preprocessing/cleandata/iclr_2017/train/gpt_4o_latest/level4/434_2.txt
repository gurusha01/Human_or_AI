This paper effectively extends batch normalization to RNNs, addressing the challenges where batch normalization has previously underperformed or failed. The experiments and datasets utilized clearly demonstrate the improvements achieved by batch norm LSTMs compared to standard LSTMs. The authors evaluate their approach across a diverse range of tasks, including character-level (PTB and Text8), word-level (CNN question-answering task), and pixel-level (MNIST and pMNIST) examples. The provided training curves convincingly highlight the potential reductions in training time, which is a critical metric for evaluation.
The pMNIST experiment, in particular, provides strong evidence of batch normalization's benefits in recurrent settings, especially for capturing long-term dependencies. I also found the insights into gradient flow to be highly valuable, particularly the discussion on how unit variance impacts tanh derivatives. The inclusion of this analysis, not only for batch normalization but also for the "toy task" (Figure 1b), was especially insightful and helpful.
In summary, this paper represents a meaningful contribution to the application of batch normalization and offers essential guidance for its successful implementation in recurrent architectures.