Deep reinforcement learning (RL), which leverages deep neural networks as function approximators in RL algorithms, has achieved notable success in addressing problems within large state spaces. This empirically focused study extends these approaches by introducing a novel algorithm that demonstrates superior performance in previously unseen 3D environments using raw sensory data, while also enabling improved generalization across different goals and environments. Notably, this algorithm emerged as the winner of the Visual Doom AI competition.
The central innovation of the proposed algorithm lies in its use of additional low-dimensional observations (e.g., ammo or health provided by the game engine) as supervised targets for prediction. Crucially, these predictions are conditioned on a goal vector (which is provided rather than learned) and the current action. Once the model is trained, the optimal action for a given state is determined by selecting the action that maximizes the predicted outcome with respect to the goal. Unlike successor feature representations, this approach relies on supervised learning, with no temporal difference (TD) relationship between the predictions of the current and subsequent states.
The authors provide a thorough review of prior work in section 2, including research on predicting future states as part of RL and goal-driven function approximators. The primary contributions of this work include its emphasis on Monte Carlo estimation (as opposed to TD methods), the use of low-dimensional "measurements" for prediction, the incorporation of parameterized goals, and, perhaps most significantly, the empirical comparisons to relevant prior approaches.
Beyond the results in the Visual Doom AI competition, the authors demonstrate that their algorithm can learn generalizable policies capable of adapting to limited goal changes without requiring additional training.
The paper is well-written, and the empirical findings are compelling, making it a valuable contribution to the field.
Some minor suggestions for improvement:
- The supervised training approach involves an approximation, as it assumes an on-policy setting while learning from a replay buffer. Specifically, the Monte Carlo regression assumes that the remainder of the trajectory follows the current policy, even though the samples are drawn from episodes generated by earlier policy versions. This approximation should be explicitly discussed.
- The algorithm relies on additional metadata (e.g., identifying which aspects of the sensory input are worth predicting) that competing algorithms do not utilize. The authors should more clearly acknowledge this dependency and discuss the limitations of their approach, such as its potential ineffectiveness in sensory environments where such measurements are unavailable.