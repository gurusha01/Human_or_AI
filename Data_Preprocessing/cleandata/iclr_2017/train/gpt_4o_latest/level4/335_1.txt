This paper introduces an information-theoretic framework for unsupervised learning, grounded in the infomax principle, which aims to maximize the mutual information between input and output. The authors propose a two-step algorithm to address this problem. First, they employ an asymptotic approximation of mutual information to decouple the global objective into two subproblems, each of which admits closed-form solutions. These solutions are then used as initial estimates for the global solution, which is subsequently refined using a gradient descent approach.
While the theoretical foundation and derivations presented in the paper appear sound, the clarity and organization of the material could be improved. For instance, instead of providing a step-by-step derivation of each equation, it might be more effective to first present a high-level overview of the main results and briefly outline the derivation strategy. The detailed derivations, which may obscure the core message of the results, could be deferred to later sections or moved to an appendix for better readability.
Several points that the authors may want to address are as follows:
1. Page 4, last paragraph: "From above we know that maximizing I(X;R) will result in maximizing I(Y;R) and I(X,Y^U)." While the former follows from the equality in Equation 2.20, the latter is only related via the bound in Equation 2.21. Given the potential gap between I(X;R) and I(X,Y^U), can you justify the claim that maximizing the former necessarily maximizes the latter?
2. Paragraph preceding Section 2.2.2: It is mentioned that dropout, used to prevent overfitting, may be interpreted as an attempt to reduce the rank of the weight matrix. However, no further explanation is provided to support this assertion. Could you elaborate on why this interpretation holds?
3. End of page 9: "We will discuss how to get the optimal solution of C for two specific cases." If I understand correctly, the method does not guarantee finding the optimal solution for C in either case, as the best achievable outcome is a local optimum due to the nonconvexity of the constraint in Equation 2.80 (a quadratic equality). If optimality cannot be ensured, please revise the wording to reflect this limitation.