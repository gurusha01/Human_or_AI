This paper explores deep generative models with multiple stochastic nodes, assigning them meaning through semi-supervision. From a methodological perspective, the contribution is not fundamentally novel (it closely resembles the semi-supervised approach of Kingma et al.; while this work sometimes incorporates more than two latent nodes, the extension is not particularly complex). The authors employ a fairly standard auxiliary variable technique to ensure the inference network for y is trained across all data points (by treating y as a latent variable with an observation \tilde y, where the observation is y if y is observed, or uninformative when y is unobserved). Alternatively, the authors could decouple the inference used to train the generative model (which disregards inference over y when it is observed) from the inference used to 'exercise' the model (approximating the complex p(y|x) in the model with a simpler q(y|x), effectively inferring p(y|x) for data where only x is available). The results are strong, albeit demonstrated on relatively simple datasets. Overall, this is a well-written and interesting paper, though it falls short in terms of methodological innovation.
Minor Comments:
- The title feels overly broad relative to the paper's content. I also disagree with the stark contrast drawn between deep generative models and graphical models (deep generative models are a subset of graphical models, though they are typically learned and less interpretable compared to classical graphical models; moreover, having multiple stochastic variables is not unique to graphical models, as evidenced by works like DRAW, Deep Kalman Filter, and Recurrent VAE). The term 'structure' is somewhat problematic; the paper seems more focused on disentangling and assigning semantic meaning to the latent representation of a generative model via supervision. Whether the models themselves exhibit structure is debatable.