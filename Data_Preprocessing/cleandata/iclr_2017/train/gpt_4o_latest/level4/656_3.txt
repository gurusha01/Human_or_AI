Thank you for your thorough response and clarifications.
The paper introduces the use of a scattering transform as the initial layers of a deep network. This fixed representation benefits from strong geometric properties, such as local invariance to deformations, and can be interpreted as a form of regularization or prior. The upper layers of the network are then trained for a specific supervised task. The resulting model essentially integrates a standard deep convolutional network on top of the scattering transform. Experiments on CIFAR-10 and CIFAR-100 demonstrate that the proposed method achieves performance comparable to high-performing baselines.
I find the paper highly engaging. The concept of cascading these representations appears to be a very natural and promising approach. To the best of my knowledge, this is the first study to combine predefined, structured representations with modern CNN architectures while achieving competitive results relative to state-of-the-art methods. Although approaches like ResNets and their variants achieve significantly higher performance, I believe this work effectively conveys its core message.
The paper convincingly demonstrates that lower-level invariances can be derived from analytic representations (scattering transform), which simplifies the training process by reducing the number of parameters and enables faster evaluation. The advantages of this hybrid approach become particularly important in low-data scenarios.
The authors argue that the scattering initialization prevents instabilities in the initial layers, as the operator is non-expansive. This naturally suggests that the model may exhibit greater robustness to adversarial examples. It would be highly valuable to include an empirical evaluation of this aspect. What is the practical impact of this robustness? Can this hybrid network still be fooled by adversarial attacks? If it proves resilient, the use of scattering initialization would become a particularly appealing feature.