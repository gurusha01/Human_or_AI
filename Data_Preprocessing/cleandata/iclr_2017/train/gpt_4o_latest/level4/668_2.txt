This paper proposes regularizing the estimator of a probability distribution to favor high-entropy distributions, thereby mitigating overfitting.
I find this idea appealing overall. Regularizing the model's behavior often feels more intuitive than regularizing its parameters. Behavior is interpretable, while parameters are opaque and interact in complex ways to produce that behavior. Consequently, it may be more reasonable to impose a prior on the behavior itself. In essence, the goal is to prefer parameters not because they are individually constrained (e.g., close to 0), but because they collectively yield a distribution that is plausible or low-risk from a prior perspective.
Strengths:
- The idea is both natural and well-founded (I do not share the skepticism expressed by AnonReviewer5).
- It is possible that this approach has not been extensively explored in the context of neural networks (though I am not entirely certain).
- The experimental results are promising, suggesting that this type of regularizer could become a standard tool.
Weaknesses:
- Presenting this idea as if it were largely disconnected from existing work in machine learning risks contributing to the fragmentation of the scientific literature. Many prior papers have incorporated a scaled entropy term into the optimization objective, and this is not limited to reinforcement learning. Please refer to the extensive list of related works I provided in my pre-review comments.
- Experimental results should be accompanied by statistical significance tests and error analysis. Does the trained model truly perform better on the test data distribution, or is the test set too small to draw reliable conclusions? Are the improvements consistent across multiple training sets? What specific errors does the model correct, and what new errors does it introduce?
Summary Recommendation: Revise and resubmit. Given the high volume of submissions to ICLR, I believe we should prioritize rewarding authors who not only propose novel ideas but also situate them within the broader context of existing work and rigorously evaluate their contributions. Otherwise, we risk encouraging a race to the bottom, where the literature becomes cluttered with hastily prepared papers that lack clear connections to prior research.