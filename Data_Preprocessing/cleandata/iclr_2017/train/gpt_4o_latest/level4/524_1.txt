Paraphrased Review
TL;DR:  
The paper introduces a regularization technique that involves adding noise to a representation space. The proposed method is primarily demonstrated using sequence autoencoders (Dai et al., 2015), without incorporating attention mechanisms (i.e., relying solely on the context vector). Experimental results indicate improvements over the authors' baseline on certain toy tasks.
---
Augmentation  
The augmentation method is straightforward: noise is added, interpolated, or extrapolated to the seq2seq context vector (Section 3.2). This reviewer is particularly interested in whether this approach could also be effective in non-seq2seq settings.  
Additionally, a direct comparison with dropout applied to the context vector would have been valuable.
---
Experiments  
Given that the authors are working with seq2seq architectures, it is somewhat disappointing that they did not evaluate their method on Machine Translation (MT), a domain with a wealth of established benchmarks for comparison.  
The authors conducted experiments on several toy datasets (which are not widely used in deep learning literature) as well as MNIST and CIFAR. While the method demonstrates improvements over the authors' own baselines on the toy datasets, the gains on MNIST and CIFAR appear minimal. Furthermore, the authors did not compare their results to the baseline reported by Dai et al., 2015 for CIFAR, which achieves a significantly better LSTM baseline of 25%. This baseline outperforms both the authors' baseline of 32.35% and their proposed method, which achieves 31.93%.  
The experimental results would have been far more compelling if the authors had evaluated their method on seq2seq tasks such as MT (e.g., EN-FR or EN-DE), given that seq2seq was originally developed for this purpose. If MT was not feasible, sentiment analysis tasks like IMDB or Rotten Tomatoes, as explored in Dai et al., 2015, would have been a reasonable alternative, especially since this paper heavily builds on the sequence autoencoder framework introduced in that work.
---
References  
There appear to be issues with the LaTeX formatting of the references, as many conference and journal names are missing. Additionally, the authors should update their citations to include the appropriate conference or journal names instead of relying on "arXiv."  
Specific corrections include:  
- "Listen, attend and spell" should be "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition" and should cite ICASSP.  
- If citing the above ICASSP paper, the authors should also include the Bahdanau paper "End-to-End Attention-based Large Vocabulary Speech Recognition," which was published concurrently (also at ICASSP).  
- "Adam: A method for stochastic optimization" -> ICLR.  
- "Auto-encoding variational bayes" -> ICLR.  
- "Addressing the rare word problem in neural machine translation" -> ACL.  
- "Pixel recurrent neural networks" -> ICML.  
- "A neural conversational model" -> ICML Workshop.