This paper introduces a synchronous parallel SGD method that leverages multiple backup machines. The parameter server is not required to wait for responses from all machines before updating the model, thereby reducing synchronization overhead. The idea appears reasonable and relatively straightforward.
However, my primary concern is that this method seems applicable only to a very specific scenarioâ€”namely, when the majority of learners (with the exception of a small subset) operate at a similar pace in returning results. If the efficiency of learners deviates significantly from this distribution, I doubt the proposed algorithm would perform effectively. Therefore, I recommend the following revisions:
- Include additional experiments to evaluate the algorithm's performance under varying efficiency distributions of learners.
- Assume that all learners share a common efficiency distribution and demonstrate that the proposed algorithm minimizes idle time under such conditions.