This paper presents an impressive end-to-end trained image compression and decompression system that outperforms established image compression algorithms (e.g., JPEG-2000) in terms of bit-rate vs. quality trade-offs. Beyond demonstrating the potential of 'deep learning' in a novel application, the paper's key contribution lies in introducing a differentiable "rate" function. The authors demonstrate that this function enables effective training across various rate-distortion trade-offs, which I believe could have implications extending beyond compression to other tasks that might benefit from differentiable approximations of similar functions.
The authors provided a well-considered response to my pre-review question. However, I maintain that under fixed range and quantization constraints, a sufficiently complex network should inherently learn to produce codes within the fixed range that achieve maximum entropy (i.e., approaching the theoretical upper bound). That said, the second argument presented by the authors is persuasiveâ€”enforcing a specific "form" on the compressor output necessitates a more complex network to achieve comparable compression performance. This is because such a network would need to replicate the computations currently handled by the separate variable-rate encoder used to store q.