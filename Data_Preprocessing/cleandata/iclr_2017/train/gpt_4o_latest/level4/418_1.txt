This paper presents a novel approach for training GANs by replacing simultaneous SGD and unrolling the inner optimization in the minmax game as a computational graph. The manuscript is well-written and provides clear and thorough justifications for the proposed method. The problem addressed is both significant and important. While the approach is innovative, it is worth noting that similar ideas have been explored in contexts unrelated to GANs.
The first quantitative experiment, described in Section 3.3.1, involves finding the optimal z that can generate training examples by minimizing |G(z) - x| using L-BFGS. The authors claim that successfully identifying such a z implies the generator can reproduce the corresponding training example. The results demonstrate that 0-step GANs fail to generate many training examples, whereas unrolled GANs succeed. However, I find this experiment problematic. Identifying a specific z that generates a given sample does not necessarily indicate that the corresponding mode has high probability. For instance, an identity function could outperform all GAN models under this metric. Furthermore, Cantor's proof of equivalence between all powers of real spaces implies that this issue persists even for lower-dimensional z. Realistically, any generator should be capable of producing any image by locating a sufficiently specific z. The existence of such a z does not confirm that the generator avoids mode collapse; it merely suggests that the generator behaves similarly to an identity function, capable of reproducing any image. Consequently, this metric appears to measure something tangential to diversity or mode-dropping. Another issue with this metric is that failing to find a z for a specific training example does not prove that such a z does not existâ€”it only indicates that it is harder to locate. This comparison may simply reflect that unrolled GANs have smoother functions than 0-step GANs, making optimization for z easier.
The second quantitative experiment evaluates the mean pairwise distance between generated samples and between data samples. The authors argue that closer alignment between these two values indicates that the generated samples are as diverse as the data. However, this metric is also unconvincing for two reasons: (1) the distances are computed in pixel space, which may not capture meaningful differences, and (2) a GAN generating nonsensical outputs could still perform well under this metric.
The paper does not include additional quantitative results. While the proposed method aims to optimize diversity, it would be prudent to include sanity checks such as Inception scores or SSL performance to assess the quality of the generated samples. Another potential evaluation could involve training the GAN on the tri-MNIST dataset (a concatenation of three MNIST digits), which contains 1,000 distinct and easily identifiable modes. The authors could then demonstrate that the GAN generates all 1,000 modes with approximately equal probability. While not a perfect metric, this approach would likely provide a more reliable assessment than the metrics currently used in the paper. This evaluation method has been employed in a previous ICLR submission: