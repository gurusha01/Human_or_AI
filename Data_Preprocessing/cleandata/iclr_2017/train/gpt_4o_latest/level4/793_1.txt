Review - Summary:  
This paper introduces a method for training recurrent neural networks using surprisal-driven feedback, where the network's next-step prediction error is fed back as an input to the network. The authors demonstrate their approach on language modeling tasks.
Contributions:  
The paper presents surprisal-driven feedback, which involves feeding back the model's prediction errors from previous time steps as additional input.
Questions:  
One aspect that remains unclear in the paper is whether ground-truth labels from the test set are used for the surprisal feedback mechanism. Based on the authors' claim that misprediction error is used as input, I assume this is the case.
Criticisms:  
- The paper is poorly written, and the authors should reconsider its organization.  
- Many of the equations related to BPTT are unnecessary for the main text and could be moved to the Appendix.  
- The justification for the proposed approach is not sufficiently convincing.  
- Experimental results are inadequate, as only a single dataset is evaluated.  
- The claim of achieving SOTA on enwiki8 is incorrect, as other works, such as HyperNetworks, report better results (1.34).  
- The reliance on ground-truth labels for the test set significantly limits the applicability of this method, effectively excluding most conditional language modeling tasks.
High-level Review:  
Pros:  
- The proposed modification is simple and appears to improve results. It is an interesting idea.  
Cons:  
- The method requires access to test-set labels.  
- The paper is poorly written.  
- The assumption of access to ground-truth labels during testing is problematic.  
- The experimental evaluation is insufficient.