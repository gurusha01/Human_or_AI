The paper states that "[a]fter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity."
Could you consider including (or providing here) the specific hyperparameters and the type of dropout utilized (e.g., recurrent dropout, embedding dropout, etc.)? Without this information, replicating the results would likely involve significant trial and error. As demonstrated in "Recurrent Neural Network Regularization" (Zaremba et al., 2014), sharing a baseline set of hyperparameters can greatly facilitate progress in the field.
This level of detail would likely also be beneficial for the other experiments, such as the character-level language model.
The paper addresses a critical area of research in our field: automating architecture search. Although the approach is currently computationally intensive, this trade-off is expected to improve as technology advances in the near future.
The study spans both standard vision and text tasks, evaluating the method on multiple benchmark datasets. The results demonstrate that exploring beyond the conventional RNN and CNN search spaces can yield significant improvements. While it is always desirable to see the method applied to additional datasets, the experiments presented here are already sufficient to establish that the technique is not only competitive with human-designed architectures but may also surpass them. Furthermore, the findings suggest a promising approach to tailoring architectures to specific datasets without requiring manual engineering at each step.
Overall, this is a well-written paper on an important topic, presenting strong results. I recommend it for acceptance.