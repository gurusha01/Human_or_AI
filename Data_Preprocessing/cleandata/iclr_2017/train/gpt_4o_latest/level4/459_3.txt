This paper introduces a deep multi-task representation learning framework that leverages tensor factorization and end-to-end knowledge sharing to learn cross-task sharing structures at every layer of a deep network. By doing so, the proposed approach eliminates the need for a user-defined multi-task sharing strategy typically required in conventional methods. The experimental results demonstrate that the proposed framework achieves higher accuracy while reducing the dependency on design choices.
While factorization techniques have been explored in the past for other tasks, applying them to multi-task learning (MTL) is an intriguing direction. However, I would like to note that the parameter savings primarily stem from the low-rank factorization. In conventional MTL, the size of each layer's weights could also be reduced by applying techniques such as singular value decomposition (SVD).
Additionally, I would like to point out that neural network-based MTL was initially explored earlier than the 2014 and 2015 works cited in this paper, particularly in the speech recognition community. For example, see:
Huang, J.T., Li, J., Yu, D., Deng, L., and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.