The authors present findings across various tasks aimed at identifying whether a given test example is out-of-domain or prone to misclassification. This is achieved by analyzing statistics derived from the softmax probability of the most likely class. While the raw score itself is not a particularly reliable confidence measure, the statistical patterns of out-of-domain examples differ sufficiently from in-domain examples, enabling their identification with a reasonable degree of certainty.
Below are my comments:
1. The authors correctly note that the AUROC/AUPR metric is threshold-independent. However, it is unclear whether the thresholds corresponding to specific operating points (e.g., a true positive rate of 10%) remain consistent across different datasets. In other words, it would be valuable to understand how sensitive these thresholds are to variations in test sets or different splits of the same test set. This is particularly relevant for scenarios where thresholds determined on a held-out set are applied to unseen data during evaluation.
2. The performance is evaluated using AUROC/AUPR, and comparisons are made against a random baseline. However, it is somewhat challenging to interpret the magnitude of improvement over the random baseline solely based on AUC/AUPR differences. It would be helpful if the authors could also provide statistical significance tests to quantify the strength of these differences, even though the observed gaps appear substantial in most cases.
3. In the speech recognition experiments discussed in Section 3.3, the evaluation methodology is not entirely clear. For instance, in Table 9, does an "example" refer to an entire utterance or a single (possibly stacked) speech frame? If each "example" corresponds to an utterance, are the softmax probabilities computed for the entire phone sequence by aggregating local probability estimates through a Viterbi decoding process?
4. I am curious about the rationale for excluding the blank symbol's logit in Section 3.3. Could the authors clarify why this step is necessary?
5. As I mentioned in the pre-review question, particularly in the context of speech recognition, it would have been insightful to compare the proposed model's performance against a simple generative baseline (e.g., GMM-HMM). Such a comparison would provide a useful benchmark for evaluating the model's ability to detect out-of-domain examples relative to a standard baseline.