Contributions  
This paper introduces an adaptation of batch normalization tailored for RNNs, specifically LSTMs, applied along the horizontal depth. Unlike prior work by Laurent (2015) and Amodei (2016), the study demonstrates that batch-normalizing the hidden states of RNNs can enhance optimization. Through quantitative experiments, the authors argue that the success of this approach hinges on proper parameter initialization, particularly for the gamma parameter. The experiments reveal modest performance improvements over standard LSTMs on tasks such as Sequential MNIST, PTB, Text8, and CNN Question-Answering.
Novelty and Significance  
Batch normalization has been instrumental in enabling the training of deeper networks (e.g., ResNets), and extending this technique to RNNs is a logical progression. This paper shows that such an extension is feasible with appropriate parameter initialization, in contrast to earlier findings by Laurent (2015) and Amodei (2016). The novelty lies in the choice of where to apply batch normalization (i.e., excluding the cell update) and the use of per-time-step statistics.  
However, incorporating batch normalization into LSTMs introduces additional computational complexity and bookkeeping. While the paper compares LSTM and BN-LSTM training speeds in terms of iteration count (e.g., Figure 2), it does not provide a wall-clock time comparison, which would have been valuable given the increased complexity of BN-LSTM.  
Given the widespread use of RNNs across various tasks, this work has broad relevance. That said, the performance improvements are generally modest and rely on several practical adjustments. Additionally, the paper does not address a key question: while batch normalization intuitively aids faster training, why does it also improve generalization?  
Clarity  
The paper is well-written, clearly motivated, and easy to follow. The proposed model is described in a straightforward manner, and the accompanying plots effectively support the key points.  
Summary  
This work represents an interesting but relatively incremental adaptation, demonstrating that batch normalization can be successfully applied to RNNs where previous efforts have fallen short. While the paper includes a thorough set of experiments, the empirical gains may not be substantial enough to justify the added model complexity and computational overhead.  
Pros  
- Successfully applies batch normalization to RNNs, overcoming challenges faced by prior work.  
- Provides a thorough empirical analysis of hyperparameter choices and activation behaviors.  
- Includes experiments across multiple tasks.  
- Clear and well-structured presentation.  
Cons  
- Relatively incremental contribution.  
- Relies on several practical adjustments (e.g., per-time-step statistics, noise addition for exploding variance, sequence-wise normalization).  
- Does not address computational overhead.  
- Limited to character- or pixel-level tasks; word-level tasks are not explored.