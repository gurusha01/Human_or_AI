This paper introduces a boosting-based ensemble approach for residual networks by adapting the Deep Incremental Boosting (DIB) method previously applied to CNNs (Mosca & Magoulas, 2016a). At each iteration \( t \), a new block of layers is added at a specific position \( p_t \), and the weights of all layers are transferred to the current network to accelerate training.
The proposed method lacks sufficient novelty, as it only slightly modifies the steps of DIB. Instead of appending a single layer to the end of the network, this approach inserts a block of layers at a position \( pt \) (starting from an initial position \( p0 \)) and merges layers accordingly, making it a minor adaptation of DIB.
The empirical evaluation does not incorporate data augmentation, leaving it unclear whether the observed improvements (if any) of the ensemble would persist with data augmentation. Additionally, one of the key baselines, DIB, lacks skip connections, which could compromise the fairness of the comparison. The authors justify excluding state-of-the-art ResNet variants by stating that their focus is on the ensemble approach. However, any potential gains from the ensemble may be overshadowed by inherent advantages of ResNet variants. Furthermore, the boosting procedure could be computationally prohibitive for large-scale datasets like ImageNet, where ResNet variants might outperform the proposed method. Consequently, the baselines should include state-of-the-art ResNets and Dense Convolutional Networks, as the current results appear preliminary.
Moreover, the sensitivity of the boosting process to the choice of injection points remains unclear.
While this paper adapts DIB for ResNets and provides some empirical results, the contribution is not sufficiently novel, and the experimental findings are inadequate to demonstrate the significance of the method.
Pros  
- Provides preliminary results for boosting applied to ResNets.  
Cons  
- Insufficient novelty: a minor incremental adaptation.  
- Empirical analysis is inadequate.  
Questions and Suggestions  
- Could you provide detailed information on the experimental setup, including parameters to be tuned and the algorithm used for training at each boosting step?  
- Could you elaborate on the network architecture and provide relevant references?  
- Could you include comparisons with state-of-the-art ResNet variants and Dense Convolutional Networks?  
- Could you provide a comparison of training time?  
- Do you have any results on ImageNet?