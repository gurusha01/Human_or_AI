This paper introduces a linear pipeline All-reduce method for parallel neural networks across multiple GPUs. The paper includes both theoretical analysis and experimental results. While the findings are intriguing, the manuscript's clarity and writing quality could be improved.
Comments:
- The authors compare their proposed method against several alternative approaches and demonstrate strong performance for their method. However, it is unclear whether the observed improvements stem from the proposed approach itself or from the specific implementation.
- The paper is challenging to follow, and the writing could be improved in several areas (in addition to addressing typos and missing references). In particular, the authors should provide more intuitive explanations of their proposed method in both the introduction and Section 3.
- The proposition and analysis in Section 3.2 do not support the claim that the communication cost of the linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as stated repeatedly in the paper. Instead, the analysis suggests that LP cannot achieve a speedup of 2x and log p over these methods. Specifically, Eq. (2) shows that TbroadcastBE / TbroadcastLP < 2, which does not establish an upper bound for TbroadcastLP and implies that it could be arbitrarily worse compared to TbroadcastBE. Therefore, rather than asserting TbroadcastBE / TbroadcastLP < 2, the authors should revise their claim to state TbroadcastBE / TbroadcastLP > 1 as n approaches infinity.
- Highlighting the differences between designing parallel algorithms for CPUs versus GPUs would help better motivate the paper and its contributions.