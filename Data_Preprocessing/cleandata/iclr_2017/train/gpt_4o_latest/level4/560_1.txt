This paper explores the influence of orthogonal weight matrices on the learning dynamics of RNNs. It introduces several compelling optimization strategies that enforce varying degrees of orthogonality in the recurrent weight matrix. The experiments yield key findings: strict orthogonality does not enhance learning, whereas soft orthogonality or orthogonal initialization significantly improves it. Although some proposed optimization methods involve computationally expensive matrix inversion, orthogonal initialization and certain soft orthogonality constraints are relatively efficient and hold promise for practical applications.
The experiments are conducted rigorously and provide valuable insights, with the writing being clear and accessible.
The experimental setup uses a fixed learning rate across different regularization strengths. However, learning speed may vary significantly depending on the learning rate, as different regularization strengths could allow for distinct maximal stable rates. It would be insightful to tune the learning rate for each margin individually (perhaps on shorter sequence lengths) to better understand how soft orthogonality influences learning stability. For instance, Fig. 5 indicates that a sigmoid prior improves stability, but slightly lowering the learning rate for the non-sigmoid Gaussian prior RNN might stabilize learning for weightings below 1.
Fig. 4 shows that singular values converge around 1.05 instead of 1. Would initializing to orthogonal matrices scaled by 1.05 offer any advantage over standard orthogonal initialization, particularly for tasks like the T=10K copy task?
The observation that "larger margins and even models without sigmoidal constraints on the spectrum (no margin) performed well as long as they were initialized to be orthogonal" suggests that deviation from orthogonality during training is not a significant issue for this task. This aligns with findings from Saxe et al. (2013), which showed that in deep linear networks, singular values initialized to 1 may diminish during training if required to implement the desired input-output mapping. More broadly, this raises the question of whether orthogonality is primarily beneficial as an initialization strategy, as proposed by Saxe et al., where it acts as a preconditioner that accelerates optimization without altering the fundamental problem, or as a regularizer, as suggested by Arjovsky et al. (2015) and Henaff et al. (2015), where it serves as an additional constraint in the optimization process. The experiments in this paper suggest that orthogonal initialization alone provides optimization speed benefits, while excessive regularization can degrade performanceâ€”indicating that significantly altering the optimization problem is counterproductive. This is evident in Fig. 2: for MNIST training loss, models with no margin perform nearly as well as those with margins of 1 or 0.1. However, in terms of accuracy, a margin of 0.1 performs best. This demonstrates that large or absent margins (i.e., orthogonal initialization) enable rapid optimization of the training loss, but among models achieving similar training losses, those with weights closer to orthogonality perform better. This distinction between the optimization speed advantage and the regularization benefit of orthogonality could be more explicitly addressed in the paper.
In summary, this paper offers a range of techniques and insights that are likely to prove valuable for training RNNs.