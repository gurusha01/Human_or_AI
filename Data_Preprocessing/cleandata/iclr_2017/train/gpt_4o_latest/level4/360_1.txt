In supervised learning, a major breakthrough was achieved when the semi-supervised learning framework was introduced, leveraging the weaker paradigm of unsupervised learning to infer certain properties, such as a distance metric or a smoothness regularizer, which could then be applied in conjunction with a limited set of labeled examples. This approach typically relied on the assumption of smoothness along the data manifold.
This paper endeavors to extend this analogy to reinforcement learning, though the analogy is somewhat tenuous. Rewards in RL are not directly analogous to labels in supervised learning, and positive or negative rewards do not carry the same meaning as positive and negative labels. Nevertheless, the paper makes a commendable effort to investigate this concept of semi-supervised RL, which is undoubtedly an important and underexplored area. The authors introduce the term "labeled MDP" to describe the standard MDP framework where the reward function is known. Conversely, they use the term "unlabeled MDP" to refer to the case where the reward function is unknown, though this is technically a controlled Markov process (CMP) rather than an MDP, which may lead to some confusion.
In the traditional RL transfer learning paradigm, the agent seeks to transfer knowledge from a source "labeled" MDP to a target "labeled" MDP (where both reward functions are known, but the learned policy is only available for the source MDP). In the proposed semi-supervised RL framework, the target is an "unlabeled" CMP, while the source includes both a "labeled" MDP and an "unlabeled" CMP. The core methodology involves applying inverse RL to infer the unknown "labels" (reward function) and subsequently constructing a transfer mechanism. For technical reasons, the scope is further restricted to linearly solvable MDPs. The authors present experimental results in three relatively complex domains simulated using the Mujoco physics engine.
While the work is intriguing, this reviewer believes it falls short of offering a simple and sufficiently general framework for semi-supervised RL that would garner broad interest within the RL community. Developing such a framework remains an open challenge for future research. Nonetheless, the current work is sufficiently compelling, and the problem it addresses is undoubtedly a valuable one to explore.