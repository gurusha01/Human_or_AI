This paper provides a valuable contribution by systematically and clearly analyzing the performance and trainability characteristics of various neural network architectures, particularly focusing on the basic RNN motifs that have gained popularity in recent years.
Pros:
- The paper tackles an important question that many in the community, including myself, have been eager to see addressed but lacked the computational resources to explore comprehensively. Leveraging Google's resources for this purpose is a commendable contribution to the field.  
- The study appears to have been conducted with care, lending credibility to the results presented.  
- The primary conclusion—that LSTMs are reliable in typical training environments but GRUs often emerge as the more practical choice—is both decisive and useful. While the nuances of this conclusion are more complex, the paper does an excellent job of discussing them in detail.  
- The emphasis on distinguishing between capacity and trainability is particularly valuable, as it clarifies misconceptions about why gated architectures are effective. The findings—that gated architectures are significantly more trainable but have slightly lower capacity than vanilla RNNs, with trainability outweighing capacity in challenging tasks—are insightful and well-articulated.  
- The observation about the near-equivalence of capacity when parameter counts are equal is highly informative.  
- The paper underscores the critical importance of hyperparameter tuning, an aspect that is sometimes overlooked amidst the proliferation of new architecture proposals.  
- The introduction of a metric to quantify the fraction of infeasible parameters (e.g., those leading to divergence) is a practical and thoughtful addition, addressing a common but often neglected issue in the field.  
- The manuscript is written with exceptional clarity, making it accessible and easy to follow.
Cons:
- The sections on UGRNNs and +RNNs feel somewhat preliminary. The authors have not convincingly demonstrated that the +RNN should be recommended with the same generality as the GRU. For example, stronger statistical evidence quantifying the performance differences between +RNN and GRU, particularly in Figure 4 (8-layer panel), would strengthen the case. The high standards set in the paper for declaring an architecture useful make the contributions of UGRNNs and +RNNs seem less impactful. That said, their inclusion in the paper is not problematic, as the primary goal of the work does not appear to be novelty, which is perfectly acceptable. However, I am uncertain how this will be received by the ICLR area chairs.  
- The paper provides limited detail about the hyperparameter tuning algorithm itself. While the authors mention:  
  > "Our setting of the tuner's internal parameters was such that it uses Batched GP Bandits with an expected improvement acquisition function and a Matern 5/2 Kernel with feature scaling and automatic relevance determination performed by optimizing over kernel HPs,"  
  and cite relevant references, replicating this approach would likely require additional details that are not provided.  
- Some figures, particularly Figure 4, are challenging to interpret at first glance due to their small panel sizes, the density of details, and suboptimal choices for visual clarity.  
- The neuroscience reference ("4.7 bits per synapse") feels somewhat tangential. The connection between the presented results and experimental neuroscience is weak or, at best, insufficiently explained. While this reference appears only in the discussion, it seems unnecessary and could be rephrased in less definitive terms. For instance, rather than claiming agreement between computational architectures and neuroscience, the authors could suggest: "We wonder if it is anything other than coincidence that our 5 bits result is numerically similar to the 4.7 bits measurement from neuroscience."