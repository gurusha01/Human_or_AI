This paper introduces a spatiotemporal saliency network designed to replicate human fixation patterns, thereby enabling the pruning of irrelevant video information and enhancing action recognition.
The proposed approach is compelling and demonstrates state-of-the-art performance in predicting human attention on action videos. Additionally, it shows potential for improving action clip classification.
However, the paper would benefit from a deeper exploration of the role of context in attention. For example, if context plays a significant role in guiding human attention, why is it not inherently incorporated into the model?
One notable limitation lies in the action recognition section, where the comparison between methods (1)(2) and (3) appears to be unbalanced. Specifically, the attention-weighted feature maps seem to reduce classification performance on their own, and only boost performance when combined with the original features via concatenation, effectively doubling the feature set and increasing model complexity.
Could the authors explore alternative strategies to integrate context and attention without relying on concatenation? The rationale for concatenating features from the original clip with those from the saliency-weighted clip seems to contradict the initial hypothesis that "eliminating or down-weighting pixels that are not important" would enhance performance.
Additionally, the authors should include the current state-of-the-art results in Table 4 for a more comprehensive comparison.
---
Other Comments:
Abstract:
- Typo: "mixed with irrelevant ..."
- The sentence "Time consistency in videos ... expands the temporal domain from few frames to seconds" is unclear and would benefit from a rewrite.
Contributions:
- Point 1: "The model can be trained without having to engineer spatiotemporal features" â€“ However, training data from humans would still need to be collected.
Section 3.1:
- The paper mentions that the number of fixation points is fixed for each frame. How is this achieved in practice?
- The authors state that the layers of the C3D network are frozen to values pretrained by Tran et al. What happens if gradients are allowed to flow back to the C3D layers? Would this not allow the features to be better optimized for the final task?
Section 3.4:
- The method for concatenating features should be described more precisely.
Minor Typo:
- "we added them trained central bias"