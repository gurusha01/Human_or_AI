This paper aims to learn a Markov chain to estimate a probability distribution over latent variables Z, enabling P(X | Z) to facilitate sample generation from a data distribution.
The paper, in its current state, is not suitable for acceptance due to the following issues:
1. Lack of quantitative evaluation. While the authors provide samples from the generative model, these are insufficient to adequately assess the model's performance. Refer to comment 2 for further elaboration.
2. The model's description is highly ambiguous. Significant effort was required to infer what the authors "might be doing." For instance, what does Q(Z) represent? Does it refer to the true posterior P(Z | X)? What exactly constitutes the generative model here? Conventionally, it is expressed as P(Z)P(X|Z). Variational Autoencoders (VAEs) typically use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are the authors suggesting that their model can directly sample from the true posterior P(Z | X)?
Comments:
1. Adding noise to the input does not appear to be a reasonable approach. Could the authors provide a justification for this choice?
2. Methods that learn transition operators are generally well-suited for data augmentation-based semi-supervised learning. The authors are encouraged to enhance their paper by evaluating their model on semi-supervised learning benchmarks.