This paper thoroughly investigates the concept of quantizing convolutional layers to 3 bits, employing distinct positive and negative per-layer scales. It presents a comprehensive analysis of performance, demonstrating virtually no degradation, on real-world benchmarks (notably avoiding reliance on MNIST).
The significance of this work lies in its potential to establish a lower bound for quantization methods that maintain performance integrity. As such, it could feasibly become the preferred approach for inference in resource-constrained environments and may inspire new hardware designs optimized for the proposed structure.
Additionally, the paper includes power measurements, which are arguably the most critical metric for practitioners in this domain. (Minor critique: I noticed the absence of power measurements for the full-precision baseline.)
I would have appreciated the inclusion of a state-of-the-art result on ImageNet and an evaluation on a robust LSTM baseline to strengthen the case further. A discussion on the wall-clock time required for training using this procedure would also have been valuable.