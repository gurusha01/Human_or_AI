SUMMARY  
This paper investigates the expressive capabilities of deep neural networks through various related expressivity metrics. It explores the connection between these metrics and the concept of "trajectory length," which is shown—both experimentally (at an intuitive level) and theoretically under specific assumptions—to grow exponentially with network depth in expectation. Additionally, the paper highlights the critical role of weights in the earlier layers of the network, as these significantly influence the classes of functions represented, and supports this claim with experimental evidence.
PROS  
The paper makes progress in understanding the expressive power of feedforward neural networks with piecewise linear activation functions, particularly by elaborating on the relationships between different perspectives on expressivity.
CONS  
While the paper delves into intriguing topics and provides further elaboration, it does not, in my view, introduce substantially novel contributions to the ongoing discussion.
COMMENTS  
- The paper is somewhat lengthy (especially the appendix) and appears to have been written in haste. While the main points are generally presented clearly, the results and conclusions could benefit from greater clarity regarding their assumptions and whether they are experimental or theoretical in nature. Additionally, the connections to prior work could be more explicitly articulated.
- On page 2, the statement: "Furthermore, architectures are often compared via 'hardcoded' weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another."  
This statement is only partially accurate and overlooks key aspects of the discussion in the referenced papers. For instance, [Montufar, Pascanu, Cho, Bengio 2014] does not focus on a single hardcoded function but rather on classes of functions characterized by a specific number of linear regions. That paper demonstrates that deep networks, in general, produce functions with at least a certain number of linear regions, whereas shallow networks cannot. Here, "in general" refers to the fact that, for a fixed number of parameters, the functions represented by the network (for parameter values within an open, positive-measure neighborhood) belong to the class of functions with at least the specified number of linear regions. Such results can also be interpreted in the context of networks with random weights.
- One of the expressivity measures discussed in this paper is the number of Dichotomies. In statistical learning theory, this concept is used to define the VC-dimension. A higher value in this context implies greater statistical complexity, meaning that selecting an appropriate hypothesis would require more data.
- On page 2, the statement: "We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length."  
The exponential growth of expected trajectory length with depth can be interpreted as the scaling effect of repeated compositions of the form \( a \cdot \ldots \cdot a \cdot x \), which scales inputs by \( a^d \). However, such scaling alone does not inherently explain the increase in the number of dichotomies, activation patterns, or transitions. It seems that the assumptions regarding the types of trajectories considered play a significant role here. This observation aligns with the statement on page 4: "if the variance of the bias is comparatively too large... then we no longer see exponential growth."
OTHER SPECIFIC COMMENTS  
- Theorem 1:  
It would be helpful to clarify the definition of a "random neural network," specifically whether it refers to a fixed connectivity structure with random weights. Additionally, the nature of the one-dimensional trajectory should be specified—e.g., whether it is finite in length, closed, differentiable almost everywhere, etc.
- The notation \( g \geq O(f) \) used in the theorem is ambiguous. Literally, it could be interpreted as \( |g| \geq k |f| \) for some \( k > 0 \) for sufficiently large arguments. Alternatively, it might suggest that \( g \) is not smaller than some function that is bounded above by \( f \), which holds, for example, when \( g \geq 0 \). To express asymptotic lower bounds more precisely, the notation \( \Omega \) should be used.