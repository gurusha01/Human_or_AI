The paper presents SampleRNN, a hierarchical recurrent neural network designed for modeling raw audio. The model is trained end-to-end and evaluated using log-likelihood metrics and human judgment of unconditional samples across three datasets encompassing both speech and music. The evaluation demonstrates that the proposed model performs favorably compared to the baselines.
The study highlights that the subsequence length used for truncated backpropagation through time (BPTT) has a significant impact on performance. Interestingly, a subsequence length of 512 samples (~32 ms) is sufficient to achieve good results, despite the fact that the modeled data features span much longer timescales. This finding is intriguing and somewhat counterintuitive, and I believe it merits further discussion.
The authors also attempted to reimplement WaveNet, a fully convolutional alternative model for raw audio. While they were unable to replicate the exact architecture described in the original WaveNet paper, they constructed a version with a receptive field of approximately 250 ms that could be trained within their computational constraints. This effort is commendable.
Although the architecture of the WaveNet model is described in detail, I found it challenging to locate equivalent details for the proposed SampleRNN architecture (e.g., the values of "r" used for different tiers, the number of units per layer, etc.). Additionally, a comparison of computational cost, training time, and the number of parameters between the models would be highly informative.
Surprisingly, Table 1 indicates that a vanilla RNN (LSTM) substantially outperforms the proposed model in terms of likelihood. This result is suspicious, as LSTMs typically have effective receptive fields of only a few hundred timesteps, whereas the WaveNet model's much larger receptive field should, in theory, yield better likelihood scores. Similarly, Figure 3 shows the vanilla RNN outperforming the WaveNet reimplementation in human evaluations on the Blizzard dataset. These results raise questions about the implementation of the WaveNet model, and some discussion on whether the authors anticipated these outcomes would be valuable.
Furthermore, Table 1 and Figure 4 reveal that the 2-tier SampleRNN outperforms the 3-tier model in terms of likelihood and human ratings, respectively. This result is counterintuitive, as one would expect longer-range temporal correlations to be more critical for music than for speech. This discrepancy is not addressed in the paper, and I believe it would be helpful to explore potential reasons for this behavior.
Overall, this work represents an interesting attempt to model very long sequences with long-range temporal correlations, and the results are compelling, even if the comparisons with baselines are not always entirely convincing. It would be intriguing to see how the model performs in conditional generation tasks, as this would allow for more direct and objective comparisons with models like WaveNet in that domain.
---
Additional Remarks:
- The upsampling of the model outputs is performed using `r` separate linear projections. This choice of upsampling method is not justified in the paper. Why not use simpler methods like linear interpolation or nearest-neighbor upsampling? What advantages does learning this operation provide? Do the `r` linear projections end up learning largely redundant features, apart from minor variations due to noise?
- In the third paragraph of Section 2.1.1, the authors mention using 8-bit linear PCM encoding. This contrasts with WaveNet, which employs 8-bit mu-law encoding, reportedly improving audio fidelity. Did the authors experiment with mu-law encoding as well?
- Section 2.1 discusses the discretization of the input and the use of a softmax to model this discretized input. However, no reference is provided to prior work that has made similar observations. While a reference is included in Section 2.1.1, it would be better to move it earlier to avoid giving the impression that this is a novel contribution.