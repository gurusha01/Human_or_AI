The paper introduces an intriguing incremental method for exploring new convolutional network hierarchies in a step-by-step manner, building upon a baseline network that has already achieved satisfactory recognition performance.
The experiments are conducted on the CIFAR-100 and ImageNet benchmarks, demonstrating how various ResNet models can be morphed into improved versions with slightly higher computational requirements.
While the baseline models are weaker than those commonly reported in the literature, the paper claims notable error reductions on both ImageNet and CIFAR-100 datasets.
The core idea of the paper involves decomposing convolutions into multiple smaller convolutions while increasing the number of filters. It is surprising that this approach achieves any improvement over the baseline models.
However, the paper does not provide experimental evidence for some of the fundamental assumptions underlying network morphing. This raises several key questions:
- How does the performance of morphed networks compare to networks with the same architecture trained from scratch?
- How does the incremental training time post-morphing compare to training a network from scratch?
- What accounts for the additional computational cost in the morphed networks?
- Why do the baseline ResNet models underperform compared to those reported in the literature and on GitHub? (For instance, the GitHub ResNet-101 model reportedly achieves a 6.1% top-5 error rate, whereas the paper reports 6.6%.)
Providing more evidence for the first three points would be crucial to substantiate the paper's claims.
The paper is reasonably well-written and generally comprehensible, but the lack of supporting evidence and the weaker baselines make its conclusions less convincing. I would be inclined to raise my evaluation score if additional experimental evidence were provided to address the main concerns outlined above.