The authors propose an adaptive softmax approximation designed to achieve faster performance on GPUs. The central idea, which is both logical and effective, involves employing a class-based hierarchical softmax, where the clusters and hierarchy are organized to ensure that the resulting matrix multiplications are optimally sized for GPU computation. This design is informed by their empirical evaluations. Their experimental results demonstrate that the proposed system performs exceptionally well.
Regarding the presentation, the paper exhibits a mix of clarity and ambiguity. On the positive side, the core concepts and reasoning are generally well-articulated. However, the writing itself is occasionally unclear. There are several minor typographical errors (as noted by AnonReviewer2, along with additional instances, such as the inconsistent use of \(xt\) in Section 3, where it appears to differ between the recurrent network description and the earlier discussion of feedforward networks—likely intended for the equation defining \(ht\); and the unconventional use of matrices \(A\) and \(P\) in Equation 2). Additionally, while Section 4.2 ("Intuition for 2-cluster case") is a valuable inclusion and provides helpful insights, the complexity analysis, though conceptually straightforward, could benefit from improved clarity. Specifically, this section would be more accessible with (a) the addition of another figure similar to Figure 2 and (b) a few additional sentences breaking down the argument into more digestible steps. For instance, the analysis leading to the complexity equation for placing the head of the distribution at the root of the tree only became clear to me after reviewing Equations (6) and (7) in conjunction with Figure 2. Including such an explanation—perhaps in an Appendix—might be the most effective way to enhance this section.