The authors consider the game of tic-tac-toe, selecting 1029 board configurations where a single move results in a victory for either the black or white player. There are 18 possible moves (2 players Ã— 9 locations), and a CNN is trained to predict these moves based on visual renderings of the game board. The CAM technique is employed to visualize the salient input regions responsible for the CNN's predictions. The authors observe that the predictions align with the winning board positions.
The authors make the following claims:
1. This is a very interesting finding.
2. The CNN has learned the game rules.
3. Cross-modal supervision can be applied to higher-level semantics.
However, I have concerns regarding these claims:
For (2): The claim that the CNN has learned the game rules is not substantiated by the experiments. The study only considers a single stage of the game (i.e., the final move), and the results are evaluated on the training set itself. A fundamental requirement for demonstrating an understanding of game rules is the ability to generalize to previously unseen states, which is not tested here. Even if the CNN exhibited generalization, I would caution against claiming that it has explicitly learned the game rules without further evidence.
For (3): The authors' definition of cross-modal supervision appears to involve training a model to map visual inputs (images of the board) to game moves. However, this is not novel, as CNNs have long been shown to perform mappings between different domains, such as images to labels in classification tasks. Furthermore, CNNs have been used to map images to actions in prior works, such as DQN by Mnih et al., DDPG by Lillicrap et al., and classical systems like ALVIN. The authors do not clearly articulate what new insights or contributions they are making in this regard.
For (1): The degree to which an implicit attention mechanism is interesting is subjective. The authors distinguish between the concepts of "what to do" and "what will happen," claiming that supervising for "what will happen" enables the CNN to learn "what to do." However, this idea has been extensively explored in the model predictive control (MPC) literature, where a predictive model ("what will happen next") is used to derive a control law ("what to do"). In the context of this paper, the distinction between "what will happen" and "what to do" seems unclear, as they appear to be identical in the experimental setup.
To further analyze what the CNN has learned, I recommend the following:
(a) Visualize CAM with respect to incorrect classes, such as the player losing instead of winning.  
(b) Split the data into training and validation sets, and use predictions on the validation set for visualization. This would provide more insight into the generalizable features the CNN focuses on.
In summary, understanding the decision-making process of CNNs is an important and interesting area of research. While the emergence of an implicit attention mechanism may be considered intriguing by some, many of the claims made by the authors are not adequately supported by the experiments (as detailed above).