This paper explores the usability of cortical-inspired distant bigram representations for handwritten word recognition. Rather than generating neural network-based posterior features for characters (optionally in local context), the approach utilizes posterior sets for character bigrams of varying lengths to represent words. The primary goal is to assess the feasibility of this methodology and compare it to the conventional approach.
Overall, the paper is well-written, though it lacks sufficient details regarding the comparison between the proposed method and the standard approach, as outlined below.
It would be beneficial to include the model complexity for all the models evaluated in this study, specifically the number of parameters used.
Language models are not incorporated in this work. Given that the models leverage different levels of context, language models are likely to have varying impacts on the different approaches. I recommend incorporating language models into the evaluation to provide a more comprehensive analysis.
In your comparative experiments, you restrict the dataset to 70% by focusing on longer words. However, it is well-documented that shorter words are more susceptible to recognition errors. This raises the question of whether this selection benefits one of the tasks disproportionately. Quantitative results addressing this issue should be provided to better assess the impact of using this constrained dataset. Without such clarification, I am hesitant to agree with the claim in Sec. 5 that the error rates are competitive or superior to the standard approach.
While I understand the motivation for introducing unordered open-bigrams based on cognitive research evidence, from a decision-theoretic perspective, it is unclear why the sequential order is disregarded, given the inherently monotonous nature of the classification problem. It would be valuable to see an experiment that isolates the effect of order by varying only this aspect, allowing for a clearer distinction between the impact of order and other components of the proposed approach.
End of page 1: Please clarify what is meant by "whole language method."
Page 6: Define the notation for rnn_d(x,t).
The number of targets for the RNNs modeling order 0 (essentially unigrams) differs significantly from the RNNs modeling orders 1 and higher. Consequently, the precision and recall values in Table 2 are not directly comparable between order 0 and orders ≥1. At a minimum, the column for order 0 should be visually separated to emphasize this distinction.
Minor Comments:
- A spell check is recommended.
- p. 2: "state-of-art" → "state-of-the-art"
- p. 2: "predict character sequence" → "predict a character sequence"
- p. 3, top: "Their approach include" → "Their approach includes"
- p. 3, top: "an handwritten" → "a handwritten"
- p. 3, bottom: "consituent" → "constituent"
- p. 4, top: "in classical approach" → "in the classical approach"
- p. 4, top: "transformed in a vector" → "transformed into a vector"
- p. 5: "were build" → "were built"
- References: Correct the first author's name: "Thodore Bluche" → "Theodore Bluche"