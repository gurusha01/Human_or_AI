This paper extends STOKE (Schkufza et al., 2013), a superoptimization engine for program binaries. STOKE operates by starting with an existing program and proposing modifications based on a fixed proposal distribution. Proposals are accepted or rejected using the Metropolis-Hastings criteria, which considers both program correctness and performance. This ensures that the MCMC process converges to correct, high-performance programs. The key contribution of this work is the introduction of a learned proposal distribution that conditions on program features (represented as a bag of words of all opcodes). The authors compare their method against two baselines: a uniform proposal distribution and a baseline where the proposal distribution is learned without conditioning on program features. Experimental results show that the proposed method achieves slightly better performance than the baselines.
However, the significance of this work for ICLR appears to be limited. The paper does not advance the field of representation learning but rather applies neural networks and REINFORCE to a task with non-differentiable components. Furthermore, the task of superoptimization may not align well with the interests of ICLR's audience. Conferences such as AAAI or UAI may be more appropriate venues for this work.
The proposed method does appear to be novel, as traditional MCMC-based synthesis methods lack learning components. Nevertheless, to make the contribution more compelling, the authors should demonstrate the applicability of their approach to other synthesis tasks or broader domains where MH-MCMC is used, and where a learned proposal distribution could provide significant benefits. Focusing solely on superoptimization, especially with marginal improvements over baselines, does not make the work sufficiently impactful.
Additionally, it is unclear whether any meaningful representation learning is taking place. The use of bag-of-words features to represent programs limits the neural network to learning simple correlations between opcode occurrences and beneficial modifications. This approach does not capture program semantics. A more interesting contribution would involve using a model like Tree-LSTM, which is designed to learn semantic representations of programs. The simplicity of the proposed learning method diminishes the paper's potential as a strong candidate for acceptance.
Clarifications in response to reviewer questions could improve the quality of this review. Thank you.