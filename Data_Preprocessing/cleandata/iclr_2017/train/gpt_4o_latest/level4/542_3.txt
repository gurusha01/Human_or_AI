The paper introduces a novel reinforcement learning (RL) framework designed to learn policies from sketches, which are sequences of high-level operations required to solve a specific task. The proposed model employs a hierarchical structure where the selection of sub-policies is conditioned on the current operation specified in the sketch. The learning process extends the actor-critic paradigm to accommodate this particular setting and incorporates curriculum learning strategies for tackling challenging tasks. Experimental results are presented across various learning scenarios and benchmarked against baseline methods.
The manuscript is well-structured and easy to comprehend. However, I am not fully convinced of the paper's impact, as the problem addressed can be interpreted as an option-learning task with richer supervision (i.e., the sequence of options is explicitly provided). This framing makes the problem comparatively simpler and limits its broader significance. Additionally, the practical relevance of the proposed approach is unclear, as it is not evident which real-world applications align with this setting. For instance, learning from natural language instructions appears to be a more compelling and practical direction. Consequently, the contribution of this work is incremental, sharing significant overlap with existing hierarchical RL approaches, and lacks strong motivation or a clear application domain. As a result, the paper holds limited value for the RL community.
@pros:
* Novel problem formulation with well-executed experiments  
* Straightforward adaptation of the actor-critic framework for learning sub-policies  
@cons:
* Simplified task that can be viewed as a reduced version of more complex problems, such as option discovery, hierarchical RL, or learning from instructions  
* Absence of a strong application context to enhance the relevance and appeal of the proposed approach