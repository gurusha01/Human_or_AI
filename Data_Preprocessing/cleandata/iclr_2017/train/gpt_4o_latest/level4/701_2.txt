This paper proposes a novel approach to finetuning by freezing the original network and augmenting it with an additional model. The concept is intriguing and complements existing training and finetuning methodologies. However, I believe there are a few baseline approaches that should be considered for comparison, such as:
(1) Ensemble methods: Fundamentally, the proposed approach bears similarities to ensembling, where multiple networks are combined to produce a final prediction. The method illustrated in Figure 1 should be evaluated against ensemble baselines—specifically, using multiple source domain predictors, potentially structured in a modular fashion similar to the proposed method, to compare their performance.
(2) Comparison with late fusion: Another relevant baseline would involve combining a pretrained network with a finetuned version of the same network and performing late fusion. How does the proposed method compare to such an approach?
The argument presented in Section 3.2 (and illustrated in Figure 4) is compelling: finetuning with limited data can degrade performance, which supports the rationale for freezing a pretrained network and augmenting it instead of modifying it. I agree with the authors on this point. However, aside from Figure 4, there appears to be limited empirical evidence to substantiate this claim.
Additionally, Figure 3 seems to indicate that certain module filters are either failing to converge or learning features that are not useful—such as the first two filters in Figure 3(a).
In summary, while the idea is promising and innovative, it could benefit from further development and stronger experimental validation. I am inclined to give a weak accept recommendation, but with low confidence due to the limitations in the experimental section.