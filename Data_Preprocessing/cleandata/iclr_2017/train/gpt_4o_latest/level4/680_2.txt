Update after reviewing the authors' responses and the revised paper (dated Dec 21):  
I have removed the comment regarding "insufficient comparison to past work" in the title and updated the score from 3 to 5.  
The primary reason for the score remains the novelty of the work. The introduction of HGRU and the use of the R matrix primarily aim to address the decision of "whether to continue from character-level states or switch to word-level states." These solutions appear to be tailored to symbolic computation frameworks like Theano (used by the authors) and TensorFlow. However, this does not seem to be an issue for languages such as Matlab (used by Luong & Manning) or Torch.  
---
This is a well-written paper with thorough analysis, and I particularly appreciated Figure 5. However, I find the novelty of this work to be limited. While the title suggests a focus on learning morphology, the model does not explicitly enforce learning of morphemes or subword units. For instance, constraints could potentially be applied to the weights in \( w_i \) in Figure 1 to detect morpheme boundaries, or an additional objective like Minimum Description Length (MDL) could be incorporated (though it is unclear how such constraints could be seamlessly integrated).  
Additionally, I was surprised by the minimal comparison (only a brief mention) to the work of Luong & Manning (2016) [1], which trains deep 8-layer word-character models and achieves significantly better results on English-Czech translation (e.g., 19.6 BLEU compared to the 17.0 BLEU reported in this paper). The presentation of HGRU also feels unnecessarily complex. If I understand correctly, HGRU essentially determines whether to continue decoding at the character level or reset using word-level states at boundaries, which is similar to what was done in [1]. Luong & Manning (2016) achieve this more efficiently by avoiding the need to decode all target words at the morpheme level. It would also be helpful to know the speed of the proposed model in this submission. Ultimately, the main contributions of this paper seem to be the additional analyses on what character-based models learn and the inclusion of an extra RNN layer in the encoder.  
Minor comment: Please annotate \( h_t \) in Figure 1.  
[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL.