This paper introduces a novel metric, central moment discrepancy (CMD), for aligning two distributions, with potential applications in domain adaptation. Compared to the more widely recognized maximum mean discrepancy (MMD), CMD offers the advantage of avoiding excessive penalization of the mean, thereby allowing greater emphasis on the shape of the distribution around its center.
From a theoretical perspective, the discriminative power of CMD and MMD (i.e., their ability to distinguish between two distributions) should be comparable. However, in practical scenarios, CMD might perform better because MMD directly matches raw moments, which can disproportionately penalize distributions that are not zero-centered.
In this work, CMD is applied up to the Kth order, but only the diagonal entries of the central moments are included in the CMD objective. This choice appears to be driven primarily by computational efficiency. A natural comparison to MMD would involve explicitly incorporating raw moments up to the Kth order as well. Another useful comparison would be to include all moments—not just the diagonal terms—in the CMD objective. While this approach would be computationally demanding, it could feasibly be implemented for lower-order moments, such as the 1st and 2nd orders.
The experiments in the paper compare CMD in its current form only with kernelized MMD. As a result, the claim that explicit moment matching improves performance is not strongly substantiated. To solidify this claim, CMD should be compared against MMD with explicit raw moment matching.
The assertion that CMD avoids the kernel parameter tuning challenges associated with MMD applies only to kernelized MMD, not to explicit MMD. Furthermore, there are established methods for setting kernel parameters in MMD, as discussed in works such as:
- Sriperumbudur et al., Kernel choice and classifiability for RKHS embeddings of probability distributions  
- Gretton et al., A kernel two-sample test  
Additionally, approaches like multiple kernel learning (e.g., Li et al., 2015) eliminate the need for manual parameter tuning. The paper's approach to tuning the beta parameter for MMD is unconventional; at the very least, simple heuristics—such as dividing \(|x-y|^2\) by the dimensionality or using the mean pairwise distance—should be applied before exploring beta in the manner described.
Overall, CMD appears to have the potential to outperform MMD and could find applications across various domains. However, it also faces the limitation of not being easily kernelizable (though this could be viewed as either an advantage or a drawback, depending on the context). The experimental results supporting CMD's superiority could be made more compelling with additional comparisons and analyses.