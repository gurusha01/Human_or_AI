This paper introduces Hyperband, a hyperparameter optimization method in which the model is trained using gradient descent or another iterative process. The approach builds upon the successive halving combined with random search framework proposed by Jamieson and Talwalkar, addressing the tradeoff between training fewer models for longer durations versus training many models for shorter durations. The core idea involves conducting multiple rounds of successive halving, starting with the most exploratory configuration and progressively reducing the number of experiments in each round while allocating exponentially more resources to the remaining ones. Unlike other recent methods in this domain, Hyperband does not depend on a specific model of the underlying learning curves, thereby making fewer assumptions about the model's behavior. The results indicate that this approach can be highly effective, often achieving significant speedups compared to sequential methods.
Overall, I find this paper to be a valuable contribution to the hyperparameter optimization field. The method is relatively straightforward to implement and appears to be effective across a variety of problems. It represents a natural extension of the random search paradigm to scenarios involving early stopping. In my view, Hyperband seems particularly well-suited for problems where (a) random search is expected to perform well and (b) the computational budget is limited, making it impractical to achieve the absolute best performance but where near-optimal performance is sufficient. I would, however, like to see the plots in Figure 3 extended to allow the other methods enough time to converge, as this would help clarify whether there is a significant gap between optimal and near-optimal performance.
That said, I am uncertain about the inclusion of random2x as a baseline. While I understand its utility in showcasing the advantages of parallelism over sequential methods, most of the other methods also have parallelized versions. If random2x is included, I would recommend also including baselines such as SMAC2x, Spearmint2x, TPE2x, etc. Additionally, it would be interesting to see how Hyperband performs against baselines with higher degrees of parallelism, such as 3x, 10x, and beyond.