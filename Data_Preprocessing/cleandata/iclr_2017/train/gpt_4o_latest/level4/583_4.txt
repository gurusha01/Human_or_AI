This paper introduces a quantitative metric for assessing the out-of-class novelty of samples generated by generative models. The authors tested their proposed metric on over 1000 models with varying hyperparameters and conducted a human subject study on a subset of these models.
While the authors acknowledged challenges in conducting human subject studies, they did not provide sufficient details about their own experimental setup. Specifically, the use of an "in-house" annotation tool is mentioned, but critical information is missing, such as the number of participants involved, their backgrounds, and the number of samples each participant evaluated. This raises concerns about the diversity and representativeness of the subjects, as there may be too few participants who are exposed to an excessive number of samples and/or possess expertise in this domain.
The paper aims to propose a general-purpose metric for novelty evaluation; however, the experiments are limited to a single scenarioâ€”generating Arabic digits and English letters. This narrow scope does not provide adequate evidence to support the generalizability of the proposed metric.
Additionally, the choice of defining English letters as "novel" compared to Arabic digits is debatable. For instance, how would the metric perform if the model generated Arabic or Indian letters instead? Can a person unfamiliar with Arabic handwriting distinguish it from random doodles? What makes English letters inherently more "novel" than random doodles? These questions, in my view, are better addressed through large-scale human subject studies involving tasks with clear real-world significance. For example, a meaningful study could ask participants to choose between painting A (generated) and painting B (created by an artist).