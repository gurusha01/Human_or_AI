The paper introduces a modification to the denoising autoencoder objective by incorporating an additional term. This term is well justified, as it creates an asymmetry between the encoder and decoder, compelling the encoder to learn a compressed and denoised representation of the input. To address the trivial solution that the new term might induce—where the encoder scales down the code magnitude and the decoder scales it back up—the authors propose using tied weights or a normalized Euclidean distance error. The proposed autoencoder framework bears a strong resemblance to several other autoencoder variants that have been present in the literature for quite some time. The authors validate their approach using 2D toy data distributions and the MNIST dataset. While the work is conceptually well-grounded, it appears to be an incremental and empirically underexplored extension of an established idea.