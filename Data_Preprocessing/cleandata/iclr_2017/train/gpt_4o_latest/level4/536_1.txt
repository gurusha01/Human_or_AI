SUMMARY  
This paper investigates the number of hidden units and training examples required to learn functions from a specific class. This class consists of Boolean functions with a bounded variability in their outputs.
PROS  
The paper brings forward intriguing results from theoretical computer science to explore the efficiency of representing functions with limited variability using shallow feedforward networks composed of linear threshold units.
CONS  
The analysis is restricted to shallow networks and primarily relies on synthesizing existing results without introducing substantial novel contributions. Additionally, the presentation of the main results and conclusions is somewhat unclear, as the terms and constants involved do not clearly illustrate the relationship between increased robustness and the reduced number of required hidden units.
COMMENTS  
- The abstract states, "The universal approximation theorem for neural networks says that any reasonable function is well-approximated by a two-layer neural network with sigmoid gates but it does not provide good bounds on the number of hidden-layer nodes or the weights." On page 1, the paper refers readers to a review article. Including more recent references would enhance the discussion.  
- Given the motivation outlined in the abstract, it would be beneficial to include a discussion of works that address the classes of Boolean functions representable by linear threshold networks. For example, the paper Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes by Wenzel, Ay, and Paseman examines various classes of functions representable by shallow linear threshold networks and provides upper and lower bounds on the number of hidden units required for representing different types of Boolean functions. Notably, it also offers lower bounds on the number of hidden units necessary for a universal approximator.  
- It would be valuable to analyze the learning complexity of the presented results using measures such as the VC-dimension.  
- Thank you for clarifying the constants. If noise sensitivity is held constant, larger values of epsilon correspond to smaller values of delta and 1/epsilon. However, the description in Theorem 2 uses poly(1/epsilon, 1/delta), which could still increase. Similarly, in Lemma 1, does reducing sensitivity at constant noise increase the bound on k?  
- The independence of the results from n appears to stem from defining noise sensitivity as an expectation over all inputs. This aspect warrants further discussion. A good starting point could be to provide examples of functions with bounded noise sensitivity (beyond the linear threshold functions discussed in Lemma 2). Additionally, reverse statements to Lemma 1 would be insightful, particularly in describing the noise sensitivity of juntas, even if only through simple examples.  
- On page 3, the phrase "variables is polynomial in the noise-sensitivity parameters" should likely read "inverse of."  
MINOR COMMENTS  
- On page 5, "Proposition 1" should likely be "Lemma 1."