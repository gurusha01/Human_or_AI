The paper addresses the problem of semi-supervised reinforcement learning, where the goal is to differentiate between labelled MDPs that provide rewards and unlabelled MDPs that lack any reward signal. The approach is straightforward, as it involves simultaneously learning a policy using the REINFORCE algorithm with entropy regularization, while also learning a reward model (similar to inverse reinforcement learning) to provide feedback for the unlabelled MDPs. The experiments are conducted on various continuous domains and yield promising results.
The paper is well-written and easy to follow. It builds on a simple yet effective idea of jointly learning the policy and a reward model, leading to an algorithm with interesting properties. While the proposed idea is relatively straightforward, the authors are the first to explore and test such a model. The experiments, though convincing, could be further strengthened by incorporating a mix of continuous and discrete problems.