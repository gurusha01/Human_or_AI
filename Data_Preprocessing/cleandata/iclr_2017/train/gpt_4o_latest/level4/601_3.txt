Summary: The paper introduces a new machine comprehension dataset named NEWSQA, which comprises over 100,000 question-answer pairs derived from more than 10,000 CNN news articles. It provides an analysis of the types of answers and reasoning required to address the questions in the dataset. Additionally, the paper evaluates human performance and two baseline models on NEWSQA, comparing these results with the performance on the SQuAD dataset.
Strengths:
1. The paper contributes a large-scale dataset specifically designed for machine comprehension tasks.
2. The method for collecting questions appears reasonable for generating exploratory questions, and the inclusion of an answer validation step is a commendable feature.
3. The paper introduces a novel and computationally more efficient implementation of the match-LSTM model.
Weaknesses:
1. The human evaluation presented in the study is insufficient, as the reported human performance is based on a very small subset (200 questions). This limited sample size raises concerns about the reliability of these results as a representative measure of human performance across the entire dataset, which contains thousands of questions.
2. The NEWSQA dataset bears significant similarities to the SQuAD dataset in terms of size, question type (natural language questions posed by crowdworkers), and answer format (text spans from related paragraphs). The paper attempts to demonstrate that NEWSQA is more challenging than SQuAD through two empirical observations:  
   a) The gap between human and machine performance is larger in NEWSQA compared to SQuAD. However, since the human performance is based on a small subset, this trend may not hold when human performance is evaluated on the full dataset.  
   b) Sentence-level accuracy is lower in NEWSQA than in SQuAD. However, as noted in the paper, this difference could be attributed to variations in document lengths between the two datasets, making this metric an unreliable indicator of NEWSQA's difficulty relative to SQuAD.  
   As a result, it remains unclear whether NEWSQA is genuinely more challenging than SQuAD.
3. While the authors claim that BARB is computationally more efficient and faster than match-LSTM, the paper does not provide quantitative evidence or specific metrics to substantiate this claim.
4. On page 7, in the "Boundary pointing" section, the paper should clarify the meaning of "s" in "n_s."
Review summary: Although the dataset collection approach is both interesting and promising, stronger evidence is needed to address the following points:  
1. Human performance should be evaluated on the entire dataset or at least a significant portion of it.  
2. A robust empirical study should be conducted to convincingly demonstrate that NEWSQA is more challenging (or superior in some other way) compared to SQuAD.