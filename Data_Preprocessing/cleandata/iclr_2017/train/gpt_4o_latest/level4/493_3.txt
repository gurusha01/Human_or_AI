The paper introduces an alternative approach to conditional maximum log-likelihood for training discriminative classifiers. The authors argue that conditional log-likelihood serves as an upper bound on the Bayes error, which deteriorates during training. To address this, the paper proposes improved bounds that are computed and optimized through an iterative algorithm. Extensions of this method are explored for regularized losses and a simplified form of policy learning. The approach is evaluated on various datasets.
A notable aspect of this work is its challenge to a widely accepted methodology for training classifiers. The proposed idea appears promising, and some results lend support to its validity. However, this remains a preliminary study, and further development of the concepts would be desirable. Overall, the paper lacks coherence and depth: the section on policy learning is poorly integrated with the rest of the paper, and the connection to reinforcement learning is not well justified in the two examples (ROC optimization and uncertainties). Additionally, the experimental section requires revisionâ€”for instance, the absence of a legend to distinguish the curves in the figures makes it difficult to interpret the results.