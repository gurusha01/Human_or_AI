In this paper, the author investigates the convergence dynamics of a single-layer non-linear network under the assumption of Gaussian iid inputs. The first part of the paper, which focuses on a single hidden node, is relatively clear, though I have raised some specific questions below. However, the second part, which extends the analysis to multiple hidden nodes, is much harder to follow, and the final "punchline" remains unclear. I suggest that the author prioritize building intuition in the main text and move detailed derivations and dense symbolic expressions to an appendix.
Regarding the significance of the work, it is challenging to assess how broadly applicable these results are. The Gaussian input assumption is quite restrictive, as is the assumption of iid inputs. In practice, real-world feature inputs are often highly correlated and rarely Gaussian. These assumptions are not typically made in recent works analyzing the convergence of deep networks, such as Kawaguchi, NIPS 2016. While the author claims that no independence assumption is made on the activations, this independence is effectively transferred to the input. If the activations are linear combinations of iid random variables, they are likely Gaussian-like, correct? This raises questions about the generality of the results.
Specific comments:
1. Please use the notation \( D_w \) instead of \( D \) to clarify that \( D \) is a function of \( w \), not a constant. This distinction becomes particularly confusing when transitioning between \( D(w) \) and \( D(e) \) in Section 3. Overall, the paper's notation is difficult to follow and should be introduced more systematically.
2. In Section 3, the statement "when the neuron is cut off at sample \( l \), then \( (D^{(t)})_u \)" is unclear. What is the relationship between \( l \) and \( u \)? This is another example of notational inconsistency that creates confusion for the reader.
3. In Section 3.1, the definitions of \( F(e, w) \) and \( D(e) \) are unclear. Why is \( D(e) \) introduced, and how does it relate to the rest of the analysis?
4. Theorem 3.3 appears to suggest that, for maximal probability of convergence when \( \epsilon > 0 \), \( \epsilon \) should approach 0. This implies that the radius \( r \) of the ball \( B_r \) should shrink to 0. However, this interpretation seems to contradict the behavior shown in Figure 2. Could the author clarify this apparent inconsistency?
5. Section 4 is particularly unclear, especially regarding the role of the symmetry group. What does the symmetry group represent, and is there an intuitive explanation for why it is important?
6. In Figure 5, what does \( a_j \) represent? This is not explained in the text.
I strongly encourage the author to rewrite the paper with a focus on improving clarity. In its current form, it is very difficult to extract the key takeaways from the work.