The authors introduce methodologies for wild variational inference, where the variational approximating distribution may lack a directly accessible density function. Their framework leverages Stein's operator, which operates on a given function and produces a zero-mean function with respect to a specified density function, even if the latter is unnormalized.
Quality:
The derivations appear to be technically robust. However, the authors seem to lack thoroughness and objectivity in assessing both the strengths and limitations of their proposed approach. For instance, how does the method scale to scenarios where the target distribution is high-dimensional? The logistic regression example presented involves only 54 dimensions. How would this technique perform in the context of neural networks, where the number of parameters is significantly larger? Furthermore, the logistic regression model is relatively simple, with a posterior likely to resemble a Gaussian distribution. How would the method fare in more complex posterior distributions, such as those encountered in Bayesian neural networks?
Clarity:
The paper is not well-written and is difficult to follow, lacking focus. The authors discuss an excessive number of methods, including: 1) Stein's variational gradient descent (SVGD), 2) Amortized SVGD, 3) Kernelized Stein discrepancy (KSD), and 4) Lavengin inference network, in addition to introducing Stein's discrepancy. With so many techniques presented, it is challenging to pinpoint the paper's core contributions.
Originality:
The originality of the proposed contributions is unclear. The first method described appears to overlap with prior work, specifically:
Wang, Dilin and Liu, Qiang. Learning to draw samples: With application to amortized MLE for generative adversarial learning. Submitted to ICLR 2017, 2016.
The authors should clarify how their work differs from this earlier study.
Significance:
Assessing the significance of the proposed methods is difficult. The authors only present results on a one-dimensional toy problem involving a mixture of Gaussians and a logistic regression model with 54 dimensions. Both cases involve relatively simple, low-dimensional distributions. In the regression example, the posterior is also likely close to Gaussian, making it unclear what advantages the proposed method offers over simpler approaches. Additionally, the authors do not compare their method against basic variational techniques that use Gaussian approximations.