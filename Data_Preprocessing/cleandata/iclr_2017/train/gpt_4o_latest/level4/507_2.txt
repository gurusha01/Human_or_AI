This paper introduces a novel benchmark for evaluating word representations: identifying the "odd one out." The authors expand on a concept recently proposed at the RepEval workshop but significantly scale up the number of examples by leveraging existing ontologies.
While the contribution is somewhat incremental, it represents an important advancement in establishing challenging benchmarks for general-purpose word representations. Humans, when equipped with sufficient domain knowledge, can perform this task with near-perfect accuracy, yet the experiments in this paper demonstrate that current embeddings fall short. The technical contribution is well-executed; the dataset construction is methodical, and the correlation analysis is compelling. I recommend its acceptance at ICLR.