In consideration of the authors' responsiveness and the revisions made to the manuscript—particularly the clarification of the meta-learning task—I am revising my score to an 8.
---
This paper addresses the challenge of few-shot learning with neural networks by employing meta-learning, a well-established concept that has experienced renewed interest in the past year. The authors conceptualize few-shot learning as a sequential meta-learning problem, where each "example" consists of a sequence of "training" batches, followed by a final "test" batch. At each "step," the inputs include the outputs of a "base learner" (e.g., training loss and gradients) along with the base learner's current state (parameters). The proposed approach uses an LSTM to tackle this meta-learning problem, leveraging the inner memory cells in the second layer to explicitly model the updated parameters of the base learner. The authors draw parallels between the update rules of LSTM memory cells and gradient descent. The LSTM meta-learner is updated based on the prediction loss of the base learner on the final "test" batch. Several simplifying assumptions are made, such as sharing weights across all second-layer cells (analogous to using a uniform learning rate for all parameters). The paper replicates the Mini-ImageNet dataset introduced by Vinyals et al. (2016) and demonstrates that the LSTM meta-learner achieves competitive performance with the state-of-the-art (Matching Networks, Vinyals 2016) on both 1-shot and 5-shot learning tasks.
Strengths:
- Framing the few-shot learning problem as a sequential (meta-)learning task is both intriguing and, in hindsight, intuitive. While the general idea of persisting learning across a series of tasks is not novel, the authors have made meaningful contributions to advancing the state of the art. However, I cannot definitively assess the novelty of this work as I am not deeply familiar with all recent developments in meta-learning.
- The proposed method is competitive with and surpasses Vinyals et al. (2016) in 1-shot and 5-shot Mini-ImageNet experiments.
- The base learner employed here (a simple ConvNet classifier) differs significantly from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals et al. (2016). It is always encouraging to see state-of-the-art results achieved through fundamentally different methodologies rather than incremental improvements.
- The insight regarding the relationship between LSTM memory cells and gradient descent updates appears to be novel and is compelling in its own right.
- The paper provides several practical insights for designing and training an LSTM meta-learner, which should facilitate reproducibility and inspire further applications of these ideas. These include recommendations for proper initialization, weight sharing across coordinates, and normalizing/rescaling inputs such as loss, gradients, and parameters. While some of these insights have been previously discussed (e.g., simulating test conditions during meta-training and assuming independence between meta-learner and base learner parameters when computing gradients), the authors' discussion remains valuable.
Weaknesses:
- The writing is often dense and difficult to follow. While the work itself is highly interesting, the paper is not particularly reader-friendly. I had to read through it multiple times and consult related literature to fully grasp the overarching learning problem. The task description in Section 2 (Page 2) is especially abstract, relying on notation and terminology that may be unfamiliar to those outside this subfield. Including a concrete example (e.g., based on MNIST) alongside a diagram illustrating a sequence of few-shot learning tasks would make the paper more accessible to a broader audience.
- Relatedly, the exact formulation of the N-class, few-shot learning problem remains unclear. For instance, the Mini-ImageNet dataset comprises 100 labels, with 64/16/20 used for meta-training/validation/testing. Does this imply that only 64/100 classes are encountered during meta-training? Or does it mean that each batch contains a subset of 64/100 classes, but all 100 are eventually seen during meta-training? If it is the former, how many outputs does the ConvNet base learner's softmax layer have during meta-training? Is it 64 (limited to the observed classes) or 100 (with 36 classes never observed)? Several similar details are ambiguous (see specific questions).
- The plots in Figure 2 are not particularly informative on their own, and the discussion section does little to provide meaningful insights about them.
Conclusion:
This is a strong paper with compelling results, and it appears to be a clear accept. However, the presentation of the ideas and methods could be significantly improved. If the writing is refined, I would be inclined to further raise my score.