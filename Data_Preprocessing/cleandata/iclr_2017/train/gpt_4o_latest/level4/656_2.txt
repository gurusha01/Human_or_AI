The paper explores a hybrid architecture that combines a scattering network with a convolutional network. By incorporating scattering layers, the model reduces the number of parameters and ensures that the initial layers are stable to deformations. Experimental results demonstrate that the hybrid network achieves reasonable performance and surpasses the network-in-network (NiN) architecture in scenarios with limited data.
A common question among researchers is why low-level features in convolutional networks need to be re-learned during every training process. Theoretically, using fixed features could reduce both the number of parameters and training time. To the best of my knowledge, this paper is the first to address this question. Based on the results, I believe the use of scattering features in the initial layers does not perform as well as learned CNN features. While this outcome is not entirely predictable a priori, making the findings noteworthy, I disagree with the authors' claim that the hybrid network offers superior generalization.
In the low-data regime, the hybrid network occasionally achieves higher accuracy than NiN, but NiN is a relatively old architecture, and its capacity has not been optimized for the dataset size. For the full dataset, fully learned models clearly outperform the hybrid network. Additionally, if I understood correctly, the authors have not directly compared identical architectures with and without scattering layers as the initial layers, which makes it harder to draw definitive conclusions.
The authors argue that the hybrid network benefits from the theoretical advantage of stability. However, only the initial layers of the hybrid network are stable, while the learned layers may still introduce instability. Moreover, if deep networks with potential instability outperform both scattering networks and partially stable hybrid networks, it raises questions about the practical significance of stability as defined in the context of scattering networks.
In summary, the paper addresses an important and relevant question, but I remain unconvinced that the hybrid network generalizes better than standard deep networks. While faster computation at test time could be advantageous, particularly for low-power or mobile devices, this potential benefit is not thoroughly explored in the paper.
Minor comments:
- Section 3.1.2: "learni"