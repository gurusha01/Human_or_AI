The paper presents GA3C, a GPU-based implementation of the A3C algorithm, which was originally developed for multi-core CPUs. The key contribution lies in the introduction of a queue-based system. These queues facilitate batching of data for prediction and training, enabling high GPU utilization. The proposed system is evaluated against the authors' own implementation of A3C as well as published benchmark scores.
The paper proposes a straightforward and intuitive approach to adapting A3C for GPUs. Aggregating prediction and training requests from multiple actors into batches to optimize GPU occupancy appears to be a sensible strategy, provided that latency does not become a bottleneck. The inclusion of an automatic performance tuning mechanism is also a commendable addition.
I value the clarification that GA3C achieves a throughput that is 20% higher than what is reported in the original A3C paper. However, what remains lacking is evidence that the learning speed or data efficiency is within a comparable range. Figure 3 in the paper contrasts scores obtained under significantly different evaluation protocols, rendering these results difficult to compare directly. A more compelling demonstration of comparable learning speed would involve time vs. score or data vs. score plots, showing similar or improved performance relative to A3C. For instance, this open-source implementation appears to achieve comparable performance on Breakout: