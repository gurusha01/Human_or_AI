The objective of this paper is to develop vector representations for boolean and polynomial expressions such that equivalent expressions are mapped to similar representations.
The proposed model builds upon the recursive neural network framework introduced by Socher et al. (2012). Using the syntactic parse tree of a formula (boolean or polynomial), the representation of each node is computed by applying a multi-layer perceptron (MLP) to the representations of its children. This process is recursively applied to generate the representation of the entire expression. Unlike Socher et al. (2012), the authors propose using multiple layers, which is particularly crucial for capturing operations like XOR (a point that is unsurprising). Additionally, the paper introduces a reconstruction error term, referred to as SubexpForce, which aims to ensure that the representation of a parent node can reconstruct the representations of its children (if my understanding is correct). The model is trained with a classification loss, where the label of an expression corresponds to its equivalence class. The approach is evaluated on randomly generated datasets and compared against baselines such as tf-idf, GRU-based RNNs, and standard recursive neural networks.
While I agree with the authors that learning effective representations for symbolic expressions and capturing compositionality is an important problem, I am not fully convinced by the experimental setup presented in the paper. Specifically, as noted by the authors, determining equivalence between boolean expressions is an NP-hard problem, and it is unclear whether the model achieves more than implicitly computing the truth table of the expressions. Although some parts of the paper are somewhat difficult to follow, the work is technically sound. The proposed model is well-suited to the problem and demonstrates superior performance compared to the baselines.
Pros:
- The model is relatively simple and conceptually sound.
- The use of a classification loss over equivalence classes is an interesting choice (though it should be compared to a similarity-based approach).
Cons:
- The experimental setup is unconvincing: it is doubtful whether the model can outperform truth table computation for boolean expressions (or evaluating polynomial expressions at random points in [0, 1]^n).
- Certain sections of the paper are difficult to follow (e.g., the justification for SubexpForce and the discussion on why softmax is ineffective).
- The paper lacks a comparison between classification loss and similarity-based loss.