In this paper, a well-established soft mixture of experts model is tailored to address a specific transfer learning problem in reinforcement learning (RL), specifically the transfer of action policies and value functions between related tasks. While the experimental setup bears a resemblance to hierarchical RL approaches, this connection is not explored in depth, which is a missed opportunity.
One potential implication of this study is that the choice of architecture and even the learning algorithm could be guided by the objective of the target task, rather than relying on manual design by the experimenter. This intriguing possibility is highlighted as a promising direction for future research.
---
Pros:
The paper provides a thorough explanation of how the proposed network architecture integrates with various widely used reinforcement learning frameworks, which could facilitate further exploration in this area.  
The experiments serve as strong proofs of concept, though they do not extend beyond that, in my opinion.  
Nevertheless, the findings suggest that ensembles of deep networks trained on related tasks exhibit better generalization to similar tasks when used collectively, as opposed to traditional transfer learning methods like fine-tuning.
---
Cons:
As noted in the related work section, the reuse of fixed policy libraries has been formally proposed in prior research for learning similar tasks. In hierarchical RL literature, it is well understood that reusing fixed (e.g., Fernandez & Veloso 2006) or jointly learned policies that apply to specific parts of the state space (e.g., options, Pricop et al.) can be advantageous. However, the challenge of constructing such libraries remains unresolved, and this paper does not provide significant insights into addressing this issue.  
The transfer tasks chosen effectively showcase the potential of the proposed architecture, but the study does not address negative transfer or compositional reuse in challenging scenarios previously identified in the literature (e.g., Parisotto et al. 2015, Rusu et al. 2015, 2016).  
Given that the primary contributions are empirical, it would be valuable to see how the results in Figures 6 and 7 compare when plotted against wall-clock time. Data efficiency is not a critical factor for achieving perfect play in Pong (see Mnih et al., 2015), so it would be more informative to evaluate tasks where performance is constrained by data availability. Additionally, it would be interesting to assess whether the presented results could be achieved with reduced computational resources or smaller representation sizes compared to learning from scratch, particularly when one of the source tasks is a policy already trained on the target task.  
Finally, it is somewhat disappointing that the model requires a quarter of the data needed to learn Pong from scratch just to determine that an optimal Pong policy is already present in the expert library. Simply evaluating each expert over 10 episodes and using an average-score-weighted majority vote to combine action choices would likely achieve similar performance with a fraction of the data.