This paper appears to be the first (to my knowledge) to demonstrate a straightforward yet significant finding: ConvNets used as NMT encoders can perform competitively with RNNs. The authors provide a compelling set of results across multiple translation tasks, benchmarking against highly competitive baselines. I also value the detailed discussion on both training and generation speed. I find it particularly intriguing that position embeddings play a crucial role (alongside residual connections); however, there is limited analysis to further illuminate this aspect or to explore alternative methods for encoding positional information (e.g., a speculative idea might involve embeddings that capture relative positions). My primary concern (echoing another reviewer) is that this work might be better suited for an NLP-focused conference.
A minor note: it feels slightly unusual that such a well-executed paper does not include a single figure illustrating the proposed architecture. Including a diagram for the biLSTM architecture as well would enhance clarity, especially since understanding the last paragraph of Section 2—particularly the explanation of the linear layer used to compute z—requires some effort.