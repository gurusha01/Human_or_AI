The authors propose incorporating noise into the gradients during the optimization of deep neural networks using stochastic gradient-based methods. They present results across multiple datasets, demonstrating that the approach can mitigate poor parameter initialization and is particularly advantageous for training more complex architectures.
The method is evaluated on a diverse set of tasks and architectures. While the results would be more compelling if accompanied by confidence intervals, I recognize that some experiments likely required significant computational resources. I appreciate that the results include both scenarios where gradient noise provides substantial benefits and cases where its impact is minimal compared to other optimization or initialization techniques. The breadth of experiments and the diversity of models offer reasonably strong evidence that the effects of gradient noise generalize across various settings. However, some results are less convincing. For instance, in Section 4.2, the method only showed notable improvement under sub-optimal training schemes. Additionally, the results on MNIST fall short of state-of-the-art performance. Given the simplicity of the method, I had hoped for more theoretical justification of its utility. That said, the experimental analyses—such as the exploration of the annealing procedure, the comparison with gradient stochasticity, and the contrast with weight noise—provide valuable insights.
The paper is well-written and cites relevant prior work. The proposed method is described clearly and succinctly, which is expected given its simplicity.
The idea itself is not particularly novel. As the authors acknowledge, similar algorithms have been employed in training, and the approach closely resembles simulating Langevin dynamics, albeit with the goal of identifying a single optimum rather than approximating an expected value. The contribution lies in revisiting an established technique in the context of modern, larger, and more complex models.
Despite the method's lack of originality, I believe the results are meaningful. The approach is straightforward to implement and appears to be particularly effective for challenging models with difficult initialization. This makes it important for the broader community to be aware of the technique. I anticipate that many researchers will experiment with the method, and the variety of architectures and tasks where it proves useful suggests it could become a valuable addition to the optimization toolkit.
Pros:
- The method is simple to implement.
- It is evaluated across a wide range of tasks and diverse models.
- Includes interesting experiments comparing the method to related approaches and analyzing the annealing scheme.
- The paper is well-written.
Cons:
- The idea lacks originality.
- There is no strong theoretical motivation or analysis.
- Some results are unconvincing.