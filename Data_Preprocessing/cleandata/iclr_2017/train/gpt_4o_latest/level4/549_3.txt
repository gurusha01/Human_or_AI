First, I would like to express my gratitude to the authors for their responses and clarifications. The presentation of the multi-stage version of the model is now significantly clearer.
Pros:
+ The paper formulates a sparse coding problem using a cosine loss, enabling the problem to be addressed in a single pass.
+ The energy-based formulation facilitates bi-directional coding, integrating both top-down and bottom-up information during feature extraction.
Cons:
+ The computational cost of running the evaluation in a multi-class setting could be substantial, making the approach less appealing and potentially comparable in cost to recurrent architectures.
+ Although the model is competitive and shows improvement over the baseline, the paper would be more compelling with additional comparisons (see detailed comments). The experimental evaluation is limited to a single dataset and a single baseline.
---
The motivation behind the sparse coding scheme is to enable inference in a feed-forward manner. However, this property does not hold in the multi-stage setting, as optimization would be required (as clarified by the authors).
The concept of an efficient bi-directional coding scheme is highly interesting. However, as clarified by the authors, this may not necessarily be the case, as the model requires multiple evaluations to perform classification.
An intriguing alternative might be to run the model without any class-specific bias and then evaluate only the top K predictions using the energy-based setting.
With this in mind, it would be beneficial to include a discussion (or, if possible, direct comparisons) regarding the trade-offs associated with using a model like the one proposed by Cao et al. For example, considerations around computational costs and performance would be valuable.
Using bi-directional coding only in the top layers seems like a reasonable approach, as it allows for obtaining a good low-level representation in a class-agnostic manner. However, this aspect could be explored further, such as by empirically demonstrating the trade-offs. If I understand correctly, the paper currently reports only one specific setting.
Lastly, the authors highlight that one advantage of the proposed architecture is the spherical normalization scheme, which may lead to smoother optimization dynamics. Does the baseline (or the model) incorporate batch normalization? If not, it would be relevant to test its impact.
Minor Comments:
I find Figure 2(d) somewhat confusing. I would recommend not plotting this setting, as it does not represent a function (as noted by the authors in the text).