Review - Summary:  
This paper explores enhancements to the DaDianNao (DaDN) DNN accelerator by incorporating bit-serial arithmetic. Specifically, the authors replace DaDN's bit-parallel multipliers with serial x parallel multipliers, where weights are processed in parallel while activations are processed serially. By maintaining a constant number of adders while increasing the number of units, the design allows for flexibility in tailoring energy and time consumption to the bit-width of activations. The authors demonstrate how their approach can be applied to both fully-connected and convolutional layers in DNNs.  
Strengths:  
Adapting precision on a per-layer basis is a valuable concept, though it was previously introduced in Judd (2015).  
The evaluation, which includes synthesis but not place-and-route, is thorough. However, much of this evaluation is identical to what was presented in Judd (2016b).  
Weaknesses:  
The integration of bit-serial arithmetic into the DaDN architecture represents only a minor innovation.  
A significant portion of the content in this paper has already been published, particularly in Judd (2016b) at Micro 2016. The primary new contribution here is the analysis of the architecture's performance on fully-connected layers, while the rest of the work largely overlaps with prior publications.  
The energy savings achieved are minimal, as the energy overhead of shifting activations using flip-flops nearly cancels out the benefits of reduced precision arithmetic.  
The paper does not compare its approach to more conventional methods for variable precision, such as using bit-parallel arithmetic units with data gating for the least significant bits (LSBs). While such an approach would not provide speedup, it could potentially achieve greater energy savings than the proposed bit-serial x bit-parallel method.  
Overall:  
While the Tartan and Stripes architectures are intriguing, the incremental contribution of this paper—adding support for fully-connected layers—is minor compared to the three prior publications on this topic, particularly Judd (2016b). This idea merits a single strong paper, not four.