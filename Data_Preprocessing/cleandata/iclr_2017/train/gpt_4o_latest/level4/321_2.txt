Revised Score: 6 â†’ 7
The paper introduces a method for hierarchical reinforcement learning (RL) leveraging stochastic neural networks. It proposes an innovative approach by incorporating an information-theoretic measure of option identifiability as an auxiliary reward to encourage the learning of a diverse set of sub-policies. A particularly notable result is the comparison with a strong baseline that directly combines intrinsic rewards with sparse rewards, demonstrating that this seemingly smoother reward structure fails to solve the tasks. While the authors attribute this to challenges in long-term credit assignment and the benefits of hierarchical abstraction, another plausible explanation could be the diversity constraint applied during sub-policy training, which is absent in the baseline. This raises the question of whether such insights could inform improvements to the baseline or inspire new end-to-end hierarchical policy learning approaches, akin to those proposed in hierarchical REPS or option-critic frameworks. The visualizations provided are also commendable.
The paper explores an exciting direction but could be further enhanced by addressing the following points:
1) Limited Sub-Policy Diversification: The use of concatenation and bilinear integration restricts the differentiation of sub-policies primarily to the first hidden layer weights. While this is not problematic for the tested tasks, which mainly require similar locomotion policies with minimal diversification, this limitation could become more pronounced in tasks demanding highly diverse sub-policies. For example, in manipulation tasks, solving a problem from one state might differ significantly from solving it from another state. It would be valuable to evaluate the method on more challenging, non-locomotion domains where the ideal sub-policies exhibit greater diversity.
2) Constraints on Hierarchical Policies: The manager network is trained with fixed sub-policies, and the time steps for sub-policies are predetermined. This setup necessitates highly effective "intrinsic" rewards and pre-trained sub-policies to successfully address downstream tasks. Additional discussions or experimental results on how to handle such constraints would be beneficial, particularly in the context of end-to-end hierarchical policy learning.
3) Domain-Specific Nature of Intrinsic Rewards: Due to the constraints mentioned in (2), the intrinsic or unsupervised rewards appear to be domain-specific or akin to supervised rewards, which seems unavoidable under the current framework.
Lastly, a few clarifications and questions remain:
- What is the intuition behind the poor performance of the strong baseline in Section 7.3, which fails to solve the task? Assuming the "CoM maze reward" represents this baseline, does it account for the additional experience gained by the pre-trained policies?
- What exactly is meant by "multi-policy" in comparison to stochastic neural networks (SNN)?
- In Figure 1, is the following interpretation correct? 
  1) For concatenation, the categorical value determines a different bias for the first hidden layer.
  2) For bilinear integration, a different weight matrix connects the observation to the first hidden layer.
Overall, the paper presents a promising contribution to hierarchical RL, and addressing the aforementioned points could further strengthen its impact.