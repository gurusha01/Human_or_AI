The paper builds upon a recently introduced video frame prediction approach by incorporating reward prediction to model the unknown system dynamics and reward structure of an environment. The proposed method is evaluated on several Atari games and demonstrates a strong ability to predict rewards over a horizon of approximately 50 steps. The manuscript is well-written, focused, and provides a clear articulation of its contribution to the field. Both the experimental setup and the methodology are robust. However, the results are not particularly surprising, as the deterministic relationship between system states and rewards in Atari games implies that a network capable of effectively encoding future system states in its latent space can inherently decode the associated rewards. Consequently, the paper's contribution is relatively modest. The work would be significantly strengthened if the authors could implement and evaluate the two future directions mentioned in the conclusion: incorporating artificial samples during training and integrating Monte-Carlo tree search. These additions have the potential to reduce the reliance on real-world training samples and enhance performance, making the study more impactful and valuable.