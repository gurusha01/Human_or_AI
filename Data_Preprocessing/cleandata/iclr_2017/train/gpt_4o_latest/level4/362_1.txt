This paper contributes to the growing body of work on learning optimizers/algorithms, a topic that has recently garnered significant attention. The authors adopt the framework of guided policy search at the meta-level to train the optimizers. They further choose to train on random objectives and evaluate the transferability of the learned optimizers to a set of simple tasks.
As noted below, this is a valuable contribution.
However, the justification for using reinforcement learning (RL) as opposed to gradient-based methods at the meta-level, as discussed later in the paper, is unclear and unconvincing. I strongly recommend that the authors conduct an experiment comparing these two approaches and present the comparative results. This is a critical issue, as the scalability of the proposed method may depend heavily on this distinction. In fact, demonstrating both scalability to larger domains and effective transfer to those domains remains a central challenge in this area of research.
In conclusion, while the idea is promising, the experimental evaluation is lacking.
This work was posted on arXiv shortly before we were able to release our own paper on learning to learn by gradient descent by gradient descent. In our case, we were inspired by neural art and aimed to replace the lBFGS optimizer with a neural Turing machine (both of which share similar equation forms, as detailed in the appendix of our arXiv version). Ultimately, we opted for an LSTM optimizer trained via SGD.
In contrast, this paper employs guided policy search to determine parameter updates (policy). Notably, it also highlights the importance of transferring learned optimizers to new tasks.
Overall, this is a timely and meaningful contribution to the learning-to-learn literature, which has seen a surge in interest in recent months.