This paper introduces the concept of utilizing "surprisal" as a top-down signal in RNNs. Specifically, the authors incorporate the error from the previous prediction as an additional input at the current timestep in an LSTM.
The overarching idea of surprise-driven feedback is intriguing for online prediction tasks. It is a straightforward concept that appears to yield notable performance improvements. However, the paper, in its current state, has several critical shortcomings.
- The overall quality of the paper's writing could be enhanced. For instance, Sections 2.4 and 2.5 primarily consist of equations detailing the forward and backward propagation of feedback RNNs and feedback LSTMs. However, the authors do not provide any accompanying analysis to clarify the insights these equations aim to convey. Furthermore, feedback RNNs are not evaluated in the experimental section, leaving their inclusion in the paper unexplained.
- The experimental evaluation is insufficient. The authors only test their approach on a single dataset, enwik8. To establish the robustness of the feedback LSTM, it is essential to apply the method to multiple datasets and verify whether the observed improvements are consistent. Additionally, while the authors claim state-of-the-art performance on enwik8, the hypernetwork model, which is already cited in the paper, achieves better results (1.34 BPC, as reported in Table 4 of the hypernetworks paper).
- The comparisons presented in the paper are limited to methods that do not utilize the last prediction error as an additional signal. A comparison with dynamic evaluation would be more appropriate. While feedback LSTM incorporates the prediction error as an extra input during forward propagation, dynamic evaluation backpropagates the error through the network to update the weights. Although the two approaches differ in how they propagate prediction error, both leverage "extra" supervised information via these errors.
In summary:  
Pros:  
- The idea is novel and interesting.  
- Demonstrates potential for performance improvements.  
Cons:  
- Writing quality needs improvement.  
- Limited evaluation (only one dataset).  
- Comparisons exclude methods that utilize last-timestep error signals.