This paper introduces two pruning techniques aimed at reducing the computational demands of deep neural networks. Specifically, the methods enable the removal of entire feature maps and kernel connections with minimal impact on classification accuracy.  
However, the paper has the following issues:  
1) The proposed approach appears somewhat trivial, as the pruning masks are primarily determined through simple random sampling. This limits both the novelty and scalability of the method.  
2) The experimental results focus predominantly on classification accuracy and theoretical complexity. For a paper addressing computational efficiency, it is crucial to include results on practical time consumption. It is well-known that reducing the number of operations does not necessarily translate to reduced computational time on highly parallel platforms like GPUs.  
3) Improving computational efficiency for large-scale models (e.g., ImageNet classification networks) is more significant than for smaller models (e.g., MNIST, CIFAR networks). However, the paper does not include results on large-scale networks.  
4) (Logical validity of the proposed method) Regarding feature map pruning, what would happen if a reduced-size network were trained from scratch without transferring knowledge from the pretrained large network? Could the same accuracy be achieved? If so, this would suggest that the hyperparameters of the original network are suboptimal. Experimental evidence is needed to justify the necessity of feature map pruning.  
That said, I do agree with the idea that smaller networks may exhibit better generalizability compared to larger networks.  
---
Comments on the authors' response:  
Thank you for addressing my comments.  
1) I maintain my stance that the proposed methods lack novelty and remain trivial.  
2) The inclusion of GPU implementation is appreciated. However, how does the efficiency of the implementation compare to existing frameworks such as Torch, Caffe, or TensorFlow? Is the convolution operation optimized enough?  
3) Experiments on CIFAR-100 are an improvement over CIFAR-10, but this dataset is still not truly large-scale, where computational speed-ups are more critical. Datasets like ImageNet or Places would be more appropriate for demonstrating the method's scalability.  
4) The authors did not address the question regarding the validity of the proposed methods. This is a fundamental concern and requires clarification.