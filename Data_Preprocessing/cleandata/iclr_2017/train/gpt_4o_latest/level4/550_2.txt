The paper presents a novel regularization technique for denoising autoencoders, designed to explicitly enhance robustness in the encoding phase with respect to input perturbations. The proposed regularization term is justified by its role in minimizing the conditional entropy of the encoding given the input. The modified denoising autoencoders are tested on synthetic datasets and MNIST, and their performance is compared against standard autoencoders and denoising autoencoders. However, the approach bears significant resemblance to several existing autoencoder extensions, such as contractive autoencoders, which were not included in the comparisons. The experimental section requires further refinement, with additional details needed to improve the clarity and interpretability of the figures presented.