The authors present a dynamic neural Turing machine (D-NTM) model that addresses the limitations of rigid location-based memory access inherent in the original NTM model. The paper offers two primary contributions: 1) the introduction of a learnable addressing mechanism for NTM, and 2) the use of curriculum learning with hybrid discrete and continuous attention. The proposed model is empirically validated on the Facebook bAbI task, demonstrating performance improvements over the original NTM.
Pros:
+ Thorough comparisons between feed-forward and recurrent controllers  
+ Promising results on curriculum learning with hybrid discrete and continuous attention mechanisms  
Cons:
- The NTM baseline in Table 1 appears to be significantly weaker (31% error) compared to the 20% error reported in Table 1 of (Graves et al., 2016, Hybrid computing using a neural network with dynamic external memory). This discrepancy might be due to hyper-parameter tuning. Notably, the NTM baseline from (Graves et al., 2016) outperforms the proposed D-NTM with a GRU controller. It may be beneficial to reproduce their results using the hyper-parameter configurations outlined in their Table 2, as this could potentially enhance the performance of the D-NTM.  
- Section 3 is difficult to follow, and the overall clarity of the paper requires improvement.