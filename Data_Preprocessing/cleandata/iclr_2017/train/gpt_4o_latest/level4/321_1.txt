This paper presents an intriguing study on hierarchical control, bearing similarities to the work of Heess et al. The experimental results are robust, successfully addressing benchmarks that prior research was unable to achieve. However, the analysis of these experiments could be more thorough.
(1) As noted by other reviewers, the use of the term "intrinsic" motivation seems somewhat misaligned, particularly given its established connotation in reinforcement learning. Pre-training robots for locomotion by incentivizing speed (or rewarding grasping in the case of a manipulating arm) appears highly task-specific, aligning closely with the objectives they are later expected to perform. While not identical, the pre-training tasks resemble those outlined in Heess et al.
(2) The Mutual Information regularization is a sophisticated approach and generally performs well, yet it does not seem to provide improvements in the more complex mazes 1, 2, and 3. The authors acknowledge this limitation—could additional interpretation or analysis be provided to explain this outcome?
(3) The factorization between Sagent and Srest requires clearer elaboration in the paper. While Duan et al. specify Sagent, replicability demands that Srest be explicitly defined as well—was this detail overlooked, or did I miss it?
(4) An analysis of the agent's switching behavior would add valuable insights. More broadly, further exploration of the policies, including failure modes and the impact of switching times on performance, would have been a welcome addition.