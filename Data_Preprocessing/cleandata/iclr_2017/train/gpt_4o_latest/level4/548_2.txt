This paper introduces a novel approach for accelerating optimization near saddle points. The core concept involves repelling the current parameter vector from a running average of recent parameter values. The method demonstrates faster optimization compared to several other techniques across diverse datasets and architectures.
At first glance, the proposed method appears highly similar to momentum. It would be highly beneficial to include a clear diagram that illustrates how this method differs from momentum and why it might perform better near saddle points. The toy saddle example demonstrating improved convergence is insufficient in this regard—optimization speed comparisons are inherently challenging due to the influence of numerous details and hyperparameters. Instead, a conceptual diagram highlighting a critical scenario where CPN behaves differently from—and qualitatively outperforms—momentum would be far more insightful.
Another approach to clarify the relationship with momentum would be to derive a form of \( Rt(f) \) that reproduces the exact momentum update. This could then be compared directly to the \( Rt(f) \) used in CPN.
The overly general notation, such as \( \phi(W, W) \), should be avoided—Equation 8 alone suffices.
The theoretical results, including Equation 1 and Theorem 1, should be omitted, as they are irrelevant without specifying the joint density.
From an experimental perspective, it would be helpful to standardize the results for better comparison with other methods. For example, replicating Figure 4 from Dauphin et al. while substituting SFN with the CPN method would provide a compelling demonstration of CPN's ability to escape scenarios where momentum fails.
Overall, the proposed idea has significant potential but requires more rigorous comparisons and a clearer connection to momentum and related work.