The proposed method introduces a novel approach to compress the weight matrices of deep networks by employing a new density-diversity penalty, combined with a computational optimization (sorting weights) to reduce computational overhead, and a weight-tying strategy.
The density-diversity penalty incorporates two components: the l2-norm of the weights (representing density) and the l1-norm of all pairwise differences within a layer (representing diversity).
To promote sparsity, the most frequent value in the weight matrix is periodically set to zero.
As the diversity penalty causes weights to converge to identical values, these weights are tied together and subsequently updated using their averaged gradient.
The training process alternates between two phases: 1. training with the density-diversity penalty and untied weights, and 2. training without the penalty but with tied weights.
Experiments conducted on two datasets (MNIST for vision and TIMIT for speech) demonstrate that the method achieves excellent compression rates without compromising performance.
The paper is well-written, presents innovative ideas, and appears to represent the state of the art in network compression. The proposed approach has the potential to inspire further research, and the weight-tying strategy could find applications beyond compression, particularly in learning data regularities.
However, the result tables are somewhat confusing.
Minor issues:
p1  
Language issue: "while networks that consist of convolutional layers."
p6-p7  
Tables 1, 2, and 3 are unclear. Compared to the baseline (DC), the proposed method (DP) appears to perform worse:  
In Table 1 overall, Table 2 overall FC, and Table 3 overall, DP exhibits lower sparsity and higher diversity than the DC baseline. This suggests a worse compression rate for DP, which contradicts the text claiming similar or superior performance.  
It seems likely that the sparsity metric is inverted, and what is being reported is the number of non-modal values as a fraction of the total.