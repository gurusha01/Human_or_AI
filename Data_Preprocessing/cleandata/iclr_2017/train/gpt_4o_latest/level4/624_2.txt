This paper investigates the error surface of deep rectifier networks, providing specific examples where the error surface exhibits local minima. Through several experimental results, the authors demonstrate that learning can become trapped in apparent local minima due to various factors, including the nature of the dataset and the initialization scheme. The paper offers valuable intuitions and illustrative examples of how training can fail.
While the examples in this paper are somewhat contrived, this does not diminish their theoretical significance. Simple examples where training fails are indeed useful for understanding potential pitfalls. However, the broader theoretical framing of the paper seems to target a strawman argument.
"The underlying easiness of optimizing deep networks does not simply rest just in the emerging structures due to high dimensional spaces, but is rather tightly connected to the intrinsic characteristics of the data these models are run on." This perspective, however, is already present in several works cited by the authors as not adhering to this view. For instance, Choromanska et al. analyze Gaussian inputs, which inherently tie their claims to the properties of the data. More generally, the loss function is jointly determined by the dataset and the model parameters, making it impossible to separate the error surface from dataset characteristics. The notion of "emerging structures due to high dimensional spaces" that are independent of the dataset and initialization is unclear and seems implausible. The error surface's structure is necessarily influenced by both the dataset and the model parameters.
A central concern with this paper is that it appears to address a strawman argument. Replica methods, for example, describe average behavior in infinite systems, so it is unsurprising that specific finite systems might exhibit poor optimization landscapes. The paper seems to express surprise that training can fail with poor initialization, but it is well-established that initialization is critical, even in linear networks. For instance, saddle points can significantly slow learning with poor initialization (e.g., Saxe et al., 2014).
There also appears to be a potential error in the proof of Proposition 5. If we assume cdfb(0) = 0 and cdfW(0) = 1/2, the paper claims P(learning fails) ≥ 1 - 1/2^(h²(k-1)), implying that the probability of failure increases as the number of hidden units increases. However, it seems the correct expression (ignoring bias) should be P(fails) ≥ 1 - [1 - P(w < 0)^h²]^(k-1). In this case, the limit as k → ∞ depends on how h scales with k, and it is no longer necessarily true that "one does not have a globally good behavior of learning regardless of the model size."
Additionally, the paper does not sufficiently differentiate between local minima and saddle points. Section 3.1 claims to show that training gets stuck in a local minimum, but this conclusion is based on training with a fixed number of epochs. It is unclear whether the observed behavior reflects a genuine local minimum or a saddle point. Furthermore, while rectifiers may have genuine blind spots, sigmoid or soft rectifier nonlinearities might not. For example, in the XOR problem with two hidden nodes, it was once thought there were local minima, but it was later shown that there are none (e.g., L. Hamey, "Analysis of the error surface of the XOR network with two hidden nodes," 1995).
If the goal is simply to demonstrate that training does not converge for specific finite problems, much simpler counterexamples could suffice. For instance, initializing all hidden unit weights to zero would achieve this.
In their response to prereview questions, the authors state, "If the 'complete characterization' [of the error surface] was indeed universally valid, we would not be able to break the learning with the initialization." However, as noted earlier, even for deep linear networks, poor initialization (e.g., near a saddle point) is known to hinder learning. This again suggests that the paper is addressing a strawman argument, such as the claim that "nothing can possibly go wrong with neural network training." No prior theoretical work makes such a claim.
The explanation of Figure 2 also seems counterintuitive. Scaling the input, assuming weight matrices are initialized with zero biases, should not alter the regions over which each ReLU activates. Thus, this manipulation does not appear to achieve the stated goal of concentrating "most of the data points in very few linear regions." A more plausible explanation is that the weaker scaling has not been adequately compensated by the learning algorithm, and the algorithm would converge if given more time. The authors note that training was conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling applied to the data is five orders of magnitude. Indeed, training converges without issue for scaling up to four orders of magnitude. The authors also suggest that Adam should compensate for the scaling factor, but this depends on implementation details, such as the epsilon parameter used to prevent division by zero.
In summary, while this paper presents several interesting results, there remain a number of technical concerns that need to be addressed.