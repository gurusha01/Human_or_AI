This paper proposes a variant of the semi-supervised variational auto-encoder (VAE) framework, introducing a method to incorporate structure (observed variables) within the recognition network.
I believe the presentation of inference with auxiliary variables could be omitted, as it unnecessarily complicates the exposition. While the auxiliary variable expressions are useful for implementing a unified framework, the same model could be achieved without them, resulting in a minimal extension of VAE where part of the generative space is observed. The observed variables require the posterior to condition on them to leverage the information they provide. The approach described here is not significantly different from Kingma et al. 2014, which makes the reported substantial differences in experimental results between the two methods surprising. Given the similarity of the models, the authors should provide an explanation for the observed superiority of their method over Kingma et al. 2014. Additionally, I am unclear whether the experimental setup for Fig. 5 (bottom) is consistent with Kingma et al. 2014. The authors mention using CNNs for feature extraction, but it is unclear if Kingma et al. employed the same approach.
Similarly, I have the same question regarding the comparison with Jampani et al. 2015. Specifically, does that model use the same supervision rate to ensure a fair comparison?
The experiment in Section 4.3 is compelling and highlights a useful property of the proposed approach.
The discussion of the supervision rate (along with the pre-review response) provides valuable insights into successful training protocols for semi-supervised learning.
Overall, the paper is engaging, but the title and introduction set expectations that are not fully met. The title suggests a method for interpreting general deep generative models, but the paper focuses on a semi-supervised VAE variant. While incorporating labeled examples disentangles the latent space, this is a general property of semi-supervised probabilistic models and not unique to the proposed method. Furthermore, the introduction led me to anticipate a more general approximation scheme for the variational posterior, akin to Ranganath et al. 2015, which enables highly flexible distributions. However, this is not the case here.
In summary, the contributions of this paper lie in defining a slight variation of the semi-supervised VAE and, perhaps more notably, formulating it in a way that facilitates easier software automation. However, the methodological contributions to the existing literature are limited. The authors mention plans to extend the framework within a probabilistic programming context, which indeed seems like a promising and valuable direction.
Minor note: The paper cites three of Kingma's works as Kingma et al. 2014, which is confusing. I recommend distinguishing them as Kingma et al. 2014a, 2014b, etc.