Interesting paper that introduces a method for jointly learning the automatic segmentation of words into subwords alongside their corresponding acoustic models.
While the training process treats word segmentation as a hidden variable influenced by acoustic representations, the decoding phase relies solely on maximum approximation.
The authors demonstrate notable improvements over character-based baselines. However, they do not provide a comparison with word segmentation approaches that do not assume dependency on acoustic information.
Segmentation based solely on text would result in two simpler, independent tasks. There are several publicly available tools capable of performing such segmentation, which should be cited. Some of these tools can also leverage unigram word probabilities to guide their segmentation processes.
It seems that the observed improvements stem from the use of longer acoustic units, which impose stronger acoustic constraints and reduce search confusion, effectively resembling full-word models. Fewer tokens are likely more advantageous due to reduced probability multiplication. As a thought experiment: if all possible segmentations were allowed (a mix of word fragments, characters, and full words), would the proposed model still utilize word fragments? (Note that WSJ is a closed-vocabulary task.) It would be valuable to demonstrate that the subword model can outperform even a full-word model (i.e., no segmentation).
Your model estimates p(z_t|x,z