Review - Summary:  
The paper introduces a large-scale dataset for reading comprehension, aiming to ultimately release 1 million question-answer pairs. At present, the authors have made 100,000 queries and their corresponding answers available. This dataset distinguishes itself from existing reading comprehension datasets primarily in two ways: the queries are sampled from real user queries rather than being created by crowd-workers, and the answers are generated by crowd-workers instead of being constrained to text spans from the provided passages. The paper includes an analysis of the dataset, such as the distribution of answer types, and reports the performance of several generative and cloze-style models on the MS MARCO dataset.
Strengths:  
1. The paper highlights important limitations of existing reading comprehension datasets, such as the difference in distribution between questions generated by crowd-workers and those asked by actual users of intelligent systems, as well as the restriction of answers to text spans from the passage, which limits the need for reasoning across multiple passages or pieces of text.  
2. The MS MARCO dataset introduces novel and useful characteristics compared to existing datasets: its questions are sampled from real user queries, and its answers are generated by humans.  
3. The experimental evaluation of baseline models on the MS MARCO dataset is thorough and satisfactory.  
Weaknesses/Suggestions:  
1. The paper does not include human performance results on the dataset. Reporting human performance would help gauge the dataset's difficulty. Additionally, the level of inter-human agreement would provide insights into how well the evaluation metric (used for both inter-human agreement and baseline model accuracy) handles variations in sentence structure with similar semantics.  
2. A comparison of the answer type distribution in the MS MARCO dataset with that of existing datasets, such as SQuAD, is missing. This comparison would strengthen the claim that the distribution of questions from user queries differs from that of crowd-sourced questions.  
3. The paper relies on automatic metrics like ROUGE and BLEU for evaluating natural language answers. However, it is well-documented that such metrics exhibit poor correlation with human judgment in tasks like image caption evaluation (e.g., Chen et al., Microsoft COCO Captions: Data Collection and Evaluation Server, CoRR abs/1504.00325, 2015). The authors should address how these metrics are justified for evaluating open-ended natural language answers.  
4. The paper mentions the use of a classifier to filter answer-seeking queries from Bing queries but does not report the classifier's accuracy. Including this information would shed light on what proportion of MS MARCO questions are genuinely answer-seeking. Similarly, the accuracy of the information retrieval system used to retrieve passages for the filtered queries should also be reported.  
5. The description of the best passage baseline is missing and should be included in the paper.  
6. Minor formatting issues need to be addressed, such as fixing opening quotes (e.g., on page 5, "what" should be corrected to "what").  
Review Summary:  
The paper is well-motivated, and the use of real user queries combined with human-generated answers sets the dataset apart from existing ones. However, the inclusion of human performance results and a quantitative comparison of the question distributions between user queries and crowd-sourced datasets would strengthen the paper. Additionally, the authors should discuss the appropriateness of using automatic metrics like ROUGE and BLEU, given their known limitations in correlating with human judgments for tasks such as image caption evaluation.