This study seeks to tackle the challenge of representing multi-sense words by leveraging multilingual context. Results from experiments on word sense induction and word similarity in context demonstrate that the proposed approach outperforms the baseline.
From the perspective of computational linguistics, it is particularly intriguing that languages less similar to English contribute more significantly. However, I have the following concerns about this work:
- The paper is difficult to follow, and it is unclear what distinguishes it from the baseline model [1]. A dedicated paragraph should explicitly compare and contrast the proposed approach with that prior work.
- The proposed model represents only a minor variation of the prior work [1], so the experimental design should isolate and evaluate which specific components contribute to the observed improvements and by how much. For instance, MONO has not been exposed to the same training data, making it unclear whether the proposed model is genuinely superior or if MONO's performance is hindered by its lack of access to the data or computational resources. I recommend adding a baseline where multilingual data is converted into monolingual data via alignment, and the baseline model [1] is trained on this pseudo-monolingual dataset.
- While the paper provides strong benchmarks for intrinsic evaluation, its impact would be more compelling if improvements were demonstrated on a downstream task.
[1]