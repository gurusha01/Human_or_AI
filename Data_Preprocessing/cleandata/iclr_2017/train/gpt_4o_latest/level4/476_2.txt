This paper explores whether shallow non-convolutional networks can perform as effectively as deep convolutional networks for image classification when both architectures are constrained to use the same number of parameters.  
To address this question, the authors conducted a series of experiments on the CIFAR10 dataset.  
Their findings reveal a notable performance gap between the two approaches, with deep CNNs outperforming shallow non-convolutional networks.  
The experiments are thoughtfully designed, incorporating a distillation training approach, and the results are presented in a clear and detailed manner.  
Additionally, the authors observe (in line with prior work) that student models can achieve performance comparable to their teacher models even when the student models are shallower.
In my view, these results suggest that deep convolutional networks are more effective because they inherently encode a form of prior or domain knowledge—specifically, the translation invariance property of images—which is critical for high-level recognition tasks. While the results are not entirely surprising, they are not immediately obvious either and provide valuable insights.
One intriguing observation, which the authors briefly mention, is that among the non-convolutional architectures, those with 2 or 3 hidden layers outperform those with 1, 4, or 5 hidden layers. Could the authors provide an interpretation or hypothesis to explain this phenomenon? Expanding on this point in the paper would make for an interesting discussion.
It was not entirely clear to me why the experiments were restricted to architectures with a maximum of 30M parameters. None of the experiments shown in Figure 1 appear to have reached saturation. Although the performance gap between CNNs and MLPs is substantial, I believe it would be valuable to extend the experiments further for the final version of the paper.
In the final paragraph, the authors suggest that shallow networks would perform relatively worse on an ImageNet classification task. Could the authors elaborate on why they expect this outcome? One could argue that the significantly larger training dataset size might mitigate the limitations of shallow and/or non-convolutional architectures. Since MLPs are universal function approximators, architectural choices can be interpreted as imposing priors over the function space, and in large-data regimes, such priors might become less critical. This hypothesis could be tested on ImageNet by varying the amount of training data. Furthermore, the higher resolution of ImageNet images might introduce additional factors that could influence the CNN-MLP comparison, potentially leading to different results than those observed on CIFAR10.
Finally, conducting experiments on a second dataset would strengthen the findings by demonstrating the extent to which the results generalize across different datasets.