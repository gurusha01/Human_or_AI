SUMMARY  
This paper investigates the preimages of outputs from feedforward neural networks with ReLU activations.
PROS  
The paper introduces an interesting approach involving coordinate transformations at individual layers.
CONS  
The manuscript remains underdeveloped and lacks sufficient contributions to qualify as a fully polished conference paper.
COMMENTS  
- The initial version of the paper contained numerous typographical errors and appeared to be in an unrefined state.  
- While the paper includes promising ideas, I believe it does not present enough substantial results to merit acceptance as a Conference paper. However, I would be inclined to recommend it for the Workshop track.  
- The concepts of irreversibly mixed and other notions introduced in this paper are closely related to the ideas discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I recommend citing that work and elaborating on the connections. Notably, that paper also explores the local linear maps of ReLU networks.  
- I am curious about the practical challenges involved in computing the preimages. While the definition seems straightforward, the implementation and computation might pose significant difficulties.  
DETAILED COMMENTS  
- On page 1: "can easily be shown to be many to one"—this statement is true in general.  
- On page 2: "For each point x^{l+1}"—the parentheses in the superscript are missing.  
- After eq. 6: The phrase "the mapping is unique" should include the condition "when w1 and w2 are linearly independent."  
- Eq. 1: The equation should represent a vector.  
- Above eq. 3: "collected the weights a_i into the vector w" and bias b—there is a missing period.  
- On page 2: "... illustrate the preimage for the case of points on the lines ... respectively"—please clarify which is which.  
- In Figure 1: Is this a conceptual sketch or an actual illustration of a network? If it is the latter, please specify the values of x and the weights depicted. Additionally, define and explain the arrows clearly. What do the arrows in the gray region represent?  
- On page 3: "This means that the preimage is just the point x^{(l)}"—clarify that this refers to the points that W maps to x^{(l+1)}.  
- On page 3, the first displayed equation: There is an index i on the left-hand side but not on the right-hand side. Additionally, the quantifier on the right-hand side is unclear.  
- "generated by the mapping ... w^i"—subscript is missing.  
- "get mapped to this hyperplane"—clarify that this means "mapped to zero."  
- "remaining"—remaining from what?  
- "using e.g. Grassmann-Cayley algebra"—why not use elementary linear algebra instead?  
- "gives rise to a linear manifold with dimension one lower at each intersection"—this is true only if the hyperplanes are in general position.  
- "is complete in the input space"—consider rephrasing as "forms a basis."  
- "remaining kernel"—remaining from what?  
- "kernel"—does this refer to the nullspace, a matrix of orthonormal basis vectors for the nullspace, or something else? Please clarify.  
- Figure 3: Nullspaces of linear maps should pass through the origin.  
- "from pairwise intersections"—use the symbol \cap.  
- "indicated as arrows or the shaded area"—this description is unclear and needs refinement.  
- Typos: peieces → pieces, diminsions → dimensions, netork → network, me → mean.