This paper introduces a straightforward yet impactful enhancement to reinforcement learning algorithms by incorporating a temporal repetition mechanism into the action space, allowing the policy to determine the duration for which a selected action is repeated. This modification is broadly applicable to all reinforcement learning algorithms, spanning both discrete and continuous domains, as it primarily alters the action parametrization. The paper is well-structured, and the experiments thoroughly evaluate the proposed approach using three distinct RL algorithms across three different domains (Atari, MuJoCo, and TORCS).
Below are some comments and suggestions to improve the paper:
The introduction claims that "all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k." This assertion is overly general and is contradicted by the experimental results — while action repetition proves beneficial in many tasks, it is not universally advantageous. This statement should be revised for greater accuracy.
In the related work section, adding a discussion on the connection to semi-MDPs would enhance the reader's understanding of the approach and clarify how it compares to and diverges from related methods (e.g., as mentioned in the response to pre-review questions).
Experiments:
Could you include error bars in the experimental results, derived from multiple runs with different random seeds?
For the TRPO experiments, it would be helpful to incorporate parameter sharing to align with the methodology used in the other domains. This is particularly important because the improvement observed in the TRPO experiments appears smaller than in the other two domains. It is unclear whether this discrepancy arises from the nature of the task, the absence of parameter sharing, or another factor.
The TRPO evaluation differs from the benchmarks reported in Duan et al. (ICML '16). Why not use the same benchmark for consistency?
The videos provided only showcase the policies learned with FiGAR, which lack context without a comparison to the policies learned without FiGAR. Could you include videos of the baseline policies (without FiGAR) for a more informative comparison?
How many laps does DDPG complete without FiGAR? The reward difference (557K vs. 59K) seems substantial, but additional context would clarify the magnitude of the improvement.
Could the tables be presented as histograms? This might convey the results more effectively and succinctly.
Minor comments:
-- In Figure 2, the label for the first bar should be corrected from 1000 to 3500.
-- The phrase "idea of deciding when necessary" could be rephrased as "idea of only deciding when necessary" for improved clarity.
-- "spaces.Durugkar et al." is missing a space between the words.
-- "R={4}" — why is 4 chosen? Consider using a letter to denote a constant or adopting alternative notation for clarity.