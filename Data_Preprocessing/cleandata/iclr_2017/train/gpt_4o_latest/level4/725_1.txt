Strengths:
- The paper presents an intriguing exploration of the relationship between ReLU-based deep neural networks (DNNs) and simplified stochastic feedforward neural networks (SFNNs).  
- The authors effectively demonstrate the utility of their proposed training methods through experiments on a small-scale task (MNIST).  
- The proposed multi-stage training methods are straightforward to implement, even though they lack formal theoretical justification.  
Weaknesses:
- The study does not include results on real-world tasks involving large training datasets.  
- There is insufficient investigation into the scalability of the proposed learning methods as the size of the training data increases.  
- When the hidden layers are stochastic, the model exhibits similarities with uncertainty representations found in deep Bayesian networks or deep generative models (e.g., "Deep Discriminative and Generative Models for Pattern Recognition," book chapter in Pattern Recognition and Computer Vision, November 2015). These connections should be discussed, particularly in the context of leveraging uncertainty representations to enhance pattern recognition (e.g., supervised learning via Bayes' rule) and incorporating domain knowledge, such as the concept of "explaining away."  
- The paper would benefit from discussing links to variational autoencoder models and their training processes, which also involve stochastic hidden layers.