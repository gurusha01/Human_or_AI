This is an interesting proposal that has the potential to enable more efficient training of recurrent neural networks. However, I would appreciate seeing additional experimental evidence to support the claims. While I have already raised some questions and have yet to receive responses, I would like to outline a few additional concerns and inquiries:
- Does the resulting model retain its status as a universal approximator, assuming sufficiently large hidden dimensions and number of layers?  
- More broadly, how does the expressiveness of the proposed model compare to that of an equivalent model without orthogonal matrices, particularly when the number of parameters is held constant?  
- The experimental results are somewhat underwhelming, as the number of distinct input/output sequences tested was quite limited. Additionally, as the authors themselves noted, training becomes unstable (I found the definition of "success" in this context unclear). While the authors acknowledge the need to expand the experimental section, it appears that this has not yet been addressed.