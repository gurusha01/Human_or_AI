Paper Summary
The paper revisits the concept of employing a binary classifier for two-sample testing. The proposed approach involves splitting the data into two disjoint subsets for training and testing, training a classifier on the training set, and using the classifier's accuracy on the test set as the test statistic. If the accuracy exceeds the chance level, it is concluded that the two samples originate from different distributions, leading to a rejection of the null hypothesis (H0).
The authors provide a theoretical result on the asymptotic approximate test power, which implies that the test is consistent, assuming the classifier performs better than random guessing. Experiments conducted on synthetic datasets, GAN evaluation, and causal discovery demonstrate the effectiveness of the proposed test. Furthermore, when a neural network is used as the classifier, analyzing the first linear filter layer reveals the most activated features, offering an interpretable visual representation of how the two samples differ.
Review Summary
The paper is well-structured and easy to comprehend. While the idea of using a binary classifier for two-sample testing is not novel, as acknowledged by the authors, the primary contributions lie in the analysis of asymptotic test power, the application of modern deep networks as classifiers in this context, and the empirical validation across diverse tasks. The experimental results are sufficiently convincing. Although the paper does not delve deeply into why the method performs well in practice, the overall contributions have the potential to inspire new research directions in model criticism for generative models and visualization of model failures. I recommend acceptance.
Major Comments / Questions
My primary concern pertains to Theorem 1 (asymptotic test power) and its underlying assumptions. However, I believe these issues can be addressed as outlined below.
* Under H0, the test statistic (i.e., the sum of 0-1 classification outcomes) is correctly stated to follow a Binomial(nte, 1/2) distribution. However, under H1, the terms in the sum are independent but not identically distributed Bernoulli random variables. This is because each term depends on a data point \( zi \), which may originate from either \( P \) or \( Q \). Consequently, the statement in Section 3.1 that "... the random variable \( n{te} \hat{t} \) follows a Binomial(nte, p)..." is inaccurate. Instead, the distribution should be a Poisson binomial distribution.
* Similarly, in the same paragraph, the alternative distribution of Binomial(nte, \( p = p_{risk} \)) is likely incorrect. It seems this is mentioned to invoke the Moivre-Laplace theorem for asymptotic normality.
That said, I do not see a necessity for this statement, as the Binomial distribution is not essential for the proof. The asymptotic normality can still be established using a variant of the central limit theorem (rather than the Moivre-Laplace theorem) for independent, non-identically distributed random variables. For reference, see results on the asymptotic normality of Poisson binomial distributions under certain conditions.