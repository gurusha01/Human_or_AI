This paper introduces an intriguing approach for iteratively re-weighting word representations within a document (and consequently the GRU-encoded document representation) through a straightforward multiplication operation. As the authors rightly noted, this operation functions as a "filter," diminishing attention to less relevant portions of the document and thereby enhancing the model's performance.
The results achieved are at or near state-of-the-art for several Cloze-style QA tasks. However, this paper could merit an even higher score if the following limitations were addressed more thoroughly:
1. While the proposed architecture is both conceptually simple and interesting (albeit with notable computational overhead), it is tailored to a very specific task.
2. The improvement brought by the core idea of this paper (gated attention) is relatively modest when comparing GA Reader-- to GA Reader. The latter incorporates several engineering enhancements, such as the inclusion of character embeddings, the use of word embeddings trained on a larger corpus (GloVe), and minor modeling refinements like token-specific attention in (5).
3. Additionally, I would encourage the authors to provide more clarity on the role of K (the number of hops), both from an intuitive and an empirical perspective. A deeper analysis of K's impact on different types of questions, for instance, could yield valuable insights.