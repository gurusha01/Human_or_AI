This paper extends the work of Weston (2016) by employing End-to-end memory network models for a constrained form of dialogue that incorporates teacher feedback. As noted by the authors, this setup is closely aligned with the question answering paradigm, with the key distinction being that a teacher provides feedback following the model's response. Importantly, this feedback does not always yield a positive reward, requiring the model to effectively leverage the teacher's input to enhance its performance.
The paper is generally well-written, and the authors explore several intriguing models. While the dialogue considered here is admittedly limited—more akin to question answering, as the questions do not necessitate deeper contextual reasoning—this line of investigation holds promise, particularly if the tasks are scaled to greater complexity in future work.
My primary concern lies with the paper's novelty. As the authors themselves highlight, their contributions diverge from Weston's work in two main ways:
"(i) That earlier work did not use the natural reinforcement learning/online setting, but 'cheated' with a fixed policy given in advance. It is important to address the realistic online setting and assess whether the methods, particularly FP, still work, or else what changes (e.g. exploration, balancing, see Fig 4 and Table 1) are needed. (ii) That earlier work had only simulated data, and no real-language data, so was only toy. This work uses Mechanical Turk to do real experiments, which again is important to assess if these methods, particularly FP, work on real language."
While point (ii) is a valuable addition, incorporating more human testing data alone does not meet the threshold for a strong conference contribution. The paper's central claim is that "the model also works if we collect the data online (i.e., the agent's policy is used to collect data rather than a fixed policy beforehand)." Although this represents progress, I question whether it constitutes a sufficiently significant advancement for an ICLR paper. Addressing this additional requirement on these tasks does not seem to demand substantial model innovation beyond employing epsilon-greedy exploration. Consequently, the paper falls into a borderline accept/reject category.
EDIT: After reviewing the authors' response, I have slightly revised my score. Their argument emphasizing the importance of real-world implementation as a key contribution is well-taken and adds weight to the paper's relevance.