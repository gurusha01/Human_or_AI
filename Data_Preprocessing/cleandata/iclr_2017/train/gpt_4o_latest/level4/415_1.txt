The author introduces a straightforward yet effective method for regularizing neural networks. The results achieved are impressive, and the technique proves to be effective even when applied to state-of-the-art architectures. This is particularly valuable, as many regularization methods are often tested on simpler tasks or initial configurations, where the results remain far from the best-known benchmarks. 
Overall, this is a strong paper with well-conducted experimentation. Regularization techniques should be evaluated on deep and complex architectures close to state-of-the-art performance. In contrast, other studies often test regularization on simpler models, where the benefits are evident but do not push the boundaries of current advancements.