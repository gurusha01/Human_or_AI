This paper explores the integration of policy gradient methods and Q-Learning, demonstrating improved learning performance, particularly within the Atari Learning Environment. The central idea is that entropy-regularized policy gradient results in a Boltzmann policy derived from Q-values, effectively linking the policy (π) and Q-values, and enabling the application of both policy gradient and Q-Learning updates.
I find this paper highly interesting, not only for its results and the proposed PGQ algorithm but especially for the connections it establishes between various techniques, which I found to be quite insightful.
However, I believe the paper could benefit from clearer exposition of these connections. I found the presentation somewhat challenging to follow, even though the underlying concepts are not overly complex. Specifically:
- The introduction of \(\tilde{Q}^\pi\) as "an estimate of the Q-values" is unclear, especially since Equation 5 presents it as an exact equality rather than an estimate.
- The contribution of Section 3.2 is not entirely apparent to me, and I wonder if it could be omitted to allow for more detailed explanations in other sections.
- The links to dueling networks (Wang et al., 2016) in Sections 3.3 and 4.1 are insufficiently explicit and detailed. From my understanding, the proposed architecture closely resembles dueling networks, and this similarity warrants further discussion. Additionally, in the experiments, it seems that the "variant of asynchronous deep Q-learning" used is essentially a dueling network, but this is not clearly stated. It would also be helpful to mention that PGQ can be interpreted as combining Q-Learning with n-step expected Sarsa using a dueling network. Such examples would make the connections between methods more comprehensible.
- Section 3.3, while drawing intriguing links, could be made clearer. It is difficult to discern the section's direction upon first reading. One confusing aspect is its relationship with Section 3.2, which assumes a critic outputting Q-values, whereas in Section 3.3, the critic outputs V-values. Additionally, the "μ" distribution is introduced abruptly without sufficient context.
I encourage the authors to improve the paper's readability in the final version and address the points raised in the pre-review questions, particularly concerning experimental details, the derivation of Equation 4, and the issue of the discounted distribution of states.
Minor remarks:
- The function \(r(s, a)\) in the Bellman equation in Section 2 is not formally defined. It appears odd because the expectation is over \(s'\) and \(a'\), but \(r(s, a)\) does not depend on them. Either \(r(s, a)\) should be moved outside the expectation, or the expectation should also include the reward, depending on how \(r\) is defined.
- The definition of the Boltzmann policy at the end of Section 2.1 is somewhat confusing, as it involves a sum over \(a\) of a quantity that does not clearly depend on \(a\).
- Section 4.3 appears to address the tabular case, but this is not explicitly stated.
- In Figure 1, it is unclear why the three algorithms do not converge to the same policy. In such a simple toy setting, I would expect convergence to the same policy.
Typos:
- "we refer to the classic text Sutton & Barto (1998)" → missing "by"?
- "Online policy gradient typically require an estimate of the action-values function" → requires & value
- "the agent generates experience from interacting the environment" → with the environment
- In Equation 12 (first line), there is an extraneous comma near the end, just before "dlog π".
- "allowing us the spread the influence of a reward" → to spread
- "in the off-policy case tabular case" → remove the first "case"
- "The green line is Q-learning where at the step an update is performed" → at each step
- In Figure 2, it says "A2C" instead of "A3C".
NB: I did not have time to thoroughly review Appendix A.
When initially reading the paper, I assumed Section 2 omitted mentioning that the distribution \(d^\pi\) uses a discounted weighting of states, as required for the policy gradient theorem from Sutton et al. (1999) to hold (see above Equation 2 in that paper). However, it seems no such discounting is applied during updates in practice. While this does not matter in the tabular case, could it pose an issue with DQNs? Also, which value of \(\gamma\) is being used?
Hi,
Could you please clarify the following:
1. Is \(\alpha\) kept fixed in the experiments, and are results reported for the "exploratory" policy (softmax with temperature \(\alpha\)) rather than the greedy one? If so, why?
2. In both the grid world and Atari experiments, is the critic's estimate of \(Q(s, a)\) obtained by summing the (discounted) observed rewards for up to \(t_{\text{max}}\) timesteps after taking action \(a\) in state \(s\), plus the (discounted) estimated \(V(\text{last observed state})\), as in A3C?
3. For the Q-Learning step, are you freezing the target network and clipping the error as in the Nature DQN paper? If not, why?
Thanks!
Hi,
In Section 3.2, you consider the case where "the measure in the expectation (is) independent of \(\theta\)." However, in practice, both the state and action distributions depend on \(\theta\). Could you comment on how this affects the proposed interpretation? I believe it would be important to address this in the paper.
Thanks!