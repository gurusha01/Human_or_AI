This paper introduces a nonparametric neural network model that dynamically determines the model size during training. The central concept involves randomly adding zero units and employing a sparse regularizer to automatically prune irrelevant weights. This approach can be interpreted as a random search over a discrete space, guided by sparse regularization to discard unnecessary units. The problem addressed is significant, and the paper presents intriguing results. My primary comments are as follows:
What is the additional computational complexity of the proposed algorithm? The decomposition of each fan-in weight into parallel and orthogonal components, along with the transformation into radial-angular coordinates, might introduce substantial computational overhead. The authors should discuss the relative increase in operations compared to a parametric neural network. Additionally, providing experimental results on running time would be beneficial.
It is noted that nonparametric networks tend to produce smaller networks on convex datasets, leading to inferior performance compared to parametric networks. Could the authors provide any insights into this observation?