I find it challenging to grasp the motivation behind this paper. The authors assert that they have captured a latent representation of text and image during training, enabling better translation performance without images at test time. However, they fail to convincingly demonstrate that images contribute meaningfully to the process (and the setup itself seems unusual, given the absence of images during testing). The claims appear speculative, such as: "we observed some gains, so these should come from our image models." The qualitative analysis does not sufficiently validate that the models have learned latent representations; instead, I suspect the observed gains may stem from reduced overfitting due to the inclusion of images during training.
The dataset used is too small for conducting experiments in neural machine translation (NMT). Additionally, I question the fairness of comparing their models with NMT and VNMT, especially given the description in Section 4.1: "VNMT is fine-tuned by NMT and our models are fine-tuned with VNMT." This aspect requires further clarification.
Moreover, I have concerns about the paper's presentation:
(a) The use of symbols is excessive and, at times, unnecessary. For instance, in Section 3.1, f and g are used for x (source) and y (target), which could be simplified.
(b) The use of the prime symbol (') is inconsistent, making the paper harder to follow. For example, in Section 3.1.2, references to h'\pi derived from Eq. (3) are unclear, as Eq. (3) pertains to h\pi. While the intended meaning is understandable, the presentation could be improved.
(c) In Section 3.2.2, it is unclear whether h'_z is correctly computed from \mu and \sigma. The roles of \mu' and \sigma' also require clarification.
(d) Notation such as G+O-AVG should be revised to something like G+O_{AVG}, as the minus sign misleadingly suggests an ablation test. Similar adjustments are needed for other symbols.
Additional issues include the lack of explanations for Figures 2 and 3, as well as a missing \pi symbol in Appendix A before the KL derivation.