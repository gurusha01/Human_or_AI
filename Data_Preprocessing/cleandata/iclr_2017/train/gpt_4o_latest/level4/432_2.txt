Review:  
This is a well-written paper that investigates the relationship between value-based methods and policy gradients, providing a formal connection between the softmax-like policy derived from Q-values and a regularized form of policy gradient (PG).
Presentation:  
While the first part of the paper appears to follow this narrative, I believe it could be reframed as an extension or generalization of the dueling Q-network. In my view, this would offer a more intuitive presentation of the proposed algorithm and its contributions.
Minor concern regarding the general case derivation:  
In Section 3.2, specifically in Eq. (7), the expectation over (s, a) is taken with respect to \(\pi\), which depends on \(\theta\). However, this dependency seems to be overlooked, even though it is critical for deriving the PG update. If the sampling policy for the expectation and \(\pi\) are sufficiently close, this omission might be acceptable. However, outside of specific cases (e.g., trust-region methods and similar approaches), this assumption generally does not hold. As a result, there is a risk of solving a problem that diverges significantly from the one intended.
Results:  
It would be beneficial to include a comparison with the dueling architecture, as it represents the most closely related method. Such a comparison could highlight whether and in which scenarios the proposed approach demonstrates improvements.
Overall:  
This is a strong paper with valuable theoretical contributions.