The paper introduces a novel regularization technique for neural networks aimed at maximizing the correlation between input variables, latent variables, and outputs. This is accomplished by defining a measure of total correlation among these variables and breaking it down into entropies and conditional entropies.
The authors clarify that they do not directly maximize the total correlation but rather a lower bound of it, which excludes simple entropy terms and focuses solely on conditional entropies. However, the reasoning behind omitting these entropy terms is not adequately explained.
Entropy measures are applied to probability distributions, implying that the model's variables should inherently be random. The connection between the conditional entropy formulation and the reconstruction error is not explicitly established. To bridge these perspectives, one might expect, for instance, the introduction of a noise model for the network's units.
The paper later asserts that the original ladder network is unsuitable for supervised learning with small sample sizes, supported by some empirical results. However, a more theoretical justification for this claim would have been appreciated.
The MNIST results are presented for a specific convolutional neural network architecture. Yet, most ladder network results on this dataset have been reported using standard fully connected architectures. Including results for such architectures would have been beneficial for better comparability with the original ladder network findings.