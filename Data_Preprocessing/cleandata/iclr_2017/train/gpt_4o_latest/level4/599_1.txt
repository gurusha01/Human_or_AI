This paper introduces a modified gated RNN, referred to as GRU-D, designed to handle time series data with significant missing values in the input. The approach operates on two key fronts. First, it directly addresses missing inputs by employing a learned convex combination of the most recent available value (forward imputation) and the mean value (mean imputation). Second, it introduces a dampening mechanism in the recurrent layer, akin to a secondary reset gate, which is parameterized based on the time elapsed since the last observed value for each feature.
Positives
- The task (handling missing values for time series classification) is clearly defined.
- The paper evaluates the proposed model against a variety of interesting baselines.
- The model tackles missing values in a novel, machine learning-driven manner by learning new dampening parameters.
- The extensive experimental evaluation across datasets is a significant strength of the paper.
Negatives
- The paper contains several typos that require correction.
- Section A.2.3, which discusses important related works, should be included in the main text. If space is an issue, it could replace the less precise diagrams of the model.
- The paper does not reference any methods from the statistical literature.
Key Points Informing My Decision
1. Performance and Baseline Comparisons:  
   While the results are promising, they fall short of expectations. The paper does not convincingly demonstrate that GRU-D outperforms GRU-simple (without intervals) for handling missing inputs. GRU-simple is presented as the primary baseline in the main paper, but it includes additional parameters (the intervals) that, as shown in Table 5, may hinder the model's performance more than they help. With a third of its parameters being of questionable utility, the fairness of the comparison is debatable. Furthermore, in the one table where GRU-simple (without intervals) is included, GRU-D does not show significant improvement over it.
2. Unsubstantiated Claims:  
   My primary concern lies with several claims made throughout the paper.  
   - The relationship between the presence rate of data in the dataset and the diagnostics is misinterpreted. It likely reflects that the attending physician requested relevant analyses based on the patient's condition, suggesting that an expert system trained on this data might always lag behind.  
   - The last sentence of the introduction sets overly ambitious expectations that the paper does not fulfill.  
   - The assertion that "simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values" is unsubstantiated and contradicted later in the paper.  
   - The conclusion that GRU models will continue to improve as more data is added, based on their performance improvement between a subsample and the full dataset, overlooks the fact that non-GRU models started with much stronger baseline results.  
   - Finally, the claim that GRU-D captures informative missingness by integrating masking and time intervals directly into the architecture is questionable. Since the mask is also concatenated to the input, as in GRU-simple (without intervals), the actual improvement introduced by GRU-D is unclear.
Recommendation
While the effort and work reflected in this paper are commendable and above average, I cannot recommend acceptance in its current form. The paper requires a reframing of its findings and a sharper focus on its true contributionâ€”namely, the novel parameterization of the imputation method.