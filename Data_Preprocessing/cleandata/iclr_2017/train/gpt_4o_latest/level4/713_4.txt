The authors introduce a parameterized extension of the ELU activation function and demonstrate that the proposed variant effectively mitigates the vanishing gradient problem in deep networks, outperforming existing non-linear activation functions. Their work includes both a theoretical analysis and empirical validation of the proposed method.
Notable insights into the statistical behavior of the PELU parameters are provided. Offering an explanation for the observed parameter evolution could enhance the understanding of the non-linearity. However, assessing the experimental results is challenging due to discrepancies in the number of parameters when compared to other methods.