The authors introduce two methods to combine multiple weak generative models into a stronger model, leveraging principles from boosting. The proposed approach is both simple and elegant, effectively constructing an unnormalized product of experts model, where the individual experts are trained sequentially in a greedy manner to optimize the overall joint model. However, this method leads to certain undesirable properties in the resulting joint model: an unknown normalization constant, which renders the log-likelihood on the test set intractable, and the inability to draw exact samples from the joint model. These issues are inherent to the product of experts formulation in boosting and cannot be resolved by simply substituting different base learners.
The experiments on two-dimensional toy data demonstrate that the proposed method functions as intended and that the boosting framework yields better results than individual weak learners or alternative methods like bagging. However, the results on MNIST are less compelling. Without a definitive quantitative metric, such as log-likelihood, it is difficult to draw strong conclusions from the samples presented in Figure 2, which appear qualitatively weak compared to even simpler models like NADE.
The paper could be significantly strengthened by incorporating a quantitative analysis. For instance, the authors could explore the effect of combining different types of models—such as undirected models (e.g., RBM), directed models (e.g., VAE), and autoregressive models (e.g., NADE)—and measure the performance improvement as a function of the number of base learners. However, this would necessitate a method to estimate the partition function Z or an appropriate proxy for it.