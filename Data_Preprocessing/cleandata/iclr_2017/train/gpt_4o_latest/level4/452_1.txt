This paper demonstrates that extending deep reinforcement learning (RL) algorithms to decide both which action to take and how many times to repeat it results in improved performance across various domains. The evaluation is comprehensive and highlights that this straightforward idea performs effectively in both discrete and continuous action spaces.
A few comments/questions:
- Table 1 might be more interpretable if presented as a figure with histograms.
- Conversely, Figure 3 could potentially be clearer if formatted as a table.
- What criteria were used to select the subset of Atari games for evaluation?
- While the Atari results convincingly show improvements over A3C in games requiring extended exploration (e.g., Freeway and Seaquest), it would be beneficial to include a full evaluation across all 57 games. This has become a standard practice and would allow for a more comprehensive comparison using mean and median scores.
- A more direct comparison to the STRAW model by Vezhnevets et al., which addresses similar challenges as FiGAR, would also strengthen the paper.
- FiGAR currently discards frames between action decisions, which raises the possibility of a tradeoff between repeating an action multiple times and losing intermediate information. Have you considered disentangling these effects? For instance, you could train a model that processes intermediate frames. Just a suggestion to explore.
Overall, this is a simple yet effective addition to deep RL algorithms that is likely to gain traction within the community.
--------------------
Based on the rebuttal and the revised paper, I am increasing my score to 8.