* Brief Summary:
This paper investigates an extension of multiplicative RNNs to LSTM-based models. The proposed approach bears significant resemblance to [1]. The authors present experimental results focused on character-level language modeling tasks. Overall, the paper is well-written, and the explanations are generally clear.
* Criticisms:
- The contributions of the paper are limited. While the motivation is reasonable, the work is highly similar to [1], which itself is an extension of [2]. As a result, this paper primarily serves as an application-oriented study.
- The results are promising but still fall short of the state-of-the-art performance, particularly without the use of dynamic evaluation.
- There are some unconventional choices in the modifications to standard algorithms, such as the "l" parameter in RMSProp and the application of the output gate prior to the nonlinearity.
- The scope of the experimental results is restricted to character-level language modeling.
* An Overview of the Review:
Pros:
- A straightforward modification that appears to perform reasonably well in practice.
- Clear and well-structured writing.
Cons:
- Insufficiently strong experimental results.
- Limited contributions, as the work represents a near-trivial extension of existing methods.
- Use of non-standard modifications to established algorithms.
[1] Wu Y, Zhang S, Zhang Y, Bengio Y, Salakhutdinov RR. On multiplicative integration with recurrent neural networks. InAdvances in Neural Information Processing Systems 2016 (pp. 2856-2864).  
[2] Sutskever I, Martens J, Hinton GE. Generating text with recurrent neural networks. InProceedings of the 28th International Conference on Machine Learning (ICML-11) 2011 (pp. 1017-1024).