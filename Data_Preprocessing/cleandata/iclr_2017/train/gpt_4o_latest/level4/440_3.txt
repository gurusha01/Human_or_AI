This paper addresses the challenge of model-based policy search. The authors propose leveraging Bayesian Neural Networks to model the environment and advocate for minimizing the $\alpha$-divergence as an alternative to the more conventional variational Bayes approach.
While the $\alpha$-divergence's ability to capture bi-modality is advantageous, it introduces significant complexity, and much of the paper focuses on deriving tractable approximations. To this end, the authors adopt the method of Hernandez-Lobato et al. (2016) as a surrogate for the $\alpha$-divergence.
The paper provides a clear definition of the environment/system dynamics and the policy parametrization (Section 3), which could serve as a valuable reference for other researchers. Using the learned model, simulated roll-outs generate samples of the expected return. With the environment model in place, stochastic gradient descent can be performed directly using automatic differentiation tools, eliminating the need for policy gradient estimators.
The experimental results highlight the $\alpha$-divergence's ability to capture multi-modal structures, which competing methods like variational Bayes and Gaussian Processes (GP) struggle to handle. Additionally, the proposed approach demonstrates competitive performance in a real-world batch setting.
Overall, the paper is well-written, technically robust, and effectively integrates several contemporary tools into a cohesive algorithm. However, the reliance on repeated approximations to the original quantities somewhat undermines the benefits of the initial problem formulation. Furthermore, the scalability and computational efficiency of the method remain questionable, raising concerns about its practicality for many problems. As with other Bayesian methods, this approach is likely to excel in low-sample regimes, where it could offer advantages over comparable methods such as variational Bayes and Gaussian Processes.