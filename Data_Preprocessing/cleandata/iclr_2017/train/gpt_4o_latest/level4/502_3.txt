The paper addresses a significant issue: the challenge of more effectively evaluating automatic dialogue responses, given that existing automated evaluation metrics (e.g., BLEU, METEOR) are often inadequate and, at times, misleading. The proposed method leverages an LSTM-based encoding of the dialogue context, reference response, and model-generated response(s), which are subsequently scored within a linearly transformed space. While the approach is relatively straightforward, it is also intuitive and supports end-to-end training. As the authors rightly emphasize, simplicity is advantageous for both interpretability and computational efficiency.
The experimental section presents a diverse set of experiments that, in my view, are well-conducted and aim to demonstrate the practicality of the proposed method. However, as others have noted, additional insights from the experiments would have been valuable. For instance, I suggested a detailed failure case analysis and would also recommend extending the evaluation beyond the current dataset to better establish the generalizability of the approach. In my opinion, the paper is somewhat weaker in this regard than it ideally should be.
Overall, I find the ideas presented in the paper compelling, and the approach appears reasonable, making the paper suitable for acceptance. While some qualitative results are provided, it would be highly beneficial to include a more thorough failure case analysis (e.g., a comprehensive overview of key scenarios where ADEM does not align well with human judgments).