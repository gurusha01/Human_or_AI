This paper introduces a novel 3D CNN architecture for detecting climate events, integrating an unsupervised auto-encoder reconstruction loss with YOLO-style bounding box prediction. The method is trained and evaluated on a large-scale, simulated climate dataset annotated using a computationally expensive heuristic method called TECA.
Overall, the paper is well-written (see minor comments below) and tackles an important, well-motivated problem. The authors provide sufficient model details to enable reproduction, though releasing public code would be preferable. While I find the experimental results somewhat unconvincing (discussed below), I appreciate the authors' effort to account for model capacity (via parameter count) when comparing the 2D and 3D model variants.
I am concerned that the evaluation methodology may not adequately demonstrate the effectiveness of the proposed approach. Specifically, using an IoU threshold of 0.1 allows for many suboptimal detections to qualify as true positives. While this threshold might be defensible if the primary goal is instance counting, it seems overly lenient for localization tasks. Additionally, the 3D CNN architecture—one of the core contributions of this work—appears unable to produce variable-sized bounding boxes (as noted in the last paragraph of page 7), which likely degrades performance at higher IoU thresholds, especially given that many weather events are relatively small.
The experimental results also leave questions about the impact of temporal modeling and semi-supervision. Temporal modeling appears to have little effect in the supervised setting (2D mAP: 51.45 vs. 3D mAP: 51.00) but shows a modest improvement in the semi-supervised case (2D mAP: 51.11 vs. 3D mAP: 52.92). Interestingly, the additional unlabeled data seems to negatively affect the 2D model but benefits the 3D model. Could the authors provide confidence intervals for these results? Further discussion of these trends, particularly regarding the influence of the loss weights (alpha, beta, and gamma), would be valuable.
Another concern is whether the 2D and 3D models were trained for equivalent durations, as the last paragraph of page 7 suggests they were not. Including a plot of training and validation accuracy for both models would help clarify this point.
Lastly, the absence of a baseline comparison makes it challenging to contextualize the performance of the proposed approach relative to the problem's difficulty. Could the authors report or compare against a baseline method?
Preliminary Rating:  
This paper addresses an interesting and important problem but does not provide sufficient experimental evidence to support its conclusions. I recommend including mAP trends across a broader range of IoU thresholds, as well as additional discussion of the training procedure, loss weight settings, and the lack of bounding box variability in the 3D model (as noted above).
Clarification:  
In the paper, you state, "While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e., 2D data) in this study." Could you clarify what the surface quantities represent? Do they correspond to the highest cloud level?
Minor Notes:  
- Please include publication years for the Prabhat et al. references instead of using "a" and "b."  
- The footnote in Section 4.2 could be incorporated into the main text for better readability.  
- In Section 4.3, the word "table" is not capitalized in the second paragraph, unlike elsewhere.  
- Similarly, the word "section" is not capitalized in the fourth paragraph of Section 4.3.  
Edit:  
While I appreciate the authors' responses to my questions, I remain unconvinced that the relatively poor localization performance at stricter IoU thresholds justifies the added complexity of the proposed approach. I encourage the authors to continue refining this line of research.