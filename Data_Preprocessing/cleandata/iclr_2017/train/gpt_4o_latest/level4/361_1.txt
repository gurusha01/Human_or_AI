The paper tackles the challenge of predicting learning curves. The primary distinctions from previous work are: (1) the authors train a neural network capable of generalizing across different hyperparameter configurations, and (2) they employ a Bayesian neural network with SGHMC. 
The authors provide evidence that their proposed method performs well in extrapolating partially observed curves and predicting unobserved learning curves across various architectures, including FC, CNN, LR, and VAE. This approach appears highly promising for Bayesian optimization. It would be interesting to see an experiment assessing the relative benefits of this method compared to others :)
Have you considered strategies for managing learning rate decays? Perhaps the algorithm could be tested on a random subset of data, followed by extrapolation?
I was also reflecting on alternative evaluation metrics beyond MSE and LL. In practical scenarios, identifying the most promising run is often the priority. Would it be worthwhile to assess how accurately each method pinpoints the best-performing run?
Minor suggestions:
The font sizes in the figures are too small and nearly unreadable on a printed copy. Please enlarge the font size for the legends and axes.
Figure 6: It seems that not all plots display six lines. Could overlapping lines be causing this?