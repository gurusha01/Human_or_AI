Attempts to employ chatbots for every form of human-computer interaction became a significant trend in 2016, with claims that they could handle diverse types of dialogues beyond simple chit-chat. This paper serves as a critical reality check. While it is primarily relevant to Dialog/Natural Language Processing venues (to inform software engineers about the limitations of current chatbots), it is also suitable for Machine Learning venues (to highlight to researchers the need for more realistic validation of ML methods applied to dialogues). As such, I consider this work to be of high significance.
The paper is underpinned by two important conjectures that are likely to inspire further research. Although these are not explicitly stated in the text, Antoine Bordes articulated them during a NIPS workshop presentation related to this work. Based on the metrics employed in this paper:
1) The performance of end-to-end ML approaches remains inadequate for goal-oriented dialogues.
2) When comparing algorithms, relative performance on synthetic data can serve as a reliable predictor of performance on natural data. This finding challenges prior observations, but the authors have made a commendable effort to align synthetic and natural conditions.
The paper's original algorithmic contribution involves a relatively simple addition to memory networks (the match type mechanism). However, this is the first instance of deploying and testing such networks in a goal-oriented dialogue setting, and the experimental protocol is exemplary. The paper is exceptionally clear and accessible to an audience beyond ML and dialogue researchers. I was particularly impressed by the concise appendix on memory networks, which provided an excellent summary, and the tables that elucidated the impact of the number of hops.
While this paper represents the state-of-the-art in advancing more rigorous metrics for dialogue modeling, it also highlights how brittle and somewhat arbitrary these metrics remain. This observation is more of a recommendation for future research rather than a critique requiring immediate revision.
The authors use per-response accuracy as a metric (essentially classifying the next utterance from a fixed list of responses). However, Table 3 illustrates how impractical this can be: the primary goal is to achieve a correct API call and a reasonably short dialogue. Yet, this metric would yield only 1/7 accuracy, as the six bot responses required to reach the API call must also be exact.
Would per-dialog accuracy, where all responses must be correct, provide a better metric? Table 2 demonstrates how sensitive this measure is to the experimental protocol. Initially, I was puzzled by the much lower accuracy for subtask T3 (0.0) compared to the full dialogue task T5 (19.7). The authors clarified this discrepancy by pointing me to the task definitions (Section 3.1.1), where T3 requires displaying three options, while T5 only requires displaying one.
For the concierge dataset, what would the results look like if "correct" were defined as selecting the best response rather than being among the top five?
While I cannot fault the authors for using standard dialogue metrics and even proposing new ones that may be overly pessimistic, I can envision an alternative way to represent dialogues that could yield more meaningful metrics for goal-oriented tasks. Suppose I were to sell Virtual Assistants as a service, earning revenue based on successful dialogue completions. What metric would maximize my revenue? In this restaurant scenario, the loss might be a weighted sum of errors in the API call, the number of turns required to reach the API call, and the number of user-rejected options. However, such a loss function cannot be measured using canned dialogues and would necessitate either real human users or a highly realistic simulator.
Another issue closely tied to representation learning, which this paper does not adequately address, is the mismatch between the vocabulary used by the user and that in the knowledge base (KB). For the match type mechanism to classify "Indian" as a "type of cuisine," the word must appear exactly as such in the KB. I can foresee situations where the KB employs obfuscated terminology, and it would be preferable for ML systems to learn these associations autonomously rather than relying on manual annotations by humans.