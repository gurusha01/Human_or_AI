This study investigates leveraging the stochastic behavior of neural network outputs under randomized augmentation and regularization techniques to generate targets for unlabeled data in a semi-supervised learning framework. The proposed methods involve either applying stochastic augmentation and regularization to the same image multiple times per epoch and enforcing consistency between the outputs (Π-model) or maintaining a weighted average of outputs from previous epochs and penalizing deviations of the current outputs from this running mean (temporal ensembling). The central premise is that these ensemble predictions are likely more accurate than the current network's outputs, making them effective targets for unlabeled data. Both methods demonstrate strong performance on semi-supervised tasks, with some results indicating remarkable robustness to label noise.
The manuscript is well-written and provides sufficient implementation details to reproduce the results, along with a publicly available codebase. The core idea is compelling and appears to achieve superior semi-supervised accuracy compared to prior methods. Additionally, the discussion on the impact of different data augmentation choices is insightful and valuable.
However, I was somewhat surprised by the claim that a standard supervised network achieves 30% accuracy on SVHN with 90% random training labels. This implies that only 19% of the labels are correct (9% by chance + 10% unaltered), while the remaining 81% would likely introduce inconsistent training signals. Although this outcome is theoretically possible, it seems counterintuitive. I attempted to verify this experiment in the provided GitHub repository but could not locate it.
Regarding the robustness of the Π-model and temporal ensembling to label noise, this result is more plausible given the strong emphasis on the consistency constraint for this task. However, the authors should include a discussion of the weighting function w(t) in the main paper, particularly since the incorrect label tolerance experiment shows a significant difference in w_max (10x for Π-model and 100x for temporal ensembling compared to the standard setting).
Additionally, could the authors comment on the scalability of these methods for larger datasets? For instance, applying temporal ensembling to ImageNet would require approximately 4.8 GB of storage, while the Π-model would double the training time. Furthermore, it would be helpful if the authors could discuss the sensitivity of their approach to the placement and number of dropout layers in the network architecture.
Preliminary rating:  
This is an intriguing paper with high-quality results and a clear presentation.
Minor note:  
In the second paragraph of page one, "without neither" should be corrected to "without either."