The paper investigates the properties of the Hessian of the training objective across various neural networks and data distributions. Specifically, the authors examine the eigenspectrum of the Hessian, which provides insights into the difficulty and local convexity of the optimization problem.
The paper offers several intriguing insights, such as the local flatness of the objective function and the relationship between data distribution and the Hessian. However, a notable limitation is that many of the described effects are presented as broadly applicable but are only tested in specific settings, without accompanying control experiments or rigorous mathematical analysis.
For instance, in Figure 6, the observed concentration of eigenvalues near zero raises questions about its origin. It remains unclear whether this effect is genuinely caused by training (e.g., increasing insensitivity to local perturbations) or is merely a result of the specific scaling of the initial parameters.
In Figure 8, the definition of data complexity is ambiguous. It is not clear whether fully overlapping distributions (which might result in a Hessian approaching zero) are considered complex or simple data.
Additionally, some visual elements in the paper could be improved for clarity. For example, the legends and labels in Figures 1 and 2 are difficult to read in printed format. The plots in Figure 3 have inconsistent x-axis ranges, and the Hessian matrix visualization in Figure 1 does not render properly when printed.