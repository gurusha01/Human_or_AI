The paper introduces the use of a last-layer feature penalty as a regularization technique applied to the final layer of a neural network.  
While the equations imply a per-example weighting (denoted as alpha_i), the authors demonstrate that omitting this weight yields comparable results.  
The proposed method has connections to Batch Normalization (BN) and weight decay.  
The experiments are conducted in a "low-shot" learning setting.  
The paper appears to present two distinct narratives: feature penalty as a soft alternative to Batch Normalization and its application to low-shot learning. However, it remains unclear why the feature penalty is particularly suited to low-shot learning as opposed to more traditional supervised tasks.  
Regarding the Omniglot results (91.5%), this performance seems to lag approximately 2% behind Matching Networks, which are cited in the paper but not included in Table 1. What is the reason for this omission?  
In summary, while the idea is straightforward, it feels somewhat preliminary: although the feature penalty is intended as a "soft BN," Batch Normalization alone achieves superior performance, and combining BN with the feature penalty yields even better results. Does this indicate that some key aspect of the explanation is still missing?  
-- edits after revised version:  
Thank you for incorporating additional details into the paper. While I appreciate the improvements, I still find the paper overly lengthy and hope you can condense it to the promised 9 pages. That said, I remain unconvinced that the paper is ready for acceptance, primarily due to the following issues:  
- On Omniglot, the performance remains significantly below the current state of the art.  
- The new experiments fail to definitively clarify the relationship between the feature penalty and Batch Normalization.  
- While you added an explanation for why the feature penalty is effective in low-shot settings (by arguing that it controls the VC dimension and mitigates overfitting with limited training examples), this discussion is rather basic and does not provide insights beyond the obvious.  
I am raising your score from 4 to 5 in recognition of the improvements made in the revised version, but I still believe the paper falls short of the acceptance threshold.