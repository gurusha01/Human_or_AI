This paper introduces a novel approach that extends the variational auto-encoder (VAE) framework by incorporating stochastic latent dimensionality. This is accomplished through the use of an inherently infinite prior, specifically the stick-breaking process. The model is paired with tailored inference, leveraging the Kumaraswamy distribution as an approximate variational posterior. The resulting framework, termed the SB-VAE, also includes a semi-supervised extension, following a similar approach to the original VAE paper.
There is significant interest in VAEs in the current research landscape, with many efforts focused on achieving automatic "black-box" inference for these models. The authors themselves reference parallel work from Blei's lab and others in this direction. However, there is considerable value in exploring more customized solutions for novel models, as the authors have done here. A notable contribution of this work is the efficient inference for the SB-VAE, which also highlights the potential of the Kumaraswamy distributionâ€”a tool that has seen limited use in machine learning.
While the paper is generally well-structured, certain sections are somewhat confusing. The primary source of confusion seems to stem from the intermingling of model specification and model inference in the exposition. Fortunately, the pre-review questions helped clarify most of these ambiguities.
I have two primary concerns regarding the methodology and motivation of this work. First, conditioning the model directly on the stick-breaking weights feels somewhat unconventional. Initially, I assumed the model involved a mixture probabilistic framework, but this is not the case. To the authors' credit, they address this issue (which became clearer after the pre-review questions) and clarify that they are tackling the challenging problem of employing a base distribution \( G_0 \). The key question is whether their relaxation remains useful. Based on the experimental results, the method appears to be at least competitive, suggesting that the answer is affirmative. It would be interesting to see future extensions of this work, as the authors themselves suggest.
The second concern pertains to the motivation behind this approach. The paper does not convincingly articulate why reformulating the VAE as an SB-VAE is advantageous. While I understand that the non-parametric property induced by the prior could lead to improved capacity control, this benefit (and potentially others that remain unclear) is not sufficiently explained or demonstrated. A comparison with a dropout-based approach or a more detailed discussion of how this method relates to dropout could help clarify these advantages.
In conclusion, I found this paper to be an intriguing contribution, and I believe it would be a strong fit for ICLR.