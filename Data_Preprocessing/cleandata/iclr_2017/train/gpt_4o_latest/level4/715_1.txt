This paper introduces a straightforward randomized algorithm for selecting which weights in a ConvNet to prune, aiming to reduce theoretical FLOPs during the evaluation of a deep neural network. The authors present a well-structured taxonomy of pruning granularity, ranging from coarse (layer-wise) to fine (intra-kernel). The pruning approach is empirically driven, leveraging a validation set to identify the best model from N randomly pruned candidates. However, the claims in the introduction regarding the method being "one shot" and "near optimal" are not substantiated: the approach is effectively "N-shot," as it involves generating and testing N networks, and there is no theoretical or empirical evidence to support the claim of "near optimality."
Pros:
- Provides a clear taxonomy of pruning levels.
- Includes a comparison to the recent weight-sum pruning method.
Cons:
- The experimental evaluation does not address modern architectures (e.g., ResNets) or large-scale datasets (e.g., ImageNet).
- The paper is somewhat difficult to follow.
- While feature map pruning can accelerate computation without requiring specialized sparse convolution implementations, this does not hold for finer-grained sparsity. Since the paper focuses on fine-grained sparsity, it should provide evidence that such sparsity leads to tangible performance improvements.
Another limitation of the experimental evaluation is the lack of analysis on the impact of filter pruning on transfer learning. Tasks like MNIST, CIFAR10, or even ImageNet are of limited standalone interest. Instead, both academia and industry are more concerned with the utility of learned representations for transfer to other tasks. Pruning could potentially degrade transfer learning performance: while the primary task may retain similar accuracy, transfer learning performance might suffer significantly. The paper misses an opportunity to investigate this important aspect.
Summary:  
The proposed method is simple and easy to understand, which is a strength. However, the experimental evaluation is incomplete, lacking coverage of recent architectures and larger-scale datasets. Additionally, the paper does not explore the potential impact of pruning on transfer learning, leaving an important research direction unaddressed.