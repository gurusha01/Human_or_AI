This paper evaluates zoneout across diverse datasets—character-level, word-level, and pMNIST classification—demonstrating its applicability in a broad range of contexts. While zoneout functions as a regularizer to mitigate overfitting, it also exhibits parallels to residual connections. Further exploration of this aspect, particularly through an analysis of how gradient flow enhances task performance, is highly compelling and underscores this as an intrinsic property of zoneout.
The paper is well-written and presents a comprehensive set of experiments that substantiate its claims. Having previously employed this technique in a recurrent setting, I am confident in its positive impact on various tasks. This approach is poised to become a standard technique for RNNs across multiple frameworks.