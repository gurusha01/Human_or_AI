This paper examines the Hessian of small deep networks toward the end of training. The primary finding is that many eigenvalues are close to zero, rendering the Hessian highly singular and thereby limiting the applicability of a significant portion of theoretical frameworks.
The overarching observation that deep learning algorithms exhibit singularity, undermining many theoretical results, is significant but not novel. For instance, Watanabe's work, "Almost All Learning Machines are Singular" (FOCI 2007), has already highlighted this issue. Additionally, this phenomenon has been explored in a growing body of literature. The references in this paper could be substantially expanded to include prior studies on the Hessian in deep learning, such as Dauphin et al.'s "Identifying and attacking the saddle point problem in high dimensional non-convex optimization" (NIPS 2014) or the contributions of Amari and collaborators.
From an experimental perspective, it is unclear how the findings from the small networks analyzed here would generalize to much larger networks. It is plausible that the behavior of larger networks could differ. However, one promising aspect is the emergence of a distinct bulk/outlier pattern even in these small networks. Investigating and characterizing such behavior in simpler systems is indeed valuable. Overall, while the results appear preliminary, they have the potential to be impactful if developed further.
This paper addresses an important problem but needs to better position itself within the context of related work and conduct experiments on a scale large enough to capture behavior relevant to practical applications.