This paper introduces an extension of the MAC method, where subproblems are trained on a distributed cluster arranged in a circular topology. The core concept of MAC involves decoupling the optimization process between model parameters and the outputs of its subcomponents (auxiliary coordinates). Optimization proceeds by alternating between updating the auxiliary coordinates based on the parameters and optimizing the parameters based on the outputs. In the proposed circular configuration, each update is independent, enabling massive parallelization.
The paper would benefit significantly from providing more concrete examples of how the subproblems are decomposed. For instance, could this approach be effectively applied to deep convolutional networks, recurrent models, or other architectures? From a practical standpoint, the paper's impact seems limited to demonstrating that this specific decoupling scheme outperforms alternatives, without exploring broader applicability.
There are also several comparisons that warrant further discussion:
- Circular configurations versus parameter server setups.
- Decoupled subproblems versus parallel stochastic gradient descent (SGD).
Parallel SGD, in particular, has the advantage of being straightforward to implement on top of existing neural network toolboxes. For the proposed method to be practically compelling, it would need to demonstrate significantly superior performance.
Additionally, the paper lacks clarity on what exactly is exchanged between iterations and the trade-offs involved, particularly in the context of deep feed-forward networks. Assuming one subproblem per hidden unit, the process seems to involve:
1. In the W step, different parts of the neural network traverse the cluster, performing SGD updates with respect to the coordinates stored on each machine. This requires transmitting the parameter vector for each hidden unit.
2. A synchronization step follows, aggregating parameters from all submodels, which necessitates a full traversal of the circular structure.
3. Each machine then updates its coordinates based on the complete model and a slice of the data. For a feed-forward network, this would involve generating intermediate activations for each layer and data point.
To draw a comparison with parallel SGD, one could allocate a mini-batch of size B to each machine using the proposed ParMAC method, as opposed to running such mini-batches in parallel. Completing steps 1-2-3 in the proposed method would roughly correspond to one synchronized parameter server (PS) step, where the model is distributed to workers, gradients are collected, and the model is updated.
It would be highly beneficial to include empirical comparisons to illustrate the practical advantages of the proposed method. Intuitively, it is unclear why this approach would theoretically outperform parallel SGD, except in cases involving non-smooth function optimization. Moreover, the decoupling introduced by the method fundamentally alters the optimization process, as it no longer relies on direct backpropagation. This could introduce additional complexities and may not generalize well to other architectures.