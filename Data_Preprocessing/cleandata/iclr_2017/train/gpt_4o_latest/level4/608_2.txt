The authors propose replacing the commonly used tanh activation function with periodic activation functions (e.g., sin) for gradient descent training of neural networks.  
This approach challenges conventional wisdom, and thus, substantial evidence would be required to demonstrate its practical utility.  
The experiments indicate a marginal improvement (98.0 -> 98.1) in some MNIST configurations and a significant improvement (nearly 100% higher accuracy after 1500 iterations) on a toy algorithmic task. However, it remains unclear whether this activation function is broadly effective across a wide range of algorithmic tasks or if its benefits are limited to the two tasks presented. Therefore, the evidence provided is insufficient to convincingly establish the practicality of this approach.