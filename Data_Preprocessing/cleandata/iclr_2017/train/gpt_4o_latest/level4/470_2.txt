The paper explores the integration of Variational Auto-Encoders with the Stick-Breaking process. The primary motivation is to address component collapsing and achieve a representation with stochastic dimensionality. To validate their approach, the authors evaluate the model on MNIST and SVHN datasets in both unsupervised and semi-supervised settings.
Upon closer examination, the claim that the latent variable dimensionality is stochastic appears to be inaccurate. While all latent variables are indeed "used" (enabling backpropagation), they are merely parameterized differently (via $\pi$), and the decoding process is modified to create an impression of sparsity. The utilization of these latent variables does not involve marginalization but instead resembles the soft-gating mechanism commonly employed in models like LSTMs or attention-based architectures.
Regarding Figure 5b, which illustrates the decoder input weights, the effect of component collapsing seems distinct from that of a Gaussian prior. Since $\pi$ is constrained to be positive, a small average value likely indicates that its value is near zero most of the time, thereby not necessitating updates to the weights. In contrast, under a standard Gaussian prior, component collapsing results in highly noisy inputs devoid of meaningful signals, compelling the decoder to deactivate the corresponding channel by assigning small incoming weights to the collapsed variable.
Including a histogram of the latent variables alongside the existing analysis could provide further clarity. This addition would help determine whether the associated weights are relatively large because the variables are actively used or simply because the inputs are effectively zero.
The semi-supervised results outperform a weaker variant of the model presented in (Kingma et al., 2014). However, for a more equitable comparison, the results should be benchmarked against the M1+M2 model from that paper, even if this necessitates employing two VAEs.