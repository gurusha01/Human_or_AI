Review - Summary:
The authors introduce a multi-hop "gated attention" model designed to capture interactions between query and document representations for solving cloze-style question-answering tasks. The document representation is iteratively refined over multiple hops by attending to it based on its similarity to the query representation, where a dot-product serves as the scoring/attention mechanism.  
The proposed approach achieves state-of-the-art performance on the CNN, Daily Mail, and Who-Did-What datasets, while performing comparably to existing methods on the CBT dataset.
Pros:
1. The hierarchical attention mechanism is a compelling idea, enabling the task-specific (query) representation to modulate the context (document) representation effectively.  
2. The paper is well-written, with a clear presentation and a comprehensive experimental comparison against the latest benchmarks.
Comments:
1. The proposed system incorporates several architectural components: (1) multi-hop attention across layers, (2) query-based attention for the context (gated attention), and (3) independent query encoding at each layer. While the ablation study in Section 4.4 highlights the importance of gated attention (2), the contributions of the following aspects remain unclear:  
   (1) The specific impact of employing multiple hops of gated attention on the overall performance.  
   (2) The necessity of having a distinct query encoder for each layer.  
   A deeper analysis of these factors would provide insights into simplifying the architecture.  
2. The token representations involve both L(w) and C(w). However, the role of C(w) in the model's performance is not entirely clear. The absence of C(w) (e.g., in the "GA Reader--" variant) leads to a noticeable performance drop, though other modifications in "GA Reader--" may also contribute to this decline. Consequently, it is difficult to isolate the contribution of the core idea—gated attention—to the superior performance of the proposed method.