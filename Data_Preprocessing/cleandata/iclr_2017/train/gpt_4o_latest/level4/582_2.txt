This paper introduces a method that integrates various modalities of product content (e.g., review text, images, co-purchase information, etc.) to learn a unified product representation for recommender systems. While leveraging multiple information sources is a promising strategy to address data sparsity in recommender systems, I have some concerns regarding the proposed approach:
1) Certain modalities may not be directly relevant to the recommendation task or item similarity. For instance, cover images of books or movies (used as product types in the experiments) may provide limited insight into their actual content. The paper should better justify and demonstrate the contribution of each modality to the overall task.
2) The relationship between the proposed joint product embedding and residual networks appears somewhat unclear. Traditional residual layers involve adding the original input vector to the output of an MLP, which consists of multiple affine transformations followed by non-linearities. This design facilitates the training of very deep neural networks (up to 1000 layers) by improving gradient flow. In contrast, the pairwise residual unit introduced in this paper adds the dot product of two item vectors to the dot product of the same vectors after applying a simple non-linearity. The rationale behind this architectural choice is not immediately apparent and is insufficiently explained in the paper.
3) Although a minor issue, the use of the term "embedding" to describe the dot product of two items is unconventional. Typically, embeddings refer to vectors in R^n associated with specific entities. Here, the term is used to describe the final output, which also renders the output layer in Figure 2 somewhat redundant.
In conclusion, the paper could be significantly improved by providing stronger justification for the architectural choices and adopting a more concise writing style. At 11 pages, the paper is overly lengthy, and I strongly recommend reducing its length.