This paper explores the modeling of graph sequences and introduces Graph Convolutional Recurrent Networks (GRCN), an extension of convLSTM (Shi et al., 2015) designed to handle data with irregular graph structures at each timestep. The authors substitute the 2D convolution in convLSTM with a graph convolutional operator from (Defferrard et al., 2016).
The authors propose two variants of the GRCN model. In the first variant, graph convolution is applied exclusively to the input data. In the second variant, graph convolution is applied to both the input data and the previous hidden states. The proposed models are evaluated on two tasks: video generation using the movingMNIST dataset and word-level language modeling using the Penn Treebank dataset.
For the movingMNIST dataset, the authors demonstrate that GRCN 2 outperforms convLSTM. However, their evaluation is limited to a single-layer convLSTM, whereas Shi et al. reported improved results with a three-layer convLSTM (albeit still not surpassing GRCN). It would be beneficial to assess GRCN in this multi-layer setting for a more comprehensive comparison. Additionally, while the authors show improvements over convLSTM, GRCN's performance on this task appears relatively weak when compared to more recent methods like Video Pixel Networks (Kalchbrenner et al., 2016). This discrepancy challenges the conclusion's claim that "Model 2 has shown good performance in the case of video prediction."
In the Penn Treebank experiments, the authors compare GRCN Model 1 to FC-LSTM, with and without dropout. However, the reported results differ from those in (Zaremba et al., 2014), where a test perplexity of 78.4 was achieved for the large regularized LSTM (as shown in their Table 1), outperforming GRCN. Furthermore, subsequent works, such as those incorporating variational dropout or zoneout, have improved upon Zaremba's results. It would be helpful to clarify any differences in experimental settings and provide results that are directly comparable to prior work.
Pros:
- The model is conceptually interesting.
Cons:
- The proposed contribution is relatively incremental compared to (Shi et al., 2015) and (Defferrard et al., 2016).
- GRCN's experimental results are weak relative to prior work, making it difficult to be convinced of the model's advantages.