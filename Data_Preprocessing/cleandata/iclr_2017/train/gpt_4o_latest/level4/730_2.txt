This paper introduces three enhancements to the standard LSTM architecture commonly employed in neural NLP models: Monte Carlo averaging, embed average pooling, and residual connections. Each of these modifications is straightforward to implement, making the paper highly relevant to NLP researchers exploring deep learning techniques.
However, I have concerns regarding the experiments and their findings. The residual connections do not consistently improve performance; for instance, on the SST dataset, vertical residuals show benefits while lateral residuals have a negative impact, whereas the opposite trend is observed on the IMDB dataset. More importantly, the scope of the experiments is limited to sentiment analysis tasks. It is unclear why the paper primarily focuses on text classification, as these modifications could potentially be advantageous for any NLP task utilizing an LSTM encoder. Including a broader range of tasks, such as QA, MT, and others, would significantly strengthen the paper and demonstrate the generalizability of the proposed improvements.
While the experiments presented in the paper are thorough and the analysis is insightful, the lack of diversity in tasks makes it difficult to assess the broader applicability of the modifications. As such, I believe the paper is not yet ready for publication.