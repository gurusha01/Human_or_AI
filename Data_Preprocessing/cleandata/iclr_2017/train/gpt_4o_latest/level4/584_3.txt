The paper presents a method for training joint models across multiple NLP tasks. Traditionally, these tasks are approached in a "pipeline" fashion, where later tasks depend on the outputs of earlier ones. In contrast, the authors propose a neural framework that integrates all tasks into a single model. In this approach, higher-level tasks utilize (1) the predictions from lower-level tasks and (2) the hidden representations of those tasks. The paper also introduces successive regularization, which intuitively aims to prevent significant changes to the lower-level model during the training of higher-level tasks, thereby preserving the predictive accuracy of the lower-level tasks.
From a modeling perspective, the proposed approach bears notable similarities to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al., 2016), albeit in a somewhat simpler form. The experimental setup is adequate in terms of the number of experiments conducted. However, I find the results in Table 1 unconvincing, as the patterns are inconsistent â€” in some cases, the performance of higher-level tasks decreases when trained jointly with more tasks, and even when it improves, the gains are neither significant nor stable. Regarding dependency scores, while this is not a major issue, comparing UAS/LAS when the outputs are not guaranteed to form a well-structured tree is not entirely fair.
I acknowledge that successive regularization is an intuitively appealing idea and represents an interesting direction to explore. However, the current results do not provide strong evidence that this is the optimal approach for such models. A more thorough investigation of the training methodology is needed, including strategies such as iterative training on different tasks and examining the relationship between the number of training iterations, task-specific dataset sizes, and task-specific losses.