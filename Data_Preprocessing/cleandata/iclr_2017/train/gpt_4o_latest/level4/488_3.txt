Paper Summary 
This paper explores the application of adversarial and virtual adversarial training techniques to LSTM models for text classification. Given the discrete nature of text inputs, adversarial perturbations are applied to the (normalized) word embeddings. The authors conduct extensive experiments, which highlight the advantages of these methods.
 Review Summary 
The paper is well-written and adequately referenced. Extending adversarial training to text data is a straightforward yet non-trivial contribution. The experimental section is thorough, offering comparisons with alternative strategies. The proposed method is both simple and effective, making it accessible for practitioners to implement after reading the paper.
 Detailed Review 
The paper is clear and well-structured. I have a few comments regarding the experimental setup and connections to prior research:
Experiments:
- In Table 2 (and for other datasets), could you include an SVM baseline? For example, S. Wang and C. Manning (2012)?
- Have you considered using word-dropping (masking noise) as another baseline? This approach often outperforms dropout or Gaussian noise for text-based applications (e.g., denoising autoencoders).
- In Table 5, I am unclear why virtual adversarial training performs worse than the baseline. If epsilon is properly tuned, wouldn't the worst-case scenario yield baseline-level performance? Was the validation process unreliable?
Related Work:
It might be valuable to discuss the connection between adversarial training and transductive SVMs, which achieve a similar goal. In transductive SVMs, maximizing the margin is akin to moving examples toward the decision boundary, effectively aligning with the direction of the loss gradient.
Additionally, it would be interesting to draw parallels between adversarial training and contrastive divergence. Adversarial samples closely resemble the one-step Markov Chain samples used in contrastive divergence (see Bengio, 2009). Related to this, there are techniques that explicitly aim to cancel the Jacobian at data points, such as those proposed by Rifai et al. (2011).
 References 
- Marginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K. Weinberger.  
- Stacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.  
- Learning Invariant Features Through Local Space Contraction. Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio, and Pascal Vincent, 2011.  
- Learning Deep Architectures for AI. Yoshua Bengio, 2009.  
- Large Scale Transductive SVMs. Ronan Collobert et al., 2006.  
- Optimization for Transductive SVM. O. Chapelle, V. Sindhwani, S. S. Keerthi. JMLR 2008.