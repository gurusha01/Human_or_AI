This paper presents a compelling and timely contribution with several notable aspects:  
- It introduces a framework for addressing combinatorial perception and action spaces that scales to an arbitrary number of units and opponent units.  
- It establishes deep reinforcement learning (RL) baseline results across various Starcraft subdomains.  
- It proposes a novel algorithm that combines black-box optimization with REINFORCE, enabling consistent exploration.  
However, there are several points that warrant clarification or revision:  
As raised in an earlier comment, the choice of the "gradient of the average cumulative reward" as opposed to the average reward is questionable. This approach disproportionately emphasizes later rewards at the expense of earlier ones, leading to updates that do not align with the measured objective. While the authors mention that "no large difference was observed in preliminary experiments," this raises the question: if the observed impact is negligible, why not adopt the correct objective?  
The description of DPQ is inaccurate. Contrary to its name, DPQ does not "collect traces by following deterministic policies." Instead, it employs a stochastic behavior policy while learning off-policy about the deterministic policy. This should be corrected.  
The characterization of gradient-free optimization is also misleading. The claim that it "only scales to few parameters" is contradicted by recent work, such as the TORCS paper by Koutnik et al. (2013), which demonstrates that this limitation can be addressed. This raises concerns about the methodology used in your "preliminary experiments with direct exploration in the parameter space." Did these experiments adhere to best practices in neuroevolution? For example, did you evaluate recent variants of NEAT, which have been successfully applied to similar domains?  
Regarding the specific results, I am puzzled by the DQN transfer performance from m15v16 to m5v5. The model achieves a 96% win rate in transfer (the best result) despite only reaching 13% (the worst result) on the training domain. Is this a typographical error, or can you provide an explanation for this discrepancy?