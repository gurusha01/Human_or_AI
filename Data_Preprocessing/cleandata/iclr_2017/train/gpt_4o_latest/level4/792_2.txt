Inspired by the analysis of co-label similarity effects (Hinton et al., 2015), this paper introduces a soft-target regularization method that iteratively trains the network using a weighted average of the exponential moving average of past labels and hard labels as the target argument of the loss function. The authors argue that this approach mitigates the loss of co-label similarity during early training and provides a competitive regularization alternative to dropout without reducing network capacity.
However, for a fair comparison with dropout, careful tuning of dropout parameters is essential. Demonstrating better performance than dropout for specific dropout values (Table 2) is insufficient to establish a convincing advantage. Proper cross-validation could potentially yield better results for dropout.
The baseline architectures used in the experiments are not representative of recent state-of-the-art methods, resulting in significantly lower accuracy. Additionally, the experimental setup appears to lack data augmentation, which could alter the results. The decision to limit the number of epochs to 100 without convergence tests is also questionable. Consequently, the empirical study does not convincingly establish the significance of the proposed method.
Instead of relying on predicted labels, co-label similarities could be computed using the softmax outputs of the final layer. Furthermore, the advantage over dropout is unclear in Figure 4, as dropout is fixed at 0.2 without cross-validation.
While the idea of regularizing training by preserving co-label similarities is interesting, it is not particularly novel, and the results lack significance.
Pros:
- Investigates regularization of co-label similarity during training.
Cons:
- Empirical results do not substantiate the intuitive claims of the proposed method.
- The iterative version may be unstable in practice.
Suggestions for Improvement:
1. Misclassification Error Reporting: Test data misclassification error should be reported for clear comparisons, as this is crucial for evaluating losses and final test performance. Current comparisons are inconclusive.
   
2. Clarification of Updates in Equations (3) and (4): The rationale for the specific updates in (3) and (4) should be better explained. Why not use a totally corrective update, such as taking a convex combination of all \(\cal{F}'s\) (or a subset) up to the current iteration? Parameters \(\beta\) and \(\gamma\) should be reasonably tuned, and the cross-validation range should be reported to assess whether these updates are genuinely beneficial.
3. Parameter Choices for \(nt\) and \(nb\): The justification for setting \(nt = nb\) is unsatisfactory. Cross-validation of these parameters is critical. Is \(nt = \{1, 2\}\) an unreasonably small value that could lead to instability? Why are \(nb\) and \(nt\) equal? Results from experiments with other \(nb\) and \(n_t\) values should be included.
4. Comparison with Distillation: The paper claims that co-label similarities disappear when the network starts to overfit. However, distillation (Hinton et al., 2015) captures co-label similarities after training by using a teacher model. The proposed method appears to be an iterative extension of distillation without a larger teacher model. Does the proposed method outperform a two-step distillation approach?
5. Tuning \(\lambda\) for Weight Decay: The paper does not explain how \(\lambda\) for weight decay is tuned. This should be clarified.
6. Hyperparameter Tuning: The authors state, "We considered a frozen set of hyper-parameters for the SoftTarget regularization to show that SoftTarget regularization can still work without having to conduct a large grid search." This argument is invalid in machine learning. Without proper hyperparameter tuning, it is possible that the results could worsen. A standard tuning procedure, such as random search (Bergstra and Bengio, JMLR 2012) or Bayesian optimization (Snoek et al., NIPS 2012), should be employed. Setting hyperparameters arbitrarily without searching a range or set undermines the fairness of the comparison.