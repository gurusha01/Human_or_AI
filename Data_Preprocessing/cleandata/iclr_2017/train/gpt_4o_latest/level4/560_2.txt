The paper is well-motivated and contributes to the growing body of research exploring the use of orthogonal weight matrices in recurrent neural networks. While orthogonal weights mitigate the vanishing/exploding gradient problem, it remains unclear whether enforcing orthogonality compromises representational power or trainability. Consequently, an empirical investigation into how deviations from orthogonality influence these properties is a meaningful and valuable contribution.
The manuscript is clearly written, and the primary approach for exploring soft orthogonality constraints—representing weight matrices in their SVD factorized form to explicitly control singular values—is both clean and intuitive. However, this formulation may not be ideal from a practical computational perspective, as it necessitates maintaining multiple orthogonal weight matrices, each requiring a computationally expensive update step. To the best of my knowledge, this specific approach has not been previously explored.
On the experimental front, the evaluation is somewhat limited. The paper considers two tasks: a copy task using an RNN architecture without transition non-linearities and sequential/permuted sequential MNIST. While these are reasonable choices for an initial investigation, they are toy problems and do not provide much insight into the practical utility of the proposed method. An evaluation on a more realistic task, such as language modeling, would significantly enhance the paper's impact.
Additionally, while focusing on pure RNNs is logical for isolating the effects of orthogonality, this choice feels somewhat academic. LSTMs inherently address long-term dependencies, and in the tasks where the proposed approach was compared directly to an LSTM, the latter significantly outperformed it. It would be intriguing to explore the effects of the proposed soft orthogonality constraint in other architectures, such as deep feed-forward networks, or to investigate whether incorporating it within an LSTM offers any advantages (though this seems unlikely).
In summary, the paper tackles a well-defined question with a sound and motivated approach, yielding interesting findings on toy datasets. However, the scope of the work is limited by the narrow experimental settings, both in terms of datasets and network architectures, which restricts its overall significance.