Regrettably, even after reviewing the authors' response to my earlier query, I believe this paper, in its current state, does not demonstrate sufficient novelty to warrant acceptance at ICLR.
At its core, the paper proposes replacing traditional iterative algorithms for a specific class of problems (ill-posed image inverse problems) with discriminatively trained recurrent networks. However, as R3 also points out, the concept of unrolled networks for iterative inference is not novel; such networks have been previously employed to replace CRF-type inference and to address image inverse problems (see my references [1-3]). Consequently, I contend that the central idea of the paper lacks originality—it primarily seeks to 'formalize' this approach for inverse problems. That said, the analysis presented does not appear to be uniquely tied to inverse problems, as the paper merely demonstrates that the RIM can represent gradient descent over a prior + likelihood objective.
Additionally, I found the claims regarding advantages over prior methods unconvincing. The argument about parameter sharing is a double-edged sword—it is equally plausible that untying parameters could yield better performance over fewer 'iterations.' Given that the 'training set' is synthetically generated, learning a larger number of parameters should not pose a significant challenge. Furthermore, I would argue that parameter sharing is the 'obvious' approach, and prior methods deliberately avoid tying parameters to achieve higher accuracy.
The same critique applies to the claim about handling varying noise levels or scale sizes. A single model can always be trained to accommodate multiple types of degradation, but it is likely to perform better when tailored to a specific degradation model or level. More importantly, the current experiments do not provide evidence that this capability is an inherent property of the RIM architecture. (In fact, this claim seems to contradict one of the paper's motivations—not training a single prior for different observation models, but instead training the entire inference architecture end-to-end.)
While it is possible that the proposed method offers practical advantages over prior work, these benefits do not stem from the idea of unrolling iterations, which is not novel. I strongly recommend that the authors undertake a substantial revision of the paper. This should include a thorough discussion of related work, as noted in the comments, and experimental results that clearly highlight the specific aspects of their recurrent architecture that enable superior recovery for inverse problems. Furthermore, to substantiate the claim of addressing 'inverse problems,' the paper should consider a broader range of tasks, such as in-painting, deconvolution, various noise models, and potentially scenarios involving multiple observations (e.g., HDR).