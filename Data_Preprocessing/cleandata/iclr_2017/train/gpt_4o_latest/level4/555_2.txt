This paper addresses the challenge of preselecting deep learning model architectures for novel domains. It presents a series of experiments conducted on various small-scale tasks using feed-forward deep neural networks (DNNs). The authors propose that a ranking algorithm can be derived from these results to guide the selection of model architectures for new domains.
While the objective is intriguing, I find the conclusions drawn in the paper to be neither compelling nor practically applicable for several reasons:
1. The study is restricted to simple network architectures (feed-forward DNNs). Although this reduces the complexity of the search space, it also diminishes the practical relevance of the findings. In reality, the optimal model architecture is highly dependent on the specific task or domain, and the choice of model type (e.g., DNN, CNN, LSTM) often has a far greater impact than the network size alone.
2. The experiments were conducted with certain critical hyperparameters, such as the learning rate schedule, held constant. However, it is well established that the learning rate is one of the most influential hyperparameters during training. Without tuning such key hyperparameters, the conclusions regarding the best model architecture lack credibility.
3. The experiments suggest that differences in training data are not significant. This claim is unlikely to hold, as larger datasets typically necessitate larger models (i.e., the total number of parameters). For instance, log(dataset size) could serve as an important feature in guiding model selection. This limitation likely stems from the fact that the authors did not conduct experiments on large-scale datasets.
Additionally, I believe the title of the paper does not accurately represent its content and should be revised. Furthermore, the paper cites Sainath et al. (2015) as the work that led to breakthroughs in speech recognition. However, the pivotal advancements in automatic speech recognition (ASR) occurred much earlier. The foundational work incorporating all three key components was published in 2010:
Yu, D., Deng, L., and Dahl, G., 2010, December. Roles of pre-training and fine-tuning in context-dependent DBN-HMMs for real-world speech recognition. In Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning.
A more detailed version of this work was subsequently published in 2012:
Dahl, G.E., Yu, D., Deng, L., and Acero, A., 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), pp.30-42.
In conclusion, this paper presents some preliminary findings that, while interesting, are not yet mature enough for publication.