This paper introduces a principled method for identifying flat minima, motivated by their superior generalization properties. The proposed approach augments the original loss function with an additional term that accounts for both the width and depth of the objective function. Notably, this regularization term can be interpreted as a Gaussian convolution of the exponentiated loss, effectively making it a Gaussian-smoothed version of the exponentiated loss. Such smoothing naturally suppresses sharp minima.
The development of this regularization term, inspired by thermodynamic principles, is quite compelling. However, I have a few concerns that the authors should address in their rebuttal.
1. In the reported generalization performance, the experiments highlight the number of epochs required, showing that the proposed algorithm achieves better generalization in fewer epochs compared to plain SGD. Does this refer to the number of epochs required by line 7 of the algorithm, or does it represent the total number of epochs (combining lines 3 and 7)? If it is the former, the comparison may not be entirely fair. When considering the number of epochs for SGD (line 7) multiplied by the iterations required to approximate Langevin dynamics, the improvement over plain SGD appears minimal.
2. The proposed algorithm approximates the smoothed "exponentiated" loss (where smoothing refers to convolution with a Gaussian). How does this compare to the simpler approach of smoothing the original loss directly (without exponentiation)? Is the distinction primarily in the motivation (e.g., the thermodynamic interpretation), or does the proposed method offer deeper advantages, such as enabling more accurate approximations or achieving tighter generalization bounds (in terms of the attained smoothness)? Smoothing the cost function without exponentiation allows for simpler approximations (e.g., Monte Carlo integration instead of MCMC), as discussed in section 5.3 of [reference].