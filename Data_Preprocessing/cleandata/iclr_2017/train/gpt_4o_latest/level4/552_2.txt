This paper explores recurrent networks characterized by an update rule of the form \( h{t+1} = Rx R ht \), where \( Rx \) represents an embedding of the input \( x \) into the space of orthogonal or unitary matrices, and \( R \) is a shared orthogonal or unitary matrix. While the model is conceptually interesting, it cannot be considered novel. The idea of representing input objects with matrices and using matrix multiplication for state updates has been extensively explored in the embedding-knowledge-bases and embedding-logic literature (e.g., "Using matrices to model symbolic relationships" by Ilya Sutskever and Geoffrey Hinton, or "Holographic Embeddings of Knowledge Graphs" by Maximillian Nickel et al.). Unfortunately, the experiments and analysis presented in this paper do not significantly advance our understanding of the model.
The experimental evaluation is particularly weak, as it is limited to a highly simplified version of the copy task, which is already a well-known toy problem. Moreover, as noted by the other reviewer, the model's inability to forget poses a practical challenge, and I am aware of several researchers who have encountered this limitation when applying the model to language modeling tasks.
To justify the acceptance of this paper, the authors need to demonstrate the model's utility on a more substantial, nontrivial task. As it stands, the work does not meet the bar for acceptance.
Some questions for the authors: Is there a specific reason for using the shared \( R \) instead of incorporating it into all the \( Rx \)? Additionally, are there any promising ways to leverage the fact that the model is linear in \( h \) or linear in \( Rx \)?