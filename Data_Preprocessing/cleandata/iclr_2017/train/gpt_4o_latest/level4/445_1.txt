As discussed, the authors present multiple concurrent contributions across different packages/submissions, which are, at times, challenging to disentangle.  
That said, it is remarkable to observe a system learning from natural feedback in an online manner. To the best of my knowledge, this represents a novel quality of results—especially given that, in some cases, the system achieves performance comparable to full supervision within this less constrained setting.
Several points were raised during the review process, which the authors addressed as follows:  
1. Formalization of the task (learning dialogue) and defining success:  
The task's formalization remains somewhat imprecise, particularly in terms of when success can be declared. The authors' response is partially satisfying. For this specific work, it might be beneficial to define goals more explicitly, such as aiming to match full supervision performance.  
2. Dialogue as noisy supervision and reporting classic baselines:  
Dialogue can be interpreted as a form of noisy supervision. I requested the authors to report classic supervision baselines for the specific model used, as this would provide a clearer sense of what fraction of the best-case performance is achieved through dialogue learning. The authors provided additional information along these lines, which helps illuminate the extent of the progress made and the remaining challenges.  
3. Difficulty of the MT setting and handcrafted baselines:  
I inquired whether the authors had explored how much more challenging the machine translation (MT) setting is. For instance, feedback could be manually labeled as positive or negative for analysis, or a handcrafted baseline could be tested—such as extracting rewards via template matching or using feedback length as a proxy (short feedback appears correlated with high reward/correct answers). While the authors responded to this point, it would have been more compelling if they had quantified such baselines to confirm that no simple handcrafted approach performs well on the data. However, these concerns are relatively minor.  
4. Relation to prior work (Weston'16):  
The relationship to Weston'16 remains somewhat unclear. My understanding is that this submission is intended as an independent contribution rather than a replacement for Weston'16. However, this makes the current work appear more incremental. The key contribution of this submission seems to be the online learning aspect, which facilitates greater exploration. I could not find an explicit analysis of how much this aspect matters in the experiments. The authors clarified the raised issues, and the application of reinforcement learning—particularly the use of FP—is convincing.  
Overall, while there is an incremental nature to the paper, this impression is amplified by the multiple concurrent contributions from the authors on this research thread. A more explicit comparison to prior work (especially Weston'16) is warranted, both in the text and the experiments, as partially addressed in the authors' response. Nevertheless, this specific contribution is significant, worth sharing, and likely to influence how learning is approached in less constrained settings.