This paper introduces a joint classification approach for images and audio captions, aimed at discovering word-like acoustic units that correspond to semantically meaningful visual objects. Overall, this represents a compelling and important research direction, as it enables richer data representations by regularizing the visual signal with audio and vice versa. Such an approach has broader implications, such as facilitating the training of visual models using video data, among other applications.
One significant concern, however, is the limited novelty when compared to the authors' prior work published at NIPS 2016. While the authors present a more sophisticated architecture and demonstrate improvements in recall, these gains are relatively modest, and the added architectural complexity appears somewhat ad hoc. For instance, the clustering and grouping methods described in Section 4 come across as inelegant. Instead of relying on image gridding, the authors could leverage object detection techniques (e.g., SSD, YOLO, Faster R-CNN) to generate more precise object proposals. Similarly, replacing k-means with a spectral clustering approach could address the Gaussian distribution assumption inherent in their method. Furthermore, when associating visual hypotheses with acoustic segments, a bi-partite matching approach would likely be more robust.
In summary, while I find this research direction highly promising and encourage the authors to continue exploring algorithms for multimodal datasets, the current work lacks sufficient novelty compared to the NIPS 2016 publication.