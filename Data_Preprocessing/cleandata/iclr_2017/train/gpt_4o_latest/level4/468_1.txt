This paper introduces a network quantization approach aimed at compressing the parameters of neural networks, thereby reducing the storage requirements for these parameters. The authors operate under the assumption that the network has already been pruned and focus on compressing the remaining non-pruned parameters. The problem of network compression is well-motivated and aligns with the interests of the ICLR community.
The primary limitation of the paper lies in its novelty. While the work builds extensively on the findings of Han 2015 and only marginally extends them to address certain limitations, it is important to acknowledge that the method proposed in this paper has not been previously introduced.
The paper is well-organized and straightforward to follow. However, despite its reliance on Han 2015, it is significantly longer. There appears to be some redundancy in the text, as the experiments section begins on Page 12, whereas in Han 2015, the experiments start on Page 5. This suggests that much of the introductory material could be streamlined to improve efficiency.
The experimental results demonstrate strong compression performance compared to Han 2015, with minimal loss in accuracy. However, the authors should clarify why Table 1 does not include a comparison with Han 2015 on ResNet.
Additional comments:
1) It is unclear whether the procedure illustrated in Figure 1 is an original contribution of the authors or if it has been previously described in the literature.
2) In Section 4.1, the authors approximate the Hessian matrix with a diagonal matrix. Could the authors elaborate on how this approximation impacts the final compression results? Additionally, how much performance is sacrificed due to this simplification?
Minor typos (to be addressed in the revised version of the paper):
1) Page 2, Paragraph 3, third line from the end: "fined-tuned" → "fine-tuned"
2) Page 2, one paragraph before the end, last line: "assigned for" → "assigned to"
3) Page 5, Line 2: same as above
4) Page 8, Section 5, Line 3: "explore" → "explored"