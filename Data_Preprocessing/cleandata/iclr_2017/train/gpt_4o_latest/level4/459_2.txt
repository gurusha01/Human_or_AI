The paper introduces a tensor factorization method for MTL to capture cross-task structures, thereby enhancing generalization. The presentation is well-organized and easy to follow, and the experimental results provide strong support for the proposed approach.
As noted, it would be beneficial for the final version to include discussions on the trade-off between model size and performance, as well as comparisons to related work in other domains.
A question regarding Sec. 3.3: In constructing the DMTRL, a separate DNN is pretrained for each task using the same architecture. How critical is this pretraining step? Could random initialization suffice instead? Additionally, if the dataset is imbalanced, with certain classes having very few samples, how would this impact the model's performance?