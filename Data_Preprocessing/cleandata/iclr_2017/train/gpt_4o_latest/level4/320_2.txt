This paper explores the advantages of visual servoing through the use of a learned visual representation. The authors propose initially learning an action-conditional bilinear model of visual features (extracted from a pre-trained VGG network), from which a policy is derived by linearizing the dynamics. A multi-scale, multi-channel, and locally-connected variant of the bilinear model is introduced. Since the bilinear model predicts only one-step-ahead dynamics, the paper incorporates a weighted objective that accounts for the long-term values of the current policy. The evaluation challenge is tackled using a fitted-value approach.
The paper is well-written, mathematically rigorous, and conceptually thorough. The experiments effectively highlight the advantages of employing a value-weighted objective, which stands out as a significant contribution of this work. Additionally, the paper appears to be the first to propose a trust-region fitted-Q iteration algorithm. Empirical results also demonstrate that using pre-trained visual features aids generalization.
In summary, I recommend this paper as it provides valuable insights for researchers in robotics. However, within the scope of this conference, the contribution to the "representation" problem seems somewhat limited. While the paper demonstrates the utility of a pre-trained VGG representation, it does not explore learning the representation end-to-end. This is not to suggest that end-to-end learning is necessary, but the paper places greater emphasis on the control problem than on representation learning. Furthermore, the policy representation is fixed, and values are approximated in linear form using problem-specific features. While this does not diminish the paper's value, it may make it less aligned with what I perceive to be the focus of ICLR.