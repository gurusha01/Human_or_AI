The paper introduces a methodology for adapting a trained network to a different architecture without requiring complete retraining.  
The manuscript is well-written, and the explanations are clear and easy to understand.  
Nonetheless, the results are not particularly compelling, as the chosen baselines are significantly outdated compared to the current state of the art.  
The authors should incorporate comparisons with state-of-the-art methods, such as wide residual networks.  
Additionally, the tables should include the number of parameters for each architecture to enable a fairer comparison.