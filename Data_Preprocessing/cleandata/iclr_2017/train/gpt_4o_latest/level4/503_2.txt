The paper presents Gated Multimodal Units (GMUs), which employ multiplicative weights to determine the extent to which a hidden unit incorporates different modalities in its activation. Additionally, the paper introduces a novel dataset, "Multimodal IMDb," comprising over 25k movie summaries, their corresponding posters, and labeled genres.
GMUs share similarities with "mixture of experts" models in that different parts of the model classify different examples; however, rather than gating or routing entire examples, GMUs gate individual hidden units independently. They also bear resemblance to attention mechanisms, as they assign distinct weights to different parts of the input, though the focus here is on gating input modalities.
The dataset is a valuable contribution, and the paper includes a variety of experiments exploring text representation and comparisons between single-modality and two-modality approaches. However, the paper falls short in providing a thorough discussion, experimental evaluation, and analysis against other multiplicative gating models, which constitutes the primary intellectual contribution of the work. For instance, it would be insightful to compare GMUs with mixture of experts, attention models, or other gated architectures, as these might yield competitive performance or offer meaningful scientific insights. I encourage the authors to expand on this aspect and resubmit a more comprehensive version of the paper.
As it stands, I view this work as suitable for a workshop setting but not yet ready for acceptance at a major conference.