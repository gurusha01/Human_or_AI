The paper introduces an innovative approach to handle dynamic computation graphs, which occur when the computation is influenced dynamically by the input data, as seen in LSTMs. The authors propose an `unrolling' technique applied to the operations performed at each step, enabling a novel method for batching inputs.
The proposed idea is original, and the results demonstrate the potential of the approach. However, for improved clarity in the presentation, I suggest omitting portions of Section 3 ("A combinator library for neural networks"). While the technical details presented there are generally interesting, they do not contribute to a better understanding of the paper's core concept. Additionally, the experimental results on the "Stanford Sentiment Treebank" do not convincingly support the paper's primary claim, which focuses on speed, and are somewhat unclear. It is worth noting that although the ensemble "[...] variant sets a new state-of-the-art on both subtasks" [p. 8], this achievement is not attributable to the framework or even the model itself (as seen in lines 4 and 2 of Tab. 2). Instead, it is likely, though speculative, that the improvement stems from ensemble averaging. A more explicit argumentation on this point would be beneficial.
Update as of Jan. 17th:
Following the authors' latest revision, I am raising my rating to 8, as the argumentation has been further refined and is now exceptionally clear.