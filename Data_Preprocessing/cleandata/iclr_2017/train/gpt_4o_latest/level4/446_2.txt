Paraphrased Review:
After reading the authors' rebuttal, I have decided to raise my score. I believe that ALI contributes to stabilizing GAN training, as evidenced in Fig. 8, and demonstrates the ability to learn a reasonably effective inference network.
---
Initial Review (Paraphrased):
This paper introduces a novel approach for learning an inference network within the GAN framework. The objective of ALI is to align the joint distribution of latent and observed variables as defined by the encoder and decoder networks. The method is evaluated on various datasets and appears to achieve good reconstruction performance, despite lacking an explicit reconstruction term in its cost function. This suggests that ALI is capable of learning a robust inference network for GANs.
There are currently several established methods for learning an inference network for GANs. For instance, one could train a GAN first and then learn a separate network to map X to Z by sampling from the GAN. Another approach is infoGAN (not cited), which simultaneously trains the inference network alongside the generative model. This paper would benefit from a more comprehensive comparison with these alternative methods, as well as a discussion of why ALI's inference network is superior to prior work.
Since ALI's inference network is stochastic, it would be valuable to include examples of multiple reconstructions for the same image. My understanding is that the inference network in the BiGAN paper is deterministic, which is a key distinction from this work. Highlighting this difference explicitly would strengthen the paper.
The generated samples are of high quality, but the paper lacks quantitative experiments comparing ALI's samples to those of other GAN variants. It remains unclear whether the inclusion of an inference network contributes to improved generative sample quality. Adding an inception score or similar metric for comparison could help address this question.
The paper presents two sets of semi-supervised learning results:  
1. In the first set, the hidden layers of the inference network are concatenated and used with an L2-SVM. However, concatenating feature maps is not the most effective approach for semi-supervised learning. Ideally, the semi-supervised learning path should be trained jointly with the generative model. A more compelling approach would involve designing the latent space such that part of the hidden code represents a categorical distribution and another part represents a continuous distribution (e.g., Gaussian). The categorical latent variable could then be directly used for classification, similar to semi-supervised VAE. This would allow the inference network to be trained jointly with the generative model. Additionally, demonstrating that ALI can disentangle factors of variation using a discrete latent variable, as in infoGAN, would significantly enhance the paper's contribution.
2. The second set of semi-supervised results shows that ALI achieves state-of-the-art performance. However, my impression is that the primary improvement comes from the adaptation of Salimans et al. (2016), where the discriminator is used for classification. It remains unclear how learning an inference network improves the discriminator's classification performance. How can we be certain that the proposed method enhances GAN stability? One of the key motivations for learning an inference network is to map images to high-level features, such as class labels. Therefore, it would have been more compelling if the inference path had been directly utilized for semi-supervised learning, as suggested above.