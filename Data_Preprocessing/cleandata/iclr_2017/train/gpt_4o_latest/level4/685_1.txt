This paper presents an extension of neural network language (NLM) models aimed at improving their ability to handle large vocabularies. The core idea involves generating word embeddings by combining character-level embeddings with a convolutional network.
The authors conduct a comparison between word embeddings (WE), character embeddings (CE), and a hybrid approach combining character and word embeddings (CWE). While it is relatively straightforward to incorporate CE or CWE embeddings as inputs to an NLM, their integration at the output layer poses greater challenges. To address this, the authors propose using Noise-Contrastive Estimation (NCE). While NCE accelerates training, it does not affect inference during testing, where the full softmax output layer must still be computed and normalized—a process that can be computationally expensive.
However, I found it unclear how the network operates during TESTING in an open-vocabulary setting. Since the NLM is employed only for reranking, the unnormalized probability of the target word could theoretically be extracted from the output. Nevertheless, when reranking n-best lists with the NLM feature, sentences with varying probabilities are compared, and I question whether this approach performs effectively without proper normalization.
Additionally, the authors present perplexity results in Table 2 and Figures 2 and 3, which inherently require normalization. However, the methodology for this normalization is not explicitly described. The authors mention a 250k output vocabulary, but I find it doubtful that the softmax was computed over all 250k values. Clarification on this point is necessary.
The model is evaluated by reranking n-best lists generated by an SMT system for the IWSLT 2016 EN/CZ task. In the abstract, the authors claim a gain of 0.7 BLEU. I disagree with this assertion. A standard word-based NLM—a well-established baseline—already achieves a gain of 0.6 BLEU. Thus, the proposed model contributes only an additional 0.1 BLEU improvement, which is not statistically significant. I suspect that similar variations could be achieved by training multiple models with different initializations or other minor adjustments.
Regrettably, the NLM models incorporating character-level representations at the output layer do not perform well. There is already a body of work exploring the use of character-level representations at the input layer.
Could the authors provide a discussion on the computational complexity of the proposed approach during both training and inference?
Minor comments  
- Figures 2 and 3 are labeled with the caption "Figure 4," which is misleading.  
- The citation format is unconventional. For example:  
  "While the use of subword units Botha & Blunsom (2014)"  
  should be revised to:  
  "While the use of subword units (Botha & Blunsom, 2014)."