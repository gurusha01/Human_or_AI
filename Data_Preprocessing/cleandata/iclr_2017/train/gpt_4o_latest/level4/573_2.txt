This paper introduces a framework for training models that actively seek information (e.g., by asking questions) to solve a given task. The authors present a set of tasks specifically designed to achieve this goal and demonstrate that reinforcement learning can be used to train models to successfully complete these tasks.
A key motivation for the proposed tasks stems from games like 20 Questions or Battleship, where an agent must ask questions to accomplish a given objective. Surprisingly, the authors do not explore these games as potential tasks (with the exception of Hangman). Additionally, the criteria for selecting the tasks are not entirely clear. Prior research has extensively analyzed the properties of games like 20 Questions (e.g., Navarro et al., 2010) and how humans approach them. It would be valuable to understand how the tasks proposed in this work differ from those studied in the existing literature and how humans would perform on them. For instance, Cohen & Lake (2016) recently examined the 20 Questions game in their paper "Searching large hypothesis spaces by asking questions," evaluating both human and computational performance. A similar study would significantly enhance this paper.
Developing models capable of actively seeking information to solve tasks is an intriguing yet challenging problem. In this work, all tasks require the agent to select questions from a finite set of clean and informative options. While this simplifies the analysis of agent performance, it does so at the expense of realism, as it reduces the noise inherent in more practical, real-world scenarios.
The authors also demonstrate that a relatively standard combination of deep learning models and reinforcement learning can train agents to solve these tasks as intended. While this validates their experimental setup, it also highlights potential limitations of their approach. The reliance on relatively simplistic, toy-like settings with perfect information and a fixed number of questions may oversimplify the problem.
Although the results show that the agents perform well on the proposed tasks, the lack of baselines limits the strength of the conclusions. For example, in the Hangman experiment, the frequency-based model achieves promising results. It would be interesting to compare the performance of stronger baselines, such as those leveraging letter co-occurrence or character n-gram frequencies.
In summary, this paper explores an exciting research direction and proposes a promising set of tasks to evaluate a model's ability to learn by asking questions. However, the current analysis of the tasks is somewhat limited, making it difficult to draw definitive conclusions. The paper would benefit from a stronger focus on human performance on these tasks, comparisons with robust and simple baselines, and the inclusion of more tasks related to natural language (given that this is one of the work's motivations), rather than primarily emphasizing solutions using sophisticated models.