This paper introduces an approach to enhance RNN-based language models by incorporating a pointer network to better handle rare words. The pointer network enables the model to reference words in the recent context, resulting in predictions at each time step being a combination of the standard softmax output and the pointer distribution over recent words. Additionally, the paper presents a new language modeling dataset designed to address some limitations of existing datasets.
The score I assigned to this paper is primarily due to the fact that the proposed model appears to be a straightforward adaptation of prior work by Gulcehre et al., which employs a similar methodology in the context of machine translation and summarization. The key distinctions I observe are that Gulcehre et al. utilize an encoder-decoder architecture and leverage the encoder's attention weights to identify word locations in the input, whereas this paper employs an RNN and uses a pointer network to generate a distribution over the entire vocabulary by summing the softmax probabilities of words in the recent context. The context (query) vector for the pointer network also differs, but this variation arises naturally from the distinct application.
Although the paper outlines the differences between the proposed approach and that of Gulcehre et al., I find certain claims either inaccurate or not particularly significant. For instance, in Section 1, the authors state:
"Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel."
As far as I can discern, the proposed model also utilizes the recent hidden state to construct a query vector, which the pointer network matches to prior words. Could the authors clarify what they mean in this context?
Additionally, in Section 3, the authors describe the model of Gulcehre et al. as follows:
"Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use."
This statement is incorrect. Gulcehre et al.'s model is also a mixture model, where a switching network (an MLP with a sigmoid output) is employed to create a mixture of the softmax prediction and input text locations.
Furthermore, in the following statement from Section 3:
"The pointer network is not used as a source of information for the switching network as in our model."
The meaning of "source of information" here is unclear. Are the authors referring to the fact that the switching probability is part of the pointer softmax? I am uncertain about the significance of this distinction.
Regarding the proposed dataset, there are other widely used datasets for language modeling, such as the Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and the Text8 dataset (Mahoney, 2009). Could the authors elaborate on how their dataset compares to these alternatives?
I am open to discussing the points I have raised with the authors and am willing to reconsider my evaluation if any of my concerns stem from a misunderstanding.