The primary contribution of this paper is the development of a method to ε-approximate a piecewise smooth function using a multilayer neural network with O(log(1/ε)) layers and O(poly log(1/ε)) hidden units. The activation functions employed can be ReLU, binary step, or any combination of the two. The paper is well-written and presented clearly, with arguments and proofs that are straightforward to follow. I have only two questions:
1- It would be valuable to see analogous results without relying on binary step units. How critical is the binary step unit to the proof?
2- Can you provide an example of a piecewise smooth function that necessitates at least poly(1/ε) hidden units in a shallow network?