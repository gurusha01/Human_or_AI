This paper introduces a method for iteratively enhancing the output of an existing machine translation system by identifying potential errors and proposing corrections, leveraging an attention-based model. The approach is inspired by the assumed process through which human translators refine translations.
The paper is creative and thought-provoking. However, I remain somewhat sceptical about this class of approaches, where a machine learning model is employed to identify and rectify the predictions of another model—or even its own predictions. In the first scenario, if the new model is superior, why not simply use it as the primary system instead of the original one? In the second scenario, given that the model operates with no additional information compared to its earlier predictions, why would it be more effective at spotting and fixing prior errors than it would be at introducing new errors by altering correct terms? This would only make sense if there were a clear justification showing that an iterative process can reliably converge to better results over multiple iterations.
This paper does not adequately address these concerns. For example, the authors acknowledge that "the probability of correctly labelling a word as a mistake remains low (62%)," which, while better than random chance, is not compared against a more meaningful baseline. For instance, one could contrast the existing system with a more advanced convolutional model and treat all discrepancies as errors. The oracle experiments, while interesting, are of limited value—they merely confirm that improving a translation is straightforward when the errors are pre-identified, but significantly more challenging when they are not.
While I appreciate the paper overall, I am not fully convinced that the primary claim—that iterative refinement is advantageous—has been convincingly demonstrated. To strengthen the argument, the authors would need to include more robust baselines. Specifically, they should demonstrate that an iterative refinement approach can outperform a system closely aligned with the attention-based model, both as a standalone system and in combination with a PBMT system. Additionally, it should be shown that the PBMT system is not merely acting as a regularizer for the attention-based model.
Minor comments:
- The notation is occasionally overly complex—for example, F^i = (F^{i,1}, F^{i,|F^i|}). Why use |F^i| here when F is a matrix? The length of the slice should not depend on i.
- In the discussion in Section 4, there appears to be a mismatch between the training and test conditions. Could anything be done to address this discrepancy?