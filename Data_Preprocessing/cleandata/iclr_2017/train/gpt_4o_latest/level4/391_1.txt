The paper introduces a method for pruning neural network weights during training to achieve sparse solutions. This approach is applied to an RNN-based system, which is trained and evaluated on a speech recognition dataset. The results demonstrate that significant reductions in test-time computations can be achieved with minimal impact on task performance. Interestingly, in some cases, the method even improves evaluation performance.
The experiments are conducted using a state-of-the-art RNN system, and the experimental methodology appears robust. I appreciate that the pruning effects are analyzed for networks of considerable size, and the computational savings are clearly substantial. However, it is somewhat disappointing that all experiments rely on a private dataset. Even if private training data is used, it would have been beneficial to include evaluations on a well-known test set, such as HUB5 for conversational speech. Additionally, a comparison with other pruning methods, particularly given the resemblance of the proposed approach to the work by Han et al. [2], would have been valuable to establish the relative advantages of this method. While the single-stage training process initially seems more elegant, it may not save much time in practice if extensive experimentation is required to tune the hyperparameters for the threshold adaptation scheme. Furthermore, the dense baseline would have been more compelling if it incorporated model compression techniques, such as training on soft targets generated by a larger network.
Overall, the paper is well-written and easy to follow. While the table and figure captions could be more detailed, they are sufficiently clear. The discussion on potential future speed-ups and memory savings for sparse recurrent neural networks is intriguing, though not specific to the proposed pruning method. The paper does not adequately justify certain methodological details, such as the need for the threshold to ramp up after a specific period. If this decision is based on preliminary findings, the paper should explicitly mention it.
Sparse neural networks have been extensively studied, including in the context of recurrent neural networks (e.g., sparse recurrent weight matrices were a standard feature of echo-state networks [1]). The proposed method closely resembles the work by Han et al. [2], which involves pruning weights post-training and retraining the remaining weights. While replacing this three-stage process with a single training phase is conceptually appealing, the proposed scheme still involves multiple regimes: initial training without pruning, followed by pruning at two different rates, and a final phase of training without further pruning. The primary novelty lies in applying such a scheme to RNNs, which are generally more challenging to train than feedforward networks.
Enhancing scalability is a key driver of progress in neural network research. While the paper does not offer significant novelty in terms of ideas or scientific insights, it effectively demonstrates that weight pruning can be applied to large-scale RNN systems without substantial performance degradation. The fact that this can be achieved using a simple heuristic is a noteworthy result.
---
Pros:
- The method effectively reduces the number of parameters in RNNs without significantly compromising performance.
- The experiments are conducted on a state-of-the-art system for a practical application.
Cons:
- The method shows strong similarities to prior work and lacks substantial novelty.
- There is no comparison with alternative pruning methods.
- The use of private data limits the reproducibility of the results.
---
References:
[1] Jaeger, H. (2001). The "echo state" approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.
[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.