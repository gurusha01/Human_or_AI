This paper introduces a straightforward approach of incorporating gradient noise to enhance the training process of deep neural networks. The work initially appeared on arXiv over a year ago, and since then, significant advancements have been made in the field of improving deep neural network training (e.g., batch normalization for RNNs, layer normalization, normalization propagation). However, the paper neither acknowledges nor compares its method to these more recent developments.
Specifically, the authors claim that "recent work on applying batch normalization to recurrent networks (Laurent et al., 2015) has not shown promise in improving generalization ability for recurrent architectures, which are the focus of this work." This assertion is factually incorrect, as studies such as Cooijmans et al. (2016) have demonstrated that batch normalization is indeed effective for RNNs.
The proposed technique is highly simplistic and bears resemblance to numerous training strategies previously discussed in the literature. Consequently, its contribution appears incremental at best. That said, the method could have been impactful if supported by robust empirical evidence demonstrating the superiority of this specific approach. Unfortunately, as noted earlier, the paper lacks comparisons with several existing training strategies and algorithms.
Regrettably, the work has become considerably outdated. It would not be suitable for publication at ICLR 2017.