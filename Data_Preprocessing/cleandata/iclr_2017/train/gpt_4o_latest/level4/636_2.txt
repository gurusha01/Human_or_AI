The primary contribution of this paper is the observation that dropout increases the variance of neurons, and addressing this variance—both in parameter initialization and in the test-time statistics of batch normalization—leads to improved performance. This claim is supported by experimental results that are reasonably convincing.
This finding is significant because it has broad applicability to many models commonly used in the literature. However, it is not highly novel, as prior work has noted that the simplified dropout approximations used at test time often fall short of the accuracy achieved by full Monte Carlo dropout.
The paper would benefit from additional experimental validation. In particular:
- It seems likely that the proposed correction for dropout-induced variance at test time is not limited to batch normalization. For instance, standard dropout in networks without batch normalization typically adjusts only for the mean at test time (by scaling activations by one minus the dropout probability). This work implies that accounting for the variance as well could be advantageous. Has this been empirically tested?
- How does the proposed variance correction compare to the performance of Monte Carlo dropout at test time, where predictions are averaged over a large number of random dropout masks?