This paper presents the Quasi-Recurrent Neural Network (QRNN), which significantly reduces the computational complexity associated with temporal transitions in sequence data. In essence (and with slight imprecision), the model builds upon the LSTM architecture but simplifies it by retaining only the diagonal elements in the transition matrices. Additionally, it extends the connections from lower layers to upper layers by employing general convolutions over time (where the standard LSTM can be interpreted as a convolution with a temporal receptive field of one time-step).
As noted by the authors, the QRNN shares similarities with several recent RNN variants, particularly ByteNet and strongly-typed RNNs (T-RNN). Given these related models, the novelty of the QRNN is somewhat reduced. Nonetheless, I believe the work introduces enough originality to merit publication.
The authors provide a fairly robust set of empirical results that substantiate the claims made in the paper. The findings suggest that this specific modification of the LSTM is worth further exploration by the research community.
Although I view the contribution as somewhat incremental, I recommend acceptance.