Although the number of trainable parameters is significantly reduced, the training and recognition performance cannot be improved in the same manner.  
As demonstrated by the results, the authors were unable to achieve better performance with fewer parameters. However, the proposed structure, even with an increased number of parameters, exhibits notable improvements, particularly in areas such as LM.  
The paper would benefit from reorganization and conciseness, as it is occasionally hard to follow and contains inconsistencies. For instance, the feedforward network's weights depend solely on an embedding vector (see my earlier comments on linear bottlenecks), whereas in the recurrent network, the generated weights are influenced by the input observation or its hidden representation.  
Could the authors include the number of trainable parameters for Table 6?  
Reducing the number of presented results might also enhance the paper's readability.  
Due to the writing style, I can only recommend marginal acceptance.