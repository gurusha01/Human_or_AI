This paper presents an elegant approach to addressing a significant challenge in VAEs, specifically the tendency of the model to over-regularize by deactivating latent dimensions. While techniques such as KL term annealing and "free bits" have been employed as workarounds, a more robust solution is warranted.  
The proposed method introduces sparsity in the latent representation: for each input, only a limited number of latent distributions are activated, while across the dataset, a diverse set of latents can still be learned.  
However, I am unclear about the necessity of the topology in this latent representation. Why not impose a prior over arbitrary subsets of latents instead? This approach seems to significantly enhance representational capacity without undermining the solution to the core problem. By introducing a topology, the number of ways the latents can combine is no longer exponentially large, which appears to be a missed opportunity.  
Additionally, the first paragraph on p.7 is unclear: "An effect of this â€¦samples." How does under-utilization of model capacity result in overfitting? This statement requires further clarification.  
The experimental results, while modest, are adequate.  
Overall, this paper presents an intriguing idea that addresses a fundamental issue in VAEs and is deserving of acceptance at this conference.