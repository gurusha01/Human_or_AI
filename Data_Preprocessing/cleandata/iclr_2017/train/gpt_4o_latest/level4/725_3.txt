Update: Since the authors have not provided a revised version of the paper, I am lowering my rating to "marginally below acceptance."
---
This paper explores the challenge of training stochastic feedforward neural networks (SFNNs). It introduces a method to transfer weights from a pretrained deterministic deep neural network (DNN) to a stochastic network with the same architecture. The initial transfer mechanism involves rescaling unit inputs and layer weights, along with appropriately defining stochastic latent units when the pretrained DNN employs ReLU nonlinearities. Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution demonstrate that this transfer approach works well when the pretrained DNN uses sigmoid nonlinearities but fails when ReLUs are used. To address this limitation, the paper proposes the "simplified stochastic feedforward neural network" (simplified SFNN), where each stochastic layer is followed by a layer that computes an expectation over samples from its input, thereby restricting the propagation of stochasticity. A modified weight transfer process for the simplified SFNN is introduced and justified.
The training process is divided into three stages: (1) pretraining a DNN, (2) transferring weights to a simplified SFNN and continuing training, and (3) optionally transferring weights to a full SFNN or a deterministic model (DNN) for further training. The third step is optional, as the simplified SFNN can also serve as an inference model. Experimental results on MNIST classification indicate that the simplified SFNN training can enhance the performance of a deterministic DNN model compared to a baseline DNN trained with batch normalization and dropout. Additional experiments on generative tasks (MNIST-half and the Toronto Faces Database) show that the proposed pretraining process improves test set negative log-likelihoods. Finally, experiments on CIFAR-10, CIFAR-100, and SVHN using LeNet-5, network-in-network, and wide residual network architectures demonstrate that incorporating a stochastic training step can improve the performance of a deterministic (DNN*) model.
The term "multi-modal" is somewhat ambiguous in this context, as it could also refer to tasks involving multiple sensory modalities (e.g., audio-visual speech recognition or image captioning). To avoid confusion, I recommend using the more precise term "generative tasks with a multimodal target distribution" early in the introduction and clarifying that these will be referred to as "multi-modal tasks" for brevity throughout the paper.
The paper would be more readable if "SFNN" were not used interchangeably for both singular ("stochastic feedforward neural network") and plural ("stochastic feedforward neural networks") cases. When referring to the plural, use "SFNNs."
In Table 1, why does the 3-hidden-layer SFNN initialized from a ReLU DNN have a significantly worse test NLL compared to the 2-hidden-layer SFNN initialized from a ReLU DNN?
The notation using superscripts for layer indices is confusing, as readers might interpret N² as "N squared" rather than "the number of units in the second layer."
When transferring weights back from the simplified SFNN to the DNN* model, does the process require rescaling to reverse the operations described in Equation (8)?
What does NCSFNN stand for in the supplementary material?
---
Pros:
+ The proposed model is straightforward to implement and can be applied to various tasks.
+ The MNIST results showing that stochastic training can produce a deterministic model (DNN*) with better generalization than a DNN trained with batch normalization and dropout are particularly compelling.
Cons:
- As noted above, the paper is occasionally difficult to follow due to unclear terminology and notation.
- The results on CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines included dropout and batch normalization. While this is demonstrated on MNIST, showing similar results on more challenging datasets would strengthen the paper.
---
Minor Issues:
1. "It has been believed that stochastic" → "It is believed that stochastic"
2. "underlying these successes is on the efficient training methods" → "underlying these successes is efficient training methods"
3. "necessary in order to model complex stochastic natures in many real-world tasks" → "necessary to model the complex stochastic nature of many real-world tasks"
4. "structured prediction, image generation and memory networks" → "structured prediction, image generation, and memory networks" (memory networks are models, not tasks)
5. "Furthermore, it has been believed that SFNN" → "Furthermore, it is believed that SFNN"
6. "using backpropagation under the variational techniques and the reparameterization tricks" → "using backpropagation with variational techniques and reparameterization tricks"
7. "There have been several efforts developing efficient training methods" → "There have been several efforts toward developing efficient training methods"
8. "However, training SFNN is still significantly slower than doing DNN" → "However, training a SFNN is still significantly slower than training a DNN"
9. "e.g., most prior works on this line have considered a" → "consequently, most prior works in this area have considered a"
10. "Instead of training SFNN directly" → "Instead of training a SFNN directly"
11. "whether pre-trained parameters of DNN" → "whether pre-trained parameters from a DNN"
12. "with further fine-tuning of light cost" → "with further low-cost fine-tuning"
13. "recent advances in DNN on its design and training" → "recent advances in DNN design and training"
14. "it is rather believed that transferring parameters" → "it is believed that transferring parameters"
15. "but the opposite direction is unlikely possible" → "but the opposite is unlikely"
16. "To address the issues, we propose" → "To address these issues, we propose"
17. "which intermediates between SFNN and DNN," → "which is intermediate between SFNN and DNN,"
18. "in forward pass and computing gradients in backward pass" → "in the forward pass and computing gradients in the backward pass"
19. "in order to handle the issue in forward pass" → "in order to handle the issue in the forward pass"
20. "Neal (1990) proposed a Gibbs sampling" → "Neal (1990) proposed Gibbs sampling"
21. "for making DNN and SFNN are equivalent" → "for making the DNN and SFNN equivalent"
22. "in the case when DNN uses the unbounded ReLU" → "in the case when the DNN uses the unbounded ReLU"
23. "are of ReLU-DNN type due to the gradient vanishing problem" → "are of the ReLU-DNN type because they mitigate the gradient vanishing problem"
24. "multiple modes in outupt space y" → "multiple modes in output space y"
25. "The only first hidden layer of DNN" → "Only the first hidden layer of the DNN"
26. "is replaced by stochastic one," → "is replaced by a stochastic layer,"
27. "the former significantly outperforms for the latter for the" → "the former significantly outperforms the latter for the"
28. "simple parameter transformations from DNN to SFNN are not clear to work in general," → "simple parameter transformations from DNN to SFNN do not clearly work in general,"
29. "is a special form of stochastic neural networks" → "is a special form of stochastic neural network"
30. "As like (3), the first layer is" → "As in (3), the first layer is"
31. "This connection naturally leads an efficient training procedure" → "This connection naturally leads to an efficient training procedure"
In Table 1, do the 4-layer SFNNs have one or two layers of stochastic units? What about the 3-layer networks? Is it possible that the expectation is taken in the output layer?
Finally, the paper's citations currently include authors' first initials (e.g., "Hinton, G. et al., 2012a") instead of adhering to the ICLR 2017 template style, which omits first initials (e.g., "Hinton et al., 2012a"). This formatting is distracting and should be corrected.