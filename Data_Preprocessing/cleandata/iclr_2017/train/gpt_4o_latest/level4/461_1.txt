This paper introduces a model for semi-supervised learning that promotes feature invariance to stochastic perturbations applied to the network and/or inputs. Two distinct approaches are proposed: the first applies an invariance term between different instantiations of the model or input within a single training step, while the second enforces invariance across training steps by leveraging a cumulative exponential averaging of features for the same input. The models are evaluated on CIFAR-10 and SVHN datasets, demonstrating comparable performance improvements in both cases. Additionally, the paper explores an application to noisy labels, showing some robustness to label corruption.
The authors also reference recent work by Sajjadi et al., which aligns closely with the ideas presented here. This comparison strengthens the credibility of the findings.
My primary critique is the lack of experiments on larger datasets. While CIFAR-10 and SVHN are sufficient for proof-of-concept, they are relatively small-scale benchmarks. For semi-supervised learning scenarios, it would have been valuable to test the approach on datasets with over 1 million samples, where only 1,000â€“10,000 are labeled, as this setup is more representative of real-world cases with limited labels.
Similarly, the data augmentations used in the experiments are limited to translations and (for CIFAR) horizontal flips. Although these are "standard" augmentations, as noted in the paper, it would have been interesting to explore a broader range of augmentations, particularly given that the model is designed to benefit from random sampling. The paper briefly mentions handling horizontal flips differently across the two model variants, but more details and experiments with diverse augmentations could provide deeper insights. Expanding the augmentation set and testing the model's performance with fewer labeled samples would help assess its robustness and scalability more comprehensively.
Overall, the proposed approach is straightforward and achieves promising results. However, additional experiments on larger datasets and with more varied augmentations would provide a clearer understanding of its performance characteristics.
Minor comment: The paper references "dark knowledge" in a few places, such as at the bottom of page 6, to explain results. While this is acceptable as a motivational concept, the analysis could benefit from a more concrete explanation. For example, the consistency term enforces feature invariance to stochastic perturbations more explicitly than a classification loss alone, which might offer a clearer interpretation of the observed results.