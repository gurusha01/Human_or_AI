Paper Summary: This paper introduces ENet, a novel convolutional network architecture for semantic segmentation that achieves comparable performance to the existing SegNet while being approximately 10 times faster and requiring about 10 times less memory.
Review Summary: While the results are intriguing, the paper lacks comprehensive experimental validation and is of limited relevance to the ICLR audience.
Pros:
- 10x faster
- 10x smaller
- Detailed explanation of the design rationale
Cons:
- The baseline quality is subpar. For example, the Cityscapes results achieve 58.3 IoU, whereas the state of the art is around 80 IoU, making the results less compelling.
- Experimental evidence supporting the design rationale is missing. Each claim should be substantiated with empirical results.
Quality: The work is promising but feels incomplete. If the model is 10 times faster and smaller, why not explore extending it to be 10 times deeper to improve performance? The focus on efficiency comes at the expense of quality, as evidenced by the use of a weak baseline. This limits its appeal to the ICLR audience.
Clarity: The text is generally clear, but the model description in Section 3 could be improved for better understanding.
Originality: The work compiles "practitioners' wisdom" and applies it to a specific task, which limits its originality.
Significance: While the idea of consolidating "best practices" into a single framework is interesting, the work must excel in all aspects to make a significant impact. Prioritizing speed at the expense of quality diminishes its potential influence.
Minor Comments:
- The text is written in proper English but often contains awkward sentence constructions. Specific examples are provided below.
- To enhance the paper's chances of acceptance, the authors are encouraged to explore larger models and demonstrate that the same "collected wisdom" can achieve both high speed and high quality, presenting a trade-off curve. Focusing solely on one end of the quality-speed spectrum overly restricts the paper's scope.
- Section 1: "mobile or battery powered … require rates > 10 fps." What energy budget is assumed for this? Should it be "> 10 fps && < X Watt"?
- Replace "rules and ideas" with "guidelines," as "rules" seems overly rigid.
- "Is of utmost importance" could be simplified to "is of importance," as "important" is already sufficiently strong.
- The sentence "Presents a trainable network … therefore we compare to … the large majority of inference the same way" is unclear and lacks logical coherence.
- "Scen-parsing" should be corrected to "scene-parsing."
- It is debatable whether encoder and decoder can be described as "separate."
- Clarify or remove "Unlike in Noh," as its relevance is unclear.
- "Real-time" is vague; specify X fps @ Y W.
- Replace "Other existing architectures" with "Other architectures."
- Section 3: Does the BN layer include a bias term? Can good results be achieved without any bias term?
- Table 1: Why does the initial layer perform downsampling, given that the results are half the size of the input?
- Section 4: What is meant by "settle to recurring pattern" in the context of non-linear operations?
- Section 4: "Computationally expensive" should be clarified—relative to what?
- Section 4: The claim "This technique ... speeds-up ten times" lacks experimental validation. Substituting one method for another without evidence does not inherently make the latter superior.
- Section 4: "Found one problem" should be rephrased to "issue" or "mismatch," as "problem" implies a conceptual flaw.
- Section 4: Regarding factorizing filters, why are nx1 filters described as asymmetric? A filter could be 1xn and still symmetric (e.g., -2 -1 0 1 2). Consider using "rectangular filters" instead.
- Section 4: The claim that this change increases variety is counterintuitive; the opposite might be expected.
- Section 4: Define "much better" in the context of regularization.
- Section 5.1: "640x360 is adequate for practical applications" should specify that this applies to some applications.
- Section 5.2: "Very quickly" is vague; provide quantitative details.
- Section 5.2: Correct "Haver" to "have."
- Section 5.2: Replace "in this work" with "In this work."
- Section 5.2: Clarify the purpose of class weighting. Is it for class balancing?
- Section 5.2: Replace "Cityscapes was" with "Cityscapes is."
- Section 5.2: "Weighted by the average" should clarify whether each instance is weighted relative to the average object size.
- Section 5.2: Specify that this is the "fastest model in the public Cityscapes."