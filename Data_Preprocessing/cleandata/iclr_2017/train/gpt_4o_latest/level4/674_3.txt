Summary of the paper  
The paper investigates the invertibility of convolutional neural networks within the random model framework. It introduces a reconstruction algorithm, akin to Iterative Hard Thresholding (IHT), for performing layer-wise inversion of the network.
Clarity:  
- The paper is unclear with respect to standard notations commonly used in deep learning.
Comments:  
The paper incorporates two key simplifications in its analysis of CNNs, which allow it to align with a model-based compressive sensing framework:  
1. The non-linearity (ReLU) is omitted. This is a significant simplification. For instance, with random Gaussian weights, the Johnson-Lindenstrauss (JL) lemma ensures the preservation of L2 distances. However, when ReLU is applied, the underlying metric changes (see, for example, the kernel for n=1 in ...).