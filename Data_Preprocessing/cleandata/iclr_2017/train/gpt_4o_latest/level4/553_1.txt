For over a decade, near-data processing has been a critical requirement for large-scale linear learning platforms, as the data loading time often surpasses the learning time. This necessity has driven the adoption of frameworks like Spark.
In contrast, deep learning typically operates on datasets that fit within a single machine, where the primary bottleneck lies in the CPU-GPU or GPU-GPU communication. Thus, a method addressing this bottleneck could hold significant relevance.
However, this work remains in an early stage and is restricted to linear training algorithms, making it of limited interest to the ICLR audience at this time. I would suggest submitting it to a venue more aligned with large-scale linear ML research, such as ICML. The paper is well-written and clear in its current form but would benefit from a robust benchmark on a large-scale linear task. Naturally, once the authors present compelling simulations for DNN learning, they could consider resubmitting to ICLR. That said, it remains unclear whether the flash memory FPGA can adequately handle such scenarios.
Regarding the experiments, the choice of MNIST is puzzling: the dataset is small, and linear approaches are known to perform poorly on this task (a fact the authors do not even report).