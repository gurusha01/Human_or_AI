This paper introduces an extension of the multiplicative RNN [1], where the authors apply a similar reparametrization trick to the weight matrices of the LSTM.
The paper presents some intriguing techniques, though none of them appear to be particularly critical. For example, in Eq. (16), the authors suggest incorporating the output gate within the activation function to mitigate the saturation issue in logistic sigmoid or hyperbolic tangent. Additionally, the authors share \(m_t\) across the inference of different gating units and cell-state candidates, which results in only a 1.25x increase in the number of model parameters. Furthermore, the authors employ a modified version of RMSProp, introducing an additional hyperparameter \(\ell\) and scheduling it over the training process. It would have been more insightful to apply these techniques to other baseline models and demonstrate the individual impact of each trick.
Despite the architectural modifications to the LSTM and the combination of all proposed techniques, the overall performance falls short of expectations. Why didn't the authors explore the use of batch normalization, layer normalization, or zoneout in their models? Were there specific challenges or limitations that prevented the application of these regularization or optimization methods?
In the fourth paragraph of Section 4.4, the connection drawn between dynamic evaluation and fast weights is somewhat misleading. It is difficult to interpret dynamic evaluation as a variant of fast weights. Fast weights do not rely on test error signals. The paper states that "dynamic evaluation uses the error signal and gradients to update the weights, which potentially increases its effectiveness, but also limits its scope to conditional generative modeling, when the outputs can be observed after they are predicted." Unfortunately, this assumption is problematic. Test label information should not be assumed to be available during inference. Test labels are meant to assess the model's generalization performance. While there are scenarios, such as stock prediction or weather forecasting, where test labels may be accessible at inference time, this is not the case for many other applications. For instance, in machine translation, the optimal translation is not known in advance, unlike weather forecasting. Moreover, it would have been more equitable to apply dynamic evaluation to all baseline models for comparison, particularly with respect to the BPC score of 1.19 achieved by the proposed mLSTM.
While the quality of the work is reasonable, the novelty of the contributions is somewhat limited. The proposed model often underperforms compared to existing methods and only shows improvement when combined with dynamic evaluation. However, it is worth noting that dynamic evaluation could similarly enhance other methods as well.
[1] Ilya et al., "Generating Text with Recurrent Neural Networks," ICML'11