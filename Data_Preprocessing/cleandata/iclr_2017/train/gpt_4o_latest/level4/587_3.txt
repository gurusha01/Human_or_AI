This paper explores the concept of merging multiple layers (commonly a convolution layer combined with an LRN or pooling layer) into a single convolution layer by retraining only that layer. The authors demonstrate that this approach can yield simpler and faster models with minimal accuracy degradation. The idea itself is sound. However, there are several concerns:
- The paper introduces the term 'Deeprebirth layer,' which initially gives the impression of a novel architectural component. However, it becomes apparent later in the paper that 1) it is essentially just a convolution layer, and 2) its specific implementation varies depending on whether serial or parallel pooling layers are being fused. While it is understandable to want to name the technique, labeling the layer itself—when it represents multiple, non-novel architectural variants—creates unnecessary confusion in the narrative.
- There are established methods for operator fusion that do not require retraining, and some deep learning frameworks, such as Theano and the upcoming TensorFlow XLA, already implement such techniques. Including a baseline comparison with these methods would have been valuable, especially since much of the energy savings from operator fusion stems from eliminating intermediate memory writes.
- Batch normalization (BatchNorm) can be integrated into convolution layers without retraining by appropriately scaling the weights. Were BatchNorm layers folded into the baseline results reported in Table 7? Clarifying this is important for fair comparisons.
- At the time of this review, the authors have not provided sufficient details to ensure reproducibility. Specifically, the relationship between the depth of the fused layers and the depth of the original layers in each experiment remains unclear.
- Regarding retraining: How much time (in terms of epochs) does the retraining process require? Additionally, did the authors consider leveraging techniques such as knowledge distillation to improve the retraining process?
The experiments presented are interesting, but the paper requires significant revisions to be suitable for publication.
- Open-sourcing the implementation would greatly enhance the utility of this work. While not mandatory, making the code publicly available is always a strong positive for papers of this nature.