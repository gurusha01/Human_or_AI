This paper introduces a novel approach to augment pre-trained networks for a specific task by incorporating an additional inference path tailored to a secondary task, serving as an alternative to the conventional "fine-tuning" method.
Pros:
- The proposed method is straightforward and well-articulated.
- Fine-tuning is a widely adopted technique, so enhancements to and analyses of it are likely to have broad appeal.
- The experimental evaluation spans multiple domains, including both vision and NLP.
Cons:
- The inclusion of additional modules significantly increases resource demands, doubling the parameter count and tripling the computational cost for the "stitched" network. These overheads are not adequately addressed in the paper, which raises concerns about the method's practicality in real-world scenarios where efficiency is often critical.
- Given the substantial resource costs, the central idea of the paper lacks sufficient validation. To establish that the observed performance gains stem from the unique aspects of the proposed method rather than simply the increased network capacity, additional baselines are necessary:
  1. Allow the original network's weights to be trainable for the target task alongside the additional module. Demonstrating superior performance over this baseline would confirm that freezing the original weights introduces a meaningful form of regularization.
  2. Train the full module/stitched network from scratch on the source task, followed by fine-tuning for the target task. Outperforming this baseline would validate the utility of maintaining a set of weights that remain uninfluenced by the source dataset.
- The method has not been evaluated on ImageNet, which is the most widely used benchmark for pre-trained networks and fine-tuning in computer vision. Networks pre-trained on CIFAR are rarely deployed in practice, and improved performance on CIFAR often does not translate to ImageNet. While results on smaller datasets can be acceptable for more theoretical contributions, fine-tuning is a practical domain, and demonstrating improvements in this context should necessitate an ImageNet evaluation.
Overall, the proposed idea is intriguing and holds potential, but the current evaluation does not convincingly demonstrate that the performance improvements are attributable to the method itself rather than the increased network capacity. Additionally, the absence of an ImageNet evaluation raises questions about its practical applicability.
---
Edit (1/23/17): I appreciate the clarification regarding the Stanford Cars experiment, which indeed involves transfer learning from ImageNet. However, this experiment primarily demonstrates late fusion ensembling, a conventional approach, rather than the novel "stitched network" concept central to the paper. Moreover, the results in this case are underwhelming, showing only that an ensemble of ResNet+VGG outperforms VGG alone, which is expected given that ResNet is a stronger base network than VGG. A more compelling result would be "ResNet+VGG > ResNet," though even that would not be surprising. Demonstrating the stitched network approach on ImageNet and comparing it against fine-tuning with VGG-only or ResNet-only networks could strengthen the paper significantly. However, in its current form, the experiments do not sufficiently validate the stitched network concept, in my view.