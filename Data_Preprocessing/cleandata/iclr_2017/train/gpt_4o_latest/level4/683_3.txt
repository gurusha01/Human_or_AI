The paper under review introduces a methodology for incrementally expanding a residual network by adding layers based on a boosting criterion.
The primary obstacle to publication lies in the limited empirical validation. The tasks evaluated are relatively small-scale for 2016 (with MNIST and a convolutional network being largely unremarkable as a test case at this stage). The paper does not adequately compare its results to existing literature, and the CIFAR-10 performance fails to surpass straightforward, single-network baselines from prior work (e.g., Springenberg et al., 2015 achieves 92% without data augmentation). Additionally, it is likely that a simple ResNet implementation exists that outperforms these results. While the CIFAR-100 results are somewhat more compelling, as they exceed what I typically encounter (though I have not conducted a recent survey of the literature), this is not unexpected. Ensembles generally perform well in scenarios with limited labeled training data, such as CIFAR-100, where only a few hundred samples per label are available. However, it is standard practice on both CIFAR-10 and CIFAR-100 to apply basic data augmentation techniques, which are absent here. These and other regularization methods represent simpler alternatives to the proposed iterative augmentation approach.
The method could be more effectively positioned as a solution for scenarios involving limited labeled datasets where conventional data augmentation is challenging. However, this would require the use of different benchmarks and comparisons against simpler techniques, such as standard data augmentation, dropout (particularly given its connection to ensemble methods), and similar approaches.