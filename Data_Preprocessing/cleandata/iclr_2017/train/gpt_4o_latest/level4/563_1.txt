In this paper, the authors propose an extension of the f-GAN framework by incorporating Bregman divergences for density ratio matching. The authors argue that a key limitation of f-GANs (a generalization of standard GANs) lies in the discrepancy between the theoretically motivated objective and the actual objective optimized by the generator during training, which arises due to gradient-related issues. In the proposed b-GAN framework, the discriminator functions as a density ratio estimator (r(x) = p(x) / q(x)), while the generator seeks to minimize the f-divergence between p and q by expressing p(x) as r(x)q(x).
My primary concern with this paper is that it is unclear what practical value this approach offers. While the connection to density estimation is intriguing, the conclusions drawn from this connection appear questionable. For instance, prior work in density estimation has shown that the Pearson divergence tends to be more stable. The authors assert that this observation also applies to GANs and attempt to demonstrate this in their experiments. However, the experimental section is poorly presented, with figures that fail to provide meaningful insights. Examining the graph of density ratios, for example, does not offer much clarity. The authors claim that for the Pearson divergence and a modified KL-divergence, "the learning did not stop," based solely on the graph of density ratios. This assertion is vague and unsupported by concrete evidence. Additionally, it is unclear why the standard GAN objective was not tested in light of this analysis. Moreover, while the authors criticize standard GANs for relying on heuristic objectives for the generator, they themselves employ multiple heuristic objectives and techniques to make b-GAN functional.
This paper would benefit greatly from a thorough rewrite to improve clarity and coherence. As it stands, the motivation and intuition behind the proposed approach are difficult to grasp.