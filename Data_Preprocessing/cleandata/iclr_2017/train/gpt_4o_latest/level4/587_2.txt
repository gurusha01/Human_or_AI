This paper introduces a method to reduce the size and evaluation time of deep CNN models on mobile devices by merging multiple layers into a single layer, followed by retraining the modified model. The results demonstrate that computation time can be decreased by a factor of 3 to 5, with only a 0.4% accuracy drop on a specific model.
Reducing model size and improving evaluation speed are critical for many applications. However, I have several concerns:
1. Numerous techniques exist for reducing model sizes. For instance, prior work has shown that the teacher-student approach can produce smaller models that achieve comparable or even superior accuracy to the larger teacher model. This paper does not provide a comparison with any of these established methods.
2. The proposed technique has limited generalizability, as it is specifically tailored to the models discussed in the paper.
3. The idea of replacing multiple layers with a single layer is relatively standard. For example, layers such as mean-variance normalization and batch normalization can often be absorbed without retraining or any loss in accuracy.
Additionally, it is worth noting that the DNN low-rank approximation technique was originally introduced in the context of speech recognition. For example:
Xue, J., Li, J., and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).