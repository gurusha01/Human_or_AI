This paper introduces a proximal (quasi-) Newton's method for training binary deep neural networks (DNNs). The primary contribution lies in integrating pre-conditioning with binarization within a proximal framework. The idea of employing a proximal Newton's method to interpret various DNN binarization schemes is intriguing, as it provides a novel perspective on existing approaches. However, the theoretical analysis presented in the paper is neither particularly convincing nor practically useful. The optimization problem defined in equations (3)-(4) essentially represents a mixed-integer programming problem. While the paper addresses the integer component as a constraint and incorporates it into proximal operators, the constraint set remains discrete, and there is no assurance that the proximal Newton algorithm will converge under conditions that are practically relevant. Moreover, verifying the assumption [dt^t]k > \beta in Theorem 3.1 is challenging in practice. This condition may be difficult to satisfy in DNNs due to the highly complex nature of the loss surface.