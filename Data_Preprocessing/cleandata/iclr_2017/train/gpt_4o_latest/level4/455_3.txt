This paper introduces an innovative extension to generative adversarial networks (GANs), where the conventional binary classifier discriminator is replaced with a discriminator that assigns a scalar energy value to each point in the generator's output space. The discriminator minimizes a hinge loss, while the generator aims to produce samples with low energy as determined by the discriminator. The authors demonstrate that under these conditions, a Nash equilibrium leads to a generator that aligns with the data distribution, assuming infinite capacity. The experimental setup includes a discriminator implemented as an autoencoder, with an optional regularizer that penalizes generated samples exhibiting high cosine similarity to others within the minibatch.
Pros:
- The paper is clearly written and easy to follow.
- The topic is likely to attract significant interest, as it paves the way for exploring a broader range of discriminator designs for training GANs.
- The theorems regarding the Nash equilibrium's optimality appear to be sound.
- The MNIST experiments include a comprehensive hyperparameter analysis.
- The semi-supervised results demonstrate that contrastive samples from the generator enhance classification performance.
Cons:
- The paper does not adequately clarify its relationship to prior works that expand the discriminator's scope (e.g., [1]) or employ a generative network to create contrastive samples for energy-based models ([2]).
- Visual inspection alone does not provide sufficient evidence to conclude that EB-GANs outperform DC-GANs in generating samples for the LSUN and CelebA datasets.
- The impact of the PT regularizer is challenging to evaluate beyond visual inspection, as the Inception score results are computed using the standard EB-GAN.
Specific Comments:
- Sec 2.3: The claim that a reconstruction loss will necessarily yield significantly different gradient directions is unclear.
- Sec 2.4: The abbreviation "PT" for "pulling-away" is somewhat confusing.
- Sec 4.1: Using the Inception model (trained on natural images) to compute KL scores for MNIST seems odd. A CNN trained on MNIST might be more appropriate for Inception-style scoring in this context.
- Figure 3: The histograms show minimal variation, making this figure less informative.
- Appendix A: In the proof of Theorem 2, it is unclear why the existence of a Nash equilibrium for the system is guaranteed.
Typos / Minor Comments:
- Abstract: The term "probabilistic GANs" might be better replaced with "traditional" or "classical" GANs.
- Theorem 2: Should read "A Nash equilibrium ... exists."
- Sec 3: The phrase should be "Several papers were presented."
Overall: While I have some reservations regarding the related work and experimental evaluation sections, the proposed model is sufficiently novel and supported by strong theoretical justifications, including optimality proofs and the quality of generated samples.
[1] Springenberg, Jost Tobias. "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks." arXiv preprint arXiv:1511.06390 (2015).  
[2] Kim, Taesup, and Yoshua Bengio. "Deep Directed Generative Models with Energy-Based Probability Estimation." arXiv preprint arXiv:1606.03439 (2016).