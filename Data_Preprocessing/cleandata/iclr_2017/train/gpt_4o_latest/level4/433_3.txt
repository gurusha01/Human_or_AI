Building upon prior work on the NICE model, this paper introduces a method for constructing deep feed-forward generative models. The proposed model is evaluated on multiple datasets. While it does not achieve state-of-the-art results, it contributes to the development of an intriguing class of generative models. The paper is generally well-written and clear.
Since both inference and generation are efficient and exact—highlighted as a key advantage over other models—it would be beneficial for the authors to provide motivating examples of applications where these properties are particularly valuable or necessary.
The authors assert that "unlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space." However, where is the evidence supporting this claim? I did not find any analysis in the paper demonstrating the semantic meaningfulness of the latent space learned by real NVP. Providing stronger evidence that the learned representations are practically useful for downstream tasks would significantly strengthen the paper.
Additionally, the authors' discussion around the "fixed reconstruction cost of L2" remains vague. The factorial Gaussian assumption does not inherently constrain the generative model; rather, it smooths an otherwise arbitrary distribution, and the degree of smoothing can be made arbitrarily small, as expressed by \( p(x) = \int p(z) N(x | f(z), \sigma^2) dz \). The connection between this and the loose lower bound is unclear from the current presentation in the paper.