The central argument of this paper is that given the specific architectural features of multi-GPU systems—such as the use of bi-directional PCI-E for communication and the inclusion of two independent DMA engines on modern GPUs (enabling simultaneous independent communications)—and the communication patterns inherent to synchronous SGD training for deep neural networks—characterized by large, dense, and fixed-length messages—it is logical to design communication collectives like broadcast, reduce, and allreduce tailored specifically for this use case. The paper presents the implementation of these three collectives (broadcast, reduce, and allreduce) using a linear pipelining (LP) approach on a (logical) ring topology. The LP collectives are compared against two alternatives: collectives based on a minimal spanning tree (MST) topology and those based on bidirectional exchange (BE). 
First, the paper conducts a theoretical analysis using a standard cost model widely adopted in the high-performance computing community. By incorporating assumptions about multi-GPU system architecture (e.g., very low message latency) and the communication characteristics of synchronous SGD training (e.g., very large messages), the analysis predicts that LP collectives should be approximately twice as efficient as BE collectives and log(p) times more efficient than MST collectives, where p represents the number of GPUs in the system. Second, an empirical evaluation is performed, measuring (1) the time required for each collective on a 4-GPU (k40m) system as a function of message size, and (2) the time required for each collective with a fixed 200 MB message size as a function of the number of GPUs. The results consistently demonstrate that LP-based collectives outperform the alternatives. Third, the paper evaluates DNN training performance using AlexNet and GoogLeNet on a 4-GPU system, employing three different synchronous SGD algorithms with each of the collective implementations (a total of six algorithm-collective combinations). The results show that LP collectives reduce communication overhead without impacting computation, as expected. Furthermore, convergence measurements of training loss over time reveal that LP collectives enable faster training for both DNN architectures.
While the theoretical model suggests that the costs of LP collectives should remain invariant to the number of GPUs in a multi-GPU system, the empirical results indicate that this assumption breaks down when scaling from 4 to 5 GPUs in the tested configuration. This discrepancy arises because, in a 5-GPU system, messages must traverse the QPI. Are there additional practical factors that the authors are aware of which could influence the scalability of LP collectives? If so, these should be discussed in the paper.
The sentence "Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket." is unclear. The phrasing after "but communications of different MPI processes" is particularly difficult to understand and should be revised for clarity.
The statement, "Please note the latency term is log pα, which is the smallest among algorithms in Table.1. Therefore, MST only suits for high frequent short messages," is logically flawed. The claim that MST collectives are only suitable for high-frequency, short messages does not directly follow from the observation that MST collectives have the smallest latency term. The bandwidth term, which determines how the cost scales with message size, must also be considered. If MST collectives had a superior bandwidth term compared to the other collectives, they could also be advantageous for large messages.
The expression "Let's take an appropriate block size b to ensure n/b ≪ α" appears to be incorrect, as n > b. Should the condition instead be expressed as b/n ≪ α?
The statement, "However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update," warrants further scrutiny. Is the observed inconsistency in weight estimates across devices truly due to floating-point multiplication? It seems more plausible that the issue arises from the non-commutative nature of floating-point addition, as gradients may be accumulated in different orders across devices.
Finally, I suggest replacing the term "sub-gradients" with "partial gradients" throughout the paper. In optimization literature, "sub-gradient" has a specific technical meaning that does not align with the way it is used in this paper.