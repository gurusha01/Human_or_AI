I reviewed the manuscript as of December 7th.
Summary:  
The authors examine the transferability of adversarial examples in deep neural networks. They confirm that transferability persists even in large models but highlight the difficulty of adversarially perturbing an image to achieve a specific, desired label. Additionally, the authors demonstrate real-world attacks on a vision web service and investigate the geometric properties of adversarial examples.
Major Comments:  
1. The paper presents a wide array of results, but it is unclear what the central message of the work is. As noted in the comments, the manuscript spans 15 pages, with an additional 9 pages of results in the Appendix that are heavily referenced in the main text. While there is no strict page limit for this conference, the length challenges the spirit of a conference publication. I do not recommend rejection solely based on this, but I view it as a drawback since clarity of presentation is a critical factor. If the paper is accepted, I suggest the authors further reduce its length beyond the 13-page version available elsewhere. I have marked specific sections that could be trimmed.
2. The section on the geometric understanding of adversarial examples appears similar to findings in 'Adversarial Perturbations of Deep Neural Networks' by Warde-Farley and Goodfellow (2015), particularly Figure 1.2. It is unclear what additional insights the authors provide beyond these prior results. If there are novel contributions, the authors should emphasize them more clearly.
3. The authors build on observations by Goodfellow et al. (2014) and Szegedy et al. (2013), showing that large-scale models remain vulnerable to adversarial perturbations (see also Kurakin et al. (2016)). They further demonstrate that adversarial manipulation to convert an image into a specific, desired label is significantly more challenging.
4. The authors successfully target a real-world vision API, which is an interesting result. However, it is unclear how these findings extend beyond the work of Papernot et al. (2016). The authors should clarify the novelty and significance of their contributions in this context.
From my perspective, the most compelling and novel result in this paper is the observation regarding the unique difficulty of adversarially manipulating an image to achieve a specific, desired label. Other findings largely build on prior work, and the authors need to better articulate what makes their results distinct from existing literature.
Areas to Trim the Paper:  
- Table 1 is unnecessary. The authors could simply cite prior results or include the Top-1 numbers in the text.  
- Section 2.2.1 could be condensed with heavier reliance on citations.  
- Panels in Figure 2 could be overlaid to facilitate comparisons and reduce redundancy.  
The manuscript now spans 16â€“24 pages, depending on how sections are categorized as part of the Appendix. While there is no strict page limit for this conference, this exceeds the suggested 8-page limit referenced here: