The authors explore a range of existing RNN architectures alongside two newly proposed ones to gain deeper insights into how effectively these models can encode task-specific information within their parameters and activations.
The experimental setups appear robust. To enable generalized comparisons across different architectures, it is essential to evaluate performance on multiple tasks while controlling for the influence of hyperparameters. This study achieves this by employing a diverse set of tasks with varying complexities, adopting a principled hyperparameter tuning methodology, and conducting a substantial number of tuning iterations—feasible only with the computational resources available to large industrial research groups.
The descriptions of the models and their objectives were clear and easy to follow. However, the explanations of the experiments and the presentation of results were occasionally unclear, even with the additional details provided in the appendix. These issues can likely be resolved through text revisions. For instance, in the memory task, the scaling of inputs (and consequently outputs) is not specified, making it difficult to interpret the squared error scores in Figure 2c. Additionally, the term "unrollings" in Figure 2b is ambiguous—does it refer to a time lag with additional hidden state updates between the input sequence and the output generation? Since the perceptron capacity task is central to the paper, a more precise explanation of how and when predictions are computed would be beneficial. Furthermore, the large number of graphs makes it challenging to identify the most critical results. Some of the more straightforward findings (e.g., Figure 1(b-d) in light of Figure 1a) could be moved to the appendix to allow for more detailed task descriptions in the main text.
While novelty is not the primary focus of this paper, as it primarily evaluates existing architectures, the use of mutual information to derive bits-per-parameter scores for highly non-linear parameterized functions is a novel contribution. The paper also introduces two new architectures that appear to have practical utility. Moreover, it contributes to the underexplored area of leveraging modern computational resources to better understand architectures that were originally designed under more constrained conditions. For these reasons, I believe the paper is sufficiently original.
The paper offers intriguing insights into RNN properties. For instance, while the importance of gated units for maintaining trainability in deep networks has been observed before, it is reaffirmed here in an interesting context. Additionally, the potential utility of vanilla RNNs for simpler tasks, where high capacity per parameter is advantageous due to hardware constraints, is noteworthy. The proposed +RNN architecture may also prove to have practical value, and the findings on hyperparameter robustness shed light on the popularity of certain architectures when limited time for hyperparameter tuning is available. The extensive results and hyperparameter analyses should be valuable to researchers working with RNNs. Overall, I believe this paper would be a strong addition to the ICLR conference, though it would benefit from improvements in text clarity.
Pros:
- Comprehensive analysis.
- Well-designed experiments.
- Novel approach to quantifying capacity in neural networks.
- Results have practical implications and encourage similar analyses for other architectures.
- Provides useful insights into the relative strengths of different RNN architectures.
Cons:
- Key findings are difficult to isolate (some plots seem redundant).
- Some critical experimental details are missing.