This paper presents an attention-based recurrent network designed to compare images by iteratively attending back and forth between a pair of images. The experiments demonstrate state-of-the-art performance on the Omniglot dataset, although a significant portion of the performance improvement appears to stem from using extracted convolutional features as input.
The paper has been notably improved from its initial submission, incorporating changes in response to pre-review questions. However, while efforts were made to include additional qualitative results, such as those in Fig. 2, these remain relatively limited and could be strengthened with more examples and in-depth analysis. Additionally, there is a concern regarding the attention mechanism in Fig. 2, which consistently attends to the full character. While it does zoom in, shouldn't the attention focus on specific relevant parts of the character? Attending to the entire character on a plain background seems like a trivial solution, raising questions about the true source of the substantial performance gains.
Although the paper is now more refined, it still lacks detail in certain areas, such as the specifics of the convolutional feature extractor, which appears to contribute significantly to the observed performance improvement.