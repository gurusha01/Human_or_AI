The subject of the paper, model-based reinforcement learning with a learned model, is both relevant and timely. The manuscript is well-written. However, I find the contributions to be somewhat incremental. Adding an additional head to the frame prediction network for reward prediction is a logical and reasonable extension. That said, neither the approach nor the results stand out as particularly novel or unexpected, especially considering that the original method by [Oh et al., 2015] already demonstrated the ability to increment score counters within predicted frames across various games.
I am eager to see the outcomes of applying the proposed learned joint model of frames and rewards to model-based reinforcement learning, as outlined by the authors.