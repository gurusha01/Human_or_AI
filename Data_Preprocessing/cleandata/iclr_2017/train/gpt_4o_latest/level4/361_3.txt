This paper introduces a novel Bayesian neural network architecture aimed at predicting the progression of learning curves during the training of machine learning models. The work is exploratory in nature, with the long-term objective of integrating this method into a Bayesian optimization framework. However, the current experiments focus solely on evaluating the quality of the predictions. The approach builds on the methodology presented in Domhan, 2015, but extends it by leveraging information from all tested hyperparameter configurations, as opposed to extrapolating from a single learning curve. Additionally, the paper investigates two MCMC-based inference techniques: SGLD and SGHMC. It is unclear, however, whether either of these methods were also employed in Domhan, 2015.
The results appear generally promising, particularly in the early stages of learning curves where limited information is available. In such scenarios, the ability to share knowledge across curves proves beneficial, as expected. One scenario that seems to have been overlooked, but could provide valuable insights, is when some training set curves are nearly or fully observed. This situation might further highlight the advantages of information sharing.
A potential concern with this approach is its computational overhead. The authors report that training the Bayesian network takes approximately 20-60 seconds. In the worst-case scenario, with 100 epochs, this amounts to over 1.5 hours of training time for the Bayesian network, which represents a significant portion of the total time required to train the model being optimized.
The Bayesian network generates multiple separate predictions, as illustrated in Figure 2. It would be insightful to evaluate the accuracy of these individual components. For instance, did the authors constrain the asymptotic value of the learning curve, given that they primarily predicted accuracy? If not, did the predicted values generally fall within the range [0,1]?
Below are some additional minor comments and questions:
- Figure 1: The axes should be labeled as "validation accuracy."
- Figure 6: Could you clarify the definition of LastSeenValue in the bottom-left figure? While it seems intuitive, explicitly describing it would be helpful. Additionally, why is this metric not used elsewhere as a baseline?
- Figure 7 and Table 1: Are you predicting only the final values of the learning curves, or are you predicting every value along the curves, conditioned on prior observations?
- Basis functions: Why did you choose to use only 5 basis functions? Does this choice sufficiently capture the flexibility of the learning curves? Would increasing the number of basis functions improve or degrade performance?