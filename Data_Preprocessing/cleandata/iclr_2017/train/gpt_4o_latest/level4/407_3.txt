The paper addresses significant challenges in multi-task reinforcement learning, specifically avoiding negative transfer and enabling more granular selective transfer. The proposed approach leverages a soft attention mechanism, which is highly general and shown to be applicable to both policy gradient and value iteration frameworks. The inclusion of a base network facilitates the learning of new policies when prior policies are not directly transferable. The use of state-dependent sub-policy selection offers finer control, effectively partitioning the state space among different sub-policies or experts. While the tasks considered are relatively simple, they are adequate to showcase the method's advantages. A notable limitation is that the approach is conceptually straightforward, with the findings and claims primarily supported by empirical evidence. Future work could explore extensions such as option-based frameworks, stochastic hard attention mechanisms, sub-policy pruning, and progressive networks.
In Figure 6, the red curve appears to underperform relative to the others in terms of final outcomes. An alternative or complementary analysis could involve presenting attention mask activation statistics during training. This would allow for the observation of whether the model effectively deactivates adversarial sub-policies and predominantly relies on the newly learned base policy. Such an analysis would also be useful for identifying any unusual co-adaptation behaviors that might arise.