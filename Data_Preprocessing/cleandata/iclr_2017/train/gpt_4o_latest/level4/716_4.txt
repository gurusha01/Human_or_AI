The paper presents a lightweight network for semantic segmentation that integrates multiple acceleration techniques.  
As I noted in my initial query, the authors fail to establish why any of the techniques they employ—such as factorizing filters into alternating 1-D convolutions, utilizing low-rank kernels, or leveraging modern inception network architectures—go beyond what is already known.  
I struggled to identify the core contribution or take-home message of this work. These techniques are well-established and have demonstrated their utility in detection tasks. If a paper is to be accepted simply for applying them to semantic segmentation, it sets a precedent where subsequent papers could argue for acceptance based on applying the same techniques to other tasks, such as normal estimation or saliency estimation, without offering substantial novelty.  
In their preliminary review response, the authors state:  
"I agree that most improvements from classification architectures are straightforward to apply to object segmentation, and that's exactly what we've done - our network is based on current state of the art models. Instead of repeating most of the discussion on factorizing filters, etc., that has been discussed in a lot of papers already, we have decided that it's much more valuable to describe in depth the choices that are related to segmentation only - these are the most important contributions of our paper."  
However, I do not observe any detailed analysis of these segmentation-specific choices. For instance, there is no rigorous discussion of how specific design decisions impact either performance or speed. Instead, the paper offers vague and anecdotal statements such as "these gave a significant accuracy boost," "this helped a lot," "that did not help," or "this turned out to work much better than that." Such commentary lacks depth and reads more like an informal conversation than a thorough technical discussion.  
Even if novelty is not the primary focus, and the emphasis is instead on performance or speed, I remain unconvinced. The authors limit their comparisons to [1,2] (SegNet) in terms of both accuracy and speed, without providing a clear rationale for this choice or adequately justifying it. Based on their evaluation, [1] requires approximately 1 second per frame, whereas Deeplab v2, excluding the DenseCRF, achieves a speed of 5-8 fps.