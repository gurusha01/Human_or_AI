This paper provides a concise argument that optimizing the location and size of receptive fields in a simulated eye capable of making saccades, with respect to classification error on image data where labels depend on variable-size and variable-location subimages, offers an explanation for the existence of a foveal region in, for example, the primate retina.
The argument could be strengthened by employing more realistic image datasets and establishing a clearer correspondence with the number, receptive field sizes, and eccentricities of retinal cells in species such as the macaque. However, this would introduce the challenge of identifying a biologically plausible loss function that also supports the authors' claims.
Additionally, the argument would benefit from a discussion of the timescales involved. It is likely that the density of the foveal center is influenced by the number of saccades permitted during the inference process, the size of the target subimages, and the resulting impact on overall classification accuracy.
Why does the classification error rate for Dataset 2 persist at a high 24%? This seems unusually elevated and suggests that the model may not be functioning as intended. The paper's central argument appears to assume that the model can be trained to perform as an effective classifier. If alternative training strategies or models exist that perform better and operate differently, it raises the question of why evolutionary pressures would not have led our eyes and visual cortex to adopt mechanisms more similar to those models, assuming similar optimization pressures.
Why does the model with zooming capabilities outperform the translation-only model on Dataset 1 (where all target images are uniform in size) but merely tie with it on Dataset 2 (where target images vary in size, a scenario seemingly better suited for the zooming model)? This unexpected tie, combined with the high classification error on Dataset 2, raises concerns that one or both models may not be fully optimized during training, which could weaken the paper's overall claims.
While comparing this model to other attention mechanisms (e.g., spatial transformer networks, DRAW) may not directly align with the paper's primary focus, such comparisons could help address concerns about suboptimal training or potential issues with model parameterization that might be easily resolved.