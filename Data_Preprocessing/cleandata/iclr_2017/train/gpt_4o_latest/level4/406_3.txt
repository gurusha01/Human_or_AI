The paper addresses the challenge of transferring a policy learned in a simulated environment to a real-world target system. The proposed method leverages an ensemble of simulated source domains combined with adversarial training to develop a robust policy capable of generalizing across multiple target domains.
In general, the paper tackles an important and relevant problem, offering a plausible solution. However, the concept of adversarial training employed here appears to differ from its interpretation in recent works (e.g., those involving GANs). Providing additional details on certain components, as highlighted during the question/response phase, would enhance clarity. Additionally, I recommend including results with alternative policy gradient methods, even if their performance is suboptimal (e.g., Reinforce), as well as results comparing the presence and absence of the baseline on the value function. Such comparisons would be highly valuable for the research community.