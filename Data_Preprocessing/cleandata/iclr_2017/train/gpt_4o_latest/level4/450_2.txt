The manuscript examines the problem of 2-sample testing through the lens of classifier evaluation. Under the null hypothesis, the classification accuracy of a classifier trained on two samples from the same distribution exhibits a straightforward distributional form. Consequently, a simple threshold can be established for any classifier. Improving the power of the test then becomes equivalent to training a more effective classifier. This allows researchers to direct their efforts toward advanced models, such as deep neural networks, which are challenging to analyze using traditional statistics like the MMD.
+ The proposed approach is robust and broadly applicable.  
+ The paper is timely, as deep learning has significantly influenced classification and prediction tasks but has yet to make comparable strides in statistical hypothesis testing, where kernel methods currently dominate.
- The discussion surrounding the relationship to kernel-MMD could be more balanced. For instance, kernel-MMD can also be interpreted as a classifier-based approach, and a more equitable comparison would strengthen the paper. Additionally, some aspects of the kernel-MMD comparisons appear inconsistent with the discussion:  
  * The authors employ the linear kernel-MMD, which is less powerful than the quadratic kernel-MMD, though they justify this choice based on computational efficiency.  
  * The kernel-MMD is critiqued for its complex null distribution, yet the linear-time kernel-MMD (e.g., Zaremba et al., NIPS 2013) has a Gaussian null distribution.  
Arthur Gretton's comment from December 14 during the discussion period provided valuable insights. Incorporating these insights, along with additional experiments comparing kernel-MMD to the classifier threshold on the blobs dataset, would greatly enhance the paper's clarity and depth. The open review format offers an excellent opportunity to appropriately credit these contributions by citing the comment.