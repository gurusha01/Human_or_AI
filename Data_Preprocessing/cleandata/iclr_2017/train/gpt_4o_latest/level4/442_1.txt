Update: I have increased the score because I find the arguments regarding adversarial examples to be compelling. The paper effectively demonstrates that the proposed method functions as a reasonable regularizer. However, I remain unconvinced that it is a competitive regularizer. Specifically, I believe there is insufficient evidence to show that it outperforms established regularizers such as dropout or normalization. Additionally, I suspect that tuning this method will be more challenging compared to these alternatives (as noted in my rebuttal response).
---
Summary: If I understand correctly, this paper introduces a novel approach that adapts the "bottleneck" term from variational autoencoders, which encourages the latent variable to align with a noise prior (e.g., N(0,1)), and applies it in a supervised learning context. Here, the reconstruction term log(p(x|z)) is replaced with the standard supervised cross-entropy objective.
The central claim is that this approach serves as an effective regularizer and enhances robustness against adversarial attacks.
Pros:
- The paper is well-written, and the presentation is clear and easy to follow.
- The proposed idea is reasonable, and its connection to prior work is well-articulated.
- The experiment on robustness to adversarial examples appears convincing, although I am not an expert in this domain. It would be helpful to compare the method to an external quantitative baseline for robustness against adversarial examples. This would clarify how the proposed method stacks up against other regularizers in this context. For instance, could a high dropout rate provide similar robustness to adversarial examples, albeit potentially at the cost of accuracy?
Cons:
- The MNIST accuracy results do not appear particularly strong, unless I am misunderstanding something. For example, the Maxout paper from ICML 2013 reported several permutation-invariant MNIST results with error rates below 1%. Consequently, the 1.13% error rate reported here does not convincingly establish the method as a competitive regularizer. Furthermore, I suspect that tuning this method to achieve strong performance is more challenging than tuning other regularizers like dropout.
- The method involves several architectural choices, particularly regarding the number of hidden layers before and after z. For instance, the output could directly follow z, or there could be multiple layers between z and the output. As far as I can tell, the paper specifies that p(y | z) is implemented as a simple logistic regression (i.e., a single weight matrix followed by softmax), but it is unclear why this specific choice was made. Was this choice empirically optimal?
Other:
- I am curious about the potential effects of combining this method with adversarial training. Specifically, if the model were trained against the identified adversarial examples while also using the proposed method, would it adapt by learning a higher variance p(z | x) when encountering adversarial examples?