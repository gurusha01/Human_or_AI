Review - Paper Summary  
This paper introduces a modified version of dropout tailored for RNNs, where the state of a unit is randomly retained rather than being set to zero. This approach introduces noise that provides a regularization effect while simultaneously preserving information over time. In fact, it facilitates gradient flow by allowing gradients to propagate through identity connections without attenuation. Experimental results demonstrate that this method performs effectively. While it does not outperform variational dropout on the Penn Treebank language modeling task, its simplicity suggests it has the potential for widespread adoption.
Strengths  
- A straightforward yet effective idea.  
- Comprehensive experiments elucidate the impact of zoneout probabilities and confirm its applicability across various tasks and domains.  
Weaknesses  
- Falls short of variational dropout in performance (though better hyperparameter tuning might improve results).  
Quality  
The experimental design and the manuscript are of high quality.  
Clarity  
The paper is clearly written, and the experimental details are sufficiently thorough.  
Originality  
The proposed approach is novel.  
Significance  
This work will appeal to a broad audience, particularly researchers working with RNNs.  
Minor suggestion:  
- As the authors note, Zoneout benefits from two key factors: the noise it introduces and the ability to propagate gradients without decay. It might be useful to disentangle the contributions of these two aspects. For instance, using a fixed mask over the unrolled network (varying at each time step but fixed across training cases) could help isolate the effect of identity connections alone.