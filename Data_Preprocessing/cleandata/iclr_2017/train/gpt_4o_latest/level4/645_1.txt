The authors present a method that extends non-linear two-view representation learning approaches and linear multiview techniques, integrating information from multiple sources into a novel non-linear representation learning framework.
Overall, the method is clearly described and demonstrates advantages across various experiments, such as phonetic transcription and hashtag recommendation. While the approach primarily builds upon classical tools (essentially learning a (deep) network for each view), the integration of multiple information sources appears to be effective for the datasets under consideration.
The following points would benefit from further discussion or clarification:
- What is the computational complexity of the proposed method, particularly in the representation learning phase?  
- Are there alternative strategies for combining the different networks/views? Exploring such alternatives could enhance the novelty of the proposed approach.  
- The experimental setup, especially for the synthetic experiments, requires more detailed explanation. Additionally, making the datasets publicly available would promote reproducibility.  
- The related work section is unfortunately incomplete, particularly regarding the extensive body of research on multiview/multi-modal/multi-layer algorithms proposed in recent years. For instance, works by authors such as A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, among others, have addressed similar challenges in domains like image retrieval, classification, and bibliographic data. While it is not necessary to compare against all these studies, a more comprehensive discussion of related work would provide a clearer understanding of the unique contributions and advantages of DGCCA.