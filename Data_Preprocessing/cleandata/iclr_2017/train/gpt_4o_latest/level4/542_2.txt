This paper addresses the problem of abstract hierarchical multiagent reinforcement learning (RL) using policy sketches, which serve as high-level descriptions of abstract actions. The work builds on a substantial body of prior research in hierarchical RL and introduces some novel elements by leveraging neural implementations of earlier methods for hierarchical learning and skill representations.
Policy sketches are defined as sequences of high-level symbolic labels drawn from a fixed vocabulary, initially lacking any semantic meaning. Over time, these sketches are translated into concrete policies, facilitating policy transfer and enabling temporal abstraction. The learning process is implemented using a variant of the standard actor-critic architecture.
The experimental evaluation is conducted within a standard game-like domain (e.g., maze, Minecraft, etc.).
However, the paper has two notable shortcomings. First, while the concept of policy sketches is intriguing, it is not developed in sufficient depth to make a significant contribution. A more detailed exposition within the framework of abstract Semi-Markov Decision Process (SMDP) models would have clarified their utility and impact. As it stands, the paper presents a specialized application of this idea tailored to the proposed approach, without broader generalization. Second, the experimental comparisons are not comprehensive enough, as they fail to include all relevant prior work. For instance, Ghavamzadeh et al. investigated the use of MAXQ-like abstractions in multiagent RL. A more thorough comparison with MAXQ-based multiagent RL approaches, particularly those that explicitly decompose the value function, would strengthen the evaluation.