This paper introduces a novel adversarial framework to train a model using demonstrations from a third-person perspective, enabling it to perform tasks from a first-person viewpoint. The adversarial training approach is employed to extract viewpoint-independent features (novice-expert or third-person/first-person) that the agent can leverage to execute the same policy across different perspectives.
While the proposed idea is both elegant and innovative (I found it enjoyable to read), additional experiments are necessary to substantiate the approach. The most critical concern is the absence of a baseline. For instance, what would happen if the model were trained using images from the same viewpoint? While this should outperform the proposed method, how close are the results? Moreover, how does performance vary as the viewpoint is gradually transitioned from third-person to first-person? Another significant issue is the possibility that the network might simply memorize the policy. In this scenario, the extracted features could be artifacts of the input images that implicitly encode time ticks (making them domain-agnostic) but still allow for reasonable policy execution. Given that the experiments are conducted in a synthetic environment, this is a plausible concern. A straightforward way to verify this would be to test the algorithm across multiple viewpoints, with blurred or differently rendered images, and/or under random initial conditions.
Additional ablation studies are also required. For instance, I am not entirely convinced by the gradient flipping trick described in Eqn. 5. The experiments lack an ablation analysis comparing GAN/EM-style training with the gradient flipping trick. Furthermore, the experimental results presented in Figures 4, 5, and 6 lack error bars, which diminishes their persuasiveness.