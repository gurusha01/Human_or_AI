The proposed method employs a greedy, layer-wise initialization strategy for a deep MLP model, followed by global gradient descent with dropout for fine-tuning. The initialization process begins with a randomly initialized sigmoid layer for dimensionality expansion, followed by two sigmoid layers whose weights are initialized using Marginal Fisher Analysis (MFA). MFA is a supervised dimensionality reduction technique that learns a linear transformation based on a neighborhood graph constructed using class label information. The output layer is a standard softmax layer.
This approach contributes to the growing body of heuristic layer-wise initialization techniques. However, the specific choice of initialization strategy, while plausible, is not sufficiently justified in the paper when compared to alternative methods, making it appear somewhat arbitrary. Furthermore, the paper lacks clarity in its explanation of the approach: MFA is poorly described, with undefined notations (e.g., in Eq. 4, the term "A" is not properly defined), and the role of denoising in the model remains ambiguous (is there explicit training of a denoising objective, or merely input corruption?).
The paper does not address the (arguably minor) inconsistency of applying a linear dimensionality reduction algorithm—trained without any sigmoid activation—and subsequently passing its learned representation through a sigmoid layer. Additionally, the use of sigmoid hidden layers raises questions, as they are no longer standard practice in modern architectures (why not consider using ReLUs instead?).
More critically, there appear to be methodological issues with the experimental comparisons. The paper states that default values were used for learning rate and momentum, with the number of epochs arbitrarily fixed at 400 (without early stopping) and L2 regularization set to 1e-4 for certain models. Proper hyperparameter optimization should be performed for all models under comparison, ideally using a validation set or cross-validation. This includes tuning learning rates, momentum, early stopping criteria, and layer sizes, separately for each model. This is especially important for small datasets, where different initialization strategies primarily act as indirect regularization mechanisms and thus require careful tuning. The apparent lack of thorough hyperparameter optimization (or its omission from the paper) raises significant concerns about the validity of the experimental results and the fairness of the comparisons.
While the Marginal Fisher Analysis-based initialization strategy may offer potential advantages, the paper does not yet provide a sufficiently compelling case for its effectiveness or offer meaningful insights into the nature of its benefits.
Finally, for image datasets such as CIFAR10, the authors could consider including qualitative visualizations of the filters (back-projected to the input space) learned by the different initialization schemes. This could provide valuable insights into the distinguishing characteristics of the methods under evaluation.