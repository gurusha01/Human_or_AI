This paper provides a theoretical justification for tying the word embedding and output projection matrices in RNN language models. The authors present their argument using an augmented loss function that redistributes the output probability mass among words with similar word embeddings.
I identify two primary limitations of this framework:
The augmented loss function lacks trainable parameters and is employed solely for regularization purposes. As a result, it may not yield significant improvements when applied to sufficiently large datasets.  
The design of the augmented loss function appears highly "engineered" to achieve the specific goal of parameter tying. It remains unclear how the framework would behave if the loss function were relaxed, for instance, by introducing additional parameters or modifying the method for estimating y~.
Despite these concerns, the argument is compelling and well-articulated. The simulated results effectively support the theoretical claims, and the results on the PTB dataset are encouraging.
Minor comments:
Section 3:
Could you clarify whether y~ is conditioned on the t example or the entire history?
Eq. 3.5: i should be enumerated over V rather than |V|.