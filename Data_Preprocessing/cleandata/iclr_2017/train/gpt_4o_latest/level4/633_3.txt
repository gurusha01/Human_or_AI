This paper introduces a novel joint training framework for two probabilistic models of signals (e.g., images), both of which are based on deep neural networks and referred to as the generator and descriptor networks. The proposed framework, called cooperative training, enables the two networks to train collaboratively and support each other: the generator network generates samples that serve as initial inputs for the descriptor network, while the descriptor network refines these samples to guide the training of the generator network.
The approach of coupling the training of these two models is intriguing. However, the paper is notably weak in its empirical evaluation. Specifically:
- The training datasets used are extremely small, ranging from single images to sets of 5-6 images. What is the rationale for not employing larger datasets? It seems likely that the use of such small datasets is causing overfitting and obscuring the true potential of the proposed cooperative training framework.
- For most experiments presented, it is difficult to evaluate the specific contributions of the cooperative training approach due to the absence of baseline results. While some comparisons are provided for face completion experiments, these lack evaluations against descriptor or generator networks trained independently or against other deep auto-encoder models. As a result, it is challenging to determine the extent of improvement, if any, achieved by cooperative training over alternative methods such as training the networks individually.
Additionally, in the "related work" section, the authors should discuss the connection between their approach and variational autoencoders (Kingma and Welling, 2013).
Despite the aforementioned limitations, the ideas presented in this paper are conceptually appealing and merit discussion at ICLR. The paper would be significantly improved by including more comprehensive baseline comparisons and addressing the issue of limited training data.