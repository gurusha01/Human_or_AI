1) Summary
This work addresses the problem of visual servoing, specifically target following, by leveraging spatial feature maps derived from convolutional networks pre-trained on general image classification tasks. The authors integrate bilinear models of one-step dynamics of visual feature maps across multiple scales with a reinforcement learning framework to derive a servoing policy. This policy is optimized by minimizing a regularized weighted average of distances to features predicted by the proposed visual dynamics model.
2) Contributions
+ Systematic experiments in simulation that evaluate the utility of pre-trained deep features for visual servoing.
+ Demonstrated performance improvements over a range of well-chosen baselines, including those utilizing ground truth bounding boxes.
+ A principled approach to learning multi-scale visual feature weights, supported by an efficient trust-region fitted Q-iteration algorithm to address the challenge of distractors.
+ High sample efficiency achieved through the combination of a Q-function approximator and the model-based one-step visual feature dynamics.
+ Release of an open-source virtual city environment to facilitate benchmarking in visual servoing.
3) Suggestions for improvement
- More complex benchmark:  
While the environment used is more than a simple synthetic setup, the experiments would benefit significantly from incorporating more complex visual conditions, such as increased clutter, distractors, diverse appearances and motions, and richer environmental variety. For example, the realism and diversity of object appearances could be enhanced by utilizing a larger set of 3D car models, including more realistic and varied ones available through resources like Google SketchUp, and by adding more distractor vehicles (either in traffic or parked). This is particularly important given that the approach's primary strength is its robustness to visual variations.
- End-to-end and representation learning:  
Although the current synthetic experiments already show notable improvements, it would be valuable to assess the impact of end-to-end training, such as fine-tuning the convolutional network. This could potentially improve generalization in more visually challenging scenarios. Additionally, such experiments would allow the authors to explore the advantages of deep representation learning for visual servoing, which would align well with the interests of ICLR. While the method is adaptable for this purpose, as briefly mentioned by the authors, it has not yet been implemented in the current work.
- Reproducibility:  
While the formalism and algorithms are well-articulated, the paper presents a large number of practical tricks and implementation details, described with varying levels of depth across the main text and appendix. Consolidating, simplifying, or reorganizing these details would improve clarity. A more effective solution might involve summarizing the key implementation aspects in the main text while providing a link to an open-source implementation for completeness.
- Typos:  
  - p.2: "learning is a relative[ly] recent addition"  
  - p.2: "be applied [to] directly learn"
4) Conclusion
Despite the limitations of the current experimental setup, this paper is both compelling and well-executed, bolstered by the authors' strong responses to pre-review feedback and the resulting improvements in the revised version. These factors suggest that the authors are well-equipped to address the suggested improvements, which would further enhance the quality of this already solid contribution.