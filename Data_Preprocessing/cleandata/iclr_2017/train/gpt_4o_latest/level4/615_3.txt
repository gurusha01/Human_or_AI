The paper presents an intriguing approach to addressing saddle points in optimization using an SR1 update, accompanied by a promising initial set of experiments. However, it lacks critical comparisons to contemporary second-order optimization methods, such as Adam, other Hessian-free approaches (e.g., Martens 2012), and Pearlmutter's method for fast exact Hessian-vector products. The MNIST/CIFAR learning curves do not convincingly demonstrate a clear advantage over AdaDelta or NAG, despite the claims made. Furthermore, significantly more experimentation is required to substantiate the assertion regarding mini-batch insensitivity to performance. Could the authors provide error rates on a larger-scale task to strengthen their claims?