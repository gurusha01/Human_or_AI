The authors introduce a novel approach to employing Bayesian neural networks for policy search within stochastic dynamical systems. Specifically, they focus on minimizing the alpha-divergence with alpha=0.5, diverging from the standard variational Bayes (VB) framework. The authors assert that their method is the first model-based approach capable of solving a benchmark problem that has remained unsolved for two decades. However, as I am not deeply familiar with this specific literature, I find it challenging to fully evaluate the validity of this claim.
From a technical perspective, the paper appears sound. However, the writing could benefit from improvement. The notation in Sections 2 and 3 is somewhat dense, and the introduction of numerous terms and approximations makes the content difficult to follow. Additionally, the structure of the paper could be refined to better delineate the novel contributions from the review of prior work. For instance, if my understanding of Section 2.3 is correct, it primarily reviews black-box alpha-divergence minimization. If this is the case, relocating this section to the appendix might enhance the paper's clarity and focus.
I would also like to draw attention to a related paper presented at NIPS 2016, titled "Bayesian optimization with robust Bayesian neural networks" by Springenberg et al., which demonstrates promising results using SGHMC for Bayesian optimization. Could the authors comment on the potential applicability of stochastic gradient MCMC methods (e.g., SGLD or SGHMC) within their proposed framework?
Furthermore, could the authors provide an analysis of the computational complexity of the various approaches discussed in the paper?
Regarding Section 4.2.1, I have some concerns about the experimental setup. Why is it not feasible to use the original data? In what way is it fair to simulate data using another neural network? Additionally, could the authors evaluate the performance of PSO-P on this particular problem?