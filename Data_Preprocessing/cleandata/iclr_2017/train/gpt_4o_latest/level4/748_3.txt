The paper presents a clear and easily comprehensible finding that a convolutional network can effectively replace the recurrent encoder in neural machine translation.  
In addition to well-known architectural components such as convolution, pooling, residual connections, and position embeddings, the paper introduces one notable architectural innovation: the use of two separate convolutional stacksâ€”one dedicated to computing alignment and the other to computing representations. While empirical evidence is provided to demonstrate the necessity of this design choice, the underlying reason why it is essential remains an open question.  
The experimental evaluation is thorough and convincingly establishes the effectiveness of the proposed approach. The convnet-based model demonstrates faster evaluation times, though the primary factor driving this speed-up is not entirely clear. Nonetheless, it is reasonable to expect that the speed advantage of convolutional networks could become more pronounced with a more parallelized implementation.  
My primary concern is whether the paper is a good fit for ICLR, as the contribution feels somewhat incremental and heavily application-specific. Conferences such as ACL, EMNLP, or other NLP-focused venues might be more suitable for this work.