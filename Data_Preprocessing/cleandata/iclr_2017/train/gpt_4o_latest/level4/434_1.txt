The paper demonstrates that Batch Normalization (BN), which typically does not work out-of-the-box for RNNs, can be effectively applied to LSTMs when the operator is separately applied to the hidden-to-hidden and input-to-hidden contributions. The experiments conducted indicate that this approach results in improved generalization error and faster convergence.
The manuscript is well-written, and the core idea is clearly articulated.
i) The datasets used, and by extension the statistical assumptions, are somewhat limited (e.g., the absence of continuous data and the exclusive focus on autoregressive generative modeling).  
ii) The hyperparameters remain nearly constant across the experiments, and it is not sufficiently demonstrated that they were not selected to favor one particular method. For instance, based on the description, it is plausible that a different learning rate could have enabled similar convergence speeds for vanilla LSTMs.
In conclusion, the experimental setup has notable shortcomings and does not adequately substantiate the claims. A more comprehensive exploration of the hyperparameter space could help address these concerns.