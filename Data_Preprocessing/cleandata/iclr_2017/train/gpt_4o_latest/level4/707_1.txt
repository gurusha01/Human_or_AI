This paper introduces a novel language model framework that conceptualizes entity references as latent variables. The work is organized around three distinct models tailored to specific applications: dialog generation incorporating references to database entries, recipe generation referencing ingredients, and text generation with coreference mentions.
While some aspects of the paper lack clarity, which I will elaborate on later, the authors succeed in conveying the core idea, which I find compelling and worthy of further exploration. However, the submission appears to have been rushed, as it exhibits several significant shortcomings.
The primary issue lies in the treatment of the so-called latent variables, which are not truly latent in the empirical evaluations. As clarified by the authors during pre-review Q&A, all entity mentions were provided to all model variants, making it an overstatement to describe these variables as latent when they are effectively treated as observed. This raises the question: were the models with latent variables too challenging to train effectively?
A related concern is the use of perplexity as the evaluation metric when comparing the reference-aware language models to standard language models. The comparison is problematic because the two models operate over different event spaces, leading to an inherent bias. Since mentions are assumed to be given for the reference-aware models and their mention generators are designed similarly to pointer networks, the probability scores over mentions are naturally higher compared to standard language models, which must contend with a much larger vocabulary. This is analogous to comparing language models with aggressive UNK tokenization (and a reduced vocabulary) to models without UNK (and a significantly larger vocabulary).
To address this issue, the authors should consider additional evaluations. These could include marginalizing over all possible mention boundaries (treating the latent variables as genuinely latent) or employing alternative evaluation metrics beyond perplexity, such as BLEU, METEOR, or human evaluations specific to the generation tasks.
Another major weakness is the technical accuracy and completeness of the writing. Many details remain unclear and confusing, even after the Q&A process. This may stem from the challenge of presenting three highly specialized models within a single paper. Each model requires substantial detail to be fully understood, yet these details may not be central to the paper's main narrative but are still essential for comprehension. 
To improve clarity, the authors might consider restructuring the paper to prioritize the most critical details in the main text, particularly regarding the handling of latent variables. This includes elaborating on how mention detection and coreference resolution are made genuinely latent, as well as clarifying when and how entity updates contribute to performance. The current version touches on entity updates only briefly in the context of the third application (coreference resolution) without providing empirical comparisons to justify their inclusion or demonstrate their impact.