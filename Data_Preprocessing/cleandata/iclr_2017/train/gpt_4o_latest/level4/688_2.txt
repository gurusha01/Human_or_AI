The paper addresses the challenge of reinforcement learning under the constraint of minimizing the number of policy updates. The problem is well-motivated, and the author proposes an intriguing modification to the PoWER algorithm, complemented by variational bounds on the value function (Lemmas 3 and 4), which are noteworthy on their own. Additionally, the paper includes numerical experiments on the cartpole task and an online advertising problem using real-world data. Overall, this is a strong and well-written submission. My primary concern, however, is whether it is entirely suitable for ICLR, given that the log-concavity assumption underlying the model seems to limit its applicability to simpler models where representation learning may not actually occur.
Other comments:
- The numerical experiments section lacks sufficient baselines. While I understand this is a somewhat unconventional setting, even a straightforward, well-justified baseline would have been beneficial. Considering that cartpole is a relatively simple task and the advertising dataset is likely private, generating a synthetic advertising dataset could have been an interesting alternative.
- I found the treatment of control variates as constant scalars unclear. Are these intended to serve as constant baselines? If so, they seem to be treated as hyperparametersâ€”why are they not learned or estimated instead?
- The section on constrained optimization is compelling but feels somewhat disconnected from the rest of the paper. While it appears relevant to the online advertising problem, it is not referenced in the corresponding experimental section. Additionally, it might be helpful to include a citation to the constrained MDP literature, which explores similar Lagrangian-based approaches.