This paper explores the alignment of word vectors across languages when the embeddings are independently learned in monolingual settings. The approach addresses an interesting and practical problem, as there are plausible scenarios where such a strategy could prove useful. While the paper is generally well-executed, the evaluation feels somewhat limited. Incorporating a stronger downstream task would have strengthened the work.
The inverted Softmax idea is particularly compelling.
A few minor issues that should be addressed in a revised version of this paper:
1) The paper does not reference Haghighi et al. (2008), "Learning Bilingual Lexicons from Monolingual Corpora," which appears to be a key piece of prior work, especially regarding the use of CCA for bilingual alignment. This work and its relevance to the current approach should be discussed.
2) Similarly, Hermann & Blunsom (2013), "Multilingual distributed representations without word alignment," seems to be the appropriate citation for learning multilingual word embeddings from aligned multilingual data and should be included.
3) The experiments would have been more compelling if they had included more divergent language pairs rather than focusing solely on European/Romance languages.
4) Much of the discussion around orthogonality requirements seems conceptually linked to the idea of using a Mahalanobis distance or covariance matrix to learn such mappings. This connection might be worth elaborating on in the discussion.
5) While no immediate alternative comes to mind, the term "translation (performance/etc.)" feels somewhat misleading when describing word alignment across languages. Translation typically implies a more complex process, and a more precise term might be preferable.
6) The Mikolov citation in the abstract is incorrect and needs to be fixed.