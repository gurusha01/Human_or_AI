This paper introduces a minor yet intriguing modification to enhance the performance of variational autoencoders (VAEs) by further optimizing the ELBO, starting from the predictions of the q network rather than directly using them. Additionally, it proposes the use of Jacobian vectors as a replacement for traditional embeddings when interpreting VAEs.
The concept of leveraging the Jacobian as a natural substitute for embeddings is compelling, as it appears to generalize the notion of embeddings from linear models in an elegant manner. It would be valuable to see comparisons with other approaches aimed at generating context-specific embeddings, such as clustering-based methods or more sophisticated techniques (e.g., Neelakantan et al., Efficient Non-Parametric Estimation of Multiple Embeddings per Word in Vector Space, or Chen et al., A Unified Model for Word Sense Representation and Disambiguation). However, based on the experimental evidence presented in the paper, it is difficult to conclude that the Jacobian of VAE-generated embeddings offers a significant improvement in context sensitivity over prior work.
Similarly, the idea of further optimizing the ELBO is intriguing but insufficiently explored. For instance, the tradeoff between the complexity of the q network and the additional steps for ELBO optimization, in terms of computational cost versus accuracy, remains unclear.
In summary, while the ideas presented in this paper are promising, they would benefit from further elaboration and more comprehensive evaluation.