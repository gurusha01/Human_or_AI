This paper extends a standard auto-regressive model for source code by incorporating a fixed attention policy designed to track the usage of specific token types, such as identifiers. The authors also release an open-source Python dataset. As anticipated, the inclusion of the fixed attention policy leads to improved model perplexity. However, it would be valuable to delve deeper into the results and provide a detailed analysis of how different token types contribute to the achieved perplexity. While this is mentioned in the text, a more comprehensive comparison would strengthen the work. The concept of an attention policy leveraging expert knowledge is a noteworthy contribution, though its novelty may be somewhat limited â€” for instance, the Maddison and Tarlow 2014 paper, which the authors reference, incorporates scoping rules to track previously used identifiers within scope.