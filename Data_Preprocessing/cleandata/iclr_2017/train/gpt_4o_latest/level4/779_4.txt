This paper investigates various approaches to reducing output vocabulary size to accelerate NMT decoding and training. While the work could be highly beneficial for practitioners, its primary contributions appear somewhat tangential to the core themes of representation learning and neural networks, raising questions about whether ICLR is the most suitable venue for this research.
- Do the reported decoding times account for the overhead introduced by the vocabulary reduction step?  
- Beyond machine translation, could these methods be applicable to other domains, such as language modeling, where scalability issues due to large vocabularies are also prevalent?  
- The proposed techniques address challenges arising from the use of word-level models. However, in my view, starting with a character-level or even lower-level abstraction might be a more straightforward solution to the large vocabulary problem.