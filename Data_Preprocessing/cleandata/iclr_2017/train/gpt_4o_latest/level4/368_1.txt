Review  
This paper introduces a quantitative evaluation method for decoder-based generative models by employing Annealed Importance Sampling (AIS) to estimate log-likelihoods. The need for quantitative evaluations is well-justified, as qualitative assessments of sample quality remain prevalent for models such as Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs). While existing quantitative methods like Kernel Density Estimation (KDE) are available, the authors demonstrate that AIS provides greater accuracy than KDE and enables fine-grained comparisons across generative models, including GANs, GMMNs, and Variational Autoencoders (VAEs).
The authors present empirical results comparing two decoder architectures trained on the continuous MNIST dataset using VAE, GAN, and GMMN objectives. Additionally, they trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and found that the IWAE bound underestimates true log-likelihoods by at least 1 nat, a significant margin for this dataset, as evaluated using AIS on the same model.
Pros  
The authors have made their evaluation framework publicly available, which is a valuable contribution to the research community.  
This paper provides insights into the behavior of GANs from a log-likelihood perspective, challenging the widely held assumption that GANs memorize training data. Furthermore, the authors observe that GANs fail to capture important modes of the data distribution.
Cons/Questions  
It is unclear why the experiments use varying numbers of examples (100, 1,000, 10,000) from different sources (train set, validation set, test set, or model-generated samples). For instance, in Table 2, why were results not reported using all 10,000 examples from the test set?  
In Figure 2c, why is AIS slower than AIS+encoder? Are the numbers of intermediate distributions the same for both methods?  
The use of 16 independent chains for AIS seems low compared to prior work (e.g., [Salakhutdinov & Murray, 2008] and [Desjardins et al., 2011], which use 100 chains). Could increasing the number of chains tighten the confidence intervals reported in Table 2?  
It would be helpful if the authors provided their intuition for why GAN50 exhibits a BDMC gap of 10 nats, which is an order of magnitude larger than the gaps observed for other models.
Minor Comments  
- Table 1 is not referenced in the text and lacks an explanation of the columns.  
- In Figure 2(a), do the reported values represent the average log-likelihoods of 100 training and validation examples of MNIST (as described in Section 5.3.2), or is this the total?  
- In Figure 2(c), is the dataset binarized MNIST? Why are there fewer data points for AIS compared to IWAE and AIS+encoder?  
- Are the BDMC gaps mentioned in Section 5.3.1 the same as those reported in Table 2?  
- Typo in the caption of Figure 3: "(c) GMMN-10" is labeled, but the graph title and subcaption indicate it is actually GMMN-50.