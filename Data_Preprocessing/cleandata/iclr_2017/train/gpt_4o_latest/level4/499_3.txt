Paper Summary 
This paper introduces a novel neural network architecture where the layer weights of a traditional network are derived as a function of a latent representation associated with each layer. Two specific instances are explored: (i) a CNN where the layer weights are computed from a lower-dimensional embedding vector for each layer, and (ii) an RNN where the layer weights are derived from a secondary RNN state.
 Review Summary 
Pros:  
- The idea of revisiting multiplicative RNNs and their predecessors is compelling and timely.  
- The results on language modeling (LM) and machine translation (MT) are impressive.
Cons:  
- The paper's presentation could be improved. It is overly lengthy for the conference format and requires better focus.  
- The related work section omits key connections to multiplicative RNNs and their predecessors, such as second-order networks (as defined by C. Lee Giles). The differences between this work and prior research should be explicitly discussed and motivated. Acknowledging that this work revisits and scales up older ideas would be more appropriate than omitting these connections.  
- The paper lacks clarity regarding its primary objective—whether it aims to achieve better performance or more compact networks. The RNN section seems to prioritize performance, while the CNN section appears to focus on compactness.
Suggestions:  
- The paper would benefit from being shorter and more focused, potentially deferring the CNN results to a future publication.  
- The relationship to multiplicative/second-order networks and the differences with prior work should be clearly explained.
 Detailed Review 
The concept of multiplicative networks is highly powerful, and revisiting them is a valuable contribution. This paper achieves excellent results but suffers from poor presentation and a lack of clear focus. It dwells on unnecessary details while omitting critical points. Additionally, the paper is overly long and not self-contained without the appendices.
A stronger emphasis on multiplicative RNNs and second-order networks early in the paper would be beneficial. This would allow the authors to clearly highlight the differences between their approach and prior work. Furthermore, the paper should address why multiplicative RNNs have historically been less popular than gated RNNs, such as LSTMs. It seems that their optimization challenges may have played a role, and it would be helpful to discuss whether the authors faced difficulties in tuning optimization parameters or required longer training times compared to standard LSTMs or CNNs. Regarding the naming, "hypernetwork" may not effectively convey the essence of the proposed architecture compared to "multiplicative interactions."
In Section 3.2, the claim that hypernetworks can transition between RNN and CNN settings is unclear. A simple example with equations applied to a temporal problem could help clarify this point.
The CNN and RNN sections feel disconnected. For CNNs, the focus appears to be on achieving a low-rank structure for the weights, demonstrating that similar performance can be achieved with fewer parameters. However, the motivation for this goal is unclear. If the aim is to achieve speedups or reduced memory usage for embedded applications, comparisons with alternative strategies, such as model compression (Caruana et al., 2006; Hinton et al., 2014) or hashed networks (Chen et al., 2015), should be included.  
For RNNs, the focus shifts to improving perplexity/BLEU scores, with less emphasis on model compactness. Instead of simplifying the weight structure, the RNN weights are made more complex by introducing temporal dependencies. This raises questions about training time, inference time, and memory requirements, especially since compactness is emphasized in the CNN section. The mixed messaging detracts from the paper's clarity. A more cohesive narrative focusing solely on the RNN results, with the CNN results deferred to a future publication, would better fit the conference format.
Some discussions are unclear. For example, the purpose of Figure 2 and the discussion on saturation statistics (p. 10, Figure 5) is not well-articulated. Similarly, Figure 4 lacks context—changes at word boundaries should be expected even in regular LSTMs, so a comparison is necessary to determine whether this behavior is unique to the proposed network.
The handwriting generation results are difficult to interpret. Log-loss metrics are not intuitive, and it is unclear whether the observed differences are statistically significant (e.g., under bootstrap sampling of the training set). Qualitative evaluations by humans are also insufficient, as they cannot assess whether the network is merely repeating the training set. A precision/recall metric for ink, with spatial tolerance (similar to segmentation evaluation in vision), might be more appropriate.
The MT experiments are insufficiently discussed in the main text, leaving the reader with an incomplete understanding of their significance.
Overall Recommendation:  
The paper should be shortened and clarified, with the CNN results deferred to a future publication. The relationship to multiplicative/second-order networks must be properly discussed, and unclear sections should be revised or removed to make room for a more thorough explanation of the experimental setup and results.  
 References   
- M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, "First-Order Vs. Second-Order Single Layer Recurrent Neural Networks," IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.  
- Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, "Model Compression," Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.  
- Dark Knowledge, G. Hinton, O. Vinyals, J. Dean, 2014.  
- W. Chen, J. Wilson, S. Tyree, K. Weinberger, and Y. Chen, "Compressing Neural Networks with the Hashing Trick," Proc. International Conference on Machine Learning (ICML-15).