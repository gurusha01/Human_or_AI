This paper is well-written and can be divided into two main parts:  
1. Adversarial training on ImageNet  
2. An empirical study of label leakage, single/multiple-step attacks, transferability, and the importance of model capacity  
For part [1], I believe that training without clean examples may not result in a reasonable ImageNet-level model. Ian's experiments in "Explaining and Harnessing Adversarial Examples" did not utilize BatchNorm, which could be a critical factor for training large-scale models. This section appears to extend Ian's work using the Inception-V3 model. I recommend including an experiment that involves training without clean samples to strengthen this part.  
For part [2], while the experiments address most variables in adversarial training, they lack sufficient technical depth. The depth and model capacity experiments could potentially be explained by the regularization effect of adversarial training. The discussion on label leakage is novel and noteworthy. Regarding the transferability experiment with FGSM, a closer examination of specific MNIST FGSM examples reveals an augmentation effect on the digits, where the grey regions in the image make the numbers resemble other digits. Although this phenomenon is challenging to observe with more complex datasets like CIFAR-10 or ImageNet, it might relate to the authors' observation that "FGSM examples are most transferable."  
In this section, the authors raise several intriguing questions and hypotheses but do not provide sufficient theoretical explanations to support them.  
Overall, I find these empirical observations valuable and believe they will be useful for guiding future research.