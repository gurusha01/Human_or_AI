This paper explores an interesting approach to framing architecture search as a meta-learning problem. By leveraging features extracted from networks trained on various datasets and subsequently training a "ranking classifier" (although the specifics of this classifier are not thoroughly detailed), the authors propose that it may be possible to predict a suitable architecture for a novel problem by applying the ranker to the extracted features of the new problem's setup.
One noteworthy aspect of the paper is that the authors fix several key hyperparameters across all networks. However, I believe that optimizing the learning rate (and its decay schedule) is a critical factor. My hypothesis is that many of the conclusions drawn in this paper could shift significantly if the authors conducted a proper search over learning rates. For instance, instead of training 11k networks, it might be more effective to train 2k networks with five different learning rates each, potentially yielding more compelling results.
I am also unconvinced that the protocol for generating architectures achieves sufficient diversity. Given the constraints of a maximum depth of 8 layers and 14 components overall, I suspect that many of the generated architectures are nearly identical in performance. Training such a large number of architectures across numerous tasks may therefore be inefficient. If not already implemented, the authors should consider incorporating a pruning mechanism to eliminate architectures that are too similar to existing ones.
The batch normalization experiments presented in Table 2 appear unusual and insufficiently explained. It is well-documented that optimal learning rates can vary significantly—by an order of magnitude—when using batch normalization versus not using it. Since the learning rate is fixed across all experiments, I view these results with some skepticism.
Additionally, the paper provides limited insights into the characteristics of the top-performing architectures. Including visualizations, identifying trends, or both would greatly enhance the paper's contribution in this regard.
Another concern is that the paper conflates the study of parallel versus serial architectures with the study of meta-learning, which are distinct topics. For instance, the comparison of parallel and serial performance in Table 2 is problematic because it does not control for the number of parameters or model capacity, which would be the correct basis for comparison.
Ultimately, the paper concludes that designing an effective architecture for a new domain is challenging. However, this conclusion makes it difficult to view the work as a constructive contribution. While the ranker may perform well in some cases, its reliability is inconsistent, which limits its practical utility for practitioners seeking to apply deep networks in new domains. Consequently, I remain unconvinced of the practical impact of the results presented in this paper.