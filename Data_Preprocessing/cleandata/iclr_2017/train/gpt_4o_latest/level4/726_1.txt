Review - Paper Summary  
This paper proposes a generalization of dropout grounded in information-theoretic principles. The central concept is that when learning a representation \( z \) of an input \( x \) to predict \( y \), the representation \( z \) should retain the minimal amount of information about \( x \) necessary to predict \( y \). This principle is formalized using the Information Bottleneck (IB) Lagrangian, resulting in an optimization problem that closely resembles that of variational dropout. However, Information Dropout introduces a scaling factor for the KL divergence term, which promotes the addition of noise. The noise level is parameterized as a function of the data and is optimized jointly with the model. Experimental results on CIFAR-10 and MNIST demonstrate modest improvements over binary dropout.
Strengths  
- The paper establishes an important conceptual connection between probabilistic variational methods and information-theoretic approaches, demonstrating that dropout can be generalized through both frameworks to yield closely related models.  
- The model's presentation is clear and well-structured.  
- The experimental results on cluttered MNIST are particularly compelling.  
Weaknesses  
- The results on CIFAR-10 in Figure 3(b) appear to be based on a validation set (unless the axis label is incorrect). It is unclear why the test set was not used, which complicates comparisons with results reported by Springenberg et al. and other related work in the literature.  
Quality  
The theoretical exposition is of high quality, with Figure 2 providing an insightful qualitative illustration of the model's behavior. However, the experimental results section could be improved, for instance, by aligning the CIFAR-10 results with those reported by Springenberg et al. and attempting to surpass them using Information Dropout.  
Clarity  
The paper is well-written and easy to understand.  
Originality  
The derivation of the Information Dropout optimization problem using the IB Lagrangian is novel. However, the resulting model is quite similar to variational dropout.  
Significance  
This work is likely to interest researchers in representation learning, as it offers an alternative perspective on latent variables through the lens of information bottlenecks. Nevertheless, the broader impact of the paper may be limited unless the model demonstrates substantial performance gains over standard dropout.  
Overall  
The paper provides a thoughtful theoretical derivation and promising preliminary results. However, there is room for improvement in the experimental section.  
Minor Comments and Suggestions  
- Correct "expecially" to "especially."  
- Correct "trough" to "through."  
- There may be a missing minus sign in the expression for \( H(y|z) \) above Equation (2).  
- Figure 3(a) includes error bars, but Figure 3(b) does not. Adding error bars to Figure 3(b) would enhance consistency.  
- Consider comparing Figure 2 with the activity map of a standard CNN trained with binary dropout to assess whether similar filtering effects are already occurring.