This paper presents CoopNets, an algorithm designed to train a Deep-Energy Model (DEM, referred to as the "descriptor") with the assistance of an auxiliary directed Bayesian network, termed the "generator." The descriptor is optimized using standard maximum likelihood estimation, employing Langevin MCMC for sampling. Meanwhile, the generator is trained to produce samples that are likely under the DEM in a single, feed-forward ancestral sampling step. This cooperative setup allows the generator to effectively bypass the computationally expensive MCMC sampling process, hence the term "cooperative training."
While the proposed idea is both interesting and novel, the experimental validation is insufficient. Most notably, two out of the three experiments lack a proper train/test split and fail to adhere to standard training and evaluation protocols for texture generation (see [R1]). Additionally, the datasets used are far too small, which raises concerns that these experiments merely demonstrate the model's ability to overfit. For the third in-painting task, the lack of meaningful baselines—such as VAEs, RBMs, or DEMs—makes it challenging to assess the advantages of the proposed method.
For future revisions, I recommend that the authors address the following questions through experimental analysis: What is the impact of omitting the rejection step in Langevin MCMC (e.g., compare training with and without it)? How does the generator influence the burn-in process of the Markov chain (e.g., show sample auto-correlation)? How significant is the approximation error introduced by training the generator on ({\tilde{Y}, \hat{X}) instead of ({\tilde{Y}, \tilde{X})? Comparative experiments should be conducted to evaluate these aspects.
Additionally, the paper would benefit from a clearer and more precise presentation. The use of hyperbolic language (e.g., "pioneering work" in reference to closely related but non-peer-reviewed research) and overly casual prose (e.g., "tale of two nets") detracts from the scientific rigor. For instance, the omission of the exact form of the energy function is a significant oversight that should be addressed.
PROS:  
+ The idea is both interesting and novel.  
CONS:  
- Experimental protocols are inadequate.  
- Key baselines are missing.  
- Diagnostic experiments are absent.  
[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of experts.