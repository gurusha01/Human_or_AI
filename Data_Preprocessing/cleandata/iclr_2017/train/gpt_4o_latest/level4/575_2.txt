After revisiting the paper, I remain unclear about the authors' primary objective.
The CCA objective is non-differentiable because the trace norm (sum of singular values) of T is not differentiable. Based on the title and Section 3, it seems the authors aim to address this issue. However:
-- Have the authors merely reformulated the CCA objective, or have they fundamentally altered it? This distinction needs to be explicitly clarified.
-- What is the connection between the retrieval objective and the "CCA layer"? There are multiple conceivable ways to integrate them, such as through a combination or bi-level optimization. However, Section 3 lacks a discussion on this point. Including equations to clarify this relationship would be beneficial.
-- Although the CCA objective is non-differentiable in the aforementioned sense, this has not posed significant challenges for training (e.g., while batch training is theoretically required, large minibatches work well in practice). The authors must provide a justification for why the original gradient computation is problematic in the context of their specific goals. From their response to my second question, it appears they still rely on the SVD of T, leaving me uncertain about any computational efficiency advantages offered by the proposed method.
Regarding the paper's structure, it would be more effective to introduce the retrieval objective earlier, rather than waiting until the experiments section. Additionally, I continue to recommend that the authors include a comparison with the contrastive loss, as I suggested in my previous comments.