The paper introduces a method for integrating recurrent layers into larger, potentially pre-trained convolutional networks. The goal is to leverage the feature extraction capabilities of CNNs while utilizing RNNs to capture global contextual information.
The authors evaluate their approach on two tasks: image classification (using CIFAR-10) and semantic segmentation (using PASCAL VOC12).
Strengths:  
The paper is well-written and clear, aside from a few minor typos. The proposed method is straightforward, making it adaptable for other works, and it can serve as a practical enhancement to existing systems without requiring retraining from scratch. This is particularly useful for improving system performance incrementally. The evaluation is systematic and includes a clear ablation study, which strengthens the paper's experimental rigor.
Weaknesses:  
The novelty of the work is somewhat limited, and the validation could be more comprehensive.  
- Novelty: The idea of combining recurrent layers with CNNs is not entirely new. A similar concept was proposed by Bell et al. (2016), with only minor technical differences (e.g., cascading versus parallel application of recurrent layers). While the initialization of the recurrent network using the CNN is a reasonable improvement, it appears to address a suboptimal choice in Bell et al.'s work rather than introducing a fundamentally novel concept. The contribution of using RNNs within layers, which is emphasized throughout the paper, seems to overlap significantly with Bell et al.'s approach, with only minor modifications.  
- Validation: The CIFAR-10 experiments serve as proof of concept but are insufficient for demonstrating the broader applicability of the method. For example, Wide Residual Networks (WRNs) (Zagoruyko and Komodakis, BMVC16) achieve better results on CIFAR-10 (4% error) without using recurrent layers, relying instead on network depth to spread the receptive field. The authors' response that their method is beneficial for shallow networks is valid, but the practical necessity of keeping networks shallow is unclear. An evaluation on a more challenging dataset like ImageNet would provide stronger evidence for the utility of the proposed method.
For semantic segmentation, I raised the question of whether the observed performance boost is due to the recurrent layer's specific properties or simply the addition of extra parameters to a pre-trained network. The authors claim that the recurrent layer provides a greater boost than adding an equivalent number of parameters as CNN layers because it models long-range dependencies. However, I could not find such an experiment in the paper. It would be valuable to compare the performance of the recurrent layer with that of additional non-recurrent residual layers to validate this claim. While this is not a decisive factor for acceptance or rejection, it would serve as a useful sanity check.
Additionally, the presentation of results in Table 3 is somewhat misleading. The comparison between "FCN-8s" and "FCN-8s-LRNN" gives the impression of a 10% boost, but the "FCN-8s" baseline here is not the same as the original FCN-8s from Long et al., as indicated in Table 2. This should be clarified to avoid confusion.
Another unclear aspect is the source of the performance improvement in Table 2. The authors state that inserting the L-RNN after pool3 and pool4 in FCN-8s allows the model to capture contextual information over a larger range than local convolutions. While this is plausible, it is unclear why the same benefit would not apply to FCN-32s, as the recurrence property should be independent of the 8/32 factor.
Additional Comments:  
- Figures 2b and 2c are missing from the PDF.  
- Figure 4 is overly complex, with approximately 30 boxes, making it difficult to interpret. A table format might be more effective for conveying the information.  
- Appendix A: The learning rate schedule is not well-documented. Did the authors try alternative schedules, such as polynomial decay? What is the performance with a standard schedule like step decay?  
- Appendix C: Typographical error: "maps .. is" should be corrected to "maps ... are."
In summary, while the paper has merits in terms of clarity and systematic evaluation, the limited novelty and incomplete validation weaken its overall contribution. Addressing the above concerns, particularly through additional experiments and clarifications, would significantly strengthen the paper.