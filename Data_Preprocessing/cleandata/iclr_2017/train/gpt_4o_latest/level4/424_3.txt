The authors demonstrate that smoothing a highly non-convex loss function can facilitate the training of deep neural networks.
The paper is well-written, the proposed idea is thoroughly analyzed, and the experimental results are compelling, leading us to recommend its acceptance. However, for a stronger endorsement, additional experiments would be beneficial. Specifically, how does the proposed smoothing method compare to the insertion of probes at various network layers? Another intriguing avenue to explore is its performance on challenging optimization tasks, such as algorithm learning. For instance, in the "Neural GPU Learns Algorithms" paper, the authors had to relax the weights across different RNN layers to achieve optimizationâ€”could your smoothing approach eliminate the need for such relaxation?