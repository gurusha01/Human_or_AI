Comments:  
"This contrasts with adversarial attacks on classifiers, where inspecting the inputs will reveal the original bytes supplied by the adversary, which often exhibit telltale noise."  
Is this claim accurate? If true, wouldn't it suggest that training against adversarial examples should straightforwardly make a classifier robust to such examples, given that they consistently exhibit telltale noise?  
Pros:  
- The question of whether adversarial examples exist in generative models, and how the concept of "adversarial example" translates to this context, is an intriguing one.  
- Demonstrating that a certain class of generative models lacks adversarial examples would be a highly significant finding, while showing that generative models do have adversarial examples would also be a valuable negative result.  
- The adversarial examples presented in Figures 5 and 6 appear compelling, though they seem more overt and noisier compared to the adversarial examples on MNIST discussed in (Szegedy 2014). Could this be because it is inherently more challenging to identify adversarial examples in these types of generative models?  
Issues:  
- The paper exceeds the length limit significantly, coming in at 13 pages.  
- The introduction should more clearly articulate the paper's motivation and purpose.  
- While the title references "generative models," the paper appears to focus exclusively on autoencoder-type models. This could be misleading, as researchers exploring adversarial attacks on other types of generative models, such as autoregressive models, might face unnecessary confusion or need to clarify distinctions from a paper titled "adversarial examples for generative models."  
- The introduction includes excessive background information and could benefit from being more concise.