This paper explores various approaches for generating adversarial examples targeting generative models such as VAE and VAEGAN. Specifically, the authors examine three methods: classification-based adversaries, which involve training a classifier on the latent code; VAE loss, which directly optimizes the VAE objective; and the "latent attack," which identifies adversarial perturbations in the input space to align the latent representation with that of a target input.
The problem addressed in this paper is potentially valuable and relevant to the community. To the best of my knowledge, this is the first work to systematically study adversarial examples for generative models. However, as noted in my pre-review comments, there is concurrent work titled "Adversarial Images for Variational Autoencoders," which independently proposes the same "latent attack" approach using both L2 distance and KL divergence.
Regarding novelty and originality, I found the contributions of this paper to be somewhat limited. The three proposed attack methods are standard and well-established techniques that are applied here to a new context. The paper does not introduce novel algorithms specifically tailored for attacking generative models. Nevertheless, it is interesting to observe how these established methods perform in this new problem setting.
The clarity and organization of the paper leave much to be desired. The first version of the submission focused on "classification-based adversaries" and reported primarily negative results. In subsequent revisions, the paper underwent significant changes, with the introduction of a new co-author and the addition of the "latent attack" method, which outperforms the earlier approach. However, the authors retained material from the initial version, resulting in a lengthy 13-page paper with inconsistent claims and disjointed experiments. The explanation that "in our attempts to be thorough, we have had a hard time keeping the length down" is not a sufficient justification for the lack of coherence.
In summary, while the paper addresses an interesting and relevant problem and provides a comparison of standard adversarial methods in this context, its contributions in terms of novelty are limited, and the presentation requires significant improvement.