This paper introduces a straightforward approach to address action repetitions by representing the action as a tuple (a, x), where a denotes the chosen action and x specifies the number of repetitions. The authors report improvements over A3C/DDPG, with substantial gains in some games and moderate ones in others. The idea appears intuitive, and the paper is supported by extensive experimental results.
Comments:
- The A3C scores presented in this paper differ significantly from those reported in the Mnih et al. publication (Table S3). Could the authors clarify the source of this discrepancy? If it stems from a different training setup (e.g., fewer iterations), have they verified that their replication aligns with Mnih et al.'s results under identical conditions?
- It is noteworthy that FiGAR achieves its best performance in games dominated by few action repetitions. This suggests that, in such cases, FiGAR incurs a higher computational overhead compared to A3C, which uses a fixed action repeat of 4 (resulting in 4 times fewer gradient updates). Running A3C with a reduced action repeat at a comparable computational cost might yield improved performance. Nonetheless, the automatic determination of action repetitions is an interesting feature, even if the broader takeaway seems to discourage excessive action repetition.
- The notation for r is slightly inconsistent, as it alternates between representing rewards and elements of the repetition set R (top of page 5).
- In the equation at the bottom of page 5, since the summation is indexed over decision steps rather than time steps, shouldn't the rewards r_k be adjusted to represent the sum of rewards (appropriately discounted) accrued between those time steps?
- The section discussing DDPG is somewhat unclear. The term "concatenating" loss is unconventional; does FiGAR correspond to a loss resembling Q(x, Î¼(x)) + R log p(x), with a separate loss for training the critic? It seems that REINFORCE should be applied to the repetition variable x (second term of the sum), while reparameterization should be used for the action a (first term). Could the authors clarify this?
- Is the inclusion of the 'namethisgame' entry in the tables intentional?
- A potential limitation of the proposed method is that the agent commits to an action for a fixed number of steps, regardless of subsequent events. Have the authors considered an alternative approach where the agent decides at each time step whether to persist with the current action or switch? This modification seems like it could be a relatively straightforward extension of FiGAR.