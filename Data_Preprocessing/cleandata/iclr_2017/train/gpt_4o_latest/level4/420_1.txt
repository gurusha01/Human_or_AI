This paper centers on the use of attention mechanisms in neural language modeling and presents two primary contributions:
1. The authors introduce the idea of employing separate key, value, and predict vectors for the attention mechanism, rather than relying on a single vector to perform all three functions. This represents an intriguing extension of the standard attention mechanism, with potential applicability to other domains as well.  
2. The authors demonstrate that a very short attention span suffices for language models (a result that is not particularly surprising) and propose an n-gram RNN to leverage this observation.
The paper introduces novel models for neural language modeling and conveys some compelling insights. The authors have conducted a comprehensive experimental evaluation of their proposed approaches on both a language modeling task and the CBT task.
I find the authors' responses to my pre-review questions satisfactory.
Minor suggestion: The related work section should include references to Ba et al., Reed & de Freitas, and Gulcehre et al.