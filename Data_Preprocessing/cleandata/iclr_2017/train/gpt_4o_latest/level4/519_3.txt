The paper introduces an extension of weight normalization and normalization propagation tailored for recurrent neural networks. Preliminary experiments indicate that the proposed method performs effectively.
This contribution has the potential to benefit a wide audience, given that LSTMs are fundamental components in the field.
However, the novelty of the contribution is somewhat limited, as the modification to weight normalization is relatively minor. Additionally, the experimental results are not particularly compelling: while layer normalization is shown to have higher test error due to overfitting in their example, it appears to perform better in terms of optimization. Furthermore, the authors do not seem to implement the data-dependent parameter initialization for weight normalization as recommended in the original paper.