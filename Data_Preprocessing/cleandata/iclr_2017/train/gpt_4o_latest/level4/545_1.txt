This paper introduces a novel learning model called "Compositional Kernel Machines (CKMs)," which extends traditional kernel machines by constructing compositional kernel functions using sum-product networks. The authors conceptualize convolutional networks (convnets) as effectively learned nonlinear decision functions and attribute their classification success to their compositional structure. This perspective inspires the development of compositional kernel functions, and the sum-product implementation is indeed an intriguing approach. While I agree that composition plays a significant role in the success of convnets, it is not the sole contributing factor. A key distinction between convnets and CKMs lies in the fact that convnets learn all their kernels directly from data, whereas CKMs rely on precomputed feature descriptors. This reliance, in my view, constrains the representational capacity of CKMs. The authors might find the recent paper "Deep Convolutional Networks are Hierarchical Kernel Machines" by Anselmi, F. et al. to be a relevant and insightful reference.
The experimental results presented in this paper appear to be preliminary. While it is encouraging to see promising outcomes of CKMs on the small NORB dataset, it is crucial to demonstrate competitive performance on widely recognized classification benchmarks, such as MNIST, CIFAR10/100, and even Imagenet, to establish the validity of this new learning model. In the NORB compositions experiment, CKMs seem to outperform convnets in classifying images based on their dominant objects, which I suspect is due to the use of sparse ORB features. It would be beneficial if the paper could include the classification accuracy of ORB features using matching kernel SVMs for comparison. Additionally, some experimental details require further clarification, such as the specific high and low probabilities used for sampling from each collection and the total number of generated images. 
In the NORB symmetries experiment, CKMs demonstrate better performance than convnets when trained on small datasets. However, it appears that the convnets have not fully converged. Would it be possible to include results using a larger dataset to provide a more comprehensive comparison?