ResNet and similar architectures employing shortcuts have demonstrated empirical success across various domains, making the study of optimization for such architectures highly valuable. This paper aims to explore certain properties of networks that incorporate shortcuts. While some of the experiments presented are intriguing, there are two significant concerns with the current work:
1- Linear vs. Non-linear: Although analyzing linear networks is important, it is crucial to avoid extrapolating these findings to networks with non-linear activations without sufficient evidence. This is particularly relevant for the Hessian, as the Hessian of non-linear networks tends to have a very large condition number (refer to the ICLR submission "Singularity of Hessian in Deep Learning"), even in scenarios where optimization is not inherently difficult. Consequently, I disagree with the paper's claims regarding non-linear networks. Additionally, a single plot on MNIST is insufficient to substantiate the assertion that non-linear networks exhibit behavior similar to linear networks.
2- Hessian at Zero Initialization: The justification for focusing on the Hessian at the zero initialization point is unconvincing. The zero initialization point is highly specific and does not provide meaningful insights into the Hessian during the course of optimization.