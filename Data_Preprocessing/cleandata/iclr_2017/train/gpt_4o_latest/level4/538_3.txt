This paper introduces a method to compute embeddings for symbolic expressions (e.g., boolean expressions or polynomials) such that semantically equivalent expressions are positioned close to one another in the embedding space. The proposed approach leverages recursive neural networks, where the network architecture mirrors the parse tree structure of the input symbolic expression. To train the model, the authors construct a dataset of expressions with known semantic equivalence relationships and optimize a max-margin loss function to ensure that embeddings of equivalent expressions are closer than those of non-equivalent ones. Additionally, the authors employ a "subexpression forcing" mechanism, which, as I understand it, aims to enforce some form of compositionality in the embeddings.
The results are presented on several symbolic expression datasets curated by the authors, and the proposed method convincingly outperforms baseline approaches. One particularly compelling aspect is the PCA visualization, where the operation of negating an expression is shown to approximately correspond to negating the embedding in the vector space. This behavior is reminiscent of the analogy-style embeddings observed in word2vec and GloVe papers, such as the well-known example: man - woman + queen = king.
The primary limitation of the paper is that the problem setting feels somewhat artificial. It is difficult to envision a practical scenario where a training dataset with known semantic equivalences is readily available, yet a neural network-based approach is still the preferred solution. Furthermore, the authors sidestep the issue of variable names by assuming that distinct variables correspond to different entities in the domain. While this assumption simplifies the problem, it significantly limits the applicability of the method. For instance, the proposed approach would not be practical for applications like an "equation search engine" unless variable names could be reliably canonicalized.
Additional comments:
* Regarding the complexity of the problem, I believe that determining whether two expressions are equivalent is undecidable, as it relates to the "word problem for Thue systems." On this note, it was unclear how the authors establish ground truth equivalence in their training datasets. They mention simplifying expressions into a canonical form and grouping them, but this approach seems infeasible in the general case. A pertinent question is whether equivalent expressions in the training data might have been assigned different canonical forms. Would it have been easier or more feasible to construct and compare truth tables instead?
* The "COMBINE" operation is described as having a residual-like connection. However, based on the equations provided, this is not a true residual connection, as the lower-level \( l_0 \) features are multiplied by a weight matrix rather than being passed through unchanged (as in an identity connection). A true residual connection might have been more effective at mitigating gradient explosion. Is there a specific reason the authors opted for this approach instead of an identity connection?
 In Table 3, the first tf-idf example, \( a + (c+a)  c \), appears to be equivalent to \( a + (c * (a+c)) \).
* The vertical spacing between the caption of Figure 4 and the main body text is minimal, making it appear as though the caption continues into the text.