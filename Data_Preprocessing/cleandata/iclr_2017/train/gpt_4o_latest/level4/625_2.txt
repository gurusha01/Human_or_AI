This paper introduces an architecture and associated algorithms for learning to act across multiple tasks described in natural language. The proposed system adopts a hierarchical structure and aligns closely with the options framework. However, instead of learning a discrete set of options, it learns a mapping from natural language instructions to an embedding that implicitly and dynamically defines an option. This represents a novel and intriguing perspective on options, which has only been marginally explored in the linear setting (see comments below). The use of policy distillation is particularly well-suited to this context, and this technique alone could serve as a valuable takeaway for many reinforcement learning (RL) researchers, even those less focused on natural language processing (NLP) applications.
The paper does not provide a single, straightforward, end-to-end recipe for learning with the proposed architecture. Instead, it skillfully integrates several recent advances, including generalized advantage estimation, analogy-making regularizers, L1 regularization, memory addressing, matrix factorization, and policy distillation. While I would have appreciated some analysis, I recognize that this would have been a challenging task. For instance, when you state, "while the parameters of the subtask controller are frozen," it suggests the use of a two-timescale stochastic gradient descent approach. Additionally, it is unclear how the semi-Markov decision process (SMDP) structure is handled in gradient updates when transitioning to the "temporal abstractions" setting.
I believe this approach has the potential to scale to very large domains, although the paper does not currently demonstrate this empirically. As is typical for reviewers, I am tempted to suggest conducting larger experiments. However, I appreciate that the system's performance in a "toy" domain has been demonstrated effectively. The characterization in Figure 3 is particularly insightful and provides a strong argument for the analogy regularizer and the necessity of hierarchy.
Overall, I find the proposed architecture to be inspiring and believe it would be a valuable contribution to ICLR. It also introduces novel elements, such as subtask embeddings, which could have applications beyond the deep learning and NLP communities, extending into more traditional RL research.
---
Parameterized Options
Sutton et al. (1999) did not originally explore the concept of parameterized options. This idea emerged later, perhaps first in works such as ["Optimal Policy Switching Algorithms for Reinforcement Learning," Comanici & Precup, 2010] or ["Unified Inter and Intra Options Learning Using Policy Gradient Methods," Levy & Shimkin, 2011]. Konidaris also pursued a line of research on "parameterized skills," including ["Learning Parameterized Skills," da Silva, Konidaris, Barto, 2012] and ["Reinforcement Learning with Parameterized Actions," Masson, Ranchod, Konidaris, 2015].
It is important to distinguish between two interpretations of "parameterized options." In one sense, options can have policies and termination functions represented by function approximators, akin to how value functions are approximated. These options are "parameterized" in the sense of having learnable parameters, as explored in works like Comanici & Precup (2010), Levy & Shimkin (2011), Bacon & Precup (2015), and Mankowitz, Mann, and Mannor (2016).
In another sense, options, policies, or skills can take parameters as inputs and act accordingly. This is the interpretation used by Konidaris et al., which differs from the "function approximation" perspective. In your work, the embedding of subtask arguments serves as the "input" to your options, aligning with Konidaris's notion of parameterized options.
---
Related Work
I searched the PDF but could not find references to any of S.R.K. Branavan's work. Branavan's PhD thesis focuses on using RL-based control techniques to interpret natural instructions and achieve goals. For instance, in "Reinforcement Learning for Mapping Instructions to Actions," an RL agent learns from "Windows troubleshooting articles" to interact with UI elements (the environment) using a Softmax policy (over linear features) trained via policy gradient methods.
While your work emphasizes generalization, which is not explicitly addressed in Branavan's research (to the best of my knowledge), there are significant algorithmic and architectural similarities. These should be discussed explicitly or, if feasible, compared experimentally as a baseline.
---
Zero-shot and UVFA
Under the section on "zero-shot task generalization," you might consider citing "Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning," Borsa, Graepel, Shawe-Taylor.
---
Minor Issues
- The abstract could be clearer. The second sentence mentions "longer sequences of previously seen instructions," but the term "instructions" is not clarified until much later when it is specified as "instructions described by natural language." Reordering the sentences could improve clarity.
- The term "zero-shot" may not be familiar to all readers. While "one-shot" is more widely recognized, the distinction between the two is not immediately clear in the text. Consider defining "zero-shot" explicitly early on and comparing it to "one-shot." Additionally, it might help to explain how zero-shot learning relates to the broader concept of learning with priors.
- In Section 3, the phrase "cooperate with each other" suggests a multi-agent setting, which your work does not explicitly address. You might want to rephrase this or clarify any potential connections to multi-agent systems.
- The second sentence of Section 6 is overly long and difficult to parse. Breaking it into two or three sentences would improve readability.