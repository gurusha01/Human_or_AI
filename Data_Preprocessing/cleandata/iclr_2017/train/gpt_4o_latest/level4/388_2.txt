Review - Summary:  
This paper introduces a novel deep neural network architecture tailored for question answering on the SQuAD dataset. The model is composed of two primary components: a coattention encoder and a dynamic pointer decoder. The encoder simultaneously generates attention over both the question and the document, enabling the learning of co-dependent representations for the two. The decoder iteratively predicts the start and end tokens of the answer, with the rationale that multiple iterations can help the model avoid local maxima and reduce errors. At the time of writing, the proposed model achieves state-of-the-art performance on the SQuAD dataset. The paper also includes analyses of the results, such as performance variations across question types, as well as document, question, and answer lengths. Additionally, the authors conduct ablation studies, such as evaluating the model with a single iteration in the decoder.
Strengths:  
1. The paper is well-motivated, with two key contributions: co-attending to both the document and the question, and iteratively refining the answer prediction.  
2. The proposed model architecture is novel, and the design choices appear to be well-justified.  
3. Experimental results demonstrate that the model significantly outperforms existing approaches (at the time of writing) on the SQuAD dataset.  
4. The analyses of results and the ablation studies (conducted in response to a request) provide valuable insights into the design choices of the model.  
Weaknesses/Questions/Suggestions:  
1. To better understand the impact of each additional iteration in the decoder, I suggest reporting the following: for each iteration, provide the mean F1 score for questions that converge in that iteration, along with the number of questions that converge.  
2. Figure 5 includes an interesting example (Question 3) where the model struggles to resolve between multiple local maxima despite several iterations. Could the authors report how frequently such cases occur?  
3. To evaluate the contribution of attention modeling in the encoder, it would be helpful if the authors could report the model's performance when no attention is modeled in the encoder (neither over the question nor the document).  
4. I would like to see how the model's performance varies for questions requiring different types of reasoning (as categorized in Table 3 of the SQuAD paper). This analysis could shed light on the model's strengths and weaknesses with respect to reasoning types.  
5. In Wang and Jiang (2016), attention is computed over the question for each word in the document. However, in Table 2, when conducting the ablation study to approximate Wang and Jiang's approach, the authors set \( C^D \) to \( C^Q \). Isn't \( C^Q \) the attention over the document for each word in the question? Wouldn't \( QA^D \), which represents attention over the question for each word in the document, be a closer match to Wang and Jiang's method? Please clarify this point.  
6. In Section 2.1, the variables "n" and "m" are swapped in the explanation of the Document and Question encoding matrices. This should be corrected.  
Review Summary:  
The paper proposes a novel and compelling model for question answering on the SQuAD dataset, demonstrating superior performance compared to prior models. However, to gain deeper insights into the model's behavior, I recommend additional analyses of the results and the inclusion of one more ablation study (as outlined in the weaknesses section).