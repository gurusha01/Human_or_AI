The paper introduces an extension to traditional Recurrent Language Models (RNNLMs) that enables handling unknown words. This is achieved by integrating the standard RNNLM with an auxiliary module that operates on a knowledge base (KB) and can copy facts from the KB to generate previously unseen words. The approach is demonstrated to be both efficient and significantly more effective than a standard RNNLM on a newly introduced dataset.
The quality of the writing could be improved, particularly at the start of Section 3, which is difficult to follow.
There have been recent efforts (e.g., "Pointer Sentinel Mixture Models" by Merity et al.) aimed at addressing the limitations of RNNLMs with unknown words, typically by incorporating mechanisms to copy from a longer historical context. However, the current paper's approach stands out as more compelling because it leverages external knowledge from a KB to enhance the language model. This is a more challenging task, as it requires effectively utilizing the large-scale information present in the KB. The ability to train this model efficiently is a notable strength.
The proposed architecture seems well-founded, but the clarity of the writing hinders a complete understanding, which prevents me from assigning a higher rating.
Additional comments:
- How does the model address its reliance on the KB? For instance, Freebase is no longer updated, so many emerging unseen words may not be present in it.
- What is the model's performance on standard benchmarks such as the Penn Treebank?
- How does the training time compare to that of a standard RNNLM?
- What is the significance of the knowledge context $e$ in the model?
- How is the fact embedding $a_{t-1}$ initialized for the first word?
- When a word from a fact description is selected as a prediction (copied), how is it represented in the generation history for subsequent predictions if it lacks an embedding (i.e., is an unknown word)? For example, in Section 3.1, how would the model handle the next prediction if "Michelle" is not in the embedding dictionary?