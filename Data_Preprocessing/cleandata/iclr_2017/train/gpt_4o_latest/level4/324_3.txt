This paper introduces a straightforward approach for pruning filters in two types of architectures to reduce execution time.
Strengths:  
- Effectively preserves accuracy on widely-used models such as ImageNet and CIFAR-10.
Weaknesses:  
- The paper lacks justification for why low L1 or L2 norms are appropriate criteria for filter selection. Additionally, two important baseline comparisons are missing: (1) randomly pruning filters and (2) pruning filters based on low activation pattern norms on the training set.  
- There is no direct evaluation against the numerous existing pruning and speedup techniques.  
- While FLOPs are reported, the empirical speedup achieved by the method is unclear. This is a critical metric for practitioners interested in such techniques. Reporting wall-clock speedup is straightforward, and its absence raises concerns.