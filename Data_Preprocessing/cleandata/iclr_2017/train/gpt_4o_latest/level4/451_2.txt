This paper investigates the energy landscape of the loss function in neural networks. It is generally well-written and effectively provides intuitive explanations for the results. A key contribution is demonstrating that the level sets of the loss function become connected as the network becomes increasingly overparameterized. Additionally, it quantifies, to some extent, the degree of disconnectedness by characterizing the increase in loss required to identify a connected path. This observation could have potential implications for the likelihood of escaping local minima when using stochastic gradient descent. The paper also introduces a straightforward algorithm for identifying geodesic paths between two networks, ensuring that the loss decreases along the path. Using this approach, the authors show that the loss landscape appears to grow more nonconvex as the loss value decreases, which is an intriguing finding.
That said, the work has notable limitations, which is understandable given the inherent challenges of thoroughly analyzing the loss function of neural networks. The authors are transparent about these limitations, particularly the focus on shallow networks rather than deep ones and the analysis of the oracle loss rather than the empirical loss. However, I would have appreciated a more detailed practical discussion of the bound presented in Theorem 2.4, as it is unclear whether this bound is sufficiently tight to have practical significance.