The paper investigates a straightforward method for learning reward functions in reinforcement learning from visual observations of expert trajectories, specifically in scenarios with limited training data. To generate meaningful rewards under these challenging conditions, the approach leverages a pre-trained neural network as a feature extractor (similar to prior work on task transfer with neural networks in computer vision) and models the reward function as a weighted distance to features derived from automatically identified "key-frames" of the expert trajectories.
The manuscript is well-written and provides a clear explanation of the underlying concepts while situating the proposed approach within the broader literature on inverse reinforcement learning (IRL). The simplicity of the resulting algorithm is appealing, and it has the potential to be valuable for various real-world robotic applications. However, I have three primary concerns with the paper in its current form. Addressing these issues could significantly enhance the paper:
1) While the recursive splitting method for extracting "key-frames" is reasonable and the feature selection is well-justified, the experimental evaluation lacks two important baselines:
   - What happens if feature selection is omitted, and the distance is computed across all features? Would this approach fail outright, or is there a trade-off in performance?
   - A simpler baseline than the proposed method could involve using all frames from the expert trajectories, calculating the distance to these frames in feature space, and weighting them temporally as described in the paper. How well would this alternative perform?
2) Although the decision to pair the extracted reward function with a simple RL method is understandable, the chosen controller may introduce significant bias into the experiments. Specifically, the controller requires initialization from an expert trajectory, which means the RL process begins near a good solution. Consequently, the extracted reward function is likely only evaluated within a small region of the state space, close to the initial expert-provided images (except perhaps for the human demonstrations). Without additional experiments, it remains unclear how well the proposed approach generalizes when combined with other RL methods for training the controller.
3) While the limited number of images makes training a deep neural network directly infeasible, it raises questions about the performance of alternative baselines. For instance, what happens if a random projection of the images is used to create a feature vector? How would distance measures based on raw images (e.g., L2 norm of pixel differences) or the first principal components perform? Although occlusions and other factors might render these approaches ineffective, empirical evidence is necessary to confirm this.
Minor issues:
- Page 1: The phrase "make use of ideas about imitation" is somewhat awkward.
- Page 3: "We use the Inception network pre-trained ImageNet" should be revised to "pre-trained for ImageNet classification."
- Page 4: The definition of the transition function for the stochastic case appears to be incorrect.
- Page 6: The phrase "efficient enough to evaluate" is awkwardly constructed.
Additional comments (not critical issues):
- The paper is predominantly empirical, with minimal actual learning involved in deriving the reward function and no theoretical contributions. While this is not inherently problematic, it underscores the importance of a robust empirical evaluation.
- Although the exposition is clear, the proposed approach ultimately reduces to computing quadratic distances to features of pre-selected "key-frames." While the connection to standard IRL methods in Section 2.1 is appreciated, one could argue that this derivation is not strictly necessary.