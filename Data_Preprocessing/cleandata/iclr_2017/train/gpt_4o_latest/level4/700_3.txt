This paper introduces a method to initialize the weights of a deep neural network layer-wise using a marginal Fisher analysis (MFA) model, leveraging a similarity metric.
Pros:  
The paper includes a variety of experiments, albeit conducted on relatively small datasets, to evaluate the proposed approach.
Cons:  
The work lacks comparisons with strong baselines, such as discriminatively trained convolutional networks on standard datasets like CIFAR-10.  
Additionally, the computational cost of calculating the association matrix \( A \) in Equation 4 is not clearly discussed.
Overall, this is a reasonable paper that presents a new idea and integrates it with existing techniques such as greedy layer-wise stacking, dropout, and denoising autoencoders. However, similar concepts have been explored in prior works, particularly those published 3â€“5 years ago, such as SPCANet.
The primary novelty lies in the use of marginal Fisher analysis as a new layer. While this is a potentially interesting contribution, the absence of strong baselines makes it difficult to assess the effectiveness of the proposed approach. Specifically, it would be valuable to see how a convolutional network or fully connected network, trained from scratch with a good initialization, performs on the same tasks.
To strengthen the paper, the authors should provide compelling evidence that initializing layers with MFA offers a clear advantage over using random weight matrices.