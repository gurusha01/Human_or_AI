The authors argue that the reviewers exhibit a "narrow view" by focusing on metrics such as parameter counts and accuracy fractions, as well as an alleged "complete focus on engineering." However, the authors' claims about prioritizing engineering are debatable, as the only technical contribution in the paper is the fractal network architecture, which is supported by intuitive assertions rather than rigorous analysis. A robust contribution would either involve a theoretical formulation with clear assumptions and small-scale empirical validation or a thorough empirical study with carefully designed experiments to substantiate the intuitive claims. Since the proposed approach does not fall into the first category, it requires strong experimental evidence to validate its claims. Unfortunately, the rebuttal fails to provide satisfactory answers and instead includes manipulative arguments about narrow reviewing and the publication dates of baselines, which will be discussed further below.
The paper states that "In experiments, fractal networks match the state-of-the-art performance held by residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks" and "Fractal architectures are arguably the simplest means of satisfying this requirement, and match or exceed residual networks in experimental performance." The authors' claims hinge on the assertion that fractal networks match (or exceed) the performance of state-of-the-art residual networks. However, the empirical study merely compares accuracy to a limited set of baselines without conducting a deeper empirical analysis to support claims about the differences between fractal networks and residual networks (or the ensemble explanation of residual networks proposed by Veit et al., NIPS 2016). Even this limited comparison is incomplete, as noted in my initial review. Thus, the expectation of a fair and comprehensive comparison is entirely reasonable and not merely about focusing on accuracy fractions or parameter counts. For instance, Veit et al. systematically provide empirical evidence to support their claims about residual networks, which the authors of this paper fail to do. Notably, Table 4 and Figure 3 in the paper offer only a preliminary sanity check by comparing fractal networks to plain networks, without addressing the claims about differences between residual and fractal networks.
The paper also claims that fractal networks scale to "ultra-deep" architectures, but the authors fail to report results for networks with dozens of layers. While they assert that increased depth may slow training (without specifying the extent) but not reduce accuracy, this claim is unsubstantiated, as no results are provided for networks with dozens of layers. Furthermore, Table 3 shows that error rates increase as depth grows to 160 layers. The number of parameters in the proposed architecture is significantly higher than in state-of-the-art ResNet variants, contrary to the authors' rebuttal that the increase is slight.
Regarding the lack of comparison to DenseNet, the authors argue in their rebuttal that DenseNet cites FractalNet. However, this does not justify the omission, as DenseNet was published in August and is well-known for achieving state-of-the-art results as a ResNet variant. The rebuttal also states, "Many of the variants only reported results on CIFAR/SVHNâ€¦ it is a bit difficult to have already compared to results that did not exist at submission time." However, there were clearly published results available, such as those by Huang et al. (2016b) on arXiv in July.
The authors' claim about the "simplifying power" of their architecture is also unconvincing. How can the architecture be considered simpler when it involves a more complex training procedure and significantly more parameters, while failing to scale as effectively as the baselines?
In summary, my evaluation remains unchanged, as the rebuttal does not provide satisfactory clarifications or improvements.
This paper introduces a novel architecture that avoids explicit use of residuals and instead employs a fractal structure composed of expand and join operations. The authors argue that this architecture demonstrates that large nominal depth with many short paths is the key to training "ultra-deep" networks, while residuals are incidental.
The primary limitation of this paper is the significantly higher parameter count required by FractalNet compared to baselines, which hinders its scalability to "ultra-deep" networks. The authors' rebuttal points out that Wide ResNets also require many parameters, but this comparison is not applicable to ResNet and its variants. For example, ResNet and ResNet with stochastic depth scale to depths of 110 with 1.7M parameters and 1202 with 10.2M parameters, which are far fewer than the parameters required for depths of 20 and 40 in Table 1 (Huang et al., 2016a). While the authors report fewer parameters for 40 layers using a scaling trick, this trick is not validated for other depths, including 20 layers in Table 1. Even with the scaling trick, the parameter count for 40 layers remains significantly higher than most baselines. The lack of satisfactory comparisons to these baselines undermines the authors' claims.
The authors also claim that drop-path regularization improves performance compared to the layer-dropping procedure in Huang et al. (2016b). However, the results show that the empirical benefit of drop-path disappears when standard data augmentation techniques are applied, making its effectiveness unconvincing.
DenseNet (Huang et al., 2016a) should also be included in the comparison, as it outperforms most state-of-the-art ResNets on both CIFAR-10 and ImageNet. More importantly, DenseNet significantly outperforms FractalNet while requiring substantially less computation.
Table 1 includes ResNet variants as baselines, but Table 2 only includes ResNet. As a result, the ImageNet comparison merely demonstrates that FractalNet can run on ImageNet and achieve comparable performance to ResNet, which is not a compelling result given the improvements of other baselines over ResNet. Additionally, there is no improvement in SVHN results, and this is not addressed in the empirical analysis.
The authors also list several improvements over Inception (Szegedy et al., 2015), but these intuitive claims are not supported by empirical evidence.
While the paper explores interesting intuitive directions using the proposed architecture, the empirical results fail to substantiate the claims, and the high parameter count makes the model impractical. Consequently, the contribution does not appear to be significant.
Pros:
- Proposes an interesting architecture compared to ResNet and its variants.
- Investigates differences between fractal and residual networks, which could inspire further analysis.
Cons:
- The parameter count is significantly higher than baselines, which achieve greater depths with fewer parameters.
- Claims are intuitive but lack strong empirical support.
- Path regularization does not yield improvement when standard data augmentation is applied.
- Empirical results do not demonstrate the method's promise for "ultra-deep" networks.
- Parameter count increases dramatically compared to competitors for state-of-the-art results.
Questions for Authors:
1. Can you provide a detailed comparison of depth, parameter count, and training times with ResNet and its variants in Tables 1 and 2? Could you elaborate on this?
2. Did you attempt to reduce the parameter count for 20-layer networks?
3. Why are there no results reported for 40 layers without augmentation in Table 1?
4. How were the B and C values chosen, and what other configurations were tried? How do these choices affect training efficiency and misclassification error in Tables 1 and 2? Are the results stable or highly sensitive to these parameters?
5. Can you elaborate on cases where FractalNet failed to outperform the best ResNet variant in Table 1?