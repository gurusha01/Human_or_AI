The paper introduces a second-order method for training neural networks while simultaneously ensuring that both weights and activations are binary. By leveraging binarization, the approach seeks to enable model compression, facilitating deployment on low-memory systems. The proposed method is referred to as BPN, short for "binarization using proximal Newton algorithm."
A notable strength of the method is its integration of the supervised loss function directly into the binarization process, which is both significant and advantageous. (The authors point out that existing weight binarization techniques typically overlook the impact of binarization on the loss.) The method is well-articulated and analytically connected to prior weight binarization approaches.
The experimental evaluation is thorough, spanning multiple datasets and architectures, and it convincingly demonstrates the generally superior performance of the proposed method.
One minor concern with the feed-forward network experiments is that only test errors are provided, which does not adequately demonstrate improved optimization performance. (See also the comment "RE: AnonReviewer3's questions," which notes that all baselines achieve nearly perfect training accuracy.) A more effective way to showcase superior optimization could involve making the optimization problem more challenging—such as by incorporating an explicit regularizer into the training objective or employing a data augmentation scheme—and tracking the training objective rather than solely reporting test error.
That said, the advantage of BPN becomes more evident in the subsequent LSTM experiments.