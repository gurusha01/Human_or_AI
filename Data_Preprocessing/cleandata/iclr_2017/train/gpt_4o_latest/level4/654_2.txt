I greatly appreciate the core idea presented in this paper. However, I found the current execution unconvincing. My primary concern remains the one I raised in my pre-review question, which I believe the authors have not adequately addressed. Specifically, the choice of \( q(s | s') = p(s | s') \) appears problematic, as it likely causes the forward and reverse trajectories to be almost pathologically mismatched. This, in turn, would result in a variational bound that is excessively loose and exhibits high variance.
The claim regarding the tightness of the bound in Appendix D hinges on the assumption that the transition distribution satisfies detailed balance. However, the learned transition distribution in this work does not adhere to detailed balance, invalidating the tightness claim in Appendix D. (In Section 2.1, the authors briefly mention the idea of learning an energy function rather than directly learning a transition distribution. I believe this would be an excellent approach, as it would allow the use of an MCMC transition operator that satisfies detailed balance for the energy function.) I did not review Appendix D in detail beyond this point.
The experimental results were not visually compelling. I suspect this is largely due to the mismatch between the generative and inference trajectories, as highlighted in my concern above and in the pre-review question.
Additionally, please refer to my note below regarding Section 5. I suspect that some terms are being omitted from the training gradient.
Since the paper optimizes a variational bound on log-likelihood, it is crucial to report and compare log-likelihoods against competing methods. This is a significant omission that should be addressed.
Below are detailed comments, some of which pertain to an earlier version of the paper:
- Section 1.2: The first paragraph is challenging to follow.  
  - "these modes these spurious modes" → "these spurious modes"
- Section 2.1:  
  - "s = (v,h)" → "s = {v,h}"
- Section 2.2:  
  - "with an MCMC" → "with an MCMC chain"  
  - "(ideally an MCMC)" → "(e.g., via MCMC)" — MCMC is not ideal; it is simply often the most practical approach.
- Section 3, last bullet: Consider making the temperature infinite for the final step. In this case, the last step would sample directly from the prior, ensuring that the posterior and prior are exactly the same.
- Section 4: Using an energy function would be an excellent idea! Many MCMC transition operators satisfy detailed balance, which would significantly mitigate the forward/backward transition mismatch that constitutes my primary concern with this technique.
- Equations 12 and 13: What is \(\alpha\)? How does it depend on temperature? This is never specified.
- Section 5, last paragraph in GSN section: Note that \( q \) also depends on \(\theta\). By not backpropagating through the full \( q \) chain, you are likely omitting terms from the gradient.
- Section 5, non-equilibrium thermodynamics: The non-equilibrium paper also increases the noise variance as the distance from the data increases. This should be noted.
- Figure 1: Right and left are mislabeled.
- Figure 2: Please label the panes.
- Figure 3: After how many walkback steps were these results generated?
In summary, while the paper introduces a promising idea, its current implementation has significant issues that need to be addressed. I encourage the authors to carefully consider the concerns raised here, particularly regarding the forward/backward trajectory mismatch, the lack of detailed balance, and the omission of log-likelihood comparisons.