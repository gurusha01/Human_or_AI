This paper introduces relation networks as a means to model pairwise interactions between objects in a visual scene.  
The proposed model is straightforward: an MLP with shared weights is applied to each pair of objects, followed by a final prediction generated through another MLP that aggregates non-linear functions of these object pairs.  
The experimental evaluation is conducted on a synthetic dataset specifically designed to align with the architecture described in the paper.  
However, the title of the paper overstates its contributions. While discovering objects and their relationships is indeed a critical task, this paper does not achieve such discovery. Instead, objects are represented using hand-coded ground truth attributes, and only a limited set of trivial relationships, such as relative position, are "discovered."  
The task of discovering objects and their relationships has been a longstanding challenge in computer vision (CV). Unfortunately, this paper neither cites nor compares its approach to prior work in this domain, which is often referred to as "contextual models."  
Key questions remain unanswered: Can the proposed architecture improve object detection or scene classification? How robust is it to noise, such as missing detections, inaccurate estimates, or complex textures? Would it still perform effectively when object attributes are derived from real images?  
The paper would be more convincing if experiments were conducted on real-world scenes. For indoor environments, datasets like NYUv2, Sun-RGB-D, SceneNN, or Chen et al. (CVPR 2014, text-to-image coreference) could be used. For outdoor scenarios, benchmarks like KITTI, which include relationships between cars, pedestrians, and cyclists, could serve as valuable test cases.  
In its current form, the paper addresses an overly simplified problem using a basic model that does not significantly advance beyond existing contextual models, which already capture pairwise relationships between objects using techniques such as MRFs or deep networks.