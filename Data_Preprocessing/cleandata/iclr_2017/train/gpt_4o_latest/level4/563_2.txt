This paper introduces b-GAN, a framework that trains the discriminator by estimating a density ratio that minimizes the Bregman divergence. The authors also explore the connections between b-GAN, f-GAN, and the original GAN framework, presenting a unifying perspective through the lens of density ratio estimation.
It is important to note that this unifying perspective is limited to GAN variants that optimize density ratios. GANs employing MMD in the discriminator step generally do not align with the b-GAN framework, except under specific kernel choices.
I found the dual relationship between f-GAN and b-GAN somewhat unclear. Are the conditions on the function f identical in both frameworks? If so, what distinguishes f-GAN from b-GAN, aside from the former being derived using f-divergence and the latter using Bregman divergence?
One of the initial claims was that b-GANs directly optimize f-divergence, unlike f-GAN and GAN. However, in practice, the authors optimize an approximation to the f-divergence. The quality of this approximation is not quantified anywhere, which makes b-GAN appear no more principled than f-GAN or GAN.
The experimental results were somewhat unclear and did not provide much insight into the choice of f.
Overall, I appreciated the connections drawn to the density ratio estimation literature. However, the appendix currently feels disorganized. Revising and restructuring the text would greatly enhance the clarity and quality of this paper.