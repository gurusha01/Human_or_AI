Given the authors' discussion of CVST, I anticipated Hyperband would perform significantly better; however, their experimental results do not support this expectation:  
- Although Hyperband is designed as an anytime algorithm, the authors ran it for a much shorter duration than CVST, yielding consistently inferior mean results. While the difference may not be substantial, the lack of task-specific characteristics presented in the paper raises the question of whether one could achieve comparable results to CVST's error bars simply by selecting a random configuration at no cost. Why not extend Hyperband's runtime to match CVST's and provide a fair, direct comparison?  
- Furthermore, in this experiment, the authors used a different \(\eta\) for Hyperband than in all other experiments. This raises an important question: How much tuning is required to make Hyperband effective? What would the results look like if the authors had used the same \(\eta = 4\) as in the other experiments?  
This paper focuses on Hyperband, an extension of successive halving introduced by Jamieson & Talwalkar (AISTATS 2016). Successive halving is an elegant algorithm that begins by evaluating numerous configurations and iteratively eliminates the worst-performing half, enabling exploration of many configurations within a constrained budget.  
After reading the paper during the question period and revisiting it now, I am unclear about the intended contribution of this work. The only improvement Hyperband offers over successive halving appears to be in its theoretical worst-case bounds (no more than 5x worse than random search). However, (a) this bound can be trivially achieved by allocating one-fifth of the time to running random configurations to completion, and (b) the theoretical analysis establishing this bound is stated to be beyond the scope of the paper. This leaves me wondering whether the theoretical results are the primary contribution of this work or if they are detailed in a separate paper, with the current submission serving primarily as an empirical study of the method. I hope the authors can clarify this in a revised version of the paper.  
Regarding the experiments, the paper does not demonstrate a scenario where Hyperband outperforms the authors' previous algorithm, successive halving, with its most aggressive setting of bracket \(b = 4\). In every figure, bracket \(b = 4\) performs at least as well as, and sometimes significantly better than, Hyperband. This suggests that, in practice, I would favor successive halving with \(b = 4\) over Hyperband. (And if I require Hyperband's guarantee of being no more than 5x worse than random search, I could simply allocate one-fifth of my resources to random search.)  
The experiments also include comparisons to some Bayesian optimization methods but omit comparisons to the most relevant, closely related Multi-Task Bayesian Optimization methods, which have been highly effective for deep learning over the past three years. For instance, "Multi-Task Bayesian Optimization" by Swersky, Snoek, and Adams (2013) demonstrated 5x speedups for deep learning by leveraging smaller datasets, and subsequent papers have reported even greater speedups.  
Given the existence of this prominent work on multitask Bayesian optimization, I find the introduction's portrayal of Hyperband as a groundbreaking approach to hyperparameter optimization somewhat misleading. I would prefer a more balanced framing that acknowledges the growing importance of "configuration evaluation" in hyperparameter optimization, including within Bayesian optimization, where it has already shown substantial speedups (as evidenced by prior work). The paper could then position its contribution as providing valuable theoretical insights into configuration evaluation and illustrating its significance, even in the simplest case of random search. This adjustment could be made by adding a paragraph to the introduction.  
Finally, regarding novelty, the authors should acknowledge that adaptive resource allocation for evaluations has been studied in the machine learning community for at least 23 years. For example, Maron & Moore's NIPS 1993 paper, "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation," explored similar ideas.