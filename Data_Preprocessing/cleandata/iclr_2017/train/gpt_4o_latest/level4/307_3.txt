SYNOPSIS:  
This paper presents a novel dataset designed to evaluate end-to-end goal-oriented dialog systems. The dataset is set in the restaurant domain, where the task involves finding availability and booking a table based on user-provided parameters during a dialog. The data is generated through a simulation that uses an underlying knowledge base to create samples for various parameters (e.g., cuisine, price range) and applies rule-based transformations to produce natural language descriptions. The task is to rank a set of candidate responses for each subsequent turn in the dialog, with evaluation metrics reported as per-response accuracy and per-dialog accuracy. The authors demonstrate that Memory Networks outperform basic bag-of-words baselines.
THOUGHTS:  
I appreciate the authors' effort in making an interesting contribution. However, I remain skeptical about the practicality of end-to-end trained systems in narrow-domain applications. In open-domain scenarios, there is a compelling case for avoiding hand-coding all states and responses due to scalability challenges, making end-to-end approaches more appealing. In contrast, narrow-domain settings are typically well-understood, and the primary objective is to maximize user satisfaction. Wouldn't it be more effective in such cases to leverage domain knowledge to engineer the most optimal system?
Given the restricted nature of the domain, I also find it somewhat disappointing that the task focuses on RANKING responses rather than GENERATING them, even though I understand that ranking simplifies evaluation. Additionally, I am unclear about how these candidate responses would be obtained in real-world applications. As noted in the paper (last sentence before Sec 3.2), the models appear to rank the set of all responses in the train/validation/test splits. Since a major advantage of end-to-end training is its scalability to new domains without requiring manual re-engineering, how would this information be acquired for a new domain in practice? Generating responses, rather than ranking pre-defined ones, would likely enable better generalization to new domains. In my view, this is the most significant limitation of the work.
Furthermore, since the data is generated through simulation by expanding (cuisine, price, etc.) tuples with natural language generation rules, this inherently limits the variability in training responses. While this trade-off allows for the generation of unlimited data via simulation, I could not find a detailed list of the rules used for this process. Publishing these rules would enhance the reproducibility and transparency of the work.
In conclusion, despite my reservations, I find this to be an intriguing contribution that merits acceptance at the conference.  
------
I have updated my score based on the clarifications and additional results provided.