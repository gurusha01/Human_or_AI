Paper Summary 
This paper simplifies the matching network approach by utilizing a single prototype per class, computed as the average of the embeddings of the training samples for each class. Empirical comparisons with matching networks are provided.
 Review 
The paper is well-written and effectively motivates the proposed work. This study on metric learning aims to simplify a prior approach (matching networks), which is a commendable goal. However, I am uncertain whether it achieves superior results compared to matching networks. While the space of learning embeddings for optimizing nearest neighbor classification has been previously explored, the idea of averaging prototypes is intriguing (as a non-linear extension of Mensink et al., 2013). I recommend enhancing the discussion of related work and refining the results section to better clarify the methods you outperform versus those you do not.
The related work section could be expanded to include research on learning distance metrics for optimizing nearest neighbor classification, such as Weinberger et al., 2005, and subsequent studies. Additionally, extensions that achieve similar objectives using neural networks, such as Min et al., 2009, are highly relevant to your work. For approaches with similar goals but different learning objectives, you reference Siamese networks with pairwise supervision. However, the literature on learning to rank (e.g., for web search) with triplet supervision or global ranking losses is also pertinent. In this context, one example is where "the query" defines the class, and the embedding space must ensure that positive/relevant documents are closer to the query than others. I suggest starting with Chris Burges' 2010 tutorial for foundational insights.
I am also unsure whether the reported results accurately reflect the state of the art across all tasks. While the results on Omniglot are promising, I believe you should also include the superior performance of matching networks on miniImageNet with fine-tuning and full contextual embeddings. Omitting these results could be perceived as misleading. Regarding CUB-200, my understanding is that the state-of-the-art performance is 50.1% when using features from GoogLeNet (Akata et al., 2015). Could you provide clarification or commentary on this?
In conclusion, the paper has significant potential but would benefit greatly from an improved discussion of related work and a more balanced presentation of prior empirical results.
 References 
Large Margin Nearest Neighbors. Weinberger et al., 2005  
From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010  
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al., 2009