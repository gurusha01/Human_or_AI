The paper addresses a critical issue of vanishing gradients and explores the pursuit of an optimal activation function. The authors propose a novel approach of learning activation functions dynamically during the training process. While I find the research concept intriguing, I believe the paper feels somewhat premature in its current state.
Although the experimental section is extensive, the conclusions drawn are unclear. The authors themselves seem uncertain, as evidenced by the frequent use of tentative language such as "maybe," "could mean," and "perhaps" throughout the paper. For this work to be considered for acceptance, it requires a definitive claim about its performance, backed by robust evidence. At present, this clarity and confidence are lacking. The proposed approach has the potential to be either groundbreaking or ineffective, but after reviewing the paper, I remain unconvinced as to which it is.
Additionally, the theoretical section could benefit from greater clarity and refinement.
Lastly, the impact on performance needs to be addressed. A significant advantage of ReLU lies in its simplicity and low computational cost. It is unclear how the proposed PELUs compare in terms of efficiency and computational overhead.