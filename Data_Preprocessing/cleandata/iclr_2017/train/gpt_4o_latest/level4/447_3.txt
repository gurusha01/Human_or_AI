This paper presents a simulator and a set of synthetic tasks designed to assess a dialogue agent's ability to learn from user feedback. To address these tasks, the authors employ memory networks (Sukhbaatar et al., 2015), trained using previously established supervised learning and reinforcement learning techniques. The results demonstrate that agents capable of learning from feedback (e.g., through question asking or clarification) outperform those that do not.
The motivation behind the work is compelling; dialogue agents that can directly learn from unstructured human feedback (as opposed to relying solely on reward signals) hold significant potential for real-world applications. However, the execution has notable shortcomings. All experiments are conducted within a synthetic dialogue simulator, which is highly artificial and diverges significantly from real-world dialogue scenarios. The simulator operates within a basic factoid question-answering framework, which is typically not categorized as dialogue and appears solvable using a few hand-crafted rules. Furthermore, the framework assumes that user feedback is always accurate, provided in a limited set of forms (e.g., paraphrased questions without typos), and that the agent can learn from examples of another agent asking questions or making clarifications, thereby simplifying the task considerably.
Given the artificial nature and narrow scope of the experiments, it is challenging to draw meaningful conclusions about learning from unstructured user feedback. To substantiate the hypothesis that agents can learn from such feedback, I strongly encourage the authors to extend this work by conducting experiments with real human users (even within the factoid question-answering domain, if necessary). Such experiments would offer more robust evidence that dialogue agents can effectively learn from user feedback.
---
Other comments:
- The abstract uses the term "interactive dialogue agents." What is meant by "interactive"? Since all dialogue agents inherently interact with users, the term seems redundant.
- A significant limitation of the experiments is that the agent's possible questions are predefined. If I understand correctly, in the supervised learning setup, the agent is trained to mimic the questions of a rule-based agent. In the reinforcement learning setup, the paper states: "For each dialogue, the bot takes two sequential actions $(a1, a2)$: to ask or not to ask a question (denoted as $a1$); and guessing the final answer (denoted as $a2$)." This implies the agent learns when to ask questions but not what questions to ask.
- Related to the previous point, the subsection "ONLINE REINFORCEMENT LEARNING (RL)" claims: "We also explored scenarios where the student learns the ability to decide when to ask a question and what to ask." Please clarify this by removing the phrase "what to ask."
- The paper presents an excessive number of results. While synthetic tasks allow for detailed performance measurements, the sheer volume of results presented here is overwhelming and may confuse readers. For instance, what is the purpose of including the "TrainAQ(+FP)" and "TrainMix" training settings? If these results do not directly support the main hypothesis, they should either be moved to the appendix or omitted.
- Since the paper's primary contribution lies in the proposed tasks and evaluation framework, it may be more appropriate to relegate some of the results to the appendix. For example, consider moving the vanilla-MemN2N results (Table 2) or the Cont-MemN2N results (Table 3) to the appendix.
---
Update:
After further discussion and reviewing the additional experiments provided by the authors, I have revised my score to 8.