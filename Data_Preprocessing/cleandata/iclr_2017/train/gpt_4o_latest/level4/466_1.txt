This paper establishes some theoretical guarantees for identity parameterization by demonstrating that: 1) arbitrarily deep linear residual networks are free of spurious local optima, and 2) residual networks with ReLU activations possess universal finite-sample expressivity. The paper is well-written and addresses a fundamental problem in deep neural networks. Overall, I am highly positive about this work and consider the results to be quite significant, as they effectively demonstrate the stability of auto-encoders. This is particularly noteworthy given the inherent difficulty in providing concrete theoretical guarantees for deep neural networks.
A key question that arises is how the results presented in this paper can be extended to the case of more general nonlinear activation functions.
Minor comment: one line before Eq. (3.1), clarify whether \( U \in \mathbb{R}^{? \times k} \).