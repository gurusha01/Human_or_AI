This paper presents a variant of the neural Turing machine (NTM, Graves et al., 2014) that incorporates a mechanism for storing keys and values. The authors explore both continuous and discrete approaches to memory control.
The proposed model is notably complex and appears to rely on numerous techniques to function effectively. Specifically, the cost function includes over 10 distinct terms, and various heuristics are employed to facilitate learning. However, the rationale behind these design choices and optimization strategies is not always clear. Additionally, there is no publicly available code, nor any indication of plans to release it (to the best of my knowledge).
The model is assessed on a series of synthetic tasks (the "bAbI task") and demonstrates performance that is only marginally better than a standard LSTM. However, it falls significantly short of the results achieved by other memory-augmented models introduced in recent years.
Regarding the writing, the paper's description of the model is difficult to follow. The authors describe different components, optimization techniques, and regularization methods in isolation, which makes it challenging to grasp the overall structure. The equations are also hard to interpret due to the use of unconventional notation (e.g., "softplus"), overloaded symbols (e.g., w_t, b), and inconsistent representations (e.g., equations (8-9) versus (10-11)). For instance, some equations are written in scalar form while others are in vector form, and arrows are used in place of equals signs without explanation.
Overall, the model's complexity, lack of available code, and insufficient detail in the paper make it difficult to reproduce the results. Furthermore, the performance on the bAbI tasks is underwhelming compared to other memory-augmented models.
The following questions and clarifications are suggested for the authors:
1. The model is intricate, and reproducing it based solely on the description in the paper may be challenging. Are there plans to release the code, and if so, what is the estimated timeline for its release?
2. The paper introduces multiple cost functions and regularization terms. Could the authors provide a concise summary of the overall cost function minimized by the model?
3. Some variables, such as w_t and b, appear to have inconsistent definitions. Could the authors clarify their usage?
4. The paper states that "gammat is a shallow MLP." Could the authors elaborate on this? Is gammat a function or a variable? Equation (10) suggests it is a vector (or possibly a scalar)—could this be clarified?
5. Regarding "curriculum learning for the discrete attention," could the authors compare their approach to simpler alternatives, such as rounding continuous attention?
6. In the introduction, the authors claim that "it is possible to use the discrete non-differentiable attention mechanism," citing Zaremba & Sutskever (2016), which employs REINFORCE and a simple controller. However, in Section 4, the authors state that "training discrete attention with a feed-forward controller and REINFORCE is challenging." These statements seem contradictory—could the authors address this?
7. The introduction states that "Memory network (Weston et al., 2015b) [..] uses an attention-based mechanism to index them." However, to the best of my knowledge, it was Sukhbaatar et al. (2015) who introduced the attention mechanism to memory networks. Could the authors clarify this point?
8. The introduction also claims that "memory networks […] [are] used in real tasks (Bordes et al., 2015; Dodge et al., 2015)." However, both papers reference memory network systems that differ from Weston et al.'s original formulation. This statement is misleading—could the authors revise or clarify it?