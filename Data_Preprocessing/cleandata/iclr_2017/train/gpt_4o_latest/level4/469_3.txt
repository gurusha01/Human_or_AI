This paper addresses the challenge of compressing trained convolutional networks to reduce memory usage and accelerate the forward pass. The primary contribution, as I interpret it, is the development of efficient convolution routines tailored for sparse convolutional weights under general sparsity conditions (as opposed to structured sparsity). The authors validate their approach using both AlexNet and GoogLeNet across multiple platforms and have made their code publicly available. The paper is well-written and effectively situates this work within the broader context of prior model compression techniques.
My main suggestion for the authors is to include a clear and concise summary of the speedup and memory savings achieved by their method in comparison to existing approaches. While the paper presents the sparsity levels achieved through different pruning strategies, it is not immediately clear how these results translate into performance improvements relative to other methods.