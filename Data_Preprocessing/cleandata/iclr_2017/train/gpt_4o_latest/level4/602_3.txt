SUMMARY
The paper introduces a machine reading framework designed for cloze-style question answering.  
The proposed system begins by encoding both the query and the document using a bidirectional GRU. These encoded representations are then integrated through a Gated Attention (GA) mechanism.  
GA computes the compatibility between each word in the document and the query, producing a probability distribution.  
For every word in the document, a gating mechanism is applied, weighting the query representation based on the computed compatibility.  
The gate is subsequently used to modulate the GRU-encoded document word representations.  
The resulting word vectors are then re-encoded through another bidirectional GRU.  
This process is repeated over multiple hops. After k hops, the likelihood of a word being part of the answer is determined using a log-linear model, which takes as input the final word representations along with the concatenated query representations from before and after the cloze token.  
The overall probability of a candidate being the correct answer is derived from a linear combination of the individual word probabilities.
The model is evaluated on four distinct datasets.  
The results demonstrate that the proposed approach achieves state-of-the-art performance on three out of the four benchmarks.
---
OVERALL JUDGMENT
The paper's primary contribution lies in the gated attention mechanism, which I find to be a straightforward yet compelling idea.  
The paper is well-structured, and the ablation study convincingly highlights the advantages of the gated attention mechanism.  
The GA Reader, as a complete model, surpasses prior state-of-the-art approaches on three benchmarks and shows promising results on the CBT dataset as well.  
However, I would have appreciated some discussion on why the model performs less effectively on the CBT dataset.
---
DETAILED COMMENTS
Minor: In the introduction, it should be noted that Weston et al., 2014 do not employ any attention mechanism.