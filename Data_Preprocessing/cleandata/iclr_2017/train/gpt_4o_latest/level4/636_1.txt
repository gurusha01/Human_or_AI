This paper introduces a novel initialization method tailored for specific architectures and a correction mechanism for batch normalization to address variance issues induced by dropout. While the authors present some intriguing observations, the claims lack robust empirical support.
It appears that Figure 1 is based solely on the MNIST dataset, with experiments conducted for only two dropout probabilities and a single network architecture. However, the dataset and experimental setup are not clearly described.
The convergence analysis in Figure 2 is limited to three dropout values, which may lead to an unfair comparison. For example, how does convergence behave for the optimal dropout rate determined via cross-validation? A more comprehensive evaluation with three figures (each showing results for one method across different dropout rates, including the best cross-validated result) would provide better insights. Additionally, details on the corresponding validation error and the number of test iterations are missing. Relying exclusively on MNIST raises concerns about the generalizability of the findings to other benchmarks.
In Figure 3, the results for the Adam optimizer appear closer. However, the learning rate was not optimized using techniques like random search or Bayesian optimization, and the learning rate decay schedule and regularization coefficient were fixed without tuning. A more thorough parameter tuning process might close the observed performance gap. Furthermore, the Nesterov-based competitor shows unreasonably poor accuracy compared to recent results, suggesting that this experiment may not be reliable and should be reconsidered.
In Table 2, the improvement on CIFAR-10 is negligible. For CIFAR-100, the difference is not significant without incorporating batch normalization variance re-estimation. However, there are no results provided for the "original with BN update," making it unclear whether the BN update offers general benefits. Similarly, for the SVHN dataset, results for the "original with BN update" are also missing.
To substantiate the claims, baselines incorporating batch normalization should be included in Figures 1, 2, and 3. The primary critique of batch normalization, as cited from Mishkin et al. (2016), is its additional computational cost. However, this should not preclude comparisons with batch normalization. In fact, Mishkin et al. (2016) provide comparisons to batch normalization, both with and without data augmentation, using recent state-of-the-art architectures.
None of the experiments incorporate data augmentation, leaving it unclear whether the proposed initialization or batch normalization update would improve or degrade performance in such scenarios.
Recent state-of-the-art methods, such as ResNet variants and DenseNet, scale effectively to deeper architectures and report results on ImageNet. While the authors suggest that their approach could be extended to residual network variants, it remains uncertain whether such extensions would yield any empirical benefits for these architectures.
This work requires a more comprehensive and fair comparison to establish its significance. Without this, the contribution remains limited. 
- Why are recent state-of-the-art methods, such as ResNet variants or DenseNet, not included in the comparisons in Section 2?  
- The parameters were selected from a small set of fixed values. However, tuning via random search or Bayesian optimization is a standard practice for meaningful comparisons. At present, it is difficult to discern whether the observed differences are genuinely attributable to the proposed approaches or to suboptimal parameter tuning.  
- Are there any results available for ImageNet?