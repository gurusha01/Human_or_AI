This paper provides a theoretical justification for the faster convergence (in terms of the average gradient norm achieved after processing a fixed number of samples) when using smaller mini-batches for SGD or ASGD with fewer learners. This suggests an inherent inefficiency in the speed-up achieved by parallelizing gradient descent methods to leverage hardware capabilities. Overall, the paper is well-executed and establishes a connection between algorithm design and hardware characteristics.
My primary concern is that Lemma 1 appears to be incorrect. Specifically, the factor \( Df / S \) should, in my view, be \( Df / (S \cdot M) \). Please clarify this point and verify the subsequent theorem accordingly.