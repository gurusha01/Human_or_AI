This paper introduces an extension to the GAN framework called GAP, where multiple generators and discriminators are trained concurrently, and their pairings are periodically shuffled based on a predefined schedule.
Pros:
+ The proposed method is straightforward and easy to implement.
Cons:
- The paper is difficult to follow in its current form.
- The results are indicative but fail to definitively demonstrate a performance improvement for GAP.
The paper's central claim is that GAP enhances convergence and improves mode coverage. While the coverage visualizations are intriguing, they do not provide sufficient evidence to confirm that GAP indeed improves coverage. Similarly, the effect of GAP on convergence is hard to evaluate solely from the presented learning curves. The proposed GAM-II metric has a circular dependency, as the model's performance is influenced by the set of baseline models it is compared against. Using AIS for likelihood estimation appears to be a promising evaluation method, as does the Inception score.
A more systematic approach to assess GAP's impact could involve conducting a grid search over hyperparameters and training an equal number of GANs and GAP-GANs for each configuration. Plotting a histogram of the final Inception scores or likelihood estimates for the trained models could provide clearer evidence of whether GAP consistently yields better models. Overall, the approach shows potential, but the paper leaves too many unresolved questions in its current state.
* Section 2: The phrase "Remark that when..." appears to be a placeholder or incomplete note.
* Section A.1: The description of the proposed metric lacks sufficient detail.