SUMMARY  
This paper introduces a method for enhancing medical concepts by incorporating their parent nodes from an ontology.  
The approach utilizes an attention mechanism over the parent nodes of a medical concept to generate a more comprehensive representation of the concept itself.  
The underlying idea is that for rare medical concepts, the attention mechanism will prioritize general concepts higher up in the ontology hierarchy, whereas for frequent concepts, it will concentrate on the specific concept.  
The attention mechanism is jointly trained with a recurrent neural network, and the model's performance is evaluated on two tasks.  
The first task involves predicting diagnosis categories at each time step, while the second task predicts the likelihood of heart failure occurring after the T-th step.  
The results demonstrate that the proposed model performs effectively under conditions of data scarcity.  
---
OVERALL JUDGMENT  
The proposed model is straightforward yet intriguing.  
The ideas presented have potential for further exploration, though there are areas where the authors could have improved.  
The method for learning representations of concepts within the ontology is somewhat simplistic; for instance, the authors could have employed knowledge base factorization techniques or graph convolutional methods to learn the concepts.  
It is unclear why general factorization methods for knowledge bases would not be applicable in the context of ontology learning.  
Additionally, I found it odd that the representations of leaf nodes are fine-tuned while those of inner nodes are notâ€”was there a specific reason for this design choice?  
In terms of presentation, the paper is well-written, and the qualitative evaluation is insightful.  
---
DETAILED COMMENTS  
- Figure 2: Please ensure consistent image format and resolution.