This paper introduces a method to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The authors argue that this approach simplifies learning and enables the network to dynamically allocate computation.
The core concept of the paper is straightforward and well-articulated. It represents a natural simplification of highway networks, facilitating the "shutting off" of layers while minimizing the number of additional parameters. However, the paper overlooks some critical aspects. First, it does not acknowledge that the gates in highway networks are data-dependent, which could be more expressive than learning a fixed, data-independent gate for all units. Second, the paper does not provide a fair comparison with highway networks to demonstrate that the proposed simpler approach is indeed easier to optimize.
Did the authors experiment with their proposed design of u = g(k)f(x) + (1 - g(k))x, where f(x) is a plain layer instead of a residual layer? According to the paper's reasoning, this setup should work well. If it was not tested, why not? If it was tested and failed, does this suggest that the arguments presented in the paper are incomplete or flawed?
For the MNIST experiments, the fixed hyperparameters make the plots potentially misleading if the models' performance depends on hyperparameter choices. This experiment seems inspired by Srivastava et al. (2015) and is presumably designed to evaluate optimization at extreme depths. If so, the authors should conduct a hyperparameter search and avoid using regularization techniques like dropout or batch normalization, as these are not part of the theoretical framework for the proposed architecture.
In the CIFAR experiments, the reported improvements over the baseline (wide ResNets) are marginal. It is therefore crucial to include standard deviations (or report all results) for both cases to assess whether the differences are statistically significant.
Regarding g(): Was g() consistently implemented as ReLU? If so, does this pose a risk of g(k) becoming 0 and being unable to recover? Additionally, does this imply that in the wide ResNet experiment shown in Fig. 7, most residual blocks are effectively zeroed out due to k < 0?