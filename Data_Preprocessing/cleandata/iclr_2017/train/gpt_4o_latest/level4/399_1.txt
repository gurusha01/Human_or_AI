This paper introduces a method to significantly increase the number of parameters in a single layer while maintaining computational costs comparable to (or even lower than) current state-of-the-art (SOTA) models. The approach leverages a large mixture of experts (MoE)—composed of smaller networks—where only a subset is adaptively activated through a gating network. While the concept itself is intuitive, the primary novelty lies in the design of the gating network, which is optimized to fulfill two key objectives: ensuring all experts are utilized (importance) and distributing computation evenly across them (load). 
The paper also presents two techniques aimed at increasing the batch size processed by each expert, thereby maximizing GPU parallelization. Experiments applying the proposed method to RNNs in a language modeling task demonstrate that it surpasses SOTA results with significantly reduced computation, achieved by selectively leveraging a much larger parameter space. Furthermore, results on machine translation reveal that a model with over 30 times the number of parameters can outperform SOTA while requiring only half the effective computation.
I have several comments regarding the paper:  
- The presentation could be improved. Although the paper spans 11 pages (which I find excessive), Section 3.2—the core of the paper—requires better motivation and a more intuitive explanation. For instance, Equation 8 warrants a more detailed discussion than it currently receives. Space for this could easily be reclaimed by moving experimental details (e.g., architecture and training specifics) to the appendix for interested readers. Additionally, the experiments section could be better structured by completing one experiment before transitioning to the next. There are also minor issues in the writing, such as the abrupt ending of Section 3.1.  
- The paper omits some key references in the area of conditional computation (e.g.,