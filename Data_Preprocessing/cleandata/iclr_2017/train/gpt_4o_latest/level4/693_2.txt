This paper introduces a compelling approach for quickly adapting generative models in low-data scenarios. The core idea leverages techniques from one-shot learning, particularly drawing inspiration from matching networks. To achieve this, the authors propose the generative matching networks (GMN) model, which functions as a variational auto-encoder conditioned on an input dataset. When presented with a query point, the model employs an attention mechanism in an embedding space to match the query point to points in the conditioning set, akin to the methodology used in matching networks. The experiments on the Omniglot dataset demonstrate that the proposed method can effectively adapt to new input distributions with limited examples.
While the method is undoubtedly interesting, my primary concern lies with the paper's lack of clarity. I elaborate on specific points below, but overall, I found the paper somewhat challenging to follow. Key implementation details appear scattered throughout the text, and after reading, I am not confident that I could replicate the method or results. I recommend consolidating the main implementation details into a dedicated section and explicitly defining the functional forms of the various embedding functions and their variants.
I was slightly disappointed to see that weak supervision, in the form of labels, was required. How would the method perform in a fully unsupervised setting? This could serve as an intriguing baseline for comparison.
There is a notable lack of definitions for the various functions used in the model. Providing basic insights into the functional forms of \(f\), \(g\), \(\phi\), \(\text{sim}\), and \(R\) would greatly enhance clarity. Without this, it is difficult to fully understand the mechanics of the approach.
In Section 3.2, the statement "only state of the recurrent controller was used for matching" is unclear. My interpretation, after multiple readings, is that the pseudo-input replaces a standard input. Is this correct? If so, this section requires clearer exposition. Additionally, in Section 4.2, it becomes evident that there are two versions of the model: one using a pseudo-input and another conditional version without it. What distinguishes these two versions in terms of the functional forms of the embeddings \(f\) and \(g\)? This distinction should be explicitly clarified.
The phrase "since the result was fully contrastive we did not apply any further binarization" is ambiguous. What does it mean for a result to be "fully contrastive"? This should be explained in more detail.
For consistency, the figures and tables refer to the number of shots, but this term is never explicitly defined. I assume this corresponds to \(T\). This should be clarified and made consistent throughout the paper.
Regarding Figure 2, why is the value of \(T\) only 9 in this instance? What does \(T = 0\) signify? Earlier, it is mentioned that \(T\) can go up to 20 (assuming shots corresponds to \(T\)). Additionally, the results appear to improve with an increasing number of steps. It would be informative to include results for 5 and possibly 6 steps as well. Presumably, there is a point where diminishing returns occur, and it would be valuable to identify this threshold.
In Table 1, is the VAE baseline a fair comparison? The paper mentions that \(C{\text{test}}\) influences \(Pd()\) during evaluation. Since the VAE lacks an associated \(C_{\text{test}}\), does this imply that the two models are evaluated using different metrics? The authors should clarify this point to ensure that the comparison is fair and consistent.
MNIST is a more commonly used dataset than Omniglot for evaluating generative models. Would it be feasible to conduct similar experiments on MNIST? This would allow for comparisons with a broader range of existing models.
Furthermore, the negative log-likelihood values monotonically decrease as the number of shots increases. Is there ever a scenario where increasing the number of shots negatively impacts performance? What happens when \(T = 30\) or \(T = 40\)? Exploring these cases could provide additional insights.
Finally, as a minor grammatical note, the paper is missing determiners in several sentences. Additionally, the model is referred to as "she" instead of "it" at one point. The phrase "on figure 3" should be revised to "in figure 3" in the experiments section.