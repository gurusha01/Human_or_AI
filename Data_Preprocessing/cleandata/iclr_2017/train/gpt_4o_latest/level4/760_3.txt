This paper introduces a generative model tailored for binary images. Images are constructed by placing a collection of binary features at specific locations, which are then combined using an OR operation to form the final image. In a hierarchical extension, features or classes are associated with multiple potential templates, one of which can be activated. Variables are introduced to govern the selection of active templates at each layer. A joint probability distribution is defined over both the feature appearance and the instance/location variables.
The overarching aim of this work is compelling â€” achieving the extraction of semantically meaningful features that enable compositionality in a generative image model would be a significant accomplishment. However, it is not evident that the proposed approach would inherently lead to such outcomes. Why should the learned features (building blocks) be semantically meaningful? For instance, in the motivating example of text, instead of discovering letters, the features could correspond to other sub-units (e.g., parts of letters) or elements that lack direct semantic interpretation.
The current implementation of the model has notable limitations. It is restricted to binary image patterns, and the experiments are conducted on synthetic datasets and MNIST digits. While the method successfully recovers structure and performs well on classification tasks for synthetic data that are explicitly compositional, its performance on MNIST is suboptimal. Test errors are significantly higher compared to a CNN, except in scenarios where synthetic data corruption is introduced. Further advancements to improve the model's ability to handle natural images or naturally occurring data variations would strengthen the paper.