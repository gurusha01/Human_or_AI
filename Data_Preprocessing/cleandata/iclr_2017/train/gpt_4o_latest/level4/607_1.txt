The authors present a "hierarchical" attention model for video captioning, which is structured into three components: the temporal modeler (TEM), which processes the video sequence and produces a sequential representation for the HAM; the hierarchical attention/memory mechanism (HAM), which employs a soft-attention mechanism over the sequential video representation; and a decoder that generates the final caption.
In relation to the second set of questions above, the authors appear to describe their use of an LSTM (or a comparable RNN) as the output of the Bahdanau et al. (2015) attention mechanism under the term "hierarchical memory mechanism." While I can understand this terminology, particularly given that recent trends in memory-based models often overlook the implicit memory within the LSTM state vector, this framing seems to overstate the significance of the contribution made by this paper.
I commend the authors for including an ablation study in Table 1, as such analyses are often overlooked by researchers. However, the results indicate that the contributions of the proposed components are not clearly established. Specifically, the justification for the inclusion of the TEM appears to be relatively weak.
Regarding the quantitative evaluation in Table 2, the authors define a narrowly tailored set of features to identify "fair" comparators from the literature. Considering the diversity of models and alternative training datasets available, I believe the work would be more compelling if the authors focused on achieving the best possible results, even if that involves fine-tuning the frame model. Since the primary value of this work lies in its application, incorporating elements that can substantially enhance performance would seem appropriate.
In summary, I currently do not find the contributions of this paper sufficient to merit publication at ICLR.