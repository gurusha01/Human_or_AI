Review - Paraphrased Version:
Description:
This paper introduces a reinforcement learning framework in which a meta-controller, guided by "natural-language" input, selects subtasks and conveys them to a subtask controller. The subtask controller then determines primitive actions based on the assigned subtask. The overarching aim is to enable reinforcement learning agents to handle large-scale tasks more effectively.
The subtask controller processes the subtask definition (arguments) by embedding them into vectors using a multi-layer perceptron, which incorporates an "analogy-making" regularization mechanism. These subtask vectors are integrated with inputs at each layer of a convolutional neural network (CNN). The CNN outputs, conditioned on both the observation and the subtask, are subsequently passed to one of two multi-layer perceptrons (MLPs): one to compute action probabilities for the policy (via an exponential falloff of MLP outputs) and the other to estimate termination probabilities (using a sigmoid function applied to the MLP outputs).
The meta-controller processes a list of instructional sentences, embedding them into a sequence of subtask arguments, which may not correspond one-to-one with the instructions. A context vector is generated by a CNN, taking into account the observation, the embedding of the previous sentence, the prior subtask, and its completion status. Subtask arguments are derived from this context vector through mechanisms involving memory pointer-based instruction retrieval and hard/soft decisions on whether to update the subtask.
The training process employs a combination of policy distillation and actor-critic training for the subtask controller, while the meta-controller is trained using actor-critic methods with the subtask controller kept fixed.
The system is evaluated in a grid world environment where the agent navigates, interacts with, and transforms various items or enemies. Comparisons are made against: (a) a flat controller without a subtask controller, and (b) a subtask controller that simply concatenates the subtask embedding with the input, with and without the analogy-making regularization.
Evaluation:
The proposed architecture appears conceptually sound, but it is unclear why the specific approach to integrating subtask embeddings within the subtask controller is the most appropriate or optimal choice.
The grid world environment used in the experiments does not convincingly represent a "large-scale task," as the grid's 10x10 size is quite limited. This is particularly disappointing given that scaling to large tasks was a primary motivation for the work.
Additionally, the method is not compared against any state-of-the-art alternatives, which is a significant limitation. This issue is compounded by the fact that the experiments are conducted on custom tasks rather than established benchmarks. As a result, it is difficult to contextualize the performance of the proposed approach relative to prior work.