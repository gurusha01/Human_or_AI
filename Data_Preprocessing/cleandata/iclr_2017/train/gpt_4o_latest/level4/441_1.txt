Paraphrased Review:
TLDR: The authors introduce Variable Computation in Recurrent Neural Networks (VCRNN), a method conceptually similar to Adaptive Computation Time (Graves et al., 2016). The core idea is that, at each timestep, only a subset of the state in a vanilla RNN is updated, a process referred to as "variable computation." However, the experimental results are unconvincing, and there is limited comparison to prior work and a lack of evaluation against a basic LSTM baseline.
---
Gating Mechanism  
In VCRNN, a gating mechanism is implemented via an mt vector generated at each timestep. Based on this mt vector, a subset of the vanilla RNN state—specifically, the first D states (referred to as "D-first")—is gated for updates. However, the authors introduce additional hyperparameters, epsilon and \bar{m}, without providing their values, explaining how they were chosen, or discussing their sensitivity and importance.
While the gating mechanism is novel, it feels somewhat inelegant and ad hoc. Restricting updates to only the D-first states seems arbitrary and less principled compared to a more generalized approach that could allow updates to any subset of the state. Additionally, it would be helpful to include a brief comparison to the soft-gating mechanisms used in GRUs, LSTMs, and Multiplicative RNNs (Wu et al., 2016).
---
Variable Computation  
The authors argue that VCRNN can reduce computational costs compared to vanilla RNNs. While this claim may hold in theory, it is unlikely to translate into practical benefits. The RNN sizes used in the experiments are too small to saturate modern GPU cores, meaning any computational savings are unlikely to result in noticeable reductions in wall-clock time. Furthermore, the authors do not report wall-clock time measurements, making this argument difficult to substantiate.
---
Evaluation  
The evaluation lacks sufficient comparisons to prior work and stronger baselines. Specifically, the absence of comparisons to a simple stacked LSTM architecture is a significant oversight, as LSTMs are now a standard baseline for recurrent models. 
The results on PTB BPC are also disappointing, as VCRNN fails to outperform even the vanilla RNN baseline. Moreover, the authors only compare their model to a basic RNN architecture, neglecting the many advancements in recurrent architectures since then. For instance, Chung et al. (2016) provide a comprehensive comparison in Table 1, including experiments on PTB BPC, and cite several important contributions that are missing here.
One interesting experiment presented by the authors is the visualization of per-character computation in VCRNN (Figure 2). The graph reveals that computation increases after spaces or word boundaries, which is intriguing. However, this raises the question of whether GRUs or LSTMs exhibit similar behavior. For example, what is the magnitude of state changes in GRUs or LSTMs after a space? It would be valuable to investigate this for comparison.
---
Minor Comments  
- Please include equation numbers in the paper to facilitate easier referencing during reviews and discussions.
---
References  
- Chung et al., "Hierarchical Multiscale Recurrent Neural Networks," 2016.  
- Graves et al., "Adaptive Computation Time for Recurrent Neural Networks," 2016.  
- Wu et al., "On Multiplicative Integration with Recurrent Neural Networks," 2016.