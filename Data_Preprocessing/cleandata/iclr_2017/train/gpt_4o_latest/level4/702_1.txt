This paper introduces two models for extractive document summarization: the classifier architecture and the selector architecture. These models rely on either classification or ranking in a sequential manner to select candidate sentences for summarization. The experimental results demonstrate that the proposed approaches either outperform or are comparable to the state-of-the-art (SOTA) methods.
Technical comments:
- In equation (1), there is a position-related component referred to as "positional importance." How critical is this component? Could the authors provide results without this component to assess its impact? This is particularly relevant for the discussion on document structure, especially when the model is trained on shuffled document orders but tested on the original order.
- Regarding equation (1), is the content-richness component truly essential? The score function already includes a salience term, which seems sufficient to measure the importance of $h_j$ relative to the entire document.
- For the dynamic summary representation in equation (3), why not employ the same update equation during both training and testing? During testing, the model has access to the decisions made by the decoder up to that point. Using a consistent update mechanism across training and testing could improve the model's performance.
- Section 5 is, in my opinion, the most compelling part of the paper, as it effectively highlights the differences between the two architectures and provides convincing evidence for their respective strengths.
- However, the decoding algorithm employed in this paper feels overly simplistic. At the very least, both architectures could have utilized beam search, which might have yielded better results.