This paper introduces a meta-learning algorithm designed to learn generative models from a small number of examples. The approach shares structural similarities with the matching networks proposed by Vinyals et al. (2016) and is trained within a meta-learning framework where the inputs are datasets. The authors present results on the Omniglot dataset, evaluating performance in terms of log-likelihoods and generated samples.
While the proposed idea appears promising, I found several aspects of the paper difficult to comprehend. The presentation is challenging to follow, partly because existing methods are described using terminology that diverges significantly from that of the original authors. Most critically, it is unclear which contributions are intended to be novel, as the discussion of matching networks is limited to a few sentences, despite the fact that this work heavily builds upon them. (I raised this concern in my Reviewer Question, but the revised version of the paper has not addressed this issue satisfactorily.)
The meta-learning setup is another source of confusion. A natural formulation for meta-learning in the context of generative models would involve using small datasets X as inputs and predicting the distribution from which X was sampled. However, this would imply a uniform weighting of data points, which differs from the proposed approach. Section 3.1 suggests the inclusion of some form of query q, but its role and significance remain unclear.
Regarding experimental validation, the paper does not include comparisons with prior work. This omission is problematic, as several other methods with similar objectives have already been proposed, and such comparisons are necessary to contextualize the contributions of this work.