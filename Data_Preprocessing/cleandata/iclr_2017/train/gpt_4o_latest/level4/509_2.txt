This paper introduces a method for structured program induction using program sketches in Forth, a simple stack-based programming language. The authors reformulate the broad and challenging problem of program induction into a slot-filling problem, leveraging a differentiable Forth interpreter that allows backpropagation through the slots, which are treated as random variables. The use of sketches or partial programs enables the learning of more complex programs compared to starting from scratch without prior information. The loss function optimized end-to-end through the program flow is the L2 (RMSE) error between the program memory at targeted (non-masked) addresses and the desired output. The authors demonstrate their approach by learning addition and bubble sort, employing both a Permute (3-way) sketch and a Compare (2-way) sketch.
The concept of making a programming language fully differentiable to enable the completion of partial programs (sketches) has been previously explored in the probabilistic programming community and more recently with TerpreT. The choice of Forth as the sketch definition language is particularly intriguing, as it occupies a middle ground between low-level machine code (e.g., Neural Turing Machines, Stack RNNs, Neural RAM approaches) and higher-level languages (e.g., Church, TerpreT, ProbLog).
Section 3.3.1, along with Figure 2, could benefit from greater clarity. Specifically, the color coding and the parallel between D and the input list should be explicitly explained to improve comprehension.
The experimental section is relatively sparse. For instance, in the sorting task, there is only one experimental setting (training on length 3 and testing on length 8). There is no analysis of the length at which generalization breaks (if it does), nor any investigation into the "relative runtime improvement" with respect to the training set size or the length of input sequences. Additionally, the paper lacks baselines for comparison, such as exhaustive search or neural approaches, which would strengthen the evaluation. Similarly, the "addition" experiment (Section 4.2) is described very briefly, and no baselines are provided, even though addition is a standard benchmark for neural program induction approaches. For example, when the authors state, "The presented sketch, when trained on single-digit addition examples, successfully learns the addition, and generalizes to longer sequences," it is unclear whether this generalization extends to three-digit numbers or beyond.
In summary, while the paper presents an intriguing and promising approach, the experimental results do not sufficiently support the claims or demonstrate the practical utility of the method.