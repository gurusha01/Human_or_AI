This study addresses conditional image synthesis within the autoregressive framework. Building upon PixelCNN, it trains models conditioned on text, segmentation masks, or keypoints. The experiments include keypoint-conditioned synthesis on the CUB (birds) and MHP (human pose) datasets, as well as segmentation-conditioned synthesis on the MS-COCO (objects) dataset. The primary contribution lies in extending PixelCNN to support keypoint and segmentation conditioning. The paper also provides a qualitative comparison to GAN-based synthesis methods.
Strengths:
1. The paper showcases enhanced image generation capabilities within the autoregressive framework, indicating its potential to remain competitive with the latest advancements in GANs.
2. The qualitative comparison in Figure 9 highlights that PixelCNN and GAN-based methods exhibit distinct failure modes, with PixelCNN appearing more robust against artifact introduction.
3. Efforts are made to establish quantitative evaluation through log-likelihood metrics, as presented in Table 1.
Weaknesses:
1. Comparisons to other methods are limited to qualitative results, which can be somewhat challenging to interpret. Supplementary material or an appendix with additional examples could help address this limitation.
2. Extending PixelCNN to incorporate conditioning on additional data is relatively straightforward. While this represents a solid engineering effort, it does not introduce a particularly novel or surprising concept.