This heuristic aimed at enhancing gradient descent in image classification is straightforward and effective. However, it initially seemed more suited for a workshop track paper. The original submission demonstrated the algorithm solely on a single task (CIFAR) and lacked theoretical grounding, leaving its generalizability to other tasks uncertain.
As someone working on DNNs for NLP, I noted that some observations in the paper contradicted my own experience. Specifically, in architectures combining diverse layer types (embedding, RNN, CNN, gating), I have found that ADAM-type optimizers significantly outperform simple SGD with momentum. This is primarily because ADAM eliminates the need to manually tune learning rates for each layer type. However, in my experience, ADAM performs optimally only when paired with Polyak averaging, as it tends to exhibit high fluctuations across batches.
Revised Submission:
- The authors have significantly enhanced the paper, including experiments on datasets beyond CIFAR.
- The work now qualifies as breakthrough research, making my earlier classification as a workshop track paper no longer applicable.
Based on these improvements, I have accordingly raised my rating.