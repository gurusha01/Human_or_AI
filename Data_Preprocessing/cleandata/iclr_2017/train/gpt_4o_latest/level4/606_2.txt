The paper introduces a framework for formulating data structures in a learnable manner. This represents an innovative and intriguing approach that has the potential to generalize effectively to a variety of data structures and algorithms. However, in its current form (Revision of Dec. 9th), two significant weaknesses remain: the analysis of related work and the experimental evidence.
Reviewer 2 has already highlighted some of the related work, and in particular, DeepMind (with which I have no affiliation) has produced highly relevant results, such as the neural Turing machine and subsequent work. While direct experimental comparisons may be challenging due to the complexity of re-implementation, it is crucial to at least conceptually discuss and compare the proposed framework with these prior contributions.
The experimental section primarily presents qualitative results, which do not fully or conclusively address the research questions. Below are some suggestions for improvement:
* It would be highly valuable to evaluate the accuracy of the stack and queue structures as the number of elements to store increases.
* Can the queue/stack operate correctly in arbitrary sequences of push-pop operations, even though it was trained exclusively on consecutive pushes or consecutive pops? Does the model eventually "diverge" in this more generalized setting?
* The MNIST-encoded elements, despite being represented in a 28x28 (binary?) space, belong to a ten-element set and could therefore be encoded far more efficiently through "parsing," which CNNs are well-suited to perform. Is the neural network simply learning to perform this parsing task? If so, its performance is likely to degrade significantly when tasked with stacking more than 28Ã—28/4=196 numbers (assuming optimal parsing and lossless encoding). To explore this hypothesis, experiments with increasing numbers of stack/queue elements would be necessary. Incorporating an MNIST parsing neural network as a preprocessing step to the stack/queue network could help validate or refute this claim.
* The claims regarding "mental representations" are weakly supported throughout the paper. If evidence supporting their correspondence to mental models could be provided, it would strengthen this claim. Otherwise, it would be advisable to remove these assertions and focus on the neural network aspects, perhaps mentioning mental models only as a motivating concept.