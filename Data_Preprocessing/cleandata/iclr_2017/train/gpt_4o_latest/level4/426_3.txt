This paper builds upon prior work to establish a mapping between the word embedding spaces of two languages. The embeddings are independently trained on monolingual datasets, and various forms of bilingual information are leveraged to learn the mapping. The resulting mapping is then employed to evaluate the precision of translations.
The authors introduce two key modifications in this work: "CCA" and "inverted softmax." However, based on the results presented in Table 1, CCA outperforms Dina et al. in only 1 out of 6 cases (It/En @1). The majority of the observed improvements appear to stem from the introduction of the inverted softmax normalization.
Overall, I question the novelty of this paper. Specifically:
 - Faruqui & Dyer (2014) already employed CCA and dimensionality reduction.
 - Xing et al. (2015) previously argued that Mikolov's linear mapping should be orthogonal.
Could you clarify how your work differs from Faruqui & Dyer (2014), aside from applying the method to measure translation precision?
The use of cognates instead of a bilingual dictionary is an interesting approach. Could you elaborate on how you constructed this list of cognates? It seems this method is inherently limited to languages that share the same alphabet (e.g., it would exclude Greek and Russian).
Additionally, the term "cognate" in linguistics typically refers to words with a shared etymological origin, which do not necessarily share the same written form (e.g., night, nuit, noche, Nacht). Perhaps a different term would be more appropriate? It seems likely that the words you are referring to are proper names extracted from news texts.