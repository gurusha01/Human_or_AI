This paper introduces the Neural Graph Machine, which incorporates graph regularization into neural network hidden representations to enhance learning and account for graph structures. However, the proposed model appears to be largely similar to the approach presented by Weston et al. (2012).
In their responses to the review questions, the authors have highlighted a few contributions that were not addressed in prior work:
1. They demonstrated that graph-augmented training can be applied to a variety of network architectures, including feedforward networks, CNNs, and RNNs, across a range of tasks.
2. They showed that incorporating graphs can lead to more efficient network training, e.g., a 3-layer CNN with graph augmentation performs comparably to a 9-layer CNN without it.
3. They established that graph-augmented training is effective across different types of graphs.
Nevertheless, these contributions seem to represent extensions or applications of the graph-augmented training concept rather than fundamental innovations. As such, it may be more appropriate to frame this work as an empirical study applying the model proposed by Weston et al. (2012) to diverse problems, rather than presenting it as a novel model under the name Neural Graph Machine.