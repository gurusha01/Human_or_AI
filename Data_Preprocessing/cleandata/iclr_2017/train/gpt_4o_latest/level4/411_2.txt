This paper presents an engaging and well-executed study on superoptimization, building upon the stochastic search framework of STOKE by introducing a learned stochastic search. In this approach, the STOKE proposals are generated by a neural network that takes program embeddings as input. The authors employ the REINFORCE algorithm to train an MCMC scheme aimed at minimizing the final program cost.
The manuscript is well-written, and the results effectively demonstrate the method's performance.
Comments/Questions:
- Is it correct to interpret that, within the stochastic computation graph, only the features-to-proposal mapping is learned, while the remainder of the process remains the original STOKE MCMC scheme? If so, does this mean that the 'uniform' model essentially corresponds to STOKE and serves as your baseline? This clarification should be made explicit in the paper.
- Have the authors considered learning the features themselves instead of relying on pre-existing features? While this could be challenging due to the limited data available (potentially hindering the generalization of a feature extractor), it might be worth exploring.
- In a related context, the paper 'Markov Chain Monte Carlo and Variational Inference: Bridging the Gap' by Salimans et al. discusses treating an MCMC scheme as a stochastic computation graph and optimizing it using a variational (i.e., reinforcement learning) criterion. Although the problem differs and focuses on HMC rather than MCMC, it could be valuable to cite this work as an example of a similar approach to 'meta-optimized' MCMC algorithms.