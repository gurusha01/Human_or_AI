This paper introduces a novel approach for adapting a neural network to a new task with limited training data. The dominant method in this area is fine-tuning. However, the authors propose learning a network that extracts features complementary to those of a fixed, pre-trained network. Furthermore, they explore a configuration where the new network and features are "stitched" to the pre-trained network at different hierarchical levels, rather than being structured as a parallel "tower."
The proposed method shares conceptual similarities with the Progressive Nets work by Rusu et al., as acknowledged by the authors. However, the motivations and experimental setups differ, giving this submission its own merit.
The idea of learning a "residual" through stitched connections bears a conceptual resemblance to ResNet. A comparison or contrast with ResNet approaches would strengthen the paper.
The use of a batch multiple times during training (five times in this case) is unconventional. It would be helpful to clarify whether this approach outperforms standard stochastic gradient descent (SGD).
In Figure 5, labeling the y-axis would improve clarity. Additionally, the figure would be more effective if it followed the format of Figure 4, which is significantly more readable than the current bar chart.
Regarding Figure 5 again, the concept of an "untrained model" is unclear. The rationale behind this approach is not immediately evident. For instance, is TFT-1 simply fine-tuning one additional layer compared to "Retrain Softmax"?
The results presented at the end of Section 3 appear somewhat weak due to the use of a large network. It would be valuable to evaluate how the results change when employing a smaller network.
The authors repeatedly assert that the added connections and layers are designed to learn complementary features, and they provide visual evidence to support this claim. While these visualizations are compelling, they do not constitute definitive proof. The authors might consider incorporating an explicit constraint in their loss function to encourage this behavior, such as a soft orthogonality constraint (assuming intermediate features can be projected to a common feature space). While the use of small L2 regularization might implicitly achieve a similar effect, the paper does not provide evidence or visualizations to confirm this (e.g., by showing what happens when L2 regularization is omitted).
One key question that arose while reading the paper is how an ensemble of two pre-trained networks would perform on the tasks considered. This is particularly relevant for the car classification example, where a strong baseline might involve fine-tuning VGG and ResNet on the task separately, then combining their outputs through a linear combination or simple averaging.
It is disappointing that Figures 4, 5, and 8 lack comparisons to results from prior work. Without such comparisons, it is difficult to contextualize the contributions of this paper relative to existing methods.
Overall, this paper presents an interesting and potentially impactful contribution. The challenge of efficiently reusing pre-trained classifiers for tasks with limited data is highly relevant to the community. While the paper makes meaningful progress in this direction, it falls short in certain areas, such as comparisons with stronger baselines and providing deeper insights into the method's behavior.