This paper proposes a method for learning non-linear activation functions in deep neural networks. The approach involves representing the activation function as a combination of non-linear basis functions and learning the associated coefficients. In this work, the authors specifically employ a Fourier basis. The paper also provides a theoretical analysis of the proposed method, employing algorithmic stability arguments to demonstrate that networks with learned non-linearities exhibit strong generalization behavior (i.e., vanishing generalization error with large datasets).
The primary concern I have with this paper is that expressing a non-linear activation function as a linear or affine combination of other non-linear basis functions appears to be equivalent to constructing a larger network. In this larger network, the basis functions serve as the non-linearities of the nodes, and the weights are subject to specific constraints. Consequently, the advantage of the proposed method—learning non-linearities—over simply optimizing the network's capacity for a given task (while keeping the non-linearities fixed) is unclear. Alternatively, could it be argued that the constraints imposed by the learned non-linearity approach are beneficial in some way?
Another question pertains to the two-stage training process for CNNs. When the ReLU activation is replaced with NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or are its coefficients initialized randomly?
A few minor corrections and questions:
- Page 2: "... the interval [-L+T, L+T] ..." should perhaps be "... the interval [-L+T, L-T] ..."?
- Page 2, equation for f(x): Should the term in both the sine and cosine expressions be "(-L+T) i \pi x / L," or should it omit the "x"?
- Theorem 4.2: The phrase "... some algorithm \eps-uniformly stable ..." should have the word "algorithm" removed.
- Theorem 4.5: The abbreviation "SGM" is undefined.