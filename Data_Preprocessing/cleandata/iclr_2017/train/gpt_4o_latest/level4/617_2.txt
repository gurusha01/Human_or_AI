The authors propose techniques to accelerate gradient descent by utilizing asynchronicity in a layer-wise fashion.
Although their approach achieves up to a 1.7x speedup over synchronous training, the baseline used for comparison is not robust. More critically, the authors disregard parameter-server-based methods, which are increasingly becoming the standard, and consequently fail to benchmark against the current state-of-the-art. Additionally, the paper lacks wall-time measurements. Given these shortcomings, the work is not yet suitable for acceptance at ICLR.