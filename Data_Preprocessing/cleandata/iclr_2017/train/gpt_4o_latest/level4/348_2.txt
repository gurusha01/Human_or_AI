The authors present a parameterization of convolutional neural networks (CNNs) that ensures equivariance with respect to a broad class of geometric transformations.
The mathematical framework is robust, and the content is both novel and highly engaging. The paper is generally well-written, with commendable efforts to make the mathematical concepts accessible, though minor refinements could further enhance clarity.
The theoretical framework is sufficiently general to encompass continuous transformations, although the experiments focus solely on discrete ones. While this might be perceived as a limitation, the authors justify this choice through experiments demonstrating that the selected transformations are effective enough to achieve strong performance on CIFAR.
A related line of work by Lenc & Vedaldi [1] explores equivariance empirically in CNNs, offering a complementary perspective to the theoretical approach taken here.
In addition to the recent references on scale- and rotation-equivariant deep networks suggested below, it would be appropriate to acknowledge earlier work on geometric equivariance from the 2000s. A particularly relevant study is Reisert [2], which investigated steerable filters for invariance and equivariance using Lie group theory. While that work focused on kernel machines rather than CNNs, many of the underlying tools and results are closely related to the current approach.
Some aspects of the notation could be simplified to improve readability:
Working over a lattice \( \mathbb{Z}^d \) feels unnecessarily abstract, as the inputs are always images. Restricting the discussion to \( \mathbb{Z}^2 \) would make much of the mathematics more accessible without sacrificing generality, as the extension to higher dimensions is straightforward. Indeed, the authors revert to 2D lattices later in the paper.
Similarly, it might be more intuitive to eliminate the layer index \( l \) that appears throughout the paper and instead adopt notation for the current and next layers (e.g., \( \pi \) and \( \pi' \), or \( K \) and \( D \) instead of \( K{l+1} \) and \( Kl \)).
While these suggestions are left to the authors' discretion, I strongly encourage them to consider these or other ways to streamline the notation.
A few minor points: Certain claims would benefit from supporting references (e.g., "Explicit formulas exist" on page 5, and the introduction of intertwiners on page 3). Additionally, there is a small error in the Balduzzi & Ghifary reference, where extraneous information has been included as part of an author name.
[1] Lenc & Vedaldi, "Understanding image representations by measuring their equivariance and equivalence", 2015  
[2] Reisert, "Group integration techniques in pattern analysis: a kernel view", 2008