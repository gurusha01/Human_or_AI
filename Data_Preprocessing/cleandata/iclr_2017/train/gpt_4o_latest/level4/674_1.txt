The paper aims to provide a theoretical explanation for the invertibility of deep convolutional neural networks, specifically when reconstructing images from certain intermediate layers. The authors approach this by analyzing the invertibility of a single layer, under the assumption that the convolutional filters correspond to incoherent measurements satisfying the Restricted Isometry Property (RIP).
While this is an intriguing research direction, I believe the paper is not yet ready for publication. The current treatment does not sufficiently address the phenomenon of invertibility in deep neural networks as a whole. Even after considering the authors' response, the results appear to be a minor extension of standard compressive sensing results for sparse reconstruction using incoherent measurements.
The key distinction of a deep neural network lies in its depthâ€”this "deep" aspect is what enables the forward task to succeed. As the authors themselves acknowledge, there is significant degradation when Iterative Hard Thresholding (IHT) is applied recursively. Thus, the proposed theory at best explains the partial invertibility of a single layer, which is not particularly surprising. The more interesting and challenging question is why a cascade of layers remains invertible.
For a theoretical analysis of this phenomenon to be impactful, it must extend beyond the study of a single compressive measurement-type layer and investigate how the theory applies to a cascade of layers. This is crucial because the sparse recovery theory might fail to hold beyond a single layer, and the invertibility of deep networks could instead arise from correlations between the weights of different layers. In its current form, the results for individual layers do not provide sufficient evidence that they contribute to understanding the invertibility of entire networks.