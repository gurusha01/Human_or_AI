Thank you for providing an engaging and thought-provoking manuscript. I appreciate the application of the information bottleneck (IB) principle to deep neural networks, a concept I find particularly compelling. To the best of my knowledge, this is the first paper to leverage the IB framework for training deep networks (as opposed to merely presenting the concept, as in the original works). However, please see my comments below regarding the claim of independent work.
The derivation of the variational lower bound is presented with exceptional clarity, making it accessible even to readers who may not have a strong background in variational inference. Additionally, the explanation of the IB principle is well-articulated. The experimental results are promising and suggest the potential of the proposed approach.
That said, I found the presentation of the model somewhat confusing. In the context of variational inference or information maximization, it is standard to use `p` to denote the model and `q` to represent the "inference engine," which implies that the choice of inference method is independent of the modeling process. However, the presented Variational Information Bottleneck (VIB) assumes `p(x, y)` as the underlying data distribution (approximated by the empirical distribution). Consequently, the model in this case is actually `q(y|z)p(z|x)`. On page 8, paragraph 2 of section 4.2.3, the authors introduce `p(y|x)` as the predictive distribution. Predictive in what sense? I assume the authors intended `p(y|x) = ∫ q(y|z) p(z|x) dz`, but this interpretation creates a contradiction between the two definitions.
The authors have drawn an intriguing connection between the proposed approach and variational autoencoders (VAEs), particularly with the warm-up training strategy (via tuning β). However, even though the loss function formula matches the variational lower bound used in VAEs (when β = 1), the underlying models differ significantly. For instance, in VIB, `r(z)` serves as the variational approximation to `p(z)` (and is not part of the model), whereas in VAEs, `r(z)` represents the prior distribution, which is explicitly defined during the modeling process. Similarly, in VIB, `p(z|x)` is part of the model, while in VAEs, it is the approximate posterior and can be independently chosen (e.g., one could use a deep neural network for `p(x|z)` but a Gaussian process for `p(z|x)`).
In summary, I believe the modeling procedure could be presented more clearly. I encourage the authors to address these points in their revision, as the current presentation may be confusing, particularly for readers with a Bayesian perspective. In the VAE-related discussion, it would be helpful to explicitly highlight the differences between VIB and VAEs and provide intuitive reasoning for why the VIB interpretation might be preferred.
Typos:
- Equations 9–11: Did you mean `q(y|z)` instead of `q(z|y)`?
- Figure 2: In the caption, "as β becomes smaller"—did you mean "larger"?
Claim of Independent Work:
The authors assert that their work is independent of Chalk et al. (2016), which has been available online since May 2016. Given the competitive nature of deep learning research, it is not uncommon for similar ideas to emerge simultaneously. I am inclined to trust the authors' claim and commend their transparency. However, if this claim is found to be inaccurate, I would not recommend the manuscript for acceptance.