The paper presents the joint multimodal variational autoencoder, a directed graphical model designed to handle multimodal data using a shared latent variable. The model is a relatively straightforward extension of the standard VAE, where two data modalities are generated independently from a common latent representation. To address challenges such as missing input modalities or bi-directional inference between modalities, the paper proposes modality-specific encoders trained to minimize the KL divergence between the latent variable distributions of the joint and modality-specific recognition networks. The effectiveness of the approach is demonstrated on the MNIST and CelebA datasets, evaluating test log-likelihoods as well as conditional image generation and editing.
While the proposed method is a straightforward extension of the VAE, the model should naturally inherit the probabilistic inference capabilities of the VAE. For instance, in scenarios with missing data modalities, the model should be able to infer a joint representation and reconstruct the missing modalities through iterative sampling, as proposed by Rezende et al. (2014). Given the marginal improvements observed, I am not fully convinced of the contribution of the modality-specific encoders described in Section 3.3. Furthermore, the inference methods used to generate Figure 5 appear somewhat ad hoc; I would be interested in seeing conditional image generation results obtained using a more principled approach, such as iterative sampling. Additionally, experimental results for joint image-attribute generation are notably absent.