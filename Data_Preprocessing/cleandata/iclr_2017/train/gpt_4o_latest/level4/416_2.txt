Review: A significant portion of the deep learning literature is centered around likelihood-based models. However, maximum entropy approaches represent an equally valid modeling paradigm, where information is specified through constraints rather than direct data. The limited exploration of flexible maximum entropy neural models is surprising but can be attributed to the challenges inherent in optimizing such models. Specifically, this involves (a) determining the impact of constraints on a given distribution and formulating the entropy of that complex distribution. Since there is no unbiased estimator of entropy based solely on samples, an explicit density model becomes necessary. These challenges have historically constrained progress in this area. The authors have identified that invertible neural models offer a powerful framework for addressing the maximum entropy problem, and this paper develops this approach in detail. The key contributions of the paper are: (a) recognizing that normalizing flows, due to their explicit density modeling, enable unbiased entropy estimation; (b) demonstrating that the resulting Lagrangian can be implemented as a relaxation of an augmented Lagrangian; and (c) addressing the practical challenges associated with augmented Lagrangian optimization. To the best of the reviewer's knowledge, this work is novel. The proposed approach is both natural and sensible, and its effectiveness is demonstrated across several models with clear evaluation metrics. While the experiments are sufficient to validate the method as appropriate, they fall short of establishing its necessityâ€”an example where the advantages of the flexible flow transformation are more pronounced would strengthen the case. Further elaboration on computational and scalability aspects would also enhance the paper. The reviewer suspects that this approach is well-suited for model learning but may be less applicable to inferential scenarios where a known model is conditioned on instance-specific constraints. A discussion on the appropriate use cases for this method would be beneficial. Although the issue of theoretical alignment via regularity conditions is raised, the paper provides a clear and thorough treatment, surpassing the level of theoretical discussion typically found in other works in this domain.
Quality: This is a solid paper that introduces a novel foundation for flexible maximum entropy models.  
Clarity: The paper is well-written and clear.  
Originality: The approach is refreshing and innovative.  
Significance: The work is significant for model development, though its broader adoption remains uncertain at this stage.
Minor Issues:  
1. Please ensure all equations are labeled, as others may wish to reference them even if you do not.  
2. On the top of page 4, "algorithm 1" should be capitalized to "Algorithm 1."  
3. The update for \( c \) to address stability appears somewhat opaque and raises mild concerns. Are there residual stability issues? Could you clarify why this resolves all such problems?  
4. The discussion of the support of \( p \) is somewhat superficial. Is the support in Equation (5) an additional condition on \( p \)? If so, encoding this seems non-trivial and is absent in Equation (6). For a Gaussian \( p_0 \) and invertible unbounded transformations, the support being \( \mathbb{R}^d \) is trivial, but for more general cases, this could pose challenges. For instance, in your Dirichlet example, you explicitly map to the required support, but for more complex constraints, achieving this with invertible models and known Jacobians may not be straightforward. It would be helpful to address this issue in the general treatment rather than relegating it to specific examples.
Overall: The reviewer is pleased to see this important question being addressed with such a natural and well-thought-out approach.