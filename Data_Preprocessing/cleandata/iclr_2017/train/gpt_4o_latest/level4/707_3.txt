This paper presents pointer-network-based neural models tailored for referring expressions across three small-scale language modeling tasks: dialogue modeling, recipe modeling, and news article modeling. When conditioned on co-reference chains, the proposed models demonstrate improved performance over standard sequence-to-sequence models with attention.
The proposed models are essentially adaptations of pointer networks with copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Ling et al., 2016), modified to incorporate reference chains. The architectural contributions include: 1) restricting the pointer mechanism to focus on co-referenced entities, 2) extending the pointer mechanism to operate on 2D arrays (tables), and 3) training with supervised alignments. While these modifications are practical, they represent incremental advancements from an architectural standpoint.
The empirical contributions focus on perplexity evaluation for the three language modeling tasks. While perplexity is a standard metric for language modeling, it is an unreliable proxy for assessing performance in dialogue modeling and recipe generation. Furthermore, the datasets for the dialogue and recipe tasks are small compared to typical language modeling datasets, making it challenging to assess the broader impact of the proposed models. For instance, with access to a larger dataset, a standard sequence-to-sequence model with attention could potentially learn to align referring entities without additional mechanisms. Among the three tasks, the news article modeling task (Gigaword) provides the most compelling results. However, the dataset used is non-standard, and only a single baseline is reported, limiting the generalizability of the findings.
The paper also contains several errors, including mathematical inaccuracies, grammatical mistakes, and typographical issues:
- Eq. (1) omits a summation over $z_i$.
- "into the a decoder LSTM" should be "into the decoder LSTM."
- "denoted as his" should be "denoted as."
- "Surprising," should be "Surprisingly."
- "torkens" should be "tokens."
- "if follows that the next token" should be "the next token."
- In the "COREFERENCE BASED LANGUAGE MODEL" section, the meaning of $M$ is unclearâ€”please define it.
- In the sentence: "The attribute of each column is denoted as $sc, where $c$ is the c-th attribute," clarify whether $sc$ is a one-hot vector if that is the intended meaning.
- "the weighted sum is performed" should be "the weighted sum is computed."
- "a attribute" should be "an attribute."
- In the paragraph on Pointer Switch, change $p(z{i,v} |s{i,v}) = 1$ to $p(z{i,v} |s{i,v}) = 0$.
- In the "Table Pointer" paragraph, confirm whether "outer-product" is meant instead of "cross-product," as the equations otherwise seem inconsistent.
Additional comments:
- For the "Attention-based decoder," clarify whether the attention mechanism uses word embeddings or the hidden states of the sentence encoder. Additionally, specify whether it is applied only to the previous turn of the dialogue or the entire dialogue history.
- Elaborate on the advantage of the "Entity state update" rule compared to pointer or copy networks, which are used in the dialogue and recipe tasks.
- In the Related Work section, the statement "For the task-oriented dialogues, most of them embed the seq2seq model in traditional dialogue systems while our model queries the database directly" is inaccurate. Some task-oriented dialogue models do query databases during natural language generation. For example, see "A Network-based End-to-End Trainable Task-oriented Dialogue System" by Wen et al.