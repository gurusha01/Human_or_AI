The paper introduces a method aimed at reducing the memory footprint of neural networks, albeit with an associated increase in computational cost. It extends the work of HashedNets by Chen et al. (ICML'15), where neural network parameters are mapped into smaller memory arrays via hash functions that allow for potential collisions. Instead of directly training the original parameters, the compressed memory array elements, determined by the hash function, are optimized using back-propagation. This paper introduces several enhancements, including: (1) sharing the compression space across layers of the neural network, (2) employing multiple hash functions to mitigate collision effects, and (3) utilizing a small network to combine elements retrieved from multiple hash tables into a single parameter. Figure 1 effectively contrasts the proposed approach with HashedNets.
Strengths:
+ The proposed techniques are innovative and appear to have practical utility.
+ The paper provides some theoretical insights into the benefits of using multiple hash functions.
+ Experimental results consistently demonstrate that the proposed MFH method outperforms HashedNets.
Weaknesses:
- The computational cost appears to be higher than that of HashedNets, but this is not adequately discussed.
- The immediate practical relevance of the approach is unclear, especially given that alternative pruning strategies may offer better performance and faster inference.
Overall, I believe this paper makes a valuable contribution to the deep learning community by exploring parameter-sharing techniques across neural network layers, which could inspire further research. I recommend acceptance, contingent on the authors addressing the following points:
Additional comments:
- Please provide a detailed discussion of the computational costs for both HashedNets and MFH, covering both fully connected and convolutional layers.
- Were the experiments conducted only once for each configuration? If so, please rerun them multiple times and report the average and standard error.
- For completeness, consider including U1 results in Table 1.
- In Table 1, the entry for U4-G3 appears twice with differing valuesâ€”please clarify and correct this inconsistency.
- Some sentences in the manuscript contain grammatical errors. Please revise and improve the overall writing quality.