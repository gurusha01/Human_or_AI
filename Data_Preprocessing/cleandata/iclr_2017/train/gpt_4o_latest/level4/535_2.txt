The authors adapt the image captioning framework proposed by Xu et al. (2015) for video captioning tasks. Their model is enhanced to incorporate attention across multiple layers of the ConvNet, rather than focusing on a single layer. Experimental results on datasets such as YouTube2Text, M-VAD, and MSR-VTT demonstrate that leveraging multiple layers outperforms using individual layers in isolation.
Overall, this is a competent piece of work, comparable to a well-executed course project or a workshop paper. The proposed model is logical, adequately explained, and supported by experiments that validate the advantage of attending to multiple layers. However, from a technical standpoint, the contribution feels incremental, and the paper's value to the broader research community is unclear. Certain elements, like the inclusion of the hard attention mechanism, seem unnecessary and detract from the paper's focus.
To make this work more impactful, the authors could concentrate on delivering a detailed and focused investigation of multi-level features. This would require a more comprehensive analysis of the design choices and trade-offs associated with different approaches, while minimizing extraneous components such as video features and hard attention that do not significantly enhance the paper's core contribution.