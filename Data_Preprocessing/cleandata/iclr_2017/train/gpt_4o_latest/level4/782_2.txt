This paper introduces the use of a hierarchical softmax to accelerate attention-based memory addressing in memory-augmented networks (e.g., NTM, memNN).
The proposed model constructs a hierarchical softmax over the input sequence. At each time step, it performs a discrete SEARCH to identify the most relevant input for predicting the next output. The corresponding embedding of the selected input is then used to update the state of an LSTM, which subsequently generates the output. Additionally, the embedding of the selected input is updated via a WRITE operation, implemented as an LSTM that takes the hidden state of the other LSTM as input. Due to the discrete nature of the SEARCH operation, the model is trained using REINFORCE. The experimental evaluation focuses on algorithmic tasks such as search and sort.
The primary advantage of substituting the full softmax with a hierarchical softmax lies in the reduced inference complexity, which decreases from O(N) to O(log(N)). It would be interesting to see if this reduction in complexity enables the model to handle problems that are several orders of magnitude larger than those addressed using a full softmax. However, the authors only evaluate their approach on toy sequences of up to 32 tokens, which is relatively small.
The model relies on a fairly intricate search mechanism that necessitates training with REINFORCE. While this approach appears to work for problems involving small and simple sequences, it would be valuable to assess how its performance scales with increasing problem size.
In summary, while the idea of replacing the softmax in the attention mechanism with a hierarchical softmax is intriguing, this work falls short of being fully convincing. The proposed approach feels somewhat unnatural, may be challenging to train, and might not scale effectively. Additionally, the experimental section is notably weak.