The paper introduces a learning algorithm tailored for micromanagement in battle scenarios within real-time strategy (RTS) games. It addresses a challenging sub-problem of the broader RTS domain. The assumptions and constraints employed (e.g., greedy MDP, distance-based action encoding) are well-articulated and appropriate for the problem at hand.
The primary contribution of this work lies in the zero-order optimization algorithm and its application to structured exploration. This represents an innovative use of zero-order optimization in conjunction with deep learning for reinforcement learning (RL), and the approach is well-justified using arguments akin to those supporting deterministic policy gradients (DPG). The experimental results demonstrate clear improvements over baseline methods like vanilla Q-learning and REINFORCE, which are credible. While RTS is undoubtedly a fascinating and complex domain worthy of focused research, it would have been beneficial to include results from other domains. This is particularly relevant as the proposed algorithm appears to have potential applicability beyond RTS games. Evaluating it on less intricate domains could provide better insights into its generalizability and the types of problems that might benefit from this zero-order approach. The authors could consider adding text to clarify or further motivate this point.
There are some seemingly arbitrary design choices that are justified solely by "it worked in practice." For instance, the use of only the sign of \( w / \Psi_{\theta}(s^k, a^k) \). Similarly, the statement "we neglected the argmax operation that chooses the actions" raises questions. Presumably, this and dividing by \( t \) help keep values within or near the range \([-1,1]\)? It might be worth exploring truncating or normalizing \( w / \Psi \), as relying solely on the sign could result in significant information loss. Additionally, lines such as "We did not extensively experiment with the structure of the network, but we found the maxpooling and tanh nonlinearity to be particularly important" and the claim about the superiority of Adagrad over RMSprop without further elaboration or supporting details feel unsatisfactory. These omissions leave the reader questioning whether these observations are specific to the RTS setup in this paper or if they hold more broadly.
The paper's presentation could be improved, as some concepts are introduced without sufficient context, making the exposition unnecessarily confusing. For example, when defining \( f(\tilde{s}, c) \) at the top of page 5, the \( w \) vector is not explained, leaving the reader uncertain about its origin or purpose. While this is clarified later, a brief sentence here to contextualize its role (perhaps with a reference to the relevant section) would enhance readability. Similarly, on page 7: "because we neglected that a single \( u \) is sampled for an entire episode"â€”this is actually mentioned earlier in the text and is evident from the pseudo-code, so the phrasing here is misleading.
Minor issue: "perturbated" should be corrected to "perturbed."
--- After response period:
No rebuttal was provided, so the review remains unchanged.