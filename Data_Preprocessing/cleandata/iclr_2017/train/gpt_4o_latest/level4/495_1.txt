Review - Summary  
This paper provides a detailed analysis and comparison of the representational capabilities of deep versus shallow neural networks employing ReLU and threshold units. The primary contribution lies in demonstrating that approximating a strongly convex differentiable function can be achieved with significantly fewer units when an additional hidden layer is introduced.
Pros  
The paper combines an intriguing set of tools to derive a compelling result that highlights the exponential advantage of depth in neural networks.
Cons  
The main result seems to be limited to strongly convex univariate functions.
Specific Comments  
- Thank you for the clarifications on L. However, it would still be beneficial to elaborate on this aspect further in the main text. Additionally, I recommend emphasizing the primary result more prominently. I have not yet reviewed the revised version, so you may have already addressed some of these concerns.  
- The problem formulation closely resembles that of [Montufar, Pascanu, Cho, Bengio NIPS 2014], which also establishes exponential gaps between deep and shallow ReLU networks, albeit from a different perspective. I suggest including this paper in the literature review for completeness.  
- In Lemma 3, there is an instance of "i" that should be replaced with "x."  
- In Theorem 4, the notation for ``\tilde f'' is missing the argument "(x)."  
- Regarding Theorem 11, does the lower bound always increase with L?  
- In Theorem 11, should \bf x be specified as \bf x \in [0,1]^d?