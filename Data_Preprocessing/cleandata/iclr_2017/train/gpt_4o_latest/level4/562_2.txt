This paper introduces a novel approach to tackle the mode collapse problem in GANs by employing a large ensemble of generators and discriminators, dynamically pairing them with different counterparts throughout the training process. The central idea is that by frequently swapping generator-discriminator pairs, no single pair can become overly specialized or locked together. This is an interesting concept that targets a significant challenge in GAN training. However, the paper falls short in terms of experimental validation. Specifically:
- The authors need to provide stronger justification for the GAM metric. It is not immediately clear that the GAM metric is a reliable measure for evaluating generator networks, as it depends on the predictions of the discriminator networks, which may focus on artifacts. The authors could investigate whether the GAM metric correlates with established metrics such as inception scores or human evaluations. Currently, the GAM metric is the sole quantitative evaluation presented, and it is unclear whether it is a meaningful or relevant metric to assess performance.  
- Building on the previous point, the paper lacks comparisons with other methods. For instance, why not compute inception scores and benchmark against prior approaches? Similarly, the quality of generated samples is not compared to existing methods, making it difficult to assess whether this approach offers any improvement in sample quality.
Additionally, I reiterate the following questions from the pre-review section:
- If, instead of swapping, you were to train K GANs on K subsets of the data or K GANs with different initializations (but without swapping), would you observe any improvements in results? Similarly, what happens if you train larger capacity models with dropout in both G and D? Since dropout effectively averages multiple models, it would be interesting to see if similar effects are achieved.
- In Figure 6, it seems that the validation costs remain constant as parallelization increases, while the training cost rises, leading to a narrowing gap. Does this truly indicate better generalization?
In conclusion, this is an intriguing paper that addresses a critical issue in GAN training, but it lacks compelling experimental results to substantiate its claims.