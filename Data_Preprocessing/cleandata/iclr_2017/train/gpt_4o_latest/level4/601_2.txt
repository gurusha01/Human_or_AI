Review - Paper Summary:  
This paper introduces the NewsQA dataset, a novel comprehension dataset comprising 100,000 question-answer pairs derived from over 10,000 CNN news articles. The dataset is constructed through a four-stage pipeline: article filtering, question collection, answer collection, and answer validation. The examples are categorized based on answer types and the reasoning required to answer the questions. The authors compare human and machine performance on NewsQA with that on SQuAD.
Paper Strengths:  
- I concur that models can benefit from a diverse range of datasets. Since this dataset is sourced from news articles, it may present challenges distinct from those posed by popular datasets like SQuAD.  
- The dataset is sufficiently large to support the training of data-intensive deep learning models.  
- The inclusion of questions with null answers is a valuable feature.  
- The four-stage data collection process reflects a well-thought-out methodology.  
- The proposed BARB model achieves performance comparable to a published state-of-the-art model while being significantly faster.  
Paper Weaknesses:  
- The human evaluation is insufficiently robust. The performance of two near-native English speakers on 100 examples each cannot adequately represent the entire dataset. Additionally, the model's performance on these 200 examples is not reported.  
- Although not strictly necessary for this paper, the authors could more convincingly demonstrate that NewsQA is more challenging than SQuAD by either calculating human performance using the same methodology as SQuAD or by evaluating human performance on both datasets using a consistent approach on sufficiently large, representative subsets. Other datasets, such as the VQA dataset (Antol et al., ICCV 2015), also adopt SQuAD's method for computing human performance.  
- Section 3.5 states that 86% of questions have answers agreed upon by at least two workers. However, this figure appears inconsistent with the claim in Section 4.1 that only 4.5% of questions lack agreement after validation.  
- If the same article is shown to multiple Questioners, is there a mechanism to ensure that they do not ask the same or highly similar questions?  
- The authors mention using the same hyperparameters as SQuAD. What are the results when hyperparameters are tuned using a validation set from NewsQA?  
- The 500 examples labeled for reasoning types seem insufficient to represent the entire dataset. Additionally, the model's performance on these 500 examples is not reported.  
- It is unclear which model's performance is depicted in Figure 1.  
- The two "students" mentioned—are they graduate/undergraduate students or researchers?  
- The test set appears to be quite small.  
- Suggestion: While the answer validation step is a valuable addition, the dataset could be released in two formats—one containing all answers collected during the third stage (prior to validation) and another in its current form, post-validation.  
Preliminary Evaluation:  
The NewsQA dataset is a large-scale machine comprehension dataset derived from news articles, and in my view, it is sufficiently distinct from existing datasets to provide meaningful benefits to state-of-the-art models. With improvements to the human evaluation, I believe this paper has the potential to be a strong poster presentation.