This paper investigates the optimization problem of linear ResNet and provides a mathematical demonstration that, under the conditions of 2-shortcuts and zero initialization, the Hessian's condition number remains independent of depth. While I skimmed through the proofs, I have not verified them in detail.
The result is an interesting observation for training deep linear networks. However, I believe the paper does not fully address the linear versus nonlinear network issue. Below are some questions and comments:
1. Although the revision includes some results with ReLU units, it appears these are only applied to the middle layers of the network (Section 5.3). Is this the standard approach in ResNet architectures? Additionally, ReLU is non-differentiable at zero, which does not satisfy the conditions stated in Theorem 1. Why not consider differentiable activation functions such as sigmoid or tanh instead?
2. From Equation (22) in the appendix, it seems that for nonlinear activations, the condition number depends on the derivative \(\sigma^\prime\) at zero. For instance, if tanh is used, which has a derivative of 1 at zero, the condition number would be the same for both linear and tanh activations. However, this alone may not sufficiently explain the observed differences in performance or optimization between linear and nonlinear networks, nor does it address how these differences evolve as learning progresses beyond the zero point.
3. Regarding the success of ResNet (or convolutional networks in general) in computer vision, I suspect that additional forms of nonlinearity, such as pooling, play a significant role. Can the results presented in this paper be extended to account for pooling operations as well?
Minor comments:
- Section 1, last paragraph: While low approximation error typically indicates a more expressive model class and improved training error, it does not necessarily guarantee better test error.
- Section 4.1: What is meant by "zero initialization with small random perturbations"? Why not use exact zero initialization? Additionally, how large are these random perturbations?