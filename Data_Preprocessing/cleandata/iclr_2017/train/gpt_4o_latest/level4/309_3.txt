This paper introduces an approach to train RL agents to perform auxiliary tasks alongside their primary objectives, hypothesizing that this strategy aids in learning more robust features. Specifically, the authors propose two pseudo-control tasks: controlling changes in pixel intensity and controlling the activation of latent features. Additionally, they introduce a supervised regression task that involves predicting the immediate reward following a sequence of events. The latter task is trained offline using a skewed sampling strategy from the experience replay buffer, ensuring a balanced 1/2 chance of observing reward or no reward.
The proposed agents demonstrate significant performance improvements on discrete-action-continuous-space RL tasks, achieving baseline performance with 10x fewer iterations compared to standard methods.
This approach diverges from traditional "passive" unsupervised or model-based learning paradigms. Rather than requiring the model to learn potentially irrelevant input representations or solve potentially infeasible task-modelling objectives (e.g., due to partial observability), the authors argue that learning to control local and internal features of the environment complements the process of learning an optimal control policy.
In my view, this work is novel and presents a compelling alternative to unsupervised learning by leveraging the agent's inherent capacity to exert control over its environment. The proposed auxiliary tasks are described at a high level, which aids in conveying the intuition behind the approach. However, I believe some additional low-level detail would enhance clarity. For instance, explicitly introducing the L_PC loss earlier in the main text, rather than relegating it to the appendix, would improve accessibility for readers.
The methodology is sound. While the distribution of optimal hyperparameters may differ between A3C and UNREAL, the use of top-3 performance metrics ensures that, assuming the best hyperparameters for both methods lie within the explored intervals, the optimal configurations for each method are identified.
One limitation of the paper—or perhaps an area for future work given space constraints—is the lack of detailed experimental analysis on the auxiliary tasks themselves, beyond their demonstrated impact on performance. For instance, pixel/feature control appears to have the most significant influence; in Labyrinth, A3C+PC alone outperforms nearly all configurations except UNREAL. It would have been valuable to isolate and analyze this effect in greater depth, potentially by measuring additional metrics beyond RL task performance.