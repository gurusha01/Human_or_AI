The paper introduces a nonlinear regularizer designed to address ill-posed inverse problems. It assumes that the latent variables (or causal factors) associated with the observed data reside near a low-dimensional subspace within an RKHS defined by a predetermined kernel. The proposed regularizer generalizes the linear low-rank assumption on the latent factors. Specifically, a nuclear norm penalty is applied to the Cholesky factor of the kernel matrix as a relaxation for controlling the subspace dimensionality. Empirical evaluations are presented on two tasks involving linear inverse problems: missing feature imputation and recovering non-rigid 3D structures from sequences of 2D orthographic projections. The results demonstrate that the proposed method outperforms linear low-rank regularization.
However, the clarity of the paper could be improved, particularly in the Introduction. The discussion oscillates between dimensionality reduction techniques and inverse problems, which can be confusing. A more coherent structure might involve first clearly defining the ill-posed inverse problem and then motivating the need for a regularizer, thereby naturally introducing dimensionality reduction techniques.
The rationale for relaxing the rank constraint in Eq. 1 to the nuclear norm in Eq. 2 is not sufficiently explained in this context. This relaxation does not result in a convex problem over \( S, C \) (as seen in Eq. 5) and also increases computational complexity, given that Algorithm 2 requires a full SVD of \( K(S) \) in every iteration. The authors should discuss the trade-offs compared to an alternative approach that fixes the rank of \( C \) (which could be selected via cross-validation, similar to how \( \tau \) is chosen), leaving only the first two terms in Eq. 5. For this simpler objective, an intriguing question arises: are there kernel functions for which this formulation can be solved in a scalable manner?
The proposed alternating optimization method, in its current form, is computationally expensive and appears challenging to scale to even moderately sized datasets. Each iteration requires computing the kernel matrix over \( S \) and performing a full SVD on the kernel matrix (as outlined in Algorithm 2). Additionally, the empirical evaluations are somewhat limited: (i) the dataset used for feature imputation is outdated and non-standard, (ii) for structure-from-motion on the CMU dataset, the comparison is restricted to linear low-rank regularization, and (iii) there is no discussion or analysis of the convergence behavior of the alternating optimization procedure (Algorithm 1).