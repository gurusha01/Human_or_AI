This paper presents a hardware accelerator for deep neural networks (DNNs), leveraging the inherent tolerance of DNNs to low-precision inference. The proposed design achieves a 1.90x performance improvement over a state-of-the-art bit-parallel accelerator without any accuracy degradation, while also being 1.17x more energy-efficient. Notably, the approach does not require network retraining and demonstrates superlinear performance scaling with respect to area.
The first issue is that the paper appears to be a better fit for the hardware or circuit design community rather than ICLR. The inclusion of circuit diagrams makes it more relevant to those domains.
The second issue concerns the "takeaway for the machine learning community." Based on the authors' response, the primary takeaway is the use of low precision to reduce inference costs. However, this is not sufficiently novel. At least four papers at last year's ICLR addressed the use of low precision to improve DNN efficiency, and similar ideas have already been explored in the authors' prior work.