This paper introduces a novel neural network model for sentence representation, drawing inspiration from the success of residual networks in Computer Vision and observations on word morphology in Natural Language Processing. While the paper demonstrates that the proposed model achieves state-of-the-art results on multiple datasets, it lacks compelling evidence, intuition, or motivation to justify the design of the network architecture.
Specifically:
- The contribution of this paper is unclear: is it focused on character-aware word embeddings, residual networks, or the combination of both?
- The discussion of residual networks in Section 3.3 appears insufficient, as it overlooks key differences between image representation and sentence representation. Although the results indicate that incorporating residual networks is beneficial, the reasoning behind this remains unconvincing. Could the authors provide an explanation of what the residual component captures in the context of sentence modeling?
- The paper integrates several components into the classification framework, including a character-aware model for word embeddings, residual networks, and attention weights in Type 1 features. However, the contribution of each component to the overall performance is unclear. Table 3 only reports results for one component. Could additional ablation studies be included to clarify the individual contributions of these components?
- In Equation (5), what does $i$ represent in $G_i$?
- The citation format is incorrect.