The lifespan of datasets in recent research appears to be shrinking rapidly. For instance, the SQuAD dataset has seen intense activity since its release online just a few months ago, with the top leaderboard performance now reaching 82%. This is particularly striking given that the dataset's formal presentation at EMNLP'16 occurred only a month ago, and the reported machine performance at the time of submission was merely 51%. A plausible explanation for this rapid progress is that the dataset may not have been sufficiently challenging.
The submitted paper on NewsQA seeks to address this issue by introducing a dataset of comparable scale, created using alternative QA collection strategies. Most notably, the authors gather questions without requiring answers from the same crowdworkers, aiming to foster more diverse and challenging questions. Another significant distinction is that the questions are generated without exposing the content of the news articles, and the dataset utilizes a larger subset of the CNN/Daily Mail corpus (12K/90K) compared to the much smaller subset (500/90K) used by SQuAD.
In summary, I believe the NewsQA dataset represents a commendable attempt to construct a more challenging, large-scale reading comprehension benchmark, which is a highly active area of research currently lacking fully satisfactory datasets. While the dataset has its limitations, it offers potential advantages over existing alternatives.
However, the paper itself feels rushed, as there are several areas where the authors could have made improvements. This raises concerns about the dataset's overall quality. For instance, the human performance on SQuAD reported by the authors (70.5–82%) is lower than the original SQuAD benchmark (80.3–90.5%). Such discrepancies could arise from differences in the annotators' levels of attentiveness or reading comprehension. Since not all humans exhibit the same level of precision or understanding, it would be helpful for the authors to explain these differences and, if possible, conduct a more rigorous measurement of human performance. This is particularly important because a human performance level of 74.9% for NewsQA could suggest that the dataset's difficulty stems more from noise in the QA collection process than from genuine challenges in comprehension and reasoning. This, in turn, implies that lower model performance might result from incorrect answers provided by human annotators rather than the inherent difficulty of the task.
Additionally, I question the decision to withhold the news articles when generating questions. This approach might lead to overly generic or repetitive questions due to the lack of sufficient context. A potential improvement could involve presenting news articles with certain sentences or phrases randomly redacted. This hybrid approach would provide question generators with some context while still limiting their access to the full material.
Another way to discourage crowdworkers from creating overly simple questions could be to integrate an automatic QA system during the process. Workers would then be required to construct QA pairs that existing state-of-the-art systems cannot answer correctly.