This is an 18-page paper, including an appendix, that provides a mathematical derivation of the infomax principle for a realistic neural population in the presence of noise. The original Bell & Sejnowski infomax framework addressed only the noise-free case. The authors present results using natural image patches and the MNIST dataset, which qualitatively align with findings from other methods.
The approach appears interesting and potentially offers a more general framework for unsupervised learning. However, the paper is quite lengthy, and I found it challenging to follow the progression of ideas. For instance, the introduction of the hierarchical model was confusing, and it required multiple readings to grasp its purpose. The term "hierarchical" may not be the most appropriate here, as it does not refer to a deep network hierarchy but rather to a decomposition of the tuning curve function into distinct components. I suggest the authors condense the paper to clearly convey the central message and key steps more succinctly, while relegating the detailed mathematical derivations to a supplementary document.
Additionally, the authors should consider referencing the work of Karklin & Simoncelli (2011), which is closely related. Their study also employs an infomax framework for a noisy neural population to derive on and off cells in the retina and explores the conditions under which orientation selectivity arises.