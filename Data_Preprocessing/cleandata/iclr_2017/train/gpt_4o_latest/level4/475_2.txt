This paper introduces a method for training deep generative models with discrete hidden variables by leveraging the reparameterization trick. The approach is then applied to a specific DBN-like architecture, demonstrating that this architecture achieves state-of-the-art density modeling performance on MNIST and related datasets.
The manuscript is well crafted, with a presentation that is both comprehensive and precise. The inclusion of multiple appendices that provide detailed justifications for various design choices is commendable. It is refreshing to see this level of care in exposition, and I wish more papers in our field would adopt a similar approach.
The reported log-likelihood results are impressive, particularly given that most competing methods rely on continuous latent variables. However, one limitation of the experimental setup is the lack of analysis to disentangle the contributions of the architecture from those of the inference algorithm. For instance, it would be informative to see how a comparable architecture performs when trained with VIMCO, or how the proposed algorithm fares when applied to an existing discrete architecture.
I have some reservations regarding the variance of the gradients in the general formulation of the algorithm. My comment on the "variance of the derivatives of F^{-1}" below outlines this concern. While the authors' response is convincing, it would be beneficial to explicitly discuss this issue in the paper, along with "engineering principles" for selecting the smoothing distribution. This is important because the problem seems likely to arise unless users are made aware of it. For example, my suggestion of using widely separated normals might seem intuitive but could lead to issues unless one carefully examines the gradients â€” something that is often overlooked in the era of autodiff frameworks.
Another point of concern is the number of sequential operations required for inference in the RBM model. (As an aside, is this actually an RBM or a more general Boltzmann machine?) The q distribution is modeled autoregressively, processing variables one at a time. While Section 3 briefly mentions the possibility of grouping variables in the q distribution, and Appendix A elaborates on this, the proposed solution still involves decomposing the joint distribution into a product of conditionals and applying the CDFs sequentially. As a result, it seems that handling all variables sequentially remains unavoidable, which could become computationally expensive.
Minor issue: the second paragraph of Section 3 should include a reference to Appendix A.