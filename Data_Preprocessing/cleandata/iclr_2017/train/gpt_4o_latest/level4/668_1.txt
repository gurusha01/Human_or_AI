The paper conducts an experimental study on a slightly modified version of the label smoothing technique for training neural networks and presents results across various tasks. While the concept of smoothing is not novel, its application to a broad range of machine learning tasks has not been explored previously.
Comments:  
The paper should include state-of-the-art results for speech recognition tasks (e.g., TIMIT, WSJ), even if the models being compared are not directly equivalent.  
The back-propagation of label smoothing through the softmax function is straightforward and computationally efficient. However, is there an efficient method for back-propagating entropy smoothing through the softmax function?  
Although classification accuracy may remain unaffected, this type of smoothing prevents the model from estimating the true posterior distribution.  
This limitation could pose challenges in complex machine learning scenarios where decisions rely on higher-level reasoning based on posterior estimates, such as in language models for speech recognition.  
The proposed smoothing approach requires additional motivation to justify its use.