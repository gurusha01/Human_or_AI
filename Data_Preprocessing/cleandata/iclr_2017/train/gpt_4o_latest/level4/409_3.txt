The paper presents a novel dataset named MusicNet, which appears to be inspired by ImageNet, offering dense ground truth labels for over 30 hours of classical music in raw audio format. This dataset is highly valuable for the music information retrieval (MIR) community, as no publicly available dataset of this scale has existed before. Its availability has the potential to significantly enhance the application of modern machine learning techniques, such as deep learning, in this domain, where progress has historically been hindered by the lack of sufficiently large datasets. The paper is well-written and clearly articulated.
The paper also includes a set of "example" experiments utilizing the dataset, which I find less compelling. The authors focus on a single task: identifying pitches in isolated audio segments. Pitch detection is a relatively low-level feature of music, and given that isolated fragments are used as input, this task is not particularly challenging. In fact, it could likely be addressed effectively with simpler methods, such as peak picking on a spectral representation, without requiring machine learning. It remains unclear what added value the machine learning approach brings to this task, if any. While I have not conducted this comparison myself, I believe it is the authors' responsibility to demonstrate that machine learning is indeed beneficial in this context.
One of the dataset's strengths lies in the diversity of label information it provides. A more compelling demonstration would have involved showcasing multiple prediction tasks spanning both low-level features (e.g., pitch, onsets) and high-level features (e.g., composer identification). This could have been achieved with simpler models and perhaps using spectrogram input instead of raw audio, as the comparison between these input types seems tangential to the dataset's introduction. As it stands, the relatively unremarkable nature of the experiments detracts from the paper's primary contribution, which is the introduction of a unique and large-scale public dataset.
That being said, the experiments appear to have been conducted rigorously, with proper evaluation and analysis of the resulting models.
Regarding Section 4.5, it is not surprising that a pitch detector learns filters resembling pitches (i.e., sinusoids). However, the observation that this requires a substantial amount of data is noteworthy. It would be more impactful, though, to demonstrate similar findings for higher-level tasks. While the authors compare the features learned by their model favorably to prior work on end-to-end learning from raw audio, they overlook the fact that the tasks in that prior work were significantly more high-level.
Lastly, some may question whether ICLR is the right venue for introducing a new dataset. Personally, I believe it is an excellent choice, as it ensures the dataset reaches the appropriate audience. However, this decision ultimately rests with the organizers and program committee. I mention this because I do not believe the paper warrants acceptance based solely on its experimental results.