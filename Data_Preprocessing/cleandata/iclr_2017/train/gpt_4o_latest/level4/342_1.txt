The study integrates variational recurrent neural networks (VRNNs) and adversarial neural networks to address domain adaptation challenges for time series data. The proposed approach is evaluated against several competing algorithms using two healthcare datasets derived from MIMIC-III in domain adaptation scenarios.
The primary contribution of the work is modest, as it builds upon VRNN by incorporating adversarial training to learn domain-invariant representations. Experimental results demonstrate that the proposed method significantly outperforms competing algorithms. However, the source of this advantage remains unclear. The main distinction between the proposed method and R-DANN lies in the use of variational RNNs instead of standard RNNs. Limited insights are provided to explain how this change leads to such a substantial performance improvement or the pronounced differences in temporal dependencies observed in Figure 4.
Detailed comments:
1. Please clarify what is depicted in Figure 1. Specifically, is Figure 1(b) the t-SNE projection of representations learned by DANN or R-DANN? Section 4.4 suggests it is the latter. The highly structured plot for VRADA in Figure 1(c) is unexpected. What do you believe are the two dominant latent factors represented in this plot?
2. In Table 2, the two baselines exhibit a notable performance gap when evaluated on the entire target set (including the validation set) versus the test set alone. In contrast, VRADA performs almost identically across these two settings. Could you provide an explanation for this observation?
3. Please elaborate further on Figures 3 and 4. How should the x-axis in Figure 3 and both the x- and y-axes in Figure 4 be interpreted? Additionally, the two plots on the right side of Figure 4 appear strikingly regular compared to the ones on the left. Could you comment on this discrepancy?