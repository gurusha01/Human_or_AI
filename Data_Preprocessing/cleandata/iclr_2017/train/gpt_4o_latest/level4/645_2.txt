This paper introduces a deep extension of generalized CCA, with its primary contribution being the derivation of the gradient update for the GCCA objective.
I take issue with the claim that "this is the first Multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views." [R1] presents a Multiview representation learning approach that is both nonlinear and capable of handling more than two views, making it highly relevant to the method proposed in this paper. The objective function in [R1] seeks to maximize the correlation between views while minimizing self and cross-reconstruction errors, which is conceptually akin to a nonlinear extension of PCA+CCA for multiple views. A comparison between the proposed method and the approach in [R1] is essential to demonstrate the advantages of DGCCA, and the absence of such a comparison leaves the paper incomplete. Furthermore, the authors should revise their strong claim accordingly.
The related work section is sparse and overlooks significant advancements in two-view nonlinear representation learning, which are worth discussing to provide a more comprehensive context.
References:  
[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. HLT-NAACL 2016: 171-181