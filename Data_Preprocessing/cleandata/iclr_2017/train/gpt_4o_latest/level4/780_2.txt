This paper examines the ring-based AllReduce method for multi-GPU data-parallel training of deep neural networks.
Comments  
1) The term "linear pipeline" may confuse readers, as this technique is commonly referred to as the "ring-based approach" in the AllReduce literature. To enhance clarity and facilitate connections to prior work, the authors should adopt the standard terminology.  
2) The cost analysis of the ring-based AllReduce method has already been addressed in existing literature. This paper extends that analysis to the context of multi-GPU deep neural network training and concludes that the scaling behavior is invariant with respect to the number of GPUs.  
3) The ring-based AllReduce method is already implemented in NVIDIA's NCCL library. While the authors claim their implementation predates NCCL, this distinction should be clarified further.  
4) Overlapping communication with computation is a well-established technique in systems like TensorFlow and MXNet. The scheduling approach proposed by the authors partially leverages this overlap, performing backpropagation for step t-1 while conducting reduction. However, the dependency pattern could be further optimized. Specifically, the forward pass of layer t depends on the parameter update of layer t from the previous iteration, which could be addressed using a dependency-aware scheduler.  
5) Since this paper focuses on analyzing AllReduce, it would be beneficial to include a detailed comparison of tree-based reduction, the ring-based approach, and the all-to-all method. The current paper lacks a discussion of the all-to-all approach, which would provide a more comprehensive analysis.  
Summary  
This paper discusses existing AllReduce techniques for data-parallel multi-GPU training of deep neural networks, with cost analysis derived from prior results. While the findings are not particularly surprising, as they align with established AllReduce analyses, the work may still be useful to some readers as a baseline reference. However, the analysis could be further improved (see comment 5).