The paper presents an extension of the HashedNets framework, incorporating several innovative modifications. Rather than relying on a single hash function, the proposed HFH method employs multiple hash functions to link each "virtual" (to-be-synthesized) weight location to several elements of a shared parameter vector (common across all layers). These elements are subsequently processed through a compact MLP to generate the final weight values.
The concept is both intriguing and original, and the experimental results indicate significant improvements over HashedNets. However, HashedNets itself is not a particularly strong baseline for neural network model compression, especially when compared to more recent pruning- and quantization-based methods. The experiments in this paper show that the proposed approach achieves lower accuracy at equivalent compression ratios compared to pruning-based methods, without offering any runtime speedup advantages. While the authors note that the technique is only 20% slower (which is a pleasantly surprising result), it remains unclear why this method would be preferred over competing techniques for the types of networks evaluated in the experiments. The authors propose that the method could be combined with pruning-based approaches, which might be true, but no experimental evidence supporting this claim is provided. Additionally, the paper highlights the ease of setting the compression ratio as a benefit of HFH, but I do not find this advantage compelling enough to outweigh the significant drawbacks in terms of accuracy and speed.
In response to a query, the authors clarify that the method performs particularly well for compressing embeddings. In this context, the approach does seem to represent a meaningful contribution, given its minimal overhead and notable training-time benefits. If the paper had focused on this application and provided experimental results on tasks such as language modeling or other scenarios involving high-dimensional sparse/one-hot inputs with large embedding layers, I would be inclined to recommend acceptance enthusiastically. However, for the CNN and MLP architectures that are the primary focus of the experiments, I do not believe the proposed method is well-suited, despite my appreciation for the underlying idea.