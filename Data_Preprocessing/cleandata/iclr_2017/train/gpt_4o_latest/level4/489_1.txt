The authors propose a methodology for evaluating sentence embedding techniques by assessing how well the embeddings retain information about sentence length, word content, and word order. They investigate several widely-used embedding methods, including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are comprehensive and yield intriguing insights into the representational capabilities of common sentence embedding approaches, such as the observation that word order exhibits surprisingly low entropy when conditioned on word content.
Investigating the types of information encoded in representation learning methods for NLP is a crucial and relatively underexplored area. For instance, the wave of research on word embeddings largely subsided following a series of careful experimental studies demonstrating that most embeddings are fundamentally similar, culminating in the work "Improving Distributional Similarity with Lessons Learned from Word Embeddings" by Levy, Goldberg, and Dagan. As representation learning continues to gain prominence in NLP, this type of research will become increasingly significant.
While this paper makes a meaningful contribution by proposing and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are relatively straightforward and may not directly align with practical requirements for sentence embeddings (as the authors themselves acknowledge, performance on these tasks does not serve as a definitive measure of embedding quality). For instance, as noted by the authors, the ability of an averaged vector to encode sentence length is unsurprising and can be attributed to the central limit theorem (or more precisely, to concentration inequalities such as Hoeffding's inequality).
The experiments on word order were particularly compelling. A relevant reference for this type of conditional ordering procedure is "Generating Text with Recurrent Neural Networks" by Sutskever, Martens, and Hinton, which describes the process of converting a bag of words into a sentence as "debagging."
Although this work represents an initial step toward a deeper understanding of sentence embeddings, it is an important one, and I recommend this paper for publication.