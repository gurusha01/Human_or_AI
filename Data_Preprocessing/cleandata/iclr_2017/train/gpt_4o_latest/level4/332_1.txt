This paper introduces a model designed to learn across multiple views of objects. The central idea is the use of a triplet loss function, which enforces that two different views of the same object are closer in the learned feature space than an image of a different object. The proposed method is evaluated on object instance and category retrieval tasks and is benchmarked against baseline CNNs (untrained AlexNet and AlexNet fine-tuned for category classification) using fc7 features with cosine distance. Additionally, the paper includes a comparison with human perception on the "Tenenbaum objects."
Strengths: Employing a triplet loss for this problem introduces some degree of novelty (though this may be somewhat limited due to concurrent work; see below). The paper is written in a reasonably clear manner.
Weaknesses: The paper does not include key references to related work in this domain and lacks a direct comparison with an existing approach.
Additional Comments:
The "image purification" paper is highly relevant to this work:
[A] Joint Embeddings of Shapes and Images via CNN Image Purification. Hao Su, Yangyan Li, Charles Qi, Noa Fish, Daniel Cohen-Or, Leonidas Guibas. SIGGRAPH Asia 2015.
This prior work learns to map CNN features to (hand-designed) light field descriptors of 3D shapes to achieve view-invariant object retrieval. A direct comparison with this approach would strengthen the paper, particularly in the context of cross-view retrieval experiments like those presented in Table 1 of [A]. Notably, the code and data for [A] appear to be publicly available.