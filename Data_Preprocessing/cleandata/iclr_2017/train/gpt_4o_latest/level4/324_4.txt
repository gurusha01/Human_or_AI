This paper introduces a straightforward approach—pruning low-weight filters from ConvNets—to reduce FLOPs and memory usage. The method is evaluated on VGG-16 and ResNets using CIFAR10 and ImageNet datasets.
Pros:
- Produces structured sparsity, which inherently enhances performance without requiring modifications to the convolution operation.
- Extremely simple to implement.
Cons:
- Lacks an analysis of how pruning affects transfer learning.
Overall, I have a positive impression of this work. Although the core idea is quite basic, I am not aware of any prior studies that propose the exact same approach while providing a solid set of experimental results. As such, I lean toward recommending acceptance. However, the primary limitation is the absence of an evaluation of how filter pruning influences transfer learning. For instance, tasks like CIFAR10 or even ImageNet are of limited standalone interest. The broader focus in both academia and industry lies in the utility of learned representations for transfer to other tasks. It is reasonable to hypothesize that filter pruning (or any form of pruning) could negatively impact transfer learning. While the main task performance may remain unaffected, transfer learning performance could degrade significantly. This paper misses an opportunity to investigate this important aspect.
Minor Issue: The title of Fig. 2 mentions VGG-16 in (b) and VGG_BN in (c). Are these referring to the same model?