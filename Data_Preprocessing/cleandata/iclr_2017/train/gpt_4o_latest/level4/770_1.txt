This paper introduces a method to integrate knowledge base (KB) facts into language modeling, allowing the model to either generate a word from the entire vocabulary or select relevant KB entities at each time step.
The authors validate their approach on a newly created dataset, WikiFacts, which aligns Wikipedia articles with Freebase facts. They also propose a modified perplexity metric that penalizes the likelihood of unknown words, offering a more nuanced evaluation.
At a high level, I appreciate the motivation behind this work—named entities are often critical for downstream tasks but challenging to model purely through statistical co-occurrences. Leveraging KB facts could provide a valuable resource to address this issue.
That said, I found the paper difficult to follow, particularly in Section 3, and believe the writing requires significant improvement. 
- The definitions of f{symbkey}, f{voca}, and f_{copy} are missing or unclear.
- The notations w^v and w^s are confusing and need clarification.
- It seems that e_k represents the average of all previous fact embeddings, but this should be explicitly stated to avoid ambiguity.
- While (ht, ct) = fLSTM(x{t−1}, h{t−1}) is introduced, ct does not appear to be used in the subsequent explanation.
- The concept of "fact embeddings" is not well-articulated. My understanding is that they are formed by concatenating the relation and entity (object) embeddings. For anchor or "topic-itself" facts, do you learn embeddings for the special relations while using entity embeddings from TransE? This aspect needs to be clarified.
Regarding the generation of words from KB entities (fact descriptions), the approach of generating a symbol position first feels somewhat counterintuitive. Since most entities consist of multiple words, it is crucial to preserve their order. Additionally, incorporating prior knowledge could be beneficial—for instance, it is common to refer to "Barack Obama" simply as "Obama."