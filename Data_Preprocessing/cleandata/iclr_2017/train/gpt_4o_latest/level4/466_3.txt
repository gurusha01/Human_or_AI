This paper explores identity parametrization, commonly referred to as shortcuts, where the output of each layer takes the form h(x) + x instead of just h(x). This approach has been demonstrated to perform effectively in practice (e.g., ResNet). The discussions and experiments presented in the paper are engaging. Below are a few comments on the paper:
- Section 2: The study of linear networks is intriguing in its own right. However, it is unclear how these findings could provide insights into non-linear networks. For instance, while you have proven that every critical point is a global minimum, it would be beneficial to include a discussion on how these results for linear networks relate to non-linear networks.
- Section 3: The construction presented is interesting, but the expressive power of residual networks is within a constant factor of general feedforward networks. It is unclear why a new proof is necessary, given the existing results on the finite sample expressivity of feedforward networks. Clarifying this point would be appreciated.
- Section 4: The experiments are well-executed, and the choice of using random projection on the top layer is particularly innovative. However, combining this choice with all-convolutional residual networks makes it challenging for readers to disentangle the individual contributions of each component. I suggest reporting results for all-convolutional residual networks with a learned top layer, as well as for ResNet with a random projection on the top layer, to provide clearer insights.
Minor comments:
1. I disagree with the claim that Batch Normalization can be reduced to an identity transformation. Including this in the abstract without a proper discussion may not be appropriate.
2. On page 5, above Assumption 3.1: x^(i) = 1 does not necessarily imply ||x^(i)||_2 = 1.