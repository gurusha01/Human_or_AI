This paper introduces a method for transferring skills between tasks in a control setting trained via reinforcement learning (RL). The core idea is to enforce similarity between the embeddings learned for two different tasks using an L2 penalty. The experiments are conducted in the MuJoCo environment, with one set of experiments using joint/link states as input (Sections 5.2 and 5.3) and another set using pixel-based inputs (Section 5.4). The results demonstrate successful transfer between arms with differing numbers of links and between a torque-driven arm and a tendon-driven arm.
One limitation of the paper is the assumption that time alignment is trivial, given that the tasks are episodic and within the same domain. However, time alignment is a form of domain adaptation/transfer that the paper does not address. This could potentially be tackled through techniques such as subsampling, dynamic time warping, or learning a matching function (e.g., a neural network).
General remarks: The approach is compared against canonical correlation analysis (CCA), which is an appropriate baseline. However, since the paper is primarily experimental, an additional baseline could be included where "f" and "g" (the embedding functions for the two domains) are initialized as random projections. This would help confirm whether the poor performance of the "no transfer" version is due to over-specialization of the embeddings. Additionally, it might be worth noting that the problem of learning invariant feature spaces is related to metric learning (e.g., [Xing et al. 2002]). Furthermore, the paper does not draw connections to multi-task learning in machine learning. For the knowledge transfer scenario (Section 4.1.1), it could also be useful to consider annealing the \(\alpha\) parameter.
The experimental section feels somewhat rushed. For instance, the baseline consistently achieving a performance of 0 (indicating no transfer) is uninformative, and a larger sample budget should have been tested. Additionally, Figure 7.b lacks results for "CCA" and "direct mapping," which raises questions. Another experimental concern is whether and how the authors controlled for the number of training iterations for the embeddings in the transfer setting compared to the non-transfer setting.
In summary, the study of transfer in RL is a valuable contribution, and the experiments presented in this paper are sufficiently interesting to warrant publication. However, the paper would benefit from a more thorough and detailed exploration of its methods and results.