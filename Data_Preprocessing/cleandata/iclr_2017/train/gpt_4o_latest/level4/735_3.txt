This paper introduces a method for non-linear kernel dimensionality reduction using a trace norm regularizer in the feature space. The authors propose an iterative minimization strategy to achieve a local optimum of a relaxed optimization problem.
However, the paper contains several issues, and the experimental evaluation is unconvincing. The comparisons are limited to outdated methods and are conducted on overly simplistic toy datasets. 
The authors claim state-of-the-art performance, but the use of the oil dataset as a benchmark is questionable, as it is not a standard benchmark dataset. Furthermore, the comparisons are made against older methods, which weakens the validity of the claims. The experimental results should better illustrate robustness to more complex noise and outliers, especially since this was highlighted as a key motivation in the introduction.
The paper also fails to address the out-of-sample problem, which is a known limitation of kernel-based methods compared to latent variable models (LVMs). This issue should have been discussed and addressed in the context of the proposed approach.
The paper contains the following errors:
- In the last paragraph of Section 1, the authors claim that the paper proposes a closed-form solution to robust KPCA. This is incorrect, as the proposed method involves iteratively solving a series of closed-form updates and performing Levenberg-Marquardt optimization. This iterative process cannot be considered a closed-form solution.
- In the same paragraph (and elsewhere in the text), the authors assert that the proposed method can be easily generalized to incorporate other cost functions. This is misleading, as such generalizations would typically require solving a significantly more complex optimization problem, often without the benefit of closed-form updates in the inner loop.
- The third paragraph of Section 2 states that the paper introduces a novel energy minimization framework for solving problems of the general form of Equation (2). However, the actual problem solved by the authors is a doubly-relaxed version of the original problem. It is unclear how solving for a local optimum of this doubly-relaxed problem relates to the original problem the authors intended to address.
- The paper incorrectly claims that Geiger et al. defined non-linearities on a latent space of pre-defined dimensionality. This is inaccurate. Geiger et al.'s method determines the dimensionality of the latent space dynamically through a regularizer that promotes sparsity in the singular values. As a result, the latent space is not fixed but is constrained to be smaller than or equal to the dimensionality of the original space.
Additionally, the statement about LVMs such as GPLVMs that "the latent space is learned a priori with clean training data" is unclear. Within the Gaussian process framework, different noise models can be used, and the proposed method also assumes Gaussian noise (as seen in Equation 6), which is the simplest case for GP-based LVMs.
Finally, the paper's use of terms like "pre-training" and the claim that certain techniques lack a training phase are ambiguous. For instance, KPCA involves a closed-form update during training, which constitutes a training phase, even if it is not iterative.
In summary, while the paper introduces an interesting approach, it suffers from technical inaccuracies, weak experimental validation, and unclear claims. Addressing these issues would significantly strengthen the contribution.