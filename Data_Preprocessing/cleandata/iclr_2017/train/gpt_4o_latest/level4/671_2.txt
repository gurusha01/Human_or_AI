The authors propose a comprehensive framework for defining a broad spectrum of recurrent neural network architectures, encompassing seq2seq models, tree-structured models, attention mechanisms, and a novel class of dynamically connected architectures. Central to this framework is the introduction of a new, versatile recurrent unit termed the TBRU. The TBRU integrates a transition system (which specifies and constrains its inputs and outputs), an input function (mapping raw inputs to fixed-width vector representations), a recurrence function (determining the inputs to each recurrent step based on the current state), and an RNN cell (responsible for computing the output from the fixed and recurrent inputs). The paper provides numerous examples of how this framework can be instantiated, including sequential tagging RNNs, Google's Parsey McParseface parser, encoder-decoder networks, tree LSTMs, and other less conventional examples that highlight the framework's flexibility and utility.
The standout contribution of this work lies in its ability to seamlessly incorporate dynamic recurrent connections through the specification of the transition system. Notably, the paper investigates the utility of these dynamic connections in the context of syntactic dependency parsing, both as an independent task and in a multitask setup with extractive summarization. In this multitask scenario, the framework enables the use of shared compositional phrase representations as features for both parsing and summarization, replacing the discrete parse features used in prior work. This approach is particularly elegant and straightforward within the proposed framework. The experimental results demonstrate that multitasking improves summarization accuracy, while incorporating additional structure into existing parsing models enhances their accuracy without incurring additional computational complexity (e.g., compared to attention mechanisms).
The primary contribution of this work—its ability to represent and leverage dynamic recurrent connections—should be made as explicit and prominent as possible early in the paper. This is a key strength of the framework, but it risks being overshadowed by the broader discussion of its generality and its ability to represent existing architectures like attention and seq2seq models. This emphasis on generality can detract from the novelty of the work. For example, both AnonReviewer6 and I initially overlooked this critical point. To better highlight this contribution and its significance, the paper would benefit from a more detailed analysis of the representations enabled by the framework and their role in achieving the reported experimental results. Additionally, clarifying the distinction between a stack LSTM and Example 6 would help to further underscore the novelty and utility of the proposed approach.
In summary, this paper offers a meaningful contribution to the field, though its presentation could be refined to better emphasize its core innovations, and the analysis of experimental results could be expanded to provide deeper insights.