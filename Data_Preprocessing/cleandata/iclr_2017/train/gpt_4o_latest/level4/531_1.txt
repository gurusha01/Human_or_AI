This paper introduces a generative video model that decomposes scenes into a background and a collection of 2D objects (sprites). The optimization is conducted within a VAE framework.
The authors' use of the outer product of softmaxed vectors (producing a delta-like 2D map), combined with a convolution, is a particularly intriguing approach for achieving image translation with differentiable parameters. This method appears to be a compelling alternative to more complex differentiable resamplers (e.g., those employed in STNs) when translation alone is required.
Below, I provide several comments on aspects of the paper, particularly the experimental section, which lacks clarity. The experimental results, in particular, feel underdeveloped, with some findings only mentioned in passing without being explicitly presented, not even in the appendix.
For a highly novel and unconventional approach, focusing solely on synthetic experiments might be justifiable. However, while the method does exhibit some originality, it is disappointing that no effort was made to address a problem involving real-world data.
As an example, I suggest exploring aerial videos (e.g., those captured by drones), as the planar assumption made by the authors would likely hold in such scenarios.
Additionally, I recommend that the authors carefully proofread the manuscript. There are missing references (e.g., "Fig. ??"), incomplete sentences (e.g., the caption of Fig. 5), and the aforementioned shortcomings in the presentation of the experimental results.