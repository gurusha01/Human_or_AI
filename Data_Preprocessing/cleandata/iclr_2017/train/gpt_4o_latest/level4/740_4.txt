This paper introduces ParMAC, a novel parallel and distributed framework based on the Method of Auxiliary Coordinates (MAC) for learning nested and non-convex models, which are composed of multiple processing layers (e.g., deep networks). The core concept of MAC is to optimize the nested objective function, which is traditionally learned using chain-rule gradients. However, such methods are often inconvenient and difficult to parallelize. MAC addresses this by strategically breaking nested functional relationships through the introduction of auxiliary coordinates as equality constraints, and then optimizing a penalized function via alternating optimization over the original parameters (W step) and the auxiliary coordinates (Z step). Specifically, the W step updates the parameters by decomposing the nested model into independent submodels and training them using existing algorithms, while the Z step ensures consistency between the inputs and outputs of the submodels.
In this paper, the authors build upon MAC by proposing ParMAC, which adapts the inherent parallelism of MAC to distributed systems through data parallelism and model parallelism. The key assumption underlying ParMAC is that, in distributed systems with large datasets, minimizing data movement across the network is critical, as communication time often dominates computation time in modern architectures. The authors analyze the parallel speedup and convergence of ParMAC and demonstrate its effectiveness through an MPI-based implementation for optimizing binary autoencoders. The proposed framework is evaluated on three color image retrieval datasets.
The paper is well-organized, and the presentation is clear. However, I have the following questions:
- The MAC framework solves the original problem approximately. If a sigmoid function is used to smooth the stepwise function, naive optimization methods could potentially be applied more easily. What distinguishes the proposed approach from this alternative? Why is a new method necessary in this context?
- The authors do not compare their ParMAC framework with other distributed approaches designed for optimizing the same type of nested functions.