Paraphrased Review:
---
Paper Summary:  
This paper introduces a novel algorithm for generating k-adversarial images by altering a minimal fraction of image pixels, without requiring access to the classification network's weights.
---
Review Summary:  
The generation of adversarial images is a topic of considerable theoretical and practical relevance. While this paper presents a new method for addressing the problem, it has several shortcomings. The manuscript is overly verbose (spending excessive time on experiments of limited significance), poorly organized (key details of the main algorithm are scattered across sections 4 and 5, with an essential component introduced only in the experimental section 6), and, most critically, the experiments presented are not particularly compelling, leaving the primary conclusions ambiguous.  
This research direction appears promising but is not yet presented in a polished form. Substantial revisions would be necessary for this paper to be suitable for ICLR.
---
Pros:  
- The topic is engaging and relevant.  
- The black-box setting is particularly pertinent.  
- The paper includes multiple experiments.  
- Demonstrates that adversarial images can be created by altering only 1–5% of pixels.  
---
Cons:  
- The paper is overly lengthy, yet crucial details are insufficiently addressed.  
- Some experiments lack broader appeal.  
- Key experiments omit important metrics or additional baselines.  
- Limited technical innovation.  
---
Quality: The description of the method and the experimental setup requires improvement.  
Clarity: While the text is generally clear and somewhat formal, it is overly verbose and could benefit from greater conciseness.  
Originality: Although I am unaware of other works conducting these exact experiments, the approach and results are not particularly surprising.  
Significance: The work is incremental, and the experimental shortcomings diminish its potential impact.  
---
Specific Comments:  
- The paper should be shortened by 30–40%. Reducing its length would encourage more concise argumentation and descriptions, while also focusing on the most relevant experiments.  
- Section 4 appears problematic. If a single modified pixel can take values far outside the [LB, UB] range, then the test sample is evidently out of the training distribution. This would naturally cause most classifiers (e.g., decision forests or non-linear SVMs) to fail. These results would only be meaningful if the modified pixel values were constrained within the [LB, UB] range.  
- The range [LB, UB] is not explicitly defined. Is it specified anywhere? How does p = 100 compare to [LB, UB]? For clarity, p should be reported as a proportion of [LB, UB].  
- The modification is applied post-normalization. Is this a realistic scenario?  
- In Algorithm 2, why not constrain values to the [LB, UB] range?  
- Section 6, "implementing algorithm LocSearchAdv," is unclear regarding how p is adjusted, and new variables are introduced without explanation, leading to confusion.  
- In Section 6, what happens if p is not adjusted? What if a simple greedy random search is employed (e.g., testing 100 sets of 5 random pixels with value 255)?  
- In Section 6, PTB is calculated over all pixels, including unmodified ones. Why is this the case? This makes the LocSearchAdv PTB value incomparable to FGSM, as it conflates with PTBPixels (e.g., the claim of "far less average perturbation" in many cases).  
- Section 6 lacks discussion on the average number of model evaluations, which is equivalent to the number of queries made to a system being attacked. This metric is crucial for evaluating the "effectiveness" of black-box attacks. Currently, only an upper bound of 750 network evaluations is mentioned.  
- How does the number of network evaluations change when p is adjusted versus when it is not?  
- The paper emphasizes the top-k aspect, but only one experiment is provided. Either expand on this or temper the claims.  
- Why is FGSM ineffective for batch-normalized networks? Has this been reported previously? Are there other published methods that are effective in this context? Comparing against additional methods would enhance the paper.  
- If Section 4's results are of limited interest, what should readers take away from Section 6? That good results can be achieved by modifying only a few pixels? What about selecting the "top N" largest modified pixels from FGSM? Would this suffice? The baselines and key conclusions should be further developed.  
---
Minor Comments:  
- Footnotes are overused; most should be integrated into the main text.  
- Reiterate the definitions of key variables (e.g., p, r, LB, UB) two or three times for clarity.  
- Tables 1, 2, and 3 should be converted into figures.  
- The last sentence of the first paragraph in Section 6 is uninformative.  
- Replace "very tiny" with "small."  
---