Summary:  
The paper "Deep Variational Information Bottleneck" investigates optimizing neural networks for variational approximations of the information bottleneck (IB; Tishby et al., 1999). Using the MNIST dataset as an example, the authors demonstrate how this approach can be applied for regularization and enhancing robustness against adversarial attacks.
Review:  
The IB framework holds significant potential for impactful applications, such as regularization, adversarial robustness, and privacy, as highlighted in the paper. Integrating the IB with recent advancements in deep learning to broaden its applicability is a commendable direction. However, given that the theoretical contribution is a relatively straightforward application of established concepts, I would have expected a more comprehensive experimental section.
Since the proposed method enables scaling the IB, a more compelling demonstration would involve applying it to a larger and more complex dataset than MNIST. Additionally, it remains uncertain whether the proposed approach would effectively regularize deeper and more intricate neural networks with numerous layers.
Why was dropout omitted from the quantitative comparison of adversarial robustness in Figure 4?
What was the rationale behind selecting 12 samples?
What do the error bars in Figure 1(a) represent?
On page 7, the authors state that "the posterior covariance becomes larger" as beta "decreases" (did they mean increases?). Is this assertion accurate? It is difficult to assess based on Figure 1, as the plots are scaled differently.
It may also be valuable to compare the proposed approach to variational fair autoencoders (Louizos et al., 2016), which similarly aim to learn representations that minimize the information shared with a specific aspect of the input.
Overall, the paper is well-written and easy to understand.