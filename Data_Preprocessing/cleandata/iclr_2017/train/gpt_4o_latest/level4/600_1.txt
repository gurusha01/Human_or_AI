The paper introduces the concept of a group sparse autoencoder, which enforces sparsity in the hidden representation at the group level, where groups are defined based on labels (i.e., supervision). The hidden representation of the p-th group is utilized for reconstruction, with a group sparsity penalty applied, enabling the model to learn more discriminative, class-specific patterns in the dataset. Additionally, the paper proposes combining both group-level and individual-level sparsity, as outlined in Equation (9).
The clarity of the paper could be improved.  
- Do you exclusively use the p-th group's activation for reconstruction? If so, in Equation (9), do you employ all individual hidden representations for reconstruction, or do you still restrict it to the subset of representations corresponding to that specific class?  
- In Equation (7), the right-hand side appears to lack a summation over p, which might be a typographical error.  
- Is the algorithm fully end-to-end trainable? It seems that the group sparse CNN is essentially the GSA, where the input data is derived from features extracted by sequential CNNs (or other pretrained CNNs).
Additional comments are as follows:  
- The group sparse autoencoder is a (semi-) supervised method since it leverages label information to form groups, whereas the standard sparse autoencoder is fully unsupervised. Consequently, it is not surprising that the group sparse autoencoder learns more class-specific patterns compared to the sparse autoencoder. A fairer comparison would be with autoencoders that incorporate classification into their objective function.  
- While the authors claim that the GSA learns more group-relevant features, Figure 3(b) does not convincingly support this assertion. For instance, in the first row, several filters do not resemble the digit "1" (e.g., the filter in the very last column appears more like a "3").  
- Beyond visual inspection, have you observed any improvements in classification performance using the proposed algorithm in the MNIST experiments?  
- The paper lacks a comparison to an appropriate baseline model. The baseline should not be the sequential CNN alone but rather the sequential CNN combined with a sparse autoencoder. Additionally, more controlled experiments are necessary to compare the performance of Equations (7)-(9) under varying values of \(\alpha\) and \(\beta\).
Missing reference:  
Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016. This work explores block orthogonal matching pursuit for dictionary learning, where the blocks (i.e., projection matrices) are constructed based on class labels for discriminative training.