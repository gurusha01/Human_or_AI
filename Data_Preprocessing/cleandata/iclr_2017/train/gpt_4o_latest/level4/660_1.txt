The paper presents an extension of the Adam optimizer that dynamically adjusts the learning rate by evaluating the changes in the cost function values during training. The authors provide empirical evidence of the effectiveness of the Eve optimizer on tasks involving CIFAR convolutional networks, logistic regression, and recurrent neural networks.
I have the following concerns regarding the paper:
- The proposed approach is SENSITIVE to arbitrary shifts and scaling of the cost function.
- A more equitable comparison with other baseline methods would involve incorporating an additional exponential decay learning rate schedule within the lower and upper bounds of \(dt\). From Figure 2, it appears that \(1/dt\) effectively behaves like an exponential decay.
- The introduction of three additional hyperparameters (\(k\), \(K\), and \(\beta_3\)) adds complexity.
In summary, I believe the method has a fundamental flaw, and the paper provides minimal novelty. There is no theoretical justification for the proposed modification, and the authors should consider discussing the potential failure modes of their approach. Additionally, I found Section 3.2 difficult to follow. The clarity and overall writing quality of the method section could be significantly improved.