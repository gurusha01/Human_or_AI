[UPDATE]  
After reviewing the authors' response and the revised manuscript, I have raised my review score for two key reasons:  
1. I appreciate the authors' additional investigation into the differences between their work and prior approaches (e.g., Scheduled Sampling, Unsupervised Learning with LSTMs) and the insights they provided. The paper empirically demonstrates that the 100%-Pred training scheme outperforms others for high-dimensional video data and long-term predictions. It would be beneficial for the authors to briefly discuss these findings in the final version, either in the main text or the appendix.  
2. The revised manuscript includes more comprehensive results than the original submission. The results and discussions presented in this paper are likely to be valuable to the research community, as high-dimensional video prediction involves computationally intensive, large-scale experiments.  
---
Summary  
This paper introduces a novel RNN architecture for action-conditional future prediction. The proposed architecture integrates actions directly into the recurrent connections of the LSTM core, leading to improved performance compared to the prior state-of-the-art architecture [Oh et al.]. The paper also systematically explores and compares various architectural designs, such as frame-dependent versus frame-independent modes and observation-dependent versus prediction-dependent architectures. Experimental results demonstrate that the proposed architecture, combined with a fully prediction-dependent training scheme, achieves state-of-the-art performance across several challenging visual domains. Additionally, the paper shows that the proposed prediction architecture can enhance exploration in a 3D environment.  
---
Novelty  
The novelty of the proposed architecture is moderate. The primary distinction between this work and [Oh et al.] lies in how actions are incorporated: this paper integrates actions into the LSTM core, whereas [Oh et al.] combines actions after the LSTM. Furthermore, the concept of jumpy predictions has already been introduced in prior work, such as [Srivastava et al.].  
---
Experiment  
The experiments are well-designed and thorough. The paper evaluates various training schemes and compares multiple architectural designs across diverse and rich domains (e.g., Atari games, 3D environments). Moreover, the proposed method achieves state-of-the-art performance in several domains and demonstrates its utility in model-based exploration.  
---
Clarity  
The paper is well-written and easy to understand.  
---
Overall  
While the novelty of the proposed architecture is limited, the method achieves strong results on Atari games and 3D environments. Additionally, the systematic evaluation of different architectures provided in the paper will be a valuable resource for the research community.  
---
Reference  
Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov. Unsupervised Learning with LSTMs. ICML 2016.