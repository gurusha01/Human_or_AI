The foundation of this work lies in the understanding that decorrelated neurons (e.g., neurons that activate exclusively for background or foreground regions) provide independent pieces of information for subsequent decision-making. This approach effectively offers "complementary viewpoints" of the input to the following layers, which can be interpreted as ensembling or expert combination within the model itself, rather than relying on an ensemble of separate networks.
To achieve this, the authors introduce a reasonable method to decorrelate the activations of intermediate neurons, aiming to supply complementary inputs to the final classification layers. Specifically, they partition the intermediate neurons into "foreground" and "background" subsets and incorporate auxiliary losses that enforce these neurons to remain inactive on background and foreground pixels, respectively.
The authors demonstrate that this approach enhances classification performance on a mid-scale classification task (a subset of ImageNet, using a ResNet with 18 layers instead of the typical 150 layers), when compared to a "vanilla" baseline that does not include these auxiliary losses.
I found the paper enjoyable to read, as the proposed idea is straightforward, clever, and appears to be effective.  
However, I do have a few concerns:  
- Firstly, the proposed method seems highly tailored to vision tasks. In the vision domain, it is well-established that masking features (during both training and testing) can be beneficial, for example...