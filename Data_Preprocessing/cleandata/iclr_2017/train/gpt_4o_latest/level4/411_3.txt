Two aspects of this paper stood out to me:
1. The concept of employing a data-dependent proposal distribution for MCMC is particularly intriguing. While I was not previously familiar with this approach, it appears to have been introduced earlier. Upon reviewing the prior work, I found the (Zhu, 2000) paper to be largely incomprehensible, whereas the (Jampani, 2014) paper on informed sampling provided a much clearer explanation. However, this alone might not be a strong enough reason to justify acceptance at ICLR.
2. The results presented are notably impressive. A common heuristic is that optimization techniques typically yield around a 10% improvement in performance. The baseline MCMC results on randomly-generated programs align with this expectation, showing approximately a 15% improvement. However, the proposed algorithm achieves a remarkable ~33% speedup, which is both surprising and deserving of publication.
The primary argument against accepting this paper is its alignment with ICLR's focus. ICLR is not typically where I look for general machine learning research (venues like NIPS and ICML are more appropriate for that). Instead, ICLR emphasizes advancements in the automatic representation of data and models. While this paper might tangentially fit under that umbrella by addressing the representation of (generated) programs, it will likely face competition from more directly relevant submissions. A programming language conference might ultimately be a better venue for maximizing its impact.
That said, I recommend accepting this paper because it offers valuable insights, and the results are compelling.