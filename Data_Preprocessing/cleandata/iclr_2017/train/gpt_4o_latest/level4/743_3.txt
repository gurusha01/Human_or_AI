The concept of universality, which is independent of input distribution and dimension and relies solely on the algorithm, is an intriguing one. However, as an empirical investigation, this paper falls somewhat short:
1. The deep learning example includes only a single algorithm. A more compelling case could have been made by comparing distributions across multiple algorithms.
2. The definition in equation (1) and much of the content in Section 2.1 appear to overlap significantly with Deift (2014), Section 1.3. That work examines several algorithms for solving linear systems, making the notion of universality more convincing. In contrast, this paper lacks sufficient algorithmic comparisons under the same problem setup.
3. From a practical standpoint, practitioners are likely to be concerned with both the mean and variance of running times, which are obscured in equation (1). This raises questions about the practical utility of the distribution itself for algorithm tuning.
At a minimum, the paper should include a broader range of empirical comparisons to demonstrate that universality holds across a diverse set of algorithms.