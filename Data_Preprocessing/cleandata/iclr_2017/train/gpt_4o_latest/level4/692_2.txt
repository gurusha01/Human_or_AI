The paper introduces an approach to enhance the attention mechanism for sentiment classification by incorporating global context derived from a Bi-LSTM. The proposed models demonstrate superior performance compared to several existing models across three sentiment analysis datasets.
However, the central idea of leveraging Bi-LSTM to compute global context for attention is not novel, as it has been explored in prior works, such as Luong et al. (2015) and Shen & Lee (2016). In particular, Luong et al. (2015) explicitly proposed combining global context with local context for attention mechanisms.
With respect to the experiments, while it is commendable that the model performs well without relying on techniques like dropout or pre-trained word embeddings, it would be even more compelling if the model could also achieve strong results when such techniques are employed. The authors are encouraged to present results for models incorporating these techniques and provide comparisons with existing results in the literature.  
Reference:  
Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015