The authors present a learnable method for reducing the dimensionality of filters in deep neural networks. While the approach is intriguing, the work appears somewhat underdeveloped.
1. The manuscript contains numerous typographical errors.  
2. The experimental results are relatively weak and fail to demonstrate significant improvements in accuracy. Alternatively, the authors could frame this work as a compression technique and compare it against low-rank approximations of filters in DNNs, but this comparison is missing.  
3. Beyond compression, the proposed OMG method could be interpreted as a regularization technique aimed at reducing the network's excess capacity to enhance generalization. However, this perspective is not explored in sufficient depth.  
4. If the authors intend to position their approach in the context of 1-shot learning, they should benchmark it against methods like siamese and triplet networks. Unfortunately, such evaluations are absent.