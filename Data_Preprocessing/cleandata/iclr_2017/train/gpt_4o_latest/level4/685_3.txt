In this submission, the authors explore an intriguing approach to character-based language modeling that incorporates word-level representations in the context and, optionally, in the output. However, the approach lacks novelty, as it has already been addressed in prior works, such as (Kim et al., 2015) and (Jozefowicz et al., 2016), both of which are cited in the submission. These earlier studies extend beyond the current work by employing RNNs/LSTMs and, in the case of Jozefowicz et al., offering a comparative discussion of various character-level modeling approaches. Such a discussion is notably absent in this submission and should be included to contextualize the current work. The remaining novelty lies in the application of the approach to machine translation, but the discussion on how reranking N-best lists addresses the OOV problem is unclear. The authors should elaborate on the translation-specific aspects of the OOV issue. Additionally, some claims in the submission appear overstated, such as the assertion in Sec. 2.3 that the approach makes "the notion of vocabulary obsolete." This claim is inconsistent with the authors' own reservations about interpreting perplexity without an explicit output vocabulary. For instance, modeling frequent word forms is still expected to contribute significantly, as demonstrated in works like arXiv:1609.08144.
In Sec. 2.3, the statement that the objective requires a finite vocabulary is only accurate if the units are restricted to full word forms. By using subwords or individual characters, it is possible to implicitly model larger or even infinite vocabularies under the log-likelihood criterion, albeit with a different model than the one proposed here. This statement should be qualified accordingly.
The use of character embeddings in the output requires clarification, as the description in Sec. 2.4 lacks sufficient detail. Similarly, the configuration and parameterization of NCE training, as described in Sec. 3.4, should be elaborated to provide a clearer understanding of how these choices were made.
In Sec. 4.1, the authors should acknowledge that (Kim et al., 2015) reached similar conclusions regarding the performance of character embeddings in the output and discuss the improvement suggestions provided in that work. In Sec. 4.2, the authors mention the difficulty of interpreting perplexity for models without an explicit output vocabulary. However, there are methods to calculate and interpret perplexity for unknown words, as shown in (Shaik et al., IWSLT 2013), which should be referenced. Additionally, the size of the full training vocabulary should be reported in Sec. 4.4 and Table 4.
Minor Comments:
- p. 2, bottom: "three different input layer" → "three different input layers" (plural)
- Fig. 1: The fonts within the figure are too small.
- p. 3, first item below Fig. 1: "that we will note WE" → "that we will denote WE"
- Sec. 2.3: "the parameters estimation" → "the parameter estimation" (or "the parameters' estimation")
- p. 5, first paragraph: "in factored way" → "in a factored way"
- p. 5, second paragraph: "a n-best list, a nk-best list" → "an n-best list, an nk-best list"
- Sec. 4.2, last sentence: "Despite adaptive gradient," → verb and article missing.
Additional Questions and Suggestions:
- In Sec. 4.2, the authors mention the difficulty of interpreting perplexity for models without an explicit output vocabulary. Have they considered renormalizing perplexity to the character level, as discussed in (Shaik et al., IWSLT 2013)?
- Did the authors take into account the conclusions and improvement suggestions from (Kim et al., AAAI 2015) regarding the performance of character-level embeddings?
- Can the authors provide more details on the configuration of the NCE training?
- Based on the notation, it appears that a feed-forward neural network was used. Can the authors confirm this?
- Is the character-level word embedding used in this work the same as that in (Kim et al., AAAI 2015)? This is not explicitly cited in Sec. 2.1.
Notation and Definitions:
- Please define the use of the colon in the first equation of Sec. 2.1.
- The term \( P^H((w:H) \in D) \) is not defined before its introduction in the second equation of Sec. 2.3. Additionally, the sentence introducing this equation refers to "this probability" without explicitly stating what probability is being referenced.
- Please define \( e^{out} \) and \( e^{char-out} \) in Sec. 2.4. Are these terms the same as \( e^{out}_w \) in Sec. 2.2 and \( e^{char} \) in Sec. 2.1?