Updated Review: The authors have done an excellent job addressing and incorporating reviewer feedback. Notably, they conducted additional experiments and included a new, stronger baseline (the ConvNet -> LSTM baseline requested by multiple reviewers). However, I still have two remaining concerns that were previously raised: (1) each model's architecture (e.g., number of hidden units) should be tuned independently, and (2) a pure time series forecasting baseline (without trend preprocessing) should be evaluated. Based on the revisions, I am increasing my score from a clear rejection to a borderline decision.
---
This paper focuses on time series prediction problems where the targets include the slope and duration of upcoming local trends. This is an important setting for real-world applications (e.g., financial markets), where decisions (e.g., buy or sell) are often influenced by local changes and trends. The primary challenge lies in distinguishing genuine changes and trends (e.g., a downturn in share price) from noise. The authors propose an intriguing hybrid architecture (TreNet) with four components: (1) preprocessing to extract trends, (2) an LSTM that takes these trends as inputs to model long-term dependencies, (3) a ConvNet that processes a local window of raw data at each time step, and (4) a higher-level "feature fusion" (dense) layer that combines the outputs of the LSTM and ConvNet. On three univariate time series datasets, TreNet outperforms competing baselines, including those based on its individual components (LSTM + trend inputs, CNN).
Strengths:
- The paper addresses a compelling problem setting that can reasonably be argued to differ from other sequential modeling tasks in deep learning (e.g., video classification). This is a thoughtful example of task-driven machine learning.
- Assuming the authors' premises hold, the proposed architecture appears intuitive and well-designed.
Weaknesses:
- While the problem setting (decisions driven by trends and changes) is interesting, the authors do not provide a strong justification for their specific formulation of the machine learning task. Trend targets are derived using a deterministic algorithm applied to raw data, rather than being provided by an external source (e.g., a data oracle). Consequently, this problem could just as easily be framed as a standard time series forecasting task, where the next 100 steps are predicted and then converted into trends using the same algorithm. Accurate forecasts would naturally yield accurate trends.
- The proposed architecture, though interesting, lacks sufficient justification—particularly the decision to process the extracted trends and raw data through separate LSTM and ConvNet layers, which are only combined later via a shallow MLP. A more intuitive alternative would be to feed the ConvNet's output into the LSTM, possibly augmented with the trend input. Without a clear rationale, this unconventional design choice feels arbitrary.
- Building on the previous point, natural baselines such as raw->ConvNet->LSTM and {raw->ConvNet, trends}->LSTM are missing from the experiments.
- The paper assumes, rather than demonstrates, the utility of the extracted trends and durations as inputs. It is plausible that, with sufficient training data, a powerful ConvNet->LSTM architecture could learn to identify these trends directly from raw data, provided they are predictive.
- Related to the above, two additional baselines are notably absent: raw->LSTM and {raw->ConvNet, trends}->MLP. The authors propose a complex architecture without adequately demonstrating the value of its individual components (trend extraction, LSTM, ConvNet, MLP). This makes the baselines unnecessarily weak.
One broader concern is the practice of using the same LSTM and ConvNet architectures for both the baselines and TreNet. While this might seem like an apples-to-apples comparison, it could inadvertently disadvantage one approach due to differences in hyperparameter tuning. A more thorough evaluation would involve independently optimizing each architecture.
Regarding related work and baselines: It is reasonable to limit the scope of analysis and experiments to a representative set of baselines, particularly in a deep learning conference paper. However, the authors overlook a significant body of work on financial time series modeling using probabilistic methods and related techniques. This is another way to approach the "separate trends from noise" problem—by treating observations as noisy. For example, the following paper is relevant: J. Hernandez-Lobato, J. Lloyd, and D. Hernandez-Lobato. Gaussian process conditional copulas with applications to financial time series. NIPS 2013.
While I appreciate the direction of this research, I do not believe the current manuscript is ready for inclusion at ICLR. My policy during interactive review is to remain open-minded and willing to adjust my score, but I feel that a major revision would be necessary. I encourage the authors to use this feedback to refine their work for submission to a future conference (e.g., ICML).