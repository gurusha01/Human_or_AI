The paper introduces the semantic embedding model (SEM) for multi-label prediction.  
In my comments, I raised concerns about the assumption that the number of labels to predict is known beforehand. The authors responded by stating that this is an orthogonal issue, but I disagree with this characterization.  
I also sought clarification on how SEM differs from a basic multi-layer perceptron (MLP) with a softmax output, trained using a two-step approach instead of stochastic gradient descent. Given the apparent similarities, it seems reasonable to compare SEM against this straightforward baseline.  
Regarding the sampling strategy used to estimate the posterior distribution and its distinction from the method proposed by Jean et al., I acknowledge that there are slight differences. However, I strongly believe the paper should explicitly reference Jean et al. and clearly highlight these distinctions.  
Finally, I have a question about the terminology: why is the model referred to as using "semantic" embeddings? Typically, this term implies that the embeddings capture some semantic relationships, but such properties do not seem evident in this work.