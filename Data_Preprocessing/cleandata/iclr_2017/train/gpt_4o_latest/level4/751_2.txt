- The issue of managing highly rewarding or hazardous states is significant and has been thoroughly explored in the reinforcement learning (RL) literature. Following the pre-review feedback, the authors state that they have included comparisons with expected SARSA; however, I believe that incorporating these results along with additional comprehensive baselines is necessary before this paper can be considered for acceptance.
- Moreover, there is a growing body of work on utilizing reward replay buffers in deep RL agents (e.g., Jaderberg et al., "Reinforcement learning with unsupervised auxiliary tasks"; Blundell et al., "Model-free episodic control"; Narasimhan et al., "Language understanding for text-based games using deep reinforcement learning"). Such methods could potentially help the agent avoid revisiting catastrophic states, and their relevance to the proposed approach should be further explored.
- In general, the proposed method lacks a strong theoretical foundation. For example, why is catastrophe not directly incorporated as a signal for the learner, rather than being modeled separately?