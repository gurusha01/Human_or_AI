The authors have conducted thorough work in compiling all the reported data. However, many of the findings do not appear particularly surprising to me:
- Finding 1 demonstrates that all architectures and batch sizes achieve full GPU utilization (or the same percentage of utilization).
- Finding 2 suggests that the hyperbolic relationship can be inferred from the linear trend in Figure 9. While I agree this conclusion is plausible, its relevance hinges on whether it holds for the latest model generations. These models, clustered in the upper left corner of Figure 9, do not exhibit a pronounced linear trend on their own. Consequently, I believe there is insufficient evidence to assert asymptotic hyperbolic behavior. For such a conclusion, the linear trend should become more pronounced as models approach the upper left corner.
- Finding 3 appears to be a straightforward extension of Finding 1: If slower models are more accurate and faster models consume the same power, then Finding 3 logically follows.
- Finding 4 is similarly related to Finding 1: If all architectures fully utilize the GPU, inference time should scale proportionally with the number of operations.
Perhaps the most intriguing observation is that all tested models seem to utilize the same percentage of the GPU's computational resources. This is somewhat unexpected, as one might anticipate that more complex models would struggle to fully utilize computational resources due to inter-dependencies. However, GPU utilization was not directly measured, and since the authors used an older GPU, it is reasonable to assume that all models could fully exploit the available computational power.
Finally, I believe these findings would be more compelling if they were contextualized with compression techniques or tested on production networks, as this would enhance their practical relevance.