This paper presents a multi-view learning framework for deriving representations from acoustic sequences. The authors explore the application of bidirectional LSTMs in conjunction with contrastive loss functions. Experimental results demonstrate advancements over prior approaches.
While I lack specific expertise in speech processing, I support accepting this paper due to the following contributions:  
- Exploring the application of a well-established architecture in a novel domain.  
- Proposing innovative objectives tailored to the domain.  
- Establishing new benchmarks for evaluating multi-view models.  
I encourage the authors to open-source their implementation to facilitate reproducibility, enable comparisons, and foster further advancements in this area.