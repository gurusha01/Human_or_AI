This paper introduces a training methodology for deep neural networks. Initially, the network undergoes standard training. Subsequently, weights with small magnitudes are set to 0, while the remaining weights continue to be trained. Finally, all weights are jointly fine-tuned. Experimental results across diverse image, text, and speech datasets demonstrate that this approach achieves high-quality outcomes.
The proposed method is both novel and intriguing. While it bears some resemblance to Dropout, as acknowledged in the paper, the deterministic nature of weight clamping sets it apart.
The primary strength of this approach lies in its simplicity. It requires only three hyper-parameters: the proportion of weights to clamp to 0, and the number of training epochs allocated to the initial dense phase and the subsequent sparse phase. With these parameters, the method can be seamlessly integrated into the training process of various network architectures, as evidenced by the experiments.
However, I have some reservations about the current empirical evaluation. As highlighted during the question phase, it appears that the baseline methods were not trained for as many epochs as the proposed method. Standard techniques, such as reducing the learning rate upon "convergence" and continuing training, could be applied. The authors' response suggests that these techniques might yield effective results. A more comprehensive empirical analysis, examining performance across epochs, learning rates, and other factors, would enhance the paper. Additionally, an investigation into the impact of the sparsity hyper-parameter would be a valuable addition.