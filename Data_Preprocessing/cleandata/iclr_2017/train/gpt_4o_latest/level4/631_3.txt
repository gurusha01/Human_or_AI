This paper introduces a cascade of paired (left/right, up/down) 1D RNNs as a module within CNNs to efficiently incorporate global context information into features without requiring the stacking of numerous convolutional layers. Experimental results are provided for image classification and semantic segmentation tasks.
Pros:
- The paper is well-written and easy to follow.
- Sufficient implementation details are provided, making the work reproducible even without access to the source code.
- The integration of 1D RNNs into CNNs is an underexplored area that warrants further experimental investigation.
Cons (detailed below):
(1) The contributions, particularly in comparison to Bell et al., are incremental.
(2) The proposed L-RNN module is not utilized as broadly as suggested in the introduction.
(3) The classification experiments lack persuasiveness.
(1,2): In the introduction, the authors highlight two key differences compared to Bell et al.: (1) the L-RNN module is treated as a general block that can be inserted at any layer of a modern architecture, such as within a residual module, and (2) the L-RNN can be incorporated into a pre-trained FCN by initializing with zero recurrence matrices, enabling end-to-end fine-tuning (as discussed in Section 4).
While these contributions seemed promising in the introduction, they felt less compelling after reviewing the experimental sections. Regarding the first contribution, I anticipated seeing the L-RNN block integrated throughout the CNN, starting from earlier layers. However, in the classification and segmentation experiments, the module is only placed near the network's end. Although the approach differs from Bell et al. in technical details, it remains quite similar in overall design. The paper does not directly compare its approach to Bell et al., leaving it unclear whether the proposed design offers any tangible advantages or is merely a variation with comparable performance. Additionally, the question remains unanswered as to how the L-RNN would perform if integrated earlier in the network, as implied in the introduction.
The second contribution appears more robust but still falls short of being a "substantive difference." Bell et al. also incorporate 1D RNNs into an ImageNet-pretrained VGG-16 model. That said, the zero-initialization method proposed here may be more elegant, as it avoids the need for two-stage training (freezing lower layers initially and unfreezing them later).
(3): I am generally skeptical of the value of classification experiments on CIFAR-10 when presented in isolation, without accompanying results on datasets like ImageNet. CIFAR-10 is neither a particularly challenging task nor a reliable indicator of generalization to other tasks. In contrast, ImageNet has proven useful for producing features that generalize well across tasks. Demonstrating strong performance on ImageNet is more likely to indicate a model's ability to learn transferable features. Ideally, the paper would also show that the benefits observed on ImageNet transfer to at least one downstream task, such as object detection.
Another issue with the CIFAR experiments is the lack of a direct comparison between models A-F with and without the L-RNN module. The current results make it difficult to discern whether the L-RNN module provides a meaningful improvement. Overall, the CIFAR experiments offer limited insights.
Minor suggestion:
- Figure 4 is difficult to interpret. The pixelated, rounded corners on the yellow boxes are visually distracting.