The paper presents a novel application of the sticky HDP-HMM, aimed at accurately identifying the number of components in bird and whale songs across diverse datasets. It is commendable to see the model applied to such an intriguing dataset. However, my primary concerns with the paper pertain to its structure and the representation choices within the model. Specifically:
The paper's organization could be significantly improved. There is considerable repetition in the introduction that does not add much value. The first and last two sentences of the abstract could be omitted, as they are redundant. Much of the abstract essentially reiterates the introduction. Similarly, the second paragraph of section 2.3 repeats content already covered in the introductionâ€”by this point, the reader is already familiar with the paper's objectives. Additionally, references such as Kershenbaum (2014) may not be familiar to most readers. The description of the data would be more appropriately placed in the experiments section. The phrase "Different hypotheses for the songs were emitted" in the introduction is somewhat awkwardly worded. Figure 4 should be moved to the introduction as the first figure, while Figure 5 would fit better in the methods section. A summary of Table 1 should also be included in the experiments section. Overall, the writing could be made more concise, which would free up space for these figures. On the other hand, the description of the HDP-HMM, which largely follows existing literature, is well-executed.
Some broader questions regarding the methods employed:
If scalability in inference is a key concern, why was Gibbs sampling chosen? Why not use the beam sampler (van Gael 2008), which has been considered state-of-the-art for MCMC inference in the HDP-HMM? More broadly, why rely on MCMC at all? For very large datasets, much of the Bayesian machine learning community has shifted toward stochastic variational inference as a more practical alternative (e.g., Wang, Paisley, and Blei 2011).
If the primary interest lies in determining the number of clusters, how do you address the issue that DP mixture models are known to be inconsistent for estimating the true number of clusters (Miller and Harrison 2013)?
MFCC features are designed for the human auditory system rather than those of birds or whales. In your datasets, do you adjust the MFCC scale to better align with the auditory systems of the animals producing the songs?
Finally, a suggestion for future work that could build on the results presented here:
Given the recent success of LSTMs in speech recognition, it is possible that deep learned representations could outperform linear features (such as the cluster means in an HDP-HMM) for modeling animal songs. Have you considered exploring a hybrid approach, akin to recent work that combines autoencoders with graphical models (Johnson, Duvenaud, Wiltschko, Datta, and Adams 2016)?