This paper introduces two RNN-based architectures for extractive document summarization. The first model, termed Classifier, considers the sequential order of sentences as they appear in the original document, while the second model, Selector, selects sentences in a non-sequential, arbitrary order. In both architectures, the concatenated RNN hidden states from the forward and backward passes of a sentence are utilized as features to compute a score that reflects content richness, salience, positional importance, and redundancy. The models are trained in a supervised manner, with the authors employing "pseudo-ground truth generation" to derive training data from abstractive summaries. Experimental results indicate that the Classifier model outperforms the Selector model and achieves near state-of-the-art performance on certain evaluation metrics.
The proposed approach can be seen as an extension of the work by Cheng and Lapata (2016). However, its performance is only marginally better and, in some cases, even worse. The authors highlight a key difference in how they generate gold labels for supervised training by transforming abstractive summaries. Yet, they also acknowledge that one possible explanation for their models' inconsistent performance compared to Cheng and Lapata's extractive model is the noisier ground truth labels produced by their unsupervised greedy approximation method. If Cheng and Lapata's method for constructing training data yields better results, is there a specific reason for not adopting a similar approach?
To make the proposed models more compelling, they need to consistently outperform this baseline, which closely resembles the proposed methods. This is particularly important since the primary contribution of the paper lies in presenting improved neural architectures for extractive document summarization.