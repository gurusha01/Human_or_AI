This paper introduces a novel generative model inspired by an annealing process, where the transition probabilities are directly learned to optimize a variational lower bound on the log-likelihood. The proposed approach is innovative and intriguing, but the paper would benefit from more comprehensive quantitative validation and a deeper exploration of its connections to prior work.
Regarding related work, both AIS and RAISE are closely related algorithms that share significant mathematical similarities with the proposed method. Simply mentioning these methods briefly in the related work section is insufficient; the paper should provide a detailed discussion of how these methods relate to the proposed variational walkback approach. Based on my understanding, the proposed method appears to be an extension of RAISE, where the transition probabilities are learned rather than predefined using an existing MRF. While this extension is both interesting and valuable, the relationship to prior work must be explicitly clarified.
The analysis presented in Appendix D seems flawed. The derived formula for the ratios of prior and posterior probabilities is only valid under the assumption of constant temperature, where the ratio becomes very large. However, when the temperature varies, Neal (2001) provides the correct analysis, which leads to a different result.
A key strength of the proposed method is its ability to optimize a variational lower bound on the log-likelihood, with even more precise estimates achievable through importance sampling. Given this, it should be straightforward to report log-likelihood estimates for the method. The absence of such estimates is puzzling, especially since there are numerous prior results on MNIST for comparison. Additionally, RAISE would serve as a natural baseline to evaluate whether the ability to learn transition probabilities offers a tangible advantage.
I believe the core idea of the paper is solid, and I would consider raising my score if the authors address the aforementioned concerns in a revised version.
Minor comments:
- The statement, "A recognized obstacle to training undirected graphical modelsâ€¦ is that ML training requires sampling from MCMC chains in the inner loop of training, for each example," seems overly critical. The standard algorithm, PCD, typically requires only a single step per mini-batch, which is a more efficient approach.
- Some of the methods mentioned in the related work section lack proper citations.
- The justification for the method is framed in terms of "carving the energy function in the right direction at each point," but this may not accurately reflect the method's essence. Isn't the primary advantage that it optimizes a lower bound on the log-likelihood, thereby enabling a globally consistent allocation of probability mass?