This paper demonstrates that an LSTM can be modified such that its gates depend only on a fixed window of recent inputs—ht = f(xt, x{t-1}, ...x{t-T})—rather than the conventional formulation—ht = f(xt, h_{t-1}). By doing so, the computation shifts from a predominantly serial process to a more parallelizable one, thereby increasing both speed and efficiency. The idea is simple, effective, and intriguing, but its presentation is somewhat hindered by unclear language.
- I recommend that the authors clarify their explanation of the proposed model.  
- Additionally, it would be helpful to explicitly include some big-O complexity analyses or provide concrete examples to illustrate the sources of the speed improvements.  
- Beyond these points, the experiments appear sufficient, and I found the paper enjoyable to read.
If this approach can be reliably replicated and proves effective across diverse settings, it has the potential to be a significant contribution and a widely adopted neural network component.