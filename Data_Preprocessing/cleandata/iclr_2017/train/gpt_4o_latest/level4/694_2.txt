- It is once again demonstrated that end-to-end training with deep neural networks yields significant improvements over traditional hybrid systems that rely on hand-crafted features. The results are impressive for the small-vocabulary grammar task defined by the GRID corpus. The engineering effort behind this work is clearly commendable, and it will be intriguing to observe how the system performs on large-vocabulary language modeling tasks. A comparison to human lip-reading performance in conversational speech scenarios would also be highly compelling.
- Conventional AV-ASR systems that utilize weighted fusion of audio and visual posteriors effectively reduce to pure lip reading when the weight is entirely shifted to the visual modality. Numerous performance curves are presented for this visual channel under low audio SNR conditions, covering both grammar and language modeling tasks.
- Traditional hybrid AV-ASR approaches also employ sentence-level sequence training using objectives such as fMPE, MPE, and MMI (refer to earlier literature). Therefore, it cannot be claimed that this work introduces the first sentence-level objective for lip-reading models, as this would be analogous to asserting that sequence training did not exist in hybrid LVCSR ASR systems prior to the advent of CTC.