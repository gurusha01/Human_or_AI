This paper introduces Generative Adversarial Parallelization (GAP), a scheduling approach designed to train N Generative Adversarial Networks (GANs) concurrently. GAP operates by periodically shuffling the assignments between the N generators and N discriminators over the course of training. This randomized pairing compels each generator to compete against various discriminators, which the authors argue helps mitigate the common issue of "mode collapse" in GANs.
I have three primary concerns regarding this submission:
1) The authors suggest selecting the best generator after training the N GANs using the GAM metric. I disagree with this approach for two reasons. First, a single GAN is unlikely to capture the full complexity of the true data distribution. In other words, a single generator with limited capacity will either represent one mode well or inadequately represent multiple modes. Second, the GAM metric depends on discriminator scores, which can be unreliable and prone to focusing on artifacts. Controlled mode collapse is not inherently problematic in such cases. Consequently, I believe a better alternative would be to retain all generators and combine them into a mixture model. This would, however, necessitate determining appropriate mixture weights, which could be achieved, for example, through rejection sampling based on discriminator scores.
2) The paper lacks a theoretical or conceptual comparison to dropout. Fundamentally, the proposed method bears a strong resemblance to dropout: each generator competes against all N discriminators, but at each epoch, N-1 discriminators are effectively "dropped" for each generator. Furthermore, in dropout, all neurons are retained after training, forming an approximation of a large ensemble of neural networks. A discussion of these parallels would strengthen the conceptual foundation of the work.
3) The qualitative results are unconvincing. Most of the figures focus exclusively on GAP, without providing baseline comparisons. For instance, the GAN and LAPGAN papers include similar samples, but the lack of baseline samples here makes it difficult to assess the improvements introduced by GAP. Additionally, Figures 3 and 4 are not compelling; for example, the generator in Figure 3 appears to be under-parameterized, which undermines the results.
As a minor point, I recommend removing Figure 2 for three reasons: it may be subject to copyright restrictions, it occupies significant space, and it adds little value to the explanation. Additionally, the indices (i_t) in Algorithm 1 are undefined and should be clarified.
In summary, while this paper presents promising ideas, it requires further conceptual refinement and more robust experimental validation to reach its full potential.