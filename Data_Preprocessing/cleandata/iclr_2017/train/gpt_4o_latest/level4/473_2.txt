This paper introduces a theoretical framework for linking parameters between input word embeddings and output word representations in the softmax layer.  
Experiments conducted on the PTB dataset demonstrate significant improvements.  
While the concept of sharing or tying weights between input and output word embeddings is not novel (as highlighted by others in this discussion), I view this as the primary drawback of the paper. However, the proposed justification for this approach appears novel to me and is certainly intriguing.  
I was initially concerned that the results were limited to a single dataset, PTB, which is somewhat outdated in this field. I appreciate that the authors made an effort to test on at least one additional dataset, and I believe it would be valuable to incorporate these results into the paper if it is accepted.  
Have you considered exploring the use of character-level or sub-word units in this context?