This paper presents a novel approach to language modeling that is applicable to both programming languages and natural languages. The method employs a program synthesis algorithm to explore the program space and utilizes count-based estimation to determine program weights. This represents a significant departure from neural network-based methods, which depend on gradient descent and are consequently much slower to estimate. Traditional count-based methods, such as regular n-gram models, are limited by their simplicity, as they struggle to capture large contexts and scale poorly with increasing context size. The proposed method leverages MCMC to synthesize programs, enabling it to learn context-sensitive probabilities through count-based estimation. As a result, it is both computationally efficient and capable of modeling long-range dependencies.
Experimental results on programming language datasets, specifically the Linux kernel corpus, demonstrate that this method significantly outperforms both LSTM and n-gram language models. On the Wikipedia corpus, the method is competitive with state-of-the-art models, though it does not surpass them. Additionally, the estimation and query times are substantially faster than those of LSTM-based language models and are comparable to those of n-gram models.
There is some debate about whether this paper aligns with ICLR's emphasis on neural network-based approaches. However, in the interest of fostering diversity and encouraging novel ideas, such "outside" contributions should be welcomed at ICLR. This paper has the potential to inspire further research at the intersection of program synthesis and machine learning, a theme that gained traction at NIPS 2016.
Pros  
1. Innovative approach.  
2. Strong empirical results.  
Cons  
1. Certain key algorithmic details are missing from the paper and should be included, at least in an appendix, for completeness.  
Comments  
1. Please add n-gram results to the table for the Wikipedia experiments.