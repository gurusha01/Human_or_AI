The paper establishes a connection between stochastically perturbing a model's parameters during training and optimizing a mollified objective function. While Eqs. 4-7 were somewhat challenging to interpret, particularly regarding the precise meaning of the weak gradient \( g \), Eq. 8 is intuitive, and Section 2.3 effectively demonstrates that, for a specific class of mollifiers, minimizing the mollified loss is equivalent to training with Gaussian parameter noise.
The authors extend this framework by introducing generalized mollifiers to enable a more advanced annealing effect, which can be applied to state-of-the-art neural network architectures such as deep ReLU networks and LSTM recurrent networks. However, the resulting annealing behavior can be counterintuitive. For instance, in Section 4, the Binomial (or possibly Bernoulli?) parameter transitions from 0 (representing deterministic identity layers) to 1 (representing deterministic ReLU layers), suggesting that the network initially undergoes a phase of noise addition. This progression may, in fact, counteract the intended effect of annealing.
The annealing schemes presented appear highly engineered in practice. For example, Algorithm 1, which governs how units are activated within a given layer, involves a sequence of nine distinct steps.
Given the conceptual nature of the authors' contribution—where the application of the mollifying framework is novel compared to existing annealing schemes—it would have been beneficial to allocate part of the paper to analyzing simpler models with basic (non-generalized) mollifiers. For instance, it would have been insightful to include examples where perturbation schemes derived from the mollifier framework are demonstrably more effective for optimization than standard heuristically designed perturbation methods.