The authors investigate the application of low-rank approximation to matrix multiplication in RNNs. This approach significantly reduces the number of parameters, and with the addition of a diagonal component (referred to as low-rank plus diagonal), it is demonstrated to perform comparably to a fully-parameterized network across various tasks.
The paper is well-executed, with the primary limitation being certain claims about conceptual unification. For instance, the first line of the conclusion states, "We presented a framework that unifies the description of various types of recurrent and feed-forward neural networks as passthrough neural networks." However, this framework is not a novel contribution of the paper, as it is already well-established within the community, and RNNs have been described in this manner previously.
Aside from this minor issue, the main contribution lies in successfully implementing low-rank RNNs, with results that are generally on par with fully-parameterized networks. However, the results are not significantly better, raising questions about the practical advantages of adopting low-rank networks. While the contribution may not be particularly strong in terms of performance improvements, achieving comparable results with fewer parameters is non-trivial, and the studies are thoughtfully conducted and clearly presented.