This paper presents a strong contribution by introducing structure into attention mechanisms, where the attention posterior probabilities are treated as structured latent variables. The authors demonstrate the approach using segmental attention (akin to semi-Markov models) and syntactic attention (similar to projective dependency parsing) across both synthetic tasks (e.g., tree transduction) and real-world applications (e.g., neural machine translation and natural language inference). While the performance improvement of structured attention over simple attention in the latter tasks is modest, the results are promising. Overall, this is a clear accept.
The paper is well-written, the proposed method is novel and compelling, and the experiments provide a solid proof of concept. That said, the potential of structured attention in neural MT appears underexplored. Segmental attention could serve as a foundation for neural phrase-based MT, while syntactic attention offers a pathway to integrate latent syntax into MT. These directions seem highly promising. Specifically, it would be valuable to investigate whether incorporating (semi-)supervision into these attention mechanisms—such as using posterior marginals derived from an external parser—could enhance the learning process or at least provide better initialization for the attention components.
This work also appears to be the first significant application of backpropagation through forward-backward/inside-outside algorithms (Stoyanov et al., 2011). As highlighted in Section 3.3, for general probabilistic models, the forward step over structured attention computes first-order moments (posterior marginals), while the backpropagation step computes second-order moments (gradients of marginals with respect to log-potentials, equivalent to the Hessian of the log-partition function). This generalization broadens the applicability of the proposed method to any graphical model where these computations are efficient. For instance, could a generalized matrix-tree theorem enable backpropagation for non-projective syntax? On the downside, the reliance on second-order statistics might introduce numerical instability in certain scenarios, particularly due to the use of the signed log-space field. Was this issue encountered in practice?
Minor comments/typos:
- Last paragraph of Section 1: "standard attention attention"
- Third paragraph of Section 3.2: "the on log-potentials"
- Section 4.1, Results: "... as it has no information about the source ordering" — could you clarify what is meant here?