This paper introduces a generative model for video sequence data, where each frame is assumed to be produced by compositing a 2D sprite onto a static background at each timestep. The sprite is allowed to dynamically change both its appearance and position within the image across frames. The approach follows the Variational Autoencoder (VAE) framework, utilizing a recognition/inference network to recover the latent state at each timestep.
The paper includes results on simple synthetic datasets, such as a moving rectangle on a black background and the "Moving MNIST" dataset. However, the results appear preliminary, and the assumptions underlying the model seem overly restrictive for application to real-world video data. On the Moving MNIST dataset, the numerical results are not competitive with state-of-the-art methods.
In terms of novelty, the model itself is not particularly innovative, and the paper omits several important citations. For instance, the forward model can be interpreted as a variant of the DRAW model by Gregor et al. (ICML 2014). Additionally, the work "Efficient Inference in Occlusion-Aware Generative Models of Images" by Huang & Murphy (ICLR) is highly relevant, as it employs a variational autoencoder with a spatial transformer and an RNN-like sequence model to capture the appearance of multiple sprites on a background.
Lastly, the paper lacks sufficient detail in its exposition, which raises concerns about reproducibility. For example, the description of the recognition model is unclear, and critical low-level details, such as the initialization strategy, are not provided. These omissions hinder the ability to fully understand and replicate the work.