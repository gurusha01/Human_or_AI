The authors have not addressed or resolved any of the pre-review comments. Therefore, I reiterate them here:
Please avoid making unscientific statements such as the following:  
"The working procedure of this model is just like how we human beings read a text and then answer a related question."  
Are you suggesting that "human beings" employ an LSTM-like model to process text? If so, can you provide a citation from a credible neuroscience study to support this claim? The answer is likely no, so such statements should be removed from future drafts.
In general, your experiments focus on simple classification tasks, and the baseline methods you compare against are straightforward models like NB-SVM. As such, I recommend revising the title, abstract, and introduction to reflect this scope more accurately and avoid exaggerated claims, such as using "Learning to Understand" in the title.
Additionally, your attention-based approach appears to share similarities with dynamic memory networks proposed by Kumar et al. Their work also includes experiments on sentiment analysis, and it would be valuable to clarify how your model differs from theirs and to provide a comparative analysis.
Other reviewers have also pointed out missing related work and the need to situate this paper more effectively within the context of current literature.  
Given that no effort has been made to address the pre-review questions and feedback, I am skeptical that this submission will be ready for publication in its current state.