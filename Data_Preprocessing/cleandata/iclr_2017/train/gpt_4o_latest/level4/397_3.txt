This paper explores the integration of autoregressive models with Variational Auto-Encoders (VAEs) and discusses methods to regulate the amount of information encoded in the latent representation. The authors demonstrate state-of-the-art performance on datasets such as MNIST, OMNIGLOT, and Caltech-101.
The insights presented in the paper, such as the impact of using a more expressive decoder on latent code learning, the bit-back coding mechanism, and lossy decoding, are clearly articulated but lack novelty. However, the distinction made between an autoregressive prior and the inverse autoregressive posterior is both novel and intriguing.
The proposed model leverages the recent advancements in PixelRNN/PixelCNN and VAEs, incorporating Inverse Auto-Regressive Flows. This combination allows the authors to achieve state-of-the-art results on MNIST, OMNIGLOT, and Caltech-101. Additionally, the insights provided enable some degree of control over the information encoded in the latent space.
This paper consolidates various insights on VAEs that are dispersed across multiple publications into a cohesive and well-written presentation. Using these insights, the authors successfully develop state-of-the-art models for datasets with relatively low complexity. However, further experimentation on larger-scale datasets will be necessary to validate the approach.