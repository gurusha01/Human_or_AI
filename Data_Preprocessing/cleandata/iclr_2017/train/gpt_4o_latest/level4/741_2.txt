Review - Summary  
This paper explores the use of tic-tac-toe as a toy problem to study CNNs. A dataset is generated consisting of tic-tac-toe boards where one player is a single move away from winning. A CNN is trained to classify these boards based on two tasks: (1) identifying the player who can win (2 classes) and (2) determining the position they should move to win (9 classes), resulting in a total of 18 labels. The CNN achieves perfect performance on the task, and the paper aims to analyze how the CNN arrives at its decisions.
The primary tool for this analysis is Class Activation Mapping (CAM) (Zhou et al., 2016), which highlights regions of implicit attention in the CNN. These attention maps (localization heatmaps) are then used to infer actions (i.e., the square where a player should move). The attention maps demonstrate that:  
1. They focus on specific squares in the tic-tac-toe board rather than arbitrary regions, even though each square has a uniform color.  
2. They can be used to select the correct (winning) moves.  
These findings are used to argue that the network understands:  
1. The structure of tic-tac-toe boards.  
2. The rules for winning tic-tac-toe.  
3. The concept of two players.  
Follow-up experiments show similar results under different visual representations of tic-tac-toe boards and with incomplete training data.
---
More Clarifying Questions  
- How exactly is CAM implemented in this work? In the original CAM method, a specific class of interest (e.g., cat or dog) must be selected for visualization. It is unclear how one of the 18 possible classes is chosen here for CAM visualization and how that visualization is used to determine an action.  
- How was the test set for the results in Table 1 constructed? Out of the 1,029 possible board states, how many were allocated to the test set, and was the label distribution consistent between the training and test sets?  
- How is RCO computed? Is rank correlation or Pearson correlation used? If Pearson correlation is used, it might be worth considering rank correlation, as suggested in "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?" by Das et al. (EMNLP 2016). Additionally, what does the notation \(10^3\) next to RCO in Table 1 signify?  
---
Pros  
- The proposed approach of deriving an action from a visualization technique is highly novel.  
- The paper provides a clear demonstration of a CNN leveraging context to make accurate predictions.  
- Using a toy domain like tic-tac-toe to study attention mechanisms in CNNs is a promising direction that could lead to deeper insights into implicit and explicit attention mechanisms.  
---
Cons  
1. Distinction Between "What Will Happen" and "What to Do":  
   - The paper differentiates between predicting "what will happen" (e.g., which player will win) and determining "what to do" (e.g., where a player should move to win). This distinction is central to the paper's argument that generalizing from "what will happen" to "what to do" demonstrates concept learning (Section 2.1). However, this distinction is unclear and potentially flawed:  
     - In the specific setup proposed, the "what will happen" labels already include both the winning player and the winning move (board position). How would "what to do" labels differ from these? Both tasks seem to involve the same 18-way classification, making the distinction between the two tasks redundant.  
   - Furthermore, this distinction is typically addressed within the Reinforcement Learning (RL) framework, but the paper does not present its method in relation to RL. In RL, "what will happen" corresponds to the reward for a given action, while "what to do" corresponds to the optimal action to maximize that reward. From this perspective, generalizing from "what will happen" to "what to do" is not a novel concept.  
   - Alternate models that could be considered include:  
     - A deep Q-network (Mnih et al., 2015) that predicts the value of each possible action (where an action is a combination of player and board position).  
     - Directly using the argmax of the current model's softmax output as the predicted action.  
   - While implementing these alternate models is not necessary, the paper should clarify how its approach differs from these existing methods.  
2. Lack of Comparison to Related Work:  
   - The paper does not compare its approach to prior work that uses visualization techniques to analyze deep RL networks. For example:  
     - "Dueling Network Architectures for Deep Reinforcement Learning" (Wang et al., ICML 2016) uses saliency maps to analyze differences between state-value and advantage networks.  
     - "Graying the Black Box: Understanding DQNs" (Zahavy et al., ICML 2016) also employs saliency maps to investigate network behavior.  
   - Such comparisons are important to contextualize the novelty and significance of the proposed method.  
3. Claims About Saliency Maps in Section 2.3:  
   - The paper claims that saliency maps (Simonyan et al., 2013) cannot activate on grid squares because the squares have constant intensity. However, no theoretical or empirical evidence is provided to support this assertion.  
   - Additionally, the notion of "information" referenced in Section 2.3 is unclear. Is it referring to the entropy of pixel intensity distributions? If so, why is this relevant? Since methods like saliency maps depend on context as well as local intensities, any measure based solely on a single patch seems insufficient to justify the claim.  
4. Presentation of Results:  
   - The results in Section 7 would be better integrated into the preceding sections, alongside relevant discussions. This would improve the flow and clarity of the paper.  
---
Overall Evaluation  
The experiments presented in this paper are novel, but their significance and conclusions are not entirely clear. The methods and objectives are not well-articulated and lack connections to broader, relevant contexts, as highlighted in the Cons section. Additionally, some aspects of the proposed approach are unclear or potentially flawed, as detailed in the clarifying questions. My assessment may change if these issues are addressed with further details.