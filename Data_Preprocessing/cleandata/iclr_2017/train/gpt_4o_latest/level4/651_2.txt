The proposed method effectively trains neural networks without employing traditional nonlinearities, instead utilizing multiplicative gating through the CDF of a Gaussian evaluated at the preactivation. This approach is framed as a relaxation of a probit-Bernoulli stochastic gate. Experiments are conducted using both this method and relevant baselines.
The work demonstrates some degree of novelty and interest. However, the paper provides limited discussion on why this parameterization is preferable to other similar approaches (e.g., sigmoidal, softsign, etc.). The contribution would be more compelling with deeper empirical analysis of why the method is effective and broader exploration of the surrounding conceptual space. While the CIFAR results are reasonable by current standards, the MNIST results are notably poorâ€”neural networks achieved better than 1.5% error rates over a decade ago, and the SOI map results (as well as the ReLU baseline) exceed 2%. Similarly, the TIMIT frame classification results are not particularly impactful without additional evaluation of word error rates within a full speech recognition pipeline, though this is a relatively minor issue.
The claim that SOI map networks without additional nonlinearities are comparable to linear functions is somewhat misleading, as these networks are nonlinear in expectation. For instance, scaling or shifting an input example will not produce a linearly scaled or shifted expected output. In this respect, these networks are more nonlinear than ReLU networks, which are at least locally linear.
Finally, the plots are challenging to interpret when viewed in grayscale.