This paper extends the Spin Glass analysis introduced by Choromanska et al. (2015a) to ResNets, yielding novel dynamic ensemble results for ResNets, as well as exploring the connection to Batch Normalization and analyzing the loss surface of ResNets.
The manuscript is well-written and provides numerous insightful explanations of the results. While the technical contributions build upon the Spin Glass model analysis of Choromanska et al. (2015a), the updated version successfully addresses one of the unrealistic assumptions in the original model. Additionally, the analysis introduces novel dynamic ensemble results and examines the connection to Batch Normalization, offering deeper insights into the structural properties of ResNets.
It is crucial, however, to demonstrate this dynamic behavior in a regime without Batch Normalization to disentangle its effects on the ensemble feature. While the authors claim that a steady increase in the L2 norm of the weights preserves this feature, the experimental setup for Figure 1 appears too restrictive to empirically validate the claim. Providing results on CIFAR-10 without Batch Normalization to illustrate the effect of L2 norm increases, along with empirical evidence supporting the claims in Theorem 4, would significantly strengthen the paper.
This work establishes an initial rigorous framework for better analyzing the inherent structure of state-of-the-art ResNet architectures and their variants. Such a framework has the potential to inspire more impactful results toward a deeper understanding of current state-of-the-art models. Rather than solely focusing on improving ResNet performance through incremental heuristics, it is equally important to advance a solid theoretical understanding.
- The issue of unrealistic assumptions in Spin Glass analysis for neural network loss landscapes was highlighted as an open problem in COLT 2015 [1]. Could you discuss how this issue impacts your analysis?
- Regarding the assumption that the minimum of (12) should hold, do you consider this assumption realistic?
- While Choromanska et al. (AISTATS 2015) supported their theoretical claims with extensive empirical results, your work does not provide similar empirical validation. Could you include reasonable empirical results to support your claims?
- What is the specific empirical setup for Figure 1?
- The FractalNet paper on arXiv claims that residuals are incidental. Could you elaborate on this claim within the context of your framework? Additionally, how does your framework relate to densely connected convolutional networks (Huang et al., 2016)?
[1] A. Choromanska, Y. LeCun, G. Ben Arous, Open Problem: The landscape of the loss surfaces of multilayer networks, in the Conference on Learning Theory (COLT), Open Problems, 2015