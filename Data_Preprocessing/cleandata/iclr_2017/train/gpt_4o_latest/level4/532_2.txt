Final Review: The authors were very responsive, and I concur with Reviewer 2 that their experimental setup is not flawed after all, which led me to increase the score by one. However, I still believe the paper suffers from a lack of experiments, and the results remain inconclusive. As a reader, I am typically looking for one of two things: either gaining new insights and a deeper understanding of a concept or learning about a method that achieves better performance. This paper falls into the second category but fails to substantiate its claims with more thorough and rigorous experimentation. In summary, the paper lacks sufficient experimental validation, the results are inconclusive, and I am not convinced that the proposed method would be particularly useful. Therefore, I do not consider it suitable for publication at a conference of this level.
---
The paper introduces a method that trains a policy network alongside the main network to select a subset of data during training, aiming to achieve faster convergence with less data.
Pros:
- The paper is well-written and easy to follow.
- The algorithm is clearly explained.
Cons:
- Section 2 states that validation accuracy is used as one of the feature vectors for training the NDF. This compromises the validity of the experiments, as the training process utilizes data from the validation set.
- The method has been tested on only one dataset. Papers claiming faster convergence rates should demonstrate results on multiple datasets and network architectures to establish consistency. This is especially important for larger datasets, as the proposed method relies on using less training data per iteration. For instance, experiments on large-scale datasets like ImageNet are necessary to validate the approach.
- As discussed in detail in the pre-review questions, if the paper claims faster convergence, it must compare learning curves against other baselines, such as Adam. Using plain SGD as a baseline is an unfair comparison, as it is rarely used in practice. This holds true regardless of the black-box optimizer employed. It is possible that Adam alone, as the black-box optimizer, performs as well as or better than Adam combined with NDF.