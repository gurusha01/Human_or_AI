This paper presents an experimental investigation into the robustness of state-of-the-art convolutional neural networks (CNNs) against various types of "attacks" in the context of image classification. Specifically, the study focuses on attacks designed to deceive the classification system by introducing carefully crafted perturbations to images. These attacks aim to either (1) cause the system to misclassify the image as any incorrect class (non-targeted attack) or (2) force the system to classify the image as a specific, pre-determined class chosen by the attacker (targeted attack). For example, an attacker might manipulate an image of an ostrich to make it classified as a megalith. While the motivation for such an attack may not be immediately clear in this example, studying these vulnerabilities is important for (1) improving the robustness of current systems and (2) addressing potential risks, such as those associated with autonomous vehicles.
The paper is primarily experimental in nature. It systematically compares various attack strategies (previously introduced in the literature) across popular CNN architectures, including VGG, GoogLeNet, and ResNet-50/101/152, for both types of attacks mentioned above. The experiments are well-executed and clearly presented. A particularly compelling aspect of the study is the inclusion of attacks on "clarifai.com," a black-box classification system. Additionally, the paper provides some analysis and insightful explanations in Section 6 to elucidate why CNNs are vulnerable to such attacks.
In summary, the key findings of the paper are as follows: non-targeted attacks are relatively easy to execute, even on black-box systems. Targeted attacks, on the other hand, are more challenging with existing methods. However, the authors propose a novel approach that significantly improves the success rate of targeted attacks (e.g., achieving ~20% success on clarifai.com compared to 2% with prior methods), though it remains far from perfect.
That said, the paper has a few notable weaknesses:
- The authors treat the three ResNet-based networks as distinct models, despite their clear architectural similarities. This correlation is evident in Table 7, as their differences primarily stem from variations in depth. Consequently, the statement "One interesting finding is that [...] the first misclassified label (non-targeted) is the same for all models except VGG-16 and GoogLeNet" may be somewhat misleading, as it applies to the three ResNet-based networks due to their shared architecture.
- The evaluation of the attacks on the black-box system relies on a subjective measure. While this is understandable given that clarifai.com provides labels that differ from ImageNet, it raises concerns about the fairness of the reported numbers, even though the qualitative results appear convincing.
- The novelty of the proposed approach—optimizing an ensemble of networks rather than a single network—is limited. However, since this is not the primary focus of the paper and the approach is effective, this limitation is acceptable overall.
- The paper is somewhat lengthy, which is expected given its extensive evaluation. Nevertheless, some sections could be streamlined to reduce redundancy (e.g., Section 2.3 overlaps significantly with Section 1).
- The paper would benefit from a more thorough discussion of recent related work, particularly the theoretical findings of Fawzi et al. (NIPS'16), in Section 6. The alignment between Fawzi et al.'s theoretical insights and the experimental results presented in this paper, especially in Section 6, warrants further exploration.
In conclusion, this paper provides a valuable contribution to the community by offering insights that could help improve existing architectures and better understand their vulnerabilities. 
One question regarding Equation (7): Why did you choose to use \( f(x) \) instead of \( J(x) \)? Since \( f(x) = \text{argmax } J(x) \), \( f(x) \) is highly non-linear, which could make gradient descent less efficient. While optimizing with \( J(x) \) instead of \( f(x) \) does not exactly align with the same objective, the outcomes might be quite similar. Did you experiment with this alternative, or have I misunderstood something?