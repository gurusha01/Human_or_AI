This study advances our understanding of the landscape of deep networks by examining their topology and geometry. The paper provides a theoretical analysis of the former and an empirical investigation of the latter. While the contributions are narrowly focused (ReLU networks with a single hidden layer and a heuristic for computing the normalized geodesic), the findings are original and hold potential significance. As such, they could serve as foundational steps for further progress in this domain.
Pros:
1. The paper introduces new theoretical insights regarding the existence of "poor" local minima in ReLU networks with a single hidden layer, which depend on both the input distribution and the size of the hidden layer.
2. It proposes a heuristic algorithm to compute the normalized geodesic between two solution points, offering a measure of the curvature of the path connecting them.
Cons:
The scope of the results is quite limited in both the topology and geometry analyses.
1. The theoretical analysis is restricted to ReLU networks with a single hidden layer. Given the critical role of depth in modern deep learning architectures, this result does not fully address the practical architectures of interest.
2. The normalized geodesic metric has limitations in assessing the ease of connecting two equally good points. For instance, while a straight line between two points may be considered "easy" by this metric, it might traverse a narrow valley, posing significant challenges for gradient-based optimization methods in practice. Moreover, the proposed algorithm for computing the normalized geodesic is a greedy heuristic, which raises concerns about the reliability of the estimated geodesics it produces.
Despite these limitations, I acknowledge the inherent difficulty of the problems tackled in this paper and find the contributions both valuable and intriguing.