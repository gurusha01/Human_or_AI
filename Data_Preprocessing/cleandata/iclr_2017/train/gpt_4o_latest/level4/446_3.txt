This paper enhances the GAN framework by incorporating latent variables. The observed dataset is augmented by sampling latent variables \( z \) from a conditional distribution \( q(z|x) \). A joint distribution over \( x \) and \( z \) is then modeled using a joint generator \( p(x,z) = p(z)p(x|z) \). Both \( q \) and \( p \) are optimized by training them to deceive a discriminator. This represents a valuable extension to GANs, as enabling inference within GANs unlocks a range of applications that were previously only feasible with approaches like VAEs.
The results presented are highly encouraging. The CIFAR-10 samples are among the best I have encountered to date (excluding methods that leverage class labels). Furthermore, achieving semi-supervised performance comparable to Salimans et al. without relying on feature matching suggests that the proposed method may contribute to improving the stability of GAN training.