The authors present a neural pruning method applied to pre-trained models, leveraging an approximation of the cost function's change, and demonstrate its superiority over alternative criteria. The approach achieves significant speedups while preserving acceptable accuracy levels, aided by finetuning post-pruning. However, the comparisons to existing methods are insufficient, as the GFLOPS graphs include only a few basic baselines and omit comparisons to prior work. Including such comparisons would strengthen the case for the proposed method's effectiveness.