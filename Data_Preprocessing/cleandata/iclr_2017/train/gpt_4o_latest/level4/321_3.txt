I appreciate the setting introduced in this paper, but I have several critiques and questions:
(1) What is the failure model for this work? As the richness of behaviors increases in complexity, I anticipated that this approach might encounter challenges with the diversity of skills it can uncover.
(2) Referring to Sec 5.3 -- "let X be a random variable denoting the grid in which the agent is currently situated" -- is the space discretized? If so, why was this choice made, and what would happen if it were not discretized?
(3) Building on the first point, does the proposed approach extend to more complex embodiments? For instance, could it handle a 5-link swimmer instead of a 2-link swimmer? I believe this is a critical aspect to evaluate the generality of the method.
(4) The authors state that "Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge."
I do not fully agree with this claim. The rewards proposed in this paper also appear to be quite hand-crafted and tailored to a seemingly narrow set of control tasks.