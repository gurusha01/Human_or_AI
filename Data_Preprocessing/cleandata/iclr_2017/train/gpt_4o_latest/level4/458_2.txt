The paper presents a novel regularization term designed to guide the optimizer toward identifying a flat local minimum with reasonably low loss, rather than converging to a sharp region of low loss. This approach is inspired by empirical findings suggesting that local minima associated with good generalization performance typically exhibit flatness. To implement this, the authors propose a regularization term based on free local energy, and since the gradient of this term lacks a tractable closed-form solution, it is estimated using a Monte Carlo approach with an SGLD sampler. The experimental results provide evidence supporting the flatness of favorable local minima and demonstrate the performance of the proposed method in comparison to the Adam optimizer.
The paper is well-written and easy to follow, and I found it enjoyable to read. The integration of the free energy concept into the optimization framework is intriguing, and the motivation for pursuing flatness is well-supported by a set of experiments. However, I have a few concerns. First, I am unsure if the first term in Eq. (8) is correctâ€”it seems it should be f(x') rather than f(x). Additionally, I am curious why the authors did not include experimental results on RNNs when evaluating performance, especially since char-LSTM was already used in the flatness experiments. Including experiments on a broader range of models and applications in deep learning (e.g., RNNs, seq2seq architectures, etc.) would strengthen the authors' claims. Lastly, I found the inconsistent use of terminology, such as "free energy" and "free entropy," somewhat confusing.