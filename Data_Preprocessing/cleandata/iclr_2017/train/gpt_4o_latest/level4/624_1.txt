The primary strength of this paper lies in revisiting the critical role of initialization in deep networks and challenging the widespread belief that advancements in modern architectures and gradient descent techniques have rendered optimization issues, such as local minima and saddle points, largely irrelevant.
The paper presents compelling counterexamples that demonstrate how poor initialization, combined with specific data configurations, can lead optimization to converge to suboptimal solutions. However, these examples appear to be artificially constructed. More critically, the paper does not account for widely adopted heuristics that likely mitigate such issues, including non-saturating activation functions (e.g., leaky ReLU), batch normalization, and skip connections (e.g., ResNet), all of which are designed to facilitate gradient flow.
While the paper effectively highlights potential pitfalls of initialization (specifically in standard ReLU networks), it neither proposes novel solutions or workarounds nor conducts a systematic investigation into how commonly used heuristic techniques in architecture, initialization, and training might alter this landscape. A broader and more comprehensive analysis, particularly one yielding insights of practical significance, would significantly enhance the paper's value for its audience.