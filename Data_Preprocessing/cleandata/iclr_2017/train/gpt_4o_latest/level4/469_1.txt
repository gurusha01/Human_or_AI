The paper presents an implementation of sparse-full convolutions and a model designed to estimate the potential speed-up achievable at various sparsity levels for CNNs.
The first contribution leans more towards engineering, but the authors' decision to make the source code publicly available is highly commendable.
The second contribution is particularly compelling, as prior pruning methods have primarily focused on reducing memory usage, often yielding only minimal speed improvements. Incorporating knowledge of execution speed into a pruning algorithm appears to be a more effective approach to addressing this issue. The authors adopt a systematic methodology to construct the model and conduct a thorough evaluation of its performance.
The same concept could potentially be extended beyond pruning existing models to designing new architectures. Specifically, it could guide the selection of layers and their parameters to optimize throughput rates, which presents an intriguing avenue for future research.
One aspect that is lacking is a discussion on the transferability of the performance model to GPUs. Addressing this would enhance the technique's applicability and facilitate broader adoption.
Additional areas for improvement include: improving the clarity of Figure 4, as the points (e.g., small red circle vs. small red square) are difficult to distinguish, and enlarging the figure overall; clarifying whether the "base learning rate" in Section 3 refers to the initial or final rate in the annealing schedule; and correcting typographical errors such as "punning" (p.4) and "spares" (p.5).