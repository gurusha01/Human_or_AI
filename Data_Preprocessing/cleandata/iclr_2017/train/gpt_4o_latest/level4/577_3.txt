The authors present a method to examine the predictive capabilities of intermediate layer activations by training linear classifiers and assessing their error on a test set.
The paper is well-motivated, aiming to illuminate the progression of model training and provide insights that could inform the design of deep learning architectures.
The authors appear to justify their use of linear probes based on two main factors:
- Convexity
- The typical linear nature of the final layer in neural networks
In the second-to-last paragraph of page 4, the authors acknowledge the possibility that intermediate features may be ineffective for a linear classifier. While this observation is valid, I consider it the primary limitation of the paper. The authors do not provide sufficient justification for how their proposed analysis could be practically useful for architecture design. For instance, the example involving skip connections (Figure 8) seems to imply that skip connections might be detrimental. However, this appears to contradict the well-documented success of ResNet architectures.
Although the results are intriguing, they are not particularly unexpected, and I struggle to see how they offer actionable insights into understanding deep models, as the authors claim.