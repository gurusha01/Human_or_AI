This paper employs an LSTM model to predict what it terms "open bigrams" (character bigrams that may or may not have intervening letters) from handwriting data. These open bigrams are subsequently utilized in a decoding step to predict the written word. The experiments suggest that the system performs marginally better than a baseline model employing Viterbi decoding. However, I have significant concerns about this paper:
- The claim of being "cortically inspired" is problematic. At best, the approach is inspired by psychology or cognitive science, as open bigrams have been shown to aid word recognition (Touzet et al., 2014). However, the implied cortical basis, which is hinted at through analogies to deep neural networks for object recognition and their connection to the visual cortex, lacks grounding. Is there any direct neuroscientific evidence supporting the existence of a distinct cortical layer for open bigrams in handwriting recognition? Dehaene's work is a theoretical proposal, so the authors need to provide more concrete evidence from "findings in cognitive neurosciences [sic] research on reading" (p. 8) to substantiate these claims. Additionally, the authors seem to misunderstand the structure of deep neural networks, claiming that "deep neural networks are based on a series of about five pairs of neurons [sic] layers." If this refers specifically to Krizhevsky's AlexNet, it should be cited explicitly. However, the implication that all deep neural networks require five layers is incorrect. Similarly, the assertion that ten layers are "quite close to the number of layers of an efficient deep NN" is vagueâ€”what network, what task, and under what conditions?
- The model is insufficiently described. Appendix A.3. provides a brief description of the setup, but it omits critical details such as the objective function and the reasoning behind considering the network output only at every two consecutive time steps rather than at each time step (if that is indeed the case). The paper emphasizes that it "is focused on the decoder" (p. 6) rather than the entire problem, but this is problematic. This focus effectively reduces the task to reconstructing a word from its open bigrams, which is only tangentially related to handwriting recognition and could have been evaluated on any text corpus. For instance, as illustrated by the example on page 4, handwriting is not essential to demonstrate the open bigram hypothesis. This raises the question of why these specific tasks were chosen if the primary interest lies in the decoding mechanism.
- The comparison is not entirely fair. The Viterbi decoder appears to have access only to unigrams, whereas the proposed model has access to significantly more information but shows only marginally better performance. Did the Viterbi model have access to word boundary information (referred to as "extremities" at one point, albeit somewhat confusingly), which seems to have given the open bigram model a performance edge? Additionally, why is there no comparison to a model such as rnn_0,1' (unigram+bigram+boundary markers)? The dataset also seems biased in favor of the proposed approach, with longer words being included. Overall, I am not convinced that the paper demonstrates the utility of open bigrams effectively.
While I appreciate the core idea of the paper, I remain unconvinced by its claims.
Minor points:
- There are several typos throughout the paper. Examples include: "independant" (Fig.1), "we evaluate an handwritten", ", hand written words [..], an the results", "their approach include", "the letter bigrams of a word w is", and "for the two considered database."
- Would it not be straightforward to incorporate the frequency of bigram occurrences to improve the decoding process? Normalizing over full counts instead of binary occurrence counts could be beneficial.
- The results in Table 5 are identical to those in Table 2 (albeit with different precision), except for the addition of edit distance and SER metrics. This is confusing and should be clarified.