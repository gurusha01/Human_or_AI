This paper utilizes Taylor approximations of neural networks to distinguish between the convex and non-convex components of the optimization process. By doing so, the authors are able to bound the training error using the Taylor optimum and regret (Theorem 2). This represents a solid theoretical contribution with relevance to widely-used deep neural networks. Furthermore, the empirical evaluations support the theoretical findings.