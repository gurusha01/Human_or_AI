The authors introduced a method to evaluate the generation of out-of-distribution novelty. Their approach suggests that if a model trained on MNIST digits produces samples that are judged by another model, trained on both MNIST and letters, to resemble letters, then the MNIST-trained model can be considered capable of generating novel samples. Some empirical results were presented to support this claim.
However, the concept of novelty is inherently difficult to define, and the proposed metric raises concerns. A simple combination of the MNIST and letters datasets does not accurately reflect the natural distribution of handwritten digits and letters. This implies that a model trained on such a combination may not reliably distinguish between digits and letters. Consequently, the proposed metrics, out-of-class count and out-of-class max, lack meaningful interpretability. Regarding the "novel" samples shown in Fig. 3, they are evidently still digits. It appears that the authors may have binarized the samples. If the samples were quantized to 8-bit instead, the resulting images would likely resemble digits even more closely.