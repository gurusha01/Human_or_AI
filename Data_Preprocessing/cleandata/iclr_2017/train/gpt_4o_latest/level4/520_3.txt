Certainly! Here's the paraphrased review:
---
First, the paper aims to evaluate whether auto-regressive models can achieve results comparable to the GANs of Reed et al. (2016a). But does it truly succeed in doing so? The resolution of the outputs is extremely poor, and this limitation is only discussed in the second-to-last paragraph of the paper. Furthermore, Figure 9 presents just three results—were these chosen randomly or selected to favor PixelCNN? This limited evidence hardly seems sufficient to draw definitive conclusions.
The segmentation masks and keypoints provide very strong input constraints. Given the low resolution, it's difficult to discern how much coherent object and scene detail the model is actually generating. For instance, the cows in Figure 5 appear as little more than color blobs. Any color blob that adheres to a precise cow segmentation mask will naturally resemble a cow.
That said, the diversity of the generated outputs is impressive.
However, how can we evaluate the extent to which the model is "replaying" training data? Figure 8 attempts to address this, but I'm left wondering whether some of the "red birds," for example, are largely copied from specific training examples.
I'm also dissatisfied with the responses to the pre-review questions. My questions were not adequately addressed. The paper would benefit from providing concrete metrics on training time, number of epochs, and testing time. Additionally, the authors fail to explain why high-resolution comparisons are not feasible. Yes, the model is slower at test time, but is it prohibitively slow or merely inconvenient? Given the limited number of comparisons in the paper, even if generating a result takes an hour, that doesn't seem like an insurmountable issue.
To clarify my perspective: I don't believe PixelCNN is the optimal approach for deep image generation. While causal neighborhoods have been used successfully in texture synthesis methods, this was largely due to the lack of better global optimization techniques at the time (e.g., Kwatra et al., "Texture Optimization for Example-based Synthesis," which introduced one of the first alternatives). Making hard, sequential decisions about pixel values in one part of the image before synthesizing the rest seems fundamentally flawed. Coarse-to-fine synthesis was one strategy used in texture synthesis to mitigate this strict causality, and some deep image synthesis methods are exploring similar ideas. A more robust approach would involve a deeper network where all output pixels are conditioned on all others, with this conditioning emerging implicitly in the network's intermediate layers. That said, I could be mistaken, and the advantages highlighted in the paper may outweigh these drawbacks. However, the paper does not present a balanced or transparent discussion of these disadvantages.
Overall, the results are intriguing, particularly the ability to generate diverse outputs. However, the resolution is extremely low, especially given the richness of the inputs. The network benefits significantly from the strong guidance provided by the inputs (and at least it learns to respect them).
The field of deep image synthesis is advancing rapidly. It's time to move beyond "proof of concept" papers that merely demonstrate feasibility and toward more comprehensive comparisons. This paper has the potential to provide such a comparison but falls short in this regard. There is no clear apples-to-apples comparison between PixelCNN and GANs, nor an explanation of why such a comparison is infeasible. Additionally, there is no large-scale evaluation—whether qualitative or through user studies—to assess the quality of the results.
--- 
This version preserves the original structure, meaning, and technical content while rephrasing for clarity and flow.