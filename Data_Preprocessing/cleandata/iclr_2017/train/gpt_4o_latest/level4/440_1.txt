This paper presents a novel approach for model-based control of stochastic dynamical systems using policy search, which involves: (1) learning the stochastic dynamics of the system via a Bayesian deep neural network (BNN) that accommodates stochastic inputs, and (2) optimizing policies through simulated rollouts generated from the learned dynamics. The BNN is trained using \(\alpha\)-divergence minimization, a methodology previously introduced by the authors. The proposed approach is validated and compared across both simulated and real-world domains.
The manuscript is well-structured and clearly written, making it accessible to readers. The use of \(\alpha\)-divergence for fitting Bayesian neural networks is intriguing and appears to be a novel contribution in this context. The application to model-based control demonstrates significant practical potential, particularly given the explainability that a system model can provide for policy decisions. Overall, the paper makes a meaningful contribution to the field.
However, I have several questions and suggestions for improvement:
1) In Section 2.2, additional clarification is needed regarding the use of the random \(z_n\) input in the neural network. Is it simply concatenated with other inputs, or is there a specific mechanism for handling it?
2) While the paper emphasizes the importance of stochastic inputs, only a scalar stochastic input is used throughout the experiments. Is this sufficient to demonstrate the method's capabilities? Additionally, how computationally challenging would it be to extend the approach to higher-dimensional stochastic inputs?
3) How critical is the assumption of normality for \(z_n\)? Furthermore, how is the variance \(\gamma\) determined in practice?
4) The paper mentions that the hidden layers of the neural network use rectifiers, but this aspect is not further explored. Does this choice play a specific role in optimizing the \(\alpha\)-divergence, beyond the general benefits of rectifiers in mitigating the vanishing gradient problem?
5) In Equation (3), the denominator \(\mathbf{y}\) appears to be a typographical error and should likely be \(\mathbf{Y}\).
6) Section 2.3 would benefit from a discussion of the computational complexity involved in training BNNs. This would help readers assess the practical feasibility of the proposed approach.
7) Between Equations (12) and (13), it would be helpful to include a citation for the time embedding theorem and provide guidance on how to select the embedding dimension.
8) In Figure 1, the subplots should be labeled with the letters referenced in the text on page 7 for easier cross-referencing.
9) In Section 4.2.1, it is unclear whether the gas turbine dataset is publicly available. If it is, the source should be specified. Additionally, more details about the dataset, such as the dimensionality of the variables \(Et\), \(Nt\), and \(A_t\), should be provided.
10) The comparisons with Gaussian processes could be expanded to include variants that support stochastic inputs, such as the work by Girard et al. (2003). At the very least, this line of research should be acknowledged in Section 5 to provide a more comprehensive discussion of related work.
References:
Girard, A., Rasmussen, C. E., Quiñonero Candela, J., & Murray Smith, R. (2003). Gaussian process priors with uncertain inputs—application to multiple-step ahead time series forecasting. Advances in Neural Information Processing Systems, 545–552.