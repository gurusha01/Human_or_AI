The objective of this paper is to develop "a collection of experts that are individually meaningful and that have disjoint responsibilities." In contrast to a standard mixture model, the authors "employ a different mixture for each dimension d." While the results appear encouraging, the paper's presentation requires substantial improvement.
Comments:
The paper begins abruptly without providing any motivation. What is the intended application, or even the algorithm or architecture, that this work is designed for? This should be clearly outlined at the start.
The subsequent explanation lacks clarity. Several claims are made without adequate justification, such as: "the experts only have a small variance for some subset of the variables while the variance of the other variables is large."
Given that both the experts and the weights are being learned, could this framework be reinterpreted in the context of dictionary learning? Please include a discussion of the relevant related work.
The horse dataset is relatively small compared to the feature dimensionality, which raises concerns about whether the conclusions can be generalized.