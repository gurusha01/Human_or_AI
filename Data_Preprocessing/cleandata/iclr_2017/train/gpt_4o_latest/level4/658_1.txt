This paper addresses the challenge of interpretable representations, specifically focusing on Sum Product Networks (SPNs). The authors propose that SPNs, as powerful linear models capable of learning parts and their combinations, have not yet been fully leveraged for generating embeddings to exploit their representations.
Pros:
- The concept is intriguing, and the topic of interpretable models/representations is highly relevant.
- Using embeddings to interpret SPNs is a novel and promising idea.
- The experiments are engaging, though they could benefit from further extension.
Cons:
- The authors' contributions are not entirely clear, and several claims require additional support. For instance, SPNs are already considered interpretable due to the ability to visualize bottom-up propagation of information from visible inputs at each stage, as well as top-down parsing, as demonstrated in prior work (e.g., Amer & Todorovic, 2015). Another example is Proposition 1, which asserts that SPNs are perfect encoder-decoders because max nodes always have one maximum value. However, what happens in cases of uniformly distributed nodes or two equal maximum values? Did the authors encounter such scenarios, and if so, how were these edge cases addressed?
- A more comprehensive comparison could have been made against state-of-the-art generative models such as Generative Adversarial Networks (GANs), Generative Stochastic Networks (GSNs), and Variational Autoencoders (VAEs), rather than focusing on comparisons with RBMs and NADE.
I recommend that the authors dedicate additional effort to evaluating their approach against the suggested methods, clarifying their contributions, and avoiding overstatements. I also concur with the points raised by Anon-Reviewer1.