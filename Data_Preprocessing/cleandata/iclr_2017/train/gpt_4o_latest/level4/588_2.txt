[EDIT: The author responses thoughtfully addressed my primary concerns. The provided GitHub links for data and code will be highly beneficial for reproducing the results (though I haven't examined them in detail, this is a great addition). The revision resolves many issues, including the addition of new results. Consequently, I am raising my rating from a 5 to a 6 and recommend the paper for acceptance.]
This paper introduces the application of deep neural networks for detecting and localizing extreme weather events in simulated weather data. The problem is analogous to object detection in computer vision, where the input is either a 2D "image" (multichannel spatial weather data) or a 3D "video" (temporal weather data), and the output consists of a bounding box (spatial-temporal localization of the weather event) and a class label (weather event type). However, it diverges from standard object detection tasks due to the presence of multiple heterogeneous input channels and the scarcity of labeled data.
The authors propose a straightforward yet reasonable deep network for this task, inspired by approaches in computer vision. While proposal-based systems like Faster-RCNN are currently dominant in vision, the proposed method is simpler and serves as a solid starting point. Although there is limited innovation in the detection system itself, the paper represents a valid adaptation of computer vision techniques to a new domain.
The paper explores both a supervised approach (using only ground truth bounding box locations and labels) and a semi-supervised approach that incorporates a reconstruction loss as regularization. The losses employed are fairly standard and appropriate for the task. However, the terminology around the "semi-supervised" loss is somewhat confusing, as it uses the same labels as the supervised loss while adding the reconstruction loss. This makes the "semi-supervised" loss effectively stronger, which could benefit from clearer explanation.
The paper is generally easy to follow, but the notation is imprecise in places. For instance, above equation 5, the loss is described as a weighted combination of reconstruction error and bounding box regression loss, but it is actually a combination of supervised and unsupervised losses (Lsup and Lunsup). Additionally, Lrec is not explicitly defined (though it seems to correspond to Lunsup). Minor issues like this, along with references such as "figure 4 and 4," should be addressed for clarity.
The most significant concern with the paper lies in its experimental results. Only a single figure and table (figure 4 and table 4) are presented, and the metrics used (e.g., mean average recall) are not clearly defined. The experiments compare 2D and 3D versions of the model, as well as supervised and semi-supervised approaches, but the results are inconsistent (e.g., why does 2D supervised outperform the seemingly stronger 3D semi-supervised model?). It is unclear how many events are included in the training/testing data, and there is no discussion of the absolute quality of the results. The experiments are sparse, with no ablation studies or detailed analysis. Additionally, it is uncertain whether future researchers will be able to reproduce the experimental setup, making open-sourcing the data or providing a clear path to reproducibility essential.
A minor point: the authors employ both a classification loss and an "objectness" loss. This is unusual, as objectness is typically used in two-stage object detection systems where class-agnostic proposals are generated in the first stage and classified in the second. It seems likely that removing the objectness loss would not affect the results, as the classification loss should provide a stronger supervisory signal. This non-standard choice should be justified, ideally through experimental evidence.
Overall, this is a borderline paper. Applying computer vision techniques to a domain with limited prior work in the community is valuable. However, I lack expertise in this specific type of data, and it is possible that deep learning methods are already widely used in the climate science literature (though I suspect this is not the case). Algorithmically, the paper offers little novelty, as the equations in section 3 are standard in the computer vision literature. The use of reconstruction loss in a data-sparse setting is interesting, but the experimental results are inconclusive, and the validation is insufficient. Furthermore, reproducibility is a concern unless the data is made publicly available. Overall, this paper is a promising starting point, and with stronger experiments and more polished writing, it could evolve into a solid contribution.