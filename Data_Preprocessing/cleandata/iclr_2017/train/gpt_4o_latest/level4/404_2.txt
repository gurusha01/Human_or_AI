This paper presents a novel recurrent neural network (RNN) architecture called QRNN.
QRNNs share similarities with gated RNNs; however, their gate and state update functions rely solely on recent input values and are independent of the previous hidden state. These functions are computed via a temporal convolution applied to the input. As a result, QRNNs enable greater parallelism in computation, as their hidden-to-hidden transitions involve fewer operations dependent on the previous hidden state compared to GRUs or LSTMs. Nevertheless, this design may come at the cost of reduced expressiveness relative to those models. For example, it remains unclear how QRNNs handle long-term dependencies without requiring multiple stacked layers.
The paper also explores several QRNN extensions, incorporating techniques such as Zoneout, densely connected layers, and sequence-to-sequence models with attention mechanisms.
The authors evaluate their approach across a range of tasks and datasets, including sentiment classification, word-level language modeling, and character-level machine translation.
Overall, the paper is well-written and introduces an interesting approach.  
Pros:  
- Tackles an important problem.  
- Provides a solid empirical evaluation that highlights the advantages of the proposed method.  
- Demonstrates up to a 16x speed-up compared to LSTMs.  
Cons:  
- The novelty is somewhat incremental relative to (Balduzzi et al., 2016).  
Specific questions:  
- Is the densely connected layer essential for achieving good results on the IMDB task? How does a simple 2-layer QRNN compare to a 2-layer LSTM?  
- How does the i-fo-ifo pooling mechanism perform in comparison?  
- How does QRNN handle long-term dependencies? Was it tested on simple toy tasks such as the copy task or the adding task?