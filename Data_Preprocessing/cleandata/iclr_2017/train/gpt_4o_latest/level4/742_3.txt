Summary of the paper:
In this paper, the authors investigate quantities related to the expressivity of neural networks, focusing specifically on random networks. They introduce the concept of the "trajectory length" of a one-dimensional trajectory, which is defined as the length of the trajectory when points (in an m-dimensional space) are embedded by the network's layers. The authors derive growth factors for this trajectory length as functions of the number of hidden units \( k \) and the number of layers \( d \), showing that the growth factor increases exponentially with the number of layers. They also connect this trajectory length to other quantities, namely "transitions," "activation patterns," and "dichotomies." Based on their findings, the authors propose that training the earlier layers of a network results in higher accuracy compared to training only the later layers. Experimental results are provided on the MNIST and CIFAR-10 datasets.
Clarity:
The paper is somewhat difficult to follow because the motivations presented in the introduction are not clearly articulated, and the definitions used throughout the paper lack clarity.
Novelty:
The idea of studying trajectory length as a function of data transformations through a multilayer network is novel and intriguing. However, the relationship to transition numbers is expressed in terms of growth factors rather than a direct quantity-to-quantity relationship, making it challenging to interpret the implications of the findings.
Significance:
The role of the input set's geometry (of dimension \( m \)) is only weakly reflected in the analysis of activation patterns. The trajectory length analysis could provide insights into how the network organizes the input set. The experiments suggest that as the network is trained, it becomes more contractive and selective. It would be valuable to explore these phenomena further using trajectory length as a metric to disentangle nuisance factors (e.g., invariances). In supervised learning, the network does not need to be contractive everywhere but must selectively focus on class labels. A theoretical investigation into selectivity and contraction using trajectory length would enhance the paper's appeal.
Detailed comments:
Theorem 1:
- As noted by Reviewer 1, the definition of a one-dimensional input trajectory is missing.  
- It is unclear how Theorem 1 informs the design and architecture of neural networks, as suggested in the introduction. The connection to transitions in Theorem 2 appears weak.
Theorem 2:
- In the proof of Theorem 2, the meaning of \( T \) and \( t \) is unclear. The notations are confusingâ€”what is the expectation taken with respect to? Is it \( W{d+1} \), or both \( W{d+1} \) and \( Wd \)? While avoiding overloaded notation is understandable, introducing \( E{d+1} \) might help clarify this. Additionally, it is unclear how the recursion is applied when \( T \) and \( t \) have different definitions. For instance, it seems \( T{d+1} \) is treated as a random variable, while \( td \) is fixed. Are you fixing \( Wd \) and treating \( W{d+1} \) as random?  
- In the same proof, does the recursion apply only for \( d > 1 \)? The analysis assumes \( W \in \mathbb{R}^{k \times k} \), but the case \( W \in \mathbb{R}^{k \times m} \) is not addressed. In this case, you cannot assume \( |z^{(0)}| = 1 \).  
- Should the case \( d = 1 \) be analyzed separately to understand how it scales with \( m \)?
Theorem 4 in the main text:
- Is the proof missing? Or is Theorem 4 in the main text equivalent to Theorem 6 in the appendix?
Figures 8 and 9:
- The reduction in trajectory length during training seems to reflect the network becoming contractive to map training points to their corresponding labels. For example, this behavior aligns with discussions on contraction in deep networks.