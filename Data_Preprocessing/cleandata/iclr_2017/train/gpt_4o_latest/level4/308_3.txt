SUMMARY  
This paper tackles significant challenges associated with training generative adversarial networks (GANs). It explores the implications of employing an asymmetric divergence function and identifies sources of instability during GAN training. Subsequently, it introduces an alternative approach based on smoothening techniques.
PROS  
The paper presents solid theoretical insights and addresses meaningful questions with well-reasoned answers.  
It makes an innovative application of concepts from analysis and differential topology.  
It outlines potential strategies to mitigate instability in GAN training.  
CONS  
The paper is somewhat lengthy and highly technical. Certain sections and their implications require further elaboration, which is reasonable to leave for future work.  
MINOR COMMENTS  
- Section 2.1: Consider condensing this section, for instance, by moving all proofs to the appendix.  
- Section 3 offers a clear, intuitive, and straightforward solution.  
- Page 2, second bullet: This also implies that \( P_g \) is smaller than the data distribution in some other \( x \), which consequently makes the KL divergence non-zero.  
- Page 2: Replace "for not generating plausibly looking pictures" with "for generating not plausibly looking pictures."  
- Lemma 1: This result could hold in a more general setting.  
- Theorem 2.1: This appears to be standard analysis; a reference could replace the proof.  
- Theorem 2.4: It would be helpful to remind readers about \( p(z) \).  
- Lemma 2: Similar to Theorem 2.1, this seems to be basic analysis, and a reference could suffice. Also, clarify the domain of the random variables.  
- Typo: "relly" -> "rely."  
- Theorem 2.2: Clarify whether the closed manifolds have boundaries.  
- Corollary 2.1: The phrase "assumptions of Theorem 1.3" is unclear, as Theorem 1.3 is not found in the paper.  
- Theorem 2.5: Replace "Therefore" with "Then."  
- Theorem 2.6: Replace "Is a..." with "is a."  
- The numbering of the theorems is somewhat confusing and could benefit from clarification.