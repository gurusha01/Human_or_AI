The authors investigate whether halting time distributions for various algorithms across different scenarios demonstrate "universality," meaning that after rescaling to zero mean and unit variance, the distribution remains independent of the stopping parameter, dimensionality, and ensemble.
The concept of universality as described is intriguing. However, I have identified several shortcomings in the paper:
To enhance practical relevance, the actual stopping time might be more significant than the scaled version. While the discussion on exponential-tailed halting time distributions is a promising starting point, I am uncertain about its practical applicability. Nevertheless, the theoretical insights presented in the paper could still be of interest.
Particularly for ICLR, it would have been more compelling to explore comparisons across optimization methods such as stochastic gradient descent, momentum, and ADAM on various deep learning architectures. Over which parameters does universality persist? How do different initializations affect the halting time distribution? For instance, I would anticipate that a well-chosen initialization could truncate part of the right tail of the distribution.
Additionally, I found the paper challenging to follow. Below are some issues related to clarity:
- Abstract: The phrase "even when the input is changed drastically" is unclear. It is not evident from the abstract what "input" refers to in this context.
- I. Introduction: The statement "where the stopping condition is, essentially, the time to find the minimum" is confusing. A condition is not a time. Presumably, the authors meant that the stopping condition is met when the minimum has been found.
- I.1: The parameters dimension \(N\), \(\epsilon\), and ensemble \(E\) are introduced without explanation. While later sections provide some context and examples, it is initially difficult to grasp their meaning. Including examples here would improve clarity.
- I.3: The sentence "We use \(x^\ell\) for \(\ell \in Z = \{1, \dots, S\}\) where \(Z\) is a random sample from of training samples" is poorly phrased. Either \(Z\) is a random sample, or \(Z = \{1, \dots, S\}\); it cannot be both.
- II.1: The meaning of \(M\) was difficult to locate. Since this parameter appears to be critical for universality in this context, it would be helpful to explicitly define it earlier in the text.