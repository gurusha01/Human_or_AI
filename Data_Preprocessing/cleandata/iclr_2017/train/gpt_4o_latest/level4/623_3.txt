Studying the Hessian in deep learning, this paper's experiments indicate that the eigenvalue distribution is concentrated around zero, with the non-zero eigenvalues being linked to the complexity of the input data. I find the majority of the discussions and experiments to be both intriguing and insightful. However, there is substantial room for improvement in the current paper.
Quality:  
The arguments presented in the paper could benefit from greater effort and more comprehensive experimental validation. Conducting some of the experiments proposed in the conclusion would significantly strengthen the paper. Additional suggestions include:  
1. Including plots that depict the eigenvalue distribution for other machine learning methods would provide a useful comparison to deep learning.  
2. There are concerns regarding the scaling of the weights. It would be prudent to normalize the weights before calculating the Hessian, as failing to do so could lead to misleading results.  
3. It may be worthwhile to identify a quantitative measure of the Hessian's singularity, as it is challenging to draw conclusions solely from visual inspection of the plots.  
4. Adding plots that show the Hessian during the optimization process is essential, as the focus is primarily on the Hessian's behavior during optimization rather than after convergence.  
Clarity:  
1. The absence of references to figures in the main text creates confusion, making it difficult for readers to understand the context of each figure. For instance, when examining Figure 1, it is unclear whether the Hessian is computed at the start of optimization or after convergence.  
2. The text within the figures is too small and difficult to read.