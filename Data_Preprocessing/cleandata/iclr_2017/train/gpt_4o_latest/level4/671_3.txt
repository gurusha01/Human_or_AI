Overall, this is a well-written paper. Developing a unified framework for these emerging neural models is an important and valuable pursuit.
That said, it is unclear whether the DRAGNN framework (in its current form) constitutes a significant standalone contribution. The central idea is relatively simple: leverage a transition system to unroll a computation graph. Implementing models in this manner allows for code reuse, as modules can be flexibly combined. While this is certainly a useful feature, it seems to be more of an example of good software engineering practice rather than a novel machine learning research contribution.
Additionally, there seems to be limited motivation to adopt DRAGNN, as the framework does not appear to offer any inherent advantages or "free benefits." For instance:
- Writing a neural network in an automatic differentiation library (e.g., TensorFlow or DyNet) inherently provides gradients "for free."
- In the VW framework, there are built-in efficiency optimizations provided by "the credit assignment compiler," which would otherwise require significant effort to implement manually. Furthermore, VW offers a range of principled algorithms for training models (e.g., avoiding exposure bias).
I feel that my concerns regarding the limitations of the framework remain insufficiently addressed. Let me reframe my question: Can you provide concrete examples of models that cannot be (elegantly) expressed within the DRAGNN framework? For instance, how would the framework handle cases where I wanted to implement...