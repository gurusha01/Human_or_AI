The authors present a method to efficiently augment a variant of SVM with numerous virtual instances and report promising preliminary results. The paper is engaging, with a well-thought-out methodology, but it includes partially unsupported and potentially misleading claims.
Pros:
- Thoughtful methodology with reasonable design decisions.
- Potential utility for smaller datasets (n < 10,000) with significant statistical structure.
- Interesting connections with the sum-product literature.
Cons:
- Scalability claims are vague and unclear.
- The paper does not provide a comprehensive narrative about the properties and applicability of the proposed approach.
- Experimental results are limited and preliminary.
The scalability claims are particularly problematic. While the paper highlights scalability issues as a drawback of convolutional networks (convnets), it seems that the proposed CKM method is less scalable than a standard SVM, which itself struggles with scalability compared to deep neural networks. The scalability benefits appear to be confined to datasets with fewer than 10,000 instances. Even if the method could scale to datasets with significantly more instances, it remains unclear whether its predictive accuracy would rival that of convnets in such scenarios. Additionally, the computational cost of generating virtual instances (e.g., performing 10^6 operations for 10^4 training points and 100 test points) seems daunting. For larger datasets, such as those with 10^6 training instances and 10^5 test instances, the feasibility of this approach is questionable. Since scalability is a major limitation of SVMs (e.g., with Gaussian kernels) on modern datasets, the paper must significantly expand and clarify its claims in this regard. Furthermore, the introduction's assertion that convnets scale quadratically with additional training data requires more detail and appears misleading, as convnets typically scale linearly with dataset size.
The paper also suffers from a lack of clarity and presentation issues. As noted, it fails to provide a complete and coherent account of the method's properties and applicability, with critical details often omitted. Broad claims, such as "Just as support vector machines (SVMs) eclipsed multilayer perceptrons in the 1990s, CKMs could become a compelling alternative to convnets with reduced training time and sample complexity," should be avoided. Instead, the authors should focus on providing precise and evidence-based information. For instance, convnets are a type of multilayer perceptron used both in the 1990s and today, and they were not "eclipsed" by SVMs; the two methods have distinct advantages. The claim that CKMs could surpass convnets in scalability is misleading without evidence. Can CKMs handle datasets with millions of training and test instances? If so, would their predictive accuracy be competitive with convnets in such cases? The paper would benefit from specificity and transparency about the method's strengths and limitations.
The robustness of CKMs to adversarial examples, due to the use of virtual instances, is an interesting point. However, there are numerous approaches to improving the robustness of deep networks to adversarial examples, and it would be valuable to compare CKMs to these methods. Additionally, the concept of using virtual instances is not inherently tied to kernel methods. The authors might consider whether a similar approach could be applied to deep networks. While a full exploration of this idea could warrant a separate paper, a brief discussion in the current work would be worthwhile.
One of the key advantages of SVMs (e.g., with Gaussian kernels) over deep networks is their ability to perform well with minimal human intervention. However, CKMs appear to require significant intervention, both in terms of architecture design (similar to neural networks) and in ensuring that virtual instances are generated appropriately for the specific application. The process of creating plausible virtual instances is not straightforward and deserves further discussion. Moreover, unlike general-purpose models such as SVMs with Gaussian or linear kernels or standard convnets, CKMs in this paper seem highly tailored to the NORB dataset. While a tailored approach is valid, the paper should clearly outline how the proposed method could be applied to other problems or what design choices would need to be made for different applications. The early suggestion that CKMs are a general alternative to convnets should be revised to avoid misleading implications.
The experiments provide some insight into the advantages of the proposed approach but are highly limited. To fully understand the method's properties, strengths, and weaknesses, a broader range of datasets and training/test sizes is necessary. The comparisons are also insufficient. For example, why not compare against an SVM with a Gaussian kernel or an SVM using convnet features from the dataset (e.g., the light blue curve in Figure 3)? Other works that combine kernel methods with deep networks should also be considered. The claim that the approach mitigates the curse of dimensionality is plausible but not thoroughly explored. In fact, the curse of dimensionality may impact the scalability of generating virtual instances. Additionally, it is unclear how CKMs would perform without the use of ORB features.
Even if the method can be adapted to scale to datasets with n >> 10,000, it is uncertain whether it would outperform convnets in such scenarios. In the experiments presented, convnets match CKMs in performance after 12,000 examples and would likely surpass CKMs on larger datasets. However, this remains speculative due to the lack of experiments on larger problems.
The methodology, inspired by sum-product networks, is reasonably original in its application to kernel methods and is worth exploring further. While the approach has potential significance, this significance is not convincingly demonstrated in the paper.
The quality of the work is high in terms of thoughtful methods and insights but is undermined by broad claims and a lack of precise detail.
In summary: The paper is promising but requires more specific details and a transparent discussion of the method's applicability, strengths, and limitations. Providing code for reproducibility would also be highly beneficial.