Paraphrased Review:
Summary:  
This paper introduces a novel approach, the Neural Answer Construction Model, aimed at addressing the task of answering non-factoid questions, with a specific focus on love-advice queries. The model is characterized by two primary innovations: 1) it integrates semantic biases inherent in questions into word embeddings, and 2) it optimizes not only for the relevance between questions and answers but also for the optimal combination of sentences in the generated answer. The model is evaluated on a dataset from a Japanese online QA platform and demonstrates a relative improvement of 20% (absolute improvement of 6%) over the baseline model (Tan et al. 2015). Additionally, the paper explores several baseline models, including ablated versions of the proposed approach.
Strengths:  
1. The motivations for the proposed method are well-founded, particularly the need to account for the contextual ambiguity of words and the necessity of generating new answers rather than merely selecting from pre-existing ones on QA platforms.  
2. The paper introduces several novel contributions: 1) embedding semantic biases from questions into word embeddings using a modified paragraph2vec model that incorporates question words, titles, and category tokens; 2) modeling the optimal combination of sentences (e.g., conclusion and supplemental sentences) in the constructed answer; 3) designing abstract answer scenarios inspired by automated web-service composition frameworks (Rao & Su, 2005); and 4) employing an attention mechanism to emphasize key topics in the conclusion sentence within the supplemental sentence, similar to the approach in Tan et al. (2016).  
3. The proposed model achieves a significant performance improvement, outperforming the current best method (Tan et al. 2015) by 20% relatively (6% absolutely).  
4. The paper includes ablation studies that highlight the contributions of various model components, such as incorporating semantic biases and leveraging attention mechanisms, to the overall performance gains.
Weaknesses/Suggestions/Questions:  
1. The process for determining the abstract patterns (e.g., sympathy, conclusion, supplement, and encouragement) used in love-advice answers is unclear. How were these patterns identified? Additionally, the paper should quantify the performance improvement achieved by using these patterns compared to a scenario where candidate answers are drawn from a unified corpus rather than specific corpora for each pattern.  
2. The abstract patterns appear to be domain-specific, meaning that patterns for love-advice questions may differ from those for business-related queries. This suggests that the patterns must be manually designed for each question type, limiting the model's generalizability across domains.  
3. The paper should explicitly analyze the impact of combinational optimization between sentences. A comparison of model performance with and without this optimization, while keeping the rest of the architecture constant, would be valuable. Additionally, plotting model accuracy as a function of combinational optimization scores could provide insights into the significance of this component.  
4. The paper claims that existing non-factoid QA systems cannot generalize to questions outside those stored in QA platforms and positions this as a key contribution. To substantiate this claim, the authors should experimentally demonstrate how well the proposed model generalizes to out-of-domain questions. While the human evaluation involved questions not present in the evaluation dataset, the paper should analyze how distinct these questions were from those in the dataset.  
5. For the human evaluation, were the outputs of both the proposed model and the QA-LSTM model evaluated by the same human experts, or did different experts evaluate each system? If the same experts evaluated both, how were their ratings combined for each question?  
6. The authors could consider conducting a human evaluation where non-expert workers compare the outputs of the proposed model and the QA-LSTM model, selecting which output they prefer as an answer to the question. Such an evaluation would focus on overall preference rather than individual sentence quality or combination. Based on the qualitative examples in Table 4, the QA-LSTM outputs sometimes appear more direct (e.g., "You should wait until you feel excited") compared to the proposed model's outputs, which can seem less direct (e.g., "It is better to concentrate on how to confess your love to her").  
7. For a given question, is the ground-truth answer different between the tasks of answer selection and answer construction?  
8. The paper states that Attentive LSTM (Tan et al. 2016) is evaluated as the current best answer selection method (Section 5.2). However, its accuracy is reported to be lower than that of QA-LSTM in Table 1. The authors attribute this to the length disparity between questions and answers, which introduces noise in the attention mechanism. Did this issue not arise in the dataset used by Tan et al. (2016)?  
9. In the Conclusion section, the paper claims a 20% improvement over the current best method, referring to QA-LSTM as the benchmark. However, in Section 5.2, Attentive LSTM is described as the current best method. The authors should clarify this apparent inconsistency.  
10. Minor correction: Remove the space between "20" and "%" in the abstract.
Review Summary:  
The paper addresses an intriguing and practically valuable problem in non-factoid QA. The motivations for the proposed approach are sound, and the experimental results demonstrate that the model outperforms the baseline. However, the reliance on abstract patterns for answer construction appears to involve manual design, which limits the model's generalizability to other types of non-factoid questions. Additionally, the paper would benefit from more in-depth analysis of the results to better understand the contributions of individual model components.