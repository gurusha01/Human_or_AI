This study investigates the continuous-time dynamics of gradient descent in the context of training two-layer ReLU networks (with one input and one output, resulting in a single layer of ReLU units). The work is noteworthy because it avoids some of the unrealistic assumptions commonly employed in prior studies with similar objectives. Specifically, this paper does not assume independence between inputs and activations, nor does it rely on noise injection (which often simplifies the analysis). However, the removal of these simplifying assumptions comes with certain limitations in the scope of the analysis, namely:
1. Restricting the study to a single layer of nonlinear units.
2. Omitting the bias term in the ReLU activation while assuming Gaussian inputs (thereby precluding the use of the constant input trick to simulate the bias term).
3. Enforcing a strong assumption on the input-output representation through bias-free ReLU networks, specifically the existence of orthonormal bases to model this relationship.
That being said, the paper provides an original and valuable analysis within this new framework. For instance, by leveraging the symmetry inherent in the problem under assumption 3 listed above, the authors successfully reduce the high-dimensional gradient descent dynamics to a two-dimensional system. This reduction from the original parameter space to a bivariate system enables a rigorous examination of the dynamics, such as convergence to a saddle point in the symmetric case or convergence to the optimum in the non-symmetric case.
Clarification Needed: In the first paragraph of page 2, there appears to be a potential inconsistency. Toward the end of the paragraph, it is stated that "Initialization can be arbitrarily close to origin," yet at the beginning of the same paragraph, the initialization is described as "randomly with standard deviation of order 1/sqrt(d)." Could you clarify this apparent contradiction?
Minor Comments:
1. In Section 1, second paragraph: The statement "We assume x is Gaussian and thus the network is bias free" seems unclear. Did you mean to say "zero-mean Gaussian" instead?
2. The term "standard deviation" is repeatedly misspelled as "standard derivation" throughout the paper.
3. On page 6, last paragraph, first line: The reference to "Corollary 4.1" should be corrected to "Corollary 4.2."