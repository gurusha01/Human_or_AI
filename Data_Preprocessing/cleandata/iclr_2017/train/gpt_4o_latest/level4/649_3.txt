This paper explores the question of whether and how syntactic dependencies should be incorporated into unsupervised word representation learning models such as CBOW and Skip-Gram, with particular attention to the distinction between bound (e.g., word+dependency type, 'She-nsubj') and unbound (e.g., word alone, 'She') representations for context during training. The experimental findings are highly mixed, and no proposed novel approach consistently outperforms existing methods.
The study is well-structured, and I have no significant concerns regarding its validity. However, I do not believe this work will appeal broadly to the ICLR audience. The paper addresses a relatively narrow aspect of representation learning that is highly specific to NLP, and its primary contributions are negative results. A more suitable venue might be a short paper at an ACL conference.