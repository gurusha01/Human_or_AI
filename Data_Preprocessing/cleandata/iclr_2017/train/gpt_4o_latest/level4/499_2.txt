This paper introduces an intriguing novel approach for training neural networks, wherein a hypernetwork is employed to generate the parameters of the primary network. The authors demonstrate that this method can reduce the total number of model parameters while still achieving competitive performance on image classification tasks. Notably, the hyperLSTM with non-shared weights delivers impressive results compared to conventional LSTM and its variants on a few language modeling tasks, which is highly thought-provoking.
--pros
This study provides evidence that neural network parameters can be generated by another network while maintaining competitive performance, as shown through several relatively large-scale experiments. The core idea is highly innovative, and the experimental results are robust and convincing.
--cons
The paper could be significantly improved with a sharper focus. Specifically, the key advantage of the hypernetwork approach remains somewhat unclear. While the paper argues that this method achieves competitive results with fewer trainable parameters, the computational complexity at runtime is equivalent to that of a standard main network for static architectures like ConvNets. Moreover, for dynamic architectures such as LSTMs, the computational cost is even higher. The observed improvements of hyperLSTMs over conventional LSTMs and their variants appear to stem primarily from an increase in the number of model parameters.
--minor question
The ConvNet and LSTM models used in the experiments lack a large softmax layer. In many word-level tasks, such as language modeling or machine translation, the softmax layer can exceed 100K parameters. Would it be challenging for the hypernetwork to generate such a large number of weights in these cases? Additionally, would this significantly slow down the training process?