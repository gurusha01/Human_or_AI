The paper explores improved training strategies for Neural GPU models and examines the limitations of the model.
Pros:
- The paper is well-written.
- It presents a variety of investigations.
- Source code is made available.
Cons:
- The title is misleading, as the work does not extend the Neural GPU model itself but focuses solely on its training strategies.
- There are no comparisons with similar architectures (e.g., Grid LSTM, NTM, Adaptive Computation Time).
- The experiments are limited to a few toy tasks; additional experiments on more diverse tasks would strengthen the work.
- The paper reports only negative results. To fully understand these negative results, it is important to identify what is lacking to make the model work. This aspect has not been explored further.
- Certain details are unclear or omitted, such as whether gradient noise was applied in all experiments or the sequence lengths used in cases like Figure 3.
- The number of NTM computation steps is inaccurately described. It is stated as O(n), but it is actually variable.
While the paper highlights the model's limitations, it does not provide a clear understanding of why the model fails. Although specific examples of failure are presented, there is no deeper investigation into the reasons behind these failures or potential solutions to address them.
It seems you intended to reference ICLR 2017.