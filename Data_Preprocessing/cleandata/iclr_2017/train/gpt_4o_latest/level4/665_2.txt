This paper introduces a dataset that offers unique advantages over existing reading comprehension benchmarks. Unlike previous datasets, MS MARCO is constructed from query logs, which reflect real-world questions posed by users, rather than artificially solicited questions that may not align with practical scenarios.
However, there are potential limitations associated with using query logs. Users may tailor their language and queries to align with the capabilities of current search engines, resulting in questions that are less complex in terms of language or content. To address this, the authors could have been more selective with the query logs by down-sampling straightforward questions that can be resolved through simple keyword matching and up-sampling more complex queries that necessitate paraphrasing or synthesizing information across multiple sentences.
While it is commendable that there are increasing efforts to create large-scale reading comprehension datasets, my primary concern is whether the majority of the questions in MS MARCO can be answered through basic text matching, without requiring advanced reading comprehension or reasoning.
Additionally, the paper appears somewhat rushed, as it lacks sufficient analytic and quantitative comparisons with other datasets. I would have appreciated a deeper exploration of the challenges posed by MS MARCO, supported by detailed statistics. For instance, the authors could analyze the dataset by categorizing questions into: (1) those with exact matches in the text snippet, (2) those requiring paraphrasing but with the answer directly available in the text snippet, (3) those necessitating synthesis of information from multiple sentences, and (4) those requiring external knowledge. While the authors have stated in their response that (4) is unlikely, a more formal and comprehensive analysis would enhance the paper's contribution.