This paper is both engaging and relatively straightforward to follow.  
The authors propose a concise yet clever approach to significantly enhance the capabilities of Neural Programming Interpreters. By incorporating recursion, the NPI framework demonstrates improved generalization from a limited number of execution traces.  
This work serves as a compelling example of how a modest but meaningful extension can substantially increase the practicality of a machine learning method.  
I also commend the authors for maintaining consistent notation with the original DeepMind paper. As someone who is not an expert in this domain, I found it convenient to read this paper alongside the original work.  
However, I do have one critique regarding the generalization proofs, which I found somewhat imprecise. For the numerical examples presented in the paper, it is feasible to enumerate all possible execution paths leading up to the next recursive call. That said, how would this method extend to a continuous input space, such as the 3D car example discussed in the original paper? It appears that proving generalization in the continuous case would remain computationally intractable.  
Lastly, are there plans to release the source code?