Review - Summary:  
There exists a variety of pruning techniques aimed at reducing the memory footprint of CNN models, each differing in granularity (e.g., layer, feature maps, kernels, or intra-kernel), pruning ratio, and sparsity of representation. This paper introduces a method to identify the optimal pruning masks from multiple trials. The proposed approach is evaluated on CIFAR-10, SVHN, and MNIST datasets.
Pros:  
- Introduces a method for selecting the optimal pruning mask from N trials.  
- Provides an analysis of different pruning techniques.  
Cons & Questions:  
- "The proposed strategy selects the best pruned network through N random pruning trials. This approach enables one to select a pruning mask in one shot and is simpler than the multi-step technique." How does the method achieve a one-shot selection of the best pruning mask if N random pruning trials are required? (answered)  
- The paper lacks experiments on larger CNN architectures such as AlexNet, VGG, GoogLeNet, or ResNet. (extended to VGG, which is acceptable)  
- Since the ultimate goal is to reduce model size for embedded systems, it would be beneficial to report the memory savings (in MB) achieved by the proposed method compared to other approaches, such as Han et al. (2015).  
Miscellaneous:  
- Typographical error in the caption of Figure 6a: "Featuer" (corrected).