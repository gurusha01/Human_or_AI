This paper presents:
1. Simple, constructive proofs for deriving e-error upper bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.  
2. Generalizations of these results to broader function classes, including smooth and vector-valued functions.  
3. Lower bounds on the size of neural networks as a function of their depth, demonstrating that shallow architectures require exponentially more units to approximate certain functions.  
The manuscript is well-written and straightforward to follow. The technical content, including the proofs provided in the Appendix, appears to be correct. While the proof techniques are relatively simple (and occasionally adapt arguments from Gil, Telgarsky, or Dasgupta), they are effectively synthesized to yield precise and impactful results. As such, I am inclined to recommend acceptance.