This paper tackles the challenge of supervised learning from strongly labeled data containing label noise, a highly practical and pertinent issue in applied machine learning. The authors highlight that traditional sampling-based methods, such as Expectation-Maximization (EM), are inefficient, overly slow, and incompatible with end-to-end training pipelines. To address this, they propose simulating the effects of EM through a noisy adaptation layer—essentially a softmax—that is incorporated into the architecture during training but omitted during inference. Their proposed algorithm is tested on the MNIST dataset, demonstrating improvements over existing methods designed to handle noisy labeled data.
Some comments for consideration:  
1. The paper does not address the increased training complexity introduced by incorporating two softmax layers into the model.  
2. What is the justification for using consecutive (serialized) softmax layers instead of designing a compound objective with two distinct losses or a network with parallel losses and separate gradient flows?  
3. The proposed architecture, which consists of only two hidden layers, does not reflect the larger and deeper models commonly used in practice. It remains unclear whether the reported results would generalize to more complex networks.  
4. The evaluation is limited to MNIST, a dataset that is overly simplistic and not representative of real-world challenges. Why was this dataset chosen, and how would the approach perform on more complex benchmarks?