In this paper, the authors introduce several techniques for sampling visualizations from generative models with high-dimensional latent spaces, such as VAEs and GANs. For instance, they emphasize the well-known yet often underappreciated observation that the probability mass of high-dimensional Gaussian distributions tends to concentrate near a thin hypershell at a specific radius. Based on this, they propose using spherical interpolations (great arcs) instead of the more commonly employed linear interpolations. Along similar lines, they suggest a visualization approach for analogies and methods to enhance structure within VAE latent spaces.
I find it challenging to provide a definitive recommendation for this paper. On one hand, I enjoyed reading it and can see myself potentially applying some of the proposed ideas (e.g., spherical interpolations and J-diagrams) in my own future work. On the other hand, it is clear that this paper does not align with the typical expectations of a machine learning paper; it does not introduce a novel model, training method, or offer significant theoretical or empirical insights, and it lacks the scientific rigor and depth seen in many other ICLR submissions. However, it goes beyond merely presenting useful "tricks." Taking everything into account, I believe this paper merits a broader audience, though I am not entirely convinced that ICLR is the most suitable venue for it.