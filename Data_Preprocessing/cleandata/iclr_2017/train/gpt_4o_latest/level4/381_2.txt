This paper introduces an innovative approach to pruning filters from convolutional neural networks, supported by a robust theoretical foundation. The proposed method is based on the first-order Taylor expansion of the loss variation when pruning a specific unit. This results in a straightforward weighting scheme that combines the unit activation with its gradient relative to the loss function, outperforming the commonly used heuristic of relying solely on activation magnitude for pruning. This approach is intuitively appealing, as it prioritizes the removal of filters not just with low activation, but also those whose incorrect activation values have minimal impact on the target loss.
The authors conduct a comprehensive evaluation against multiple baselines, including an oracle that establishes an upper performance bound, albeit at a high computational cost. The proposed method is both elegant and practical, demonstrating strong generalization across various tasks while being computationally efficient. It integrates seamlessly with traditional fine-tuning procedures. Furthermore, the paper effectively highlights the trade-offs between increased computational speed and reduced performance, offering valuable insights for real-world applications.
It would be beneficial to include comparisons with additional baselines, such as [1]. However, the proposed method appears more advantageous as it avoids the need to train a new network, likely making it significantly faster.
A potential future extension could involve pruning parts of filters (e.g., for 3D convolutions). While this would require modifications to the convolution operator's implementation and may introduce additional complexity, it could yield further computational speedups.
[1]