The authors introduce "information dropout," a variation of dropout that is framed within an information-theoretic context. In this approach, a dropout layer constrains the amount of information transmitted through it, which the authors formalize using a variational bound.
However, it is not evident why imposing such an information bottleneck is theoretically advantageous. While Bayesian interpretations provide a solid foundation for parameter noise, there is no equivalent theoretical justification for activation noise. Although the information bottleneck does restrict the flow of information, the authors do not present a rigorous argument to demonstrate how this leads to improved generalization.
The experimental results are unconvincing. On CIFAR-10, the reported performance is inferior to that of the original paper proposing the network architecture (Springenberg et al.). Additionally, the VAE results on MNIST are notably poor.