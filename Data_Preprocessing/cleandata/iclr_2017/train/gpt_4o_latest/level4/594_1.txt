The author introduces the use of low-rank matrices in both feedforward networks and recurrent neural networks (RNNs). Specifically, they apply their method to a GRU and a feedforward highway network.
Additionally, the author presents the passthrough framework as a contribution, which is intended to describe both feedforward and recurrent networks. However, this framework appears to lack significant novelty compared to the formalism already established by LSTMs and highway networks.
The empirical evaluation is conducted on a variety of datasets, including MNIST, memory/addition tasks, sequential permuted MNIST, and the character-level Penn Treebank.
That said, there are several issues with the evaluation:
- In the highway network experiments, the author does not provide a baseline comparison. As a result, it is difficult to assess the impact of the low-rank parameterization. Furthermore, it would be valuable to compare the results with a highway network that incorporates a capacity bottleneck across layers (e.g., first layer of size $n$, second layer of size $d$, third layer of size $n$) rather than in the gate functions. Additionally, details on how hyperparameter values were selected are missing.
- For the character-level Penn Treebank, the experimental setting differs from prior work, which prevents direct comparisons. Moreover, the overall bits-per-character (bpc) perplexity appears relatively high for this dataset, leaving uncertainty about how the low-rank decomposition would perform when applied to a stronger baseline.
- The author claims state-of-the-art performance on the memory task. However, their approach uses significantly more parameters than the uRNN (41K compared to 6.5K for the memory task), making the comparison somewhat unfair to the uRNN. It would be useful to evaluate how the low-rank RNN performs with a total parameter count of 6.5K. More broadly, it would be insightful to analyze the impact of the matrix rank while keeping the state size fixed.
- Including the baseline and uRNN curves in Figure 2 for the memory/addition task would provide additional clarity.
- The experiments do not make it clear when to use low-rank matrices alone versus low-rank matrices combined with diagonal components.
Overall, the current evaluation is not particularly convincing, with the exception of the results on the sequential MNIST dataset.