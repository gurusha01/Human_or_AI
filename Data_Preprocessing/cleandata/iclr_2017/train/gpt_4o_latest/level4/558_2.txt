The paper introduces a novel exploration strategy for reinforcement learning, leveraging locality-sensitive hashing (LSH) to construct a table of state visit counts, which are then utilized to promote exploration in a manner inspired by MBIE-EB (Strehl and Littman).
This approach has several notable strengths: (1) It is relatively simple compared to existing alternatives such as VIME, density estimation, and pseudo-counts. (2) The paper evaluates the method across a diverse set of environments, including classic benchmarks, continuous control tasks, and Atari 2600 games. Furthermore, comparisons are provided against several other algorithms, including recent DQN variants. The results demonstrate clear improvements over the baseline, though the performance relative to other exploration methods varies by domain/game. However, this variability is acceptable, given the simplicity of the proposed technique. (3) The paper also includes an analysis of the sensitivity of the method to the granularity of the abstraction, which is an important consideration.
My primary concern lies in the robustness of the conclusions. There appears to be significant engineering effort involved in making the method work, leaving questions about how sensitive the results are to hyperparameter choices or specific implementation details. For instance, how critical were design decisions such as using PixelCNN (or tying its weights), adding noise to the autoencoder output, or incorporating custom modifications to BASS? The sensitivity analysis for granularity highlights that the choice of resolution has a notable impact, and the results across games are inconsistent.
Another issue is the authors' decision to use state-based counts rather than state-action-based counts, which deviates from the theoretical foundation of MBIE-EB. This choice is puzzling, as the use of LSH was presumably intended to approximate tabular counts in line with MBIE-EB. The authors do not provide an explanation for why state-based and state-action-based counts yield similar performance in Atari, leaving this an open question.
Additionally, the method seems well-suited for integration with DQN, yet the authors omit experiments involving DQN, despite comparing their approach to several DQN-based variants. The justification for using TRPO, based on its property of safe policy improvement, is unclear in the context of adding exploration bonuses, which may alter this property.
The case study on Montezuma's Revenge, while intriguing, relies on domain-specific knowledge and feels somewhat disconnected from the rest of the paper.
In summary, the paper presents a simple and elegant idea for improving exploration, tested across a variety of domains. However, it is unclear which components of the method are essential versus merely helpful, raising concerns about the long-term reproducibility and impact of the work.
---
After Response:
Thank you for the detailed rebuttal, and I apologize for the delay in my reply.
I appreciate the additional clarification regarding the robustness of SimHash and the comparison between state-based and state-action-based counting.
The paper tackles an important problem—exploration—by proposing a "simple" counting method based on hashing, which stands as a compelling alternative to density estimation approaches like those of Bellemare et al. If reviewer discussions were possible, I would now advocate for acceptance of the paper. Specifically, I am less concerned than Reviewer 3 about achieving state-of-the-art performance in Montezuma's Revenge, as the strength of this work lies in the simplicity of the hashing approach and the broad comparison of domains against the baseline TRPO. This paper effectively demonstrates that simple hashing methods still hold promise. While there remain concerns about the reproducibility of the results due to the "fiddly bits" required to make the method work, the proposed approach is a valuable and contrasting contribution to exploration research that warrants attention.
Additional Note (Not Critical to Decision): The rebuttal's argument regarding DQN and A3C appears to misinterpret my original comment. I only referred to DQN, not A3C. DQN is generally less sensitive to parameter tuning than A3C, and Bellemare et al. (2016) used DQN as their primary model for Montezuma's Revenge. Thus, the omission of experiments involving DQN in the Atari domain remains puzzling. The reference to Figure S9 from Mnih et al. seems misplaced, as that figure pertains to asynchronous one-step Sarsa with varying thread counts, which is inherently sensitive to parameters due to its asynchronous nature and the parameter being varied (thread count). This is not indicative of DQN's stability, as DQN is single-threaded, uses experience replay (slowing policy updates), and employs a target network updated infrequently. It is possible the authors misreferenced the figure in their rebuttal, as I could not locate a Figure 9 in the cited work.