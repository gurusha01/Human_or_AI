This paper introduces a design principle for computational blocks in convolutional networks, leveraging repeated applications of expansion and joining operations to create a fractal-like structure.
The primary focus of this work is experimental evaluation, aiming to demonstrate that a residual formulation is not strictly necessary to achieve strong performance, at least for certain tasks.
That said, I find the experimental evaluations presented in the paper unconvincing. The main concern is the absence of a robust baseline, which is essential for isolating and clearly demonstrating the improvements brought by the proposed approach. While I acknowledge that constructing such a baseline is challenging for a novel architectural principle, this makes it even more critical to invest additional effort into this aspect. Establishing a strong baseline would ensure that the insights from this work remain relevant even as more advanced architectures emerge. Furthermore, comparisons between architectures should account for factors like the number of parameters and computational cost to ensure fairness. Below are some specific comments:
- In Table 1, the comparisons to ResNets should include the ResNets from He et al. (2016b) and Wide ResNets, as these provide a reasonable alternative in the absence of a proper baseline. Notably, the former outperforms FractalNet on CIFAR-100, while the latter surpasses it on both CIFAR-10 and CIFAR-100. Although the authors compare results without augmentation, they did not conduct additional experiments without augmentation for these competing architectures.
- The 40-layer FractalNet should not be compared to other models unless parameter reduction techniques are applied to those models as well, ensuring a fair comparison.
- A thorough comparison with Inception networks is also necessary. It seems that the seemingly "ad-hoc" design of Inception modules is motivated by reducing the computational footprint, which is not a primary focus of FractalNet. Given the conceptual similarity between the two (e.g., shorter and longer paths without shortcuts), a strong baseline could be constructed by simplifying the Inception designâ€”such as replacing the concatenation operation with a mean operation over equally sized convolution outputs. As a side note, Inception networks have already demonstrated that residual connections are not essential for achieving top-tier performance [1].
- It is worth mentioning that Residual and Highway architectures exhibit a form of anytime property, as evidenced by lesioning experiments in Srivastava et al. and Viet et al.
- The architecture-specific drop-path regularization is an intriguing idea, but its standalone benefit is unclear since it is used alongside other regularization techniques like dropout, batch normalization, and weight decay.
In conclusion, the experiments presented in the paper do not convincingly establish the utility of the proposed architecture.
[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. "Inception-v4, inception-resnet and the impact of residual connections on learning." arXiv preprint arXiv:1602.07261 (2016).