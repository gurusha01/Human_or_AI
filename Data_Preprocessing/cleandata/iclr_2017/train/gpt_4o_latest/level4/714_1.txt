This paper applies the method proposed by Jonschkowski & Brock to learn a low-dimensional state representation, specifically represented as the final layer of a neural network. The experimental setup uses this method to derive a one-dimensional state representation of a simulated robot's head position from synthetic image data.
The topic of learning state representations is an active and valuable area of research, particularly for developing representations in interactive domains like robotics. However, the method itself does not appear to introduce any novelty beyond the work of Jonschkowski & Brock. The main contribution of the paper lies in its experimental evaluation, which is limited to a single task. In this task, the paper assesses the correlation between the learned state representation and the ideal state representation (i.e., the robot's head position).
As the authors themselves acknowledge, the experiments are highly preliminary, focusing on a single, simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experimental results more compelling, comparisons to prior methods such as those by Lange et al. '12, Watter et al. NIPS '15, and Finn et al. ICRA '16, which also address state representation learning from raw images, are necessary. Additionally, a baseline comparison using PCA on the images would be informative, especially for such a simple task. Without these comparisons, it is not possible to properly evaluate the effectiveness of the method.
Furthermore, as noted in the pre-review questions, the related work section should discuss other state representation learning approaches, including those by Watter et al. NIPS '15, Finn et al. ICRA '16, and van Hoof et al. IROS '16.
In conclusion, this paper lacks both novelty and significance, as it primarily implements an existing method and presents results on a single, simple task. The absence of comparative evaluations makes it difficult to interpret the results. Incorporating more challenging tasks and experimental comparisons would greatly enhance the paper. Additionally, the paper does not propose any novel contributions to the field of state representation learning or address key challenges in this domain. On the positive side, the paper is generally well-written and clear.