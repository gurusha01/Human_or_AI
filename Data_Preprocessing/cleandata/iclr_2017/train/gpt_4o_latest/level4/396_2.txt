The paper introduces a model for image generation that first synthesizes the background and subsequently incorporates the foreground by generating a foreground mask and its corresponding appearance. The appearance image is then warped using the mask, and the mask is transformed via a predicted affine transformation to overlay it onto the background. Through evaluations conducted with AMTurkers, the authors demonstrate that their generated images are perceived as more natural 68% of the time compared to images produced by a DC-GAN model, which lacks a figure-ground-aware image generation mechanism.
The segmentation masks effectively capture objects in highly constrained datasets (e.g., birds), suggesting that the approach may be limited when applied to datasets with more diverse shapes, as acknowledged by the authors. Nonetheless, the architectural innovations presented in the paper hold potential significance.
It would be interesting to explore whether this layered model is capable of generating multiple layers of foreground (e.g., occluding foregrounds) or if its functionality is restricted to figure-ground awareness alone.