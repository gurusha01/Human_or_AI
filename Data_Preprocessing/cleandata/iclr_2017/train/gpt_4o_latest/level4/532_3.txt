This paper introduces a novel approach to enhance standard gradient descent algorithms by incorporating a "Data Filter," which serves as a curriculum teacher. The filter determines which training examples the target network should process to optimize learning. The filter itself is trained concurrently with the target network using Reinforcement Learning (RL) algorithms, with rewards derived from the state of training relative to a pseudo-validation set.
Stylistic comment: Please adopt the more common citation style of "(Author, year)" when the author is not explicitly referenced in the sentence. For example, "and its variants such as Adagrad Duchi et al. (2011)" should be written as "such as Adagrad (Duchi et al., 2011)." However, cases like "proposed in Andrychowicz et al. (2016)" should remain unchanged.
The paragraph containing "What we need to do is, after seeing the mini-batch Dt of M training instances, we dynamically determine which instances in Dt are used for training and which are filtered" requires clarification. Specifically, what does "seeing" mean in this context? It should be explicitly stated that the forward pass is performed first, features are computed, and then a decision is made about which examples to include in the backward pass.
Unclear methodological choices:
1. Why is the REINFORCE policy updated only at the end of an episode (Algorithm 2), while the actor-critic model is trained at each step (Algorithm 3)? While it is true that REINFORCE has high variance, this does not preclude training it at each stepâ€”unless there are experimental results supporting this choice, which should either be included in the paper or explicitly mentioned.
2. Why not use the same reward signal for both the REINFORCE and actor-critic models? Similarly, why not train REINFORCE with the actor-critic reward and vice versa? The claim that REINFORCE requires waiting for the episode to end is not entirely accurate, especially given that the data is i.i.d. The definition of an "episode" could range from a single training step or one mini-batch \( D_t \) to the entire multi-epoch training process.
Concerns with the experimental setup:
- Is Figure 2 based on a single experiment per setup, or does it aggregate results from multiple runs with different initial weights? If it is based on a single run, there is no way to determine whether the results are statistically significant or due to chance. This is a critical issue.
- Many state-of-the-art optimization methods, such as Adam and RMSProp, are widely used. It is surprising that they were not included in the experiments.
- The learning rates for the RL and supervised learning (SL) components are not clearly specified. How quickly should the RL component adapt to changes in the SL component? It is unclear whether this interaction was systematically explored.
- The environment, i.e., the target network being trained, is highly non-stationary. It would be valuable to measure how the policy evolves over time. For example, Figure 3 could reflect either the policy adapting to the changing environment or the policy remaining static while the features change. The latter scenario might indicate a failure of the policy to adapt.
- The paper does not adequately address the non-stationarity of the environment. As the target network progresses, the feature distribution changes, which impacts optimization. This issue should be explicitly discussed.
- How is the "pseudo-validation" data used to guide the policy selected? The second paragraph of Section 3.2 suggests that it is a subset of the training data, but the algorithms imply that the same data is used to train both the policies and the networks. This inconsistency should be clarified.
Overall assessment:
The proposed idea is both novel and promising, and the paper is generally well-written. However, there are significant methodological concerns and gaps in the experimental design. Clearer explanations and either stronger justifications for the experimental choices or additional experiments are necessary to address these issues. Unless the authors can provide convincing responses, I believe it would be better to delay publication and resubmit a more robust version of this work. This idea has the potential to be impactful, but the current results do not fully support its promise.