Training highly non-convex deep neural networks is a critical practical challenge, and this paper presents a compelling investigation of a novel approach for more effective training. The empirical results, both in the manuscript and in the authors' responses during the discussion phase, convincingly demonstrate that the proposed method consistently improves accuracy across various architectures, tasks, and datasets. The algorithm itself is straightforward (alternating between training the full dense network and a sparse version), which is an advantage, as it increases the likelihood of adoption by the research community.
The paper would benefit from revisions to include the additional experiments and insights shared during the discussion, particularly the accuracy comparisons under the same number of training epochs.