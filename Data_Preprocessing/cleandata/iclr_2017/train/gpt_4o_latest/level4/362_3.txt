This paper introduces a novel approach to learning a custom optimizer tailored to a specific class of optimization problems. In the context of training machine learning algorithms, I interpret a class to correspond to a model, such as "logistic regression." The authors ingeniously frame this as a reinforcement learning problem and leverage guided policy search to train a neural network that maps the current state and history to a step direction and magnitude. Overall, I find this to be an innovative idea and a valuable contribution to the rapidly expanding meta-learning literature. That said, there are several areas where the paper could be strengthened.
First, the authors assert that their method learns the regularities of an entire class of optimization problems, as opposed to exploiting regularities in a specific set of tasks. However, this distinction remains somewhat unclear to me. For instance, in the case of learning an optimizer for logistic regression, the authors appear to claim that training on a randomly sampled set of logistic regression problems enables the model to generalize to logistic regression as a whole. I am skeptical of this claim, as the randomly sampled data introduces its own biases. In the paper, for example, "The instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each." This suggests that the optimizer is trained to handle instances of logistic regression within this specific family of training inputs rather than logistic regression problems in general. A straightforward experiment to validate broader generalization would be to repeat the current experiments with test instances drawn from an entirely different distribution. It would be particularly compelling to analyze how performance changes as the test distribution diverges further from the training distribution.
Could the authors elaborate on the choice of architecture employed in this work? Specifically, why was a single layer with 50 hidden units and softplus activations chosen? Why not, for example, a deeper architecture with 100 units per layer and ReLU activations? Presumably, this choice was made to mitigate overfitting, but given the network's limited capacity, how do the results scale when the input dimensionality increases beyond 2 or 3?
Additionally, I would be very interested in visualizing the learned policy on a simple 2D function using a contour plot. How do the optimization steps taken by the network compare to those of traditional hand-engineered optimizers on a random problem instance?
In summary, I find this paper to be highly interesting and methodologically impactful. My primary concern is that the results may be somewhat overstated, as the problems addressed remain relatively simple and constrained. However, if the authors can demonstrate that their approach yields robust policies across a broader and more diverse set of problems, the contribution would be truly outstanding.
Minor comments:
- In Section 3.1, should you be using \piT^ to denote the optimal policy? Currently, both \pit^ and \pi^* are used, which could be confusing.
- Are the problems considered noiseless in this work? That is, are state transitions deterministic given an action? It would be fascinating to see how the method performs on noisy problems.