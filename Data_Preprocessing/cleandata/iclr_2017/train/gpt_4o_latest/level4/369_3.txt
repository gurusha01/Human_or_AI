This paper introduces a novel ternary weight quantization method that maps weights to either 0 or one of two layer-specific, stochastically learned values. Unlike prior work, these quantized values are independently learned alongside other network parameters. The proposed method demonstrates strong quantization performance, maintaining or even surpassing the accuracy of corresponding full-precision networks on CIFAR10 and ImageNet.
Strengths:
- The paper is generally well-written, and the algorithm is clearly explained.
- The proposed approach performs effectively in experiments, achieving significant compression without sacrificing performance (and occasionally improving it).
- The analysis of sparsity evolution during training was particularly interesting, though it remains unclear if any actionable insights can be derived from it.
Comments:
- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Could the authors provide the average activation sparsity for each network to validate this assumption? Even if the assumption does not fully hold, demonstrating relatively similar average activation values across networks would strengthen the comparison.
- In Section 5.1.1, the authors state that using a fixed threshold parameter \( t \) (set at 0.05) across all layers allows for varying sparsity due to differences in relative weight magnitudes. In Section 5.1.2 (paragraph 2), they extend this idea, suggesting that additional sparsity can be achieved by assigning different \( t \) values to each layer. How are these layer-specific thresholds determined? Are results for this multi-threshold approach included in any of the tables or figures? If not, could they be added?
- The authors assert that "ii) Quantized weights play the role of 'learning rate multipliers' during backpropagation" as an advantage of using trained quantization factors. Could the authors clarify why this is beneficial?
- The captions for figures and tables lack detail and could be more descriptive.
Preliminary Rating:
This paper presents interesting results and demonstrates strong experimental performance, but its novelty is somewhat limited.
Minor Notes:
- Table 3 reports FLOPS instead of energy for the full-precision model. Why is this the case?
- Section 5: "speeding up" (incomplete phrase).
- Section 5.1.1: Figure reference error in the last line.