The paper investigates the effects of employing customized number representations on the accuracy, speed, and energy efficiency of neural network inference. Experiments are conducted using several standard computer vision architectures, such as VGG and GoogleNet, and the results suggest that floating-point representations are generally preferable to fixed-point representations. Furthermore, it is shown that floating-point numbers with approximately 14 bits are sufficient for the tested architectures, leading to only a minor reduction in accuracy.
The paper offers a comprehensive overview of floating-point and fixed-point representations and addresses a critical but underexplored topic in deep learning. While there are several areas where the paper could be improved, I am inclined toward a weak accept, provided the authors address the following concerns:
1. The paper does not make it explicit that its focus is exclusively on neural network inference. To clarify this, the word "inference" should be included in the title and abstract. Additionally, the paper should explicitly state that its findings may not generalize to neural network training, as the dynamics of training could differ significantly.
2. The paper does not explore the possibility of incorporating quantization techniques during training, which could potentially allow for the use of fewer bits during inference. This aspect warrants discussion.
3. It is unclear whether the reported running time and power consumption metrics account for all modules or are limited to multiply-accumulate units. Additionally, the accuracy of these metrics should be discussed, considering variations in hardware designs and potential discrepancies between simulation and real-world production. The authors should elaborate on the simulation methodology in the paper.
4. The discussion surrounding "efficient customized precision search" appears to be of limited importance. In scenarios involving critical hardware considerations, even a 20x increase in simulation time may be acceptable. Exhaustive search processes can be parallelized, and it may be more beneficial to invest additional simulation time to identify the exact optimal configuration rather than relying on approximations. Moreover, weak configurations can often be filtered out after evaluating only a small subset of examples.
5. The paper should discuss Nvidia's Pascal GP100 GPU, which supports FP16, and cite relevant Nvidia papers or documentation.
Additional comments:
- The sections of the paper addressing "efficient customized precision search" are somewhat unclear and could benefit from further clarification.
- As a suggestion for future work, the authors could investigate the impact of number representations on batch normalization and recurrent neural networks.