The challenge of leveraging all available information across multiple modalities to learn a meaningful "joint" embedding is an intriguing and promising direction, particularly for enhancing recommender systems in "cold start" scenarios. To the best of my knowledge, approaches combining as many modalities as proposed in this paper are rare, and a successful implementation could have significant impact. However, several aspects of the proposed architecture appear sub-optimal:
1. One of the key advantages of neural network-based systems is their ability to be trained end-to-end in a joint manner. The proposed method, however, relies on largely pre-trained modules for different modalities that are stitched together. While this approach might be justifiable in cases where training data is severely limited, the Amazon dataset reportedly contains 10M product pairs. This volume of data seems sufficient to enable joint training, yet the paper does not address this possibility. Since I have not worked with this dataset myself, I may be missing some context, but the lack of discussion on this topic is a significant omission. The absence of a jointly fine-tuned model is, in my view, a major limitation of the proposed approach.
2. The explanation and motivation for the "pairwise residual units" are unclear. If I understand correctly, the residual formulation applies a ReLU layer to the concatenation of modality-specific embeddings, generating a new similarity score (post dot product) that is then added to the similarity score derived from the direct concatenation. However, it is unclear why this approach is preferable to simply using an additional fully-connected layer to combine the modality-specific embeddings into a final embedding (potentially of lower dimensionality). This alternative should at least be included as a baseline for comparison, especially if the pairwise residual unit is presented as a novel contribution. The explanation provided in the paper is unconvincing, particularly regarding how the residual approach reduces the parameter count.
3. A more minor point: While the use of TextCNN for text embeddings seems reasonable, I wonder how an LSTM-based approach might perform in comparison. Additionally, the details of how TextCNN is applied are not clearly described in the paper. According to the authors' response to a question, the model processes the concatenation of the first 10 words from the title and product description. For the product description in particular, this truncation seems overly restrictive and likely insufficient to capture meaningful information.
Overall, the paper would benefit from more thorough justification of the design choices made. Additionally, I am not familiar with the state of the art on this dataset, so I cannot assess whether the comparisons presented are comprehensive. Only one competing technique is evaluated, and none are tested in the more challenging cold-start scenarios, which raises concerns about the completeness of the evaluation.
Minor issue: In the second paragraph of page 3, there is a placeholder reference "(cite Julian)" that needs to be corrected.