The manuscript introduces an attention-based framework for video description, employing three LSTMs and two attention mechanisms to sequentially generate words from a sequence of video frames. Specifically, the LSTM encoder for frames (TEM) utilizes a spatial attention mechanism to compute a weighted average per frame, while a second LSTM (HAM) applies attention over the hidden states of the encoder. A third LSTM, operating temporally in parallel with the second, generates the sentence one word at a time.
Strengths:
- The paper addresses a relevant and compelling problem in video description.
- To the best of my knowledge, the proposed use of two layers of attention in this specific configuration is novel, though the paper overstates its contributions without sufficient attribution (see below).
- The approach is evaluated on two datasets, MSVD and Charades, demonstrating performance comparable to related work on MSVD and improvements on Charades.
Weaknesses:
1. Claims of Contribution/Novelty:
   1.1. The Hierarchical Attention/Memory (HAM), one of the main contributions, lacks sufficient differentiation from prior work:
   - The equations (6–8) do not appear to significantly deviate from the models in Xu et al. or Yao et al. While Xu et al. focus on spatial image locations and Yao et al. on frames, this work attends to encoded video representations \( h_v^i \). The primary distinction seems to be the use of an additional LSTM for decoding, whereas Xu et al. employ the same LSTM for generation.
   - The claim in Section 3.2 that \( fm \) "memorizes the previous attention" is unclear, as \( Hm^{t'-1} \) only includes the last hidden state, and \( f_m \) does not access the attention weights \( \alpha \). This issue, raised in prior comments, remains unresolved.
   - The assertion that "attention is a function of all previous attentions and network states" is misleading. While LSTMs inherently model dependencies, the formulas and Figure 1 suggest that \( Hg^{t'-1} \) and \( Hm^{t'-1} \) only include the last hidden state, not the full network states.
   - The claim of multi-layer attention in HAM is vague, as the source of this multi-layer structure is not apparent.
   1.2. The statement in Section 3.1 that CNN features "discard low-level information useful for modeling motion" implies that the proposed approach addresses this issue. However, the model cannot capture motion dynamics since the attention \( \rho \) between frames is unavailable during prediction. Additionally, the use of high-level VGG conv5 features contradicts the claim of capturing low-level information.
2. Related Work:
   - The distinction between HAM and prior works (e.g., Yao et al., Xu et al.) should be clarified, and these papers should be explicitly cited in the HAM section.
3. Conceptual Limitations:
   - The model's spatial and temporal attention mechanisms are independent. Spatial attention, fixed during sentence generation, cannot adapt to different words in the sentence. For instance, in describing "the dog jumps on the trampoline," the model should focus on the dog for "dog" and the trampoline for "trampoline." However, the fixed spatial attention may consistently attend to the same regions, limiting flexibility.
   - The encoder lacks an explicit mechanism to focus on different aspects of a frame, potentially leading to uniform spatial attention across frames.
4. Inconsistencies in Model Description:
   - Equation 11 conflicts with Figure 1 regarding how the model incorporates the previous word. Equation 11 suggests the softmax output is used, which is unusual. Typically, the ground truth word is used during training, and the predicted word (via hardmax) is used during testing. This discrepancy should be clarified.
5. Clarity:
   5.1. Consistent notation across equations (2–5) and (6–9) would improve readability. The rationale for introducing different notations is unclear.
   5.2. Figure 1 lacks sufficient detail. Additional figures or expanded annotations would help clarify the architecture. If space is a concern, standard equations (e.g., for LSTMs, softmax, or log-likelihood loss) could be omitted or inlined.
6. Evaluation:
   6.1. The claim that the proposed architecture achieves state-of-the-art results is overstated:
   - On the MSVD dataset, Pan et al. (2016a) report higher METEOR scores (33.10) than those achieved in this work.
   - The model does not outperform all prior methods across metrics, as Yu et al. achieve superior results.
   - For the Charades dataset, the claim is premature, given the limited number of methods evaluated. Ablation results (Table 1) should also be reported for Charades to substantiate claims.
   6.2. The paper lacks qualitative results for the attention mechanisms. Visualizations of spatial and temporal attention would help assess their effectiveness. For instance, are the attention distributions diverse, peaky, or uniform?
   6.3. Performance improvements over ablations are marginal. For example, the gains over Att+No TEM are only 0.5 METEOR, 0.7 BLEU@4, and a decrease of 1.7 CIDEr.
   6.4. The absence of human evaluation is a missed opportunity. While the authors argue it is infeasible, evaluating a subset of test data is manageable. Even if other authors do not provide code/models, sharing predicted sentences is common practice and sufficient for human evaluation. If sentences were unavailable, this should be explicitly noted.
   
7. Unaddressed Reviewer Comments:
   - Several prior comments, such as incorporating SPICE evaluation and addressing clarity issues, remain unresolved.
8. Hyperparameter Inconsistencies:
   - The number of sampled frames differs between ablation analysis (40 frames) and performance comparison (8 frames). This inconsistency raises questions about the validity of the results. Performance for all ablations with 8 frames should be reported.
Other (Minor/Discussion Points):
- Equation 10: How are \( hm \) and \( hg \) handled? The provided LSTM formulas only account for two inputs. Are they concatenated?
- Section 4.1 lacks a corresponding Section 4.2.
- The claim in Section 4.1 that the architecture can "model the temporal structure of a video sequence" and "map visual space to language space" applies broadly to many existing approaches, such as Venugopalan et al. (2015 ICCV).
Summary:
The paper makes ambitious claims regarding its approach and results, but the novelty is limited, and the experimental results are not sufficiently compelling compared to prior work or ablations. Improved clarity, additional visualizations, and a more thorough evaluation would significantly strengthen the manuscript.