After the rebuttal:
The paper presents an intriguing set of results, many of which were generated after the initial submission. However, the novelty remains limited, and the presentation could be improved. 
The most significant issue for me now is the mismatch between the title and the content. The authors explicitly target deterministic encoder-decoder models (as outlined in Section 3.2), which are not equivalent to generative models, even though generative models often employ this architecture. While the small experiment involving sampling is interesting, it does not alter the primary focus of the paper. This inconsistency is problematic. A straightforward solution would be to replace "generative models" with "encoder-decoder networks" in the title. If this adjustment is made, I would lean toward recommending acceptance.
---
Initial review:
The paper introduces three methods for generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN) and provides a comparative analysis of these approaches. While adversarial examples in discriminative models are well-studied, I am not aware of prior work on adversarial examples for generative networks, making this work novel (though the concurrent work by Tabacof et al. should be cited). The paper has improved significantly since the initial submission, but I still have several comments regarding its presentation and experimental evaluation. I am currently on the borderline and may revise my rating during the discussion phase.
Detailed comments:
1. The paper exceeds the recommended page limit of 8 pages, spanning 13 pages. Reviewers must read multiple papers and versions, making this excessive length burdensome. The authors should condense the paper and/or move content to the appendix. It is the authors' responsibility to ensure the paper is concise and accessible. The justification that "in our attempts to be thorough, we have had a hard time keeping the length down" is not acceptable—this challenge must be addressed.
2. I deliberately avoided using the term "generative model" earlier because it is unclear whether the attacks described truly target generative models. Specifically, the authors train encoder-decoders as generative models (VAE or VAE-GAN) but then remove all stochasticity (sampling) and the prior on latent variables, treating the models as deterministic encoder-decoders. It is unsurprising that a deterministic deep network can be easily fooled. It would be far more compelling to investigate whether the probabilistic nature of generative models enhances their robustness to such attacks. Am I misunderstanding something? I would appreciate clarification from the authors and adjustments to the claims in the paper if necessary.
3. The paper is motivated by potential attacks on a data channel utilizing a generative network for compression. However, the attack scenario described in Section 3.1 is unconvincing. It occupies substantial space without adding much value. First, experiments on natural images are essential to assess whether the proposed attack could succeed in a realistic scenario. Second, I am unaware of any practical applications of VAEs for image compression—attacking JPEG would be far more relevant.
4. The experiments are limited to MNIST and, in the latest version, SVHN (which is a welcome addition). While robust generative models for general natural images are lacking, it is standard practice to evaluate generative models on datasets of faces. This would be a natural and valuable domain for testing the proposed methods.
Minor remarks:
1. The use of "Oracle" in Section 3.2 seems inappropriate. Typically, an oracle has access to (part of) the ground truth, which does not appear to be the case here as far as I understand.  
2. At the beginning of Section 4: "All three methods work for any generative architecture that relies on a learned latent representation z"—"are technically applicable to" would be more precise than "work for."  
3. Typo in Section 4.1: "confidentally" should be corrected.