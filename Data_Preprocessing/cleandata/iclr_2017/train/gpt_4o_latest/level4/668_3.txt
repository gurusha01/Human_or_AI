The authors introduce a straightforward concept: they penalize overconfident predictions by incorporating the entropy of the predictive distribution as a regularization term. They explore two variations of this approach. In the first, they penalize the divergence from a uniform distribution. In the second, they penalize the deviation from the base rates, which they refer to as the "unigram" variation. However, I find this terminology somewhat unconventional, as I am not accustomed to multi-class labels being described as unigrams. This raises the question: what would constitute a bigram in this context?
The proposed idea is simple and, while it has been utilized in reinforcement learning, it has not yet gained traction as a regularization technique for improving generalization in supervised learning.
That said, the theoretical justification for the approach remains underdeveloped. The authors' responses comparing their method to L2 regularization reveal some gaps. For instance, a basic number line example with polynomial regression clearly demonstrates how L2 regularization can prevent a model from overfitting by discouraging it from perfectly fitting every data point. In contrast, it seems relatively easy to fit every data point while still achieving arbitrarily high entropy. Naturally, the unregularized objective is to maximize log likelihood rather than merely maximize accuracy. There may be an interesting interaction between the log likelihood objective and the entropy-based regularization, but the paper does not provide a detailed explanation of this interplay.
One plausible scenario comes to mind: when the network outputs probabilities close to 0, the loss can become very large (if the true label is 1). Entropy regularization might stabilize the gradient in such cases, mitigating sharp losses for outlier examples. If so, the primary benefit of this regularization could be faster convergence. I encourage the authors to empirically analyze this hypothesis, particularly by examining the distribution of gradient norms.
The paper's strength lies in its empirical thoroughness. The authors rigorously evaluate their method across a variety of well-known benchmarks, including both CNNs and RNNs. Their results suggest that, on certain datasets—especially in the context of language modeling—the confidence penalty outperforms label smoothing.
At this stage, I consider this paper a borderline contribution, but I remain open to revising my assessment if the authors address the identified concerns and provide further clarifications.
Typo:
In the related work section: "Penalizing entropy" should be corrected to "Penalizing low entropy."