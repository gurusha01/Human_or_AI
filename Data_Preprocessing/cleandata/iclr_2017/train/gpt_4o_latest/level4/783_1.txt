This paper was straightforward to follow, and the primary idea was conveyed with clarity.
The main points of the paper (along with my concerns) can be summarized as follows:
1. Synchronous algorithms face challenges with straggling nodes, which the algorithm must wait for. From my experience, this issue has not occurred on platforms like Amazon EC2 cloud. However, it does arise on our university's shared cluster when some users heavily load certain nodes. Perhaps if the nodes were dedicated solely to the user's job, this issue would be less significant. (It is unclear what type of cluster was used to generate Figures 3 and 4.) Additionally, how many experiments were conducted? In my experience, gradients are typically received on time from all nodes at nearly equal speeds. However, in less than 0.1% of iterations, I have observed that one node might take up to twice as long. The increasing shape of the curve in the results suggests a potential inefficiency in the communication implementation. Could this be due to serialized communication? Would using a method like "MPI_Reduce" (even if waiting for the slowest node) result in faster performance?
2. Asynchronous algorithms reduce waiting time but may converge more slowly. Furthermore, these algorithms risk divergence if stale gradients are not carefully managed. While they provide strong guarantees for convex functions, applying them to non-convex deep neural networks (DNNs) can introduce additional challenges.
3. The authors propose using gradients from the first "N" workers out of "N+b" available workers. My concern here is that the focus is solely on worker nodes. What happens if the parameter server becomes a bottleneck? How would the authors address this issue? That said, if the number of nodes (N) is not very large and a deep DNN is used, I can imagine that communication overhead might account for no more than 30% of the runtime.
My primary concern lies with the experiments. Using different batch sizes typically requires adjusting the learning rate, correct? How were the learning rates and other parameters tuned for experiments like those in Figure 5? While the authors provide formulas in (A2), these could introduce bias into the results. Wouldn't it be more representative to tune "\(\gamma, \beta\)" for each N? Additionally, it would be preferable to run the experiments multiple times and report the average, best, and worst-case behaviors. As it stands, the results could simply be coincidental.