The authors highlighted certain limitations of existing deep architectures, particularly their challenges in optimization on small to mid-sized datasets, and proposed a method to stack marginal Fisher analysis (MFA) for constructing deep models. The proposed approach was evaluated on several small to mid-sized datasets and compared against various feature learning methods. Additionally, the authors incorporated established deep learning techniques, such as backpropagation, denoising, and dropout, to enhance performance.
However, the paper's novel contribution is limited. MFA is a well-established method, and the authors neither provided theoretical nor empirical justification for stacking MFAs. Furthermore, the comparison omitted deep architectures that require backpropagation across multiple layers, which the paper initially aimed to address. Instead, all compared methods were trained layer by layer. It remains unclear whether randomly initialized deep models, such as DBNs or CNNs, would perform poorly on these datasets. Additionally, the rationale behind the chosen model architectures and hyper-parameters for different datasets is not adequately explained. The paper's writing also requires significant improvement, as many details are missing, such as the specific manner in which dropout is applied to the MFA.