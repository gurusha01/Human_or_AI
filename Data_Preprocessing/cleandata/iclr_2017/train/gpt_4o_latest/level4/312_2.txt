This paper explores the search for optimal neural network architectures within the actor-critic framework. The approach models a deep neural network (DNN) as a variable-length sequence and leverages reinforcement learning (RL) to identify the target architecture, which functions as the actor. In this context, node selection constitutes an action, while the evaluation error of the resulting architecture serves as the reward. The method employs an auto-regressive two-layer LSTM as both the controller and critic. The proposed approach is tested on two distinct problems and benchmarked against several human-designed architectures.
This is a highly compelling paper! Manually designing architectures is a challenging task, and it is often unclear how close hand-crafted networks come to achieving optimal results. The proposed method is innovative, and the authors provide a thorough and detailed explanation, including all the enhancements made to refine the approach. The datasets used for evaluation effectively demonstrate the method's capabilities. It is particularly fascinating to observe the differences between the generated architectures and those designed by humans. The paper is exceptionally well-written and highly accessible. Additionally, the discussion of related work is comprehensive and provides a clear contrast with prior literature.
It would be valuable to include data on the time required for training and to examine the correlation between the time or resources needed for training and the resulting model quality. Furthermore, exploring how human-initialized models perform and evolve over time would be an interesting avenue for further analysis.
In summary, this is an excellent and thought-provoking paper.