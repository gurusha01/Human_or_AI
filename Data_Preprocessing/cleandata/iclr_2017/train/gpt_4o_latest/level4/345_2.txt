The authors introduce a novel method for compressing neural networks by retraining them with a mixture of Gaussians prior applied to the weights. The means and variances of the mixture components are learned during training, and the network is subsequently compressed by setting all weights to the mean of their inferred mixture component (which may lead to a loss of precision). The compressed network is stored in a format that saves only the mixture index and leverages the sparsity enforced during training.
Quality:
A significant limitation of the proposed method is its apparent inability to work on VGG, which limits its applicability in production settings (though this might be addressable with further improvements). Additionally, it seems that processing AlexNet might be too time-consuming, which further restricts the method's practical utility. However, if these issues are resolved, the method could become a valuable contribution.
In Figure 2, two observations stand out. First, on the left side, there are numerous points with improved accuracy, which is not observed for LeNet5-Caffe. Do the authors have any intuition for this discrepancy? Second, regarding the Spearmint optimization, have the authors identified any patterns or insights into which hyperparameter settings were effective? Sharing such findings could be beneficial for others attempting to apply this method. 
Lastly, I appreciate the improvements made to Figure 7 in its latest version.
Clarity:
Section 2 on MDL is particularly well-written and provides a solid theoretical foundation. While Sections 4, 5, and 6 are concise and include the key details, it might be helpful to expand on the models used in the experiments, such as specifying the number of layers and parameters.
In Section 6.1, the authors state, "Even though most variances seem to be reasonably small, there are some that are large." However, this is difficult to verify from Figure 1, as the vertical histogram predominantly shows the zero component. A log histogram or separate histograms for each component might make this clearer. Additionally, in Figure 2, what do the larger points represent compared to the smaller ones? Do they indicate a better compression-to-accuracy loss ratio?
Some additional points for clarification are listed below.
Originality:
While prior work has explored compressing neural networks by reducing the number of bits used to store parameters and exploiting sparsity, the proposed approach of directly learning the quantization through a Gaussian mixture prior during retraining is a more principled and innovative method.
Significance:
The method demonstrates state-of-the-art performance on the two examples provided for MNIST. However, these networks are relatively shallow compared to the deep architectures used in modern state-of-the-art models. This limits the practical significance of the method. If the approach could be extended to work effectively on deeper networks like VGG or ResNet, it would represent a highly significant contribution.
Minor Issues:
- Page 1: There appears to be an extra space before the first author's name.
- Page 3: "in this scenario, pi_0 may be fixed..." appears to have a missing backslash in the TeX code.
- Page 6, Section 6.2: There are two incorrect spaces in "the number of components, \tau."
- Page 6, Section 6.3: "in experiences with VGG" should likely be "in experiments with VGG."
- Page 12: "Figure C" should likely be "Figure 7."