This paper tackles the challenge of enabling networks to dynamically adjust the number of units during training. The proposed method achieves this in a straightforward yet elegant and well-justified manner. Specifically, units with zero input or output weights are added or removed during training, while a group sparsity norm is employed for regularization to encourage unit weights to approach zero. The primary theoretical contribution demonstrates that, with appropriate regularization, the loss is minimized by a network with a finite number of units. However, in practice, this result does not guarantee that the resulting network will avoid overfitting or underfitting the training data. Nonetheless, preliminary experiments suggest that these issues do not arise in the tested scenarios.
One potential benefit of methods that adaptively determine the number of units in a network is the reduction of the hyperparameter tuning burden. However, a drawback of this approach (and perhaps similar ones) is that it does not fully resolve this issue. The network still relies on several hyperparameters that indirectly influence the number of units, such as those governing how frequently new units are added and how quickly weights decay to zero. It remains unclear whether these hyperparameters are easier or harder to tune compared to those in conventional methods. To their credit, the authors do not claim to have simplified the training process, but it is somewhat disappointing that this benefit is not realized.
The authors highlight that their approach enables the training of networks with fewer units while achieving performance comparable to parametrically trained networks. This is a potentially significant advantage, as smaller networks can reduce runtime, power consumption, and memory usageâ€”critical factors for mobile devices. However, the paper does not experimentally compare the proposed method to existing techniques for reducing the size of parametrically trained networks (e.g., pruning). As a result, it is unclear whether this approach is competitive with state-of-the-art methods for network size reduction.
Another limitation of the proposed method is that the same hyperparameters influence both the number of units in the network and the training time. Consequently, training could be significantly slower compared to parametric approaches with fixed hyperparameters. While parametric methods often require computationally expensive techniques like grid search for hyperparameter selection, prior experience with similar problems can sometimes simplify this process. Thus, the cost of grid search is not always incurred, whereas the slower training time of the proposed approach may be an inherent limitation. The authors do not address how this issue scales for much larger networks, raising concerns about the practicality of this method for large-scale applications due to potentially prohibitive training times.
Overall, the experiments presented are informative and encouraging but lack comprehensiveness and fail to fully convince. Additional experiments on significantly larger problems would be necessary to establish the practicality and broader applicability of this approach.
In summary, this is an interesting and well-written paper that presents a potentially valuable contribution. The overarching vision of creating networks capable of growth and adaptation through lifelong learning is compelling, and this work represents a step toward realizing that vision. However, the current findings remain largely speculative.