Although a substantial reduction in trainable parameters is achievable, regrettably, this does not translate to a corresponding decrease in the complexity of the training and speech recognition processes. 
The results, as presented, indicate that the authors were unable to achieve improved performance with a reduced parameter set. 
However, it is noteworthy that the proposed architecture, even with an increased number of parameters, demonstrates significant improvements, particularly in the language model.
To enhance clarity, the paper would benefit from reorganization and condensation, as certain sections can be challenging to follow and exhibit inconsistencies. 
For instance, the weights of the feedforward network are determined solely by an embedding vector, which raises concerns regarding linear bottlenecks, whereas in the recurrent network, the generated weights are influenced by both the input observation and its hidden representation.
It would be helpful if the authors could provide the number of trainable parameters corresponding to Table 6. 
Limiting the number of results presented could also contribute to improved readability. 
Due to the writing style, I can only marginally recommend acceptance.