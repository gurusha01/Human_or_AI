This manuscript utilizes Taylor series expansions of neural networks to distinguish between convex and non-convex components of the optimization problem, thereby facilitating the establishment of bounds on the training error in terms of the Taylor optimum and regret, as formally stated in Theorem 2. This theoretical contribution has notable implications for widely-used deep neural networks. Furthermore, the empirical evaluations presented in the paper provide supporting evidence for the theoretical findings, thereby validating the approach.