This paper provides a timely reality check on the trend of utilizing chatbots for various human-computer interactions, which gained significant attention in 2016 with claims of effectively handling complex dialogs. The work presented here is highly significant, not only for its relevance to Dialog and Natural Language Processing venues, where it can educate software engineers about the limitations of current chatbots, but also for Machine Learning venues, as it highlights the need for more realistic validation of machine learning models applied to dialogs.
Two underlying conjectures, explicitly stated by Antoine Bordes during a NIPS workshop presentation, are likely to spark further research. These conjectures, considered in the context of the metrics chosen in this paper, suggest that:
1. The performance of end-to-end machine learning approaches remains insufficient for goal-oriented dialogs.
2. The relative performance of algorithms on synthetic data can be a good predictor of their performance on natural data, a finding that departs from previous observations. The authors have made a concerted effort to ensure that the synthetic and natural conditions are closely matched.
Although the original algorithmic contribution is relatively straightforward, involving a simple addition to memory networks (match type), its deployment and testing on a goal-oriented dialog represent a novel approach. The experimental protocol is rigorous, and the paper's clarity is exceptional, making it accessible to a broad readership beyond machine learning and dialog researchers. The appendix on memory networks and the accompanying tables, which elucidate the influence of the number of hops, are particularly noteworthy.
This paper represents the state-of-the-art in exploring more rigorous metrics for dialog modeling. However, it also underscores the brittleness and arbitrariness of these metrics, a point more pertinent to future research directions than to revisions of the current work. The use of per-response accuracy, essentially the classification of the next utterance among a fixed list of responses, is highlighted as potentially misleading, as evidenced by Table 3. This table demonstrates that achieving a correct API call and a reasonably short dialog, despite requiring exact bot responses, would only yield a 1/7 accuracy.
The alternative metric of per-dialog accuracy, where all responses must be correct, is shown to be sensitive to the experimental protocol, as illustrated by Table 2. The significant difference in accuracy between subtask T3 (0.0) and the full dialog T5 (19.7) is attributed to the specific task definitions, with T3 requiring the display of three options and T5 only one.
For the concierge data, considering 'correct' as being among the best rather than the 5-best responses could potentially offer a more nuanced evaluation. While the authors' use of standard dialog metrics is understandable, and their proposal of new, albeit pessimistic, metrics is commendable, there is room for developing metrics that more meaningfully capture the essence of goal-oriented dialogs. 
One potential approach could involve representing dialogs in a way that maximizes revenue, such as in a scenario where Virtual Assistants are sold as a service, with payment upon successful dialog completion. In this context, a suitable metric might be a weighted sum of errors in the API call, the number of turns to reach the API call, and the number of rejected options by the user. However, measuring such a loss would require interaction with a real human user or a realistic simulator, as it cannot be accurately assessed with canned dialogs.
Another critical issue, closely related to representation learning, which this paper does not adequately address, is the scenario where the user's vocabulary does not exactly match the vocabulary in the knowledge base. Specifically, for the match type algorithm to effectively code 'Indian' as 'type of cuisine', the word 'Indian' must occur exactly in the knowledge base. This oversight highlights the need for machine learning models to learn associations between terms, especially in cases where the knowledge base uses obfuscated terminology, rather than relying on manual descriptions.