This paper presents a straightforward count-based exploration method for high-dimensional reinforcement learning applications, such as Atari Games, which relies on state hashing to implicitly group similar states together. The hashing process can be performed using either hand-designed features or learned features obtained through unsupervised training with an auto-encoder. A bonus, similar to that in Upper Confidence Bound (UCB), is assigned to new states to facilitate further exploration.
The paper is well-structured and backed by comprehensive experiments. However, its generalizability to a broader range of Atari games, particularly those with distinct scenarios like Montezuma's Revenge, which may benefit from state clustering approaches, warrants further investigation. Conversely, the effectiveness of such methods in fully continuous state spaces, as seen in RLLab experiments, is uncertain.
One aspect that remains unclear is the necessity of updating the hash code during training, which I speculate is required for adaptability and achieving lower reconstruction errors in the initial stages of training for a specific game, with stabilization becoming more critical thereafter. Section 2.3, focusing on learned embeddings, is pivotal but confusing; an algorithmic illustration would greatly enhance clarity in future revisions, addressing a crucial point that the authors failed to elucidate in response to my inquiry.