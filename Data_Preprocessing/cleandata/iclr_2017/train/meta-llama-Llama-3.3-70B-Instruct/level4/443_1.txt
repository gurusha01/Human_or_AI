This paper presents a novel approach utilizing an SSNT model to represent p(x|y), enabling a noisy channel model for conditional generation that supports incremental generation of y. The authors also introduce an approximate search strategy for decoding and conduct a thorough empirical evaluation.
Strengths: The paper is well-structured and clearly written, with the SSNT model being a notable contribution, and its application in this context is well-justified. The empirical evaluation is comprehensive and yields promising results.
Weaknesses: A potential concern is whether the added complexity in training and decoding is justified. Alternatively, the benefits of the proposed method might be achievable by reranking the outputs of a standard seq2seq model using a score that combines p(y|x), p(x|y), and p(y), as demonstrated by Li et al. (NAACL 2016) in conversation modeling. However, the ability to rerank during search could be beneficial, and experiments exploring this aspect would be valuable.
Additional Feedback:
- To improve clarity, Section 2 could be reoriented to focus on modeling p(x|y) rather than switching between p(y|x) and p(x|y) as in the original Yu et al. paper.
- Initially, the proposal of a noisy-channel model to address the "explaining away" problem seems counterintuitive due to the introduction of an explicit, uncalibrated p(y) term. Nevertheless, incorporating an explicit p(x|y) term appears to be a clever strategy, given that seq2seq models often perform significant target-side language modeling.