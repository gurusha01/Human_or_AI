This paper presents a novel approach that integrates policy gradient and Q-Learning methods, demonstrating improved learning capabilities, particularly in the Atari Learning Environment. The core concept revolves around recognizing that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, effectively linking policy and Q functions. This linkage enables the application of both policy gradient and Q-Learning updates, as embodied in the proposed PGQ algorithm.
The paper is deemed highly interesting, not only for its results and the introduction of the PGQ algorithm but also for the insightful connections it establishes between various techniques. However, the clarity of these connections could be improved. Despite the underlying concepts not being overly complex, the presentation was found to be somewhat challenging to follow, requiring a significant amount of time to fully comprehend.
Several areas were identified where clarity and detail could be enhanced:
- The introduction of the notation \tilde{Q}^pi lacks precision, initially described as "an estimate of the Q-values," while equation 5 presents it as an exact equality without estimation.
- The purpose and contribution of section 3.2 were not entirely clear, raising the question of whether it could be omitted to allow for the expansion of other sections with more detailed explanations.
- The connections to dueling networks, as discussed by Wang et al. in 2016, were found to be insufficiently explicit and detailed, particularly in sections 3.3 and 4.1. Given the similarity between the proposed architecture and dueling networks, a more comprehensive discussion would be beneficial. Additionally, acknowledging that PGQ can be viewed as combining Q-Learning with n-step expected Sarsa using a dueling network could facilitate a deeper understanding of the methodological links.
- Section 3.3, while drawing interesting connections, was difficult to follow due to its complexity and the abrupt introduction of certain concepts, such as the "mu" distribution. Clarification on the relationship between sections 3.2 and 3.3, particularly regarding the critic's output (Q values in 3.2 and V in 3.3), would be helpful.
Improving the paper's readability and addressing the points raised in pre-review questions, especially those concerning experimental details, the derivation of equation 4, and the issue of the discounted distribution of states, would significantly enhance the manuscript.
Minor observations include:
- The function r(s, a) in the Bellman equation of section 2 is not formally defined, leading to confusion regarding its dependence on the state and action.
- The definition of the Boltzmann policy at the end of section 2.1 is somewhat confusing due to the sum over "a" of a quantity that does not clearly depend on "a".
- It is not explicitly stated that section 4.3 pertains to the tabular case.
- The reason for the three algorithms not converging to the same policy in the simple toy setting of Figure 1 is not clear.
Typos and minor errors were also noted:
- The reference to Sutton & Barto (1998) is missing the word "by".
- "Online policy gradient typically require an estimate of the action-values function" should be "requires" and "value".
- "the agent generates experience from interacting the environment" should be "with the environment".
- A comma needs to be removed near the end of the first line of equation 12, just before "dlog pi".
- "allowing us the spread the influence of a reward" should be "to spread".
- "in the off-policy case tabular case" should be "in the tabular case".
- "The green line is Q-learning where at the step an update is performed" should be "at each step".
- Figure 2 incorrectly labels A2C instead of A3C.
The appendix A was not thoroughly reviewed due to time constraints. Initially, it was assumed that the distribution d^pi used a discounted weighting of states for the policy gradient theorem to hold, as per Sutton et al. (1999). However, it appears that no such discounting is applied in practice, which could potentially be problematic for Deep Q-Networks (DQNs), depending on the value of gamma used.
Clarifications are requested on the following points:
1. Is alpha kept fixed in experiments, and are results reported for the "exploratory" policy rather than the greedy one? If so, what is the rationale behind this choice?
2. How is the critic estimate of Q(s, a) obtained in both the grid world and Atari environments? Is it by summing the discounted observed rewards for up to t_max timesteps after taking action a in state s, plus the discounted estimated V of the last observed state, similar to A3C?
3. Are the target network frozen and the error clipped during the Q-learning step, as in the Nature DQN paper? If not, what is the justification for the alternative approach?
Furthermore, in section 3.2, the case is considered where "the measure in the expectation is independent of theta". However, in practice, both state and action distributions depend on theta. Commentary on how this affects the proposed interpretation would be valuable and should be included in the paper to enhance clarity and accuracy.