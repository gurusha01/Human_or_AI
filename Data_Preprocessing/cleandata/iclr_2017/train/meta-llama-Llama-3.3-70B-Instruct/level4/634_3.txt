This paper introduces a novel approach to visualizing the features and hidden units of a neural network, as well as generating adversarial examples, by performing gradient descent in the pixel space from a given hidden unit in any layer. This can be achieved by either selecting a pair of images and using the difference in unit activations as the objective for gradient descent or using the activation of the unit for a specific image. Overall, the method appears promising, and the following comments are provided:
Certain statements in Section 4.1 regarding the relationship between positive/negative signs and class changes are unclear and lack mathematical justification. The contradictory evidence from MNIST and face datasets supports this skepticism.
The authors rely on the PASS score throughout the paper, citing it with an intuitive explanation. However, a brief description of its actual function would be beneficial.
The correlation between the PASS score and metrics such as L2, Linfinity, or visual estimation of adversarial example quality is incomplete. The significance of these numerical results is unclear.
The claim in Section 5.2 that "LOTS cannot produce high-quality adversarial examples at lower layers" seems inaccurate for the MNIST dataset.
A major limitation of the paper is the lack of quantitative results, such as extracting adversarial examples at various layers, incorporating them into the training set, and comparing network performance on the test set. Additionally, the absence of comparisons with other methods makes it challenging to assess the merits of this work.
-----
EDIT after rebuttal: The authors' response to concerns regarding experimental validation has enhanced the paper's interest, prompting a revision of the score accordingly.