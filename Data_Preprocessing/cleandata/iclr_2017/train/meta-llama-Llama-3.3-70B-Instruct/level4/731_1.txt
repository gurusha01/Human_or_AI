This paper proposes an enhancement to the PV-DBOW and PV-DM document embedding methods introduced by Le & Mikolov in 2014. The key innovation is the incorporation of a binary encoding level, which involves inserting a sigmoid function with trainable parameters after the standard training stage of the embedding. This binary encoding enables the generation of a binary vector for each document, allowing for compact storage and efficient comparison of documents.
The binary vector is obtained by applying the sigmoid function to the embedding vector components, forcing the output to be binary. This binary representation is shown to outperform the Semantic hashing method proposed by Salakhutdinov & Hinton in 2009. The experimental approach is sound, as it compares the proposed method to the existing one using the same setup, while also demonstrating the benefits of combining the new representation with a Restricted Boltzmann Machine (RBM).
The advantages of this method include the improved performance of the binary representation and the sound experimental approach. However, there are some drawbacks, such as the incremental nature of inserting the sigmoid function to produce binary codes, which is not a novel contribution. Additionally, the explanation provided is too abstract and difficult to follow for non-experts. A comparison with efficient indexing methods used in image retrieval, such as the Inverted multi-index, is also missing.
A more detailed examination of the paper reveals several areas for improvement. The motivation for producing binary codes is not clearly stated, and the experimental section could benefit from including timings and memory usage numbers to demonstrate the benefits of binary embeddings. The figures could be improved by including more information about the model parameters, training objective, and characteristic sizes. Specifically, Figure 2 could be clarified by explaining why the "embedding lookup" and "linear projection" cannot be merged into a single smaller lookup table.
The text also contains some unclear statements, such as the claim that the length of binary codes is not tied to the dimensionality of word embeddings, without providing a clear explanation. The experimental setup is based on the one used by Salakhutdinov & Hinton in 2009, but this is not explicitly stated, and it is unclear whether there are any differences between the two setups. The comparison of binary codes using Hamming distances could be explicitly stated, and the statement about binary codes performing well despite their lower capacity could be clarified to indicate that this refers to their smaller size compared to real vectors.
Some suggestions for improvement include dropping Figure 5 if space is limited, specifying the comparison method used for the 300D real vectors, and providing more information about the raw performance of the large embedding vectors without pre-filtering with binary codes. Additionally, the concept of "transferring" from Wikipedia to other domains could be clarified, as Wikipedia's purpose is to cover all topics and lexical domains. Overall, while the paper proposes an interesting enhancement to document embedding methods, there are areas for improvement in terms of clarity, comparison to existing methods, and experimental evaluation.