This manuscript investigates various memory-augmented architectures, including key, key-value, and key-predict-value models, as well as simpler near memory-less RNN architectures. The incorporation of an attention model that leverages multiple decompositions is a noteworthy concept that warrants further exploration, potentially in alternative tasks where its strengths could be more pronounced. The experimental results on the Wikipedia corpus are intriguing, showcasing a diverse range of model types, which is a key area where the proposed models excel. In contrast, the same models evaluated on the CBT dataset yield comparable but less compelling demonstrations of the differences between the models.
The authors have made their Wikipedia corpus publicly available, which is a valuable and interesting contribution. Upon examining the corpus, it appears that a model capable of handling longer-term dependencies could potentially achieve better performance on this dataset. For instance, the first article in train.txt discusses "George Abbot", with the next mention of "Abbot" occurring 40 tokens later, followed by another mention 15 tokens thereafter, resulting in gaps of dozens of timesteps between occurrences of "Abbot". Conducting an analysis based on readily available information, such as token reoccurrence or average sentence length, may provide a useful approximation for determining the optimal attention window size.
Overall, this paper is well-written, poses interesting questions about the span lengths used in existing language modeling approaches, and serves as a potential foundation for future research directions.