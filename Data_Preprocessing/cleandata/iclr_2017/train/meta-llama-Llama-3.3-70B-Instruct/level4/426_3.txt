This manuscript builds upon existing research to establish a correspondence between the word embedding spaces of two languages, where the embeddings were initially trained on separate monolingual datasets, and leverages various types of bilingual information to learn this mapping, which is subsequently utilized to evaluate the accuracy of translations.
The authors introduce two novel components: "CCA" and "inverted softmax". An examination of Table 1 reveals that CCA outperforms Dina et al in only one out of six scenarios (It/En @1), with the majority of improvements attributed to the incorporation of inverted softmax normalization.
Upon reviewing the paper, it is unclear what specific aspect constitutes a genuine innovation. The authors mention that:
- Faruqui & Dyer (2014) previously employed CCA and dimensionality reduction
- Xing et al (2015) argued that Mikolov's linear matrix should be orthogonal
It would be beneficial to clarify how the current work differs from Faruqui & Dyer (2014), beyond the application of the method to measure translation precision.
The use of cognates instead of a bilingual dictionary is an interesting approach. However, it would be helpful to explain the methodology used to obtain the list of cognates. This approach appears to be limited to languages sharing the same alphabet, potentially excluding languages such as Greek and Russian.
Furthermore, in linguistic contexts, the term "cognate" typically refers to words with a common etymological origin, which may not necessarily share the same written form (e.g., night, nuit, noche, Nacht). It is possible that the authors are referring to proper names in news texts, and an alternative term might be more suitable.