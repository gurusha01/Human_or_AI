This study presents a novel approach to jointly classify images and audio captions for discovering acoustic units that correspond to visually semantic objects, enabling a more comprehensive data representation by regularizing visual and audio signals. This research direction is intriguing, as it facilitates the training of visual models from video data, among other applications.
However, a significant concern is the limited novelty of this work compared to the authors' previous publication at NIPS 2016. Although the authors propose a more advanced architecture, which yields an improvement in recall, the gains are modest, and the added architectural complexity seems somewhat arbitrary. The clustering and grouping methods in Section 4 appear to be simplistic, relying on image gridding and k-means clustering. A more sophisticated approach would involve utilizing object detectors, such as SSD, Yolo, or FasterRCNN, to generate accurate object proposals, and employing spectral clustering to relax the Gaussian assumption of the distributions. Furthermore, assigning visual hypotheses to acoustic segments could be improved by using bi-partite matching techniques.
In conclusion, while this research direction is promising and worthy of further exploration, the current work lacks sufficient novelty compared to the authors' previous publication. Nevertheless, the authors are encouraged to continue developing algorithms that can leverage multimodal datasets, with suggestions for improvement in methodology and approach to enhance the overall impact of their research.