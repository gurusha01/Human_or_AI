This paper introduces relation networks to capture pairwise interactions between objects within a visual scene. 
The proposed architecture is straightforward, employing a shared-weight MLP to process each object pair, followed by a predictive MLP that aggregates non-linear functions of these pairs. 
The experimental evaluation relies on a synthetic dataset tailored to the paper's hand-crafted architecture.
However, the paper's title overpromises, as it fails to deliver on the discovery of objects and their relations. Instead, it relies on hand-coded ground truth attributes for each object and only identifies a limited set of trivial relationships, such as relative positioning.
The discovery of objects and their relationships is a long-standing challenge in computer vision, with a substantial body of literature on contextual models that this paper neglects to cite or compare to.
It remains unclear whether the proposed architecture can enhance object detection or scene classification, particularly in the presence of noise, such as missing detections, inaccurate estimates, or complex textures. Furthermore, its effectiveness when object attributes are estimated from real images is untested.
More convincing results could be obtained by conducting experiments on real-world scenes, such as indoor datasets like NYUv2, Sun-RGB-D, SceneNN, or Chen et al's CVPR 14 dataset, and outdoor datasets like KITTI, which could serve as a benchmark for relationships between cars, pedestrians, and cyclists.
Without demonstrating its applicability to real-world scenarios, this paper addresses an overly simplistic problem with a model that does not significantly advance beyond existing context models, which also capture pairwise relationships between objects using techniques like MRFs or deep networks.