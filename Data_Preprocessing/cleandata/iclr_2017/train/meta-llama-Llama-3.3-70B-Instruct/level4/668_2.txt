This paper proposes a regularization approach for the estimator of a probability distribution, favoring high-entropy distributions to mitigate overfitting. I find this concept appealing, as it focuses on regularizing the model's behavior rather than its parameters, which can be more interpretable. By doing so, it allows for a more informed prior selection over the behavior, prioritizing parameters that collectively lead to a plausible or low-risk distribution.
The idea presented is sound and natural, and it is possible that its application in neural networks has not been thoroughly explored. The experimental results appear promising, suggesting that this type of regularizer could be widely adopted.
However, the paper's presentation as a novel, disconnected concept is misleading, as numerous machine learning papers have incorporated scaled entropy terms in their optimization objectives, extending beyond reinforcement learning. A more comprehensive review of related work is necessary.
Furthermore, the experimental results lack accompanying significance tests and error analysis, raising questions about the model's actual performance on the test data distribution, the robustness of improvements across multiple training sets, and the types of errors introduced or fixed by the model.
In summary, I recommend revision and resubmission. Given the high volume of ICLR submissions, I would like to recognize authors who not only attempt new ideas but also contextualize and carefully evaluate them. This approach helps maintain the quality of the literature, avoiding a proliferation of hastily prepared papers with unclear relationships, which can confuse readers.