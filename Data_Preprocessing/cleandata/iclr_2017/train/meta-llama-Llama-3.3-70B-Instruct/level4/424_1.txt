This paper explores the connection between stochastically perturbing model parameters during training and utilizing a smoothed objective function for optimization. Although the explanation of the weak gradient g in Eqs. 4-7 is unclear, Eq. 8 is straightforward, and Section 2.3 effectively demonstrates the equivalence between minimizing the smoothed loss and training with Gaussian parameter noise for a specific class of smoothing functions.
The authors extend this concept by introducing generalized smoothing functions to achieve a more complex annealing effect, which can be applied to contemporary neural network architectures, such as deep ReLU networks and LSTM recurrent networks. However, the resulting annealing effect, as shown in Section 4, can be counterintuitive, where the Binomial (or Bernoulli) parameter increases from 0 (deterministic identity layers) to 1 (deterministic ReLU layers), indicating that the network initially introduces noise, potentially reversing the annealing effect.
The annealing schemes employed in practice appear to be highly engineered, as exemplified by Algorithm 1, which involves a 9-step process to determine unit activation at each layer. Given the conceptual nature of the authors' contribution, which uniquely applies the smoothing framework to annealing, it would have been beneficial to dedicate a section of the paper to analyzing simpler models using basic smoothing functions. For instance, it would be informative to see examples where the perturbation schemes derived from the smoothing framework outperform standard, heuristically defined perturbation schemes in optimization, demonstrating the effectiveness of the proposed approach.