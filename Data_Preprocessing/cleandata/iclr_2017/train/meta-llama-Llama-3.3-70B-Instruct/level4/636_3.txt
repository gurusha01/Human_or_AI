This paper proposes a method to mitigate the input and activation variance caused by dropout in neural networks, and also introduces a useful inference technique involving the re-estimation of batch normalization parameters with dropout disabled prior to testing.
The authors effectively demonstrate the impact of dropout on input and activation variance, and subsequently adjust the initial weights to achieve unit variance, thereby preventing the explosion or vanishing of activation outputs. The approach is shown to be a viable initialization technique for deep networks, yielding performance comparable to or slightly surpassing existing methods. However, the limited experimental validation and marginal differences in accuracy compared to existing approaches make it challenging to assess the efficacy of the proposed method. Further support for the network's stability using the proposed approach could be garnered by examining the statistics of output activations and gradients over multiple training epochs.
The authors may want to consider validating the impact of backpropagation variance on their approach. Moreover, the comparisons drawn with batch normalization may not be entirely apt, as the latter encompasses more than just weight initialization. While the proposed approach is a promising initialization technique, its superiority over existing methods is not entirely clear, and additional evidence is needed to fully substantiate its benefits.