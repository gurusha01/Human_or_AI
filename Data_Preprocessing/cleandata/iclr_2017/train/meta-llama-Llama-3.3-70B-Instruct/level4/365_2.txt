This paper presents several methods for compressing fully-connected neural networks without significant performance degradation, including the introduction of a density-diversity penalty and its associated training algorithm. At its core, the technique involves explicitly penalizing the overall magnitude of the weights and the diversity between them, resulting in sparse weight matrices with a limited number of unique values. Although the authors propose a more efficient method for computing the gradient with respect to the diversity penalty, they still apply the penalty with a relatively low probability of 1-5% per mini-batch.
The approach demonstrates impressive compression capabilities for fully connected layers, with minimal accuracy loss. However, it is unclear whether the computational cost of sorting weights, even if only applied to a small fraction of mini-batches, may render this method impractical for larger networks. It is possible that the induced sparsity could help mitigate some of this cost.
A significant limitation of this paper is the multitude of components in the proposed approach that are not thoroughly investigated independently. These components include sparse initialization, weight tying, probabilistic application of the density-diversity penalty, setting the mode to 0, and an alternating schedule between weight-tied standard training and diversity penalty training. The authors do not provide sufficient discussion on the relative importance of each component, and the only quantitative metric presented is the compression rate, which is a function of both sparsity and diversity, making it difficult to compare them separately. It would be beneficial to see a detailed analysis of how each component affects diversity, sparsity, and overall compression.
To clarify, Section 3.1 states that the density-diversity penalty is applied with a fixed probability per batch, while Section 3.4 implies a structured phase alternation between the application of density-diversity penalty and weight-tied standard cross-entropy. Is the scheme in Section 3.4 applying the density-diversity penalty probabilistically only during the density-diversity phase?
Preliminary rating:
This paper is interesting, but it lacks sufficient empirical evaluation of its numerous components, giving the impression that the algorithm is a collection of tricks that ultimately achieve good performance without fully explaining its effectiveness.
Minor notes:
The equation in Section 4 should be resized to fit within the margins, which can be achieved using the \resizebox{\columnwidth}{!}{...} command in LaTeX.