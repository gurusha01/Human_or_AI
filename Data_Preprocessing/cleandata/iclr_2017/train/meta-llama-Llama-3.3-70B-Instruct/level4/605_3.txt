This manuscript presents a novel variational autoencoder extension tailored to accommodate arbitrary tree-structured outputs. The evaluation of its density modeling capabilities is undertaken through experiments on both a synthetic arithmetic expression dataset and a first-order logic proof clause dataset.
Strengths of the paper include:
+ The clarity and quality of the writing, which effectively conveys the ideas presented.
+ A comprehensive definition of tree structures that can encapsulate a broad range of tree types encountered in real-world applications.
+ An elegant and well-explained procedure for tree generation and encoding.
+ Although the experimental scope is limited, the experiments themselves are thorough, with a notable mention of the use of Importance Weighted Autoencoders (IWAE) to enhance the estimation of log likelihoods.
However, several limitations are observed:
- The performance improvement over a baseline sequential model is found to be minimal.
- The experimental scope is constrained, both in terms of the datasets considered and the evaluation metrics employed for comparison with other approaches. Specifically, it is noted that: (a) results from real-world datasets are scarce, with the proposed model underperforming the baseline in the sole instance, and (b) there is an absence of evaluation regarding the utility of the learned latent representation for tasks such as classification.
- The model's capability to generate trees in a time frame proportional to the tree depth is highlighted as a benefit, yet this claim lacks empirical validation within the experiments.
The tree generation and encoding procedures demonstrate cleverness through the repeated application of common operations. The incorporation of weight sharing and gating operations appears crucial for the model's performance, but their effectiveness remains difficult to assess without an ablation study (as modifications are not evaluated side-by-side in Tables 1 and 2). Conducting experiments in additional domains, such as source code modeling or parse trees conditioned on sentences, would be beneficial in showcasing the model's utility. While the model exhibits promise and applicability across various data types, the narrow scope of the experiments raises concerns.
Specific comments on the manuscript include:
* Section 3.1: The distinction between types should be revised from three to two.
* Section 3.6: The explanation of the variable-sized latent state is somewhat confusing due to the omission of details regarding the determination of the number of z's to generate.
* Sections 4.2-4.3: It is queried whether the disjointness of the test set from the training set was verified during dataset generation.
* Table 1: The absence of results for variable latent states in the case of depth 11 trees is noted, prompting inquiry into the reason behind this omission.