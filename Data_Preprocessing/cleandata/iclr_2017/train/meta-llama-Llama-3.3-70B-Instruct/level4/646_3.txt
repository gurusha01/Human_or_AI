This manuscript presents an interactive extension of the bAbI dataset, augmenting it with supplementary questions and answers to address instances where insufficient information is available to respond to a query. The concept of interactive question answering is indeed intriguing and well-justified in the paper. Nevertheless, the explanation of the bAbI extension falls short. For instance, the baseline DMN and MemN2N models for the interactive QA task are described as taking both statements and the question as input to estimate an answer, which fundamentally differs from and is more challenging than the approach used in the paper, as it does not differentiate between feedback and the original context. A more equitable comparison might involve treating each question, whether original or supporting, as a distinct instance. Furthermore, the methodology behind generating the supporting questions and user feedback is unclear, including the number of templates and words utilized. These details on dataset creation are lacking, and considering space constraints, it might be beneficial to condense or omit basic explanations of concepts like GRU and sentence encodings, instead referencing the original papers.
Another concern arises from the model's approach to generating synthetic questions. If the generation relies on just one or two templates, it seems more efficient to predict the template values directly rather than using an RNN decoder to generate entire questions. For example, predicting "which" or "which bedroom" could be sufficient instead of generating "Which bedroom, master one or guest one?" This approach appears to merely predict additional supporting facts rather than genuinely interacting with users. The fact that the model is applied to only three of the original twenty tasks further compromises the reliability of the conclusions drawn.
In summary, while the paper proposes a compelling idea with strong motivation, the experimental design and results are not persuasive enough to warrant acceptance at ICLR.