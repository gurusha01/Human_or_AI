The significance of preserving highly rewarding or hazardous states has been a subject of extensive research within the reinforcement learning (RL) community, and its importance cannot be overstated. Following the preliminary review comments, the authors have acknowledged comparing their approach to expected SARSA, a crucial baseline; however, a more comprehensive evaluation against additional baselines is necessary to fully assess the paper's contributions and merits acceptance.
Furthermore, recent studies have explored the utilization of reward replay buffers in deep RL agents, as seen in works by Jaderberg et al., Blundell et al., and Narasimhan et al., which demonstrate potential in reinforcing agents to avoid revisiting states associated with catastrophic outcomes. Incorporating such perspectives could enhance the robustness of the proposed method.
Upon closer examination, the presented approach appears to lack a strong theoretical foundation. Notably, the decision to employ a separate model for catastrophe detection, rather than directly incorporating catastrophe as a signal for the learner, raises questions about the methodology's design choices and overall efficacy.