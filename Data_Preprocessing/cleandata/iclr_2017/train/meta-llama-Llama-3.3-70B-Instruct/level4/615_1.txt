This paper introduces L-SR1, a novel second-order method for deep neural network training, purportedly addressing the critical optimization issues of Hessian poor conditioning and saddle point proliferation. The approach can be seen as combining the SR1 algorithm of Nocedal & Wright (2006) with limited-memory representations from Byrd et al. (1994). However, a more rigorous theoretical foundation is lacking, which is typically provided in works such as those by Dauphin (2014) or Martens, and additional intuition behind the method would be beneficial. The experimental results are unconvincing, as they omit wall-clock time performance and demonstrate only a marginal advantage over competing methods in terms of epochs. Although the authors may still be optimizing their implementation, the lack of compelling experimental evidence raises questions about the practical value of L-SR1 for deep model training, making the work premature for publication.