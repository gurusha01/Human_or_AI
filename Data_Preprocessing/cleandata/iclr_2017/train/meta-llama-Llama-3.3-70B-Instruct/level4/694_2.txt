This work reaffirms the significant advantages of end-to-end training using deep networks over conventional hybrid systems that rely on hand-designed features, yielding substantial improvements. The outcomes are particularly impressive for the limited vocabulary grammar task as defined by the GRID corpus, demonstrating exceptional engineering prowess. It would be intriguing to observe how this approach performs on large vocabulary language modeling tasks. A comparison with human lip-reading capabilities for conversational speech would also be a valuable addition. 
Traditional audio-visual automatic speech recognition (AV-ASR) systems, which employ weighted fusion of audio and visual posteriors, essentially revert to pure lip reading when the weight is entirely on the visual modality. Numerous experiments have illustrated the performance of this visual channel under low audio signal-to-noise ratio (SNR) conditions for both grammar and language model tasks.
It's also worth noting that traditional hybrid methods for AV-ASR have been trained at the sentence level using sequence training objectives such as fMPE, MPE, and MMI (as seen in earlier references). Therefore, it cannot be claimed that this study introduces the first sentence-level objective for lip-reading models, analogous to the misconception that sequence training did not exist in hybrid large vocabulary continuous speech recognition (LVCSR) systems prior to the introduction of Connectionist Temporal Classification (CTC).