The paper exhibits several notable strengths, including the ingenious application of Mixture of Experts (MoE) to enhance model capacity, thereby facilitating the training of large models on extensive datasets in a computationally efficient manner. Furthermore, the effective batch size for training the MoE has been significantly increased. The experimental results, which investigate the impact of augmenting the number of MoEs, are also noteworthy and align with expectations.
However, the paper has some weaknesses. One major limitation is that it does not provide a comprehensive comparison of different methods for expanding model capacity to leverage large datasets. A more in-depth discussion on the use of MoE versus alternative approaches, considering factors such as computational efficiency, would be beneficial to contextualize the contributions of this work.