This paper presents a straightforward randomized approach to pruning weights in ConvNets, aiming to reduce theoretical FLOPs in deep neural network evaluation. It offers a comprehensive taxonomy of pruning granularity, ranging from coarse (layer-wise) to fine (intra-kernel), and employs an empirical strategy that utilizes a validation set to select the optimal model from N randomly pruned models. However, the introduction's claims of being "one shot" and "near optimal" are unsubstantiated, as the method involves generating and testing N networks, lacking evidence or theoretical support for the optimality of the solution found.
The paper has several strengths, including:
- A well-organized taxonomy of pruning levels
- A comparative analysis with the recent weight-sum pruning method
Nevertheless, it also has several weaknesses:
- The experimental evaluation fails to include recent models (such as ResNets) and large-scale datasets (like ImageNet)
- The paper's structure and presentation can be challenging to follow
- While feature map pruning can accelerate computation without specialized sparse convolution implementations, finer-grained sparsity may not yield similar performance improvements, and the paper should provide evidence to support its consideration of fine-grained sparsity
Furthermore, the experimental evaluation overlooks the impact of filter pruning on transfer learning, which is a crucial aspect, as the value of learned representations lies in their ability to be transferred to other tasks. The paper misses an opportunity to explore this direction, leaving unanswered questions about the potential harm pruning could cause to transfer learning, even if the primary task's performance remains unaffected.
In conclusion, while the proposed method is simplistic, which can be beneficial, the experimental evaluation is incomplete, failing to cover recent models and larger-scale datasets, which limits the paper's overall impact.