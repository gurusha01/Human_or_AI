This manuscript examines the utilization of syntactic dependencies in unsupervised word representation learning models, such as CBOW and Skip-Gram, with an emphasis on comparing bound (word and dependency type, e.g., 'She-nsubj') and unbound (word only, e.g., 'She') context representations during training. The empirical findings are highly inconsistent, and no newly proposed method consistently surpasses existing approaches.
The paper is thorough and methodologically sound, with no significant concerns regarding its validity. Nevertheless, I question its relevance and appeal to the broader ICLR community. The research focuses on a relatively specialized aspect of representation learning that is unique to natural language processing, and the results are predominantly negative. In my opinion, a shorter presentation at a conference like ACL would be a more suitable platform for this work.