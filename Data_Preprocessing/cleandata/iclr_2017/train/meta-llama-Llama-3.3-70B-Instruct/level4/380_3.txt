This submission presents various approaches to augment the generator function in generative adversarial training with supplementary gradient information. Initially, it outlines a general framework for incorporating this additional gradient information, denoted as K(pgen), into the generative adversarial training objective function (Equation 1). The authors then demonstrate that the optimal discriminator's shape is indeed influenced by the added gradient information (Proposition 3.1), a somewhat expected outcome. Furthermore, they propose three specific methods for constructing K(pgen): the negative entropy of the generator distribution, the L2 norm of the generator distribution, and a constant function, which bears resemblance to the EBGAN objective introduced by Zhao et al. in 2016.
The discussion then transitions to an empirical evaluation of the method, where K(p_gen) is set to approximate the entropy of the generator distribution. At this juncture, it appears that the objective function under examination is essentially the standard GAN objective, supplemented by a regularization term that encourages diversity (high entropy) in the generator distribution. The authors' aspiration is that this regularization will transform the discriminator into an estimate of the energy landscape of the data distribution.
The empirical evaluation entails 1) presenting contour plots of the obtained generator distribution for a 2D problem, 2) investigating generation diversity in MNIST digits, and 3) showcasing samples for CIFAR-10 and CelebA. The results for the 2D problem are compelling, as the discriminator scores clearly translate to unnormalized values of the density function. The MNIST results also provide valuable insight: more prototypical digits are assigned larger scores (unnormalized densities) by the discriminator, while less prototypical digits receive smaller scores. However, the sample experiments in Section 5.3 are less convincing due to the absence of baseline models for comparison.
To further enhance the manuscript, I recommend that the authors address three key aspects. Firstly, while entropy regularization leads to a discriminator that estimates the energy landscape of the data distribution, it is essential to understand how this regularization reshapes the generator function. Visualizing the mean MNIST digit according to the generator, along with other statistics, would be beneficial. Secondly, a comparison of the samples produced by the proposed methods to state-of-the-art models, in terms of visual quality, is necessary. Thirdly, the authors should discuss the shortcomings of this method compared to vanilla GAN, including potential computational overhead, and provide a qualitative and quantitative analysis of the differences between the two proposed entropy estimators.
Overall, the paper is well-written, and I support its acceptance.
As an open question to the authors: What future breakthroughs should be pursued to derive a GAN objective where the discriminator serves as an estimate of the data density function after training?