This paper presents a method to decrease the size and evaluation time of deep CNN models on mobile devices by merging multiple layers into a single layer, followed by retraining of the modified model. The results demonstrate a computation time reduction of 3x to 5x, accompanied by a minimal accuracy loss of 0.4% for a specific model.
Minimizing model size and accelerating model evaluation are crucial in various applications. However, several concerns arise:
1. Numerous techniques exist for reducing model sizes, such as the teacher-student approach, which has been shown to achieve comparable or even superior accuracy to larger models using significantly smaller ones. Unfortunately, this paper does not provide a comparison with these existing methods.
2. The proposed technique has limited applicability, as it is tailored to the specific models discussed in the paper, restricting its broader utility.
3. The approach of replacing multiple layers with a single layer is not particularly novel, as certain layers, such as mean variance normalization and batch normalization, can be combined without requiring retraining or sacrificing accuracy.
It is worth noting that the concept of DNN low-rank approximation originated in speech recognition, as seen in earlier work, such as Xue et al. (2013), who applied singular value decomposition to restructure deep neural network acoustic models, as presented at INTERSPEECH.