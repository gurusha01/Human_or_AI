This work presents an intriguing approach to targeting saddle points in optimization using an SR1 update, with some promising initial experimental results. However, a key limitation is the lack of comparison to recent second-order optimization methods, including Adam and other Hessian-free techniques, such as those proposed by Martens in 2012, as well as Pearlmutter's method for fast exact Hessian multiplication. The provided MNIST and CIFAR curves do not clearly demonstrate an advantage over AdaDelta or NAG, contrary to the claims made. Furthermore, more extensive experimentation is necessary to substantiate the assertion of mini-batch insensitivity to performance. To strengthen the claims, it would be beneficial to report error rates on a larger-scale task to validate the approach's effectiveness.