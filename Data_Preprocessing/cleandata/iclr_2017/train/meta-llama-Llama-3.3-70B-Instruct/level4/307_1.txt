This paper, recently submitted to arXiv, introduces a novel, publicly available dataset and tasks designed for goal-oriented dialogue applications. The dataset and tasks are artificially constructed using rule-based programs, allowing for the evaluation of various aspects of dialogue system performance, including API calls, option display, and full-fledged dialogue interactions.
The contribution of this paper to the dialogue literature is noteworthy, as it has the potential to facilitate future research in the development and understanding of dialogue systems. However, there are potential drawbacks to this approach that need to be considered. Firstly, the suitability of Deep Learning models for these tasks, compared to traditional methods such as rule-based systems or shallow models, is unclear. This is because Deep Learning models typically require a large number of training examples, and differences in performance between neural networks may be attributed to regularization techniques rather than the models themselves. Additionally, tasks 1-5 are deterministic, which means that evaluating performance on these tasks may not accurately assess the ability of models to handle noisy and ambiguous interactions, such as inferring user goals or executing dialogue repair strategies, a crucial aspect of dialogue applications. Nevertheless, this direction of research is still considered interesting and worthy of exploration.
As mentioned in the comments, the paper lacks a baseline model that incorporates word order information. This is a significant weakness, as it creates an unfair comparison that makes neural networks appear overly strong, while simpler baselines may be competitive or even outperform the proposed neural networks. To ensure a fair evaluation and accurately assess the effectiveness of representation learning for this task, it is essential to include an additional non-neural network benchmark model that takes into account word order information. For instance, experimenting with a logistic regression model that utilizes word embeddings, bi-gram features, and match-type features could provide a more convincing demonstration of the utility of Deep Learning models for this task. If such a baseline is included, the rating would be increased to 8.
A minor comment regarding the conclusion: the statement "the existing work has no well-defined measures of performances" is not entirely accurate. End-to-end trainable models for task-oriented dialogue have well-established performance measures, as seen in "A Network-based End-to-End Trainable Task-oriented Dialogue System" by Wen et al. While non-goal-oriented dialogue systems can be more challenging to evaluate, they can still be assessed using human subjects, as demonstrated by Liu et al. (2016) for Twitter, and "Strategy and Policy Learning for Non-Task-Oriented Conversational Systems" by Yu et al.
Following the addition of new results to the paper, the score has been updated accordingly.