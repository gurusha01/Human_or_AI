This paper presents a straightforward and easily comprehensible finding that convolutional networks can serve as a viable alternative to recurrent encoders in neural machine translation. 
In addition to incorporating familiar architectural components, including convolution, pooling, residual connections, and position embeddings, the paper introduces an intriguing design variation: utilizing two convolutional stacks, one for alignment calculation and the other for representation computation. Although empirical evidence supports the necessity of this design choice, the underlying rationale behind its requirement remains unclear.
The experimental assessment is exhaustive and convincingly demonstrates the effectiveness of the proposed approach. Notably, the convnet-based model exhibits faster evaluation times, although the primary factor contributing to this speedup is not explicitly identified. Nevertheless, it is reasonable to assume that the speed advantage of convnets will become more pronounced with a more parallelized implementation.
My primary reservation concerns the paper's suitability for ICLR, as the contribution appears to be relatively incremental and narrowly focused on a specific application. In my opinion, conferences such as ACL, EMNLP, and other NLP-focused gatherings might be a more appropriate venue for this work.