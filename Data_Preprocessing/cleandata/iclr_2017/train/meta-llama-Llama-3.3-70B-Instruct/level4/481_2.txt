This paper examines the phenomenon of adversarial examples and adversarial training using the ImageNet dataset, yielding several notable findings despite vague final conclusions. The paper is well-structured and clear, making it easy to follow. Although some concerns are raised (detailed below), the paper presents valuable contributions, warranting publication.
Strengths:
The paper introduces the novel concept of 'label leaking', which, although only significant in large datasets, is a crucial consideration for future research in this area. The use of the 'clean accuracy' to 'adversarial accuracy' ratio as a robustness measure is more reasonable than existing approaches in the literature.
Weaknesses:
The paper's title appears somewhat misleading, as the conclusions are drawn from experiments on ImageNet. Section 4 is considered the primary contribution, but subsections 4.3 and 4.4 are not exclusive to large-scale datasets, making the emphasis on 'large-scale' in the title and introduction seem unwarranted. The conclusions are largely based on observational results from experiments, and additional testing is necessary to validate these hypotheses. Without such verification, the conclusions seem premature, as findings from one ImageNet dataset cannot be generalized to all large-scale datasets.