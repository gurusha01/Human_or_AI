Update after reviewing the authors' revised paper dated Dec 21 and their responses:
I have reevaluated my initial assessment and revised my score from 3 to 5, also removing the comment regarding insufficient comparison to prior work from the title.
The primary reason for this score revision lies in the novelty of the proposal. The introduction of HGRU and the utilization of the R matrix essentially boils down to determining whether to proceed from character-level states or utilize word-level states. Although these solutions appear to be tailored to symbolic frameworks such as Theano and TensorFlow, this does not pose an issue for languages like Matlab or Torch.
-----
This paper is well-structured, with a thorough analysis, and I particularly appreciate Figure 5. However, upon closer examination, I believe that the work presented lacks substantial novelty. The title suggests a focus on learning morphology, yet the model does not incorporate explicit mechanisms for learning morphemes or subword units. For instance, constraints could be imposed on the weights in w_i in Figure 1 to identify morpheme boundaries, or an additional objective like MDL could be employed, although it is unclear how these constraints can be seamlessly integrated.
Furthermore, I am surprised by the limited comparison to the work of Luong and Manning (2016) [1], which trains deep 8-layer word-character models and achieves significantly better results on English-Czech, with a BLEU score of 19.6 compared to the 17.0 BLEU score reported in this paper. The HGRU concept appears to be overly complicated in terms of presentation. If I understand correctly, HGRU essentially determines whether to continue the character decoder or reset using word-level states at boundaries, which is similar to the approach taken in [1]. Luong and Manning (2016) also improve efficiency by avoiding the need to decode all target words at the morpheme level, and it would be beneficial to know the speed of the model proposed in this ICLR submission. The novel contributions of this paper may lie in the different analyses of what a character-based model learns and the addition of an extra RNN layer in the encoder.
One minor suggestion: consider annotating h_t in Figure 1.
[1] Minh-Thang Luong and Christopher D. Manning. 2016. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. ACL.