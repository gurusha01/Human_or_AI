This manuscript presents a novel simulator and a suite of synthetic question answering tasks that facilitate interaction with a "teacher" through questioning, with the goal of enhancing an intelligent agent's performance via user feedback. The study explores this concept in both offline supervised and online reinforcement learning environments, yielding results that demonstrate improved model performance through questioning.
The novelty of this idea, which has received relatively little attention in the research community, is a significant strength, and this paper provides a solid foundation for further exploration. Notably, the manuscript examines three distinct task types where user feedback can benefit the agent. The writing is clear and concise, offering a detailed description of the tasks, models, and experimental settings.
Additional considerations and inquiries include:
- The motivation behind utilizing both vanilla-MemN2N and Cont-MemN2N is unclear; does the use of both models contribute to the paper's overall contributions by yielding unique insights?
- In the Question Clarification setting, what is the distribution of misspelled words across question entities, answer entities, relation entities, or none of these? If misspelled words predominantly originate from relation entities, the problem may be less complex than initially suggested.
- The statement on Page 10, "The performance of TestModelAQ is worse than TestAQ but better than TestQA," is inconsistent with the results presented in Tables 2 and 4 for Task 2.
- How would the outcomes be affected by a smaller or nonexistent conversational history?
- In Figure 5, Task 6, the accuracy of the "good student" decreases when questioning ceases, despite already possessing relevant facts; this behavior is unexpected, as questioning should not provide additional information in this scenario.
- In Figure 5, Task 2, the "poor student" achieves approximately 70% accuracy without questioning, which seems unexpectedly high; can this result be explained?
- Figure 1, Task 2 AQ, contains an error in the last sentence, which should display a negative response "(-)" instead of the current positive indication.
Preliminary Evaluation:
This work represents a promising initial step in the direction of developing dialogue agents that learn from unstructured user interactions, laying a foundation for future research in this area.