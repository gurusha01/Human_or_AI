This manuscript proposes a methodology for representing text documents and paragraphs as compact binary codes, facilitating efficient similarity search and retrieval through the utilization of hashing techniques. The approach extends the real-valued paragraph vectors introduced by Le & Mikolov by incorporating a stochastic binary layer into the neural network architecture. The study compares two binarization methods for the final activations: (1) introducing noise to sigmoid activations to promote discretization, and (2) binarizing activations during the forward pass while maintaining real-valued representations during the backward pass, employing straight-through estimation. The results, obtained using straight-through estimation on the 20 newsgroup and RCV1 text datasets with 128 and 32-bit binary codes, are encouraging.
The application presented is noteworthy and significant, and the paper's exposition is clear and well-organized. However, from a machine learning perspective, the novelty of the approach is somewhat limited. The discussion of binary hashing literature beyond semantic hashing and Krizhevsky's binary autoencoders (2011) is lacking. A crucial baseline comparison, where real-valued paragraph vectors are first learned and then converted to binary codes using established hashing methods (such as random projection LSH by Charikar, BRE by Kulis & Darrell, ITQ by Gong & Lazebnik, and MLH by Norouzi & Fleet), is absent.
Given the limited novelty and the omission of this key baseline, I do not recommend the paper for publication in the ICLR conference proceedings in its current form. The paper may be more suitable for NLP conferences, as its focus is more applied.
Additional comments:
- From a practical standpoint, learning real-valued paragraph vectors and then quantizing them for indexing might be more straightforward. However, an end-to-end approach, as proposed, may yield better performance. An empirical comparison between the proposed end-to-end method and a simpler two-stage quantization approach would be valuable.
- The work by Bengio et al., "Estimating or Propagating Gradients Through Stochastic Neurons," discusses straight-through estimation and alternative methods, which may be relevant.
- The paper's argument that binary codes cannot exceed 32 bits due to their unsuitability for document hashing is not entirely accurate, particularly in light of multi-probe hashing mechanisms (e.g., "Multi-Index Hashing" by Norouzi et al.).
- The survey "Hashing for Similarity Search: A Survey" by Wang et al. provides an overview of related work on binary hashing and quantization, which seems to be overlooked in the manuscript.