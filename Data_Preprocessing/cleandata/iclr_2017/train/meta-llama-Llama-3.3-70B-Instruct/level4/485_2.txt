This paper explores the capability of neural networks to efficiently represent low-dimensional manifolds by embedding them into a lower-dimensional Euclidean space. The authors introduce a specific class of manifolds known as monotonic chains, which are composed of intersecting affine spaces separated by hyperplanes that define monotonic intervals of spaces. They propose a method to embed such chains using a neural network with a single hidden layer and provide a bound on the number of parameters required for this embedding. Additionally, they investigate the effects of noise on the manifold.
The experimental section involves embedding synthetic data from a monotonic chain using a distance preservation loss, which supports the theoretical bound on the number of parameters needed. Another experiment examines the embedding of faces, known to lie on a monotonic chain, by varying elevation and azimuth, using a regression loss.
The research direction undertaken in this paper, focusing on the behavior of manifolds within neural networks, is highly promising and warrants further exploration. However, the current manuscript requires additional refinement. Notably, the experiments are limited to regression loss and shallow networks, whereas the practical application of this research often involves large, high-dimensional datasets that necessitate deeper networks. Therefore, it is crucial to extend this work to deeper networks. Moreover, the efficacy of embedding when using a classification loss instead of regression loss should be investigated.
The theoretical sections would benefit from clearer exposition. As someone not deeply familiar with the literature in this area, I found the proof methods, although relatively straightforward, difficult to follow due to a lack of clarity on what was being proven. For instance, a formal statement of the expectations for an embedding that "accurately and efficiently" preserves a monotonic chain would enhance understanding. Clarifying these aspects would strengthen the manuscript.