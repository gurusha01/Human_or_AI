This study examines the continuous-time dynamics of gradient descent in training two-layer ReLU networks, comprising one input and one output layer, thereby only having one layer of ReLU units. The research is notable for not relying on unrealistic assumptions commonly found in similar previous works. Notably, it does not assume independence between the input and activations, nor does it employ noise injection, which can simplify analysis but may not reflect real-world scenarios. However, the removal of these simplifying assumptions restricts the analysis to several key limitations:
1. The network is confined to only one layer of nonlinear units.
2. The bias term in ReLU is discarded, while the input is assumed to be Gaussian, preventing the use of the constant input trick to simulate the bias term.
3. A strong assumption is imposed on the representation of the input/output relationship via bias-less ReLU networks, specifically the existence of orthonormal bases to represent these relationships.
Given these constraints, the paper presents novel analysis within this new framework, which is both interesting and valuable. For instance, by leveraging the symmetry in the problem under the third assumption mentioned, the authors successfully reduce the high-dimensional dynamics of gradient descent to a bivariate dynamics. This reduction to a two-dimensional space enables rigorous analysis of the dynamics' behavior, such as convergence to a saddle point in symmetric cases or to the optimum in non-symmetric cases.
A clarification is needed regarding the first paragraph of page 2, where it is stated that "Initialization can be arbitrarily close to origin," yet at the beginning of the same paragraph, it is mentioned that the initialization is "random with a standard deviation of order 1/sqrt(d)." These statements appear to be inconsistent.
Several minor comments on the draft are also noteworthy:
1. In Section 1, the second paragraph states, "We assume x is Gaussian and thus the network is bias-free." It would be more precise to specify "zero-mean Gaussian" in this context.
2. The term "standard deviation" is misspelled as "standard derivation" in multiple instances throughout the paper.
3. On page 6, in the last paragraph, the first line references Corollary 4.1, which should be corrected to Corollary 4.2.