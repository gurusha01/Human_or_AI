This paper introduces an extended GAN framework, referred to as GAP, which involves the parallel training of multiple generators and discriminators, with their pairings periodically shuffled. 
The strengths of this approach include:
+ Its simplicity and ease of replication, making it accessible for further research and implementation.
However, several weaknesses are noted:
- The paper's presentation is convoluted, hindering clear understanding.
- The results, while indicative, fail to definitively demonstrate a performance advantage of GAP over existing methods.
The central claim of the paper is that GAP enhances convergence and mode coverage. Although the visualizations of mode coverage are suggestive, they do not provide sufficient evidence to conclusively support the superiority of GAP in this regard. Similarly, assessing the impact of GAP on convergence based on learning curves proves challenging. The proposed GAM-II metric is problematic due to its circular dependency on the baseline models used for comparison. In contrast, estimating likelihood via Annealed Importance Sampling (AIS) and utilizing the Inception score appear to be more promising evaluation methods.
A more systematic approach to evaluating GAP's effectiveness could involve conducting a grid search over hyperparameters, training an equal number of standard GANs and GAP-GANs for each setting, and then analyzing the distribution of final Inception scores or likelihood estimates of the trained models. This would help clarify whether GAP consistently produces better models. While the approach shows promise, the current form of the paper leaves too many questions unanswered.
Specifically, 
* Section 2 contains a remark that seems more like a task to be addressed rather than a completed thought.
* Section A.1 lacks a detailed description of the proposed metric, which is necessary for a comprehensive understanding.