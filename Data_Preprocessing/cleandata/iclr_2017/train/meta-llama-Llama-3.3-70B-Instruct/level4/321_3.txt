I appreciate the context established in this paper, but I have several concerns and inquiries:
(1) The failure model underlying this work is not clearly defined. As the complexity of behaviors increases, I anticipate that this approach may struggle with discovering a diverse range of skills, and I would like to see a more detailed analysis of this potential limitation.
(2) In Section 5.3, the introduction of a random variable X to denote the agent's current grid position raises questions about the discretization of the space. Is the space indeed discretized, and if so, what is the rationale behind this decision? Furthermore, what are the implications if the space is not discretized?
(3) Building on my initial concern, I wonder whether this approach can be effectively applied to more complex embodiments, such as a 5-link swimmer, as opposed to the 2-link example presented. Evaluating the approach's performance in such scenarios is crucial to assessing its generality and robustness.
(4) The authors draw a comparison with the work of Heess et al. (2016), suggesting that their framework is distinct in its use of intrinsic rewards as the sole signal during pre-training, requiring only minimal domain knowledge. However, I disagree with this characterization, as the rewards proposed in this paper appear to be highly tailored to a specific set of control tasks, which may limit their applicability and versatility.