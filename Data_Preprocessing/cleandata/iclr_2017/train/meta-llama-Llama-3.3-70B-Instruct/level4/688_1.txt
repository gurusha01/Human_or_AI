This manuscript introduces iterative PoWER, a variant of the PoWER policy gradient algorithm that operates off-policy within the reward-weighted family.
Although I lack in-depth knowledge of this particular lower bound scheme, it appears to result in less conservative step sizes in the policy parameter space. Typically, expectation-based algorithms, including their KL-regularized counterparts similar to TRPO, employ small step sizes, and this approach may offer a viable means of accelerating these algorithms.
However, the experimental description provided in Section VI is inadequate for replication purposes. Several crucial details are unclear, such as the specifics of the force applied to the cart, the method of applying negative force, the state representation, the initial state distribution, and the episode duration. Furthermore, utilizing a linear policy may be insufficient for achieving both swing-up and balance in the cart-pole scenario, raising questions about the scope of the experiments. Additional information regarding the policy's noise magnitude, its selection process, and the experimental setup would be necessary for a comprehensive understanding.
The footnote on page 8 is confusing, as it mentions the use of Newton's method without elaborating on the computation of gradients and Hessians. Initially, I interpreted the argmax_theta operator as a surrogate for an expectation-maximization (EM)-style step, similar to the interpretation of Eq (8) in the Kober paper, which further adds to the confusion.