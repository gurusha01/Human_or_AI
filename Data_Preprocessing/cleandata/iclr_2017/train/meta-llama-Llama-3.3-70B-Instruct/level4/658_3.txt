The authors present a novel approach utilizing Sum-Product Networks (SPNs) to generate embeddings for input and output variables, which are then decoded using Mixed Product Networks (MPNs) to obtain the output variables. This method offers the advantage of predicting label embeddings, thereby decoupling dependencies within the predicted space. Experimental results demonstrate that SPN-based embeddings outperform those produced by Restricted Boltzmann Machines (RBMs).
This paper is dense and somewhat challenging to read. Upon closer examination, the primary contributions of the authors can be summarized as follows:
1. They propose a scheme involving the learning of SPNs over output variables (Y) and the subsequent use of MPNs for decoding, or the use of SPNs alone for embedding input variables (X).
2. They introduce a method for decoding MPNs with partial data.
3. The authors conduct an analysis to determine when their scheme yields perfect encoding and decoding.
4. They perform an extensive series of experiments comparing various applications of their proposed method for prediction tasks on multi-label classification datasets.
The main concerns regarding this paper are:
- The primary focus of the paper is on using generative models for representation learning, yet the experiments primarily involve discriminative tasks, such as predicting multiple output variables (Y) from input variables (X). The only discriminative baseline considered is L2 regularized logistic regression, which lacks structure on the output. It would be beneficial to compare the performance of a discriminative structured prediction method, such as Conditional Random Fields (CRFs) or belief propagation.
- The numerous experiments suggest that the proposed encoder/decoder scheme outperforms alternative methods. However, more details on the relative computational complexity of each method would be helpful.
- The reason behind the superior performance of this method compared to alternatives, such as MADE, is unclear. Does it learn a better model of the output distribution, or is it more effective at separating correlations in the output into individual nodes? Does it utilize larger representations?
- The experiments are overly extensive and, as presented, detract from the paper's clarity. The sheer number of results and graphs makes it challenging to understand and verify the claims. It would be more effective to present a concise table summarizing the key results, such as a baseline X->Y, an average result for the proposed method, and an average result for a competitive method, including both exact match and Hamming losses.
A suggested presentation format could be:
Input | Predicted Output | Decoder | Hamming Loss | Exact Match
----
X | P(Y) | CRF | xx.xx | xx.xx (baseline)
SPN E_X | P(Y) | n/a | xx.xx | xx.xx
X | SPN EY | MPN | xx.xx | xx.xx (given X, predict EY, then decode with MPN)
This format would facilitate easier verification of the results and improve the overall readability of the paper.