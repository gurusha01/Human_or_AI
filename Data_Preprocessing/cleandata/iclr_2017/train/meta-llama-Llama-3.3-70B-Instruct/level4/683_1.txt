This manuscript presents an ensemble method for residual networks, leveraging the Deep Incremental Boosting approach previously applied to CNNs (Mosca & Magoulas, 2016a). The technique involves adding a new block of layers at a specified position $p_t$ at each step $t$, and then copying the weights of all layers to the current network to accelerate training.
However, the novelty of this method is limited, as it primarily adapts the existing Deep Incremental Boosting framework. Rather than appending a layer to the network's end, this adaptation adds a block of layers at position $pt$ (initiating at a predetermined position $p0$) and merges layers accordingly, resulting in a minor modification to the original DIB method.
The empirical evaluation lacks data augmentation, leaving uncertainty as to whether the ensemble's potential improvements would persist when data augmentation is applied. Furthermore, the comparison to one of the primary baselines, DIB, is compromised by the absence of skip connections, which could negatively impact the fairness of the comparison. The authors justify the exclusion of state-of-the-art ResNets by focusing on the ensemble approach, but any potential gains from the ensemble could be offset by inherent features of ResNet variants. The boosting procedure may also be computationally restrictive for ImageNet training, where ResNet variants might perform significantly better. Therefore, the inclusion of state-of-the-art ResNets and Dense Convolutional Networks as baselines is necessary to make the results more conclusive.
Additionally, the sensitivity of the boosting process to the selection of the injection point remains unclear.
While this paper applies DIB to ResNets and provides some empirical analysis, the contribution is not sufficiently novel, and the empirical results are inadequate to demonstrate the method's significance.
Pros:
- Provides preliminary results for boosting ResNets
Cons:
- Lacks sufficient novelty, representing an incremental approach
- Empirical analysis is unsatisfactory
- The following details would be beneficial:
  - Experiment setup, including parameters to be tuned and training algorithms at each boosting step
  - Network architecture specifics and relevant references
  - Elaboration on the comparison to state-of-the-art ResNet variants and Dense Convolutional Networks
  - Comparison of training times
  - Results on ImageNet, if available