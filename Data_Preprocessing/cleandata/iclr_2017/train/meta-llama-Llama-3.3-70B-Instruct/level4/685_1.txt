This paper presents an enhanced neural network language model (NLM) that effectively handles large vocabularies by leveraging word embeddings derived from the combination of character-level embeddings and a convolutional network.
The authors conduct a comparative analysis of word embeddings (WE), character embeddings (CE), and a hybrid approach combining character and word embeddings (CWE). While it is straightforward to utilize CE or CWE embeddings as input to an NLM, their application at the output layer poses a challenge. To address this, the authors propose the use of Noise Contrastive Estimation (NCE), which facilitates accelerated training without impacting inference during testing, although the full softmax output layer must still be computed and normalized, a potentially costly operation.
The methodology employed for utilizing the network during testing with an open-vocabulary setup was unclear. Given that the NLM is solely used for reranking, it is possible to obtain the unnormalized probability of the target word at the output. However, when comparing different sentences through NLM feature-based reranking of n-best lists, it is uncertain whether this approach yields satisfactory results without proper normalization.
Furthermore, the authors provide perplexity values in Table 2 and Figures 2 and 3, which require normalization. Nevertheless, the normalization procedure used is not explicitly stated. The mention of a 250k output vocabulary raises doubts about the feasibility of calculating the softmax over such a large number of values. Clarification on this aspect is necessary.
The model's performance is evaluated through the reranking of n-best lists generated by a statistical machine translation (SMT) system for the IWSLT 2016 EN/CZ task. The abstract claims a 0.7 BLEU score improvement, which is disputed. A standard word-based NLM already achieves a 0.6 BLEU score gain, implying that the proposed model offers only an additional, statistically insignificant improvement of 0.1 BLEU. It is speculated that similar variations could be achieved through multiple model trainings with different initializations.
Regrettably, NLM models incorporating character representations at the output do not perform well, whereas several existing works have successfully utilized character-level representations at the input.
A discussion on the computational complexity during both training and inference phases would be beneficial.
Minor observations include:
- Figures 2 and 3 are incorrectly captioned as "Figure 4", which is misleading.
- The citation format is unconventional, as seen in "While the use of subword units Botha & Blunsom (2014)", which should be reformatted to "While the use of subword units (Botha & Blunsom, 2014)".