This manuscript proposes an extension of boosting to learn generative models of data, where a strong learner is formed by taking a geometric average of "weak learners". These weak learners can be either normalized or un-normalized generative models, such as VAEs or RBMs (referred to as genBGM), or a classifier trained to distinguish between the strong learner at iteration T-1 and the true data distribution (discBGM), which bears resemblance to Noise Contrastive Estimation and GANs.
The approach is backed by robust theoretical guarantees, providing strict conditions under which each boosting iteration is guaranteed to enhance the log-likelihood. However, a notable drawback is the absence of a normalization constant for the resulting strong learner, as well as the reliance on heuristics to weight each weak learner, which appears to have practical implications as seen in Section 3.2. Furthermore, the discriminative approach is hindered by a computationally expensive training procedure, requiring the generation of a substantial number of samples from the previous strong learner via MCMC at each boosting round.
The experimental section is the most underwhelming aspect of the paper. The method is evaluated on a synthetic dataset and a single real-world dataset, MNIST, for both generation and feature extraction for classification purposes. While the synthetic experiments effectively demonstrate the method's capabilities, the results on MNIST are less convincing due to the weak baseline models. For instance, a moderately sized VAE can achieve 90 nats within hours on a single GPU, which is a readily achievable benchmark. Additionally, I strongly believe that combining base learners is more of an academic exercise, given the significant burden of implementing and training K different models and algorithms. This section fails to address a more fundamental question: whether it is more beneficial to train a large VAE by maximizing the elbow or to train multiple iterations of boosting using smaller VAEs. The experimental details, particularly regarding the sampling procedure used to draw samples from the BGM, are also lacking, and the inclusion of likelihood estimates obtained via AIS would strengthen the paper.
Regarding novelty and prior work, a notable reference to "Self Supervised Boosting" by Welling et al [R1] is missing. Upon initial review, there appear to be significant similarities between the GenBGM approach and this prior work, which warrants discussion. 
Overall, I remain undecided. The concept of boosting generative models is intriguing, well-motivated, and has potential for significant impact. Given the theoretical contributions and the potential of this idea, I am willing to overlook some of the highlighted issues, with the hope that the authors will address them in their rebuttal.