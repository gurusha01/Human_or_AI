This paper investigates the challenge of training in the presence of substantial label noise, proposing two primary approaches to address this issue. The first method employs a latent variable model, which utilizes the EM algorithm to alternate between estimating the true label and maximizing parameters based on the true label. In contrast, the second approach directly integrates out the true label, simplifying the optimization process to focus on p(z|x).
The strengths of this paper include its examination of a critical training scenario, particularly relevant to large datasets with potentially inaccurate annotations. However, a limitation is that the results presented on MNIST are synthetic, making it difficult to determine the applicability and success of these methods on real-world datasets.
Several points warrant further consideration: the computational expense of Equation 11, especially in scenarios involving large datasets like ImageNet with 1000 classes, is a concern. Additionally, it would be beneficial to assess the ability of both the EM and integration methods to recover the corrupting distribution parameters. 
Overall, while this paper is satisfactory, its ideas are not particularly innovative, as prior research has also explored methods for handling label noise. To enhance the paper, the authors could focus on achieving state-of-the-art results on datasets known to contain label noise or demonstrate a reliable method for estimating true label corrupting probabilities.