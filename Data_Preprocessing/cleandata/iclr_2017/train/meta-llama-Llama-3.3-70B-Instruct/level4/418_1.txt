This study presents a novel approach to training Generative Adversarial Networks (GANs) by replacing simultaneous Stochastic Gradient Descent (SGD) and unrolling the inner optimization in the min-max game as a computational graph. The paper is well-structured and clearly explains the underlying justification. The problem addressed is significant and crucial. Although the approach is innovative, similar concepts have been explored in unrelated fields.
The first quantitative experiment, described in section 3.3.1, involves finding the optimal z that can generate training examples by utilizing L-BFGS on |G(z) - x|. The authors claim that if such a z is found, the generator can produce the specific training example. The results show that 0-step GANs struggle to generate many training examples, whereas unrolled GANs are more successful. However, this experiment is flawed, as finding a z that generates a particular sample does not guarantee that the mode is of high probability. In fact, an identity function could outperform all GAN models in this metric, and this issue persists even in lower-dimensional z spaces due to Cantor's proof of equivalence between real spaces. A more realistic expectation is that any image can be generated from a generator by finding a specific z, which does not necessarily prove that the generator is not missing modes. Instead, it suggests that the generator is similar to an identity function, capable of generating any possible image. This metric may be measuring something unrelated to diversity or mode-dropping. Furthermore, the inability to find a z for a specific training example does not prove its non-existence, only that it is harder to find, which may indicate that unrolled GANs have a smoother function than 0-step GANs, making it easier to optimize for z.
The second quantitative experiment examines the mean pairwise distance between generated samples and data samples. The authors argue that similar distances between the two indicate diverse generated samples. However, this metric is unconvincing, as it measures distances in pixel-space and may yield good results even if the GAN model generates low-quality samples.
The paper lacks additional quantitative results. To verify the method's optimization of diversity, including quality metrics such as Inception scores or SSL performance would be beneficial. Another potential metric is training a GAN using this approach on the tri-MNIST dataset, which consists of 1000 easily identifiable modes, and demonstrating that the GAN can generate all modes with equal probability. Although not perfect, this metric is arguably more effective than those presented in the paper, as seen in a recent ICLR submission.