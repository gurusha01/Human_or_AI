Summary: This paper introduces a method for converting dense to sparse networks in Recurrent Neural Networks (RNNs), which progressively sets more weights to zero during the training phase, resulting in a model with reduced storage requirements and increased inference speed.
Pros:
The proposed pruning method is notable for not requiring re-training and not impacting the RNN training phase. It achieves a significant sparsity of 90%, leading to a substantial reduction in the number of parameters.
Cons & Questions:
A potential concern is whether the process of carefully selecting hyperparameters for different models and applications could become overly cumbersome. Additionally, clarification is needed on equation 1, specifically whether 'q' represents the sparsity of the final model. It would be beneficial to have a formula to predict the sparsity, number of parameters, and accuracy of the final model given a set of hyperparameters before initiating training.
The trade-off between the number of units and sparsity to achieve better parameter counts or accuracy, as seen in table 3, or improved speed in table 5, is a valuable insight. However, it is essential to include results for the GRU sparse big model to demonstrate comparable accuracy with a decent compression rate and speedup, similar to the comparison between RNN Sparse medium and RNN Dense. The advantage of pruning for high speedup is diminished if it comes at the cost of significant accuracy loss.
Furthermore, the discrepancy in sparsity values between table 3 and table 5 warrants explanation, as the text mentions an average sparsity of 88%, while table 5 reports 95%. It is also unclear whether the models used in table 3 differ from those in table 5.
The introduction states that unlike previous approaches, such as Han et al. (2015), further increases in training time are undesirable due to the already extensive training times required for state-of-the-art results in speech recognition. However, Han et al. (2015) actually mentions that Huffman coding does not require training and is implemented offline after fine-tuning, which suggests that both methods should have similar training times for LSTM models. The origin of the 3-4x extra training time in Han et al. (2015) and its absence in the proposed approach requires clarification.