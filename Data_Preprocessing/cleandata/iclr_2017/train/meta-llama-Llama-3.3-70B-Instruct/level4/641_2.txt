The authors present two novel variational methods centered on posterior approximations that lack a tractable density. The first approach builds upon the concept of "amortized SVGD" (Wang and Liu, 2016), with the key innovation being the utilization of SGLD as the inference network. The second method draws from a NIPS paper (Ranganath et al., 2016) that focuses on minimizing Stein divergence using a parametric approximating family, where the authors introduce the innovation of defining test functions within a Reproducing Kernel Hilbert Space (RKHS), thereby obtaining an analytical solution to the inner optimization problem.
The methodology employed is incremental, with the majority of the content up to Section 3.2 serving as motivation, background, or related work. Notably, the concept of a "wild variational approximation" was previously defined in Ranganath et al. (2016) as a "variational program," and it would be beneficial for the authors to clarify any distinctions between these concepts.
Initially, Section 3.2 appears intriguing as it provides an analytical solution to the maximization problem posed in Ranganath et al. (2016). However, this approach relies on the use of a kernel, which is unlikely to scale efficiently in high-dimensional settings, effectively rendering it equivalent to selecting a simple test function family in practice. To achieve scalability in high dimensions, a more complex kernel would be required, along with the capability to learn its parameters, which does not offer a significant advantage over parameterizing the test function family as a neural network, as done in Ranganath et al. (2016).
Section 4 introduces a Langevin inference network, which essentially involves selecting the variational approximation as an evolving sequence of Markov transition operators, akin to the approach in Salimans et al. (2015). The terminology "inference network" is somewhat misleading, as it does not conform to the conventional understanding of an inference network, where parameters are determined by the output of a neural network. Instead, the authors define global parameters for the SGLD chain that are applied uniformly across all latent variables, which may be considered suboptimal. Furthermore, it is unclear what distinguishes this approach from the variational approximation used in Salimans et al. (2015), aside from the objective function used for training.
The experimental section is limited, focusing on a toy mixture of Gaussians posterior and Bayesian logistic regression. These experiments do not address potential issues that may arise in high-dimensional and real-world data, such as the lack of scalability for the kernel, comparisons to Salimans et al. (2015) for the Langevin variational approximation, and considerations of runtime or training difficulty.
Minor comments include:
* The authors' understanding of previous work on expressive variational families and inference networks appears to be unclear. For instance, they argue that Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015 require handcrafted inference networks, when in fact, these works assume the use of any neural network for amortized inference and do not necessitate an inference network. Perhaps the authors intend to refer to handcrafted posterior approximations, which is partially true; however, the mentioned works are algorithmic in nature and offer flexibility in their design choices.
* The paper's motivation could be improved, and the authors should clarify their definition of an inference network to avoid confusion.
* It is recommended that the authors refrain from referring to a variational inference method based solely on the class of approximating family, as this terminology has been used in the literature to describe any variational method that imposes minimal constraints on the model class, as seen in black box variational inference (Ranganath et al., 2014).