This paper investigates the problem of transferring knowledge from a smaller network to a larger one, building upon the foundations established by Net2Net (ICLR 2015) and NetMorph (ICML 2016).
Comments:
- 1) The paper explores a macroscopic problem by decomposing the morphing process into multiple fundamental operations. Although these atomic operations were introduced in Net2Net and NetMorph, a comprehensive study of the generalized modularized process has been lacking, making this paper's inquiry novel.
- 2) The proposed solution, which involves combining multiple atomic transformations, appears to be reasonably sound.
- 3) In the "related work" section, it would be beneficial to replace "network morphism" with "knowledge transfer" in the subsection title, as most relevant studies are referred to as knowledge transfer, facilitating connections to existing research.
- 4) The author presents experiments on ResNet variants, demonstrating that initialization from ResNet yields better error rates than training from scratch. However, the source of this improvement remains unclear.
- 5) A significant advantage of this knowledge transfer approach (Net2Net, NetMorph) is the potential to accelerate training and model exploration. Unfortunately, the experiments do not demonstrate this benefit, possibly due to the suboptimal initialization of BatchNorm, which is a major drawback of this paper.
- 6) The proposed method can, in principle, perform complex transformations, such as converting an entire ResNet from a single convolutional layer. However, the experiments only involve simple module transformations, which can be addressed by atomic operations. It would be more intriguing to investigate the outcomes of more complex transformations, even if they are less effective.
In summary, this paper explores a novel problem of knowledge transfer at a macroscopic level. The proposed method may be of interest to the ICLR community. To make the results more convincing and practically useful, it is essential to improve the experiments (as noted in comment 5), and the authors are strongly encouraged to do so.