This paper proposes a novel integration of Variational Auto-Encoders (VAEs) with the Stick-Breaking process, aiming to address component collapsing and achieve a representation with stochastic dimensionality. The authors evaluate their approach on MNIST and SVHN datasets in both unsupervised and semi-supervised settings.
Upon closer examination, it appears that the claim of stochastic dimensionality in the latent variable may be misleading: while all latent variables are utilized for backpropagation, they are parametrized differently using $\pi$, and the decoding process is modified to create an illusion of sparsity. Notably, this process does not involve marginalization and bears resemblance to the soft-gating mechanism commonly employed in LSTMs and attentional models.
Regarding Figure 5b, which displays the decoder input weights, the effect of component collapsing may differ from that of a Gaussian prior. The positivity of $\pi$ means that a small average value may indicate values close to zero most of the time, potentially eliminating the need for weight updates. In contrast, a standard Gaussian prior would result in a noisy input without signal, prompting the decoder to suppress the corresponding channel by reducing incoming weights from the collapsed variable.
To provide further insight, including a histogram of the latent variables alongside the existing plot could help determine whether the associated weights are substantial due to actual usage or merely because the inputs are often zero. 
The semi-supervised results outperform a weaker variant of the model presented in (Kingma et al., 2014), but a more comprehensive comparison would involve evaluating the results against the M1+M2 model from the same paper, potentially requiring the use of two VAEs to ensure a fair assessment.