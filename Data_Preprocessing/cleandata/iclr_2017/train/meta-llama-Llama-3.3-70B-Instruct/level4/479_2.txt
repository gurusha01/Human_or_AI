This paper presents an iterative memory updating model for cloze-style question-answering tasks, which is an intriguing approach that yields promising results. However, I have several comments to provide feedback on the paper:
1. Upon closer examination, it appears that the paper actually proposes two distinct models, rather than a single one. The first model comprises "reading", "writing", "adaptive computation", and "Answer module 2" components, while the second model consists of "reading", "composing", "writing", "gate querying", and "Answer module 1" components. Based on the methodology and experimental results, the "adaptive computation" model seems to be simpler and more effective. Notably, without the two-time memory update in a single iteration and the composing module, this model bears similarities to the neural Turing machine.
2. Could the authors provide more details on the multi-layer perceptron (MLP) settings used in the composing module, as this information is not explicitly stated?
3. The paper explores different hidden state sizes, including [256, 368, 436, 512], but the rationale behind these specific numbers is unclear. What motivated the selection of these particular values, and are there any heuristics or techniques that facilitated their discovery?
4. To further strengthen the paper, I recommend conducting more ablation studies to investigate the impact of using different values of T, such as T=1 or T=2, on the model's performance.
5. According to my understanding of the adaptive computation mechanism, the process terminates when P_T < 0. Therefore, I would like to know more about the distribution of T in the testing data, as this could provide valuable insights into the model's behavior and performance.