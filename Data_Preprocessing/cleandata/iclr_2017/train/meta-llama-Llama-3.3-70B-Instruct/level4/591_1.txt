This paper introduces a novel concept, referred to as sample importance, aimed at investigating the impact of individual samples during the training process of deep neural networks. However, the definition of this criterion remains unclear, as the term \phi^t{i,j} is not defined, whereas \phi^ti is. Despite this ambiguity, it appears that sample importance is calculated as the squared L2 norm of the gradient for a given sample i at time t, scaled by the squared learning rate, which seems inappropriate as the learning rate should not influence sample importance.
The experimental setup involves the MNIST and CIFAR datasets, utilizing suitable network architectures, hyperparameters, and initializations. Nevertheless, the size of the hidden layers seems insufficient, particularly for CIFAR, which might explain the poor performance observed in Figure 6, with a 50% error rate.
The analysis of sample importance evolution across layers during training yields somewhat trivial conclusions. For instance, the finding that "the overall sample importance differs across epochs" is expected, given that the norm of the gradient naturally varies. Additionally, the observation that the output layer consistently exhibits the highest average sample importance per parameter, which peaks early in training and then decreases, can be attributed to the backward flow of gradients and the expected reduction in gradient strength as learning progresses.
The question posed in Figure 4, "Is Sample Importance equivalent to the Negative log-likelihood of a sample?" seems misguided, as these concepts are inherently distinct.
The subpar performance on CIFAR undermines the applicability of the presented results. Furthermore, the error rate in Figure 7 for MNIST is difficult to interpret, as it would be more informative to present error rates within a range of 0 to 10 or 20%.
Despite these significant concerns, the paper does touch on some intriguing aspects, such as the distinction between "easy" and "hard" samples, which appears to align with intuitive expectations, albeit in a preliminary manner. The experimental results are also well-presented, which is a notable strength of the paper.