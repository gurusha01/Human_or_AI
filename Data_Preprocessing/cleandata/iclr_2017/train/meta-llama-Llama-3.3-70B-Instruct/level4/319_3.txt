This paper presents an investigation into the transfer of attention between a teacher network and a student network, with the goal of improving the student's performance. 
The attention transfer process involves minimizing the L2 distance between the attention maps of the teacher and student networks at various layers, in conjunction with minimizing the classification loss and an optional knowledge distillation term. The authors introduce several activation-based attention mechanisms, which are defined as the sum of absolute feature values raised to the power of p or the maximum of values raised to the power of p. Additionally, they propose a gradient-based attention mechanism, which is derived from the loss with respect to the inputs.
The authors evaluate their approaches on multiple datasets, including CIFAR, CUB, Scene, and ImageNet, and demonstrate that attention transfer can enhance the test performance of the student network. However, the student networks consistently underperform the teacher networks, even with attention transfer.
Several questions and remarks arise:
- In Section 3, the authors claim that networks with higher accuracy exhibit higher spatial correlation between objects and attention maps. While Figure 4 is persuasive, quantitative results would provide further evidence to support this claim.
- The selection process for hyperparameter values is not clear, and an analysis of the impact of $\beta$ would be beneficial.
- Reporting the teacher's train and validation loss in Figure 7b would provide additional context.
- The experiments do not clearly elucidate the advantages and disadvantages of different attention maps.
- Although attention transfer does not yield better results than the teacher, the student networks have fewer parameters, which could lead to a speed-up. Characterizing this speed-up and investigating whether attention transfer offers any benefits when the student and teacher architectures are identical would be valuable.
In summary, the strengths of the paper include:
- Clear writing and motivation.
- Consistent improvement in the student's performance with attention transfer compared to the student alone.
The weaknesses of the paper are:
- The student networks perform worse than the teacher models.
- It is unclear which attention mechanism to use in which scenario.
- The novelty of the approach is somewhat incremental compared to existing work, such as FitNet.