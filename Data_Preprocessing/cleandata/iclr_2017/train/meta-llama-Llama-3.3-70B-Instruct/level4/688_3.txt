This paper proposes a notable extension to the PoWER algorithm, driven by a clear motivation. However, a significant limitation lies in the absence of a comprehensive comparison with existing methodologies and its application to more complex problems. The experimental results fail to convincingly demonstrate the claimed advantages, versatility, and scalability of the proposed method over preceding approaches. Establishing this confidence does not necessarily entail applying the method to all large-scale domains or conducting an exhaustive hyperparameter search, but rather exploring its performance beyond the current domains. For instance, the Cartpole example optimizes only 5 parameters, and the ad targeting task lacks a comparison with alternative methods. Given its foundation in PoWER and close relationship with RWR, it is probable that this method will encounter performance limitations when applied to other domains and benchmarked against standard methods, such as importance sampling-based off-policy learning, which is known to struggle in high-dimensional or continuous action spaces due to its connection with entropy-regularized RL.