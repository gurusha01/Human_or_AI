This manuscript investigates the concept of "Sample Importance" for individual samples within a training dataset and its impact on the overall learning process.
The authors present empirical evidence demonstrating that different training cases yield larger gradients at various stages of learning and across different layers. The paper reveals intriguing results that contradict common curriculum learning approaches, which typically prioritize easy training samples. However, the definition of "easy" training cases remains ambiguous.
Furthermore, the experiments comparing the ordering of NLL or SI to mixed or random batch construction provide valuable insights. 
Potential enhancements include decoupling the magnitude of gradients from the contribution of "sample importance", as high gradients (as a function of a specific weight vector) can be influenced by weight initialization, introducing noise into the model.
Additionally, exploring improvements to the batch selection algorithm based on "Sample Importance" to outperform the baseline of random batch selection would be a worthwhile pursuit.
Overall, this paper is a solid contribution, featuring a range of experiments that examine the influence of various samples on SGD and its effects on different aspects of training.