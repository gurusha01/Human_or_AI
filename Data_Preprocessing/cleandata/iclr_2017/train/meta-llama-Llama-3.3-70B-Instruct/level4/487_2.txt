This paper presents a novel approach to sparsely connected networks and introduces an efficient hardware architecture, which achieves a significant reduction of up to 90% in memory usage compared to traditional fully connected neural network implementations. 
The authors demonstrate the effectiveness of their method by selectively removing connections in fully connected layers, resulting in improved performance and computational efficiency across three distinct datasets. Furthermore, they successfully integrate their approach with binary and ternary connect studies, yielding additional enhancements.
However, the paper's clarity is compromised by a potentially misleading statement: the proposal of sparsely-connected networks via the reduction of fully-connected network connections using linear-feedback shift registers (LFSRs). Initially, this led to the interpretation that LFSRs reduce connections by storing information in registers. Nevertheless, it is clarified that LFSRs are utilized solely as random binary generators, with any random generator being suitable but LFSRs being chosen for their convenience in VLSI implementation.
A more transparent explanation would be: this paper proposes sparsely-connected networks by randomly eliminating connections in fully-connected networks, where random connection masks are generated by LFSRs, also exploited in VLSI implementation for connection disabling.
The training process outlined in Algorithm 1 essentially involves back-propagation with binary masks disabling certain connections in each layer, which could be explicitly stated in the text for enhanced clarity.
It is worth noting that the concept of random connections in CNNs is not entirely new, having been explored in a 1998 paper by Yann LeCun and colleagues, where random connections were utilized between CNN layers.