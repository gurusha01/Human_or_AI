This manuscript presents an empirical investigation into the robustness of state-of-the-art convolutional neural networks (CNNs) against various types of adversarial attacks in the context of image classification. Specifically, an attack involves manipulating an image to deceive the classification system, resulting in either a non-targeted misclassification, where the image is classified as any incorrect class, or a targeted misclassification, where the image is classified as a pre-determined target class. For example, an attacker might alter an image of an ostrich to be misclassified as a megalith, highlighting potential vulnerabilities in current systems that could have significant implications, such as risks associated with autonomous vehicles.
The paper is primarily experimental in nature, comparing different strategies for popular network architectures (including VGG, GoogLeNet, and ResNet-50/101/152) against both non-targeted and targeted attacks. The experiments are well-conducted and clearly presented, with a notable aspect being the inclusion of attacks on "clarifai.com," a black-box classification system. The authors also provide insightful analyses and explanations to elucidate why CNNs are susceptible to such attacks, as discussed in Section 6.
In summary, the key findings indicate that non-targeted attacks can be easily executed, even against black-box systems. While targeted attacks are more challenging, the authors propose a novel approach that significantly improves upon existing methods, albeit with a success rate of approximately 20% on clarifai.com, compared to 2% achieved by previous schemes.
However, the paper has several weaknesses. Firstly, the authors treat the three ResNet-based networks as distinct, despite their obvious correlation due to similar architectures, which varies only in depth. This correlation is evident in Table 7, making it unfair to highlight differences among these networks as a significant finding. 
Secondly, the evaluation of attack effectiveness on the black-box system relies on subjective measures due to the difference in image labels returned by clarifai.com and ImageNet, which may not yield entirely fair comparisons, despite the qualitative results appearing convincing.
Thirdly, the novelty of the proposed approach, which involves optimizing an ensemble of networks rather than a single network, is somewhat limited. Nevertheless, its effectiveness and the fact that it was not the primary focus of the paper make it acceptable.
Fourthly, the paper's length is considerable, expected for an extensive evaluation study, but some content, such as the overlap between Sections 2.3 and 1, could be pruned to improve conciseness.
Lastly, the discussion in Section 6 would benefit from additional references to recent and related work, such as that by Fawzi et al. (NIPS'16), which provides theoretical insights aligning with the experimental findings, particularly in understanding why CNNs are prone to attacks.
In conclusion, this paper offers value to the community by highlighting the vulnerabilities of current architectures and potentially guiding improvements. A question arises regarding the choice of optimization objective in equation (7), where f(x) is used instead of J(x). Given that f(x) = argmax J(x) and is highly non-linear, gradient descent might be less efficient. Although optimizing J(x) instead of f(x) alters the objective, the outcomes could be similar. It would be interesting to know if this approach was considered or if there was a misunderstanding in the optimization strategy.