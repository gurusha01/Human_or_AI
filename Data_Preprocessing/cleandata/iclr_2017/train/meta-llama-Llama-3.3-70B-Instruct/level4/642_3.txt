This manuscript presents a pioneering investigation into the development of customized precision hardware for large convolutional neural networks, specifically AlexNet, VGG, and GoogLeNet. The findings demonstrate that utilizing floating-point precision with reduced bits can yield significant speed-ups of up to 7x, outperforming fixed-point representations. 
The authors also propose a method for predicting custom floating-point precision parameters directly from neural network activations, thereby circumventing exhaustive search. However, the explanation of this approach is unclear, particularly with regards to the data used for evaluating the activations of the last layer. It is uncertain whether this evaluation is performed on the entire validation set, and if so, why this method would be more efficient than computing classification accuracy.
The results of this study may be beneficial for hardware manufacturers, albeit with a notable limitation. The evaluated networks do not incorporate batch normalization, a technique widely adopted in contemporary convolutional networks. This omission raises questions about the applicability of the study's conclusions to batch normalization networks, where fixed-point representations might be more effective. Further exploration of this aspect is warranted.
Ultimately, the manuscript offers a useful examination of numerical precision trade-offs during neural network testing, although it does not introduce substantial novelty. The focus on test time is noteworthy, but training time is also an important consideration, particularly given the current research landscape where accelerating network training is a priority. 
From a non-expert perspective in digital logic design, it appears that this paper falls slightly short of the acceptance threshold.