This paper presents an adaptation of weight normalization and normalization propagation techniques for application to recurrent neural networks, with preliminary experiments indicating promising results.
The proposed contribution has the potential to benefit a wide audience, given the fundamental role of LSTMs in the field.
However, the novelty of the contribution is somewhat limited, as the modifications to weight normalization are relatively minor. Furthermore, the experimental validation is not entirely convincing, as the comparison with layer normalization reveals higher test errors due to overfitting, despite apparent optimization advantages. Additionally, the authors appear to omit the data-dependent parameter initialization for weight normalization, as originally proposed, which raises questions about the completeness of their approach.