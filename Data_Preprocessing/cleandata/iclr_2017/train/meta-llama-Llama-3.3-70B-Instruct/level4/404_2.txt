This paper presents a new recurrent neural network (RNN) architecture, termed QRNN, which bears similarities to gated RNNs but differs in that its gate and state update functions are determined solely by recent input values, independent of the preceding hidden state. These functions are computed via a temporal convolution applied to the input, enabling greater parallelization due to reduced operations in the hidden-to-hidden transition that depend on the previous hidden state, compared to GRU or LSTM models. However, this may come at the cost of reduced expressiveness, particularly in handling long-term dependencies without requiring multiple QRNN layers.
The authors also explore various QRNN extensions, incorporating techniques such as Zoneout, dense connections, and sequence-to-sequence models with attention. The approach is evaluated across multiple tasks and datasets, including sentiment classification, word-level language modeling, and character-level machine translation.
Overall, the paper is engaging and the proposed approach is noteworthy, with pros including addressing a significant problem, providing a thorough empirical evaluation that demonstrates the benefits of their approach, and achieving speed-ups of up to 16 times compared to LSTM. However, the novelty is somewhat incremental, building upon existing work (Balduzizi et al., 2016).
Several specific questions arise:
- Is the use of a dense layer necessary for achieving good results on the IMDB task, and how does a simple 2-layer QRNN compare to a 2-layer LSTM?
- What is the comparative performance of the i-fo-ifo pooling method?
- How does the QRNN architecture handle long-term time dependencies, and were experiments conducted on simple tasks such as the copy or adding task to assess this capability?