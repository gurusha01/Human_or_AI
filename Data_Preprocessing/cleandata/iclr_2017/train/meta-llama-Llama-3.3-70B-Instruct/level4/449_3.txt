This paper presents a novel design principle for convolutional network computation blocks, leveraging a fractal-like structure through repeated expand and join operations. The primary focus of the paper is on experimental evaluation, aiming to demonstrate that good performance can be achieved without relying on residual formulations, at least for certain tasks.
In my assessment, the evaluations presented in the paper are not entirely convincing. A major concern is the lack of a suitable baseline, which is essential for clearly demonstrating the improvements resulting from isolated changes. I acknowledge that establishing such a baseline is challenging due to the paper's introduction of a new architecture principle, but this underscores the need for more rigorous effort in this area to ensure the core insights remain relevant even as better-performing architectures emerge.
To facilitate fair comparisons between architectures, the number of parameters and computational requirements should be considered. Several specific points warrant attention:
- Table 1's comparison to Resnets could be strengthened by directly comparing He et al.'s 2016b Resnets and Wide Resnets to FractalNet, as these outperform FractalNet on CIFAR-100 and both datasets, respectively. The authors' comparison to other results without augmentation would be more comprehensive if they included additional experiments without augmentation for these architectures.
- The 40-layer Fractal Net should only be compared to other models if parameter reduction techniques are uniformly applied across all models.
- A thorough comparison to Inception networks is also necessary, given the similarities between Inception modules and FractalNet's use of shorter and longer paths without shortcuts. Simplifying the Inception design, such as replacing concatenation with a mean operation among equally sized convolution outputs, could provide a robust baseline. Notably, Inception networks have already shown that residual connections are not required for achieving top performance, as demonstrated in Szegedy et al.'s work on Inception-v4, Inception-ResNet, and the impact of residual connections on learning.
- It is worth noting that Residual and Highway architectures exhibit a form of anytime property, as evidenced by lesioning experiments in Srivastava et al. and Viet et al.
- While the architecture-specific drop-path regularization is an interesting concept, its benefits are not clearly discernible when used in conjunction with other regularizers like dropout, batch normalization, and weight decay.
Ultimately, the experiments do not conclusively demonstrate the utility of the proposed architecture, leaving room for further investigation and more rigorous evaluation.