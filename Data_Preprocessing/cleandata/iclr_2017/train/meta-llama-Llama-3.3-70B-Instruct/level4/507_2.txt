This manuscript introduces a novel benchmark for evaluating word representations, namely identifying the "odd one out". By leveraging existing ontologies, the authors successfully expand on a concept recently presented at the RepEval workshop, resulting in a substantially larger collection of examples.
While the innovation may be considered relatively minor, it represents a crucial step towards establishing rigorous benchmarks for general-purpose word representations. Notably, although humans can excel in this task with sufficient domain knowledge, the experiments demonstrate that current embeddings struggle to achieve comparable performance. The technical aspects of the paper are well-executed, with a logical approach to dataset construction and a compelling correlation analysis. Overall, I strongly support the acceptance of this work at ICLR.