This paper proposes an innovative application of batch normalization to Recurrent Neural Networks (RNNs), specifically Long Short-Term Memory (LSTM) networks, along the horizontal depth. Unlike preceding studies by Laurent (2015) and Amodei (2016), the authors demonstrate that normalizing the hidden states of RNNs can enhance optimization, attributing this success to the proper initialization of parameters, particularly the gamma value. The experiments yield modest performance improvements over traditional LSTMs on various benchmarks, including Sequential MNIST, PTB, Text8, and CNN Question-Answering.
In terms of novelty and significance, the extension of batch normalization to RNNs is a natural progression, given its proven effectiveness in training deeper networks, such as ResNets. The originality of this work lies in the application of batch normalization to the hidden states and the use of per-time step statistics. However, the added computational cost and complexity of the proposed batch-normalized LSTM (BN-LSTM) model are notable, and a comparison of training speeds in terms of wall-clock time would have provided a more comprehensive evaluation.
The relevance of this research is broad, as RNNs are widely used across various tasks. Nevertheless, the gains in performance are generally modest and require careful tuning of parameters. Furthermore, the work does not address the question of why batch normalization improves generalization, in addition to accelerating training.
The paper is well-structured, clear, and concise, with a well-motivated introduction and a detailed description of the model. The plots effectively illustrate the key points, making the paper easy to follow.
In summary, this work presents an interesting, albeit incremental, adaptation of batch normalization for RNNs, demonstrating its effectiveness where previous attempts have failed. The experiments are comprehensive, but the empirical gains may not be sufficient to justify the increased model complexity and computational overhead.
The strengths of this paper include:
- The successful application of batch normalization to RNNs, where previous studies have been unsuccessful
- A thorough analysis of hyperparameter choices and activations
- Experiments on multiple tasks
- Clarity of presentation
The weaknesses of this paper are:
- The relatively incremental nature of the contribution
- The need for several modifications (per-time step statistics, noise addition for exploding variance, sequence-wise normalization) to make the method work
- The lack of discussion on computational overhead
- The limitation to character or pixel-level tasks, with no exploration of word-level applications.