The authors propose a framework for assessing sentence embedding techniques by evaluating their ability to retain information regarding sentence length, word content, and word order. Their investigation encompasses several prominent embedding methods, including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments conducted are comprehensive, yielding intriguing insights into the representational capabilities of common sentence embedding strategies, notably that word ordering exhibits surprisingly low entropy when conditioned on word content.
Investigating the type of information encoded in representation learning methods for NLP is a crucial yet understudied area of research. A pertinent example is the wave of word-embeddings research, which was largely stemmed by a series of meticulous experimental results demonstrating that most embeddings are essentially equivalent, as highlighted in "Improving Distributional Similarity with Lessons Learned from Word Embeddings" by Levy, Goldberg, and Dagan. As representation learning assumes increasing importance in NLP, the significance of this type of research will continue to grow.
Although this paper makes a valuable contribution by establishing and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are relatively straightforward and may not directly correlate with real-world requirements for sentence embeddings, as acknowledged by the authors. For instance, the ability of the averaged vector to encode sentence length can be anticipated due to the central limit theorem or concentration inequalities such as Hoeffding's inequality.
The experiments examining word order were noteworthy. A relevant reference for this type of conditional ordering procedure is "Generating Text with Recurrent Neural Networks" by Sutskever, Martens, and Hinton, who describe the process of converting a bag of words into a sentence as "debagging."
While this study represents an initial step towards a deeper understanding of sentence embeddings, it is a significant one. Therefore, I recommend this paper for publication, as it lays essential groundwork for future research in this area.