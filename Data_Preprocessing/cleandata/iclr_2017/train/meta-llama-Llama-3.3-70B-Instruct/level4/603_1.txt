This manuscript addresses the code completion problem, which involves generating a distribution over the next token or sequence of tokens given partially written source code, a problem of significant interest to both industry and research. The authors propose a novel approach utilizing an LSTM model to sequentially generate a depth-first traversal over an Abstract Syntax Tree (AST), yielding improved results over preceding methods that employed more rigid conditioning mechanisms, such as those presented by Bielik et al. in 2016. However, merely enhancing previous work with LSTM-based conditioning may not constitute a sufficient contribution to warrant a standalone paper. To substantially enhance the contribution, potential avenues for exploration could include examining the impact of distinct traversal orders on predictive accuracy and investigating alternative strategies for handling unknown (UNK) tokens. Ultimately, the paper's objective is to advance code completion capabilities, and it would be beneficial to move beyond simply applying neural approaches to existing methods.
Comments:
- The last two sentences of the related work section assert that other methods are limited to examining only a subset of the source code, a statement that is both vague and inaccurate. In reality, models described in works such as Bielik et al. (2016) and Maddison & Tarlow (2014) are capable of conditioning on any part of the AST that has been generated. The key distinction in this work lies in the LSTM's ability to learn flexible conditioning without increasing computational complexity.
- The denying prediction experiments reveal that the most compelling metric is the Prediction Accuracy, defined as P(accurate | model doesn't predict UNK). It would also be insightful to examine P(accurate | UNK is not ground truth), as models trained to disregard UNK losses are expected to perform worse overall. A crucial question remains as to whether these models perform worse specifically on non-UNK tokens.