I am maintaining my original review score as the authors have not addressed the feedback provided.
This paper presents a deep learning-based approach to modeling correlated time series using a latent variable model, which incorporates Gaussian latent factors and a multifaceted training objective. The objective includes terms for reconstructing observed time series, ensuring smoothness in the latent state space through a KL divergence term that promotes similar distributions between neighboring states, and a regularizer that encourages related time series to exhibit similar latent state trajectories. The relationships between trajectories are predefined based on existing knowledge. For instance, the latent state trajectories for neighboring wind speed base stations are expected to be similar. The model is optimized using gradient descent. The authors also propose several extensions, including a nonlinear transition function based on a multilayer perceptron (MLP) and a reconstruction error term that accounts for variance. However, the model is limited by its use of a linear decoder. While the experimental results are positive, they are not entirely convincing.
The strengths of this paper include:
- The authors' focus on a challenging and worthwhile problem: integrating uncertainty modeling over hidden states with the flexibility of neural network-like models.
- The clever idea of using KL divergence to represent relationships between hidden states, combined with a Gaussian distribution over hidden states, resulting in a simple and differentiable regularization term.
- The robust and flexible approach of formulating the problem as a neural network-like loss function, which could be combined with other methods, such as variants of variational autoencoders.
However, the paper also has several weaknesses:
- The presentation is unclear, particularly in Section 3.3, where the model definition is introduced. The authors present four variants of their model with different combinations of decoders (with and without variance terms) and linear versus MLP transition functions. The results suggest that the 2,2 variant generally performs better, but not consistently across all metrics and often by small margins, making it difficult to draw firm conclusions. To improve the manuscript, I suggest: (1) focusing on the primary 2,2 variant in Section 3.3 and presenting the simpler variants as additional baselines in Section 4.1; and (2) conducting more comprehensive experiments with larger datasets to strengthen the case for the superiority of this approach.
- The authors only briefly mention learning in the framework, with references to gradient descent and ADAM, and the subsection on inference lacks clarity, ending in an ellipsis.
- The purpose of introducing the inequality in Equation 9 is unclear.
- The experimental results are not convincing, given the small size of the data and the marginal differences compared to the RNN and Kalman filter (KF) baselines, which are not particularly strong.
- The paper's position relative to variational autoencoders and related models is unclear. Recurrent variants of VAEs, such as those proposed by Krishnan et al. (2015), seem to achieve similar goals in terms of uncertainty modeling and could be easily extended to model relationships between time series using the proposed regularization strategy.
This research direction has value, with intriguing ideas and interesting preliminary results. I recommend that the authors restructure the manuscript to improve clarity, particularly in the model description, and provide more detail about the learning and inference processes. Additionally, more thorough experiments are necessary to present a clear narrative about the strengths and weaknesses of this approach.