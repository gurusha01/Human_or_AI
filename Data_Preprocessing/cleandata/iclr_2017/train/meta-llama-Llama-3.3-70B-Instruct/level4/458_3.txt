This manuscript proposes a methodical approach to identifying flat minima, driven by their enhanced ability to generalize. The authors introduce a novel term to the original loss function, leveraging both the width and depth of the objective function. This regularization term can be viewed as the Gaussian convolution of the exponentiated loss, effectively creating a Gaussian-smoothed version of the exponentiated loss. The smoothing process inherently suppresses sharp minima.
The development of this regularization term, rooted in thermodynamic concepts, is a compelling aspect of the paper. However, there are a few points that require clarification from the authors.
1. The experimental results report the number of epochs required to achieve better generalization performance with the proposed algorithm compared to plain SGD. It is unclear whether this refers to the number of epochs in the specific step of the algorithm (e.g., line 7) or the total number of epochs, including all combined steps (e.g., lines 3 and 7). If the former, the comparison may be biased. If the number of epochs for SGD is adjusted to account for the iterations needed to approximate Langevin dynamics, the gain over plain SGD appears to be minimal.
2. The proposed algorithm approximates the smoothed, exponentiated loss. A question arises regarding the comparison to a simpler approach: smoothing the original loss without exponentiation. Is the difference primarily motivational (e.g., thermodynamic interpretation), or are there deeper implications, such as more accurate approximation or improved generalization bounds due to the attained smoothness? Smoothing the cost function without exponentiation could allow for simpler approximation methods (e.g., Monte Carlo integration instead of MCMC), as discussed in section 5.3 of relevant literature.