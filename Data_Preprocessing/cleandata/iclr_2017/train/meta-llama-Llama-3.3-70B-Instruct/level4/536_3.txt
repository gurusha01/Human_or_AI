This paper explores the approximation capabilities of neural networks for noise-stable functions, a class of functions whose output is relatively insensitive to individual input variations. The problem definition is intriguing, and the paper is well-structured, making it easy to follow the proofs and arguments presented.
I have two primary concerns:
1. Presentation: The concept of noise-stability can be viewed as a measure of the intrinsic dimensionality of the data, based on the function's dependence on different dimensions. This perspective allows for the formulation of an analog to approximation theorems, grounded in the data's true dimensionality. However, it is unclear when stability-based bounds surpass dimension-based bounds, as both exhibit exponential growth. The authors' presentation of the results, emphasizing a dimension-independent bound and a constant that grows exponentially with (1/eps), oversimplifies the relationship between epsilon, stability, and dimensionality. In many cases, (1/epsilon) is dimension-dependent, which complicates the comparison between these bounds.
2. Contribution: Although the paper establishes a novel connection, its overall contribution is limited. The results are largely direct applications of existing work, and many of the lemmas restate known results. To strengthen the paper, additional discussions and results are necessary to demonstrate a more substantial contribution to the field.