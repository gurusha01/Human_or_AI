* Summary: This manuscript presents a character-level neural machine translation model, capable of translating source and target texts in an end-to-end manner. The encoder learns morphology, while a hierarchical decoder is employed. The authors demonstrate impressive results on various bilingual corpora across different language pairs, and the paper is well-structured and clearly written, with competitive results compared to existing baselines.
* Review:
     - The paper's clarity and precision are notable, making for an enjoyable read with a well-presented analysis.
     - The concept of hierarchical decoders has been previously explored, as seen in [1]; citing such works would enhance the manuscript.
     - While the paper primarily applies existing components to character-level NMT tasks, making the code available is a positive step. However, from a broader machine learning perspective, the novelty and contribution are somewhat limited.
* Some Requests:
 - Including the model sizes in Table 1 would provide additional context.
 - Adding examples of cases where the model fails to translate correctly would offer valuable insights into its limitations.
* An Overview of the Review:
Pros:
    - The manuscript is well-written and easy to follow.
    - The extensive analysis across various language pairs is a significant strength.
    - The experimental results are convincing and well-presented.
    
Cons:
    - The complexity of the model may be a drawback.
    - The paper's primary focus on applying known techniques, rather than introducing new ones, limits its overall novelty.
    - The character-level operation and use of multiple RNNs may result in slower performance compared to word-level models.
[1] Serban IV, Sordoni A, Bengio Y, Courville A, Pineau J. Hierarchical neural network generative models for movie dialogues. arXiv preprint arXiv:1507.04808. 2015 Jul 17.