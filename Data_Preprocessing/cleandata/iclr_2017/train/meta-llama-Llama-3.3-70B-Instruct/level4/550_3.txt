This paper suggests modifying the denoising-autoencoder objective by incorporating an additional term, which is justified by introducing an asymmetry between the encoding and decoding processes. This asymmetry encourages the encoder to produce a compressed and denoised representation of the input data. To prevent a trivial solution, where the encoder scales down the code's magnitude only to be scaled back up by the decoder, the authors recommend using tied weights or a normalized Euclidean distance error. The proposed autoencoder framework bears a strong resemblance to existing autoencoder models that have been previously published. The authors assess their approach using both 2D toy-data distributions and the MNIST dataset. While the work is well-motivated, it appears to be an empirical and incremental refinement of an established concept, rather than a groundbreaking innovation.