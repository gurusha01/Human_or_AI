[ Summary ]
This manuscript introduces a novel variant of the beam search algorithm designed to foster diversity among beam candidates, a well-documented issue affecting both recurrent neural networks (RNNs) and non-neural language models. The beam search methodology often generates candidates that are overly similar, leading to two primary concerns: (1) search error, where the algorithm fails to identify a globally optimal solution due to premature exclusion from the beam, and (2) the production of generic, non-diverse output text.
The proposed algorithm targets the second issue by incorporating a distinct diversity-scoring term into the search objective function. Unlike techniques such as stack decoding and future cost estimation, which are commonly used in phrase-based statistical machine translation (SMT) to mitigate search error, the presented approach focuses on enhancing output diversity rather than optimizing the original objective function's search accuracy.
[ Merits ]
The Diverse Beam Search (DBS) algorithm put forth by the authors possesses several advantages. It offers a potential solution for scenarios where traditional beam search, based on the original objective function, is insufficient due to a weakly trained model, search error, or mismatch between the objective and application goals.
[ Weaknesses ]
However, the comparison of the proposed method to more established approaches, such as stack decoding and future cost estimation, particularly in tasks like machine translation, remains unclear. The authors primarily compare their algorithm against L&J's diverse language models and basic beam search, rather than evaluating it against these traditional methods.
Notably, modifications to the objective function have been explored in the context of neural machine translation. For instance, equation (14) on page 12 of the paper "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation" illustrates such an adaptation.