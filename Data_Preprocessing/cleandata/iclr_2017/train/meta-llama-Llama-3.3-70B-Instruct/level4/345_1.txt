This paper presents an empirical Bayesian approach for learning neural network parameters and their associated priors, utilizing a mixture model prior over the weights to induce a clustering effect in the weight posterior distributions, which are approximated using delta peaks. This clustering effect can be leveraged for parameter quantization and compression of network parameters, yielding compression rates and predictive accuracy comparable to existing methods.
In contrast to earlier work, such as Han et al. (2015), which employs a three-stage process of pruning, clustering, and updating cluster centers, the proposed approach offers a more principled, single iterative optimization process. An initial experiment, described in section 6.1, demonstrates that an empirical Bayes approach without hyper-priors already results in a pronounced clustering effect, setting many weights to zero and achieving a compression rate of 64.2 on the LeNet300-100 model. However, there appears to be a minor error in the text, referencing figure C instead of figure 1.
Section 6.2 outlines an experiment incorporating hyper-priors, where the parameters of these distributions and other hyper-parameters, such as learning rates, are optimized using Spearmint (Snoek et al., 2012). The results, visualized in figure 2, show the performance of various points in the hyper-parameter space. While the text suggests that the best results lie on a line, this interpretation seems somewhat opportunistic given the limited data, and a discussion on the expected relationship would be beneficial. Currently, the results of this experiment lack sufficient interpretation.
The paper also presents results for CNN models in section 6.3, comparing them to recent works by Han et al. (2015) and Guo et al. (2016), achieving comparable compression rates and accuracy. Although the authors acknowledge that their algorithm is too slow for larger models like VGG-19, they provide some preliminary results for this model without comparing to related work. It would be helpful to explain the factors contributing to the slowdown in training compared to standard training without weight clustering and to discuss how the proposed algorithm scales with respect to relevant data and model quantities.
The primary contribution of this paper lies in its experimental results, applying standard empirical Bayesian learning concepts to introduce weight clustering effects in CNN training. While the approach is relatively straightforward, it is notable that it yields results comparable to state-of-the-art, albeit more ad-hoc, network compression techniques. To improve the paper, a clear description of the training algorithm and its scalability to large networks and datasets would be beneficial. Additionally, further discussion on the hyper-parameter search process and how compared methods address the search over hyper-parameters to determine the accuracy-compression tradeoff would be valuable, ideally evaluating methods across different points on this tradeoff.