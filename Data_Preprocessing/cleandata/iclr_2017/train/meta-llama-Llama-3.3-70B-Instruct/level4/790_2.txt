This paper explores an intriguing application of adversarial training in the context of encryption, focusing on the traditional scenario involving Alice, Bob, and Eve, where the goal is for Alice and Bob to securely exchange messages using a shared key, while preventing Eve from accessing the message. The experiments involve a straightforward symmetric 16-bit encryption task and an application related to privacy, with the concepts, ideas, and relevant literature presented in a clear and meticulous manner.
However, I have a significant concern regarding the experiments outlined in Section 3, which I regret not addressing earlier. The scenario appears to involve sharing information <A, B, C, D>, where the goal is to publicly disclose the value of D (e.g., movies watched) without revealing information about C (e.g., gender). In this context, Eve should aim to reconstruct D as accurately as possible without gaining insight into C. Nevertheless, the description in Section 3 indicates that both D and D-public are reconstructed by Bob, which raises questions about why Bob would reconstruct the latter, given that he is not a public entity and is allowed to reconstruct C, which is not tested in this scenario. Furthermore, Eve's efforts to estimate C render this scenario indistinguishable from the one presented in Section 2.
I also have two minor concerns:
1. As previously mentioned, Eve should ideally be more capable than Alice and Bob to compensate for the lack of a shared key. The authors have acknowledged this and plan to include the results of these experiments.
2. In typical encryption scenarios, the key length is usually much shorter than the message length, which could potentially make it easier for Eve to intercept, although the impact on the results is uncertain, even with a sufficiently long key.
I appreciate the innovative application of adversarial training to a distinct domain and believe it has the potential to initiate a fascinating research direction in cryptographic systems or privacy applications. However, the guarantees provided by neural network-based approaches are weak, and it is unclear whether they can be overcome. The application in the privacy setting is confusing, and the symmetric encryption example is not particularly compelling. I would appreciate it if the authors could address the major concern I raised, and I am willing to reconsider my score if this confusion can be resolved.