This study presents a novel approach to efficiently generate an ensemble of deep neural networks that surpass the performance of a single network trained for an equivalent duration. The methodology relies on utilizing a cyclic learning rate, which rapidly converges the model to a local minimum, and then saves a snapshot of the model at this point. The learning rate is subsequently increased to escape the current minimum and move towards a different attractor. By collecting these snapshots throughout a single training session, the authors achieve notable performance comparable to baseline methods and retain some benefits of traditional ensembles, but at a significantly reduced cost.
The paper is well-structured, featuring clear and informative visual aids and tables, as well as compelling results across a diverse range of models and datasets. The analysis presented in Section 4.4 is particularly noteworthy. Furthermore, the provision of publicly available code to facilitate reproducibility is a valuable addition.
To further enhance the study, it would be beneficial to include a more in-depth discussion on the accuracy and variability of each snapshot, as well as a more comprehensive comparison with traditional ensemble methods.
Preliminary assessment:
This work is intriguing, with well-designed experiments and clear exposition, making it a compelling contribution.
Minor observation:
In Figure 5, the axis for lambda ranges from -1 to 2, which seems unusual given that lambda naturally falls within the range of 0 to 1.