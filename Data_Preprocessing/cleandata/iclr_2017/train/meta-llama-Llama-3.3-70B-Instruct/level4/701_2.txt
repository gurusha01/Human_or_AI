This manuscript presents a novel approach to finetuning by augmenting the original network with an additional model while keeping the initial network frozen. The concept is intriguing and offers a complementary perspective to existing training and finetuning methods. However, to further substantiate the proposed approach, comparisons with established baseline methods would be beneficial, such as:
(1) Ensemble methods: The proposed method bears similarities to ensemble techniques, where multiple networks are combined to yield a final prediction. A comparison with ensemble baselines, potentially utilizing multiple source domain predictors with a similar modular setup, would be insightful, as illustrated in Figure 1.
(2) Late fusion comparison: Combining a pretrained network with a finetuned network derived from the pretrained one and performing late fusion could provide an interesting point of comparison.
The argument presented in Section 3.2 and Figure 4, suggesting that finetuning with limited data can compromise performance, is well-taken and provides a rationale for freezing the pretrained network and augmenting it instead of modifying it. While the authors' argument is convincing, additional empirical evidence beyond Figure 4 would strengthen their claim.
Notably, Figure 3 appears to indicate that some module filters may not be converging or are learning non-informative features, as evident in the first two filters of Figure 3(a).
In conclusion, the idea presented is noteworthy, and with further development, it has the potential to contribute significantly to the field. Therefore, I recommend a weak accept, albeit with low confidence, as the experimental section requires more convincing evidence to fully support the proposed approach.