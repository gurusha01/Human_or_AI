This manuscript presents a framework for training models to acquire information, such as through querying, to accomplish a specified task. The authors introduce a suite of tasks designed to achieve this objective and demonstrate the feasibility of using reinforcement learning to train models for these tasks.
A primary motivation for the proposed tasks is the existence of games like 20Q and Battleships, where an agent must pose questions to solve a task. Notably, the authors do not explore these games as potential tasks, aside from Hangman, which is surprising. The task selection process is also not entirely clear. Previous research, such as Navarro et al. (2010), has extensively examined the properties of games like 20Q and human problem-solving strategies. It would be valuable to see how the tasks presented in this work differentiate themselves from existing literature and how humans perform on them. For instance, Cohen & Lake (2016) investigated the 20 questions game, evaluating both human and computer performance, in their paper "Searching large hypothesis spaces by asking questions." A similar study would significantly enhance this paper.
Enabling models to actively seek information to solve tasks is an intriguing yet challenging problem. In this paper, all tasks require the agent to choose from a finite set of clean and informative questions, allowing for simpler analysis but reducing the noise present in more realistic settings.
The manuscript demonstrates that using a standard combination of deep learning models and reinforcement learning, the authors can train agents to solve these tasks as intended. While this validates their empirical setting, it may also reveal limitations in their approach, as the use of relatively simple settings with perfect information and a fixed number of questions may be overly simplistic.
Although it is interesting to see the agents perform well on all tasks, the lack of baselines limits the conclusions that can be drawn from these experiments. For example, in the Hangman experiment, the frequency-based model achieves promising results. It would be interesting to compare this to baselines that utilize letter co-occurrence or character n-gram frequencies.
In summary, this paper explores an interesting research direction and proposes a set of promising tasks to evaluate a model's ability to learn through questioning. However, the current task analysis is somewhat limited, making it difficult to draw conclusive findings. The paper would benefit from a greater focus on human performance on these tasks, simple yet robust baselines, and additional tasks related to natural language, given the motivation behind this work, rather than solely relying on relatively sophisticated models to solve them.