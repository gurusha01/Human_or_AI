This manuscript presents a novel approach to highway networks by introducing a single scalar gating parameter, rather than a full gating tensor, with the assertion that this simplification facilitates easier learning and enables flexible computation utilization.
The underlying concept is straightforward and well-articulated, representing a logical simplification of highway networks that allows for effortless "shutoff" of layers while maintaining a low number of additional parameters. However, several crucial aspects are overlooked in this context. Notably, the paper fails to acknowledge that highway network gates are data-dependent, which potentially offers greater power than learning a fixed gate for all units, independent of the data. Furthermore, a comprehensive comparison with highway networks is lacking, which is necessary to demonstrate that this simplified formulation is indeed more easily learned.
It is unclear whether the authors explored their initial design, where u = g(k)f(x) + (1 - g(k))x, with f(x) being a plain layer rather than a residual layer. Based on the manuscript's arguments, this approach should be viable. The absence of this evaluation raises questions regarding the validity and completeness of the arguments presented. If this design does not yield satisfactory results, it is essential to determine whether the arguments are flawed or incomplete.
The MNIST experiments, which employed fixed hyperparameters, may be misleading if the performance of the different models is dependent on these hyperparameters. Given that this experiment appears to be based on the work of Srivastava et al. (2015), which focused on testing optimization at aggressive depths, it is recommended that the authors conduct a hyperparameter search and refrain from using regularization techniques such as dropout or batch normalization, which are not accounted for in the theoretical arguments underlying the architecture.
In the CIFAR experiments, the improvements over the baseline (wide resnets) are minimal, making it essential to report the standard deviations (or all results) for both cases to determine the significance of the differences observed.
Several questions arise regarding the function g(): Was the ReLU activation function consistently used for g()? Does this not pose potential issues with g(k) becoming zero and failing to recover? Additionally, does this imply that for the wide resnet in Figure 7, most residual blocks are zeroed out, given that k is less than zero?