This paper introduces the Gated Multimodal Unit (GMU) model, a novel approach to information fusion that utilizes multiplicative gates to determine the influence of modalities on unit activation. The authors demonstrate the effectiveness of GMU using a large dataset collected from IMDB, showcasing its promising performance.
The proposed GMU approach appears to be intriguing, and its potential applications extend beyond movie genre prediction, making it desirable to explore its efficacy in more general scenarios. However, the paper's failure to test the algorithm in other applications is a notable limitation. In my opinion, this oversight is the most significant drawback of the paper.
Furthermore, I have concerns regarding the evaluation of information fusion performance. The abstract claims a 30% and 4% improvement in macro f-score performance compared to single-modality models for visual and textual information, respectively. Nevertheless, such improvements are expected when modalities are complementary. The crucial aspect is how the proposed GMU compares to baseline methods. Given the numerous existing fusion techniques, a comprehensive comparison on a single real dataset is challenging. While GMU performs well on the movie dataset, I anticipate that other techniques, such as fine-tuning, dropout, and distillation, may also be beneficial. A comparison with these techniques would be valuable.
Additionally, I would like to see a more in-depth discussion on the connection between GMU and the mixture-of-expert (MoE) model, as both rely on nonlinear gated functions and may suffer from local minima during optimization on small datasets. An exploration of their similarities and differences would be enlightening.
To increase the visibility and appeal of GMU, I recommend that the authors open-source their code and apply their model to more datasets, allowing for a more comprehensive evaluation of its capabilities.