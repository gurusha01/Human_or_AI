This manuscript explores the modeling of graph sequences, introducing Graph Convolutional Recurrent Networks (GRCN) as an extension of convLSTM (Shi et al., 2015) to accommodate data with irregular graph structures at each time step. The authors replace the traditional 2D convolution with a graph convolutional operator, as proposed by Defferrad et al. (2016). Two variants of the GRCN model are presented: Model 1 applies graph convolution solely to the input data, while Model 2 applies it to both the input data and previous hidden states. The approaches are evaluated on two tasks: video generation using the movingMNIST dataset and world-level language modeling using the Penn Treebank dataset.
The results on movingMNIST demonstrate that GRCN 2 outperforms convLSTM; however, this comparison is limited to a single-layer convLSTM, whereas Shi et al. reported improved results with a three-layer convLSTM, which still falls short of GRCN's performance. A more comprehensive evaluation of GRCN in this setting would be beneficial. Moreover, while GRCN shows improvement over convLSTM, its performance on this task appears relatively weak compared to recent works, such as Video Pixel Networks (Kalchbrenner et al., 2016), which contradicts the claim of "good performance" in the conclusion.
In the Penn Treebank experiments, the authors compare Model 1 with FC-LSTM, with and without dropout. However, the results differ from those reported in Zaremba et al. (2014), where a test perplexity of 78.4 was achieved with a large regularized LSTM, outperforming the GRCN's score. Furthermore, subsequent works, such as variational dropout and zoneout, have improved upon Zaremba's results. It would be helpful to clarify any differences in the experimental settings and provide directly comparable results to previous works.
The strengths of the paper include:
- The proposal of an interesting model.
However, the weaknesses outweigh the strengths:
- The overall contribution is relatively incremental, building upon existing works (Shi et al., 2015; Defferrad et al., 2016).
- The results of GRCN are weak compared to previous works, failing to convincingly demonstrate its advantages.