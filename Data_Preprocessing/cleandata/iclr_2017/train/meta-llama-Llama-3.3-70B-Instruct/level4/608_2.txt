The authors suggest replacing the traditional tanh activation function with periodic alternatives, such as sine, in the context of gradient descent-based neural network training. 
This unconventional approach would require substantial empirical evidence to justify its practical viability, as it deviates from established practices. 
The experimental results demonstrate a marginal improvement (from 98.0 to 98.1) for certain MNIST configurations, as well as a significant improvement (nearly doubling accuracy after 1500 iterations) on a specific toy algorithmic task. 
However, it remains unclear whether the proposed activation function is broadly applicable to various algorithmic tasks or if its effectiveness is limited to the two examples presented. 
Consequently, the evidence provided is inadequate to convincingly demonstrate the practical merits of this approach for real-world tasks.