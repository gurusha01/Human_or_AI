This proposed heuristic for enhancing gradient descent in image classification demonstrates simplicity and efficacy, but initially appeared to be more suited for a workshop track presentation. The algorithm's demonstration was confined to a single task, namely CIFAR, and lacked theoretical underpinnings, raising concerns about its generalizability to other tasks.
Having worked with Deep Neural Networks (DNNs) in Natural Language Processing (NLP), some findings in the paper contradicted my own experiences. Specifically, when dealing with architectures that integrate diverse layer types, such as embedding, RNN, CNN, and gating, I observed that ADAM-type techniques significantly outperformed simple SGD with momentum. This is because ADAM-type techniques eliminate the need to search for the optimal learning rate for each layer type. However, I found that ADAM works optimally when combined with Polyak averaging, as it tends to fluctuate substantially between batches.
Following revisions, the authors have substantially enhanced the paper's content, including the addition of experiments on datasets beyond CIFAR. Furthermore, the workshop track has been reclassified as breakthrough work, rendering my initial recommendation inappropriate. Consequently, I have revised my assessment and improved my rating accordingly.