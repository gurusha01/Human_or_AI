The proposed method employs a greedy layer-wise initialization approach for a deep Multilayer Perceptron (MLP) model, subsequently refined through global gradient-descent with dropout. This initialization strategy begins with a randomly initialized sigmoid layer for expanding dimensionality, followed by two sigmoid layers whose weights are set using Marginal Fisher Analysis (MFA), a supervised dimensionality reduction technique based on a neighborhood graph constructed from class labels. The output layer utilizes a standard softmax function.
This approach adds to the existing collection of heuristic layer-wise initialization schemes. However, the choice of initialization strategy, although reasonable, lacks sufficient motivation compared to alternative methods, giving an impression of arbitrariness. The paper's description of the approach is unclear, particularly in explaining MFA, with undefined notations (e.g., the variable "A" in Equation 4 is not properly defined). Additionally, the use of denoising in the model is ambiguous, raising questions about whether an additional denoising objective is trained or if it merely involves input corruption.
The potential inconsistency of applying a linear dimensionality reduction algorithm (trained without sigmoid) and then passing the learned representation through a sigmoid is not addressed. Moreover, the use of sigmoid hidden layers, which are no longer commonly used, raises questions (e.g., why not consider using Rectified Linear Units (RELU)?).
More critically, the experimental comparisons appear to have methodological issues. The paper mentions using default values for the learning rate and momentum, with an arbitrarily fixed epoch of 400 (without early stopping) and L2 regularization set to 1e-4 for some models. It is essential to properly optimize all hyperparameters using a validation set (or cross-validation), including early stopping, separately for each model under comparison (ideally including layer sizes). This is particularly important when dealing with small datasets, where different initialization strategies can act as indirect regularization schemes, requiring careful tuning. The lack of hyperparameter tuning for the alternative models used in comparisons raises serious doubts about the validity of the results.
As it stands, the paper does not make a convincing case for the Marginal Fisher Analysis dimensionality reduction initialization strategy, nor does it provide useful insights into its expected advantages. For image inputs like CIFAR10, it would be beneficial to use qualitative tools, such as visualizing the filters learned by different initialization schemes (back-projected to input space), to gain visual insights into what distinguishes these methods.