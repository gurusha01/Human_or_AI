The authors present a methodology for wild variational inference, where the variational distribution's density function may not be directly accessible. This approach leverages the Stein operator, which transforms a given function into a zero-mean function relative to a specified density, even if the latter is not normalized.
Quality:
The technical derivations appear sound, but the authors' evaluation of their work's strengths and weaknesses seems lacking in candor and thoroughness. It is unclear how the method would perform with high-dimensional distributions, as the logistic regression example only involves 54 dimensions. The scalability of this method to larger models, such as neural networks with a vast number of weights, remains unaddressed. Furthermore, the logistic regression model's simplicity and likely near-Gaussian posterior distribution do not provide a comprehensive test of the method's capabilities, particularly in comparison to more complex posteriors like those found in Bayesian neural networks.
Clarity:
The paper's writing is opaque and difficult to follow, lacking focus due to the introduction of multiple methods, including Stein's variational gradient descent (SVGD), amortized SVGD, kernelized Stein discrepancy (KSD), and the inference network, alongside an overview of Stein's discrepancy. This plethora of techniques obscures the paper's core contributions, making them hard to discern.
Originality:
The novelty of the proposed contributions is questionable, as one of the methods discussed is also explored in the work by Wang and Liu (2016), "Learning to draw samples: With application to amortized MLE for generative adversarial learning." A clear differentiation between these works is not provided.
Significance:
Assessing the importance of the proposed methods is challenging due to the limited scope of the results. The authors only demonstrate their approach on a simple 1D toy problem involving a Gaussian mixture and a 54-dimensional logistic regression model, both of which involve low-dimensional and relatively simple distributions. In the regression case, the posterior's likely proximity to a Gaussian distribution further complicates the evaluation of the method's advantages over simpler variational approaches based on Gaussian approximations, which are not compared in the paper.