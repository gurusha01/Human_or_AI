The authors suggest sampling from Variational Autoencoders (VAEs) using a Markov chain, defined as [zt ~ q(z|x=x{t-1}), xt ~ p(x|z=zt)]. However, the paper is marred by unclear notation, exaggerated claims of novelty, and a lack of acknowledgment of relevant prior work. The visual results fail to convincingly demonstrate the qualitative differences between standard sampling methods and the proposed Gibbs chain. With revisions to clarify the notation, properly contextualize the work within the existing literature, and provide more compelling visual evidence (potentially through scaled-up experiments), this paper could be suitable for a workshop presentation or even a more prominent publication.
Key concerns include:
- The omission of Rezende et al's 2014 VAE paper, which already explores the Markov chain concept presented here.
- The use of non-standard and confusing notation, such as the ambiguous definition of "p(x|z) which is approximated as q(x|z)" on page 1.
- The inconsistent and unclear definition of q(z), referred to as the learned distribution on page 2, while p(z) can also be a learned distribution.
- The incorrect assertion that sampling from q(z) is impossible, when in fact, one can sample x ~ q(x) from the dataset and then draw z ~ q(z|x).
- The lack of clarification on whether the analysis applies exclusively to continuous observed spaces or also to discrete observed spaces.
- Figures 3 and 4 are unconvincing and do not effectively support the claims made in the paper.