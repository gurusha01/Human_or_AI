The authors investigate the concept of "universality" in halting time distributions across various algorithms and settings, examining whether these distributions remain invariant after rescaling to zero mean and unit variance, regardless of stopping parameters, dimensionality, and ensemble.
The notion of universality presented is intriguing, but the paper has several limitations. From a practical standpoint, the actual stopping time may be more relevant than its scaled counterpart. While the discussion on exponential-tailed halting time distributions is a good starting point, its applicability is uncertain. Nevertheless, the paper's findings may still hold theoretical value.
For a conference like ICLR, a more compelling approach would be to compare stochastic gradient descent, momentum, ADAM, and other optimization algorithms across different deep learning architectures, exploring the parameters over which universality holds. Additionally, the impact of different initializations on the halting time distribution warrants investigation, as a sensible initialization could potentially truncate the right tail of the distribution.
However, the paper's clarity is compromised by several issues:
- The abstract mentions "even when the input is changed drastically" without specifying what "input" refers to.
- In the introduction, the statement "where the stopping condition is, essentially, the time to find the minimum" is unclear, as a condition cannot be equivalent to a time; it is likely that the authors meant to say the stopping condition is met when the minimum is reached.
- The introduction of parameters such as dimension N, epsilon, and ensemble E in Section I.1 lacks clarification, making it difficult to understand their significance without referencing later parts of the paper.
- The formulation in Section I.3, "We use x^\ell for \ell \in Z=\{1, \dots, S\} where Z is a random sample from of training samples," is inconsistent, as Z is either a random sample or a set \{1, ..., S\}.
- In Section II.1, the meaning of parameter M, crucial for universality, is not explicitly stated, requiring the reader to dedicate significant time to understanding its relevance.