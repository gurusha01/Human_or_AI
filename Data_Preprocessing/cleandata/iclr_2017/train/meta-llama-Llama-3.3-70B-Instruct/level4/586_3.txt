This paper examines improved training methods for Neural GPU models and explores their limitations. 
The strengths of the paper include:
* Clear and effective writing.
* Extensive investigative work.
* The availability of source code for further research.
However, several weaknesses are noted:
* The title is misleading, as it implies an extension of the Neural GPU model itself, rather than just its training strategies.
* The absence of comparisons with similar architectures, such as Grid LSTM, Neural Turing Machines (NTM), and Adaptive Computation Time, limits the context of the findings.
* The experimental scope is narrow, focusing primarily on toy tasks, and would benefit from inclusion of more diverse tasks.
* The results are predominantly negative, without insight into what specific factors are lacking for the model to be successful, as the underlying causes of these negative outcomes were not thoroughly investigated.
* Certain details, such as the use of gradient noise across all experiments and sequence lengths in figures like Figure 3, are either unclear or omitted.
* The computation steps for NTM are inaccurately represented as O(n), when in fact they are variable.
Ultimately, the paper's findings do not sufficiently clarify the limitations of the model, as it fails to provide a detailed analysis of why the model fails in specific examples and potential solutions to these failures. Despite presenting cases where the model fails, a deeper exploration into the reasons behind these failures and possible remedies is lacking. Additionally, a reference to ICRL 2017 seems pertinent.