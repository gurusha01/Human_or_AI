This paper presents a multi-view learning framework for acoustic sequence representation learning, exploring the application of bidirectional LSTM with contrastive losses. The experimental results demonstrate an improvement over existing work.
Despite my limited background in speech processing, I support the acceptance of this paper due to its notable contributions, including:
- The innovative application of a well-established architecture to a novel domain.
- The introduction of novel, domain-specific objective functions.
- The establishment of new benchmarks tailored for the evaluation of multi-view models.
I encourage the authors to make their implementation publicly available, enabling others to replicate the results, compare their work, and build upon this research.