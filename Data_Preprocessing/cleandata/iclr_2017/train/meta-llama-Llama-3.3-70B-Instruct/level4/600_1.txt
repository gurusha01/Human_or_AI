The paper introduces a group sparse autoencoder, which imposes sparsity on the hidden representation at the group level, with groups formed based on label information (i.e., supervised). The p-th group's hidden representation is utilized for reconstruction, incorporating a group sparsity penalty, thereby enabling the learning of more discriminative, class-specific patterns within the dataset. Additionally, the paper proposes combining both group-level and individual-level sparsity, as outlined in Equation (9).
However, the clarity of the paper could be improved. 
- It is unclear whether only the p-th group's activation is used for reconstruction. If so, does Equation (9) employ all individual hidden representations for reconstruction, or only the subset corresponding to that class?
- Equation (7) appears to be missing a summation over p on the right-hand side, which may be a typographical error.
- It is also uncertain whether the algorithm is end-to-end trainable, as the group sparse CNN seems to be equivalent to a GSA with input data extracted from sequential CNNs (or other pre-trained CNNs).
Further comments include:
- The group sparse autoencoder is a (semi-) supervised method, as it leverages label information to form groups, whereas the standard sparse autoencoder is fully unsupervised. Consequently, it is expected that the group sparse autoencoder learns more class-specific patterns than the sparse autoencoder. A more appropriate comparison would be to autoencoders that incorporate classification into their objective function.
- Figure 3 (b) does not convincingly support the claim that GSA learns more group-relevant features. For instance, the first row contains filters that do not resemble the digit 1 (e.g., the last column appears to be the digit 3).
- Beyond visual inspection, it is unclear whether the proposed algorithm yields improved classification performance on MNIST experiments.
- A comparison to the baseline model is lacking. The baseline model should be the sequential CNN combined with a sparse autoencoder, rather than just the sequential CNN. Additional control experiments are necessary to compare Equations (7)-(9) with varying values of α and β.
A relevant reference that is not cited is:
Shang et al., Discriminative Training of Structured Dictionaries via Block Orthogonal Matching Pursuit, SDM 2016, which explores block orthogonal matching pursuit for dictionary learning with blocks (i.e., projection matrices) constructed based on class labels for discriminative training.