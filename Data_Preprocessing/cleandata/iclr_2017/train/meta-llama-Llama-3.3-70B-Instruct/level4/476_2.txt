This paper explores the effectiveness of shallow non-convolutional networks compared to deep convolutional ones for image classification, assuming both architectures utilize the same number of parameters. 
The authors conducted a series of experiments on the CIFAR10 dataset, revealing a substantial performance disparity between the two approaches, with deep CNNs exhibiting superior performance. 
The experimental design is robust, incorporating a distillation training approach, and the results are presented comprehensively. 
The authors also note that student models can be shallower than their teacher models while maintaining comparable performance, a finding consistent with previous research.
In my interpretation, these results suggest that utilizing deep convolutional networks is more effective, as this model class inherently encodes a form of a-priori or domain knowledge that acknowledges the translation invariance present in images, which is crucial for high-level recognition tasks. 
While the results may not be entirely unexpected, they are not immediately obvious either.
One intriguing aspect, briefly mentioned by the authors, is that among non-convolutional architectures, those with 2 or 3 hidden layers outperform those with 1, 4, or 5 hidden layers. 
I would appreciate it if the authors could provide an interpretation or hypothesis for this phenomenon, as discussing this point further could be beneficial.
I found it unclear why the experiments were restricted to using 30M parameters at most, as none of the experiments in Figure 1 appear to be saturated. 
Although the performance gap between CNNs and MLPs is significant, I believe it would be valuable to extend the experiments for the final version of the paper.
The authors speculate that shallow nets would perform relatively worse in an ImageNet classification experiment. 
I would like the authors to elaborate on their reasoning behind this expectation. 
One could argue that the substantially larger training dataset size could compensate for the choice of shallow and/or non-convolutional architecture. 
Given that MLPs are universal function approximators, architecture choices can be seen as expressions of certain priors over the function space, and in large-data regimes, such priors may be less important. 
This issue could be investigated on ImageNet by varying the amount of training data. 
Additionally, the higher resolution of ImageNet images may have a non-trivial impact on the CNN-MLP comparison compared to the results obtained on the CIFAR10 dataset.
Conducting experiments on a second dataset would also help to validate the findings, demonstrating the extent to which these results generalize across datasets.