This manuscript presents a novel approach to enhancing RNN-based language models by integrating a pointer network, enabling improved handling of rare words. The pointer network can reference words in the recent context, resulting in a predictive output that combines the traditional softmax output with a pointer distribution over the recent words. Additionally, the authors introduce a new language modeling dataset that addresses certain limitations of existing datasets.
My evaluation of this paper is influenced by its similarity to previous work by Gulcehre et al., which employed a comparable approach for machine translation and summarization tasks. The primary differences lie in the architecture, with Gulcehre et al. utilizing an encoder-decoder framework and attention weights to point to input words, whereas this paper uses an RNN and a pointer network that generates a distribution over the entire vocabulary by summing softmax probabilities of words in the recent context. The context vector for the pointing network also differs, which is a direct consequence of the distinct application.
Although the paper discusses the differences between the proposed approach and Gulcehre et al.'s work, I find some claims to be inaccurate or overstated. For instance, the paper states that it allows the pointer component to decide when to use the softmax vocabulary through a sentinel, unlike Gulcehre et al.'s approach, which relies on the RNN hidden state. However, it appears that the proposed model also utilizes the recent hidden state to form a query vector matched by the pointer network to previous words. Clarification on this point would be appreciated.
Furthermore, the paper incorrectly describes Gulcehre et al.'s model as using a switching network instead of a mixture model. In fact, their model is also a mixture model, where an MLP with sigmoid output is used to form a mixture between softmax prediction and input text locations. The statement that the pointer network is not used as a source of information for the switching network is also unclear, and it is uncertain how significant this difference is.
Regarding the proposed dataset, there are other established datasets for language modeling, such as the Hutter Prize Wikipedia (enwik8) dataset and the Text8 dataset. A comparison between the proposed dataset and these existing datasets would be helpful.
I am willing to discuss these points with the authors and reconsider my evaluation if there are any misunderstandings on my part.