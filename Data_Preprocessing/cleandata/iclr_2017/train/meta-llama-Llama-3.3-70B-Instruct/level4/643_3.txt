I appreciate the authors' thorough response and clarifications.
This study presents a novel training approach for online sparse dictionary learning, which accommodates a non-stationary data stream. The primary objective, and challenge, is to develop a model that can adapt to new incoming data while retaining its ability to represent previously encountered data. To address this issue, the proposed method incorporates a mechanism for dynamically adding or removing dictionary atoms, drawing inspiration from adult neurogenesis in the dentate gyrus of the hippocampus.
The paper introduces two key innovations compared to the baseline approach (Mairal et al): (i) "neuronal birth," an adaptive process for increasing the dictionary size, and (ii) "neuronal death," which eliminates redundant dictionary atoms. The latter is achieved through group-sparsity regularization applied to the dictionary atoms, promoting the removal of less useful atoms and controlling dictionary growth.
A significant strength of the paper lies in its connection to the adult neurogenesis phenomenon, a unique and intriguing aspect. The writing is clear and easy to follow, making the paper a pleasure to read.
However, the overall technique may not be entirely novel, as similar concepts have been explored in the past. While the implementation of "neural death" using a sparsity-promoting regularization term is elegant, the "neural birth" process relies on heuristics that measure the dictionary's representation capability for new data. This may be challenging to set, particularly in the presence of non-stationarity or outliers in the incoming data. Nevertheless, the adaptive dictionary size is an interesting and valuable contribution.
The authors may want to consider citing relevant references from the model selection literature, such as the use of Minimum Description Length (MDL) principles for automatic dictionary size selection. Although these ideas may not be directly applicable to the online setting, they are relevant and worthy of mention. For example, Ramirez and Sapiro (2012) present an MDL framework for sparse coding and dictionary learning, which could be a useful reference: Ramirez, Ignacio, and Guillermo Sapiro. "An MDL framework for sparse coding and dictionary learning." IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.