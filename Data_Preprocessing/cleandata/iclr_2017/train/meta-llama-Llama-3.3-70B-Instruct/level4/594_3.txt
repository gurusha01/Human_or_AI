The authors investigate the application of low-rank approximation to matrix multiplication in Recurrent Neural Networks (RNNs), significantly reducing the parameter count. By incorporating a diagonal component, referred to as low-rank plus diagonal, they demonstrate comparable performance to fully parametrized networks across various tasks.
The paper is well-founded, with the sole limitation being certain assertions regarding conceptual unification. For instance, the claim in the conclusion that the framework presented unifies the description of diverse recurrent and feed-forward neural networks as passthrough neural networks is inaccurate, as this general framework is already established within the community and RNNs have been previously represented in this manner.
Setting aside this minor issue, the genuine contribution lies in the successful implementation of low-rank RNNs, yielding results comparable to those of fully parametrized networks. However, the results are not substantially improved, which raises questions about the advantages of utilizing low-rank networks. Consequently, the contribution, while not particularly strong in terms of outcomes, is noteworthy for achieving equivalent results with reduced parameters, a feat that is non-trivial. The studies were meticulously conducted and clearly explained, underscoring the value of this research.