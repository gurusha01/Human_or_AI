This paper introduces a novel problem formulation where only a subset of available Markov Decision Processes (MDPs) have accessible rewards, referred to as "semi-supervised reinforcement learning" (SSRL). The authors propose an approach called semi-supervised skill generalization (S3G), which leverages the maximum entropy control framework to address SSRL. The methodology is straightforward, involving an Expectation-Maximization (EM) algorithm that iteratively estimates a reward function and fits a control policy using this reward, essentially working with partial labels. Experimental evaluations are conducted on four MuJoCo tasks: obstacle, 2-link reacher, 2-link reacher with vision, and half-cheetah.
The paper is well-structured and clear, with the appendix providing additional context. However, some implementation details seem to be missing, which could hinder the full reproducibility of the experiments, though the authors have committed to providing the code. The connection to inverse reinforcement learning is appropriately established, but there appears to be a lack of reference to off-policy policy learning. For instance, the term \(\tau \in D_{samp}\) in equation (3) might benefit from variance reduction techniques, such as those employed in TB(\(\lambda\)) [Precup et al. 2000] or Retrace(\(\lambda\)) [Munos et al. 2016].
The experimental section presents convincing results, but further clarification on the generalization capabilities testing would be beneficial. Specifically, quantifying the range of settings used, which is described as a superset of the unlabeled and labeled MDPs, or correcting this to "union" if applicable, could provide insight into the poor performance of the "oracle" method on the obstacle and 2-link reacher tasks. This could also reinforce the explanation that the true reward function in the obstacle task is insufficiently shaped for learning in unlabeled MDPs, leading to poor performance by reward regression and oracle methods.
A correction is noted on page 4, where "5-tuple \(M_i = (S, A, T, R)\)" should be identified as a 4-tuple.
Overall, the paper is deemed good and sound. However, the completeness of parallels and references to prior work is questionable, which leads to a confidence score of 3.