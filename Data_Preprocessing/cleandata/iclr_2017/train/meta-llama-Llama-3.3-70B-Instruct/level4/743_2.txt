This paper presents several issues that undermine its validity.
On page 2, the statement "[…] for sufficiently large N and eps = eps(N)" is problematic due to the dependence of epsilon on N.
The explanation on page 3, "Universality is a measure of stability in an algorithm […] For example […] halting time for the power method […] has infinite expectation and hence this type of universality is not present," is flawed. An algorithm's naivety is determined by the existence of better alternatives, not by its halting time characteristics. Furthermore, the universal property is more restrictive than having a finite halting time expectation, and its desirability requires a more comprehensive demonstration. This paragraph's conclusions are also overly broad, as they are based on a single algorithm.
A comparison of Eq 1 and figures 2, 3, 4, and 5 reveals a discrepancy. According to Eq 1, universality implies that centered/scaled halting time fluctuations can be approximated by a distribution dependent only on A, not on epsilon, N, or E. However, the experiments only vary E, never testing the validity of the approximation with varying epsilon or N.
The definitions of ensembles/distributions parameter E and algorithm A are unclear, particularly in relation to their common usage. In the optimization setting, the functional form of the landscape function is part of A, but the scope of this definition is ambiguous, especially for computations with unknown landscape functional forms.
The conclusion claims that the paper "attempts to exhibit cases" where five questions can be answered robustly and quantitatively. However, the paper falls short in addressing these questions.
Question 1, "What are the conditions on the ensembles and the model that lead to such universality?" is not answered robustly, as the moment-based indicator is only demonstrated for one example of non-universality, involving a single algorithm and failure type.
Question 2, "What constitutes a good set of hyperparameters for a given algorithm?" is not answered convincingly, as the proposed method for choosing hyperparameters relies on observing universality, which is only demonstrated for one algorithm and failure type.
Question 3, "How can we go beyond inspection when tuning a system?" is too vague and general, and the paper does not provide a robust or quantitative answer.
Question 4, "How can we infer if an algorithm is a good match to the system at hand?" is not answered convincingly, as the paper fails to demonstrate that universality is a reliable or robust way to approach the studied algorithms, and the suggested generalization to all systems and algorithms is unfounded.
Question 5, "What is the connection between the universal regime and the structure of the landscape?" is also too vague and cannot be answered robustly or quantitatively, due in part to the unclear definitions of A and E.
The conclusion's claim that the paper validates the presence of universality in nearly all sensible computations is unsubstantiated. The paper does not properly test the presence of universality, the loss of universality in non-sensible computations, or the applicability of its findings to a broad range of computations, instead focusing on a limited set of specific algorithms.