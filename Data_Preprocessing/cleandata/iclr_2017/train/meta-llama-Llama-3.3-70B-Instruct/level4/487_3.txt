The results presented appear to be satisfactory, but the proposed baselines are suboptimal. 
For example, in Table 2, the misclassification rate for the "784-1024-1024-1024-10" configuration with floating-point fully connected (FC) layers is 1.33%, which is significantly higher than the approximately 0.8% that can be achieved with this topology. I would expect to see more substantial compression levels compared to state-of-the-art results or more competitive baselines. In fact, I can achieve a misclassification rate of 0.6% using two FC hidden layers.
Regarding the CIFAR-10 experiments, I find it puzzling that "Sparsely-Connected 90% + Single-Precision Floating-Point" performs worse than "Sparsely-Connected 90% + BinaryConnect", suggesting that binary precision is superior to floating-point precision in this case.
In my opinion, the experiments do not adequately utilize techniques that can be readily applied to floating-point but not binary precision, such as Gaussian noise or other regularization methods. Consequently, I believe the comparison between floating-point and binary precision is unfair. This criticism also applies to the original papers on binary and ternary precision.
Notably, with a standard convolutional network and floating-point precision, it is possible to achieve an error rate below 9%. This further highlights the issue with the baselines presented.
The authors' response has not alleviated my concerns. I still maintain that the techniques should be evaluated in more challenging scenarios to provide a more comprehensive assessment.