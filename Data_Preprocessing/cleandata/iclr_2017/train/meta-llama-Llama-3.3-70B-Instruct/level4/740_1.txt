The manuscript proposes a parallelization architecture for optimizing nested functions using the method of auxiliary coordinates (MAC), as introduced by Carreira-Perpinan and Wang (2012). This approach involves decomposing the optimization process into training individual layers and updating auxiliary coordinates. The authors apply this method to binary autoencoders, distributing the data across multiple machines and allowing parameters to be shared between them. The reported speedup factors are relatively impressive, particularly for larger datasets, and a theoretical performance model is presented that aligns with experimental results.
However, a major concern is that despite being presented as a general framework for nested functions, the experiments are limited to a narrow family of models, specifically binary autoencoders with linear or kernel encoders and linear decoders, and only consider two components. Although the speedup factors are promising, their significance is difficult to assess due to the limited scope of the models used, which are not widely studied or applied in the field. To enhance the impact of this work, I recommend that the authors apply this framework to more general architectures and problems.
Several questions arise from this work:
1. Can this framework be applied to generic multi-layer neural networks, and if so, what experimental results can be expected?
2. What are the implications of extending this framework to more than two components, including non-linear components, and how would this affect performance?
3. It would be beneficial to include plots illustrating performance over time for different setups, demonstrating the speedup after convergence, rather than just focusing on speedup factors per iteration. For instance, increasing the mini-batch size may improve speed per iteration but potentially hinder convergence speed.
4. Was the scenario considered where the dataset is too large to store on multiple machines simultaneously, along with the auxiliary variables?
The paper frequently cites an ArXiv manuscript by the authors with the same title. To make the paper self-contained, I suggest incorporating any necessary supplementary material into the appendix.
In its current form, due to the limited application of the framework beyond binary autoencoders, I believe this paper may not appeal to a broad audience at ICLR, leading me to suggest a weak reject.