This manuscript proposes a methodology to decrease the memory requirements of a neural network, albeit with a slight increase in computational expense. The approach builds upon the HashedNets concept introduced by Chen et al. (ICML'15), where neural network parameters are mapped to smaller memory arrays using hash functions, which may result in collisions. Instead of training the original parameters, the compressed memory array elements are trained using back-propagation, given a specific hash function. The authors introduce several novel techniques, including: (1) sharing the compression space across the neural network layers, (2) utilizing multiple hash functions to mitigate collision effects, and (3) employing a small network to combine elements retrieved from multiple hash tables into a single parameter. Figure 1 effectively illustrates the distinction between the proposed approach and HashedNets.
The strengths of the paper include:
+ The introduction of innovative and potentially useful ideas.
+ The provision of theoretical justification for the use of multiple hash functions.
+ Experimental results consistently indicating the superiority of the proposed MFH approach over HashedNets.
However, there are also some weaknesses:
- The computational cost appears to be higher than that of HashedNets, yet this aspect is not thoroughly discussed.
- The immediate practical applications of the paper are unclear, given that alternative pruning strategies may perform better and offer faster inference.
Despite these limitations, I believe this paper contributes to the deep learning community by exploring ways to share parameters across neural network layers, potentially leading to more interesting future research. I recommend acceptance, contingent upon the authors addressing the following comments:
Additional comments:
- A discussion on the computational cost of both HashedNets and MFH for fully connected and convolutional layers would be beneficial.
- To ensure reliability, experiments should be run multiple times, and the average and standard error should be reported.
- For completeness, U1 results should be added to Table 1.
- The duplicate entry of U4-G3 with different numbers in Table 1 should be corrected.
- Some sentences require grammatical improvement to enhance the overall writing quality.