This paper assesses the capability of two unsupervised learning models to acquire a generalizable physical understanding that governs the stability of a block tower. The models in question are: (1) a model designed to predict the final state of the tower given its initial state, and (2) a model that predicts the sequence of states the tower undergoes over time, also given its initial state. The generalizability of these models is evaluated by training them on towers composed of a specific number of blocks and then testing them on towers with a different number of blocks.
The paper has several strengths, including its exploration of an innovative method for evaluating representations based on their generalizability to out-of-domain data, diverging from standard approaches that utilize train and test data from the same distribution. Additionally, the experiments demonstrate that the predictions of deep unsupervised learning models on such out-of-domain data appear to be beneficial, even though these models were not explicitly trained for this purpose.
However, there are also several weaknesses. One concern is that, based on Figure 4, models trained on towers with 3 blocks seem to "generalize" to towers with 4 and 5 blocks. Yet, it's plausible that these models focus solely on the bottom 3 blocks of the 4 or 5 block towers to determine stability, which could work correctly a significant portion of the time. This raises the possibility that the models might be overfitting to 3-block towers rather than genuinely generalizing the physics involved. To substantiate the claim that the features generalize, more rigorous controls are necessary. For instance, testing a 3-block model on a 5-block test set but making only the 4th or 5th block unstable could provide clearer insights. If the model performs well under these conditions, it would be a stronger indication of true generalization.
The experimental analysis appears somewhat preliminary and could be enhanced. Visualizations of the final state for models trained on 3 blocks but tested on 5 (and vice versa) would be particularly useful in understanding whether generalization is indeed occurring. While the discriminative objective provides some insight, it might obscure aspects of physical realism that are crucial to test. Furthermore, Figures 1 and 2 lack clarity on whether the models are being tested on the same number of blocks they were trained on.
The task of predicting the final state seems to be a binary taskâ€”determining whether to remove blocks and replace them with a gray background. Predicting where blocks land in the event of a fall is likely challenging, even for humans, due to the significant impact of small perturbations on the final state. To develop a generalizable physics model, a high frame rate sequence prediction task might be beneficial. Currently, the video is subsampled to only 5 time steps, which may not capture the full dynamics of the tower's behavior.
In terms of quality, a more detailed analysis and careful selection of testing conditions could significantly enhance the paper and strengthen its conclusions. The paper is well-written and easy to follow in terms of clarity. The setting explored is novel, contributing to the originality of the work. The significance of the paper lies in its valuable addition to the growing body of work on transferability and generalizability as evaluation methods for unsupervised learning. However, more detailed experiments and analysis are required to make the paper significant enough for publication.
Minor comments include the use of the acronym IPE without expansion, a strong dependence on data augmentation which seems unnecessary given the synthetic nature of the dataset, and suggestions for improving the presentation of Table 3, such as rearranging it into a 9x3 grid for easier reading.
Overall, the direction of this work is excellent, and the preliminary results are promising. Nevertheless, additional controls and detailed analysis are necessary to draw strong conclusions from these experiments.