I appreciated the opportunity to review this submission, which presents a compelling exploration of transfer learning in the absence of target domain data, referred to as "extrapolation" by the authors. 
To the best of my knowledge, this is a relatively unexplored area, with few studies investigating the role of system dynamics knowledge in such scenarios. The paper convincingly demonstrates the importance of this knowledge and showcases the potential of the proposed EQL model through its experiments. Furthermore, I find the EQL model intriguing from an interpretability standpoint, a crucial aspect for data analysis in scientific domains.
Questions and comments:
1. Multiplication units: Although neural networks can represent multiplication via the universal approximation theorem, I am still unclear about the necessity of multiplication units in this context. Is it due to their ability to generalize better when the training data is not fully representative of future scenarios?
2. Comparing EQL to polynomial fitting: It appears that the number of layers in EQL might be analogous to the degree of a polynomial. If the underlying dynamics can be polynomially represented, what distinguishes fitting a polynomial (with degree selection) from fitting an EQL (with layer selection)? The experiments highlight the importance of basis function selection, implying that prior knowledge of the equation's form is required.
3. Error bounds: Ben-David et al. (2010) established error bounds for hypotheses trained on source data and tested on target data. I wonder if the EQL model can achieve superior error bounds compared to existing methods.
4. Comparison to uncertainty-based methods: Can the authors comment on how their approach compares to methods that model extrapolation data with uncertainty, providing a more comprehensive understanding of the EQL model's strengths and limitations?