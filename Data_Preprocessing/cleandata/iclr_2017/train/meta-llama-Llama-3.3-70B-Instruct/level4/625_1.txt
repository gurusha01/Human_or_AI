This manuscript introduces a reinforcement learning framework that utilizes natural-language input to enable a meta-controller to select subtasks, which are then communicated to a subtask controller for the execution of primitive actions based on the assigned subtask. The primary objective of this approach is to enhance the scalability of reinforcement learning agents in complex tasks.
The subtask controller employs a multi-layer perceptron, incorporating an analogy-making regularization, to embed subtask definitions into vector representations. These subtask vectors are then integrated with input data at each layer of a convolutional neural network (CNN). The outputs of the CNN, conditioned on both the observation and the subtask, are subsequently fed into two separate multi-layer perceptrons (MLPs): one for computing action probabilities within the policy, characterized by an exponential falloff of MLP outputs, and another for determining termination probability, using a sigmoid function of the MLP outputs.
The meta-controller processes a sequence of instructional sentences to generate a series of subtask arguments, which may not maintain a one-to-one correspondence with the input sentences. A context vector is derived from the observation, the embedding of the preceding sentence, the previous subtask, and its completion status, using a CNN. The subtask arguments are then computed from this context vector through additional mechanisms, involving the retrieval of instructions from memory pointers and hard or soft decisions regarding subtask updates.
The training protocol involves policy distillation coupled with actor-critic training for the subtask controller, and actor-critic training for the meta-controller, with the subtask controller parameters held constant.
The efficacy of the proposed system is evaluated in a grid world environment, where the agent navigates and interacts with various items and enemy types. Comparative analyses are conducted against two baselines: a flat controller that does not utilize a subtask controller, and a subtask control approach that simply concatenates the subtask embedding to the input, with and without the analogy-making regularization.
Assessment:
The proposed architectural design appears plausible, although the rationale behind the specific method of combining subtask embeddings within the subtask controller is not entirely clear, leaving room for questioning whether this approach is optimal.
A significant concern is that the grid world setting, with its limited 10x10 size, does not adequately represent a large-scale task, which was a primary motivation for this research. This limitation is disappointing, as it undermines the potential impact of the study.
Furthermore, the methodology is not benchmarked against current state-of-the-art alternatives, a omission that is particularly problematic given the use of a non-standard test environment. As a result, it is challenging to contextualize the performance of the proposed method within the broader landscape of existing works, based solely on the results presented.