This paper explores the concept of identity parametrization, also referred to as shortcuts, where each layer's output takes the form of h(x) + x, rather than just h(x), a technique that has demonstrated effectiveness in practice, as seen in ResNet. The paper's discussions and experiments are noteworthy, and the following comments are offered:
- In Section 2, the examination of linear networks is intriguing in its own right. However, it remains unclear how the insights gleaned from linear networks might translate to non-linear networks. For instance, the proof that every critical point is a global minimum is presented, but a discussion on the relationship between linear and non-linear networks would be beneficial to add context.
- Section 3 presents an interesting construction, but it is observed that the expressive power of residual networks is within a constant factor of that of general feedforward networks. Given the existing body of work on the finite sample expressivity of feedforward networks, it is not immediately apparent why a separate proof is necessary. Clarification on this point would be appreciated.
- The experiments in Section 4 are commendable, with the choice of random projection on the top layer being particularly noteworthy. However, since this choice is combined with all-convolutional residual networks, it becomes challenging for the reader to discern the individual effects of each component. Therefore, it is suggested that the results for all-convolutional residual networks with a learned top layer, as well as for ResNet with a random projection on the top layer, be reported to provide a clearer understanding.
Minor observations include:
1. The assertion that Batch Normalization can be reduced to an identity transformation is not entirely convincing, and its mention in the abstract without thorough discussion may not be advisable.
2. On page 5, above assumption 3.1, the condition x^(i) = 1 implies ||x^(i)||_2 = 1, which could be explicitly stated for clarity.