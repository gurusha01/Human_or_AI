This paper presents an extension of the multiplicative RNN [1] by applying a reparametrization trick to the weight matrices of the LSTM, introducing several novel techniques. 
Although the proposed methods are intriguing, their individual impact appears to be limited. For example, the modification in Eq. (16), which involves multiplying the output gate within the activation function to mitigate saturation issues in logistic sigmoid or hyperbolic tangent, does not seem crucial. Additionally, sharing $m_t$ across different gating units and cell-state candidates results in only a 1.25-fold increase in model parameters. The authors also employ a variant of RMSProp with an extra hyperparameter $\ell$, scheduled over training time. It would be beneficial to apply these techniques to other baseline models to demonstrate their individual contributions to improvement.
The new architectural modifications to the LSTM, combined with these techniques, yield underwhelming performance. It is unclear why the authors did not explore batch normalization, layer normalization, or zoneout in their models. Were there any issues with implementing these regularization or optimization techniques?
The connection between dynamic evaluation and fast weights in Section 4.4 is misleading. Dynamic evaluation is distinct from fast weights, as it utilizes error signals and gradients to update weights, potentially increasing effectiveness but limiting its scope to conditional generative modeling. However, this assumption is problematic, as test label information is not always available during inference. While some applications, such as stock prediction or weather forecasting, may provide label information at test time, others, like machine translation, do not. It would be fair to apply dynamic evaluation to all baseline models for a comprehensive comparison with the proposed mLSTM's BPC score of 1.19.
The overall quality of the work is acceptable, but the novelty is limited. The proposed model's performance is often inferior to other methods, except when dynamic evaluation is used. However, dynamic evaluation can also enhance the performance of other methods, which diminishes the novelty of the proposed approach.
[1] Ilya et al., "Generating Text with Recurrent Neural Networks", ICML'11