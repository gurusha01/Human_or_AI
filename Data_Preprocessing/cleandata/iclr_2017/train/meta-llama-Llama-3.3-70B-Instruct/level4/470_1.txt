This manuscript proposes a modified variational auto-encoder (VAE) framework that incorporates stochastic latent dimensionality by utilizing an inherently infinite prior, specifically the stick-breaking process. The authors pair this with tailored inference, employing the Kumaraswamy distribution as an approximate variational posterior, resulting in the SB-VAE model, which also includes a semi-supervised extension akin to the original VAE.
Given the current interest in VAEs and the pursuit of automatic "black-box" inference, this work offers a unique perspective by exploring bespoke solutions for new models. The authors' effort to provide efficient inference for the SB-VAE highlights the potential of the Kumaraswamy distribution, which has seen limited use in machine learning. This approach contrasts with parallel work, such as that by Blei's lab, aiming for more general solutions.
Although the paper is generally well-organized, some sections were confusing due to the intermingled discussion of model specification and inference. Pre-review questions helped clarify these points.
Two primary concerns regarding the methodology and motivation of this paper stand out. Firstly, directly conditioning the model on the stick-breaking weights seems unconventional, initially suggesting a mixture probabilistic model. However, the authors address this, explaining their investigation into using a base distribution G_0 and acknowledging the challenge. The experiments demonstrate competitiveness, suggesting the relaxation is useful, with potential for future extensions as mentioned by the authors.
Secondly, the paper could more effectively motivate why reformulating the VAE as an SB-VAE is beneficial. While the non-parametric property of the prior may lead to better capacity control, this advantage, along with potential others, is not thoroughly explained or demonstrated. A comparison with a dropout approach or a more in-depth discussion related to dropout could enhance the motivation.
Overall, this is an interesting paper that would be suitable for presentation at ICLR, offering a novel approach to VAEs with potential implications for machine learning.