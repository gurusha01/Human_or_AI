This manuscript presents a method for pruning complete groups of filters in Convolutional Neural Networks (CNNs), effectively reducing computational costs without introducing sparse connectivity. This achievement is significant as it enables the acceleration and compression of neural networks while still allowing the utilization of standard fully-connected linear algebra routines.
The authors report a 10% improvement in performance on ResNet-like architectures and ImageNet datasets, which, although notable, could potentially be matched through more optimized network design. A more comprehensive comparison involving novel network architectures would have strengthened the study, although it is acknowledged that such an undertaking can be time-consuming.
Overall, the paper is well-written and contributes valuable insights, making it a worthwhile contribution to the field.