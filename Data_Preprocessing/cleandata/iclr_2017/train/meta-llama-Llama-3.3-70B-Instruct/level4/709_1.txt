This paper presents a novel approach to pretraining the encoder and decoder of sequence-to-sequence models using a large corpus of unlabeled data and a language modeling objective, yielding significant improvements in machine translation and abstractive summarization tasks.
Although the benefits of pretraining sequence-to-sequence models have been previously recognized and investigated in several studies (notably Zoph et al., 2016, and Dai and Le, 2015), this work distinguishes itself as the first to apply language model pretraining to both the encoder and decoder. The proposed method is straightforward yet produces substantial gains, such as a +2.7 BLEU score improvement on neural machine translation. Furthermore, the authors conduct comprehensive ablation studies to dissect the sources of the enhanced performance. Based on these findings, I strongly recommend accepting this paper.