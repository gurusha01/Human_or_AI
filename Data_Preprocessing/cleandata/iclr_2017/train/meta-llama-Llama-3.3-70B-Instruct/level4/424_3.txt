The authors demonstrate that smoothing a highly non-convex loss function can facilitate the training of deep neural networks. 
This paper is well-structured and clearly presented, with a thorough analysis of the concept and persuasive experimental results, leading to a recommendation for acceptance. To further strengthen this recommendation, additional experiments would be beneficial. Specifically, a comparison of the proposed smoothing technique to the insertion of probes at various network layers would be insightful. Furthermore, investigating its performance on challenging optimization tasks, such as algorithm learning, would be of interest. For instance, the "Neural GPU Learns Algorithms" paper required weight relaxation in different RNN layers to achieve optimization - it would be intriguing to explore whether this could be circumvented using the authors' smoothing approach.