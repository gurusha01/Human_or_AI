This paper proposes a novel approach to learning directed generative models and domain adaptation by introducing a method that matches central moments to align two probability distributions, offering a fresh perspective in the realm of scoring rules. The technique presented is straightforward and demonstrates high effectiveness, particularly in domain adaptation scenarios. 
The Central Moment Discrepancy (CMD) appears to be a simple yet effective solution for the domain adaptation problem, boasting computational ease and stability with respect to tuning parameters, especially when compared to Maximum Mean Discrepancy (MMD). Although initial skepticism arose, particularly given the limited number of moments (K=5) used in the experiments, the results are quite promising. A natural extension of this work would be to explore its application in training generative models, which, while beyond the current scope, presents a compelling direction for future research.
Detailed feedback is provided below:
To potentially accelerate MMD computations, utilizing a random Fourier basis, as explored in "Fastmmd: Ensemble of circular discrepancy for efficient two-sample test" by Zhao and Meng (2015), or employing linear time estimators, such as those discussed in "A Kernel Two-Sample Test" by Gretton et al. (2012), could be beneficial. Although comparison against these optimized approaches is not necessary since the full MMD is already considered, citing them would enhance the paper's comprehensiveness.
The submission "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy" by Sutherland et al. to ICLR 2017 is also noteworthy, as it delves into optimizing the kernel used in MMD, making it a valuable reference for Section 3.
The assumption of distributions having independent marginals could be limiting; thus, understanding the implications of this assumption is crucial. 
Furthermore, given that the sample complexity of MMD is heavily influenced by the dimensionality of the input space, it would be insightful to explore the sample complexity of CMD. While the results in Figure 4 suggest a relative insensitivity, the behavior with high-dimensional output spaces, such as those encountered in generative models, remains to be fully understood.
Concerns regarding the numerical stability of central moments at higher orders during backpropagation are raised, despite the experimental results not indicating significant issues. The potential for large values of ck(X) for k >= 3 warrants further discussion, especially in the context of individual terms within the objective function, notwithstanding the stability alleviated by Proposition 1.
Figure 3 is somewhat cluttered, making it challenging to discern the positive impact of the CMD regularizer beyond the mouse class. Simplifying the visualization by reducing the number of classes could enhance clarity.
Clarification on the natural geometric interpretations of K=5 would be beneficial. It is unclear whether the choice of K=5 is based on the well-studied nature of moments up to this order or other considerations. References supporting the geometric interpretations of moments up to K=5, as well as the rationale behind the lack of natural geometric interpretation for K >= 6, would be appreciated.
Lastly, Figure 4 would benefit from the inclusion of a legend to improve readability.