This study presents an approach to iteratively refine a sentence generated by another machine translation system, specifically a phrase-based system, using a neural network. The network takes the source sentence and a window of surrounding words as input and predicts the target word. However, during testing, the actual generated words replace the gold standard words. Although this research area is intriguing, the proposed methodology and supporting experimental evidence are not convincing.
The current framework significantly limits the model's capability, essentially restricting it to basic word substitutions, such as replacing a single word without altering the sentence structure (e.g., transforming "I went to the fridge even though I was not hungry" into "Although I was not hungry, I went to the fridge" is beyond its capacity). This limitation is underscored by the average editing of merely 0.6 words, indicating a lack of substantial revision.
Specific observations include:
- Comparing improvements when using a neural system as the baseline model could yield valuable insights.
- The decision to limit T^i and L(y^{-i|k}) to a 2k-word window seems unusual, as it implies the model is unaware of the generated content outside this narrow scope when making decisions about the i-th word.
- The concept of altering individual words based on local scores appears counterintuitive, especially given the availability of the full generated sentence. Employing a global sentence-level score could facilitate more comprehensive edits and allow for non-greedy search strategies.
- A comparison with a model that simply re-ranks the k-best outputs would be beneficial.
- Instead of editing, considering an encoder-decoder model that takes x and yg as inputs to generate yref, with the ability to attend to both x and y_g during decoding, might be worthwhile.
Minor observations:
- The concept of iteratively improving generated text has also been explored in other studies, suggesting a broader context for this research.