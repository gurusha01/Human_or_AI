This manuscript proposes a Markov chain-based approach to approximate a probability distribution over latent variables Z, facilitating the generation of samples from a data distribution by modeling P(X | Z).
However, in its current state, the paper is not suitable for acceptance due to several key concerns:
1. The lack of quantitative assessment is a significant issue. Although the authors provide samples generated by their model, these alone are insufficient for evaluating the model's performance, as further discussed in point 2 below.
2. The model description is overly ambiguous, requiring considerable interpretation to discern the authors' intended meaning. Specifically, the notation Q(Z) is unclear - does it represent the true posterior distribution P(Z | X)? Furthermore, the generative model's formulation is not explicitly stated; typically, it would be defined as P(Z)P(X|Z). In the context of Variational Autoencoders (VAEs), a variational approximation Q(Z | X) is used to approximate the true posterior P(Z | X). It is essential to clarify whether the proposed model aims to sample from the true posterior P(Z | X).
Additional comments and suggestions:
1. The decision to incorporate additive noise into the input appears questionable and warrants justification. What motivated this design choice?
2. Methods that learn transition operators often lend themselves well to semi-supervised learning approaches that leverage data augmentation. To strengthen their manuscript, the authors are encouraged to evaluate their model on semi-supervised learning benchmarks, exploring its potential in this context.