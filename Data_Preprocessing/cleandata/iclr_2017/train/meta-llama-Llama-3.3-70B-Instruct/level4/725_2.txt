This manuscript establishes a link between Deep Neural Networks (DNNs), simplified stochastic neural networks (SFNNs), and proposes utilizing DNNs as initialization models for simplified SFNNs, demonstrating promising outcomes on various small-scale tasks.
The authors' exploration of the relationships between these models is noteworthy, particularly the novel connection they draw between ReLU-activated DNNs and simplified SFNNs. However, the relationship between sigmoid-activated DNNs and simplified SFNNs appears to parallel the mean-field approximation, a concept well-established in the literature for decades.
A primary concern arises regarding the practical applicability and efficacy of the proposed methodology when tackling complex, real-world tasks characterized by large training datasets. While the stochastic units may indeed enhance generalization performance in scenarios involving small training sets, it remains uncertain whether this approach will yield comparable benefits in more substantial and challenging contexts.