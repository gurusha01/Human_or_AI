This study introduces a innovative ternary weight quantization method, where weights are quantized to either 0 or one of two layer-specific values learned during training. Notably, these quantized values are distinct and learned stochastically alongside other network parameters, differing from previous approaches. This method yields impressive quantization results, matching or surpassing the performance of full-precision networks on CIFAR10 and ImageNet.
Strengths:
- The paper is well-structured and clearly presents the algorithm.
- The approach demonstrates effectiveness in experiments, achieving good compression without sacrificing performance, and sometimes even improving it.
- The analysis of sparsity and its evolution during training is insightful, although it is unclear if any practical conclusions can be drawn from it.
Some points to consider:
- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. To validate this assumption, it would be helpful if the authors provided the average activation sparsity for each network. Even if the assumption is not entirely accurate, similar average activation values between networks would make the comparison more compelling.
- In Section 5.1.1, the authors propose using a fixed threshold parameter (t = 0.05) across all layers, allowing for varying sparsity due to the relative magnitude of layer weights. This concept is further explored in Section 5.1.2, suggesting that additional sparsity can be achieved by using layer-specific threshold values. However, it is unclear how these values are determined. Does this multi-threshold approach appear in any of the tables or figures? If not, consider adding it for clarity.
- The authors claim that quantized weights acting as "learning rate multipliers" during backpropagation is a benefit of using trained quantization factors. However, it is not explicitly stated why this is advantageous.
- Figure and table captions lack descriptive detail, which could be improved for better understanding.
Preliminary Rating:
This paper presents interesting results, but its novelty is somewhat limited.
Minor notes:
- Table 3 lists FLOPS instead of energy for the full-precision model; please clarify the reason for this.
- Section 5 is titled 'Speeding Up', but could be more descriptive.
- There is a figure reference error in the last line of Section 5.1.1.