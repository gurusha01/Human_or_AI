This paper presents a novel approach to learning a customized optimizer for a specific class of optimization problems, which can be thought of as a model, such as logistic regression, in the context of training machine learning algorithms. The authors have effectively formulated this as a reinforcement learning problem, utilizing guided policy search to train a neural network that maps the current state and history to a step direction and magnitude. Overall, this is a great concept and a valuable contribution to the rapidly expanding meta-learning literature. However, there are some aspects that could be improved to make this a more robust paper.
One of my primary concerns is that the authors claim their method learns the regularities of an entire class of optimization problems, rather than exploiting regularities in a specific set of tasks. I find this distinction to be somewhat unclear. For instance, when learning an optimizer for logistic regression, the authors appear to suggest that training on a randomly sampled set of logistic regression problems enables the model to learn about logistic regression in general. Nevertheless, I am not convinced by this argument, as the randomly sampled data itself is biased. As stated in the paper, "The instances are drawn randomly from two multivariate Gaussians with random means and covariances, with half drawn from each." It seems that the optimizer is trained to optimize instances of logistic regression given this specific family of training inputs, rather than logistic regression problems in general. A straightforward experiment to demonstrate the method's broader applicability would be to repeat the existing experiments with test instances drawn from a completely different distribution. It would be even more intriguing to investigate how the results change as the test distribution deviates further from the training distribution.
I would appreciate it if the authors could provide insight into their choice of architecture, specifically the use of one layer with 50 hidden units and softplus activations. Why was this particular configuration chosen, and not, for example, 100 units, 2 layers, and ReLUs? Presumably, this is intended to prevent overfitting, but given the limited capacity of the network, it is essential to understand how these results generalize when the dimensionality of the input space exceeds 2 or 3.
Visualizing the policy learned by the network on a 2D function using a contour plot would be highly informative. It would be interesting to compare the steps taken by the learned policy on a random problem instance to those of other hand-engineered optimizers.
In general, I believe this is a fascinating paper with a significant methodological contribution. My primary concern is that the results may be overstated, as the problems addressed are still relatively simple and constrained. However, if the authors can demonstrate that this approach yields robust policies for a broad range of problems, that would be truly remarkable.
Some minor notes are included below.
In Section 3.1, should the optimal policy be denoted as \piT^ instead of the current notation of \pit^ and \pi^*?
Are the problems considered in this paper noiseless, meaning that the state transition given an action is deterministic? It would be highly interesting to explore the application of this method to noisy problems.