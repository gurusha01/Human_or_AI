This manuscript examines the energy landscape of the loss function in neural networks, presenting a well-structured and intuitive narrative. A key finding is that the level sets of the loss function become increasingly connected as the network's overparameterization grows. The authors also provide a quantification of the degree of disconnectedness in terms of the required increase in loss to establish a connected path. This discovery may have implications for the ability of stochastic gradient descent to escape local minima. Furthermore, the paper introduces a simple algorithm for constructing geodesic paths between two networks, characterized by a decreasing loss along the path. The results suggest that the loss function becomes more nonconvex as the loss decreases, which is a notable observation.
While the study has some notable limitations, acknowledging the challenges inherent in fully analyzing the network loss function, the authors transparently discuss these constraints. Specifically, the limitations include the exclusive focus on shallow networks and the oracle loss, rather than deep networks and empirical loss. Additionally, a more in-depth discussion of the practical implications of the bound in Theorem 2.4 would be beneficial, as it is unclear whether the bound is sufficiently tight to be of practical relevance.