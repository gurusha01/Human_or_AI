This manuscript presents an enhanced version of the MAC method, where subproblems are trained on a distributed cluster configured in a circular arrangement. The fundamental concept of MAC involves decoupling the optimization process between model parameters and the outputs of sub-components (auxiliary coordinates), with optimization alternating between updating coordinates based on parameters and optimizing parameters based on outputs. In the proposed circular configuration, each update operates independently, enabling massive parallelization.
To strengthen this paper, the inclusion of more specific examples illustrating the sub-problems and their decomposition would be beneficial. For instance, it would be valuable to explore the applicability of this approach to deep convolutional networks and recurrent models. From a practical standpoint, the impact of this work appears limited to demonstrating the superiority of this particular decoupling scheme over others.
Several concepts warrant comparison, including:
- The circular configuration versus parameter server architectures
- Decoupled sub-problems versus parallel stochastic gradient descent (SGD)
Given that parallel SGD is relatively straightforward to implement using existing neural network toolboxes, the proposed method must demonstrate significantly better performance to be practically useful.
The manuscript could also benefit from clearer explanations of the information exchanged between rounds and the associated trade-offs in deep feed-forward networks. Assuming one sub-problem per hidden unit, the process seems to involve:
1. In the W step, different components of the neural network are updated in a circular manner, with each machine taking SGD steps regarding the coordinates stored locally, which requires passing the parameter vector for each hidden unit.
2. A synchronization step follows to gather parameters from each submodel, necessitating a traversal of the circular structure.
3. Each machine then updates its coordinates based on the complete model for a subset of the data, which, for a feed-forward network, involves computing intermediate activations for each layer and data point.
In comparison to parallel SGD, an equivalent approach could involve placing a mini-batch of size B on each machine using ParMAC, versus running such mini-batches in parallel. Completing the aforementioned steps would roughly correspond to one synchronized parameter server-type implementation step (distributing the model to workers, collecting gradients, and updating the model).
A practical comparison of these methods would be highly beneficial, as it is challenging to intuitively understand why the proposed method is theoretically superior to parallel SGD, except in cases of non-smooth function optimization. The decoupling approach fundamentally alters the problem by bypassing direct back-propagation, which may confound comparisons and is not guaranteed to work seamlessly with other architectures.