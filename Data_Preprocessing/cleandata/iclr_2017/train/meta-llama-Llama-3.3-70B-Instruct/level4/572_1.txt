Review- Comments: 
"This difference is notable when compared to adversarial attacks on classifiers, where examining the inputs typically exposes the original bytes provided by the adversary, often accompanied by distinctive noise patterns."
Is this assertion accurate? If so, it would seem to suggest that training a classifier to be resilient to adversarial examples should be straightforward, as these examples often exhibit characteristic noise that could be learned to recognize.
Pros: 
  -The investigation into the existence of adversarial examples in generative models, as well as the extension of the definition of "adversarial example" to this context, is a compelling research direction.  
  -Discovering that a specific type of generative model is immune to adversarial examples would be a significant finding, and conversely, demonstrating the presence of adversarial examples in generative models would also be a valuable, albeit negative, result.  
  -The adversarial examples presented in figures 5 and 6 appear convincing, although they seem more pronounced and noisy compared to the adversarial examples on MNIST reported in (Szegedy 2014). This disparity raises the question of whether it is inherently more challenging to find adversarial examples in these generative models.
Issues: 
  -The paper exceeds the page limit by a substantial margin, totaling 13 pages.  
  -The introduction would benefit from a clearer statement of purpose to provide context for the reader.  
  -The title references "generative models" broadly, yet the paper focuses exclusively on autoencoder-type models. This narrow focus may cause unnecessary confusion for researchers exploring adversarial attacks on other types of generative models, such as autoregressive models, who may feel obligated to differentiate their work from this paper.  
  -The introduction contains an excessive amount of background information, which could be condensed to improve the overall flow of the paper.