This paper presents a referential game between two agents, where both observe two images and the first agent, the sender, receives a binary target variable and sends a message to the second agent, the receiver, to enable target recovery. The agents are rewarded if the receiver accurately predicts the target. The authors propose parametrizing the agents as neural networks with pre-trained image representations and training them using REINFORCE, demonstrating convergence to optimal policies and the emergence of meaningful concepts in their learned communications. The paper also explores a variant of the game with different image classes, where the agents learn even more meaningful concepts, and a multi-game setup where the sender alternates between the game and a supervised learning task, resulting in symbolic communications with more meaningful concepts.
Investigating shared representations for communication in multi-agent settings is a fascinating research direction, and this work justifies starting with a relatively simple task due to its complexity compared to standard supervised learning or single-agent reinforcement learning. The approach of learning communication between two agents and then grounding it in human language appears to be novel, offering an alternative to standard sequence-to-sequence models that focus on statistical rather than functional aspects of language. The contributions of the proposed task, framework, and analysis of communicated tokens are valuable stepping stones for future research, making a strong case for the paper's acceptance.
Additional comments include:
- Clarification is needed on how the target variable is incorporated into the sender's network.
- Tables 1 and 2 should be corrected to consistently use percentage values, and the "obs-chance purity" column in Table 1 seems to contain extremely small values, potentially due to an error.
- Minor typos, such as "assest" instead of "assess" and "usufal" instead of "usual", should be corrected.