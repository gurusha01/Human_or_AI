This paper introduces MS MARCO, a novel large-scale machine reading comprehension dataset, distinguished by its use of real user queries, authentic web documents as context passages, and human-generated free-form answers. The authors provide an analysis of the dataset and an evaluation of the performance of various QA models on it.
The strengths of this paper include:
- The utilization of genuine user queries, which diverges from the traditional approach of generating questions based on given contexts.
- The context passages are derived from real web documents, mirroring the sources search engines use to answer user queries.
- The answers are created by humans, offering a more realistic and challenging dataset compared to those with answer spans within the context.
- The dataset is large-scale, aiming for 1 million queries, with the current release comprising 100,000 queries.
However, several weaknesses are noted:
- The claim regarding the difference in question distribution between real user queries and crowdsourced questions lacks supporting evidence or studies.
- The paper fails to clearly articulate what unique insights or capabilities today's QA models can gain from MS MARCO that are not achievable with existing datasets.
- It does not discuss the challenges inherent in achieving high performance on this dataset.
- A comparison between human performance and the models presented is missing.
- Specific details, such as the train/test splits in section 4.1 and the size of the subset of MS MARCO used for the results, are not provided.
- The acronym DSSM in Table 5 is not explained.
- The experiments in section 4.2 do not convincingly demonstrate MS MARCO's superiority over other datasets.
- The close performance of Memory Networks to Best Passage in Table 6 suggests limited room for improvement, which warrants further discussion.
- The paper appears to have been rushed, with incomplete analysis, evaluation, and textual errors.
In preliminary evaluation, MS MARCO stands out as a valuable resource for the community, given its realistic representation of QA tasks faced by search engines. Despite its potential, the paper falls short in thoroughly analyzing and evaluating the dataset, suggesting that a more comprehensive approach would significantly enhance its contribution.