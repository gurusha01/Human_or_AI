I appreciate the unique perspective presented on highway and residual networks, offering a fresh insight into the representations learned at each layer in these models. By providing residual information at periodic intervals, the layers in these models maintain feature identity, thereby avoiding the issue of lesioning that is commonly observed in convolutional neural networks.
The strengths of this paper include:
- The iterative unrolling view is notably straightforward and intuitive, backed by both theoretical foundations and sensible assumptions.
- Figure 3 effectively visualizes the iterative unrolling perspective, enhancing clarity.
However, there are some limitations:
- Despite the intriguing perspective, the paper could benefit from more empirical evidence to substantiate its claims. The experiments primarily focus on image classification and language models based on character-aware neural language models, which, although relevant, do not fully explore the potential of the proposed idea.
- Figures 4 and 5 could be merged and enlarged to more clearly demonstrate the impact of batch normalization, potentially strengthening the argument presented.