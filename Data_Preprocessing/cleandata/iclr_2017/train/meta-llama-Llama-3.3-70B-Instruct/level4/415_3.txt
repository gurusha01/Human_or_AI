The concept of promoting orthogonality among weight features has been established as beneficial for deep networks in prior research. This work introduces an explicit regularization term aimed at achieving de-correlation between weight features within a layer, thereby fostering orthogonality. Additionally, the authors provide insight into the rationale behind avoiding negative correlations for enhanced de-correlation and demonstrate how this can be accomplished.
The orthogonalization of weight features is particularly advantageous when dealing with a large number of trainable parameters and limited training data, a scenario that often leads to overfitting. As the authors note, biases can facilitate the de-correlation of feature responses even when features (weights) are correlated. Techniques such as OrthoReg may offer greater benefits in the training of deeper, leaner networks, where each layer's representational capacity is constrained, leading to improved generalization.
While the observed performance improvements are modest, the research direction and accompanying findings exhibit promise, underscoring the potential of this approach.