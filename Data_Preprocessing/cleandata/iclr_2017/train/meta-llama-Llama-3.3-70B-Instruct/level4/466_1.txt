This manuscript offers theoretical assurances for identity parameterization by demonstrating two key findings: firstly, that arbitrarily deep linear residual networks are devoid of spurious local optima, and secondly, that residual networks utilizing ReLu activations possess universal finite-sample expressivity. The paper is well-structured and tackles a fundamental issue in deep neural networks. Overall, I hold a very positive view of this paper, considering its results to be highly significant as they essentially demonstrate the stability of auto-encoders, a notable achievement given the challenges in providing concrete theoretical guarantees for deep neural networks.
A crucial direction for future inquiry is the extension of the results presented in this paper to cases involving more general nonlinear activation functions.
Minor corrections include verifying the dimension specification for U, noted as U \in R ? \times k, in the line preceding Equation (3.1).