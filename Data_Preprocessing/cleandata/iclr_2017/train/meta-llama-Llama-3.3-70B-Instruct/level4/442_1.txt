Review Update: Upon reevaluation, I have increased the score due to the compelling arguments presented regarding adversarial examples. The paper effectively demonstrates that the proposed method serves as a decent regularizer; however, its competitiveness compared to existing regularizers such as dropout and normalization is not convincingly established. For instance, there is a lack of sufficient evidence to suggest that it outperforms these methods in terms of regularization. Furthermore, I anticipate that tuning this method will be more challenging than its counterparts, as discussed in my previous rebuttal.
Summary: As I understand, this paper adapts the "bottleneck" concept from variational autoencoders, which encourages the latent variable to follow a noise prior like N(0,1), and applies it to supervised learning. Here, the reconstruction term log(p(x|z)) is replaced with the standard supervised cross-entropy objective. The central argument is that this adaptation acts as an effective regularizer, enhancing robustness to adversarial attacks.
Pros:
- The presentation is clear and the paper is well-structured, making it easy to follow.
- The idea is well-founded, and its connection to previous work is adequately described.
- The experiment demonstrating robustness to adversarial examples appears convincing, although an external quantitative baseline for comparison would strengthen the argument. This would provide a clearer understanding of how this method compares to other regularizers in combating adversarial examples. For example, comparing its robustness to that achieved by using a high dropout rate would be insightful.
Cons:
- The MNIST accuracy results seem less impressive, with an error rate of 1.13%, especially when compared to results from the Maxout paper presented at ICML 2013, which listed several permutation invariant MNIST results with error rates below 1%. This raises questions about the method's competitiveness as a regularizer. Additionally, tuning this method may be more complex than tuning other regularizers like dropout.
- The architectural choices, particularly the number of hidden layers before and after z, are numerous and not extensively justified. For instance, the decision to use a simple logistic regression for p(y|z) seems arbitrary and could be better motivated by empirical evidence or theoretical reasoning.
Other Considerations:
- It would be intriguing to explore the outcome of training the model against discovered adversarial examples while utilizing the method proposed in this paper. Specifically, would the model learn to adjust its variance p(z|x) when confronted with adversarial examples, potentially leading to enhanced robustness?