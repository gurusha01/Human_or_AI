This dataset paper offers distinct advantages over existing reading comprehension challenges, primarily due to its derivation from real query logs, which reflect authentic questions posed by individuals. In contrast to artificially constructed questions, MS MARCO's queries are grounded in practical settings, providing a more realistic foundation for evaluation.
However, the utilization of query logs also introduces potential drawbacks. Users may tailor their search queries to accommodate the limitations of current search engines, potentially simplifying their language and questions. To mitigate this concern, the authors could have employed a more selective approach to query log sampling, downsampling straightforward questions that can be answered through basic keyword matching and upsampling more complex queries that necessitate advanced reading comprehension skills, such as paraphrasing and synthesizing information from multiple sentences.
The recent proliferation of large-scale reading comprehension challenges is a positive development, but a primary concern remains: whether a significant proportion of questions can be answered through relatively simple text matching, without requiring sophisticated reading or reasoning abilities.
The paper's presentation suggests a rushed completion, and I would have appreciated more in-depth, quantitative comparisons with existing datasets, as well as additional insights into the challenges posed by MS MARCO's question-answering (QA) tasks. For instance, the authors could have provided statistical analyses of QA tasks, categorizing them into types such as: (1) exact matches within text snippets, (2) paraphrasing requirements with directly available answers, (3) synthesis of information from multiple sentences, and (4) necessity for external knowledge. Although the author response acknowledges the unlikely need for external knowledge, a more comprehensive and formal analysis would be beneficial.