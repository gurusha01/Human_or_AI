I appreciate the authors' thorough response to my inquiries.
The manuscript presents a support-regularized variant of sparse coding, which incorporates the underlying data manifold structure. To achieve this, the authors modify the traditional sparse coding loss function by adding a term that promotes similar active sets for nearby points. The optimization procedure's convergence guarantees are also provided. The experimental results on clustering and semi-supervised learning demonstrate the advantages of the proposed approach.
The paper is well-structured and engaging to read. The primary contribution of this work lies in the integration and optimization of the regularization function, rather than relying on approximations or surrogates. The authors develop a PGD-style iterative method and provide a convergence analysis for it.
I appreciate the clarifications regarding the assumptions made in Section 3, and it would be beneficial to include some of this information in the manuscript.
The authors also propose an efficient encoding scheme for their method. Additionally, they have included a new semi-supervised learning experiment that showcases an interesting application of the method and its fast approximation. Although this addition is intriguing, I believe that utilizing fast encoders is not particularly innovative or a core aspect of the work. The conversion of iterative optimization algorithms into feed-forward networks to accelerate the inference process has been explored previously in similar contexts. It is expected that this can be done, and the outcome is not entirely surprising. A potentially interesting direction for evaluation would be to assess the importance of having an architecture that matches the optimization algorithm compared to a generic network, although some of this analysis has also been conducted in the past.