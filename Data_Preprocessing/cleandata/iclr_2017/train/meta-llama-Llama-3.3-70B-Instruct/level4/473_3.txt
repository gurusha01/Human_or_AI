This paper presents a theoretical justification for linking the word embedding and output projection matrices in RNN language models, leveraging an augmented loss function that distributes output probability mass among words with similar word embeddings.
Two primary limitations of this framework are apparent:
The augmented loss function lacks trainable parameters and serves solely as a regularization tool, which may not yield benefits with sufficiently large datasets.
The design of the augmented loss is heavily tailored to achieve the specific outcome of parameter tying, and it is unclear how the results would be affected by introducing parameters or estimating y~ through alternative methods.
Despite these limitations, the argument is intriguing and well-articulated.
The simulation results effectively validate the argument, and the PTB results appear promising.
Minor suggestions:
In Section 3, clarification is needed on whether y~ is conditioned on the individual example at time t or the entire historical context.
Equation 3.5 should specify that i is enumerated over the set V, rather than its cardinality |V|.