This manuscript proposes a modification to the traditional LSTM architecture, where the gate functions are conditioned on a fixed number of previous inputs, denoted as $ht = f(xt, x{t-1}, ..., x{t-T})$, rather than the conventional $ht = f(xt, h_{t-1})$. This alteration enables the network to operate more efficiently and effectively by shifting computations from a sequential to a parallel framework, thereby increasing parallelization within the sequential stream. However, the clarity of this innovative and straightforward concept is somewhat hindered by ambiguous terminology.
To enhance the manuscript, I recommend that the authors refine their explanation of the proposed model to improve readability. 
Additionally, providing explicit big O notation calculations or illustrating specific examples that demonstrate the source of the speed enhancements would be beneficial. 
The experimental evaluations appear to be satisfactory, and overall, I found the paper to be engaging.
If successfully replicated and proven to be reliable across various settings, this work has the potential to become a highly valued and standard component in neural network architectures.