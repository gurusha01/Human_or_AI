This submission presents an intriguing approach to character-based language modeling, which maintains word-level representations in both the context and output, with the latter being optional. However, it is essential to note that this approach is not novel, as evidenced by previous works such as Kim et al. (2015) and Jozefowicz et al. (2016), which are cited in the submission. Both of these studies have already expanded on this approach by utilizing RNNs/LSTMs, with Jozefowicz et al. providing a comparative analysis of different character-level modeling methods, a discussion that is lacking in this submission. 
The potential novelty of this approach lies in its application to machine translation, although it remains unclear how the reranking of N-best lists addresses the out-of-vocabulary (OOV) problem. The translation-related aspects of the OOV problem require further elaboration. Some claims made in this submission seem exaggerated, such as the statement in Section 2.3 that the approach makes the notion of vocabulary obsolete. This assertion is questionable, given the authors' own doubts about the interpretation of perplexity without an explicit output vocabulary. For instance, modeling frequent word forms can still be expected to contribute significantly, as demonstrated in arXiv:1609.08144.
In Section 2.3, the claim that the objective requires a finite vocabulary is only accurate if the units considered are limited to full word forms. The use of subwords and individual characters can implicitly cover larger, even infinite vocabularies with the log-likelihood criterion, albeit requiring a different model than the one proposed. This nuance should be acknowledged.
The description of how character embeddings are used for the output in Section 2.4 lacks clarity and needs further explanation. Additionally, the configuration and parameterization of Noise Contrastive Estimation (NCE) described in Section 3.4 could be better justified with more details on how these specific settings were determined.
In Section 4.1, it would be beneficial to reference Kim et al. (2015), which drew similar conclusions regarding the performance of using character embeddings at the output, and discuss the suggestions for potential improvements provided therein. Section 4.2 could also be enhanced by mentioning that there are methods to calculate and interpret perplexity for unknown words, as discussed in Shaik et al. (IWSLT 2013).
Section 4.4 and Table 4 should include the size of the full training vocabulary for completeness. Minor corrections are also necessary: on page 2, "three different input layer" should be pluralized; the fonts in Figure 1 are too small; on page 3, "that we will note WE" should be "that we will denote WE"; in Section 2.3, "the parameters estimation" could be rephrased as "the parameter estimation" or "the parameters' estimation"; on page 5, "in factored way" should be "in a factored way", and "a n-best list, a nk-best list" should be "an n-best list, an nk-best list"; in Section 4.2, the sentence "Despite adaptive gradient," is missing a verb and article.
The submission mentions that perplexity is hard to interpret for models without an explicit output vocabulary. However, when analyzing open vocabulary approaches, perplexity can also be renormalized to the character level, as seen in Shaik et al. (IWLST 2013). The authors should consider this and discuss it. Furthermore, Kim et al. (AAAI 2015) reached similar conclusions about the performance of character-level embeddings and provided a discussion with suggestions for improvements, which should also be considered.
More details on the configuration of the NCE training would be helpful, and confirmation is needed on whether a feed-forward neural network was used, as inferred from the notation. Additionally, it should be clarified if the character-level word embedding used is the same as the one in the Google paper by Kim et al. (AAAI 2015), which is not cited in Section 2.1.
The use of the colon in the first equation of Section 2.1 needs definition. The term P^H((w:H)\in D) is introduced without prior definition before the second equation in Section 2.3, and the sentence introducing this equation lacks an explicit reference to the probability being referred to. Lastly, the terms e^{out} and e^{char-out} in Section 2.4 should be defined, and it should be clarified if they are the same as e^{out}_w in Section 2.2 and e^{char} in Section 2.1.