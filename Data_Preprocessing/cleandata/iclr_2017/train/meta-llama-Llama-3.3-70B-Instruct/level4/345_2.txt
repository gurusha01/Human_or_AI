The authors present a novel approach to compressing neural networks by retraining them with a Gaussian mixture prior on the weights, where the means and variances are learned. This prior enables compression by setting weights to the mean of their inferred mixture component, potentially at the cost of precision, and storing the network in a sparse format that only retains the fixture index. 
Quality:
A significant limitation of the proposed method is its inability to perform well on VGG, which restricts its practical applicability. The evaluation on AlexNet is also missing, which could have provided valuable insights. Figure 2 reveals an interesting pattern, with a notable number of points exhibiting improved accuracy, unlike LeNet5-Caffe. It would be beneficial to understand the underlying reasons for this discrepancy. Additionally, the authors could provide more insight into the hyperparameter settings that yielded favorable results using spearmint optimization, as this information could be helpful for others seeking to apply this method. Figure 7 is particularly well-presented in its current form.
Clarity:
The paper excels in its theoretical introduction to MDL in Section 2. Sections 4, 5, and 6, although concise, contain essential information. However, providing more details about the models used, such as the number of layers and parameters, would enhance clarity. In Section 6.1, the claim about variances being reasonably small, except for a few large ones, is difficult to assess from Figure 1 due to the vertical histogram primarily showing the zero component. Using a log histogram or separate histograms for each component could improve visualization. The distinction between the large and small points in Figure 2, which seem to offer a favorable compression-accuracy loss ratio, warrants further explanation.
Originality:
The concept of learning quantization directly through a Gaussian mixture prior in retraining is innovative and more principled than existing approaches that focus on reducing the number of bits to store parameters or exploiting sparsity.
Significance:
While the method achieves state-of-the-art performance on MNIST examples, its applicability to deeper, state-of-the-art models like VGG or ResNet is limited. Overcoming this limitation could significantly enhance the method's practical usability and significance.
Minor issues:
- Page 1: A space precedes the first author's name.
- Page 3: The text "in this scenario, pi_0 may be fixed..." appears to be missing a backslash in TeX.
- Page 6, Section 6.2: There are incorrect blanks in "the number of components, \tau." 
- Page 6, Section 6.3: "in experiences with VGG" should likely be "in experiments with VGG."
- Page 12: "Figure C" is referenced as "Figure 7."