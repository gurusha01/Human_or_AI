This paper proposes a modified neural Turing machine (NTM) model, as introduced by Graves et al. in 2014, where both keys and values are stored in memory. The authors explore both continuous and discrete mechanisms for controlling memory access.
The proposed model appears to be highly complex, relying on numerous tricks and sophisticated techniques to function effectively. The cost function comprises over 10 distinct terms, and the learning process involves multiple hacks. However, the justification for these choices and the overall complexity of the model is not clearly explained. Furthermore, the lack of available code and unclear plans for its release raise concerns about reproducibility.
The model's performance is evaluated on a set of toy problems, specifically the "babi task," where it achieves results only marginally better than a standard LSTM but falls short of the performance of other memory-augmented models proposed in recent years.
The writing style and notation used in the paper make it challenging to follow the model's description. The equations are difficult to read due to non-standard notation, such as "softplus," and overloaded notation (e.g., w_t, b). Similar equations are presented in different formats, making it hard to understand the relationships between them. For instance, equations (8-9) and (10-11) are presented in scalar and vector forms, respectively, without clear justification.
Overall, reproducing the model and its results is likely to be difficult due to the complexity of the model and the lack of available code. The performance on the bAbI tasks is also underwhelming compared to other memory-augmented models.
To improve the clarity and reproducibility of the paper, the authors should consider releasing the code and providing a more detailed explanation of the model and its components. Specifically, it would be helpful to:
* Clarify the overall cost function minimized by the model and provide a summary of the different cost functions and regularization techniques used.
* Resolve inconsistencies in variable definitions, such as w_t and b.
* Provide a clear explanation of the "gamma_t" term, including whether it represents a function or a variable, and its relationship to equation (10).
* Compare the "curriculum learning for discrete attention" approach to simpler schemes, such as rounding the continuous attention.
* Address the apparent contradiction between the introduction and section 4 regarding the use of discrete non-differentiable attention mechanisms.
* Correct the statement in the introduction regarding the use of attention mechanisms in memory networks, which was actually introduced by Sukhbaatar et al. in 2015.
* Clarify the statement in the introduction regarding the use of memory networks in real-world tasks, as the cited papers (Bordes et al. 2015, Dodge et al. 2015) use different memory network systems than Weston et al.