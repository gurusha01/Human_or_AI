The manuscript introduces a semantic embedding model designed for multi-label prediction tasks. 
In my initial inquiry, I noted that the proposed method presupposes prior knowledge of the number of labels to be predicted, a point the authors deemed orthogonal, although I disagree with this assessment.
I sought clarification on the distinction between the proposed Semantic Embedding Model (SEM) and a basic Multi-Layer Perceptron (MLP) equipped with a softmax output function, which could be trained using a two-step approach as an alternative to stochastic gradient descent. Given the similarities, comparing SEM to this fundamental baseline seems warranted.
Concerning the sampling strategy employed to estimate the posterior distribution and its divergence from the approach outlined by Jean et al., while I acknowledge the differences are subtle, I believe it is essential to reference this existing work and explicitly highlight the distinctions.
Lastly, I question the terminology used: the model is referred to as "semantic" embeddings, a term typically implying that the learned embeddings capture semantic relationships or meanings. However, this aspect does not appear to be a focal point of the paper, prompting the inquiry into the rationale behind this designation.