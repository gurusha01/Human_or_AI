This paper presents a deep extension of generalized Canonical Correlation Analysis (CCA), with the primary contribution being the derivation of the gradient update for the GCCA objective.
However, I contest the assertion that this is the first multiview representation learning technique to merge the flexibility of nonlinear representation learning with the statistical power of incorporating information from multiple independent resources or views. A prior work, [R1], has already proposed a multiview representation learning method that is both nonlinear and capable of handling more than two views, which is closely related to the authors' proposal. The objective function in [R1] aims to maximize the correlation between views while minimizing self and cross reconstruction errors, bearing intuitive similarities to a nonlinear version of PCA+CCA for multiple views. A comparison between these two methods is essential to demonstrate the usefulness of the proposed Deep GCCA (DGCCA), and the paper is incomplete without this comparison. Furthermore, the authors should revise their claim to reflect this.
The related work section is concise and lacks discussion of significant advances in two-view nonlinear representation learning, which are worthy of mention.
References:
[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. HLT-NAACL 2016: 171-181