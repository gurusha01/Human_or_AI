This paper examines the convergence dynamics of a single-layer non-linear network, assuming Gaussian iid inputs. While the initial sections, focusing on a single hidden node, were relatively comprehensible, the latter parts, which explore multiple hidden nodes, proved challenging to follow, with the main conclusion being obscure. To enhance readability, the author should prioritize intuitive explanations and relegate detailed derivations and notation to an appendix.
Regarding significance, it is difficult to assess the generalizability of these findings due to the stringent assumptions of Gaussian inputs and iid inputs, which rarely reflect real-world feature inputs that are often highly correlated and non-Gaussian. Recent studies on deep network convergence, such as Kawaguchi (NIPS 2016), do not rely on such assumptions. Although the author claims no assumption is made on the independence of activations, this assumption appears to be transferred to the input instead, potentially implying that activations are combinations of iid random variables and may exhibit Gaussian-like behavior, which raises questions about the implications of these results.
Specifically:
1. To avoid confusion, the author should use $D_w$ instead of $D$ to indicate that $D$ is a function of $w$, not a constant, and ensure consistent notation throughout, particularly when transitioning between $D(w)$ and $D(e)$ in Section 3.
2. In Section 3, the statement regarding the neuron being cut off at sample $l$ and the expression $(D^{(t)})_u$ lacks clarity on the relationship between $l$ and $u$, exemplifying the notational inconsistencies that hinder reader understanding.
3. The introduction of $F(e, w)$ and $D(e)$ in Section 3.1 is unclear and requires further explanation.
4. Theorem 3.3 suggests that for $\epsilon > 0$, maximizing the probability of convergence requires $\epsilon$ to be close to 0, implying a ball $B_r$ with a radius $r$ approaching 0, which seems contradictory to Figure 2.
5. Section 4 is unclear, and the significance of the symmetry group remains unintuitive; a more accessible explanation for its importance would be beneficial.
6. Figure 5 lacks a definition for $a_j$.
To improve the paper's clarity and facilitate understanding of its key takeaways, a rewrite is strongly encouraged. In its current form, the paper poses significant challenges to comprehension.