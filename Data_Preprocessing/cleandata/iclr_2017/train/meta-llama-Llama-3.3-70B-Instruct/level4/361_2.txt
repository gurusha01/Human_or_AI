This manuscript explores the application of Bayesian neural networks in modeling learning curves for hyper-parameter optimization, with the goal of terminating underperforming runs early to conserve time. By integrating the parameters of parametric learning curves into the final layer of a Bayesian neural network, the authors present a logical and reasonable approach. 
A significant advantage of this paper is its focus on addressing a genuine and pressing need in the field, as evidenced by the high demand for effective early termination systems in hyper-parameter optimization. A crucial aspect that would enhance the paper's contribution, particularly given its practical orientation, is the release of the authors' code. The potential impact of this code as a valuable resource warrants clarification on whether it will be made publicly available.
The experimental section appears comprehensive, although the results are somewhat disappointing. The primary interest lies not in the accuracy of learning curve modeling, but rather in its effect on hyper-parameter optimization. The expectation was to see substantial speed improvements resulting from this method; however, the actual magnitude of the speedup remains uncertain. A more informative approach would be to plot the number of iterations required to reach a fixed objective function value, as this directly relates to the time saved. Additionally, incorporating real-time measurements would provide insight into the practical usability of these hyper-parameter optimization methods, which can sometimes be prohibitively slow.
A notable omission is a histogram illustrating the distribution of termination times across different runs, which would offer valuable intuition into the fraction of runs terminated early and the timing of these terminations. Currently, while it is evident that some runs are terminated early (given the method's ability to outperform others), the specifics of this process remain unclear.
In conclusion, this paper deserves acceptance due to its solid contribution to an intriguing problem, despite the progress being somewhat incremental. The ideas presented are sound, and with clarification on code release and additional analyses, the paper's impact could be significantly enhanced.