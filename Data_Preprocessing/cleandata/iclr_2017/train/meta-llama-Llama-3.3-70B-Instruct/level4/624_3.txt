This paper examines specific instances of neural networks and datasets where optimization is unsuccessful. However, the majority of the models and datasets considered are overly contrived and deviate from standard hyperparameter selection and parameter initialization guidelines, which diminishes the practical applicability of the analysis.
The "bad initialization on MNIST" experiment illustrates that when biases or weights are initialized with very negative values or drawn from a non-centered distribution, all ReLU activations are deactivated for all data points, thereby preventing optimization. Nevertheless, this scenario is unlikely to occur in practice, as adhering to proper initialization heuristics avoids such cases.
The authors' constructed "jellyfish" dataset is shown to be challenging for a small model to fit. Nonetheless, the size and depth of the model employed are not well-suited for this particular problem.
Proposition 4 posits that the mean for weight parameter initialization can be chosen arbitrarily. In practice, however, most initialization heuristics draw weight parameters from a distribution centered at 0, limiting the applicability of this assumption.
Furthermore, Proposition 5 focuses on infinitely deep ReLU networks, whereas in practice, very deep networks are more likely to be of the ResNet type, which may not be directly comparable to the results presented.