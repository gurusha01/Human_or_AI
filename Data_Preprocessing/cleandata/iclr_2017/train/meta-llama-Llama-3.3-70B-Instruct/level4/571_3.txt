This paper presents two novel methods for enhancing generative models, both of which rely on likelihood ratio estimates. These approaches are assessed using synthetic data and the MNIST dataset for sample generation and semi-supervised learning tasks.
Although the concept of boosting generative models and the proposed techniques are intriguing, the experimental results are not persuasive due to several concerns. 
1. The bagging baseline described in section 3.1 appears to involve refitting the model to the same dataset, raising the probability to the power of alpha, and then renormalizing, which increases the peakedness but lacks a clear justification as a suitable baseline; clarification on this procedure would be appreciated.
2. The sample generation experiment in section 3.2 utilizes a Markov chain that converges very slowly, as evident from the similarities between plots c and f, d and g, and e and h, suggesting that the generated samples may not be from the stationary distribution; thus, a qualitative evaluation using Annealed Importance Sampling (AIS) seems necessary.
3. In the same section, the selection of alphas appears arbitrary; it would be informative to investigate the outcome of choosing a more intuitive alpha value, such as alpha_i = 1 for all i.
4. The semi-supervised classification results are difficult to interpret, as the baseline Restricted Boltzmann Machine (RBM) performs comparably to the boosted models, making it challenging to draw meaningful conclusions.
The paper is generally well-written and, to the reviewer's knowledge, original, but these concerns need to be addressed to strengthen the findings.