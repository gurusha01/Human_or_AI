The authors present a innovative approach to utilizing Bayesian neural networks for policy search in stochastic dynamical systems, distinct from traditional variational Bayes methods, by minimizing alpha-divergence with an alpha value of 0.5. They assert that their methodology is the first model-based system capable of solving a longstanding benchmark problem that has remained unsolved for 20 years; however, my familiarity with this specific literature is limited, making it challenging for me to fully evaluate the validity of this assertion.
From a technical standpoint, the paper appears to be sound. Nevertheless, there is room for improvement in the clarity and structure of the writing. The notation used in sections 2 and 3 is somewhat dense, with a plethora of terminology and approximations introduced, which complicates the reading process. A clearer distinction between novel contributions and reviews of prior work would enhance the readability of the paper. For instance, section 2.3, which seems to primarily review black box alpha divergence minimization, could potentially be relocated to the appendix to streamline the main text.
It would be beneficial to consider the relevance of stochastic gradient Markov Chain Monte Carlo (MCMC) methods, such as Stochastic Gradient Langevin Dynamics (SGLD) and Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), to the authors' setup, especially in light of promising results reported in a NIPS 2016 paper titled "Bayesian optimization with robust Bayesian neural networks" by Springenberg et al.
Additionally, commentary on the computational complexity of the different approaches presented would provide valuable insight. 
In section 4.2.1, it is unclear why the original data cannot be used and what is meant by simulating data using another neural network being a fair approach. Furthermore, evaluating the performance of PSO-P on this specific problem could offer additional perspectives.