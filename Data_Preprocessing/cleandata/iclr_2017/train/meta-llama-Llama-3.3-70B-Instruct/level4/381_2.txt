This manuscript introduces a groundbreaking approach to pruning filters in convolutional neural networks, bolstered by a robust theoretical framework. The proposed methodology is grounded in the first-order Taylor expansion of the loss function when pruning a specific unit, resulting in a straightforward weighting of unit activation with its gradient relative to the loss function. This approach outperforms the simplistic use of activation magnitude as a pruning heuristic, which intuitively aligns with the goal of removing not only filters with low activation but also those where incorrect activation values would have a negligible impact on the target loss.
The authors conduct a thorough examination of multiple baselines, including an oracle that establishes an upper bound on target performance, albeit at a high computational cost. The developed method exhibits elegance and generalizability across various tasks, with the added benefit of being computationally feasible and easily integrable with traditional fine-tuning procedures. Furthermore, the study clearly elucidates the trade-offs between increased speed and decreased performance, a valuable insight for practical applications.
To further strengthen the manuscript, a comparison with alternative baselines, such as [1], would be beneficial. Nevertheless, the proposed method appears more advantageous due to its ability to bypass the need for training a new network, likely resulting in significant speed enhancements.
A potential avenue for future extension could involve exploring the removal of only specific parts of filters, such as in the case of 3D convolution. Although this may introduce complexity by requiring modifications to the convolution operator implementation, it could yield additional speed improvements.