This paper presents GA3C, a novel GPU-based adaptation of the A3C algorithm, initially designed for multi-core CPU architectures. The key contribution lies in the incorporation of a queue system, which facilitates the batching of data for predictive and training purposes, thereby optimizing GPU utilization. The proposed system is benchmarked against the authors' own A3C implementation and established reference scores.
The architecture outlined for implementing A3C on GPUs appears highly intuitive. By batching prediction requests and learning steps for multiple actors, GPU occupancy is maximized, assuming latency is not a critical factor. The inclusion of an automatic performance tuning strategy is also a notable feature.
The demonstration of GA3C's 20% higher throughput compared to the original A3C paper is commendable. However, a crucial aspect that remains unaddressed is the comparison of learning speed and data efficiency. Figure 3 presents scores obtained under disparate evaluation protocols, rendering them non-comparable. A more convincing approach to demonstrate comparable learning speed would involve plotting time versus score or data versus score, showcasing similar or enhanced performance relative to A3C. For instance, an open-source implementation has reportedly matched the performance on Breakout, highlighting the need for such comparative analyses.