This paper presents a novel simulator and a suite of synthetic tasks designed to assess a dialogue agent's capacity to learn from user feedback, leveraging memory networks (Sukhbaatar et al., 2015) trained via supervised and reinforcement learning methods. The results demonstrate that agents learning from feedback, such as through questioning or clarification, exhibit improved performance.
The paper's motivation is compelling, as dialogue agents that can learn directly from unstructured human feedback have significant potential in real-world applications. However, the execution falls short due to the reliance on a highly artificial synthetic dialogue simulator, which differs substantially from real-world dialogues. The simulator is based on a simple factoid question-answering framework, typically not considered a true dialogue, and can likely be solved using a few hand-crafted rules. Furthermore, the framework assumes that user feedback is always accurate and provided in a limited number of predefined forms, and that the agent can learn from examples of another agent asking questions or making clarifications, simplifying the task.
Due to the artificial setting and limited scope of the experiments, it is challenging to draw conclusions about learning from unstructured user feedback. To validate the hypothesis that learning from such feedback is possible, I strongly recommend that the authors conduct experiments with real human users, even within the factoid question-answering domain. This would provide more robust evidence for the feasibility of a dialogue agent learning from user feedback.
Additional comments include:
- The abstract's use of "interactive dialogue agents" is unclear, as all dialogue agents interact with users, making the term "interactive" potentially redundant.
- A significant limitation of the experiments is that the agent's questions are predefined. In the supervised learning setting, the agent is trained to imitate another rule-based agent's questions, while in the reinforcement learning setting, the agent learns when to ask questions but not what questions to ask.
- The paper states that the agent learns what to ask in the "ONLINE REINFORCEMENT LEARNING (RL)" subsection, which should be clarified or removed.
- The paper presents an overwhelming number of results, which can confuse the reader. The inclusion of certain training settings, such as "TrainAQ(+FP)" and "TrainMix," is unclear, and their relevance to the original hypothesis should be justified or removed.
- Considering the paper's contribution lies in the tasks and evaluation, it may be beneficial to move some results, such as the vanilla-MemN2N or Cont-MemN2N results, to the appendix to improve clarity.
--- UPDATE ---
Following the discussion and additional experiments provided by the authors, I have increased my score to 8.