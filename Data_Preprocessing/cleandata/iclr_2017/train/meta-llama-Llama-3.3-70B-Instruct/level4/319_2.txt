This paper introduces a novel knowledge distillation framework designed to reduce the discrepancy in the sum of statistics across feature maps between teacher and student networks. The authors provide empirical evidence that their proposed approach surpasses the FitNet-style distillation baseline in performance.
Strengths:
+ The methodology is extensively tested on a range of computer vision datasets.
+ The paper is generally well-structured and clearly written.
Weaknesses:
- The methodology appears to be constrained to convolutional architectures, limiting its broader applicability.
- The use of "attention" terminology in the paper is misleading, as the method essentially focuses on distilling the sum of squared (or other statistical measures, such as summed Lp norm) activations within hidden feature maps.
- The inclusion of gradient-based attention transfer seems incongruous and lacks direct comparison or joint utilization with the "attention-based" transfer method, suggesting it may not significantly enhance the paper's value.
- Furthermore, the computation of the induced 2-norm in equation (2) is unclear. Given that Q is a matrix in \(\mathbb{R}^{H \times W}\), its induced 2-norm corresponds to its largest singular value, which could be computationally costly to determine. It is questionable whether the authors actually intend to refer to the Frobenius norm instead.
In summary, while the proposed distillation method demonstrates practical effectiveness, the paper is marred by organizational issues and ambiguous notation, which detract from its overall clarity and impact.