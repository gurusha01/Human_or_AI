This paper addresses crucial challenges in multi-task reinforcement learning, specifically mitigating negative transfer and enabling more selective transfer. The proposed approach utilizes a soft attention mechanism, which is highly generalizable and has been successfully applied to both policy gradient and value iteration methods. The incorporation of a base network facilitates the learning of new policies when prior policies are not directly applicable. Furthermore, the state-dependent sub-policy selection mechanism provides finer control, effectively assigning different sub-policies or experts to distinct regions of the state space. Although the tasks considered are relatively simple, they adequately demonstrate the benefits of the proposed method. However, a notable limitation is the simplicity of the approach, with results and claims being largely empirical. Future extensions could involve integrating the method with an option-based framework, exploring stochastic hard attention mechanisms, sub-policy pruning, or progressive networks.
In Figure 6, the red curve appears to exhibit inferior performance compared to the others in terms of final outcome. An alternative approach to presenting figure-related information could be to include statistics on attention mask activation during the learning process. This would allow observation of whether the model learns to deactivate adversarial sub-policies and primarily rely on the newly learned base policy. Additionally, analyzing attention mask activation can help detect any unusual co-adaptation phenomena, providing valuable insights into the learning dynamics.