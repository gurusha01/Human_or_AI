The authors present an implementation of TensorFlow Fold, enabling the execution of diverse computations without altering the computation graph. This is accomplished by designing a generic scheduler as a TensorFlow computation graph that can accept graph descriptions as input and execute them. The approach yields notable advantages in tasks where computations vary for each data point, as exemplified by TreeRNN. Experimental comparisons are made against static batches with repeated graph structures and batch size 1. However, the score is limited to 7 due to the omission of a comparison with a key alternative approach, wherein a new TensorFlow graph could be generated for each dynamic batch, allowing for the execution of non-uniform batches using standard TensorFlow functionality, rather than relying on the proposed scheduling algorithm.