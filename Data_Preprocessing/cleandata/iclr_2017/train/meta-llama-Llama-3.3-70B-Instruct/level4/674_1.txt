This paper presents a theoretical framework to explain the invertibility of deep convolutional neural networks, specifically from intermediate layers back to the input image, by examining the invertibility of a single layer. The authors assume that convolutional filters can be treated as incoherent measurements that satisfy the Restricted Isometry Property (RIP). 
In my assessment, although this research direction is intriguing, the manuscript is not yet suitable for publication. The current treatment falls short of adequately explaining the invertibility phenomenon in deep neural networks. Despite the authors' response, I believe the results merely represent a minor extension of standard compressive sensing outcomes for sparse reconstruction using incoherent measurements.
A crucial distinction exists between a single layer and a deep neural network - the "deep" aspect is what enables the forward task to function effectively. As acknowledged by the authors, applying Iterative Hard Thresholding (IHT) recursively leads to significant deterioration, implying that the theory, at best, explains the partial invertibility of a single layer. The invertibility of a single layer is not surprising; however, the invertibility of a cascade of layers is a more noteworthy phenomenon.
To provide a meaningful theoretical analysis of this phenomenon, it is essential to move beyond the examination of a single compressive measurement-type layer and investigate how the theory applies to a cascade of layers. This is because the sparse recovery theory may not hold beyond a single layer, and invertibility could be a property emerging from correlations between the weights of different layers. Consequently, the current results for individual layers do not necessarily constitute a step towards explaining the invertibility of entire networks, as it remains unclear whether these findings can be generalized to deeper architectures.