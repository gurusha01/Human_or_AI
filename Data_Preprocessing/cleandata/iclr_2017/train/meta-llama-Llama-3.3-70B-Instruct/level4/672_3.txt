This manuscript presents a novel alternative to Conditional Variational Auto-Encoders (CVAEs) and Conditional MultiModal Auto-Encoders for imputing missing modalities in multimodal datasets. The proposed Joint Multimodal Variational Auto-Encoder (JMVAE) approach involves training a Variational Auto-Encoder (VAE) jointly across all modalities, supplemented by additional KL divergence penalties between the approximate posteriors given all modalities and those given a subset of modalities. 
The authors establish a connection between the JMVAE and the Variation of Information, although it is not immediately clear why the JMVAE was preferred over the potentially more refined Variation of Information method.
A concern that remains unaddressed is the method's scalability. Without access to the code or a detailed specification of the encoder architecture, it appears that this approach may necessitate training a new encoder for each subset of missing modalities, which could become impractical as the number of modalities increases; currently, this seems manageable with only two modalities.
The results in Table 1, showing that log-likelihood estimates (log p(x)) are lower when using multiple modalities compared to a single modality, are somewhat counterintuitive and warrant further explanation.
Moreover, the comparison between the representations learned by JMVAE and CVAE may be biased, as CVAE learns representations conditionally on the label (e.g., in MNIST), which should not be considered in the representation. Intuitively, this could capture "style" as demonstrated in conditional generation figures (Kingma et al., 2014).
For the CelebA dataset, comparing log-likelihoods among models that utilize GANs may not be meaningful, given that GANs do not optimize log-likelihood.
In summary, while the problem addressed is intriguing and the ideas presented are worth further exploration, the paper's execution requires additional refinement.