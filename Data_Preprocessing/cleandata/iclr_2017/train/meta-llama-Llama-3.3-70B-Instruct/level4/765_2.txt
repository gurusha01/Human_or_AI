This paper presents an extension of a recently introduced video frame prediction approach, incorporating reward prediction to learn unknown system dynamics and reward structures within an environment. The methodology is evaluated on various Atari games, demonstrating accurate reward prediction over a timeframe of approximately 50 steps. The paper is well-organized, concise, and clearly articulates its contribution to existing literature, with robust experiments and methods. Nevertheless, the findings are somewhat predictable, given the deterministic relationship between system state and reward in Atari games, where the reward can be inferred from a network that effectively encodes future system states in its latent representation. Consequently, the paper's contribution, although sound, is relatively modest. To significantly enhance the paper's impact, it would be beneficial for the authors to investigate the two proposed future research directions outlined in the conclusion, specifically exploring the augmentation of training with artificial samples and the integration of Monte-Carlo tree search. These potential avenues of research could lead to a reduction in the required number of real-world training samples while improving performance, yielding results that would be both intriguing and influential.