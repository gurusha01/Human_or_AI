This paper proposes an on-policy deep reinforcement learning approach that incorporates auxiliary intrinsic variables, representing a specific instance of a universal value function-based method, with proper citation of relevant references. A key aspect of this work is the synthesis of existing concepts to address 3D navigation challenges, although the contributions could be more explicitly outlined in the introduction and abstract.
To further strengthen the paper, it would be beneficial to explore the limitations and failure modes of this approach, including scenarios where the model struggles to generalize to changing goals. Notably, as an on-policy method, it is susceptible to catastrophic forgetting if the agent does not periodically retrain on goals from earlier phases. Investigating these challenges would provide a more comprehensive understanding of the method's capabilities.
Given that the primary contribution lies in the integration and empirical validation of key ideas, extending the results to other domains, such as Atari games, where intrinsic variables could be derived from the game's ROM, would be a valuable addition. This could help assess the broader applicability and robustness of the proposed formulations.
Overall, the paper demonstrates a clear empirical advantage of the proposed approach and offers valuable experimental insights that could inform the development of future agents, highlighting the potential of the underlying formulations and the importance of continued research in this area.