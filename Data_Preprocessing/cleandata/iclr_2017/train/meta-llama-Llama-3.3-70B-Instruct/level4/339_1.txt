The authors propose a straightforward approach to integrating a cache into neural language models, effectively enabling a copying mechanism for recently utilized words. Notably, this mechanism differs from related work on neural networks with copying mechanisms, as it does not require training with long-term backpropagation, thereby enhancing efficiency and scalability for larger cache sizes. The addition of this cache to RNN baselines yields significant improvements in language modeling performance.
A key contribution of this paper lies in the observation that utilizing hidden states hi as keys for words xi, and h_t as the query vector, naturally yields a lookup mechanism that functions effectively without requiring backpropagation-based tuning. Although this observation may be straightforward and potentially already known within certain circles, it has noteworthy implications for scalability, and the experimental results are convincing.
The concept of repurposing locally-learned representations for large-scale attention, where backpropagation would typically be prohibitively expensive, is intriguing and could potentially be leveraged to enhance other types of memory networks. 
My primary critique of this work is its relative simplicity and incremental nature compared to existing literature. Given its status as a straightforward modification of established NLP models, albeit with demonstrated empirical success, practicality, and simplicity, it may be more suited to an NLP-specific conference. Nevertheless, I believe that approaches that distill recent research into a simple, efficient, and applicable form should be recognized, and this tool is likely to be valuable to a substantial portion of the ICLR community, warranting its publication.