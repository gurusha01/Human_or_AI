This manuscript proposes the integration of autoregressive models with Variational Auto-Encoders, focusing on the regulation of information stored in the latent code, and demonstrates state-of-the-art performance on the MNIST, OMNIGLOT, and Caltech-101 datasets. 
The paper offers well-articulated discussions on various aspects, such as the impact of decoder capacity on latent code learning, bit-back coding, and lossy decoding, although these insights are not particularly innovative.
However, the distinction between auto-regressive priors and inverse auto-regressive posteriors presents a novel and intriguing concept.
By combining recent advancements in PixelRNN/PixelCNN with Variational Auto-Encoders and Inverse Auto-Regressive Flows, the authors achieve state-of-the-art results on the aforementioned datasets and exhibit some control over the information content in the latent code.
The paper effectively consolidates and presents insights on Variational Auto-Encoders from multiple sources, yielding state-of-the-art models on datasets of relatively low complexity. To further validate the approach, experiments on a larger scale would be necessary.