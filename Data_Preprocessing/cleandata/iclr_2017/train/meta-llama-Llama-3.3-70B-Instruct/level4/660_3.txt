As you acknowledged in Figure 5 Left, adjusting learning rates can be sufficient in some cases. I understand your point regarding Figure 6 Right, however, I have two concerns: 
1) it appears that not all effective learning rates lead to Adam's failure, and it seems that the selected rate was chosen because it demonstrated this behavior (notably, Adam initially converged faster than Eve on several occasions)
2) I am skeptical of the claim that "Eve always converges" since this is only demonstrated for a learning rate of 0.1, and given that Eve and Adam are distinct, a learning rate of 0.1 for Adam does not equate to the same for Eve due to the influence of d_t.
In my interpretation, you define dt over time using three hyperparameters, but it is also possible to define dt explicitly. The behavior of dt you present is not unusual and can be parameterized. If Eve indeed outperforms Adam, examining dt should reveal whether learning rates were underestimated or overestimated. While you could argue that Eve automatically adjusts this, it is worth noting that learning rates are tuned individually for each problem anyway, which somewhat mitigates the advantage of Eve's automatic adjustment.