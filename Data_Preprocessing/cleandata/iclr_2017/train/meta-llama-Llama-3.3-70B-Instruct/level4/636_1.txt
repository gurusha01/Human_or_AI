This paper introduces a novel initialization method for specific architectures and a correction technique for batch normalization to mitigate the variance introduced by dropout. Although the authors present intriguing observations, their claims lack substantial evidence to support them.
Figure 1 appears to be limited to the MNIST dataset and only explores two values of p with a single network architecture, making it unclear what dataset and empirical setup were used. 
The convergence demonstrated in Figure 2 is restricted to three dropout values, which may lead to an unfair comparison. For instance, it would be beneficial to see how convergence compares for the optimal dropout rate after cross-validation, including results for multiple methods with different dropouts. Additionally, the corresponding validation error and test iterations are not provided. Relying solely on MNIST may not be sufficient to guarantee generalizability to other benchmarks.
Figure 3 shows comparable results for the Adam optimizer; however, the learning rate was not selected using random search or Bayesian optimization, and the learning decay iterations were fixed, with the regularization coefficient set to a small value without tuning. A more thorough parameter tuning might bridge the current gap. Moreover, the Nesterov-based competitor yields unreasonably poor accuracy compared to recent results, which may indicate that this experiment is not reliable.
Table 2 fails to demonstrate significant improvement on CIFAR10, and the difference on CIFAR100 is insignificant without batch normalization variance re-estimation. However, the absence of results for 'original with BN update' makes it unclear whether the BN update is beneficial in general. Similarly, SVHN lacks results for the original with BN update.
To convincingly support the claims, baselines with batch normalization should be included in Figures 1, 2, and 3. The criticism regarding the additional computational cost of batch normalization, citing (Mishkin et al, 2016), should not preclude comparison to batch normalization. In fact, (Mishkin et al, 2016) performs such comparisons, including those with and without data augmentation using recent state-of-the-art architectures.
None of the empirical results incorporate data augmentation, making it uncertain whether the initialization or batch normalization update would be beneficial or detrimental in such cases.
Recent state-of-the-art methods, such as ResNet variants and DenseNet, have been shown to scale to considerable depths and report results on ImageNet. Although the authors claim their method can be extended to residual network variants, it is unclear whether there would be any empirical gain for these architectures.
This work necessitates a comprehensive and fair comparison to demonstrate its significance. Otherwise, the contribution appears minimal. 
Questions and suggestions for improvement include:
- Why not compare to recent state-of-the-art methods like ResNet variants or DenseNet in Section 2?
- The parameters are set to fixed values or selected from a small set of values. However, tuning with random search or Bayesian optimization is essential for obtaining meaningful comparisons. At this stage, it is challenging to determine whether the differences are due to the proposed approaches, as the parameters have not been reasonably fine-tuned.
- Are there any results on ImageNet?