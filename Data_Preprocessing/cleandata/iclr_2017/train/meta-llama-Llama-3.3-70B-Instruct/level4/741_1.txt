The paper examines the game of tic-tac-toe, selecting 1029 board combinations where a single move leads to victory for either player. A convolutional neural network (CNN) is trained on visual representations of the game board to predict 18 possible moves, with 2 players and 9 locations. The class activation mapping (CAM) technique is used to visualize the salient input regions responsible for the CNN's predictions, revealing that the predictions correspond to winning board locations.
The authors claim that this finding is:
1. Highly interesting.
2. Evidence that the CNN has learned the game rules.
3. Demonstrates the applicability of cross-modal supervision to higher-level semantics.
However, I disagree with claim (2) as there is no experimental evidence to support the notion that the CNN has learned the game rules. The study only considers a single stage of the game, namely the last move, and the results are based on the training set, which is insufficient to demonstrate generalization or implicit representation of game rules. Even if the CNN were to generalize, it would be premature to claim that it has learned the game rules.
Regarding claim (3), the authors' definition of cross-modal supervision appears to be limited to training from images to game moves, which is a well-established capability of CNNs. The mapping of images to actions, such as in DQN or DDPG, is a classical application of CNNs, and it is unclear what new insights the authors are attempting to convey.
As for claim (1), the interestingness of an implicit attention mechanism is subjective. The authors argue that supervising the CNN for "what will happen" allows it to learn "what to do," but this concept is extensively studied in model predictive control literature, where the model predicts "what will happen next" and is used to infer a control law, or "what to do." However, in the presented experimental setup, "what will happen" and "what to do" seem to be equivalent.
To further analyze what the CNN has learned, I recommend:
(a) Visualizing CAM with respect to incorrect classes, such as the player losing, to gain insights into the CNN's decision-making process.
(b) Splitting the data into training and validation sets and using the predictions on the validation set for visualization, which would provide more informative results about the generalizable features the CNN attends to.
In summary, understanding the decision-making process of CNNs is a fascinating research area. While the emergence of an implicit attention mechanism may be considered interesting by some, many of the authors' claims are not supported by experimental evidence, and further analysis is necessary to fully comprehend the CNN's behavior.