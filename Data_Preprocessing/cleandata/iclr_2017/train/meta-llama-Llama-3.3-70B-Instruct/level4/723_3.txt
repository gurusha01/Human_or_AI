This paper examines the application of cortical-inspired distant bigram representations for recognizing handwritten words, deviating from the conventional approach of using neural network-based posterior features for characters, optionally within a local context. Instead, it utilizes sets of posterior probabilities for character bigrams of varying lengths to represent words, with the primary objective of assessing the viability of this method and comparing it to the standard approach.
The submission is well-structured and clearly written, although it lacks comprehensive information regarding the comparison between the proposed and standard methods, as discussed below.
To provide a more thorough understanding, it would be beneficial to include the model complexity of all the models employed, specifically the number of parameters used.
Notably, language models are not incorporated in this study. Given that the different models leverage varying levels of context, the impact of language models on these approaches is likely to differ. Therefore, it is recommended that the evaluation include the use of language models to provide a more comprehensive analysis.
In the comparative experiments, only 70% of the data is utilized by selecting longer words. However, it is well-established that shorter words are more susceptible to misrecognition. The question arises whether this constrained corpus is advantageous for one of the tasks or not, and corresponding quantitative results should be provided to better assess the effect of using this limited dataset. Without clarification on this point, it is challenging to agree that the error rates are competitive or superior to the standard approach, as claimed at the end of Section 5.
The introduction of open-bigrams in an unordered manner is motivated by evidence from cognitive research. However, from a decision-theoretic perspective, it is puzzling why the order should be disregarded, given that the underlying sequential classification problem is inherently monotonic. An experiment that varies only the use of order would be intriguing, allowing for the differentiation of the effect of order from other aspects of the approach.
On page 1, the term "whole language method" requires explanation to ensure clarity.
Additionally, on page 6, the notation for rnn_d(x,t) should be defined.
The number of targets for the RNNs modeling order 0 (unigrams) and orders 1 and larger differ significantly. Consequently, the precision and recall numbers in Table 2 are not readily comparable between order 0 and orders >=1. At the very least, the column for order 0 should be visually separated to highlight this distinction.
Minor comments include the recommendation for a spell check. Specifically, corrections are needed for "state-of-art" to "state-of-the-art" (p. 2), "predict character sequence" to "predict a character sequence" (p. 2), "Their approach include" to "Their approach includes" (p. 3), "an handwritten" to "a handwritten" (p. 3), "consituent" to "constituent" (p. 3), "in classical approach" to "in the classical approach" (p. 4), "transformed in a vector" to "transformed into a vector" (p. 4), and "were build" to "were built" (p. 5). Furthermore, the first author's name in the references is incorrect, listed as "Thodore Bluche" instead of "Theodore Bluche".