This paper examines the characteristics of the Hessian matrix of the training objective for diverse neural networks and data distributions, with a particular focus on the eigenspectrum, which is linked to the optimization problem's difficulty and local convexity.
The paper presents several intriguing findings, including the local flatness of the objective function and the relationship between the data distribution and the Hessian. However, one limitation of the paper is that many of the described effects are presented as general trends, yet they are only tested in a specific context without the benefit of control experiments or rigorous mathematical analysis to support the claims.
For instance, the concentration of eigenvalues around zero, as illustrated in Figure 6, raises questions about whether this phenomenon is truly a result of the training process, such as increased insensitivity to local perturbations, or if it is an artifact of the specific scaling chosen for the initial parameters.
Furthermore, Figure 8 lacks a clear definition of data complexity, leaving ambiguity as to whether fully overlapping distributions, which would result in a zero Hessian, are considered complex or simple. 
Additionally, some plot legends, such as those in Figures 1 and 2, and labels are difficult to read when printed. Figure 3's plots also have inconsistent x-axis ranges, and the Hessian matrix image in Figure 1 does not display properly in printed format.