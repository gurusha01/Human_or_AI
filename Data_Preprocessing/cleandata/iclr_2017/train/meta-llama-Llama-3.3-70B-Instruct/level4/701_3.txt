This manuscript introduces a novel approach to adapting neural networks to new tasks with limited training data, deviating from the conventional fine-tuning method. The core idea involves learning a network that extracts features complementary to those of a fixed network, with the additional consideration of integrating the new network or features at various hierarchical levels. 
The concept presented bears similarities to the Progressive Nets paper by Rusu et al., although the motivations and experimental designs differ, lending this submission its own merit. A comparison with the ResNet approach, particularly in the context of learning residuals with stitched connections, would provide valuable insights.
The training methodology, specifically the reuse of a batch five times consecutively, raises questions about its effectiveness compared to standard SGD. 
Figure 5 would benefit from clearer labeling, specifically on the y-axis, and a revision from a bar chart to a format similar to Figure 4, which is more readable. Additionally, the inclusion of an "untrained model" in Figure 5 is puzzling, and clarifying its purpose and the rationale behind it, including how it compares to simply fine-tuning an additional layer ("Retrain Softmax"), would be helpful.
The results in section 3 might be influenced by the size of the network used, and exploring how these results change with a smaller network could provide a more comprehensive understanding. 
While the authors argue that the added connections and layers are intended to learn complementary features, supported by some visual evidence, introducing an explicit constraint in the loss function to encourage this, such as a soft orthogonality constraint, could strengthen their claim. The current reliance on small L2 regularization lacks clear justification or visualization of its effects.
A significant question arises regarding the performance of an ensemble of two pre-trained networks on the tasks considered, particularly in the context of the cars classification example, where fine-tuning VGG and ResNet separately and combining their outputs could serve as a strong baseline.
The absence of comparative results from previous publications in figures 4, 5, and 8 limits the ability to contextualize this work within the broader field. 
Overall, this work addresses an interesting and community-relevant problem of efficiently reusing previously trained classifiers for retraining on small datasets. While it takes a positive step forward, it falls short in certain aspects, such as comparisons to more robust baselines and deeper understanding of the proposed methodology.