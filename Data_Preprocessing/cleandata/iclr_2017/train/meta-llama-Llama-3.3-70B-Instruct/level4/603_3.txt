This manuscript investigates the task of source code completion utilizing neural network architectures. The authors propose several models, which are essentially variations of Long Short-Term Memory (LSTM) networks, tailored to accommodate the unique characteristics of the chosen data representation, where code is depicted as a sequence of pairs consisting of nonterminal and terminal symbols, with terminals optionally being EMPTY. An additional refinement is the introduction of a "deny prediction" option, a feature that seems reasonable in the context of integrated development environments (IDEs), as it may be preferable for the model to abstain from prediction when its confidence is low.
The empirical results indicate that the models perform worse than prior research in predicting terminals but demonstrate better performance in predicting nonterminals. However, the distinction between terminals and nonterminals appears unusual, and the significance of this dichotomy is unclear. A more straightforward metric for evaluating the system's usefulness could be the frequency with which it correctly suggests the next token that appears in the code. It would be beneficial to calculate and report a single, summary metric to encapsulate the performance.
Overall, the paper is satisfactory but bears the impression of applying LSTM models to an existing dataset without significant innovation. The results are acceptable but not exceptional. Furthermore, there are some writing issues that require improvement, as noted below. In aggregate, the contribution does not seem substantial enough to justify publication in a premier conference like ICLR.
Detailed comments include:
* The NT2NT model seems unconventional, as it predicts nonterminals and terminals independently, conditioned on the hidden state.
* The related work section requires revision. For instance, the work by Bielik et al. does not generalize the entirety of the research listed at the beginning of Section 2, and the citation for Maddison (2016) is incorrect.