The author undertakes a comparative analysis of Deep Neural Networks (DNNs) and human visual perception, adopting both quantitative and qualitative approaches. 
The first key finding involves a psychophysical experiment conducted on both humans and a model, with subsequent comparison of the results. Although the psychophysical data was obtained from a separate study, it was utilized to determine the noise level of additive noise required to make a just-noticeable-difference for humans in distinguishing between noiseless and noisy images across approximately 1110 images. The authors defined a neural network metric to measure a similar property in DNNs and then correlated the noise level patterns between DNNs and humans. The results showed that DNNs are superior predictors of human noise level patterns compared to simpler image perturbation measures, such as RMS contrast.
A second finding entails comparing DNNs to humans in terms of error patterns in a series of controlled experiments using stimuli that illustrate classic properties of human visual processing, including segmentation, crowding, and shape understanding. The authors employed an information-theoretic single-neuron metric of discriminability to assess error patterns in DNNs. The top layers of DNNs were able to replicate human error patterns across stimuli to some extent.
The third finding involves comparing DNNs to humans in terms of contrast sensitivity across a series of sine-grating images at different frequencies. The authors defined a DNN correlate for this property using the cross-neuron average of the L1-distance between responses to a blank image and responses to a sinusoid of each contrast and frequency. They then qualitatively compared the results for DNN models to known human results from the literature, finding a similar bandpass response for low-contrast gratings and a mostly constant response at high contrast.
The strengths of this paper include:
* The concept of comparing DNNs to psychophysical results in a detailed, quantitative manner is commendable.
* The definition of "linking functions" or metrics that express how specific behavioral results are generated from neural networks is a valuable contribution.
* The handling of psychophysical data demonstrates careful consideration and expertise.
However, there are some limitations:
* The paper does not present novel findings, as existing results have already demonstrated that DNNs are good models of the human visual system in various ways.
* A more significant contribution would have been to:
    (a) develop a comparison metric sensitive enough to distinguish between various DNN models, or
    (b) identify a substantial gap between DNNs and humans that remains unaddressed. Although the authors found a 24% gap between the explained variance of DNNs and inter-human consistency, this gap was not thoroughly explored.
* The paper's layout and presentation of quantitative results, particularly for the second and third findings, were unclear. For instance, Figure 8 was relegated to the appendix, and the quantification of model-human similarities for the data shown in Figure 8 was not provided.
* The quantification of model-human similarity for the data shown in Figure 3 was also missing, and it would have been beneficial to compare the human contrast-sensitivity curve to that of models in a more quantitatively precise manner.