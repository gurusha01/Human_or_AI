This paper discusses the design choices and experimental results of TerpreT [1], focusing on learning simple loop programs and list manipulation tasks, which serves as a bridge between the programming languages and machine learning communities. Unlike recent efforts in program induction by the ML community, this work emphasizes utilizing programming language design to constrain the search space, leveraging control flow structures (such as if-then-else, foreach, zipWithi, and foldli templates), immutable data, and type systems (including penalizing ill-typed programs and limiting searches to well-typed ones). From a high-level perspective, this approach occupies a middle ground between the ML strategy of making all aspects continuous for gradient descent and the PL approach of discretizing elements for structured, heuristic-guided combinatorial search.
A notable strength is the inclusion of a relevant baseline, \lambda^2, though the addition of a fully neural network program synthesis baseline would provide a more comprehensive comparison, even if it might only succeed on the simplest tasks. The unavailability of TerpreT and the experimental code is a significant drawback.
The recommendations for programming language design to facilitate gradient descent-based inductive programming are intriguing, but their applicability and performance on more complex tasks beyond loops are uncertain. The current tasks, while interesting, may bias the search towards a specific subset of constraints, such as those structuring control flow.
Overall, the paper is of sufficient quality for presentation at ICLR, though as a non-expert in program induction/synthesis, this assessment is provisional.
In terms of writing clarity, the paper could be improved. For instance, summarizing the model variants in a table, including boolean indicators for their features, would enhance readability. Specific suggestions include correcting "basis modern computing" to "of" in the introduction and rephrasing the training objective on page 3 to treat the output of \(r_R^{(T)}\) as a classification problem, detailing the use of cross-entropy as the loss criterion to better cater to the broader ML community. 
[1] "TerpreT: A Probabilistic Programming Language for Program Induction", Gaunt et al. 2016