This manuscript investigates various approaches for instance-level image retrieval using deep convolutional neural networks (CNNs). The methodology involves leveraging a pre-trained network for image classification (such as VGG) as a feature extractor, and then applying post-processing techniques commonly used in traditional retrieval pipelines that rely on hand-crafted features (e.g., SIFT + Fisher Vectors), which the authors refer to as "traditional wisdom".
More specifically, the authors explore several aspects, including the optimal location for feature extraction within the network (i.e., neuron activations of a convolutional layer), the most effective type of feature aggregation and normalization, the impact of image resizing, and the benefits of combining multiple scales. 
Although this experimental study is well-motivated and reasonable in its approach, it is significantly flawed by its failure to acknowledge two major recent studies ([a] "End-to-end Learning of Deep Visual Representations for Image Retrieval" by Gordo et al. and [b] "CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples" by RadenoviÄ‡ et al., both presented at ECCV'16) that directly contradict many of the paper's claims. These works have demonstrated the effectiveness of training for retrieval using siamese architectures, achieving outstanding performance and rendering many of the paper's findings outdated, questionable, or incorrect.
Several claims made in the paper are misleading, including:
  - The assertion that features aggregated from feature maps have achieved state-of-the-art performance in image retrieval tasks in recent years. However, until the publication of [a], the state-of-the-art was still dominated by methods based on sparse invariant features (as shown in the last table of [a]).
  
  - The claim that the proposed method outperforms state-of-the-art methods on four typical datasets is inaccurate, given the dominance of [a] and [b] in the current state-of-the-art.
  
  - The opinion that instance retrieval using unsupervised methods is preferable or the only option when a large number of training samples are not available is questionable. The method presented in "End-to-end Learning of Deep Visual Representations for Image Retrieval" by Gordo et al. surpasses the state-of-the-art on the UKB dataset (achieving 3.84 without QE or DBA) despite being trained for landmarks retrieval, demonstrating that training is both possible and beneficial even with insufficient data.
  
  - Many of the findings are neither new nor surprising (e.g., aggregating several regions in a multi-scale manner was previously achieved by Tolias et al.), which limits the paper's overall interest.
Furthermore, the experimental design has several issues. For example, tuning experiments are only conducted on the Oxford dataset using a single network (VGG-19), and it is unclear whether these conditions are representative of all datasets and networks. The Oxford dataset, for instance, is known to behave differently from the Holidays dataset. Additionally, the tuning process appears overly aggressive, suggesting that the authors may be tuning on the test set (as seen in Table 3).
In conclusion, the paper's failure to account for recent advancements in the field, particularly those presented in [a] and [b], makes it seem outdated. The lack of consideration for these significant developments undermines the validity and relevance of the paper's contributions to the current state of the art.