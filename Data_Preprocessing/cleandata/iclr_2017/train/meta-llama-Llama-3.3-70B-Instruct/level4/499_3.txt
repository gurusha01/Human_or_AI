Paper Summary 
This paper introduces a novel neural network architecture, where the weights of each layer are computed as a function of a latent representation associated with that layer. The authors present two instances of this architecture: a convolutional neural network (CNN) where each layer's weights are derived from a lower-dimensional embedding vector, and a recurrent neural network (RNN) where each layer's weights are computed from a secondary RNN state.
 Review Summary 
Strengths: 
- The idea of revisiting multiplicative RNNs and their predecessors is commendable, and the language model (LM) and machine translation (MT) results are impressive.
Weaknesses: 
- The paper requires improvement in terms of writing quality, as it is too lengthy for the conference format and needs refocusing.
- The related work section should discuss the relationship between the proposed architecture and multiplicative RNNs, as well as their generic tensor product predecessors, such as Order 2 networks.
- The paper's focus is unclear, as it seems to oscillate between achieving better performance and reducing network complexity.
It is recommended that the paper be shortened and clarified, potentially by omitting the CNN results for future publication. The authors should also provide a clear explanation of the relationship between their work and earlier research on multiplicative and Order 2 networks.
 Detailed Review 
The proposed architecture is a powerful one, and its introduction is a welcome development. However, the paper suffers from poor presentation and a lack of clear focus, which detracts from its overall impact. The authors spend excessive time on minor details while omitting crucial points, resulting in a paper that is too long and not self-contained without the appendices.
To improve the paper, the authors should consider spending more time discussing multiplicative RNNs and Order 2 networks at the beginning of the paper. This would allow them to highlight the differences between their work and earlier research. Additionally, they should explain why multiplicative RNNs were less widely used than gated RNNs, including any challenges they faced during optimization and training.
The term "hypernetwork" may not be the most effective choice, as it may not clearly convey the nature of the proposed architecture to readers. In Section 3.2, the authors seem to imply that different settings of hypernetworks can be used to vary between RNN and CNN architectures, but this is not entirely clear. A simple example or equation illustrating this concept would be helpful.
The work on CNNs and RNNs appears disconnected, with the CNN section focusing on achieving a low-rank structure for the weights and the RNN section targeting better perplexity and BLEU scores. The authors should clarify their goals and motivations for each section and provide a more cohesive narrative. For the CNN section, they should consider comparing their approach to alternative strategies, such as model compression or hashed networks.
The RNN section seems to prioritize better performance over model compactness, which may result in larger and more computationally expensive models. The authors should comment on the training time, inference time, and memory requirements for these models. To improve the paper's clarity and focus, it may be beneficial to omit the CNN results and focus solely on the RNN results, deferring the publication of the CNN results until a more comprehensive comparison with memory-conscious methods can be performed.
Some discussions in the paper are unclear, such as the message conveyed by Figure 2 or the discussion on saturation statistics. Similarly, Figure 4 may not effectively demonstrate the unique characteristics of the proposed network, as the comparison to regular LSTMs is lacking.
The results on handwriting generation are difficult to evaluate, as the log-loss metric is not easily interpretable. The authors should consider using alternative metrics, such as precision and recall, to provide a more comprehensive assessment of their model's performance.
The machine translation experiments are not sufficiently discussed in the main text, and the authors should provide more details about their approach and results.
Overall, the paper requires significant revisions to improve its clarity, focus, and overall quality. The authors should prioritize discussing the relationship between their work and earlier research on multiplicative and Order 2 networks, and they should strive to provide a more cohesive and well-organized narrative. 
 References 
M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, "First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,"IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, "Model Compression," The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.
Dark knowledge, G Hinton, O Vinyals, J Dean 2014
W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)