This study presents a spatiotemporal saliency network designed to replicate human fixation patterns, thereby facilitating the removal of irrelevant information from videos and enhancing action recognition capabilities.
The research is intriguing, having demonstrated state-of-the-art performance in predicting human attention on action videos, as well as showing potential in aiding action clip classification.
To further strengthen the paper, a discussion on the significance of context in attention would be beneficial. For example, if context plays a crucial role in capturing human attention, it is essential to explore why it is not automatically integrated into the proposed model.
One area of weakness lies in the action recognition section, where the comparison between methods (1)(2) and (3) appears to be biased. Notably, the attention-weighted feature maps actually decrease classification performance, only yielding improvements when the feature and model complexity are doubled by concatenating the weighted maps with the original features.
It would be valuable to investigate alternative methods for combining context and attention without relying on concatenation. The rationale behind concatenating features extracted from the original clip and the saliency-weighted clip seems to contradict the initial hypothesis that eliminating or down-weighting non-essential pixels would enhance performance.
The authors should also consider including current state-of-the-art results in Table 4 for comparative purposes.
Additional comments include:
- Abstract: A typo is present in the phrase "mixed with irrelevant...". The sentence "Time consistency in videos... expands the temporal domain from few frames to seconds" requires clarification.
- Contributions: The statement "The model can be trained without having to engineer spatiotemporal features" overlooks the need for collecting human training data.
- Section 3.1: The process of controlling the number of fixation points to be fixed for each frame is unclear and warrants explanation.
- In practice, freezing the layers of the C3D network to pre-trained values by Tran et al. raises questions about the potential benefits of allowing gradients to flow back to the C3D layers, enabling features to be optimized for the final task.
- Section 3.4: The precise method of concatenating features needs to be elaborated.
- A minor typo is present in the phrase "we added them trained central bias".