The authors present two methods for integrating multiple weak generative models into a more robust one, leveraging boosting principles. This approach is straightforward and refined, essentially yielding an unnormalized product of experts model, where individual experts are trained sequentially to optimize the overall joint model. However, this methodology leads to a joint model with certain undesirable characteristics, including an unknown normalization constant and an intractable log-likelihood on the test set, as well as the inability to draw exact samples from the joint model. Unfortunately, these issues are inherent to the product of experts formulation of boosting and cannot be resolved by employing different base learners.
The experimental results on 2D toy data demonstrate the feasibility of the proposed method, showing that the boosting formulation outperforms individual weak learners and alternative methods like bagging. In contrast, the experiments on MNIST are less compelling, as the absence of a definitive metric, such as log-likelihood, makes it challenging to draw conclusions from the samples in Figure 2, and visually, they appear inferior to even simple models like NADE.
To substantially enhance the paper, I recommend incorporating a quantitative analysis, which could involve examining the effects of combining undirected (e.g., RBM), directed (e.g., VAE), and autoregressive (e.g., NADE) models, as well as measuring the improvement as a function of the number of base learners. Nevertheless, this would necessitate developing a method to estimate the partition function Z or a suitable proxy.