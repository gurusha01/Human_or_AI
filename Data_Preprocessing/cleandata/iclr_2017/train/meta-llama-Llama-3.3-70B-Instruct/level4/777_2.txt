This paper introduces a novel regularization method for neural networks, aiming to enhance correlation among input variables, latent variables, and outputs by defining a total correlation measure that decomposes into entropies and conditional entropies. However, the authors opt to maximize a lower-bound of this total correlation, neglecting simple entropy terms and focusing solely on conditional entropies, without providing a clear rationale for this omission.
The application of entropy measures implies that the model's variables should be treated as random variables, following probability distributions. Nevertheless, the connection between the conditional entropy formulation and the reconstruction error remains implicit, suggesting the need for an explicit link, potentially through a noise model for the network units, to reconcile these perspectives.
The paper also asserts that the original ladder network is ill-suited for supervised learning tasks with limited sample sizes, supported by empirical results. A more in-depth theoretical analysis explaining this limitation would have strengthened the argument. Furthermore, the presentation of MNIST results for a specific convolutional neural network architecture, whereas most ladder network results on this dataset are based on standard fully-connected architectures, limits comparability with existing ladder neural network outcomes, making results for such architectures desirable for a more comprehensive evaluation.