The authors present a straightforward concept, wherein they impose a penalty on confident predictions by utilizing the entropy of the predictive distribution as a regularizer. This idea is explored through two variations: one that penalizes divergence from a uniform distribution, and another that penalizes distance from base rates, referred to as the "unigram" variation, although the terminology seems unconventional as it doesn't align with typical descriptions of multi-class labels. 
This concept, while simple, has seen limited application as a regularizer for enhancing generalization in supervised learning, despite its use in reinforcement learning. However, the justifications provided for this approach lack thorough analysis, and the comparisons made to L2 regularization in the author responses contain inconsistencies. A basic example using polynomial regression on a number line illustrates how L2 regularization can prevent overfitting, whereas it appears feasible to satisfy high entropy while fitting every data point, highlighting a need for clearer explanation of the interplay between the log likelihood objective and the regularization objective.
A plausible scenario could be that when the network outputs probabilities close to 0, it incurs high loss for labels equal to 1, and the entropy regularization stabilizes the gradient, preventing sharp losses on outlier examples, potentially leading to faster convergence. Empirical analysis of the effect on the distribution of gradient norms could provide valuable insights.
The paper's strength lies in its rigorous empirical evaluation, where the proposed idea is extensively tested on various benchmarks, including CNNs and RNNs, showing promising results, especially in language modeling tasks where it outperforms label smoothing in some cases.
Currently, I consider this paper a borderline contribution, but I am open to revising my assessment based on further revisions. Additionally, a minor correction is needed in the related work section, where "Penalizing entropy" should be clarified to mean penalizing low entropy.