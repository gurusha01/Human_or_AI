As previously noted, the authors' submissions encompass multiple concurrent contributions across various packages, which can be challenging to distinguish at times.
Nevertheless, it is noteworthy to observe a system capable of learning from natural feedback in an online manner, achieving results that are, to the best of my knowledge, unprecedented - particularly in cases where performance approaches that of full supervision in a less constrained setting.
Several concerns were raised and subsequently addressed by the authors:
1. The formalization of the task, specifically learning dialogue, lacks precision, making it difficult to determine when success can be declared.
The authors' response is partially satisfactory, suggesting that establishing more specific goals, such as matching full supervision performance, could be beneficial for this particular work.
2. Building on the previous question, dialogue can be viewed as a form of noisy supervision. The authors were asked to provide classic supervision baselines for the model used, which would help quantify the fraction of best-case performance achieved through dialogue learning.
The authors provided additional information, shedding light on the progress made towards the overall goal and highlighting open challenges.
3. There is a need to understand the increased difficulty of the machine translation (MT) setting. Potential approaches include hand-labeling feedback as positive or negative for analysis or testing a handcrafted baseline that extracts rewards via template matching or uses feedback length as a proxy.
The authors responded, although quantifying such baselines would have provided clearer confirmation that no simple handcrafted baseline could achieve comparable performance, rendering these concerns relatively minor.
4. The relationship to prior work, specifically Weston'16, is not entirely clear. It is understood that this submission is independent of Weston'16, rather than replacing it. However, Weston'16 makes this submission appear more incremental. The key aspect of this submission is the online component, leading to increased exploration, but an analysis of its impact is lacking in the experiments.
The authors clarified the issues, and their application of reinforcement learning, particularly FP, is convincing.
While the paper has an incremental nature, exacerbated by the authors' multiple concurrent contributions on this research thread, a more explicit comparison to prior work, including Weston'16, is necessary - both in the text and experiments, as partially done in the authors' response. Nonetheless, this contribution is deemed significant, worthy of sharing, and likely to impact how we learn in less constrained settings.