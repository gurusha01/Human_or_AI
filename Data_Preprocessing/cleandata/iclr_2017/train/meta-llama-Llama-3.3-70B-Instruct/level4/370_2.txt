Summary: 
This paper presents a novel model training approach aimed at enhancing accuracy by navigating the challenges of overfitting and underfitting. The proposed method, termed DSD, involves a sequence of training steps: initially training a dense network, followed by pruning to create a sparse network, then training the sparse network, and finally adding connections back to train the model as a dense network again. This DSD technique is versatile and can be applied to various neural network architectures, including CNN, RNN, and LSTM. The improved accuracy achieved through DSD is attributed to several factors: avoiding saddle points, enhancing robustness to noise through sparsity, and breaking symmetry to allow for more complex representations.
Pro:
A key contribution of this paper is demonstrating that models have the potential to achieve higher accuracy, as evidenced by the successful compression of models without sacrificing accuracy. This implies significant redundancy in models trained with conventional methods, suggesting that larger models can attain better accuracies with more sophisticated training schemes.
Cons & Questions:
However, the accuracy improvements reported are modest, typically in the range of 2-3% for most models. This raises questions about the cost of such improvements, particularly in terms of computational resources and performance, given that training large models is resource-intensive and time-consuming, even with high-performance GPUs.
Furthermore, it is unclear whether repeatedly applying the DSD cycle (e.g., DSDSD) could lead to further accuracy enhancements. Are there inherent limitations to this iterative approach, or can it be indefinitely extended to pursue higher accuracy gains?