The paper presents an innovative extension of traditional Recurrent Language Models, enabling them to handle unknown words by integrating a knowledge base (KB) module. This module allows the model to copy facts from the KB to generate unseen words, demonstrating improved efficiency and performance over standard RNNLMs on a new dataset.
However, the writing quality could be enhanced, particularly in the introduction to Section 3, which is somewhat difficult to follow.
Recently, similar approaches, such as "Pointer Sentinel Mixture Models" by Merity et al., have attempted to address the limitations of RNNLMs in dealing with unknown words by incorporating mechanisms to copy from a longer past history. In contrast, the current paper proposes a more intriguing approach by leveraging knowledge from an external source (KB) to enhance the language model. This is a more challenging task, as it requires effectively utilizing the large scale of the KB. The convenience of training this model is a notable advantage.
The proposed architecture appears to be sound, but the unclear writing hinders a comprehensive understanding, preventing a higher rating.
Additional comments and questions:
* How does the model address its dependence on the KB, considering that Freebase is no longer updated, which may lead to a lack of new unseen words?
* What are the results on standard benchmarks like the Penn Tree Bank?
* How does the training time compare to that of a standard RNNLM?
* What is the significance of the knowledge context $e$ in the model?
* How is the fact embedding $a_{t-1}$ initialized for the first word?
* When a word from a fact description is chosen as a prediction (copied) and has no embedding (unknown word), how is it encoded in the generation history for subsequent predictions? For instance, in the example in Section 3.1, what happens if "Michelle" is not in the embedding dictionary when predicting the next word?