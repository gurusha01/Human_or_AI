Summary of the paper
This manuscript examines the invertibility of convolutional neural networks within a random model framework, proposing a reconstruction algorithm akin to Iterative Hard Thresholding (IHT) for layer-wise network inversion.
Clarity:
- The presentation lacks clarity due to inconsistencies with standard deep learning notations, which may hinder comprehension.
Comments:
The analysis of the convolutional neural network (CNN) in this paper relies on two key simplifications that facilitate its mapping to a compressive sensing framework based on models:
1- The omission of non-linearity, specifically the Rectified Linear Unit (RELU), represents a significant simplification. For instance, in the context of random Gaussian weights, it is known from the Johnson-Lindenstrauss (JL) lemma that the L_2 distance can be preserved. However, the application of RELU alters the metric, as evident from the kernel formulation for n=1, highlighting the need for careful consideration of non-linear effects in the analysis.