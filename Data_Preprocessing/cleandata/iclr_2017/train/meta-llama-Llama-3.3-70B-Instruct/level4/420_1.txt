This manuscript concentrates on the application of attention mechanisms in neural language modeling, presenting two primary contributions:
1. The authors introduce a modified attention mechanism that utilizes distinct vectors for key, value, and predict functions, diverging from the conventional approach that relies on a single vector to perform these tasks. This innovation has the potential to be applied to other domains beyond language modeling.
2. The authors demonstrate that language models can achieve satisfactory performance with extremely short attention spans, a finding that, while not entirely unexpected, leads to the proposal of an n-gram RNN designed to leverage this characteristic.
The paper introduces novel architectures for neural language modeling and conveys several intriguing insights. A comprehensive experimental evaluation of the proposed concepts is conducted on both language modeling and CBT tasks, providing a thorough analysis.
The authors' responses to the preliminary questions posed during the review process are satisfactory and adequately address the concerns raised.
A minor suggestion is that the related work section would benefit from the inclusion of references to Ba et al., Reed & de Freitas, and Gulcehre et al. to further enrich the discussion of existing literature in the field.