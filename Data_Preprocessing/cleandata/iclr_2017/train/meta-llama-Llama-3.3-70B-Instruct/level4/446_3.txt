This manuscript presents an extension of the Generative Adversarial Network (GAN) framework, incorporating latent variables to enhance its capabilities. The approach involves augmenting the observed dataset by sampling latent variables z from a conditional distribution q(z|x), and subsequently modeling the joint distribution of x and z using a joint generator model p(x,z) = p(z)p(x|z). Both the conditional distribution q and the generator model p are then optimized through an adversarial process, where they aim to deceive a discriminator. This modification constitutes a significant enhancement of GANs, as it enables them to perform inference, thereby expanding their potential applications to areas previously dominated by alternative methods, such as Variational Autoencoders (VAEs).
The experimental results demonstrate considerable promise. Notably, the generated CIFAR-10 samples are among the most impressive to date, rivaling the quality of those produced by methods that utilize class labels. Furthermore, the proposed method achieves semi-supervised results comparable to those reported by Salimans et al. without relying on feature matching, suggesting that it may also contribute to improving the stability of GAN training.