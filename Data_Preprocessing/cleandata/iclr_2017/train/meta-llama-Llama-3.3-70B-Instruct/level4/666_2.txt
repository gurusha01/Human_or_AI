This paper presents a novel regularization technique for k-shot learning, leveraging orthogonal grouping of neural network units. The approach enforces maximal similarity among units within a group and encourages orthogonality between units from different groups. Although the motivation behind this method is appealing, the empirical analysis provided lacks conviction.
Several concerns arise:
1. The technique's performance is heavily dependent on the hyperparameters alpha and beta, with suboptimal choices leading to significant performance degradation, outweighing the modest gains achieved with proper tuning.
2. The optimal performance at a group size ratio of 0.5 seems counterintuitive, as the figures suggest that filter banks typically contain more orthogonal groups. This discrepancy raises questions about the alignment between the empirical evidence and the proposed approach's motivation.
3. The manuscript is marred by numerous typos, incorrectly formatted references, and unclear phrasing, which hinders comprehension.
I appreciate the authors' responses to my pre-review questions and would like further clarification on the following points:
1. Question 2: I am unsure if modifying \theta{map} alone would facilitate learning. Is \theta{map} solely used for defining groups? If so, it is unclear how the proposed method can be applied in a purely unsupervised setting.
2. Question 3: My previous inquiry was not related to fixed clustering based on pre-trained network filters. Clustering can be performed at each step of the k-shot learning process. I am perplexed by the visualization of filter groupings, whereas the actual algorithm groups activations.
Overall, while the paper is intriguing, it requires more robust empirical validation and improved presentation to strengthen its contribution.