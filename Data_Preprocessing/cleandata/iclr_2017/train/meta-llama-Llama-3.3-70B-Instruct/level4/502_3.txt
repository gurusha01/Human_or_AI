This paper tackles a crucial issue in evaluating automatic dialogue responses, as current methods like BLEU and METEOR often fall short or provide misleading results. The authors propose a straightforward yet intuitive approach, utilizing LSTM-based encoding of dialogue context, reference response, and model response(s) in a linearly transformed space, allowing for end-to-end training. The simplicity of this method is a notable advantage, facilitating both interpretability and speed.
The experimental section presents a comprehensive range of experiments that appear to be well-conducted, aiming to demonstrate the approach's applicability. However, as also noted by others, more in-depth insights from these experiments would be beneficial. Specifically, a detailed analysis of failure cases would be valuable, and exploring the approach's generalizability beyond the current dataset is essential. In my view, the paper could be strengthened in this regard.
Overall, I find the proposed ideas and approach to be sensible, and the paper is acceptable. The inclusion of qualitative results is a positive aspect, but a more thorough failure case analysis would be highly beneficial, such as providing an overview of significant cases where the proposed method does not align well with human judgment.