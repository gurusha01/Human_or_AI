This manuscript proposes a generative framework for modeling video sequences, wherein each frame is composed of a static background and a dynamically changing 2D sprite. The sprite's appearance and location are permitted to vary across frames. The approach adopted in this paper is based on the Variational Autoencoder (VAE) methodology, which utilizes a recognition network to infer the latent state at each time step.
Preliminary results are presented on simplistic synthetic datasets, including a moving rectangle on a black background and the "Moving MNIST" dataset. However, these results appear to be non-competitive with state-of-the-art performance on the Moving MNIST data. Furthermore, the assumptions underlying the model seem overly restrictive, potentially limiting its applicability to real-world video data.
The proposed model itself lacks novelty and overlooks relevant existing work. For instance, the forward model can be viewed as a variant of the DRAW model introduced by Gregor et al. at ICML 2014. Another pertinent study is the work by Huang and Murphy (ICLR), which employed a variational autoencoder with a spatial transformer and a sequence model resembling a recurrent neural network to generate images of multiple sprites on a background.
A significant concern with this paper is the brevity of its exposition, which omits crucial details necessary for reproducibility. Specifically, the architecture of the recognition model is not clearly specified, and low-level aspects such as initialization strategies are not discussed. These omissions undermine the paper's overall clarity and reproducibility.