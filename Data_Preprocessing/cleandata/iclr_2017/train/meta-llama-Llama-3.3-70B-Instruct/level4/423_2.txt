This paper proposes an extension of the GAN framework by incorporating multiple discriminators, motivated by two key perspectives: 
(1) utilizing multiple discriminators can be viewed as optimizing the value function through random restarts, potentially improving optimization in the face of the value function's nonconvexity, and 
(2) employing multiple discriminators may mitigate optimization issues that arise when a single discriminator is overly critical, as a generator receiving feedback from multiple discriminators is less likely to receive consistently poor gradient signals from all of them.
The core concept presented appears straightforward to implement and constitutes a valuable addition to the arsenal of GAN training techniques.
However, the effectiveness of the GAM (and by extension, GMAM) evaluation metric is not entirely convincing. Without empirical evidence indicating convergence of the GAN game, even approximately, it is challenging to assert that the discriminators provide meaningful insights into the generators' performance relative to the data distribution, particularly regarding mode coverage and probability mass allocation.
In contrast, the learning curves depicted in Figure 3 are more persuasive, suggesting that increasing the number of discriminators exerts a stabilizing influence on the learning dynamics. Nevertheless, Figures 3 and 4 also seem to indicate that the unmodified generator objective exhibits greater stability even with a single discriminator. This raises the question of whether multiple discriminators are necessary for training the generator using an unmodified objective.
Overall, while the ideas presented in this paper demonstrate promise, a more comprehensive analysis, akin to that in Figures 3 and 4, across additional datasets is necessary before considering it ready for publication.
UPDATE: Following discussion with the authors, the rating has been revised to a 7.