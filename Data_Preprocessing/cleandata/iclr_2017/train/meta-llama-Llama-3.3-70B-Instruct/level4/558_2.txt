The paper presents a novel exploration scheme for reinforcement learning, leveraging locality-sensitive hashing to construct a visit count table that encourages exploration in a manner similar to MBIE-EB. This approach has several appealing aspects: its simplicity compared to alternatives like VIME, density estimation, and pseudo-counts; the comprehensive results across various domains, including classic benchmarks, continuous control, and Atari 2600 games; and the comparison with several recent algorithms, primarily DQN variants. The results demonstrate a clear improvement over the baseline, although the comparison with other exploration algorithms yields more domain-dependent outcomes, which is acceptable given the technique's simplicity. Additionally, the paper investigates the sensitivity of the approach to the granularity of the abstraction.
However, a significant concern is the apparent engineering effort required to achieve these results, which raises doubts about the robustness of the conclusions. The impact of hyper-parameter values and specific design choices, such as the use of PixelCNN, weight tying, output noisification in the autoencoder, and custom additions to BASS, on the overall performance is unclear. The granularity results show sensitivity to the resolution choice, and the performance across games is inconsistent.
The decision to use state-based counts instead of state-action counts, deviating from theoretical foundations, is puzzling, especially since the use of LSH aims to approximate tabular counts as in MBIE-EB. The similarity in performance between state-based and state-action based counts in Atari games lacks explanation. Furthermore, the omission of DQN, despite its relevance and the comparison with DQN-based variants, seems odd. The justification for choosing TRPO over DQN, citing safe policy improvement, is not entirely convincing when exploration bonuses are added.
The case study on Montezuma's Revenge, although interesting, relies on domain knowledge and does not align well with the rest of the paper. In conclusion, the paper proposes a simple and elegant idea for exploration, tested across many domains, but the critical components of this approach and its robustness are not fully clear, which could affect its long-term impact.
After the authors' response, which provided additional insights into the robustness of SimHash and the comparison between state and state-action counting, the paper's merits become more apparent. It addresses an important problem with a simple counting method via hashing, offering an alternative to density estimation approaches like Bellemare et al. The wide comparison across domains and the baseline TRPO underscores the paper's value. While there are still concerns about the reproducibility of the results due to the numerous tweaks required for the approach to work, it presents an interesting, contrasting approach to exploration that deserves attention. The simplicity of the hashing method and its performance across various domains against the baseline are significant contributions, despite the technique's sensitivity to certain parameters and design choices.