This paper presents an approach to improve sentiment classification by incorporating a global context, calculated using a Bi-LSTM, into the attention mechanism. The proposed models demonstrate superior performance to several existing models in the field across three sentiment analysis datasets.
However, the core concept of utilizing Bi-LSTM to compute global context for attention mechanisms is not entirely new, having been explored in previous studies, such as those by Luong et al. (2015) and Shen & Lee (2016). Notably, Luong et al. (2015) have already suggested combining global context with local context for attention, which shares similarities with the current proposal.
In terms of experimental design, while it is commendable that the model performs well without relying on techniques like dropout or pre-trained word embeddings, it would be more comprehensive to evaluate the model's performance when these techniques are employed. The authors should consider presenting results that include such enhancements and compare them directly to existing literature outcomes.
Ref:
Luong et al. Effective Approaches to Attention-based Neural Machine Translation. EMNLP 2015