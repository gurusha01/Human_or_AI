This paper aims to acquire vector representations of boolean and polynomial expressions, enabling equivalent expressions to have similar representations. 
The proposed model utilizes a recursive neural network, initially introduced by Socher et al. (2012), where node representations are derived by applying a multilayer perceptron (MLP) to the representations of its children, recursively applied to obtain the full expression's representation. Notably, this paper extends Socher et al.'s work by employing multiple layers, crucial for capturing XOR operations, and introduces a reconstruction error, SubexpForce, allowing the recovery of child expressions from parent expressions. The model is trained using a classification loss, with labels corresponding to equivalence classes, and evaluated on randomly generated data, outperforming baselines such as tf-idf, GRU RNN, and standard recursive neural networks.
Although the importance of learning representations for symbolic expressions and capturing compositionality is acknowledged, the experimental setup raises concerns. The NP-hard nature of deciding boolean expression equivalence leads to questions about the model's ability to outperform implicit truth table computation. The paper is technically sound, with a well-adapted model, but its clarity is sometimes compromised. 
The strengths of the paper include:
- A relatively simple and sound model.
- The use of a classification loss over equivalence classes, which could be compared to using similarity measures.
However, several weaknesses are noted:
- The experimental setting is unconvincing, as it is unclear whether the model can surpass truth table computation for boolean expressions or polynomial expression evaluation at random points.
- Certain sections, such as the justification of SubexpForce and the discussion on softmax limitations, are challenging to follow.
- A comparison between classification loss and similarity loss is lacking, which would provide further insight into the model's performance.