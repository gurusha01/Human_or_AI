The authors present a novel, learnable method for reducing the dimensionality of learned filters in deep neural networks, which is an intriguing concept. However, the current manuscript appears to be somewhat underdeveloped.
1. The manuscript contains numerous typographical errors that need to be addressed.
2. The experimental results are somewhat underwhelming, failing to demonstrate significant improvements in accuracy. A more effective approach might be to position this work as a filter compression mechanism, which would require a comparison to low-rank filter approximation techniques in deep neural networks. Unfortunately, this crucial comparison is lacking.
3. Beyond compression, the proposed method can also be seen as a regularization technique to reduce unnecessary network capacity and enhance generalization. However, this aspect is not explored in sufficient depth.
4. If the authors wish to compare their approach to other one-shot learning methods, it would be essential to evaluate their method against siamese and triplet learning networks, a comparison that is currently absent.