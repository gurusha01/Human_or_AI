This paper presents an innovative extension of the HasheNets approach, introducing a novel methodology termed HFH. The proposed method utilizes multiple hash functions to map each virtual weight location to multiple components of a shared parameter vector, which are then processed through a small multilayer perceptron (MLP) to generate the final weight. This concept is intriguing and demonstrates substantial improvement over the original HasheNets technique through experimental results.
However, considering the broader context of neural network model compression, HasheNets itself may not be the most compelling technique, particularly when compared to more recent advancements in pruning- and quantization-based methods. The experiments conducted in this study reveal that the proposed HFH approach yields inferior accuracy at comparable compression ratios to pruning-based methods, without providing any runtime speedup benefits. Although the authors note that the technique incurs only a 20% slowdown, which is somewhat surprising, it is unclear why this method would be preferred over competing approaches for the types of networks presented in the experimental results. The authors suggest potential synergy with pruning-based approaches, but unfortunately, no experimental evidence is provided to support this claim. Furthermore, the paper highlights the ease of setting the compression ratio as a benefit of HFH, but this advantage seems insufficient to outweigh the significant drawbacks in terms of accuracy and speed.
In response to a query, the authors highlighted the technique's efficacy in compressing embeddings, where it appears to offer a genuinely useful contribution due to its minimal overhead and substantial train-time benefits. If the paper had focused exclusively on this setting and presented experimental results on tasks such as language modeling, which involve high-dimensional sparse or one-hot inputs requiring large embedding layers, it would have been a strong candidate for acceptance. Nevertheless, for the CNN and MLP networks that constitute the primary focus of the experiments, the technique seems less suitable, despite the inherent appeal of the underlying idea.