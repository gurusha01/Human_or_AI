The authors present a novel "gated attention" approach, leveraging a multi-hop mechanism to model interactions between query and document representations for cloze-style question answering. This involves sequential attention to the document representation across multiple hops, utilizing the similarity between query and document representations as the scoring function. The proposed method demonstrates improved or comparable performance to state-of-the-art results on various datasets, including CNN, Daily Mail, Who-Did-What, and CBT.
The strengths of this work include:
1. The innovative application of hierarchical attention to modulate context representation based on task-specific query representation, showcasing a thoughtful approach to capturing complex interactions.
2. The clarity of presentation, coupled with a comprehensive experimental comparison to the latest findings, enhances the overall quality of the submission.
However, several aspects warrant further clarification:
1. The architecture encompasses multiple components, including multi-hop attention, query-based gated attention, and independent query vector encoding at each layer. While the ablation study in Section 4.4 underscores the significance of gated attention, it remains unclear:
 
  (a) The extent to which multiple hops of gated attention contribute to the overall performance.
  (b) The importance of employing a specialized query encoder for each layer.
Elucidating these points would facilitate a more streamlined architecture.
2. The representation of tokens using L(w) and C(w) raises questions about the cruciality of C(w) to the method's performance. The notable performance decline in the absence of C(w), as observed in "GA Reader--", suggests its potential importance. However, due to concurrent changes in "GA Reader--", it is challenging to discern the isolated contribution of gated attention to the method's superior performance.