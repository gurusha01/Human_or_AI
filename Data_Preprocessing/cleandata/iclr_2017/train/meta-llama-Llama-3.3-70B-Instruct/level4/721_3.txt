A novel sparse coding approach is presented, which jointly learns features and their associated transformations. However, the inference process for per-image transformation variables proves to be challenging, prompting the authors to propose tying these variables across all data points, effectively converting them into global parameters, and utilizing multiple transformations for each feature. Additionally, a tree-based transformation model is suggested, where each path through the tree generates a feature by multiplying the root feature with the transformations linked to the edges. The one-layer tree model achieves comparable reconstruction error to traditional sparse coding while using fewer parameters.
This work represents a valuable contribution to the sparse coding and transformation model literature, as it addresses a difficult inference problem inherent to transformation models. Nevertheless, the usefulness of the overall approach is questionable. The authors assume that jointly learning sparse features and transformations is a crucial objective, yet this assumption is not substantiated through arguments or experimental evidence. It appears that this method does not enable novel applications, enhance our understanding of the what/where pathways in the brain, or improve natural image modeling capabilities.
The authors claim that their model extracts pose information; however, during testing, inference is only performed over the sparse coefficients associated with each feature-transformation combination, similar to traditional sparse coding. The benefits of associating each coefficient with a transformation are unclear, particularly given the existence of other models that achieve a similar what/where split.
It would be beneficial to verify that the $x{v->b}$ values undergo significant changes from their initial values. Despite the use of tied transformations, the loss surface remains problematic, suggesting that these values may not change substantially. A comparison with a model featuring fixed $x{v->b}$ values, chosen from a reasonable parameter range, would help determine whether the proposed model offers improved performance according to some metric.
One of the conceptually intriguing aspects of the paper is the introduction of a tree of transformations. However, the advantages of deeper trees are not convincingly demonstrated, with the approach only shown to work on toy data with vertical and horizontal bars.
Furthermore, it is unclear how the method could be extended to accommodate multiple layers. While transformation operators $T$ can be defined in the first layer, as they operate on the input space, this is not feasible in the learned feature space. Additionally, the processing of pose information in a hierarchical manner and the learning process in a deep version of the model are not well-defined.
In conclusion, I do not recommend this paper for publication, as it fails to clearly articulate the problem being addressed, and the method, although moderately novel, does not convincingly demonstrate the benefits of its innovative aspects.