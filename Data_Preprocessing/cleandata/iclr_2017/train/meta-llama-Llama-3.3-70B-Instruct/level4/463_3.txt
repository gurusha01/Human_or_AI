This study tackles the issue of supervised learning from strongly labeled data contaminated with label noise, a highly relevant problem in the realm of applied machine learning. The authors acknowledge the limitations of sampling methods like EM, which are not only inefficient and slow but also incompatible with end-to-end training. To address this, they introduce a novel approach that mimics the effects of EM through a noisy adaptation layer, essentially a softmax function, incorporated into the architecture during training and discarded during inference. The efficacy of the proposed algorithm is demonstrated on the MNIST dataset, where it outperforms existing methods designed to handle noisy labeled data.
Several points warrant further consideration:
1. The paper lacks an examination of the potential increase in training complexity resulting from the incorporation of an additional softmax layer into the model.
2. It is unclear why the authors opted for consecutive softmax functions rather than exploring alternative architectures, such as a compound objective with dual losses or a network featuring parallel losses and dual gradient sets.
3. The proposed architecture, limited to only two hidden layers, may not accurately represent the more complex and deeper models commonly used in practice, raising concerns about the scalability of the reported results to larger networks.
4. The evaluation of the approach is restricted to the MNIST dataset, which is notably simplistic and may not provide a realistic assessment of the method's performance in more challenging scenarios.