This paper provides a valuable contribution by systematically examining the performance and trainability of various neural network architectures, specifically the fundamental RNN motifs that have gained popularity recently.
The strengths of this paper include:
* It addresses a crucial question that many researchers, including myself, have been eager to explore but lacked the computational resources to investigate thoroughly, making efficient use of Google's resources to benefit the community.
* The research appears to have been conducted meticulously, ensuring the reliability of the results.
* The findings, which suggest that LSTMs are reliable but GRUs are generally the preferred choice in typical training environments, are both decisive and practically useful, with the paper nicely discussing the subtleties involved.
* The emphasis on distinguishing between capacity and trainability helps clarify a common misconception about the effectiveness of gated architectures, highlighting that they are more easily trainable but have slightly lower capacity than vanilla RNNs, with the benefits of better trainability outweighing the costs of lower capacity in challenging tasks.
* The observation that capacity is nearly equivalent at equal numbers of parameters is particularly useful.
* The paper underscores the importance of hyperparameter tuning, a crucial aspect that is sometimes overlooked in the plethora of papers on new architectures.
* The idea of quantifying the fraction of infeasible parameters, such as those that diverge, is a valuable contribution, as it addresses a practical problem commonly encountered when working with these networks.
* The paper is written in a clear and concise manner.
However, there are some weaknesses:
* The exploration of UGRNNs and +RNNs seems somewhat preliminary, and it is unclear whether the +RNN should be recommended with the same level of generality as the GRU, particularly in the absence of more comprehensive statistics on the significance of differences between +RNN and GRU performances, as seen in Figure 4 (8-layer panel).
* The paper provides limited details about the hyperparameter tuning algorithm itself, which could make replication challenging, despite referencing relevant studies.
* Some figures, such as Figure 4, are initially difficult to read due to small panel sizes, excessive details, and poor visual design choices.
* The reference to neuroscience ("4.7 bits per synapse") appears somewhat gratuitous, as the connection between the computational results and experimental neuroscience is tenuous and not well-explained, and could be rephrased to acknowledge the speculative nature of this comparison.