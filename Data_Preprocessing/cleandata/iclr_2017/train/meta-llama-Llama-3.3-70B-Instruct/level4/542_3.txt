This paper presents a novel reinforcement learning (RL) framework designed to learn policies from sketches, which are sequences of high-level operations for task completion. The proposed model employs a hierarchical structure, where sub-policies are selected based on the current operation in the sketch. The learning process is founded on an extended actor-critic model, incorporating curriculum learning for challenging tasks. The authors provide experimental results across various learning problems, comparing their approach to baseline methods.
The paper is well-organized and easy to understand. However, its impact is questionable, as the problem it addresses can be viewed as a variant of option-learning with enhanced supervision, given the sequence of options. This simplification limits the problem's complexity and, consequently, the paper's influence. Furthermore, the practical applications of this setting are unclear, making it less relevant compared to more pressing issues like learning from natural language instructions. Since the proposed model builds upon existing hierarchical reinforcement learning methods and lacks a compelling motivation or real-world application, its contribution to the RL community is marginal.
Key strengths of the paper include:
* The introduction of an original problem, accompanied by well-designed experiments.
* A straightforward adaptation of the actor-critic method to learn sub-policies.
However, significant weaknesses are:
* The task's simplicity, which can be seen as a simplification of more complex problems, such as options discovery, hierarchical RL, or learning from instructions.
* The absence of substantial underlying applications to bolster the approach's interest.