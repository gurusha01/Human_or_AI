This manuscript presents a groundbreaking finding, being the first to demonstrate that Convnets can be a viable alternative to RNNs in NMT encoders. The authors provide a compelling array of results across multiple translation tasks, benchmarked against highly competitive baselines. Additionally, the detailed analysis of training and generation speeds is a notable strength. The significance of position embeddings, alongside residual connections, is a fascinating discovery; however, further examination and comparison with alternative methods for capturing positional information, such as relative position embeddings, would provide valuable insights. A potential area of exploration could involve investigating the use of embeddings that represent relative positions, which might offer an interesting avenue for future research. My primary concern, echoing that of another reviewer, is that this work may be more suited to an NLP conference due to its focus.
On a minor note, it is somewhat surprising that this otherwise well-crafted paper lacks a visual representation of the proposed architecture, which would greatly enhance clarity. Including a figure to illustrate the biLSTM architecture would also be beneficial, particularly in elucidating the final paragraph of Section 2, where the introduction of a linear layer to compute z requires close attention to fully comprehend.