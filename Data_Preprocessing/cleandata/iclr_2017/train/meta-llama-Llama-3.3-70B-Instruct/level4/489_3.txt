This manuscript reports on a series of experiments that examine the types of information captured by prevalent unsupervised methods for learning sentence representations. The findings are noteworthy and counterintuitive. Notably, the results demonstrate that word order can be reconstructed from bag-of-words representations, and that LSTM sentence autoencoders can encode interpretable features even from randomly permuted nonsensical sentences.
The development of effective unsupervised sentence representation learning techniques is a crucial, yet largely unresolved challenge in the field of NLP. Research of this nature appears to be directly beneficial in addressing this issue. Furthermore, the experimental framework presented in this study is likely to have broader applicability to various representation learning systems. While some results seem unusual, there are no significant technical concerns, and the findings are deemed informative. Therefore, acceptance of this manuscript is recommended.
One minor concern that warrants attention is:
- The substantial decline in CBOW performance observed in Figures 1b and 4b lacks explanation and seems improbable, necessitating further investigation. It is essential to verify that these results are reproducible using a different codebase and random seed, while implementing the same model. Fortunately, this issue is largely independent of the manuscript's primary findings.
Two suggestions for improvement in the writing are:
- Although the results regarding word order and CBOW are surprising, it is slightly misleading to state that CBOW predicts word order. Instead, CBOW does not capture word order, but it is possible to probabilistically reconstruct word order from the information it encodes.
- The statement "LSTM autoencoders are more effective at encoding word order than word content" is not meaningful, as these two quantities are not comparable.