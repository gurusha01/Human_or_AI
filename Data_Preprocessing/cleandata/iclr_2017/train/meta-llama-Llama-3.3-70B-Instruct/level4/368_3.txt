This paper presents a methodology for evaluating generative models, including VAE, GAN, and GMMN, which is a significant contribution to the field as it moves beyond the current practice of visually assessing generated images to determine model quality. However, upon comparison with the NIPS 16 paper "Measuring the reliability of MCMC inference with bidirectional Monte Carlo" by Grosse et al., the technical advancements appear to be minimal or potentially nonexistent, with the primary contribution being the application of this method to generative models.
In Section 2.3, there seems to be an error in the notation. The authors state E[p'(x)] <= p(x), but it appears the correct expression should be E[log p'(x)] <= log E[p'(x)] = log p(x). Furthermore, it is unclear for which value of x this inequality holds, particularly if p(x) is normalized, as it cannot be true for all values of x. Additionally, there are minor typos and areas where the equations could be more precisely defined.
On page 5, it is mentioned that the AIS procedure can be initialized with q(z|x) instead of p(z), but the choice of x in this context is not specified. It is possible that the intention was to use Ep(x)[q(z|x)], but this needs clarification.
The usage of the term "overfitting" on page 8 is confusing. Typically, a model A is considered to overfit relative to model B if A's training accuracy significantly exceeds its test accuracy, not merely if A's test accuracy is higher than B's. The last sentence on page 8 might more accurately state that VAE-50 underfits less than GMMN-50, rather than implying an overfitting scenario.
The experimental results provide valuable insights, notably highlighting that GANs and GMMNs exhibit lower test accuracy compared to VAEs, despite generating visually appealing samples. This discrepancy underscores the importance of quantitative evaluation methods for generative models, as proposed in this paper.