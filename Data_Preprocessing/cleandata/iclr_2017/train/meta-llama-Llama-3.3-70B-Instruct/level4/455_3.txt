This manuscript presents an innovative extension of generative adversarial networks, wherein the conventional binary classifier discriminator is replaced by a discriminator that assigns a scalar energy value to each point in the generator's output domain. The discriminator is optimized using a hinge loss, while the generator strives to produce samples with low energy under the discriminator. The authors demonstrate that achieving a Nash equilibrium under these conditions results in a generator that effectively matches the data distribution, assuming infinite capacity. Experimental evaluations are conducted using an autoencoder-based discriminator, with an optional regularizer that penalizes generated samples exhibiting high cosine similarity to other samples within the minibatch.
The manuscript has several strengths, including:
* Clear and well-structured writing.
* The topic has significant potential to spark interest, as it paves the way for exploring a broader range of discriminators in GAN training.
* The theorems regarding the optimality of the Nash equilibrium appear to be theoretically sound.
* A thorough examination of hyperparameters is performed in the MNIST experiments.
* The semi-supervised results demonstrate that contrastive samples generated by the model can enhance classification performance.
However, there are some weaknesses and areas for improvement:
* The manuscript does not clearly establish connections to other works that have expanded the scope of the discriminator (e.g., [1]) or utilized a generative network to provide contrastive samples to an energy-based model (e.g., [2]).
* Based on visual inspection alone, it is challenging to determine whether the proposed EB-GANs produce superior samples compared to DC-GANs on the LSUN and CelebA datasets.
* The impact of the PT regularizer is difficult to assess beyond visual inspection, as the Inception score results are computed using the vanilla EB-GAN.
Some specific comments and suggestions for improvement include:
* In Section 2.3, the reasoning behind the reconstruction loss producing distinct gradient directions is unclear.
* In Section 2.4, the abbreviation "PT" for "pulling-away" may cause confusion.
* In Section 4.1, using an Inception model trained on natural images to compute KL scores for MNIST seems inappropriate; instead, an MNIST-trained CNN could be used to compute Inception-style scores.
* Figure 3 exhibits limited variation across the histograms, making it less informative.
* In Appendix A, the proof of Theorem 2 does not clearly explain why a Nash equilibrium of the system exists.
Minor comments and typos include:
* In the abstract, "probabilistic GANs" could be rephrased as "traditional" or "classical" GANs.
* In Theorem 2, the statement "A Nash equilibrium ... exists" could be rephrased for clarity.
* In Section 3, the phrase "Several papers were presented" should be used instead of the current wording.
Overall, while there are some concerns regarding the related work and experimental evaluation sections, the novelty and theoretical justification of the model, combined with the quality of the generated samples, make it a worthwhile contribution. 
References:
[1] Springenberg, Jost Tobias. "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks." arXiv preprint arXiv:1511.06390 (2015).
[2] Kim, Taesup, and Yoshua Bengio. "Deep Directed Generative Models with Energy-Based Probability Estimation." arXiv preprint arXiv:1606.03439 (2016).