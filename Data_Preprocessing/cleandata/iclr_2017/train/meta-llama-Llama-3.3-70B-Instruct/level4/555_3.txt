This paper explores an innovative approach to architecture search by framing it as a meta-learning problem. The authors propose a method where features are extracted from networks trained on diverse datasets, and a "ranking classifier" is trained to predict suitable architectures for new problems. However, the classifier's details are not thoroughly described. The approach involves running the ranker on extracted features for a new problem setup to infer a promising architecture.
A notable aspect of the paper is the fixation of key hyperparameters across all networks. I believe that optimizing the learning rate and its decay schedule is crucial. The paper's conclusions may be significantly altered if the authors had performed a thorough search over learning rates instead of training 11,000 networks. A more efficient approach could involve training 2,000 networks with five learning rates each, potentially yielding more compelling results.
I question the effectiveness of the protocol used to generate diverse architectures, given the limited maximum depth of eight layers and 14 components. Many of the generated architectures may have similar performance, making it unnecessary to train numerous networks on multiple tasks. The authors should consider implementing a pruning mechanism to filter out redundant networks, if not already done.
The batch normalization experiments in Table 2 appear unusual and underexplained. It is well-established that optimal learning rates can differ significantly when using batch normalization versus not using it. Given the fixed learning rate across all experiments, I view these results with skepticism.
The paper lacks insights into the top-performing architectures. Providing visualizations or trends would be beneficial in understanding the results.
The work seems to conflate the study of parallel and serial architectures with meta-learning, which are distinct issues. I take issue with the comparison of parallel and serial performance in Table 2, as it would be more appropriate to filter architectures by their parameter count or capacity.
Ultimately, the paper concludes that designing a good architecture for a new domain is challenging. However, the results suggest that the proposed ranker is not always reliable, making it difficult to see the paper as a constructive contribution. I am unsure whether this result will be practically useful for researchers applying deep nets to new domains.