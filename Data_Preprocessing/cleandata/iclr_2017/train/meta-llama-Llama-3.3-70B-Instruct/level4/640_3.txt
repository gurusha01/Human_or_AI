This paper presents a Bayesian adaptation of the skipgram model for learning word embeddings, introducing two key modifications to the original model. Firstly, the model leverages aligned sentences from multiple languages for training, enabling context words for a target word to originate from either the same sentence or an aligned sentence in a different language, thereby facilitating the learning of multilingual embeddings. Secondly, each word is represented by multiple vectors, each corresponding to a different sense of the word, with a latent variable z determining the appropriate sense based on the context.
I find the concept of utilizing a probabilistic model to capture polysemy intriguing, and the proposed model offers a compelling generalization of the skipgram model in this direction. However, I encountered difficulties in following the paper due to its complex formulation, which could potentially be simplified. For instance, considering a target word w and a context c, where c can be in either the source or target language, might enhance clarity without significantly altering the model. The results presented in Tables 2 and 3 appear to be somewhat underwhelming.
The core idea of representing word senses through latent variables in a probabilistic model is the paper's strongest aspect. Nevertheless, I believe that a clearer presentation of the method would substantially strengthen the paper. Additionally, I have reservations regarding the experimental results.
The paper's strengths include its innovative extension of the skipgram model to account for polysemy. However, its weaknesses lie in its unclear writing and the relatively low performance of the models as reported in the results.