This paper introduces a novel regulariser for convolutional neural networks (CNNs) that targets positive correlations between feature weights while preserving negative correlations, with an alternative variant that penalises all correlations irrespective of sign. The authors denote these approaches as "local" and "global", although these terms may be perceived as overly broad and potentially confusing due to their general nature.
The experimental validation undertaken is rigorous and comprehensive, encompassing several benchmark datasets such as MNIST, CIFAR-10, CIFAR-100, and SVHN, and demonstrating improvements in the majority of cases. Although these enhancements may appear modest, it is noteworthy that the baseline performances are already highly competitive, as acknowledged by the authors. However, this raises questions regarding statistical significance in certain instances. Further results utilizing the global regulariser, beyond just MNIST, would be of interest to elucidate the impact of selectively penalising positive correlations, as this distinction forms the core novelty of the paper.
A primary concern is the ambiguity arising from the paper's interchangeable discussion of activations and filter weights under the umbrella term "features". Nevertheless, the authors have indicated their intention to address this ambiguity.
The paper appears to overlook the interaction between the proposed regularisation technique and the choice of nonlinearity, which could be pivotal, especially given the objective of achieving uncorrelated feature activations solely through weight penalisation in a data-agnostic manner, without considering nonlinearity effects. The authors have mentioned plans to discuss this aspect, which is crucial for a comprehensive understanding.
Regarding the role of biases, the authors' response to the question about combining their technique with the "multi-bias" approach, while informative, did not fully address the underlying concern. The "multi-bias" method challenges the assumption that features should not exhibit positive correlation or redundancy, which underpins this work. The current perspective suggests that correlated features are acceptable as long as model capacity is not unnecessarily wasted on them, as observed in the "multi-bias" approach where weights are shared across correlated feature sets.
The introduction's dichotomy between regularisation methods that reduce model capacity and those that do not seems somewhat arbitrary, particularly with weight decay classified among the former and the proposed method among the latter. This classification appears to depend heavily on one's definition of model capacity, noting that weight decay does not actually decrease the parameter count in a model.
In summary, while the work may be considered somewhat incremental, its execution is sound. The results, although not groundbreaking, are convincing and contribute to the field, even if the improvements over existing baselines are modest.