SUMMARY 
This study examines the capacity of deep neural networks to express complex functions under various measures of expressivity, highlighting the relationship between these measures and the 'trajectory length', which exhibits exponential dependence on network depth, both experimentally and theoretically, under specific conditions. The importance of initial layer weights is also underscored due to their significant influence on the range of functions the network can represent, as demonstrated through experimental results.
PROS 
The paper contributes to the ongoing discussion on the expressive power of feedforward neural networks equipped with piecewise linear activation functions, providing a detailed analysis of the interconnections between different perspectives on this topic.
CONS 
Although the paper elaborates on interesting aspects related to neural network expressivity, it does not introduce substantially novel concepts to the existing body of knowledge, in my assessment.
COMMENTS
- The manuscript is somewhat lengthy, particularly the appendix, and appears to have been composed under time pressure. While the main arguments are clearly presented, the distinction between theoretical and experimental results, along with the underlying assumptions, could be more transparent. Furthermore, the connection to prior research could be more explicitly stated.
- On page 2, the statement regarding the comparison of architectures via 'hardcoded' weight values overlooks crucial aspects of the discussion in the referenced literature. Specifically, the work by Montufar, Pascanu, Cho, and Bengio (2014) examines classes of functions characterized by a specified number of linear regions, showing that deep networks inherently produce functions with a minimum number of linear regions, a capability not shared by shallow networks. This is significant for understanding the generic behavior of deep networks, even those with random weights.
- The paper discusses the number of dichotomies as a measure of expressivity, a concept also used in statistical learning theory to define the VC-dimension, where higher values indicate greater statistical complexity and thus a higher demand for data to select a suitable hypothesis.
- The assertion on page 2 that the three measures of expressivity are directly proportional to the trajectory length warrants closer examination. The exponential increase in expected trajectory length with depth can be seen as a scaling effect, but this alone does not account for the increase in dichotomies or activation patterns. The assumptions regarding the types of trajectories considered play a critical role, as hinted at by the observation that exponential growth is not observed when the variance of the bias is too large.
- In Theorem 1, it would be beneficial to provide more detailed specifications regarding the "random neural network," including the connectivity structure and the nature of the one-dimensional trajectory (e.g., its length, closure, and differentiability). The notation "g ≥ O(f)" could be clarified to avoid ambiguity, potentially by using the Ω notation for expressing asymptotic lower bounds.
OTHER SPECIFIC COMMENTS 
- Clarification on the randomness and structure of the neural network, as well as the characteristics of the trajectory, would enhance the precision of Theorem 1.
- The use of "g ≥ O(f)" in the theorem could be misleading; adopting a more standard notation for asymptotic relationships, such as Ω, might improve readability and avoid potential misinterpretations.