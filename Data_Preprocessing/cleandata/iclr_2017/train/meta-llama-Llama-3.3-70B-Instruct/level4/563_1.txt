This paper proposes an extension of the f-GAN framework by leveraging Bregman divergences for density ratio matching, addressing the criticism that the actual optimization objective of the generator during training deviates from the theoretical objective due to gradient issues. The b-GAN approach employs a discriminator as a density ratio estimator, where the generator aims to minimize the f-divergence between the distributions p and q by expressing p(x) as r(x)q(x).
However, the usefulness and practical implications of this approach remain unclear. Although the connection to density estimation is intriguing, the derived conclusions appear questionable, particularly in comparison to existing density estimation literature where the Pearson divergence has demonstrated greater stability. The authors' claim that similar stability holds for GANs is supported by experimental results, but the presentation of these experiments is confusing and lacks illuminating visualizations. The analysis of density ratios is uninformative, and the assertion that learning did not converge for certain divergences is unsubstantiated and lacks concrete evidence. Notably, the normal GAN objective was not explored in this context, despite the critical examination of its limitations. Furthermore, the b-GAN approach relies on multiple heuristic objectives and tricks, which seems to contradict the criticism of traditional GANs for using heuristic generator objectives.
A rewritten version of this paper with improved clarity would significantly enhance its readability and understanding. Currently, the motivation and intuition behind this work are difficult to discern, hindering a comprehensive evaluation of the proposed approach.