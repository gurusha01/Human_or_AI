This manuscript extends a conventional auto-regressive model for source code by incorporating a fixed attention mechanism that focuses on specific token types, such as identifiers, and accompanies this with the release of an open-source Python dataset. As anticipated, the introduction of this fixed attention policy enhances the model's perplexity. However, a more in-depth analysis of these findings, particularly in terms of how different token types contribute to the achieved perplexity, would be beneficial. Although the text hints at this, a more comprehensive comparison is desirable. The concept of leveraging expert knowledge through an attention policy is a valuable contribution, yet its novelty may be somewhat limited; for instance, the authors cite the Maddison and Tarlow 2014 paper, which employs scoping rules to track previously used identifiers within scope, a notion that shares similarities with the proposed approach.