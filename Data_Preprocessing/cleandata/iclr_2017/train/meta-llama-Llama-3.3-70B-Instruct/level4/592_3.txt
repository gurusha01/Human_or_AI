This manuscript presents a sophisticated approach to addressing a crucial issue in Variational Autoencoders (VAEs), where the model excessively regularizes itself, resulting in the elimination of latent dimensions. Although existing solutions, such as annealing of the KL term and "free bits," have been employed to circumvent this problem, a more effective solution is still required.
The proposed solution involves introducing sparsity into the latent representation, where only a subset of latent distributions is activated for each input, while still allowing multiple latents to be learned across the dataset.
However, it is unclear why the authors necessitate a topological structure in the latent representation. An alternative approach could involve placing a prior over arbitrary subsets of latents, potentially increasing the representational power without compromising the solution. This would allow for a greater number of possible latent combinations, which currently seems limited.
The first paragraph on page 7 is perplexing, as it suggests that under-utilization of model capacity can lead to overfitting, which is counterintuitive. 
The experimental results, although modest, are sufficient to support the claims made in the paper.
Overall, this manuscript introduces an intriguing idea that may provide a solution to a fundamental problem in VAEs, making it a worthy contribution to this conference.