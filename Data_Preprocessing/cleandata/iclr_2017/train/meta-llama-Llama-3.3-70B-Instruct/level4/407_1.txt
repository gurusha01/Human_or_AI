This paper presents an adaptation of a well-established soft mixture of experts model for a specific transfer learning problem in reinforcement learning, where action policies and value functions are transferred between similar tasks. Notably, the experimental setup bears resemblance to hierarchical reinforcement learning, although this aspect is not thoroughly explored. 
A potential implication of this work is that architectural and algorithmic choices could be defined in terms of the target task's objective, rather than relying on manual engineering, which presents an intriguing direction for future research.
The strengths of the paper include:
- A thorough explanation of how the network architecture integrates with various common reinforcement learning setups, facilitating future work.
- The experiments serve as effective proofs of concept, although they do not extend beyond this scope in my opinion.
- The work provides compelling evidence that collections of deep networks trained on related tasks generalize better to new tasks when used collectively, rather than through traditional transfer learning methods like fine-tuning.
However, there are several weaknesses:
- The concept of reusing libraries of fixed policies for similar tasks has been previously proposed, and it is known in hierarchical reinforcement learning that reusing libraries of fixed or jointly learned policies can be beneficial. However, the challenge of building such libraries remains unresolved, and this paper does not provide convincing insights in this regard.
- The selected transfer tasks effectively demonstrate the potential of the proposed architecture but do not address negative transfer or compositional reuse in challenging situations highlighted in prior work.
- Given the empirical nature of the main contributions, it would be interesting to see the results in figures 6 and 7 plotted against wall-clock time, as the relatively low data efficiency may not be a significant limitation in achieving perfect play in certain games. Considering tasks where final performance is limited by data availability would be more enlightening. Additionally, it would be valuable to explore whether the presented results can be achieved with reduced computation or representation sizes compared to learning from scratch, especially when one of the source tasks is a policy trained on the target task.
- Finally, it is somewhat underwhelming that the model requires a substantial amount of data to recognize that a perfect policy is already present in the expert library. A simpler approach, such as evaluating each expert for a few episodes and using a weighted majority vote to mix action choices, might achieve comparable performance with a smaller fraction of the data.