This paper presents a novel approach to training joint models for multiple NLP tasks, deviating from the traditional pipeline method where subsequent tasks rely on the output of preceding ones. Instead, the authors propose a neural framework that integrates all tasks into a single model, where higher-level tasks leverage both the predictions and hidden representations of lower-level tasks. Additionally, the paper introduces successive regularization, which intuitively aims to minimize disruptions to lower-level tasks when training higher-level ones, thereby preserving reasonable prediction accuracy.
From a modeling perspective, the proposed approach bears similarities to existing work, such as Zhang and Weiss (ACL 2016) and SPINN (Bowman et al, 2016), albeit in a simplified manner. The experimental setup is comprehensive, but the results in Table 1 are not entirely convincing, as the patterns are inconsistent and sometimes show decreased performance in higher-level tasks when trained with additional tasks. The comparison of dependency scores also raises concerns, as evaluating UAS/LAS without guaranteeing well-formed tree outputs may not be entirely fair.
While the concept of successive regularization is intriguing and intuitively sound, the current results do not provide sufficient evidence to support its effectiveness. A more thorough investigation of the training schema is necessary, including explorations of iterative training on different tasks, the relationship between training iterations and dataset size, and the impact of task-specific loss functions. Without such a detailed analysis, the benefits of successive regularization remain uncertain, and the results presented in the paper are not compelling enough to demonstrate its value in joint model training.