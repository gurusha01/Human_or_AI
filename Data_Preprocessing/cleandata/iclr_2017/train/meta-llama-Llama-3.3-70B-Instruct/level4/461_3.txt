This manuscript introduces a semi-supervised approach based on "self-ensembling," where the model is trained to regress towards a consensus prediction derived from previous epochs, in conjunction with the standard supervised learning loss. This concept bears resemblance to the "dark knowledge" idea, and the paper demonstrates the effectiveness of ladder networks in scenarios with limited labeled examples, among others. The authors propose two variants of this technique: a computationally intensive version that requires two passes through the same example at each step, resulting in high variance, and a temporal ensembling method that offers greater stability and reduced computational cost, albeit at the expense of increased memory requirements and an additional hyperparameter.
Overall, my assessment of this work is favorable. However, I do have some concerns, primarily regarding the temporal ensembling approach, which may demand substantial memory and infrastructure for large-scale experiments, such as those involving ImageNet. I am also perplexed by the experiments presented in Figure 2 and Section 3.4, which investigate the method's robustness to noisy labels. The claim that a classifier can achieve 30% or approximately 78% accuracy despite 90% of the labels being randomized seems implausible to me, and I struggle to understand the underlying mechanism.
Some minor suggestions for improvement include: 
bolding the best-in-category results in the tables to enhance clarity, 
discussing the ramp-up of w(t) in the main paper to provide additional context, 
including state-of-the-art results for the fully-supervised case in the tables for a more comprehensive comparison, and 
reconsidering the decision to limit the number of SVHN examples used, as the stated reason that it would be "too easy" appears somewhat contrived, and using all available examples would facilitate more straightforward comparisons to prior work.