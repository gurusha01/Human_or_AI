This paper presents an extension and in-depth analysis of the gradient regularizer introduced by Hariharan and Girshick in 2016, which was shown to enhance low-shot learning performance by penalizing gradient magnitudes. The current work demonstrates that this regularizer is equivalent to directly penalizing the magnitude of feature values, albeit with differing weights for each example.
The analysis provided in the paper includes two key examples that illustrate the benefits of feature penalty in learning better representations. The first example involves the XOR problem, where a specially designed network is used to show how a feature penalty can encourage a representation that makes XOR linearly separable. The second example examines a two-layer linear network and demonstrates how adding a feature penalty improves the stability of a second-order optimizer. Furthermore, the paper interprets this regularizer as imposing a Gaussian prior on both features and weights, suggesting that it has a soft whitening effect similar to Batch Normalization, albeit in a softer manner.
Experimental results indicate modest improvements on a synthetic XOR test set. On the Omniglot dataset, feature regularization outperforms most baselines but falls short of Moment Matching Networks. An experiment on ImageNet, similar to the one conducted by Hariharan and Girshick in 2016, also shows effective low-shot learning capabilities.
The strengths of this paper include its simple yet effective modification of the Hariharan and Girshick 2016 proposal, comprehensive analysis from both theoretical and empirical perspectives, and the potential broader impact of its connection to Batch Normalization.
However, several weaknesses are noted. The paper introduces the gradient regularizer of Hariharan and Girshick but expresses concern over the lack of clarity regarding why small gradients on every sample lead to good generalization experimentally. This central issue is not fully addressed. Additionally, the purpose and generality of section 2.1 are unclear, focusing on a specific case (XOR with a non-standard architecture) where feature regularization helps learn a better representation, but the intended takeaway is not explicitly stated. The analysis relies heavily on the choice of a specific non-linearity (x^2), which is not commonly used in recent neural network literature. Moreover, equation 3 seems to depend on assuming either an L2 or cross-entropy loss without providing a more general class of losses for which it holds.
The experiments on Omniglot and ImageNet are conducted with Batch Normalization, making it difficult to compare the effects of feature regularization and Batch Normalization directly. It is suggested that results without Batch Normalization should be provided for a more straightforward comparison. The ImageNet experiment could be improved by using the same class split as Hariharan and Girshick and measuring performance with more than one novel example per class.
Minor issues include the need for a brief comparison to Matching Networks in section 3.2, clarification of the intuition regarding data-cases recommending parameter updates, specification of whether the SGM penalty of Hariharan and Girshick is implemented from scratch or using their code, and consideration of using "proportional to" instead of "equal to" in equation 13. The presentation could be enhanced by moving detailed derivations to an appendix and focusing the main text on the results.
In overall evaluation, this paper offers an interesting analysis but lacks a clear explanation for why a gradient or feature regularizer improves low-shot learning performance. Despite this, the experiments support the conclusion, and the analysis itself is intriguing and may pave the way for a clearer understanding in the future. The work represents a somewhat novel extension and analysis of Hariharan and Girshick 2016, with some points requiring further clarification.