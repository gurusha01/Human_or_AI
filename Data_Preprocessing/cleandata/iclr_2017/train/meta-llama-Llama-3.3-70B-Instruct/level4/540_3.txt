This paper presents a novel approach to adapt a pre-trained network to various architectures without requiring a complete retraining process. The text is well-organized and the explanations are clear and straightforward to understand. Nevertheless, the reported results lack persuasiveness due to the choice of baseline models, which are significantly outdated compared to current state-of-the-art standards. To strengthen the paper, it is essential to include a comparison with cutting-edge architectures, such as wide residual networks, to provide a more comprehensive evaluation. Additionally, the inclusion of parameter counts for each architecture in the tables would facilitate a more accurate and fair comparison among the different models.