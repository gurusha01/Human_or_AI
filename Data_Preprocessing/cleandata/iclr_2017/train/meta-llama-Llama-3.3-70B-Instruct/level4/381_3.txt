The authors present a neural pruning method that commences with pre-trained models, leveraging an approximation of the change in the cost function to achieve superior performance compared to other criteria. Notably, they achieve substantial speedups while preserving acceptable accuracy, attributed to the fine-tuning process that follows pruning. However, the comparison to existing techniques is limited, as the GFLOPS graphs only illustrate a few basic baselines without including any prior work for reference. A more comprehensive comparison would strengthen the argument for the approach's superiority, thereby increasing its convincingness.