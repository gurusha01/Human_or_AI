This manuscript proposes a novel approach to program induction using program sketches in Forth, a stack-based language, by transforming the complex problem of program induction into a manageable slot-filling task. The introduction of a differentiable Forth interpreter enables backpropagation through the slots, which are treated as random variables. The use of sketches or partial programs allows for the learning of more complex programs compared to starting from scratch, leveraging prior information. The optimization process involves minimizing the L2 loss (RMSE) between the program memory at targeted addresses and the desired output, facilitating end-to-end learning through program flow. The authors demonstrate the effectiveness of their approach by learning addition and bubble sort using both Permute (3-way) and Compare (2-way) sketches.
The concept of creating a fully differentiable language for writing partial programs (sketches) to be completed later has been explored in the probabilistic programming community and more recently with TerpreT. The choice of Forth as the sketch definition language is noteworthy, as it occupies a middle ground between machine code (approaches like Neural Turing Machine, Stack RNN, Neural RAM) and higher-level languages (such as Church, TerpreT, ProbLog).
However, certain sections, such as 3.3.1 and Figure 2, could benefit from clarification, including explanations of the color coding and the parallel between D and the input list. The experimental section appears sparse, particularly for the sorting task, which only includes a single experimental setting (training on length 3 and testing on length 8), without investigating the generalization limits or relative runtime improvements with respect to the training set size and input sequence length. Furthermore, the absence of baseline comparisons, including exhaustive search or neural approaches, makes it challenging to assess the effectiveness of the proposed method. Similarly, the addition experiment in section 4.2 lacks detailed description and baseline comparisons, which are standard in neural program induction approaches. The claim that the sketch generalizes to longer sequences when trained on single-digit addition examples requires further clarification on the extent of this generalization (e.g., to three digits or more).
In conclusion, while the paper presents an intriguing approach, the experimental support appears insufficient to substantiate the claims and demonstrate the usefulness of the method.