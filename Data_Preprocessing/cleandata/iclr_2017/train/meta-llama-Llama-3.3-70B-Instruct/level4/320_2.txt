This manuscript explores the advantages of employing visual servoing with a learned visual representation. The authors suggest learning an action-conditional bilinear model of visual features, extracted from a pre-trained VGG network, to derive a policy through linearization of the dynamics. They introduce a variant of the bilinear model that incorporates multi-scale, multi-channel, and local connectivity. To address the limitation of the bilinear model's one-step-ahead prediction, the authors propose a weighted objective that takes into account the long-term values of the current policy, with evaluation performed using a fitted-value approach.
The paper is well-structured, mathematically rigorous, and conceptually thorough. The experimental results demonstrate the effectiveness of using a value-weighted objective, constituting a significant contribution. Notably, this work appears to be the first to outline a trust-region fitted-Q iteration algorithm. The use of pre-trained visual features is also empirically shown to enhance generalization.
In recommendation, I suggest accepting this paper as it has the potential to benefit numerous robotics researchers. However, in the context of this conference, I find the contribution to the "representation" problem to be somewhat limited, as it primarily demonstrates the utility of a pre-trained VGG representation without exploring end-to-end learning. Although this does not detract from the paper's value, it does allocate more attention to the control problem than to representation learning. Furthermore, the policy representation is fixed, and values are approximated in linear form using problem-specific features, which may not fully align with the focus of ICLR, although this does not diminish the paper's overall merit.