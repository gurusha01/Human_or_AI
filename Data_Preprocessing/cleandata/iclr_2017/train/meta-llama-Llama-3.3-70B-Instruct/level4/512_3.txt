This paper presents a method for learning non-linear activation functions in deep neural networks by representing them as a combination of non-linear basis functions and learning the corresponding coefficients, with the authors specifically utilizing a Fourier basis. The paper also provides a theoretical analysis of the proposed approach, leveraging algorithmic stability arguments to demonstrate the good generalization behavior of networks with learned non-linearities, particularly showing that the generalization error vanishes as the dataset size increases.
A key concern with this paper is that expressing a non-linear activation function as a linear or affine combination of other non-linear basis functions can be seen as equivalent to creating a larger network where the nodes use the basis functions as non-linearities, with specific constraints on the weights. This raises questions about the value of learning non-linearities compared to optimizing network capacity for a given task with fixed non-linearities. It is unclear whether the constraints implied by the learned non-linearity approach offer any specific advantages.
Additionally, clarification is needed regarding the initialization of the NPFC(L,T) activation in the two-stage training process for CNNs, particularly whether it is initialized to approximate ReLU or with random coefficients.
Several minor corrections and questions were also noted:
- On page 2, the interval notation seems to be incorrect, potentially needing correction from [-L+T, L+T] to [-L+T, L-T].
- The equation for f(x) on page 2 may require adjustment, specifically in the terms involving sin and cos, to either include or exclude "x" for consistency.
- In Theorem 4.2, the term "algorithm" appears unnecessary and could be removed for clarity.
- Theorem 4.5 references "SGM," which is not defined in the provided context, suggesting a need for further explanation or definition.