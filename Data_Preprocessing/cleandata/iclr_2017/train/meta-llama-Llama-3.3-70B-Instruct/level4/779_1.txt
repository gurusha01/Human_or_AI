This manuscript presents an exhaustive experimental analysis of vocabulary selection strategies aimed at mitigating the computational costs associated with neural machine translation. 
The investigation encompasses a broad spectrum of techniques, from straightforward approaches like word co-occurrences to more intricate methodologies involving Support Vector Machines (SVMs).
The experimental framework is robust, thorough, and practically relevant, yielding valuable insights. Notably, the most effective vocabulary selection method demonstrates a remarkable ability to achieve a high coverage proportion comparable to the full-vocabulary model, as evident in Figure 3. However, the scope of experiments in Section 4.3, which focuses on vocabulary selection during the training phase, appears somewhat constrained, and additional experiments in this area would be beneficial.
A significant concern with this paper is the lack of innovative contributions. The methodologies employed are largely standard and relatively simplistic, with noticeable similarities to the work of Mi et al. (2016), suggesting limited novel content beyond existing research. Although the research is methodically sound, its originality is compromised.
Minor observations include a question regarding the word co-occurrence measure in Section 2.1: was any smoothing technique applied to enhance the robustness of this measure against low count data?