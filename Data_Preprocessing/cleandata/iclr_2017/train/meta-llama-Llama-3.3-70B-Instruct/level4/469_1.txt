This paper presents an implementation of sparse-full convolutions and a corresponding model that investigates the potential speed-up of various sparsity levels for Convolutional Neural Networks (CNNs).
The first contribution, which focuses on the engineering aspect, is notable for making the source code publicly available, a gesture that is greatly appreciated by the community.
The second contribution is particularly intriguing, as existing pruning methods have primarily concentrated on reducing memory usage, resulting in only marginal speed improvements. Incorporating knowledge of runtime speed into a pruning algorithm appears to be a sensible approach to addressing this issue. The authors' methodical construction and thorough evaluation of the model are commendable.
The concept presented has the potential to be applied not only to pruning existing models but also to designing new architectures, where layers and parameters can be selected to achieve optimal throughput rates, presenting a promising direction for future research.
However, one notable omission is a discussion on the transferability of the performance model to Graphics Processing Units (GPUs), which would facilitate broader adoption of the technique.
Areas that require improvement include enhancing the readability of Figure 4, where the distinction between certain points (such as small red circles versus small red squares) is challenging, and considering an increase in the figure's size. Additionally, clarification is needed regarding the "base learning rate" mentioned in Section 3, specifically whether it refers to the initial or final rate within the annealing schedule. Lastly, there are minor typographical errors, including "punning" (page 4) and "spares" (page 5), which should be corrected.