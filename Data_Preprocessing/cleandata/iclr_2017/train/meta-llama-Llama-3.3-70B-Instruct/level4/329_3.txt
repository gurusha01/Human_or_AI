The authors propose a methodology to enhance the Generative Adversarial Network (GAN) training process by incorporating an additional term derived from denoising autoencoders. This approach is justified by the fact that denoising autoencoders inherently capture the data distribution they are trained on. Although denoising autoencoders alone have not yielded impressive generative models through sampling methods, this paper demonstrates their successful combination with GANs.
In my evaluation, the paper is well-structured, logically sound, and presents a conceptually solid idea grounded in fundamental principles. However, I perceive the idea as evolutionary rather than revolutionary, building upon existing concepts without a significant departure. The experiments section could benefit from a more systematic approach, providing deeper insights into the advantages and trade-offs of different design choices, rather than solely demonstrating the efficacy of a particular variant.
In addition to these general comments, I have previously posted specific queries and criticisms in the pre-review section, and I appreciate the authors' responses. Following their replies, my primary concern lies in the validity of the intuition presented by Alain & Bengio (2014) regarding denoising autoencoders when applied to nonlinear feature spaces. Specifically, if the denoiser function's behavior is influenced by the Jacobian of the nonlinear transformation Phi, it raises questions about whether this dependency can be effectively exploited by the optimization algorithm.