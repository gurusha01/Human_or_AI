This paper presents two novel extensions to the existing state-of-the-art summarization approaches based on sequence-to-sequence models that incorporate attention and copy/pointer mechanisms.
1. The authors introduce a 2-pass reading mechanism, where the representations generated during the first pass are utilized to re-weight the contribution of each word in the sequential representation of the second pass. This "read-again" process is applied to both Gated Recurrent Units (GRU) and Long Short-Term Memory (LSTM) architectures, as described by the authors.
2. On the decoder side, the authors employ a softmax function to choose between generating words from the decoder vocabulary and copying a source position, with a new approach to representing the previously decoded word, Y_{t-1}. This enables the exploration of a smaller decoder vocabulary, resulting in faster inference times without compromising summarization performance.
The paper claims to achieve a new state-of-the-art on the DUC2004 dataset; however, the comparison with other models on the Gigaword dataset appears incomplete, lacking more recent results, such as those after Rush 2015. While the overall work is sound, some scientific aspects are missing, including:
- An analysis of the computational costs added by the second-pass reading to the end-to-end system.
- An evaluation of the decoder's small vocabulary trick without the second-pass reading on the encoder side, in terms of both summarization performance and runtime speed.
- A comparison of the 2-pass reading mechanism with other recent approaches to improving sentence embeddings, such as self-attention and/or LSTM-Networks, as proposed by authors like Cheng et al. (2016) and Parikh et al. (2016).