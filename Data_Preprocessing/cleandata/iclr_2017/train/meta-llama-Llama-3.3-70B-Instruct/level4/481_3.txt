This manuscript is well-structured and presents its content in a clear manner, comprising two primary sections: 
1. Adversarial training on the ImageNet dataset, and 
2. An empirical examination of label leakage, single and multiple step attacks, transferability, and the significance of model capacity.
Regarding the first section, it appears questionable whether training devoid of clean examples can yield a reasonably performing ImageNet-level model. The experiment conducted by Ian in "Explaining and Harnessing Adversarial Examples" is notable for its omission of BatchNorm, a component potentially crucial for the training of large-scale models. This portion of the paper seems to build upon Ian's work, utilizing the Inception-V3 model. To strengthen this section, an additional experiment investigating the effects of training without clean samples would be beneficial.
The second section encompasses a broad range of variables pertinent to adversarial training, although it lacks a deeper technical analysis. The observations related to depth and model capacity can be attributed to the regularizing effect of adversarial training. The concept of label leaking presents a novel contribution. In the transferability experiment employing FGSM, a meticulous examination of specific MNIST examples generated by FGSM reveals an augmentation effect, where certain features, such as grey areas in images, can make numbers appear more similar to other numbers. Although this phenomenon is more challenging to observe in complex datasets like CIFAR-10 or ImageNet, it may underlie the authors' observation that FGSM examples exhibit the highest transferability.
While this section poses several intriguing questions and hypotheses, it falls short in providing theoretical underpinnings for these empirical findings. 
Overall, the empirical insights presented in this paper are deemed valuable for informing future research endeavors.