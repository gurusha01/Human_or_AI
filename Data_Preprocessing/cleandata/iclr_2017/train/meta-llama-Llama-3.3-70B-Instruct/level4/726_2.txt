This paper establishes a fascinating link between dropout, the "information bottleneck" concept introduced by Tishby et al, and Variational Autoencoders (VAEs). The classification process of predicting 'y' from 'x' is decomposed into three components: an inference model z ~ q(z|x), a prior distribution p(z), and a classifier y ~ p(y|z). The optimization objective, given by E{(x,y)~data} [ E{z~q(z|x)}[log p(x|y)] + lambda * KL(q(z|x)||p(z))], with lambda <= 1, effectively creates an information bottleneck at 'z', where lambda regulates the upper limit on the number of bits transmitted through 'z'.
Notably, this objective is analogous to the VAE objective, but with a downweighted KL divergence between the posterior and prior, utilizing an encoder that processes 'x' as input and a decoder that solely predicts 'x'.
The discussion of related work in section 2 is satisfactory. However, in section 3, a brief recap of the definition of mutual information would be beneficial for clarity. The connection to VAEs explored in section 5 is particularly intriguing.
Unfortunately, the experimental results on MNIST and CIFAR-10 datasets are underwhelming, which is somewhat disappointing given the potential flexibility of the proposed method compared to other dropout variants. Furthermore, the substantial discrepancy between the reported CIFAR-10 results and the original architecture's performance is unclear. Additionally, the version of 'beta' used in figure 3a is not explicitly stated.
In summary, while the theoretical framework presented in the paper holds promise, the experimental results lack convincing evidence of significant improvements. I recommend that the authors conduct further experiments to demonstrate substantial enhancements, particularly on CIFAR-10 and potentially on more complex problems, to strengthen their claims.