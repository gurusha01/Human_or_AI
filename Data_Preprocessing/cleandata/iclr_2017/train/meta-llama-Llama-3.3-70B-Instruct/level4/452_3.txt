This paper presents a straightforward yet effective modification to reinforcement learning algorithms by incorporating a temporal repetition component into the action space, allowing the policy to determine the duration for which the chosen action is repeated. This extension is applicable to all reinforcement learning algorithms, encompassing both discrete and continuous domains, as it primarily alters the action parametrization. The paper is well-written, and the experiments provide an extensive evaluation of the approach using 3 different RL algorithms across 3 distinct domains (Atari, MuJoCo, and TORCS).
To further enhance the paper, several points are worth considering:
The introduction contains the statement that "all DRL algorithms repeatedly execute a chosen action for a fixed number of time steps k," which is overly broad and contradicted by the experimental findings. It is observed that repeating an action is beneficial in numerous tasks but not universally applicable. Therefore, this statement should be rephrased for greater precision.
In the related work section, a discussion on the relationship to semi-MDPs would be beneficial in providing the reader with a deeper understanding of the approach, including its similarities and differences. This could be informed by the response to the pre-review questions.
Regarding the experiments, several suggestions can be made:
- The inclusion of error bars in the experimental results, derived from running multiple random seeds, would be valuable.
- Conducting experiments with parameter sharing in the TRPO experiments would enhance consistency across domains, particularly given the smaller improvement observed in the TRPO experiments compared to the other two domains. Currently, it is challenging to discern whether the smaller improvement is due to the task's nature, the lack of parameter sharing, or another factor.
- The TRPO evaluation differs from the results reported in Duan et al. (ICML '16). It would be beneficial to use the same benchmark for comparison.
- The videos provided only demonstrate policies learned with FiGAR, which are less informative without a comparison to policies learned without FiGAR. Including videos of policies learned without FiGAR would serve as a useful comparison point.
- The number of laps DDPG completes without FiGAR is not specified, despite the substantial difference in reward achieved (557K vs. 59K).
- Visualizing the tables as histograms could more effectively communicate the results.
Minor comments include:
- The label for the first bar in Figure 2 should be corrected from 1000 to 3500.
- The phrase "idea of deciding when necessary" could be improved to "idea of only deciding when necessary."
- A space is missing in "spaces.Durugkar et al."
- The notation "R={4}" could be enhanced by using a letter to indicate a constant or an alternative notation.