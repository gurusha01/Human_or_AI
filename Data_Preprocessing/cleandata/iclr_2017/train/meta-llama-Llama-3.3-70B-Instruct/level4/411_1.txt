This research extends the STOKE superoptimization engine (Schkufza et al., 2013) for program binaries, which operates by modifying existing programs based on a proposal distribution and accepting these modifications according to the Metropolis-Hastings criteria. This criteria considers both program correctness and performance, making it likely for the Markov Chain Monte Carlo (MCMC) process to converge on correct, high-performance programs. Typically, the proposal distribution is fixed, but this work innovates by learning the proposal distribution as a function of program features, specifically the bag of words of all opcodes in the program. Experimental comparisons with baseline methods, including a uniform proposal distribution and a learned proposal distribution without feature conditioning, show slightly improved performance for the proposed method.
However, the significance of this work in the context of ICLR appears limited, as it represents a straightforward application of neural networks and REINFORCE to a task with non-differentiable components, rather than a breakthrough in learning representations. The task of superoptimization itself may not be of broad interest to the ICLR audience, suggesting that conferences like AAAI or UAI might be a more suitable fit.
The method proposed is novel in its application of learning to MCMC-based synthesis, which typically lacks a learning component. To enhance the impact of this work, demonstrating its applicability to other synthesis tasks or more broadly to tasks utilizing Metropolis-Hastings MCMC, where a learned proposal distribution could offer benefits, would be beneficial. The current focus on superoptimization, with only marginal improvements over baselines, is not sufficiently compelling.
Furthermore, it is unclear whether substantial representation learning occurs in this approach. The use of a bag-of-words feature to represent programs means the neural network can only learn correlations between opcode presence and effective moves, without any deeper understanding of program semantics. A more interesting contribution might have involved using a model like Tree-LSTM, which attempts to learn program semantics. The simplicity of the learning method employed diminishes the paper's suitability for acceptance. Additional insights from the authors in response to reviewer questions would be helpful in providing a more comprehensive evaluation.