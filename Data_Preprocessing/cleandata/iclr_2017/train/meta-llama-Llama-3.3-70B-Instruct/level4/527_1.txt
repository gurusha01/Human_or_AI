* Brief Summary: 
This manuscript presents an extended version of multiplicative RNNs, specifically tailored to LSTM models, yielding a proposal that bears a strong resemblance to existing work in [1]. The authors provide experimental results for character-level language modeling tasks, demonstrating a well-structured and clearly explained paper.
* Criticisms:
- The paper's contribution is somewhat limited, as the concept, although well-motivated, has been previously explored in [1] and builds upon [2], essentially positioning this work as an application-oriented study.
- While the results are promising, they fall short of the current state-of-the-art, particularly when dynamic evaluation is not employed.
- Certain unconventional modifications to standard algorithms are noted, including the "l" parameter in RMSProp and the premature multiplication of the output gate before nonlinearity application.
- The experimental scope is restricted to character-level language modeling, lacking diversity in evaluated tasks.
* An Overview of the Review:
Pros:
- The proposed modification is straightforward and demonstrates reasonable practical performance.
- The manuscript is well-written and easy to follow.
Cons:
- The experimental results do not sufficiently surpass existing benchmarks.
- The contribution of the paper is not substantial, representing a nearly trivial extension of existing algorithms.
- The incorporation of non-standard modifications to established algorithms may raise concerns.