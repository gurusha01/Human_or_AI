This manuscript presents a novel approach to mitigating the mode collapsing issue in Generative Adversarial Networks (GANs) by employing a large ensemble of generators and discriminators, which are dynamically paired and re-paired throughout the training process. The underlying concept is that by preventing any single generator-discriminator pair from becoming overly specialized, the mode collapsing problem can be alleviated. This idea is intriguing and tackles a significant challenge in GAN training. However, the paper falls short in terms of empirical evidence. Specifically:
- The authors should provide a more rigorous justification for the GAM metric, as its effectiveness in evaluating generator networks is not immediately apparent, particularly given its reliance on discriminator predictions that may focus on artifacts. It would be beneficial to investigate the correlation between the GAM metric and other established evaluation methods, such as inception scores or human assessments, to validate its relevance.
- Closely related to the previous point, the authors should conduct a more comprehensive comparison with existing methods, including the evaluation of inception scores and generation quality. Currently, it is unclear whether the proposed method yields superior sample quality compared to other approaches.
Reiterating questions from the pre-review section:
- Would simply training K GANs on K data splits or K GANs with varying initial conditions (without swapping) yield similar improvements? Similarly, what are the effects of training larger capacity models with dropout in both the generator and discriminator, considering that dropout essentially averages multiple models?
- Figure 6 suggests that as parallelization increases, the validation costs remain constant, while the training cost increases, resulting in a narrowing gap. Does this truly imply enhanced generalization performance?
In conclusion, while the paper addresses a crucial issue in GAN training and presents an interesting idea, it lacks convincing results to support its claims.