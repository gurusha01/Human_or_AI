The paper proposes a novel framework for formulating data structures in a learnable manner, which has the potential to generalize well to various data structures and algorithms. However, in its current form (as of the December 9th revision), two significant weaknesses persist: the analysis of related work and the provision of experimental evidence.
As Reviewer 2 has already highlighted, there are notable related works, particularly those by DeepMind (with which I have no affiliation), such as the neural Turing machine and subsequent research, that present highly relevant results. Although direct comparisons in the experimental section may be challenging due to the complexity of reimplementation, it is essential to conceptually mention and compare these works.
The experimental section primarily presents qualitative results, which do not conclusively address the topic. To improve this, several suggestions are proposed:
* Investigating the accuracy of stack and queue structures for an increasing number of elements to store would be highly informative.
* Examining whether a queue or stack can be utilized in arbitrary situations involving push-pop operations, even if it was only trained on consecutive pushes or consecutive pops, would be valuable. Does it diverge in this enhanced setting?
* The encoded MNIST elements, although in a 28x28 (binary) space, belong to a ten-element set and can be encoded more efficiently through parsing, which CNNs can perform well. It is essential to determine if the neural network is simply learning to parse, in which case its performance can be expected to degrade significantly when dealing with more than 196 numbers (assuming an optimal parser and lossless encoding). Conducting experiments with an increasing number of stack or queue elements and using an MNIST parsing neural network in conjunction with the stack/queue network could help support or refute this claim.
* The claims regarding "mental representations" lack substantial support throughout the paper. If evidence indicating correspondence to mental models can be found, it would substantiate the claim. Otherwise, it is recommended to remove this claim from the paper and focus on the neural network aspects, potentially mentioning mental models as motivation.