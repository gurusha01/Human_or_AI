This paper presents a unique application of the sticky HDP-HMM, aiming to accurately determine the number of components in bird and whale songs across various datasets. The use of this model on an intriguing dataset is a welcome development. However, the paper's structure and representation choices raise several concerns. 
The organization of the paper could be substantially enhanced. The abstract contains redundant introductory material that could be omitted, including the first and last two sentences. Additionally, much of the abstract repeats the introduction, and the second paragraph of section 2.3 reiterates information already presented. The reference to Kershenbaum (2014) may be unfamiliar to many readers, and the data description would be better suited to the experiments section. The introduction's phrase "Different hypotheses for the songs were emitted" is somewhat unusual. Figure 4 would be more effective as the first figure in the introduction, while Figure 5 would be better placed in the methods section. A summary of Table 1 should be included in the experiments section. Overall, the writing could be more concise, allowing for the inclusion of these figures. The description of the HDP-HMM, which largely follows existing literature, is well-executed.
Several questions arise regarding the methods employed:
The choice of Gibbs sampling for scalable inference is puzzling, given the existence of more advanced methods like the beam sampler (van Gael 2008), which has recently been considered state-of-the-art for MCMC inference in HDP-HMM. More broadly, the use of MCMC is questionable, as the majority of the Bayesian machine learning community has shifted towards stochastic variational inference for large datasets (e.g., Wang, Paisley, and Blei 2011).
If the primary focus is on estimating the number of clusters, how do the authors address the known inconsistency of DP mixture models in estimating the true number of clusters (Miller and Harrison 2013)?
The MFCC features are calibrated to the human auditory system, rather than those of birds or whales. Have the authors considered calibrating the MFCC scale to better match the auditory systems of the animals that generated the songs?
A potential direction for future research, using the current results as a baseline, could involve exploring the effectiveness of deep learned representations, such as those used in speech recognition with LSTMs. The authors may want to consider a hybrid model, similar to recent work combining autoencoders and graphical models (Johnson, Duvenaud, Wiltschko, Datta, and Adams 2016).