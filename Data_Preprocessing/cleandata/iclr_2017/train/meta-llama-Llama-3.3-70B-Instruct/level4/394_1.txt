The authors present a straightforward approach to regularizing recurrent neural networks, which bears resemblance to dropout but differs in that units are set to their preceding time step values element-wise with a specified probability, rather than being zeroed out.
The paper is generally well-written, with a clear methodological explanation that addresses issues raised during the pre-review phase. The discussion of related work is comprehensive and likely the most up-to-date on the topic of RNN regularization.
The experimental section is devoted to comparing the proposed method with the current state-of-the-art on a range of NLP benchmarks and a synthetic problem, all of which involve sequences of discrete values. Additionally, an experiment demonstrates that the sequential Jacobian for long-term dependencies is significantly higher than in the dropout case.
While the paper shows promise, several concerns arise. 
1) As previously noted during the pre-review questions, it would be beneficial to include the results of experiments that involve a thorough hyperparameter search, i.e., a standard model selection process. Given the apparent availability of resources, it is unclear why this was not undertaken. It is also worth reiterating that, as shown in Table 2, validation error is not a reliable predictor of test error in the relevant dataset, making overfitting in the model selection process a significant concern. Furthermore, zoneout does not appear to yield substantial improvements in other tasks.
2) The mathematical investigation of zoneout is limited. For instance, an analysis of the gradient form from unit K at time step T to unit K' at time step T-R would have been insightful, particularly since these gradients are not necessarily zero for dropout. Moreover, exploring whether zoneout has a variational interpretation, in line with Yarin Gal's work, is a pertinent question. While a potential connection can be seen by treating zoneout within a ResNet framework and applying dropout to the incremental parts, little effort is devoted to explaining why zoneout is effective, despite the existence of relevant starting points in the literature.
3) The datasets used are exclusively symbolic, and it would have been beneficial to explore a broader range of data types, such as continuous data from dynamical systems. The transferability of zoneout to such domains is not immediately apparent.
Given the current proliferation of "tricks" for improved RNN training, it is essential to distinguish zoneout from existing methods. While zoneout is an appealing idea that is simple to implement, the paper falls short in its experimental evaluation (points 1 and 3) and theoretical insights (point 2). Consequently, the paper's contribution is diminished, amounting to a minor improvement, a well-written text, a mediocre experimental evaluation, and limited theoretical insight.