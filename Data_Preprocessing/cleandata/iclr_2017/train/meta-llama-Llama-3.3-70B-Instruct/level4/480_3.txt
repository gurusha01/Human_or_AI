This paper presents a recurrent architecture designed to predict both motion and action states of agents simultaneously. The manuscript is well-structured, clearly presented, and substantiated by robust experimental results. The authors demonstrate that requiring the network to predict motion yields positive outcomes on action state classification, enabling more accurate classification with reduced training data. Furthermore, they illustrate that the information learned by the network is interpretable, organized hierarchically, and provides valuable insights.
However, there are notable weaknesses. A crucial discussion on the interaction between motion and behavior, essential for realizing the benefits of the proposed model, is absent from the paper. Additionally, the authors fail to discuss how their approach could be scaled up to more complex scenarios, such as those involving insects like fruit flies and utilizing visual input to model more general behaviors. This criticism stems from the discrepancy between the title and abstract, which claim to model general behavior, and the experiments, which focus on two specific and relatively simple scenarios. This disparity makes the original claim somewhat overstated, lacking the additional evidence needed to support it. A more precise claim, such as using "insects" or "fruit flies" instead of "animals," would be more appropriate.