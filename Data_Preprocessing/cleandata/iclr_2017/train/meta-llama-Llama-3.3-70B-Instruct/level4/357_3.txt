1) Summary
This manuscript explores the efficacy of separating appearance and motion cues for predicting future frames in natural videos, introducing a novel two-stream encoder-decoder framework, MCNet. The architecture comprises two distinct encoders: one utilizing a convolutional network (convnet) for individual frames and another combining a convnet with a Long Short-Term Memory (LSTM) network for sequences of temporal differences. These are followed by combination layers and a deconvolutional decoder that also leverages residual connections from both encoders. The model is trained end-to-end, employing the objective and adversarial training strategy outlined by Mathieu et al.
2) Contributions
+ The proposed architecture is innovative and well-motivated, bearing some resemblance to the two-stream networks developed by Simonyan & Zisserman, which have proven highly effective in real-world action recognition tasks.
+ The qualitative results presented are extensive, insightful, and convincingly demonstrate (both qualitatively and quantitatively on the KTH and Weizmann datasets) the advantages of decoupling content and motion, particularly in simple scenes with periodic motions, as well as the necessity of incorporating residual connections.
3) Suggestions for Improvement
Bias in Static Datasets:
In response to preliminary concerns regarding the static nature observed in the qualitative results, the authors introduced a baseline method involving the copying of pixels from the last observed frame. While the updated experiments on the KTH dataset affirm the method's efficacy under these conditions, the baseline's superior performance on the UCF101 dataset raises questions about the adequacy of reporting average statistics alone. To further validate the method's and other methods' performance in future frame prediction, additional quantitative analysis is necessary. This could involve disambiguating results on UCF101 by scene type, such as measuring overall motion quantity (e.g., l2 norm of time differences) and reporting PSNR and SSIM metrics per quartile or decile. Ideally, incorporating more realistic datasets, such as the Hollywood 2 dataset by Marszalek et al., which features complex motions, or video datasets like the KITTI tracking benchmark, would significantly enhance the paper.
Additional Action Recognition Experiments:
Conducting further experiments on UCF-101 for action recognition tasks through fine-tuning, as suggested in pre-review queries, would also substantially improve the paper. Since video classification requires learning both appearance and motion features, the two-stream encoder and combination layers of the MCNet+Res architecture appear particularly suited for this, potentially allowing for unsupervised pre-training of content and motion representations as hypothesized by the authors. These experiments would help alleviate concerns about the static nature of the learned representations.
4) Conclusion
In summary, this paper presents an intriguing architecture for a significant problem but necessitates additional experiments to support the authors' claims. Should the authors conduct the suggested experiments and yield convincing results, the paper's relevance for ICLR would be clearly established.
5) Post-Rebuttal Final Decision
The authors have undertaken a considerable amount of additional work in response to the reviewers' suggestions, providing substantial and compelling experimental evidence. This renders the paper one of the most experimentally thorough in addressing this problem. Consequently, I have increased my rating and recommend accepting this paper. The effort is commendable.