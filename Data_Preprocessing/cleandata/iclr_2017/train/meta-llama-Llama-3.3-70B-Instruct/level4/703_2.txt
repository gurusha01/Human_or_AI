Review Summary:
This paper proposes an enhancement to the DaDianNao (DaDN) DNN accelerator by incorporating bit serial arithmetic, wherein the traditional bit-parallel multipliers are replaced with serial x parallel multipliers that process weights in parallel and activations serially. By increasing the number of units while maintaining a constant number of adders, the authors demonstrate the ability to tailor computational time and energy expenditure according to the number of bits used for activation representation. The proposed configuration is shown to be applicable to both fully-connected and convolutional layers of DNNs.
Strengths:
The utilization of variable precision for each network layer is a valuable approach, although it was previously explored in Judd (2015). The evaluation, which includes synthesis of the units, is thorough; however, it lacks place and route analysis and bears resemblance to the evaluation presented in Judd (2016b).
Weaknesses:
The concept of integrating bit-serial arithmetic with the DaDN architecture is relatively minor. Furthermore, the authors have previously published a substantial portion of this work at Micro 2016 in Judd (2016b), with the current paper's primary contribution being the analysis of the architecture's application to fully-connected layers. The reported energy gains are modest, as the additional energy consumed by flip-flops for shifting activations largely offsets the energy savings from reduced precision arithmetic. Notably, the authors do not compare their approach to more conventional variable precision methods, such as using bit-parallel arithmetic units with data gating of least significant bits, which could potentially yield better energy efficiency without sacrificing speed.
Overall:
While the proposed architectures are intriguing, the incremental contribution of this paper, specifically the addition of support for fully-connected layers, is limited when considered in the context of the authors' three previous publications on this topic, particularly Judd (2016b). The idea presented warrants a single, comprehensive paper rather than multiple, incremental publications.