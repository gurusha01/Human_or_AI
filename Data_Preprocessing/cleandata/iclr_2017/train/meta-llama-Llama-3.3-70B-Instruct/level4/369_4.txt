This manuscript presents an in-depth examination of the concept of quantizing convolutional layers to 3 bits, utilizing distinct positive and negative per-layer scales. A comprehensive analysis of its performance, demonstrating negligible loss, is conducted on realistic benchmarks, notably avoiding the overused MNIST dataset.
The significance of this work lies in its potential to establish a lower bound for quantization methods that preserve performance, making it a viable approach for inference in resource-constrained environments and potentially informing the design of novel hardware that leverages the proposed architecture.
Additionally, the paper provides power consumption measurements, a crucial metric for serious practitioners in this field. However, a minor omission is the lack of measurements for the full-precision baseline.
To further strengthen the manuscript, achieving state-of-the-art results on ImageNet and comparing performance with a robust LSTM baseline would be desirable. Moreover, discussing the wall time required to obtain results using the proposed training procedure would provide valuable insight.