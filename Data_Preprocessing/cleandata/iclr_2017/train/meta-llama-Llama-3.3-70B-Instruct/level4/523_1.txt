The authors propose an adaptive softmax approximation designed to optimize performance on GPUs, leveraging a class-based hierarchical softmax approach. This sensible idea involves distributing clusters or hierarchies to facilitate optimally-sized matrix multiplications for GPU computation, as supported by their empirical tests. The results demonstrate the effectiveness of this system.
Regarding the presentation, the paper exhibits both clarity and ambiguity. The underlying concepts and logic are well-defined, but the writing is sometimes unclear. Several minor typos are present, such as those mentioned by AnonReviewer2 and additional instances, including notation inconsistencies in Section 3, where the recurrent network description references an xt that appears to differ from the xt used in the preceding paragraph on feedforward NNs. The use of matrices A and P in Eq2 is also unusual. While Section 4.2, which provides intuition for the 2-cluster case, is a valuable inclusion, the complexity analysis could be improved by adding a figure similar to Figure 2 and incorporating a few explanatory sentences to break down the argument into more manageable steps. For instance, the analysis on the preceding page became clearer when considering Eq (6) and (7) in conjunction with Fig(2), particularly in deriving the equation for the complexity of placing the head of the distribution at the root of the tree. An appendix might be a suitable location to provide such additional explanations.