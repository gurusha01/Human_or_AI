SUMMARY 
This manuscript explores the comparative representational capabilities of deep and shallow neural networks, utilizing ReLU and threshold units, and demonstrates that the addition of one hidden layer can significantly reduce the number of units required to approximate a strongly convex differentiable function.
PROS 
The authors present a unique integration of methodologies, yielding a notable finding that highlights the exponential advantages of depth in neural networks.
CONS 
A limitation of the primary result is its focus on strongly convex univariate functions, which may restrict its broader applicability.
SPECIFIC COMMENTS 
- The discussion on L warrants further clarification in the main body of the paper to enhance comprehension, and consideration should be given to more prominently highlighting the main result. It is possible that some of these concerns may have been addressed in the revised version.
- The problem formulation bears resemblance to the work presented in [Montufar, Pascanu, Cho, Bengio NIPS 2014], which also explores the exponential disparities between deep and shallow ReLU networks from a distinct perspective. Incorporating this reference into the overview would provide additional context.
- In Lemma 3, a variable "i" appears to be incorrectly used and should be replaced with "x".
- Theorem 4 is missing the variable "(x)" in the notation "\tilde f".
- Regarding Theorem 11, it is unclear whether the lower bound consistently increases with L.
- In Theorem 11, the domain of \bf x is specified as [0,1]^d, which may merit further explanation or justification.