This paper addresses the issue of decoding diverse solutions from neural sequence models by introducing an additional term to the log-likelihood of standard models, promoting diverse solutions. To facilitate inference, the authors employ a modified beam search approach.
A significant advantage of this work is its focus on generating diverse solutions in RNN/LSTM models, a topic that has received limited attention. The paper is also well-organized and easy to understand.
However, the novelty of this paper is somewhat limited. Previous research in probabilistic graphical models has already explored the concept of introducing additional terms to objective functions to encourage diversity. Although this paper applies this concept to RNN/LSTM models, the underlying idea is not substantially new. While it is true that most prior work has focused on probabilistic graphical models, the fact that RNN/LSTM models can be interpreted as probabilistic models reduces the perceived novelty.
The proposed diverse beam search method appears to be straightforward, partitioning the search space into groups without considering intra-group diversity to reduce the search space. In comparison to previous work on diverse solutions in probabilistic graphical models, which often involves developing complex algorithmic solutions for efficiency, the approach presented in this paper seems simplistic.
The experimental results demonstrate improvement over previous methods (Li & Jurafsky, 2015, 2016). Nevertheless, the rigor of these comparisons is difficult to assess, as they are based on the authors' own implementation of these methods. 
Upon learning that the authors have made their code publicly available, some concerns regarding the experimental rigor have been alleviated, prompting a revised rating of 6.