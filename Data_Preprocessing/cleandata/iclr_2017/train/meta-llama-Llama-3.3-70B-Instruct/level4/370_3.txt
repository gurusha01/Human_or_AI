This manuscript introduces a novel training approach for deep neural networks, which involves a three-stage process: initial standard training, followed by the clamping of low-magnitude weights to zero while continuing to train the remaining weights, and ultimately, joint retraining of all weights. The efficacy of this method is demonstrated through experiments on diverse datasets, including image, text, and speech, yielding high-quality outcomes.
The concept presented is innovative and intriguing, bearing some resemblance to Dropout, although the deterministic nature of the weight clamping technique distinguishes it, as acknowledged in the paper.
A significant advantage of the proposed technique is its straightforward implementation, requiring only three hyperparameters: the count of weights to be clamped to zero, and the number of training epochs allocated to the initial dense phase and the subsequent sparse phase. This simplicity allows for its seamless integration into the training regimen of various networks, as evidenced by the experimental results.
However, a concern arises regarding the empirical evaluation presented. It appears that the baseline methods were not trained for an equivalent number of epochs as the proposed method. Utilizing standard techniques such as reducing the learning rate upon convergence and continuing training could potentially enhance performance, as suggested in the response. To fortify the manuscript, a more comprehensive analysis of performance in relation to epochs, learning rates, and other factors would be beneficial. Furthermore, an investigation into the impact of the sparsity hyperparameter could provide additional insights.