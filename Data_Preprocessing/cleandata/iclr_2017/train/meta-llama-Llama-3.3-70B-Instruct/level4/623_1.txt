This study examines the Hessian of compact deep networks towards the end of the training process, yielding a primary finding that numerous eigenvalues are roughly zero, rendering the Hessian highly singular and thus limiting the applicability of a substantial amount of theoretical framework.
The central argument that deep learning algorithms exhibit singularity, which in turn undermines various theoretical outcomes, is crucial yet not novel, as it has been previously established by Watanabe in "Almost All Learning Machines are Singular" (FOCI 2007), contributing to a growing collection of research exploring this phenomenon. The paper's references could be more comprehensive, as prior studies, such as Dauphin et al.'s "Identifying and attacking the saddle point problem in high dimensional non-convex optimization" (NIPS 2014) and the work of Amari and others, have also investigated the Hessian in deep learning.
From an experimental standpoint, it is challenging to determine how the results obtained from the small-scale networks analyzed in this study would generalize to larger networks, and it is plausible that the behavior of larger networks would differ. However, the emergence of a distinct bulk/outlier behavior even in these smaller networks is a promising finding. Elucidating this behavior in simpler systems is valuable, although the results presently seem preliminary and would likely be more impactful with further development.
Ultimately, this paper tackles a significant problem but would benefit from better contextualization within the relevant literature and the conduct of more extensive experiments to uncover large-scale behavior pertinent to practical applications.