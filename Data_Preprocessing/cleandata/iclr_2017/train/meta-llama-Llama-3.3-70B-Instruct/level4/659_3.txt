This paper presents a sequence transduction approach that leverages a monotonic alignment between input and output, which is assumed to be provided as part of the training data, utilizing the Chinese Restaurant process in experimental settings. 
The concept is reasonable, yet its utility is restricted to domains where such a monotonic alignment is feasible. However, as noted during the pre-review discussion, there exists substantial overlapping related work, including probabilistic models incorporating hard-alignment, such as Sequence Transduction With Recurrent Neural Network (Graves et al, 2012), and efforts to integrate external alignments into end-to-end models, like A Neural Transducer (Jaitly et al, 2015). Consequently, the novelty of the proposed approach is questionable.
Furthermore, concerns arise regarding the evaluation methodology. Comparing a model reliant on external alignment to a vanilla soft-attention model that learns alignments from scratch may not be entirely fair. A potential control experiment could involve pretraining the soft-attention model to align with the external alignment, potentially reducing overfitting on smaller datasets where the proposed approach yields the most significant improvements. Notably, on larger datasets, such as SIGMORPHON, the observed improvements are marginal and language-specific.
In summary, the two primary concerns are: (a) the approach lacks sufficient novelty due to existing related work, and (b) the comparative evaluation between models with and without external alignment may not be adequately controlled.