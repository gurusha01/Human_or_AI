I concur with Reviewer 2's assessment of the paper's intriguing aspect. The concept of dynamically adding or removing units to enable a model to adapt to the problem and data requirements, rather than relying on user prior knowledge, is a promising direction. 
The authors present a notable theoretical finding, which demonstrates that under fan-in or fan-out regularization, the optimal error function is achieved with a finite number of parameters, preventing the network from growing indefinitely and overfitting the data. This result bears some resemblance to traditional approaches like Lasso or Elastic Net, where regularization yields sparse weights. However, I would have appreciated more intuitive explanations and discussions surrounding this theorem, as it seems somewhat expected and warrants further exploration. 
To enhance the paper, I suggest allocating more space to dissecting the theoretical results, potentially at the expense of prior work discussions. Please refer to the suggestions outlined below (point 2). 
I have several additional comments:
1. A compelling experiment would involve demonstrating that a model with dynamically added or removed nodes can outperform a network with a fixed number of nodes, showcasing the efficiency of this approach. This would highlight the method's ability to optimize performance and memory usage by eliminating unnecessary nodes and replacing them with needed ones. Although experiments in Figure 2 provide mixed results, they are not entirely clear and require careful interpretation. 
In some cases, non-parametric networks perform better, while in others, they underperform compared to parametric ones. Nevertheless, the method's usefulness lies in its ability to discover structure. What is unclear, however, is why non-parametric learning sometimes outperforms parametric learning when the final network is known in advance. Further insight into this phenomenon would be beneficial.
2. I recommend providing a more in-depth discussion of Theorem 1's implications and significance. Currently, the theorem seems to be presented without adequate context or explanation. Beyond the proofs in the Appendix, what is the core insight of the theorem, and how can it be distilled into plain English? The conclusion appears almost intuitive, so I wonder if there are more profound implications at play. 
As mentioned earlier, I believe this theoretical result deserves more attention, accompanied by additional experiments to substantiate its claims. For instance, can the regularizer parameter lambda be predicted based on the data, or is there an inherent property that facilitates guessing the optimal lambda? My intuition suggests that lambda plays a crucial role in determining the final network structure, but I would like to see this explored further. 
How sensitive is the final network structure to initialization, and do different random weight initializations yield distinct networks? What happens when fan-in and fan-out regularizers are combined â€“ does the theoretical result still hold?
I also have a few supplementary questions:
1. Could you clarify why adding zero units allegedly changes the regularizer value? For example, does the L2 norm remain unchanged when zero values are added?
2. The definition of zero units as having either fan-in or fan-out weights equal to zero seems incomplete. I presume you intended to specify that both fan-in and fan-out weights must be zero to remove the unit without altering the output f. This point warrants further clarification.
In light of these comments, I have revised my rating to 7, with the hope that the authors will address these concerns and provide a more comprehensive and intuitive presentation of their work.