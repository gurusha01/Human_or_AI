This manuscript endeavors to synthesize recent research on straightforward "reading comprehension" tasks, which entail matching questions with answers found in a passage, and subsequently investigate the structural knowledge acquired by these models, proposing potential enhancements. The reading comprehension datasets utilized, such as CNN/Daily Mail, are relatively simple, as they typically do not require reasoning chains based on multiple pieces of evidence, unlike more complex datasets like MCTest. Numerous models have been proposed for this task, and the authors categorize them into "aggregation readers" and "explicit reference readers." The study reveals that aggregation readers organize their hidden states into a predicate structure, enabling them to mimic explicit reference readers. Furthermore, the authors experiment with incorporating linguistic features, including reference features, into existing models to enhance performance.
The rephrasing and reorganization of the manuscript to emphasize that aggregation readers learn a predicate structure are commendable, as are the inclusion of results regarding the dimensionality of the symbol space. The effort to classify and organize various reading comprehension models into broader categories is also valuable, given the numerous models being developed in the field, resulting in a complex landscape. 
However, concerns arise regarding the simplicity of the demonstrated predicate structure and its potential to provide insights for developing improved models in the future. Since explicit reference readers do not need to learn this structure, and the CNN/Daily Mail dataset has limited room for improvement, as demonstrated by Chen et al. in 2016, the pursuit of substantial performance enhancements on these datasets may be unrealistic. More complex datasets would likely involve multi-hop inference, which is not addressed in this paper. Additionally, the manuscript's message is somewhat disjointed and difficult to follow, and could benefit from increased focus.
In the context of the proliferation of competing neural network models for NLP tasks, contributions like this one, which aim to organize and analyze the landscape, are valuable. Nevertheless, this manuscript might be more suitable for an NLP conference or journal, such as TACL, where its insights and analyses could be more effectively appreciated.