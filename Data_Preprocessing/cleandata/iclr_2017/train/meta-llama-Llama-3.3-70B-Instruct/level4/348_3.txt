This manuscript introduces a novel inductive bias in the design of convolutional neural networks (CNNs), with thorough mathematical underpinnings and derivations. The proposed architecture aims to generate equivariant representations with steerable features, achieving this with a reduced parameter count compared to traditional CNNs, making it particularly advantageous in scenarios with limited data. The authors establish intriguing and innovative links between steerable filters and "steerable fibers". The architecture draws significant inspiration from the authors' prior work, as well as the concept of "capsules" introduced by Hinton in 2011. An empirical comparison against state-of-the-art architectures, such as ResNets, on the CIFAR10 dataset demonstrates the superiority of the proposed architecture, especially in the small data regime. However, the absence of empirical evaluations on larger datasets like ImageNet or COCO restricts the contribution to a primarily theoretical one. Further empirical assessment of the equivariance properties would be beneficial. It remains unclear why this architecture outperforms on CIFAR10, as the utility of capturing equivariances in classifying different object category instances is not immediately apparent. An alternative dataset, such as action recognition in videos, might offer a more intuitive illustration of the architecture's strengths.