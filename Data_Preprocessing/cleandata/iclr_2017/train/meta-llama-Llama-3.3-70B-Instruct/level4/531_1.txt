This paper presents a generative video model that combines a background with 2D objects, optimized within a Variational Autoencoder (VAE) framework. 
The authors introduce an innovative approach using the outer product of softmaxed vectors, resulting in a delta-like 2D map, followed by a convolutional operation to achieve image translation with differentiable parameters. This method appears to be a promising alternative to complex differentiable resamplers, such as those employed in Spatial Transformer Networks (STNs), when translation is the primary requirement.
I have noted several areas that require clarification, particularly in the experimental section, which seems to be hastily presented with some results merely mentioned without being fully reported, even in the supplementary materials.
While it might be acceptable for a highly novel and unconventional proposal to rely solely on synthetic experiments, the moderate novelty of this method makes the absence of real-data experiments disappointing. 
For instance, applying this method to aerial videos captured from drone platforms could be fruitful, as the planar assumption underlying the authors' approach would likely hold in such scenarios.
I recommend that the authors thoroughly review the paper to address issues such as missing references, incomplete sentences (e.g., the caption of Figure 5), and the experimental section's lack of clarity, to improve the overall quality of the presentation.