The authors propose an extension of GANs, termed BiGAN, which incorporates an inference pathway from the data space to the latent space, as well as a discriminator operating on the joint latent/data space. They demonstrate that the theoretical properties of GANs remain applicable to BiGAN and assess the quality of the unsupervised features learned through the inference pathway by evaluating their performance on supervised tasks after fine-tuning the deeper layers.
One potential structural issue with the paper is that it dedicates a substantial amount of space to discussing the relationship between BiGAN and GANs, including their theoretical properties, which may not be directly relevant to the primary goal of learning unsupervised features. As a result, the focus on unsupervised feature learning is sometimes lost, and it is not until page 6 that it reemerges as a key aspect of the paper. A more balanced presentation, with a greater emphasis on the main narrative of unsupervised feature learning, would be beneficial.
The BiGAN framework is nonetheless an elegant and compelling extension of GANs. However, the practical significance of the theoretical properties is not entirely clear, particularly given that the model does not appear to be fully converged. For instance, Figure 4 suggests that G(E(x)) may be performing a simple nearest neighbor retrieval, which raises concerns that the model might be memorizing samples rather than learning meaningful representations. Additionally, it would be informative to know the discriminator's performance after training.
Regarding the primary objective of learning powerful features, the method achieves competitive, albeit not state-of-the-art, performance on most evaluated tasks (Tables 2 and 3). It would be interesting to investigate whether improvements to the BiGAN training procedure and the convolutional architecture used could enhance the results.
The paper is well-written and provides most of the necessary details, although additional information on the training process, such as learning rates and initialization, would be helpful for reproducing the results.
Overall, the paper presents a fascinating framework for future research, despite the somewhat unimpressive results in terms of both feature evaluation and GAN learning.
A minor suggestion is to highlight the best performance numbers in Tables 2 and 3 for easier comparison.