This manuscript presents the Quasi-Recurrent Neural Network (QRNN), a model that substantially reduces the computational complexity associated with temporal transitions in sequence data. In essence, the QRNN modifies the traditional LSTM architecture by retaining only the diagonal elements of the transition matrices and extending the connections between layers to incorporate general temporal convolutions, thereby generalizing the standard LSTM's single time-step receptive field.
The authors acknowledge the QRNN's relationship to other recent RNN variants, such as ByteNet and strongly-typed RNNs (T-RNN), which somewhat tempers the model's novelty. Nevertheless, in my assessment, the QRNN retains sufficient innovative elements to warrant publication.
The empirical results provided by the authors offer a convincing validation of the paper's claims, suggesting that this specific LSTM modification deserves further consideration from the research community.
Although I perceive the contribution as somewhat incremental, I suggest accepting the paper for publication, as it still presents a valuable addition to the field.