Initially, the paper attempts to evaluate whether auto-regressive models can replicate the GAN results of Reed et al. (2016a), but the low resolution of the generated images raises concerns. The resolution limitations are not adequately addressed until late in the paper, and Figure 9 presents only three results, which may have been cherry-picked to favor PixelCNN, making the conclusion unconvincing.
The use of segmentation masks and keypoints as input constraints is notable, but the low resolution makes it challenging to assess the emergence of coherent object and scene details. For instance, the cows in Figure 5 appear as mere color blobs, and any color blob conforming to the cow segmentation mask would resemble a cow.
Although the amount of variation in the generated images is impressive, it is unclear how much the model is simply reproducing training data. Figure 8 attempts to investigate this, but it is uncertain whether the "red birds," for example, are largely copied from specific training instances.
The responses to the pre-review questions are unsatisfactory, as they fail to provide concrete information on training time, epochs, and testing time. The paper does not justify the inability to make high-resolution comparisons, citing only that it is slower at test time without specifying whether this is prohibitively slow or merely slightly inconvenient. Given the limited number of comparisons in the paper, a generation time of an hour does not seem excessively prohibitive.
It is essential to acknowledge my biases: I am skeptical about the suitability of PixelCNN for deep image generation. The use of causal neighborhoods in texture synthesis methods was largely due to the lack of a clear global optimization approach (Kwatra et al.'s Texture Optimization for Example-based Synthesis being an early alternative). The strict causality inherent in PixelCNN seems misguided, as it makes hard decisions about pixel values in one part of the image before synthesizing another. A more suitable approach might involve a deeper network where all output pixels are conditioned on all other pixels, allowing for implicit conditioning to emerge at intermediate network levels. Although I may be incorrect, and the paper's advantages could outweigh its disadvantages, this work does not adequately acknowledge the drawbacks.
In summary, while the results are intriguing, particularly in terms of generating diverse outputs, the extremely low resolution is a significant limitation, especially considering the richness of the inputs. The network relies heavily on the input constraints, which guide the generation process. The deep image synthesis field is rapidly evolving, and it is essential to move beyond "proof of concept" papers to more comprehensive comparisons. This paper has the potential to provide a more in-depth comparison but falls short in this regard, lacking a thorough apples-to-apples comparison between PixelCNN and GAN, as well as a conclusion on why such a comparison may be impossible. Furthermore, there is no large-scale comparison, either qualitative or quantitative, of the result quality, which is a notable omission.