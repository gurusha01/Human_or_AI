After reevaluating the manuscript in light of the authors' rebuttal, I have opted to revise my assessment upward. The evidence presented, particularly in Figure 8, suggests that ALI exerts a stabilizing influence on GAN training and successfully learns a viable inference network.
----------------
Initial Review Paraphrased:
This manuscript introduces a novel approach to learning an inference network within the GAN framework. The primary objective of ALI is to align the joint distribution of latent and observable variables as defined by an encoder-decoder network pair. Through training on multiple datasets, ALI demonstrates satisfactory reconstruction capabilities, even in the absence of an explicit reconstruction term in its cost function. This indicates that ALI is indeed capable of learning a competent inference network for GANs.
Currently, there are multiple methodologies for learning inference networks in GANs, including post-training approaches that involve sampling from the GAN and training a separate network to map observations to latent variables, as well as concurrent training methods like infoGAN. A comprehensive comparison with these existing methods, along with a discussion on the superiority of ALI's inference network, would significantly enhance the manuscript.
Given the stochastic nature of ALI's inference network, it would be beneficial to include multiple reconstructions of the same image to illustrate its variability. Notably, the inference network proposed in the BiGAN paper is deterministic, highlighting a key distinction between these works. Emphasizing this difference could provide valuable context.
While the quality of generated samples is impressive, the lack of quantitative experiments to compare ALI's performance with other GAN variants raises questions about the contribution of the inference network to the quality of generated samples. Incorporating metrics like the Inception score for comparative purposes could help address this concern.
The manuscript presents two sets of semi-supervised learning results. The first approach involves concatenating the hidden layers of the inference network followed by an L2-SVM. Ideally, a more integrated approach, where the semi-supervised path is trained concurrently with the generative path, would be more compelling. An innovative strategy could involve partitioning the latent code into categorical and continuous distributions, utilizing the inference network on the categorical latent variable directly for classification, akin to semi-supervised VAE. Demonstrating ALI's capability to disentangle factors of variation using a discrete latent variable, similar to infoGAN, would substantially enhance the manuscript's quality.
The second set of semi-supervised learning results indicates that ALI can achieve state-of-the-art performance. However, it appears that the significant performance gain may primarily stem from adapting the methodology of Salimans et al. (2016), where the discriminator is utilized for classification purposes. It remains unclear how learning an inference network improves the discriminator's classification performance. To ascertain the proposed method's impact on GAN stability, further clarification is needed. Given that one of the primary motivations for learning an inference network is to establish a mapping from images to high-level features such as class labels, directly leveraging the inference path for semi-supervised learning, as previously suggested, would be more insightful.