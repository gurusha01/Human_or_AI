This manuscript presents a proximal quasi-Newton method for training binary deep neural networks (DNNs), with a key contribution being the integration of pre-conditioning and binarization within a proximal framework. The application of a proximal Newton method to interpret various DNN binarization schemes is noteworthy, offering a fresh perspective on existing approaches. Nevertheless, the theoretical underpinnings of this work are not entirely persuasive or informative. The optimization problem formulated in equations (3) and (4) essentially reduces to a mixed integer programming problem. Although the authors treat the integer component as a constraint and incorporate it into the proximal operators, the constraint set remains discrete, and there is no assurance that the proximal Newton algorithm will converge under conditions that are practically meaningful. Specifically, verifying the assumption that [dt^t]k > Î², as stated in Theorem 3.1, can be challenging in practice, particularly given the potentially complex nature of the loss surface in DNNs.