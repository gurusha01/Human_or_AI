This manuscript investigates the challenge of adapting solutions from existing tasks to address a new task within the reinforcement learning paradigm, highlighting the crucial issues of preventing negative transfer and facilitating selective transfer. The proposed methodology combines existing solutions with the solution being learned for the new task using a convex combination. By assigning non-negative weights to each solution, the approach effectively disregards solutions with negative effects and allocates more weight to more relevant solutions in each state. The authors derive the "A2T" learning algorithm, applicable to both policy and value transfer, for REINFORCE and ACTOR-CRITIC algorithms, and validate it through experiments on synthetic Chain World and Puddle World simulations, as well as the Atari 2600 game Pong.
+ The paper introduces an innovative approach to transfer reinforcement learning.
+ The experimental design is well-crafted to demonstrate the proposed method's capabilities.
However, a significant aspect of transfer learning is the algorithm's ability to automatically determine whether existing solutions to known tasks are sufficient to solve the novel task, thereby avoiding the need for learning from scratch. This aspect is not explored in the paper, as most experiments rely on a learning-from-scratch solution as the base network. It would be intriguing to investigate the algorithm's performance without a base network. Furthermore, while the proposed algorithm appears to accelerate learning speed in Figures 3, 5, and 6, the overall performance does not seem to surpass that of the solo base network. Providing examples where existing solutions complement the base network would strengthen the argument.
Additionally, if the base network is disregarded, the proposed approach can be viewed as a form of ensemble reinforcement learning, leveraging the strengths of learned agents with diverse expertise to tackle the novel task.