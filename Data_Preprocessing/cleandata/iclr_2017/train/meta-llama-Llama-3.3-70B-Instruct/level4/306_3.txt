In consideration of the authors' revisions and their efforts to clarify the meta-learning task, I am revising my evaluation to a score of 8.
This manuscript presents a novel approach to few-shot learning using neural networks, leveraging the concept of meta-learning, which has experienced a resurgence in popularity over the past year. The authors frame few-shot learning as a sequential meta-learning problem, where each example comprises a sequence of batches of training pairs, followed by a final test batch. The inputs at each step include the outputs of a base learner, such as training loss and gradients, as well as the base learner's current state, including its parameters. The paper applies a Long Short-Term Memory (LSTM) network to this meta-learning problem, utilizing the inner memory cells in the second layer to directly model the updated parameters of the base learner. This approach is motivated by the similarities between the update rules of LSTM memory cells and gradient descent. The updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final test batch. The authors make several simplifying assumptions, including sharing weights across all second-layer cells, analogous to using a single learning rate for all parameters. The paper recreates the Mini-ImageNet dataset proposed by Vinyals et al. in 2016 and demonstrates that the meta-learner LSTM is competitive with the current state-of-the-art, namely Matching Networks (Vinyals, 2016), on 1-shot and 5-shot learning tasks.
The strengths of this manuscript include:
- The innovative and natural formulation of the few-shot learning problem as a sequential meta-learning problem, which, although not entirely new, advances the state-of-the-art in this area.
- The competitive performance of the proposed approach with the current state-of-the-art on 1-shot and 5-shot Mini-ImageNet experiments.
- The use of a distinct base learner, a simple ConvNet classifier, which differs from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals (2016), showcasing the versatility of the method.
- The novel insight into the relationship between LSTM memory cell updates and gradient descent updates, which is an interesting contribution regardless of its novelty.
- The provision of practical insights for designing and training an LSTM meta-learner, including initialization, weight sharing, and the importance of normalizing loss, gradient, and parameter inputs, which will facilitate the replication and application of this work to new problems.
However, the manuscript also has some weaknesses:
- The writing is often opaque, making the paper challenging to follow. Despite the interesting content, the manuscript would benefit from a clearer and more concise presentation. A concrete example, potentially accompanied by a diagram illustrating a sequence of few-shot learning tasks, would enhance the accessibility of the paper to a broader audience.
- The precise nature of the N-class, few-shot learning problem is not clearly defined. Specifically, the role of the 100 labels in the Mini-ImageNet dataset, with 64, 16, and 20 labels used during meta-training, validation, and testing, respectively, needs clarification. It is unclear whether only 64 classes are observed during meta-training or if all 100 classes are observed across different batches. This ambiguity extends to other details, such as the number of outputs in the softmax layer of the ConvNet base learner during meta-training.
- The plots in Figure 2 are not very informative on their own and are not adequately discussed in the manuscript, which diminishes their impact.
Overall, this is an interesting paper with compelling results, suggesting a clear accept. However, improvements in the presentation and clarity of the ideas would significantly enhance the manuscript's quality. I anticipate revising my score upward if these issues are addressed.