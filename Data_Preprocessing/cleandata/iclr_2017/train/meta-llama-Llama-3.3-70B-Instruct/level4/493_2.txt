The paper introduces novel bounds on misclassification error, which enable the training of classifiers using an adaptive loss function. This algorithm operates in successive steps, where parameters are trained by minimizing log-loss weighted by the probability of the observed class as determined by the parameters from previous steps. The proposed bound improves upon the standard log-likelihood approach, particularly in scenarios where outliers or underfitting hinder the learning algorithm's ability to optimize the true classification error effectively. Experimental results validate the theoretical intuition, demonstrating cases where the new algorithm yields improved classification error due to underfitting when using standard log-loss, as well as cases where the new bounds offer no improvement because the log-loss sufficiently fits the dataset.
The paper also explores connections between the proposed concept and reinforcement learning, along with classifiers that incorporate "uncertain" labels. While the paper is generally well-written and easy to follow, upon closer examination, it becomes challenging to fully comprehend due to the intermingling of two distinct problems: (a) optimizing the classification error of a randomized classifier that predicts a class with a certain probability, and (b) optimizing a deterministic classifier in a manner robust to outliers and underfitting.
The confusion arises because the standard approach to supervised classification involves using deterministic classifiers at test time, with log-loss being an upper bound on the classification error of these deterministic classifiers. However, the bounds discussed in the paper pertain only to randomized classifiers. A critical question is whether the experiments utilize the randomized classifier, as suggested by the sentence "Assuming the class is chosen according to p(y|X, θ)," or the more conventional deterministic classifier argmax_y P(y|x, θ).
It appears there are two possible interpretations: (i) the paper focuses on learning randomized classifiers, in which case it should compare performances with the deterministic counterparts commonly used in practice, or (ii) the paper makes sense if one accepts that optimizing criterion (a) is a suitable surrogate for (b). In both scenarios, clarifying the write-up is essential, as the algorithm does not minimize an upper bound on the classification error in case (ii), and in case (i), the approach differs from standard binary classification practices.
Several comments are noteworthy: the section on "allowing uncertainty in the decision" could be enhanced with references such as Bartlett & Wegkamp (2008) or Sayedi et al. (2010). Additionally, a "-" sign appears to be missing in the P(1|x, θ) expression within L(θ, λ) in Section 3. The idea presented is interesting and original, and while the initial score is relatively low, it could be increased with the necessary clarifications.
In final consideration, the paper is sufficiently clear in its current form, although justification for why and to what extent the error of the randomized classifier serves as a good surrogate for the error of the true classifier could be improved. The concept of a "smoothed" version of the 0/1 loss is more acceptable in standard classification setups but less clear in sections dealing with an additional "uncertain" label. Consequently, the score has been increased from 5 to 6.