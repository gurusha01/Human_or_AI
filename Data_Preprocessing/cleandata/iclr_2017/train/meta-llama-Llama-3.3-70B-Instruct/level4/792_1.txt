The paper proposes a regularization approach utilizing soft targets, which are generated by combining the true hard labels with the current model's predictions. Notably, a similar methodology was presented in Section 6 of the work by Hinton et al. (2016), "Distilling the Knowledge in a Neural Network".
Strengths:
+ The paper provides an in-depth analysis of co-label similarity, offering valuable insights.
Weaknesses:
- The baselines used in the experiments appear to be suboptimal. It is uncertain whether the authors have identified the most effective hyperparameters. For instance, a simple 5-layer fully connected MNIST model with 512 hidden units, trained without any regularizer using Adam and He initialization, achieved an accuracy of 0.986, surpassing the 0.981 accuracy reported in the paper for the same architecture.
- The novelty of the proposed idea is limited, as it closely resembles the approach outlined in Hinton et al. (2016). This lack of originality may not be sufficient for a publication in a top-tier conference like ICLR.