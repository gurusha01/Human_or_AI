This paper presents a noteworthy investigation into a novel approach for enhancing the training of highly non-convex deep neural networks, a challenge of significant practical importance. The empirical assessment, as outlined in the manuscript and further elucidated by the authors' responses during the discussion, compellingly illustrates the method's ability to yield consistent accuracy improvements across a diverse range of architectures, tasks, and datasets. A key strength of the proposed algorithm lies in its simplicity, involving an alternating sequence of training a fully dense network and its sparse counterpart, a characteristic that enhances its potential for adoption by the broader research community due to its ease of implementation.
To further strengthen the manuscript, it is recommended that the authors incorporate the additional experimental results and commentary provided during the discussion, with particular emphasis on comparisons of accuracy achieved when controlling for the number of training epochs.