This manuscript presents a comprehensive analytical framework for predicting the training and evaluation durations of various neural networks under different software, hardware, and communication configurations. 
The presentation is exceptionally lucid, with the authors incorporating a wide range of variables into their runtime calculations, including worker count, bandwidth, platform, and parallelization approach. Notably, their findings align with previously published results, thereby validating their methodology.
Additionally, the authors have made their code openly accessible and provided a functional live demonstration, which enhances the overall usability of their tool. 
As mentioned in their response, future updates will enable users to upload customized networks and model splits, significantly expanding the utility of this interface.
To further augment the study, it would be beneficial to explore the application of this framework to more contemporary network architectures that incorporate skip connections, such as ResNet and DenseNet, to demonstrate its versatility and applicability to a broader range of models.