This paper introduces a novel neural architecture, termed DRAGNN, designed for the transition-based framework, utilizing Transition-Based Recurrent Units (TBRUs) to compute hidden state activations. The authors demonstrate that DRAGNN can encompass a broad range of existing transition-based methods, and its design facilitates the implementation of multitask learning systems. The experimental results showcase the effectiveness of DRAGNN in achieving near state-of-the-art performance on two tasks: parsing and summarization.
The paper is divided into two primary sections: the presentation of DRAGNN and its applications. 
In the first section, the proposed DRAGNN is presented as a streamlined tool for constructing transition-based systems. However, its novelty is questionable, given the established nature of transition-based frameworks and the prevailing trend of leveraging neural networks in NLP for such systems. In comparison to the Stack-LSTM (Dyer et al., 2015), the differences with DRAGNN appear subtle. While DRAGNN is undoubtedly a potent architecture, its contribution may be more accurately attributed to software engineering advancements.
The second section illustrates the application of DRAGNN in developing new transition-based systems for various tasks, including multitask learning. The implementations are well-executed, reaffirming DRAGNN's capabilities, particularly in multitask learning scenarios. Nevertheless, it is essential to acknowledge that the solutions employed are largely based on existing literature, which complicates the assessment of novelty in the context of the conference.