This paper presents an experimental examination of a modified label smoothing technique for training neural networks, with results reported across a range of tasks. Although the concept of label smoothing is not novel, its application to a broad spectrum of machine learning tasks has not been previously explored.
Comments:
To provide a comprehensive evaluation, the paper should include state-of-the-art results for speech recognition tasks, such as TIMIT and WSJ, even if direct model comparisons are not feasible.
The back-propagation of label smoothing through softmax is a straightforward and efficient process. However, it is unclear whether an efficient solution exists for the back-propagation of entropy smoothing through softmax.
While the proposed smoothing technique may not affect classification accuracy, it can lead to inaccurate estimates of the true posterior distribution. This limitation could have significant implications for complex machine learning problems, where decisions are made at a higher level based on posterior estimates, such as in language models for speech recognition.
Further motivation is required to justify the proposed smoothing technique, as its benefits and potential applications are not fully elucidated.