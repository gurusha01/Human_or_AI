This manuscript proposes two distinct RNN-based architectures for extractive document summarization, namely Classifier and Selector, which differ in their approach to sentence ordering. The Classifier model considers the original sentence order, while the Selector model selects sentences in a random order. For both architectures, the authors utilize the concatenated hidden state of the RNN from forward and backward passes as features to calculate a score that encapsulates content richness, salience, positional importance, and redundancy. The models are trained using supervised learning, with "pseudo-ground truth generation" employed to create training data from abstractive summaries. Experimental results indicate that the Classifier model outperforms the Selector model, achieving near state-of-the-art results for certain evaluation metrics.
The proposed architecture can be seen as an extension of the work by Cheng and Lapata (2016). However, the performance improvement is marginal, and in some cases, even inferior. The authors attribute one key difference to their method of transforming abstractive summaries into gold labels for supervised training. Nevertheless, the experimental results suggest that the unsupervised greedy approximation used to generate ground truth labels may introduce noise, potentially leading to inconsistent performance compared to Cheng and Lapata's extractive model. This raises the question of whether adopting a similar approach to constructing training data, as in Cheng and Lapata, might yield better results.
To establish the credibility of the proposed models, it is essential that they consistently outperform the baseline, which shares similarities with the proposed methods, as the primary contribution lies in the improved neural architectures for extractive document summarization.