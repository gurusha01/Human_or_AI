This paper primarily presents a straightforward application of Stochastic Gradient Variational Bayes (SGVB) to state-space models, showcasing its potential in this domain. The core concept involves reformulating a state-space model as a deterministic temporal transformation, where innovation variables serve as latent variables. Notably, the prior distribution over these innovation variables does not vary with time. The approach focuses on approximate inference over the innovation variables rather than the states themselves, addressing a specific problem without exploring broader aspects, such as how priors over model parameters could depend on past observations. Despite this narrow focus, the application is intriguing. However, the presentation could be more concise and clearer, as the paper delves into detailed specifics quickly, potentially missing an opportunity for a more impactful explanation.
The paper and its appendix are commendable for their thoroughness and detail. The experimental results, although based on simplified examples, demonstrate promising outcomes.
A few specific points warrant clarification or reconsideration:
- In Section 2.1, the notation suggests setting betat = wt, with the acknowledgment of possible variants. It would be beneficial to explicitly state that components not included in betat, such as Ft and B_t, are not subject to Bayesian treatment but are instead optimized through other means.
- The last paragraph of Section 2.2 highlights a key contribution as constraining the latent space to fit the transition model. This achievement seems somewhat straightforward and may not constitute a significant novelty.
- Equation 9 introduces an interpretation that supposedly implies the factorization of the recognition model. However, this factorization is not inherently implied, as alternative formulations, such as q(beta|x) = q(w|x,v)q(v), could also be valid, indicating a need for further justification or clarification of the proposed factorization.