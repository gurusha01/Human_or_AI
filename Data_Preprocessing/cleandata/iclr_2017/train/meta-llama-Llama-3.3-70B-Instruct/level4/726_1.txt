This paper proposes a novel generalization of dropout, grounded in information theoretic principles. The core concept revolves around learning a representation z of input x, with the goal of predicting y, by selecting a z that minimizes the information it carries about x while maintaining its predictive capability for y. This notion is formalized through the Information Bottleneck Lagrangian, leading to an optimization problem that bears resemblance to variational dropout, albeit with a distinct scaling factor associated with the KL divergence term that fosters noise introduction. The degree of noise added is parameterized as a function of the data and optimized concurrently with the model. Experimental evaluations on CIFAR-10 and MNIST datasets demonstrate modest improvements over binary dropout.
The paper's strengths include:
- Illuminating a crucial conceptual bridge between probabilistic variational methods and information theoretic approaches, demonstrating that dropout can be generalized through both frameworks to yield similar models.
- An exemplary presentation of the model.
- Impressive experimental results on the cluttered MNIST dataset.
However, several weaknesses are noted:
- The results presented in Figure 3(b) for CIFAR-10 appear to be based on a validation set, raising questions about the exclusion of the test set, which complicates comparisons with existing literature, such as the findings by Springenberg et al.
In terms of quality, the theoretical exposition is of high caliber, with Figure 2 providing a valuable qualitative insight into the model's operation. Nevertheless, the experimental results section could be enhanced, for instance, by replicating and improving upon the CIFAR-10 results reported by Springenberg et al. using information dropout.
The paper is commendable for its clarity, making it easily accessible to readers. The originality of the work lies in the derivation of the information dropout optimization problem using the IB Lagrangian, although the final model bears a close resemblance to variational dropout.
The significance of this paper stems from its potential to spark interest among researchers in representation learning, as it offers an alternative perspective on latent variables as information bottlenecks. However, its broader impact may be limited unless substantial improvements over simple dropout can be demonstrated.
In conclusion, the paper presents a theoretically insightful derivation and preliminary results of good quality. There is room for improvement in the experimental section.
Minor comments and suggestions for improvement include:
- Correcting typographical errors, such as "expecially" to "especially" and "trough" to "through".
- Addressing a potential missing minus sign in the expression for H(y|z) preceding Eq (2).
- Incorporating error bars into Figure 3(b) for consistency with Figure 3(a).
- Considering a comparative analysis between Figure 2 and the activity map of a standard CNN trained with binary dropout to assess whether similar filtering effects are observed.