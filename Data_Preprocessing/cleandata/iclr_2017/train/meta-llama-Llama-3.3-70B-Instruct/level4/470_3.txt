This study pioneers the exploration of stick-breaking priors and their associated inference methods within the context of Variational Autoencoders (VAEs), offering a comprehensive explanation of the background, priors, posteriors, and their Dirichlet Non-Conjugate Prior (DNCP) forms. The clarity and quality of the writing are noteworthy.
The experimental findings indicate that stick-breaking priors do not consistently outperform spherically Gaussian priors in fully unsupervised settings when evaluated based on log-likelihood, a result that demonstrates the authors' commitment to rigorous scientific reporting. However, in semi-supervised settings, the outcomes are more favorable.
Several observations and suggestions are worth noting:
- Section 2.1 could benefit from acknowledging prior research on non-Gaussian priors, such as DRAW, the generative ResNet paper, the IAF paper, and Ladder VAEs.
- A minor correction is needed in Section 2.2, where two commas are incorrectly used.
- The text surrounding Equation 6 would be improved by referencing the appendix for the closed-form KL divergence derivation.
- For clarity, the phrase "The v's are sampled via" should be modified to "In the posterior, the v's are sampled via" to avoid ambiguity between the prior and posterior.
- The final paragraph of Section 4 is particularly well-crafted.
- In Section 7.1, the terminology "density estimation" could be expanded to "density and mass estimation" for technical accuracy.
- Also in Section 7.1, using only 100 Importance Sampling (IS) samples may be insufficient.
- Figure 3(f) presents an intriguing result, showing the effectiveness of k-NN on raw pixels.