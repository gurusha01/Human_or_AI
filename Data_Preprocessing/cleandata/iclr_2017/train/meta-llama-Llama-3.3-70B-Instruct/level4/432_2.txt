This paper presents a compelling exploration of the relationship between value-based methods and policy gradients, providing a formalized connection between the softmax-like policy derived from Q-values and a regularized form of policy gradients.
In terms of presentation, the initial sections appear to follow a logical flow, but reframing the content as an extension or generalization of the dueling Q-network could offer a more intuitive introduction to the novel algorithm and findings.
One minor concern arises in the general case derivation, specifically in Section 3.2, Equation (7), where the expectation over (s,a) is taken with respect to π, a function of θ. The dependence on θ seems to be overlooked, which is crucial for the policy gradient update derivation. Although this approximation is often acceptable when the policies are sufficiently close (e.g., in trust-region methods), it may not hold in general, potentially leading to the solution of a different problem than intended.
The results section could be strengthened by including a comparison with the dueling architecture, as it is the most closely related method. Such a comparison would help clarify whether and in which games the proposed approach yields an improvement.
Overall, the paper is well-written, offering valuable theoretical insights, and its strengths make it a notable contribution to the field.