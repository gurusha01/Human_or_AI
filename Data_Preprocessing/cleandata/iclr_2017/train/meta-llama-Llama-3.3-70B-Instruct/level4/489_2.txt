This manuscript presents a comprehensive evaluation of various unsupervised sentence embedding methods through a series of auxiliary prediction tasks. The authors investigate the capacity of classifiers to predict word order, word content, and sentence length, with the goal of determining the type and amount of information captured by different embedding models. A key aspect of the study is the comparison between an encoder-decoder model (ED) and a permutation-invariant model, CBOW, although an analysis of skip-thought vectors is also provided, albeit with limitations due to differences in training corpora.
The analysis yields several intriguing and somewhat unexpected findings, which the authors carefully examine and, for the most part, effectively explain. However, the discussion of the word-order experiment could be improved, as it would be beneficial to frame the investigation in terms of comparing model performance to a theoretical upper bound derived from natural language statistics, an approach partially explored in Section 7. A more upfront consideration of natural language statistics would have strengthened the explanation of the observed results. Similar concerns arose regarding the word-order experiments.
In my view, the most compelling results pertain to the ED model, particularly the finding that the LSTM encoder appears to be relatively insensitive to natural-language ordering statistics, which seems counterintuitive given the potential benefits for per-parameter expressivity. Additionally, the decrease in word content accuracy for high-dimensional embeddings is puzzling and may warrant further investigation, potentially through decoder handicapping.
Overall, this paper provides a valuable exploration of the information content stored in various sentence embedding models, yielding insightful results and contributing meaningfully to the field. I strongly recommend acceptance of this manuscript.