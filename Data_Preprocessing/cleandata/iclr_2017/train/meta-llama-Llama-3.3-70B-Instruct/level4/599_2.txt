This paper presents an approach to addressing supervised multivariate time series tasks with missing values, building upon the concept of utilizing recurrent neural networks (specifically, GRU) for sequence-based supervised learning, such as classification. The key innovation lies in modifying the input and hidden layers of RNNs to accommodate missing values.
The strengths of this paper include:
1) The crucial insight into leveraging missing values, coupled with the intriguing observation of a decaying effect in healthcare applications, which offers a valuable perspective.
2) The experimental design appears robust, with a thorough selection of baseline algorithms and a comprehensive analysis of the results.
However, there are several limitations:
1) The novelty of this work is somewhat limited, as the primary architectural modification involves introducing a decaying smooth factor to the input and hidden layers.
2) The datasets employed in this study are relatively small, which may impact the generalizability of the findings.
3) The observed decaying effect may not be universally applicable across different domains, potentially restricting the scope of this research.