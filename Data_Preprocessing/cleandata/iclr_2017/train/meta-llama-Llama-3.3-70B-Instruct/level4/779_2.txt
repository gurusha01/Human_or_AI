This paper presents a comparative analysis of various strategies for identifying a concise vocabulary list in neural machine translation, with the key result being the superiority of word alignment dictionaries over other methods.
In my assessment, for this paper to have a substantial impact, it must provide a compelling rationale for prioritizing vocabulary over alternative approaches, such as characters or subword units like BPE. Several potential justifications, including synthesizing morphology and handling transliteration, could be advanced, but these would necessitate specific models and experiments not included in this study. As it stands, I consider this paper a useful yet modest contribution, demonstrating the effectiveness of word alignment in generating short lists, but failing to make a strong case for abandoning research in other areas.
Minor comments:
The paper could benefit from mentioning additional approaches to vocabulary modeling, such as the discriminative word lexicon proposed by Mauser et al. (2009) and the neural variant introduced by Ha et al. (2014).
Providing the coverage rate of the actual full vocabulary, rather than the limited 100k "full vocabulary," would be informative, as this technique could potentially be applied to much larger vocabularies.
When reducing vocabulary size during training, the method employed by Mi et al. (2016), which involves taking the union of all vocabularies in a mini-batch, seems unusual. Using the vocabulary of a single sentence would preserve the probabilistic semantics of the translation model, as p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, which is deterministic in this context. In contrast, the mini-batch vocabulary approach no longer constitutes a sensible probability model. Therefore, although it may be more challenging to implement, comparing this approach to using a single sentence's vocabulary would be a worthwhile and logical evaluation.