In the absence of the authors' response, the initial assessment remains unchanged.
---
This manuscript proposes a nonlinear dynamical framework for analyzing multiple related multivariate time series. The model consists of a linear observation component conditioned on latent variables, a linear or nonlinear dynamical system governing the evolution of consecutive latent variables, and a similarity constraint imposed between any two time series, which is provided as prior knowledge and is non-learnable. Notably, the predictions and constraints yielded by the model's components follow a Gaussian distribution, as the model estimates both the mean and the variance or covariance matrix. The inference process is unidirectional.
The model's performance is evaluated across four datasets and benchmarked against several baseline models, including plain auto-regressive models, feed-forward networks, recurrent neural networks (RNNs), and dynamic factor graphs (DFGs), which are essentially RNNs that facilitate both forward and backward inference of latent variables.
The proposed model, distinguished by its introduction of lateral constraints between different time series and its capability to predict both the mean and covariance, presents an interesting approach. However, it is not without limitations.
A significant oversight is the lack of reference to variational auto-encoders and deep Gaussian models, which similarly predict both the mean and variance during the inference phase. This omission is noteworthy, given the relevance of these models to the work presented.
Furthermore, the datasets utilized in the evaluation are remarkably small. For instance, the WHO dataset comprises only 91 time series, each consisting of 520 time points (52*10). Although the experimental results suggest the proposed model may outperform RNNs, the small dataset size and the high variance in the results underscore the need for additional experiments with longer time series to validate these findings. Enhancing the paper with detailed information about the model's architecture (e.g., the specifics of the multi-layer perceptron (MLP) used) and a comparison of the time complexity among the models, particularly between DFGs and the proposed model, would be beneficial.
A minor observation is that the footnote on page 5 appears to pertain to the structural regularization term rather than the dynamical term, as might be expected.