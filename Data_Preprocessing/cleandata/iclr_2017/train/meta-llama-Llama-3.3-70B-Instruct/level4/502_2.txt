This paper introduces a novel evaluation metric for dialogue systems, demonstrating a stronger correlation with human annotations. While I concur that traditional metrics like BLEU are overly simplistic and fail to capture sufficient semantic information, the proposed metric appears to be excessively complex and difficult to interpret.
Alternatively, equation 1 could be utilized as a retrieval-based dialogue system, implying that the approach suggested in this paper essentially involves training one dialogue model to assess another. This raises a fundamental question: why should we trust this model's evaluation? This concern is also pertinent to the final point in my detailed comments.
Detailed comments:
- What aspects of dialogue systems does this metric capture or evaluate? Unlike BLEU, which is known to capture n-gram overlap, it is challenging to determine what this model evaluates. If this is indeed the case, it becomes difficult to address potential issues, such as data dependence.
- Why not adopt an incremental approach to building the model? As equation (1) illustrates, this metric relies on both context and reference to compute a score. Is it possible to demonstrate a score function that utilizes only the reference, thereby ensuring that this metric draws from the same information source as BLEU or ROUGE?
- Regarding equation (1), can the metric be designed as a nonlinear function? The comparison between BLEU (or ROUGE) and the new metric in Figure 3 seems to resemble a comparison between exponential and linear scales.
- I find the two reasons presented in section 5.2 unconvincing when considered together. To strengthen the argument, I would like to see the correlation with average scores. A more robust approach would be to present results both with and without averaging.
- Table 6 suggests that the metric favors short responses, which is contrary to BLEU's behavior of penalizing short sentences. However, human annotators also tend to assign high scores to short responses, as longer sentences are more likely to contain irrelevant words. Can the length factor be controlled for during annotation, or is the observed correlation unsurprising?