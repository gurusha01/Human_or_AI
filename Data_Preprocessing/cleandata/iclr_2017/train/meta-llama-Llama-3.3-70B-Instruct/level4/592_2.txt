This manuscript introduces a variant of the variational autoencoder (VAE) that incorporates a discrete latent variable, which selectively activates a subset of the latent code, referred to as an "epitome," for each sample. The rationale behind this approach is to encourage the model to utilize a larger portion of the latent code by allowing different latent variables to be active for different samples, thereby addressing the issue of latent variable over-pruning.
Although the problem of latent variable over-pruning is a significant concern in variational inference, the proposed solution does not appear to offer a substantial improvement over existing methods, such as a mixture of VAEs. A mixture of VAEs, which utilizes a categorical variable and multiple VAEs, would have been a suitable baseline for the experiments in this paper. The primary distinction between a mixture of VAEs and the proposed epitomic VAE lies in the sharing of parameters between the different components.
The experimental section presents several misleading results. Firstly, the log-likelihood of the proposed models is estimated using the Parzen window estimator, whereas a more accurate lower bound on the likelihood, readily available for VAEs, is not reported. In the reviewer's experience, it is feasible to achieve a continuous MNIST likelihood of over 900 nats using a moderately sized VAE.
Furthermore, the presentation of results switches between binary MNIST and continuous MNIST experiments, which is confusing due to the distinct challenges posed by these datasets for likelihood-based models. Continuous MNIST is more difficult to model with high-capacity likelihood-optimizing models, as the dataset lies in a proper subspace of the 784-dimensional space. Models that aim to maximize the likelihood often exploit this property by concentrating the probability around the subspace at the expense of accurately modeling the data. In contrast, samples from a well-tuned VAE trained on binary MNIST or a VAE trained on continuous MNIST with added noise tend to appear more realistic than those presented in the experimental results.
The claim that the VAE uses its capacity to "overfit" to the training data is unsubstantiated, as no evidence is provided to demonstrate that the reconstruction likelihood on the training data is significantly higher than on the test data. The use of the term "overfitting" in this context is misleading, as it deviates from its technical definition.
Additionally, the application of dropout in the dropout VAE is not clearly specified, leaving ambiguity as to whether dropout is applied to the latent variables or the hidden layers of the encoder/decoder. These two options would exhibit distinct behaviors, and clarification is necessary.
The samples and reconstructions from the MNIST eVAE appear to be a more diverse version of 2D VAE samples/reconstructions, characterized by blurriness and a lack of precise stroke positioning. This is consistent with an interpretation of the eVAE as a type of mixture of smaller VAEs, rather than a higher-dimensional VAE. Therefore, it is misleading to claim that the eVAE outperforms a high-dimensional VAE based on this evidence.
In the reviewer's opinion, the manuscript is not yet ready for publication. A more robust baseline VAE, evaluated using the evidence lower bound or another reliable method, is essential for comparing the proposed eVAE to VAEs.