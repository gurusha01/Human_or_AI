This paper explores the alignment of word vectors across languages, where the embeddings have been learned separately in monolingual environments. The strategy addressed in this paper has plausible applications, making it an intriguing problem. The paper is generally well-executed, although it falls short in terms of evaluation, and attempting a more robust downstream task would have been beneficial.
The concept of inverted Softmax is particularly noteworthy.
To enhance the paper for publication, several minor issues should be addressed:
1) The authors should acknowledge and discuss the prior work of Haghighi et al (2008), "Learning Bilingual Lexicons from Monolingual Corpora," which is a key reference for using CCA in bilingual alignment.
2) Similarly, the paper by Hermann & Blunsom (2013), "Multilingual distributed representations without word alignment," should be cited for learning multilingual word embeddings from aligned multilingual data.
3) Conducting experiments with more linguistically diverse language pairs, rather than solely focusing on European/Romance languages, would strengthen the results.
4) The discussion on orthogonality requirements seems related to using a Mahalanobis distance or covariance matrix to learn such mappings, which could be explored further in the discussion.
5) Consider alternative terminology to "translation" when referring to word alignment across languages, as it may imply a more complex process than intended.
6) The Mikolov citation in the abstract contains an error and should be corrected.