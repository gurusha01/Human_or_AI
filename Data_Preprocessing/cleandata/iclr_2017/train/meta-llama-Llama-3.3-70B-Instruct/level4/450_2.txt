This submission explores the concept of 2-sample testing through the lens of classifier evaluation. When a classifier is applied to two samples from the same distribution, the classification accuracy distribution takes on a straightforward form under the null hypothesis, allowing for the derivation of a simple threshold for any classifier. Consequently, developing a more powerful test becomes a matter of training a more effective classifier, with potential areas of focus including deep neural networks, for which statistical measures like the MMD can be challenging to characterize.
The strengths of this approach include its soundness and broad applicability. Furthermore, the paper's timing is opportune, given the significant impact of deep learning on classification and prediction tasks, despite its relatively limited influence on statistical hypothesis testing compared to kernel methods.
However, the discussion surrounding the relationship between the proposed approach and kernel-MMD could be improved. For instance, kernel-MMD can also be viewed as a classifier-based method, warranting a more nuanced comparison. Additionally, the choice of linear kernel-MMD for comparisons, which is less powerful than the quadratic kernel-MMD, seems contradictory to the discussion, particularly given the justification based on computational time. It is also worth noting that the linear time kernel-MMD, as seen in the work of Zaremba et al. (NIPS 2013), exhibits a Gaussian distribution under the null, contrary to the arguments presented against kernel-MMD due to its complex null distribution.
The insightful comment from Arthur Gretton on December 14 during the discussion period was highly beneficial. Incorporating these insights, along with additional experiments comparing kernel-MMD to the classifier threshold on the blobs dataset, would greatly enhance the understanding of the paper. The open review format provides a unique opportunity to acknowledge and cite these contributions appropriately, ensuring proper credit for the experiments and insights shared.