This paper presents a straightforward approach to reducing computational complexity and memory usage in ConvNets by eliminating low-weight filters. The methodology is tested on VGG-16 and ResNets using the CIFAR10 and ImageNet datasets.
The strengths of this paper include:
- The creation of structured sparsity, which enhances performance without requiring modifications to the underlying convolutional implementation
- The simplicity of its implementation
However, a notable weakness is:
- The lack of assessment on how filter pruning affects transfer learning capabilities
Overall, I have a favorable view of this work. Although the core concept is relatively simple, it appears to be a novel contribution with a well-presented set of experimental results. Thus, I am inclined to recommend acceptance. A significant omission, however, is the evaluation of the impact of filter pruning on transfer learning performance. Given that the value of learned representations lies in their ability to be transferred to other tasks, which is of greater interest in both academic and industrial contexts than the specific tasks of CIFAR10 or ImageNet, it is crucial to investigate this aspect. There is a potential risk that while the primary task performance may remain unaffected, transfer learning capabilities could be substantially impaired. Unfortunately, this paper misses the opportunity to explore this critical direction.
A minor point to consider: Figure 2's title references VGG-16 in subplot (b) and VGG_BN in subplot (c); it would be beneficial to clarify whether these refer to the same model architectures.