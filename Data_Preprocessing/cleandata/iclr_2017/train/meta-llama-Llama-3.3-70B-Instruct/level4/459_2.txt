The authors propose a tensor factorization method for multi-task learning (MTL) that enables the discovery of cross-task structures, leading to improved generalization capabilities. The presentation is well-organized and easy to follow, with experimental results that effectively support the approach.
To further enhance the paper, it would be beneficial to include an analysis of the trade-off between model size and performance in the final version, as well as explore applications in related fields.
A clarification is sought regarding Section 3.3, where a deep neural network (DNN) is trained for each task with an identical architecture to construct the DMTRL. The importance of this pretraining step is questioned, and it is wondered whether random initialization would yield comparable results. Additionally, the impact of class imbalance, where certain classes have limited examples, on the model's performance is also a concern that warrants investigation.