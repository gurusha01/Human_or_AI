This paper proposes Gated Multimodal Units (GMUs), a novel approach that utilizes multiplicative weights to modulate the influence of various modalities on the activation of hidden units. Additionally, the authors introduce the "Multimodal IMDb" dataset, comprising over 25,000 movie summaries, posters, and corresponding genres.
The GMU concept bears resemblance to the "mixture of experts" framework, where different components of the model specialize in distinct examples, albeit with a key difference: individual hidden units are gated independently, rather than routing entire examples. Similarly, GMUs share similarities with attention models, as they assign varying weights to different input modalities, with a focus on modality-specific gating.
The introduction of the Multimodal IMDb dataset is a significant contribution, and the paper presents an array of experiments exploring text representation and single-modality versus two-modality settings. However, a crucial aspect that is lacking is a thorough discussion, experimentation, and analysis comparing GMUs to other multiplicative gate models, which constitutes the core intellectual contribution of the paper. For instance, a comparative analysis with mixture of experts, attention models, or other gated models could yield valuable insights and potentially impressive performance. The authors are encouraged to further develop their work and submit a revised paper when ready.
In its current form, the paper is suitable for a workshop, but it requires additional development to meet the standards of a major conference.