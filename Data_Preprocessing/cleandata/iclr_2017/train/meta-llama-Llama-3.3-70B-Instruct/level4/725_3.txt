Update: As the authors have not provided a revised version of the paper, I am downgrading my rating to "marginally below acceptance".
This paper explores the problem of training stochastic feedforward neural networks by proposing a weight transfer mechanism from a deterministic deep neural network to a stochastic network with the same topology. The initial weight transfer process involves rescaling unit inputs and layer weights, as well as specifying stochastic latent units when the pretraining DNN employs ReLU nonlinearities. Initial experiments on MNIST classification and a toy generative task with a multimodal target distribution demonstrate that this simple transfer process is effective when the pretraining DNN uses sigmoid nonlinearities, but not when it uses ReLUs. 
To address this issue, the paper introduces the concept of a "simplified stochastic feedforward neural network" (SFNN), where each stochastic layer is followed by a layer that takes an expectation over samples from its input, thereby limiting the propagation of stochasticity in the network. A modified weight transfer process from a pretraining DNN to the simplified SFNN is described and justified. The training process involves three steps: (1) pretraining a DNN, (2) transferring weights from the DNN to a simplified SFNN and continuing training, and (3) optionally transferring the weights to a full SFNN and continuing training, or transferring them to a deterministic model (DNN*) and continuing training. The third step can be skipped, and the simplified SFNN can be used directly as an inference model.
Experimental results on MNIST classification show that using simplified SFNN training can improve the performance of a deterministic DNN model over a DNN baseline trained with batch normalization and dropout. Experiments on two generative tasks (MNIST-half and the Toronto Faces Database) demonstrate that the proposed pretraining process improves test set negative log-likelihoods. Furthermore, experiments on CIFAR-10, CIFAR-100, and SVHN using the LeNet-5, network-in-network, and wide residual network architectures show that incorporating a stochastic training step can enhance the performance of a deterministic (DNN) model.
However, there are some areas that require clarification and improvement. For instance, the term "multi-modal" is used to refer to generative tasks with a multimodal target distribution, which could be confused with tasks that involve multiple sensory modalities. It would be more precise to use the term "generative tasks with a multimodal target distribution" initially and then use "multi-modal tasks" for brevity throughout the paper.
Additionally, the notation using superscripts to indicate layer indexes is confusing, as it can be misinterpreted as exponentiation. It would be better to use a different notation to avoid this confusion. The paper would also benefit from clearer explanations and justifications for certain concepts, such as the weight transfer process and the use of simplified SFNNs.
The results presented in the paper are promising, but there are some inconsistencies and areas that require further investigation. For example, Table 1 shows that the 3 hidden layer SFNN initialized from a ReLU DNN has a significantly worse test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN, which warrants further explanation. Moreover, when transferring weights back from the simplified SFNN to the DNN* model, it is unclear whether some sort of rescaling is necessary to undo the operations in Equation (8).
The paper has several strengths, including the ease of implementation and application of the proposed model to other tasks. The MNIST results showing that stochastic model training can produce a deterministic model (DNN*) that generalizes better than a DNN trained with batch normalization and dropout are particularly exciting. However, there are also some weaknesses, such as the paper being occasionally hard to follow due to notation and terminology issues. The results on CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization, as demonstrated on MNIST.
Minor issues include inconsistent terminology, notation, and formatting. For instance, the use of "SFNN" to refer to both singular and plural cases can be confusing, and it would be better to use "SFNNs" when referring to multiple stochastic feedforward neural networks. The citation style is also inconsistent, with some references including authors' first initials and last names, while others only include last names. It would be better to adopt a consistent citation style throughout the paper.
In conclusion, while the paper presents some promising results and ideas, it requires significant revisions to address the issues mentioned above. With careful attention to notation, terminology, and formatting, as well as further clarification and justification of certain concepts, the paper has the potential to make a valuable contribution to the field.