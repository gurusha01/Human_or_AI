This paper proposes an enhanced approach to density estimation for sparse data, specifically text documents, utilizing deep generative Gaussian models, namely variational auto-encoders (VAEs). It also introduces a method to derive word embeddings from the model's generative parameters, allowing for interpretability comparable to Bayesian generative topic models.
To understand the contributions, it's essential to outline the generative process described in the paper: a K-dimensional latent representation is sampled from a multivariate Gaussian distribution, followed by a multilayer perceptron (MLP) with parameters θ that predicts unnormalized potentials over a vocabulary of V words. These potentials are then exponentiated and normalized to form the parameters of a multinomial distribution, from which word observations are sampled to create a document. The intractable inference is addressed using the VAE formulation, where an inference network with parameters φ predicts the mean and variance of a normal distribution for each document, enabling reparameterized gradient computation.
The first contribution, although straightforward, involves incorporating tf-idf features to inject global information into local observations, which the authors argue is particularly beneficial for sparse data like text.
The second contribution is more noteworthy, as it involves optimizing the generative parameters (θ) and variational parameters (φ) in a manner reminiscent of the original Stochastic Variational Inference (SVI) procedure. Here, the variational parameters φ are treated as global, while the predicted mean μ(x) and covariance Σ(x) for each observation x are considered local variational parameters. Unlike the standard VAE, where local parameters are indirectly optimized through the global parameters used in their prediction, this approach directly optimizes local parameters while keeping the generative parameters fixed, followed by updating the global variational parameters. This procedure, derived from SVI, makes the contribution more relatable and less of a novel trick.
However, some aspects require clarification. For instance, the functional form of the gradient used in the optimization of local parameters is not provided, and the use of the initial prediction of local parameters in the gradient step for global variational parameters, without considering the optimization of local parameters, is unclear. The authors should discuss why the initialization problem of the generative model is expected to be worse for sparse data, as this underpins the rationale for the proposed optimization technique.
The final contribution presents a method to derive word embeddings from the generative model parameters, allowing for context-sensitive interpretations of what the model has learned, particularly since the latent variable models an entire document.
Regarding the figures, there seems to be a discrepancy between the caption and the legend of Figures 2a and 2b, which needs clarification. If interpreted according to the caption, it appears that deeper networks with more data exposure benefit from the optimization of local parameters. However, it's unclear whether models without local parameter optimization have reached a performance plateau, as the training time is not directly comparable.
The analysis of singular values offers an interesting insight into the model's capacity utilization but requires a more detailed explanation to enhance interpretability, especially for Figures 2c and 2d.
The evaluation of the word embedding technique lacks a predictive task assessment, and the reproducibility of Table 2b is compromised due to the vague description of the document creation process. It seems that a more careful design might be necessary to achieve the illustrated results.
In summary, while both the inference technique and the embedding method appear useful, presenting them separately might have allowed for a more in-depth exploration of each. Clarifications and additional details are necessary to fully appreciate the contributions and methodology presented in the paper.