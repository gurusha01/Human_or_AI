This paper introduces a large-scale reading comprehension dataset, aiming to ultimately release 1 million questions and answers, with 100,000 queries and their corresponding answers currently available. The dataset distinguishes itself from existing reading comprehension datasets by utilizing user queries rather than those generated by crowd-workers, and by having answers generated by crowd-workers rather than being limited to spans of text from the provided passage. An analysis of the dataset is presented, including the distribution of answer types, and the performance of various generative and cloze-style models on the MS MARCO dataset is also reported.
The key strengths of the paper include:
1. It highlights the limitations of current reading comprehension datasets, noting that crowd-worker-generated questions have a different distribution than those asked by actual users of intelligent agents, and that answers are often restricted to spans of text rather than requiring reasoning across multiple pieces of text or passages.
2. The MS MARCO dataset offers novel and useful characteristics, including questions sampled from user queries and answers generated by humans, setting it apart from existing datasets.
3. The experimental evaluation of baseline models on the MS MARCO dataset is satisfactory, providing a solid foundation for further research.
However, several weaknesses and suggestions for improvement are identified:
1. The paper lacks a report on human performance on the dataset, which is crucial for estimating the difficulty of the dataset and understanding the degree of inter-human agreement. This agreement would also reflect how well the metric used can handle variance in sentence structure with similar semantics.
2. A comparison between the answer type distribution in the MS MARCO dataset and that in existing reading comprehension datasets, such as SQuAD, would be beneficial. This comparison would substantiate the claim that the distributions of questions asked by crowd-workers differ from those of user queries.
3. The use of automatic metrics like ROUGE and BLEU for evaluating natural language answers is questioned, given their known poor correlation with human judgment in tasks such as image caption evaluation. Justification for the use of these metrics in evaluating open-ended natural language answers is needed.
4. Details about the classifier used to filter answer-seeking queries from all Bing queries, including its accuracy, would provide insight into the percentage of MS MARCO questions that are indeed answer-seeking queries. Similarly, the accuracy of the information retrieval-based system used to retrieve passages for filtered queries should be reported.
5. The description of the best passage baseline should be included in the paper to provide a comprehensive understanding of the methodologies employed.
6. Minor formatting adjustments, such as correcting opening quotes, would improve the paper's readability.
In summary, the paper is well-motivated, with the use of user queries and human-generated answers making the dataset unique. However, human performance on the dataset, a quantitative comparison between the distribution of user queries and crowd-sourced questions, and commentary on the use of automatic metrics in light of their limitations in correlating with human judgments are necessary for a more comprehensive evaluation.