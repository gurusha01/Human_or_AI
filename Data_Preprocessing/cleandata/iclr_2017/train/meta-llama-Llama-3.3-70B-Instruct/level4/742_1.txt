This paper explores the expressivity of deep networks through a combination of theoretical and empirical methods, focusing on random networks with Gaussian weights and hard tanh or ReLU activation, evaluated based on criteria such as neuron transitions, activation patterns, dichotomies, and trajectory length.
However, the justification for the newly introduced expressivity measures appears to be lacking, with the trajectory length being a particularly questionable metric. Its validity as a measure of expressivity is solely supported by its proportionality to other measures in the context of random networks.
The paper's excessive length and obscurity hinder its clarity, making it challenging to contextualize the work properly. Some of the findings seem straightforward and unremarkable.
Detailed comments include:
On page 2, the statement that much of the work on achievable functions relies on unrealistic architectural assumptions, such as exponentially wide layers, is disputed. For instance, "Deep Belief Networks are Compact Universal Approximators" by Leroux et al. demonstrates that deep, narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression, contradicting this claim.
The comparison of architectures is also noted to limit the generality of conclusions, but previous work, such as Leroux et al., has focused on mathematical proofs, leading to general conclusions on the representative power of deep networks. In contrast, the approach proposed, based on random networks, is harder to generalize, as these networks are not typically used in practice.
The study of random networks after initialization is argued to be irrelevant, as these networks are intermediate steps not used for computations. Justification is needed to explain why their representative power is relevant.
The results on random networks are claimed to provide natural baselines for comparing trained networks, but it is unclear how the representative power of random networks relates to that of the whole class of networks or the class of networks after training. These are the classes of networks that are of primary interest, and a justification for why studying random networks helps in understanding them is necessary.
On page 5, the proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary is noted to depend on the network being random, which seems to limit the generalizability of this finding to other networks.
On page 6, the expressivity with respect to remaining depth is considered a trivial concern, equivalent to expressivity with respect to depth. This makes the remark in figure 5 about the number of achievable dichotomies depending only on the number of layers above the swept layer seem unremarkable.
On page 7, the network width of 100 for MNIST is considered too small, resulting in poor performance and making it difficult to generalize the results to relevant situations.