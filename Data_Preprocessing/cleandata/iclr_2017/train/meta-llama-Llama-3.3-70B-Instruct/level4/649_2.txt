This paper assesses the impact of various context types on the quality of word embeddings across multiple benchmarks.
I have mixed feelings about this submission. On the positive side, it builds upon a crucial research direction by isolating specific parameters from embedding algorithms, with a focus on context; however, I struggle to discern a clear conclusion from the experimental results. The findings do not seem to demonstrate a substantial and consistent benefit of one context type over others. This raises questions - is this due to the benchmarks' inability to detect existing differences, or are the differences truly negligible?
Although I am willing to accept this paper, I believe a more comprehensive version would be preferable, one that delves deeper into these fundamental questions and provides more insight.
Furthermore, the discussion of context types could be expanded to include other relevant forms, such as those explored in "Open IE as an Intermediate Structure for Semantic Tasks" (Stanovsky et al., ACL 2015), which warrants consideration in this context.