This paper explores the application of machine learning (ML) to Interactive Theorem Proving (ITP), specifically predicting the usefulness of a given statement in proving a conjecture. The authors frame this as a binary classification task, proposing a dataset and deep learning-based baseline models. 
From a machine learning perspective, one of the paper's primary objectives should be to introduce the problem in an accessible manner for the ML community. While the paper is generally well-written, certain sections lack clarity, particularly Section 2. Key terms like LCF, OCaml-top level, and deBruijn indices are used without explanation or reference, making them difficult to understand for those without an ITP background. 
Furthermore, the data splitting process into training and test sets is unclear. It is unspecified whether the examples in the train and test sets pertain to the same conjecture or always to different conjectures. The application of deep learning models is also unclear. For instance, in the leftmost architecture of Figure 1, where each character is embedded into a 256-D vector and processed until the global max-pooling layer, it is unclear if this layer computes the maximum along each feature and across all input characters.
Another concern is the exclusive focus on deep learning methods as baselines. Comparing these with standard NLP techniques, such as Bag of Words followed by SVM, would provide valuable insights into the problem's difficulty. Although neural networks are likely to outperform these methods, the comparison would offer a benchmark.
The analysis of the algorithm's success and failure cases is also lacking. Examining these cases could provide insights to inform the design of future models. 
In general, the application of ML to theorem proving is an intriguing research direction. However, the paper's opacity, particularly in data construction, hinders understanding. Clarifying these aspects would significantly improve the paper. While the baseline models perform well, there is a lack of insight into their limitations. The statement that the models are unable to perform logical reasoning is too vague and could be supported with examples of mistakes. 
Given my limited familiarity with the ITP literature, assessing the dataset's value is challenging. However, based on the references, it appears to be derived from benchmark conjectures and proofs used in the ITP community, suggesting its potential quality. 
Currently, my rating is a weak reject, but addressing these concerns could lead to a revision to an accept rating.