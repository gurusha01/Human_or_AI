This paper presents a novel nonparametric neural network approach that dynamically determines the model size during training by introducing random zero units and leveraging sparse regularization to eliminate irrelevant weights. Essentially, this method can be viewed as a random search strategy over a discrete space, facilitated by sparse regularization to remove redundant units. The problem addressed is significant, and the paper yields intriguing results. The following points require further consideration:
The computational complexity of the proposed algorithm needs to be clarified. Specifically, the process of decomposing fan-in weights into parallel and orthogonal components, followed by a transformation into radial-angular coordinates, may incur substantial additional computational costs. A detailed analysis comparing the operational complexity of this method to traditional parametric neural networks would be beneficial. Additionally, experimental evaluations of runtime performance would provide valuable insights.
Notably, the nonparametric networks tend to yield smaller networks when applied to convex datasets, which can result in inferior performance compared to parametric networks. It would be enlightening to gain a deeper understanding of this phenomenon and the underlying reasons for this observed behavior.