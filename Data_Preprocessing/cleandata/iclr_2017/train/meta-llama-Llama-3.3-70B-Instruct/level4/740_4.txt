This paper introduces ParMAC, a parallel and distributed extension of the Method of Auxiliary Coordinates (MAC), designed to learn complex, nested, and non-convex models through a deep net composition of multiple processing layers. The MAC approach optimizes nested objective functions by introducing auxiliary coordinates as equality constraints, thereby breaking down nested relationships and enabling optimization through alternating steps: the W step, which updates parameters by splitting the model into independent submodels, and the Z step, which ensures consistency across submodel inputs and outputs. ParMAC builds on this by translating MAC's inherent parallelism into a distributed system framework, leveraging data parallelism and model parallelism to minimize data movement and reduce communication overhead, a critical consideration in large-scale datasets and modern architectures. The authors provide analysis on parallel speedup and convergence, along with an MPI-based implementation for optimizing binary autoencoders, and demonstrate ParMAC's efficacy on three color image retrieval datasets.
The paper's structure and presentation are commendable, making the content accessible. However, several questions arise:
- Given that the MAC framework provides an approximate solution to the original problem, and considering that smoothing functions like sigmoid can facilitate naive optimization methods, what distinguishes the ParMAC approach from these alternatives, and what benefits does it offer over existing methods?
- A notable omission is the comparison of ParMAC with other distributed approaches aimed at solving the same nested function optimization problems, which would provide valuable insights into its relative performance and advantages.