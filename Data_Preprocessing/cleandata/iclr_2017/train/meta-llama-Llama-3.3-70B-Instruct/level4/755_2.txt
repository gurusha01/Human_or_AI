This manuscript investigates the optimization of linear ResNet and provides a mathematical demonstration that, for 2-shortcuts and zero initialization, the Hessian's condition number is depth-independent. Although I have briefly reviewed the proof, I have not thoroughly verified its accuracy.
The finding is a valuable insight into the training of deep linear networks. However, I believe the paper does not fully address the distinction between linear and nonlinear models. Several questions arise:
1. The revised manuscript includes results using ReLU units, but they appear to be added only to the middle positions of the network (Section 5.3). Is this a standard practice in ResNet architectures? Furthermore, ReLU is non-differentiable at zero, which contradicts the condition stated in Theorem 1. Why not consider using differentiable activations like sigmoid or tanh instead?
2. Equation (22) in the appendix suggests that, for nonlinear activations, the condition number depends on the derivative of the activation function at zero. If tanh is used, which has a derivative of 1 at zero, the condition number would be the same for both linear and tanh activations. Nevertheless, this might not be sufficient to explain the performance differences between linear and nonlinear networks or how the situation changes after learning the zero point.
3. The success of ResNet (and convolutional networks in general) in computer vision may be attributed to various types of nonlinearity, including pooling. Can the results presented in this manuscript be generalized to include pooling as well?
Minor comments:
- In the last paragraph of Section 1, it is mentioned that low approximation error typically implies a more powerful model class and better training error. However, this does not necessarily translate to better test error.
- In Section 4.1, the term "zero initialization with small random perturbations" is used. What is the rationale behind not using exact zero initialization, and what is the magnitude of the random perturbation?