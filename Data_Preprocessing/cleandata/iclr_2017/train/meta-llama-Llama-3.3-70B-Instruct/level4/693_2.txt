This paper presents a novel approach to rapidly adapting generative models in low-data regimes by leveraging techniques from one-shot learning, specifically drawing inspiration from matching networks. The proposed generative matching networks model is essentially a variational auto-encoder that can be conditioned on an input dataset. When given a query point, the model utilizes an attention mechanism in an embedding space to match the query point to points in the conditioning set, akin to the operation of matching networks. The experimental results on the Omniglot dataset demonstrate the model's capability to quickly adapt to new input distributions with few examples.
Although I find the method intriguing, my primary concern with this paper is its lack of clarity. As detailed below, I encountered difficulties in following the paper due to the scattered presentation of details, which left me uncertain about my ability to implement the method and replicate the results. To address this, I recommend consolidating the major implementation details into a single section and explicitly defining the functional forms of the various embedding functions and their variants.
I was somewhat disappointed to note the reliance on weak supervision in the form of labels. It would be interesting to explore the method's performance in a completely unsupervised setting as a baseline.
The paper lacks clear definitions of the different functions used. Providing basic insights into the functional forms of f, g, Ï†, sim, and R would be beneficial for understanding the methodology. Without such clarification, the operational details remain obscure.
In Section 3.2, the statement "only state of the recurrent controller was used for matching" is unclear. Upon multiple readings, my interpretation is that the pseudo-input is used in place of a regular input, but this requires further clarification. Additionally, Section 4.2 introduces two versions of the model: one utilizing a pseudo-input and a conditional version without it. The difference in functional form between these models, particularly how the formulas for the embeddings f and g change, needs to be explicitly stated.
The phrase "since the result was fully contrastive we did not apply any further binarization" is unclear. Elucidating what it means for a result to be fully contrastive would be helpful.
For enhanced clarity, the figures and tables refer to the number of shots, but this term is not defined. Assuming it corresponds to T, consistency in terminology should be maintained.
In Figure 2, the value of T is limited to 9. Clarification on what it means for T to be 0 and why it does not extend up to 20 (as implied by the correspondence of shot to T) is necessary. The results appear to improve with an increased number of steps, so including results for 5 and possibly 6 steps would be beneficial. It is logical to assume that diminishing returns will occur at some point.
Regarding Table 1, the fairness of using a VAE as a baseline is questionable. The evaluation metric seems to differ between the models, as Ctest affects Pd() but is not applicable to the VAE. The authors should clarify this discrepancy to ensure an apples-to-apples comparison.
Given that MNIST is a more common dataset than Omniglot for evaluating generative models, conducting similar experiments on MNIST would facilitate comparisons with a broader range of models.
Furthermore, the negative log-likelihood values decrease monotonically with the number of shots. It would be interesting to explore if there are scenarios where increasing the number of shots could have adverse effects and to examine the behavior at T=30 or T=40.
On a minor note, the paper contains grammatical issues, including missing determiners in several sentences and an incorrect reference to the model as "she" instead of "it." Additionally, "On figure 3" should be corrected to "in figure 3" in the experiments section.