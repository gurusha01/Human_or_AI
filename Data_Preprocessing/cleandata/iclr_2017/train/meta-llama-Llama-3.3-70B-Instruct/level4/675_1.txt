This paper's excessive length, spanning 17 pages without supplementary material, is notable, particularly given the lack of an official page limit at ICLR. While some flexibility is acceptable, exceeding the conventional 8-page limit by more than double may be considered unreasonable, as it places an undue burden on reviewers.
The paper introduces a novel artificial dataset for sequence learning, generated from the MNIST dataset of handwritten digits. Additionally, the authors propose "incremental learning," a training schedule for recurrent networks based on sequence length. Experimental results demonstrate the superiority of this schedule over the absence of a schedule and other intuitive schedules on this dataset, with ablation studies verifying the curriculum's impact on performance.
However, several concerns arise: 
- The paper lacks novelty, as the proposed incremental learning schedule is not a new concept and has been explored previously, such as in the work of Bengio (2015) and Ranzato (2015). The only novel aspect is the ablation study conducted to confirm the curriculum's effect on performance.
- The authors only evaluate their hypothesis on the proposed artificial dataset, raising questions about its applicability to real-world sequential datasets, such as language modeling. It is likely that the reported 74% performance gain may not be replicable in scenarios with large vocabulary sizes.
- The value of the artificial dataset is unclear, given the abundance of real-world sequential datasets in various domains. The creation of this dataset appears to be primarily for demonstrating the proposed ideas, rather than addressing a specific need. Experiments on other datasets would have strengthened the paper.
- The paper's excessive length is partly due to unrelated experiments, such as those in Sections 6.2 and 6.4, which do not contribute to the main narrative and could be omitted to improve the paper's focus and conciseness.