This work presents a novel approach to integrating Recurrent Neural Networks (RNNs) within a convolutional network architecture to facilitate the propagation of spatial information across images, yielding promising results in classification and semantic labeling tasks.
The manuscript is well-structured, and the idea is clearly described. The experiments appear to be meticulously designed, and the authors refrain from making exaggerated claims. While the paper may not be groundbreaking, it contributes to the field as a solid example of incremental science.
The strengths of this paper include:
* A clear and concise description of the concept
* Well-constructed experiments that effectively validate the approach
* A simple yet effective idea that combines established techniques
* The absence of overstated claims
* A thorough comparison with related work architectures
However, there are some weaknesses:
* The idea, although innovative, can be seen as an incremental advancement, building upon existing concepts (e.g., Bell 2016)
* The results, while promising, do not surpass the current state-of-the-art
In terms of quality, the ideas presented are sound, and the experiments are well-designed and analyzed.
The clarity of the manuscript is generally good, making it easy to follow, although some relevant details are omitted (as noted in the comments below).
The originality of the work is limited, as it combines well-known ideas in a new way.
The significance of this research lies in its contribution to the development of best practices for building neural networks for tasks such as semantic labeling and classification.
Specific comments and suggestions for improvement include:
* In Section 2.2, the statement "we introduction more nonlinearities (through the convolutional layers and ...)" is inaccurate, as convolutional layers are linear operators.
* The same section raises the question of why RNNs cannot incorporate pooling operators, as there is no apparent impediment to doing so.
* Section 3 contains a typographical error, referring to "into the computational block" without specifying which block is being referenced.
* Figure 2b and 2c are missing, and the references to them should be corrected or the figures themselves should be included.
* Consider adding a brief description of GRU in the appendix for completeness.
* The last sentence of Section 5.1 is unclear, particularly in relation to the non-linearities provided by convolutions, ReLU, and pooling in ResNet.
* Section 5.2.1 (and Appendix A) lacks explicit details on how the learning rate is adjusted, whether manually or automatically, and whether the learning rate schedule is consistent across all experiments.
* The claim in Section 5.2.1 that the baseline is "certainly strong" is overstated, given that the best known results for the Pascal VOC12 competition report an mIoU of 85.4.
* A discussion on the increased memory usage and computational cost associated with the proposed approach is necessary, as these are significant considerations.
* Minor typographical errors, such as "Modules" instead of "modules" in Section 5.2.3, and "Furthermoe" instead of "Furthermore" in Section 6, should be corrected.
* The redundancy between Appendix C and Figure 5 should be addressed.