This paper presents a significant contribution by introducing a construction that enables the eps-approximation of piecewise smooth functions using a multilayer neural network. Notably, this achievement is realized with a network that utilizes O(log(1/eps)) layers and O(poly log(1/eps)) hidden units, allowing for the use of ReLU, binary step, or a combination of these activation functions. The clarity and coherence of the paper are commendable, with arguments and proofs that are straightforward to comprehend. However, two aspects warrant further clarification:
1. The reliance on binary step units in the proof is intriguing. Could the authors elaborate on the centrality of these units to their argument, and whether similar outcomes could be attained without them?
2. It would be insightful to explore the existence of a piecewise smooth function that necessitates at least poly(1/eps) hidden units when using a shallow network, thereby highlighting the limitations of such architectures in certain scenarios.