The proposed method involves training neural networks without conventional nonlinearities, instead utilizing multiplicative gating based on the cumulative distribution function (CDF) of a Gaussian evaluated at the preactivation, which is presented as a relaxation of a probit-Bernoulli stochastic gate. Experiments are conducted using both approaches.
This work exhibits some novelty and interest. However, it lacks a thorough explanation of why this particular approach is preferred over other similar parameterizations, such as sigmoidal or softsign functions. The study would be more convincing with additional empirical analysis to understand why this method is effective and a more in-depth exploration of related conceptual spaces. While the CIFAR results are acceptable by current standards, the MNIST results are underwhelming, with neural networks achieving better than 1.5% accuracy over a decade ago, and the SOI map results, as well as the ReLU baseline, exceed 2%. Additionally, the TIMIT results on frame classification are not particularly noteworthy without an evaluation of word error rate within a speech pipeline, although this is a minor concern.
The claim that SOI map networks without additional nonlinearities are comparable to linear functions is somewhat misleading, as these networks are, in expectation, nonlinear functions of their inputs. Modifying an input example by multiplying or adding a constant will not result in a linear change in the expected output of the network. In this regard, they exhibit more nonlinearity than ReLU networks, which are at least locally linear.
The provided plots are challenging to interpret when viewed in grayscale.