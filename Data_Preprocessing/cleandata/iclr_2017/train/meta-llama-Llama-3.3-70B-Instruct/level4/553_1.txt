For over a decade, the need for near data processing has been a crucial aspect of large-scale linear learning platforms, as data loading times often surpass the actual learning time, thereby necessitating the development of frameworks like Spark. 
In contrast, deep learning typically operates with datasets that can fit within a single machine, with the primary bottleneck being the transfer of data between the CPU and GPU or between GPUs. Consequently, a method that effectively addresses this bottleneck could be highly relevant.
However, the current work remains in its preliminary stages and is confined to linear training algorithms, which limits its appeal to the ICLR audience. As such, I suggest that it would be more suitable for publication in a conference that caters to the large-scale linear machine learning community, such as ICML. The paper is well-structured and clearly written, but it would likely benefit from a comprehensive benchmarking on a large-scale linear task. Once the authors have conducted convincing simulations of deep neural network learning, they may reconsider submitting to ICLR, but it is uncertain whether the flash memory FPGA can support such computations.
The choice of MNIST for experimental purposes is somewhat perplexing, given that this task is relatively small and linear approaches are known to perform poorly on it, a fact that the authors do not even acknowledge in their results.