This paper explores the performance of various neural language models that utilize attention mechanisms to query context information from their recent history. The authors introduce a novel approach by dividing the attended vectors into distinct key, value, and prediction components, which appears to yield improved results. Additionally, they find that a simpler model that concatenates recent activation vectors achieves comparable performance to more complex attention-based models.
The experimental methodology is generally sound, but some concerns arise regarding the selection of vector dimensionality in the attention mechanism. Although the authors adapt hidden layer sizes to maintain a similar number of trainable parameters across models, this does not account for the potential benefits of higher-dimensional key/value/prediction vectors, which may perform better due to their increased capacity, regardless of their dedicated tasks. The explicit separation of these vectors may also limit the model's ability to discover and exploit overlapping information, as well as adapt to tasks with varying dimensional requirements.
While memory-augmented RNNs and attention mechanisms are not novel, their application to language modeling is relatively new, and the authors' strategy of separating key and value functionality has not been previously explored in this context. The proposed n-gram RNN, although not particularly original, serves as a useful baseline to test the ability of more complex models to capture long-term dependencies. Its competitiveness with more complicated approaches highlights the need for further investigation into the effectiveness of these methods.
The paper is generally well-written, with clear explanations, although the computation of the representation h_t was initially unclear due to the ambiguous use of terms like "hidden" and "output". The results are significant, as they demonstrate that learning long-term dependencies remains an unsolved problem. The authors provide a thorough comparison with prior work, and the success of their key/value/prediction separation in attention-based systems is noteworthy, albeit requiring further investigation to control for hyperparameter choices.
The strengths of this paper include its impressive and interesting results, good comparison with earlier work, and the introduction of the n-gram RNN as a useful baseline. However, the weaknesses include the potential weakness of the claim that the key/value/prediction separation is the primary reason for the performance increase, due to the relationship between the attention mechanism and the number of hidden units. Additionally, the model descriptions could be improved, and it would be beneficial to explore the application of attention to larger context sizes.