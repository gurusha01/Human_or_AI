This paper examines the misclassification error of discriminators, noting that although a uniform probability prior for classes is reasonable at the outset of optimization, the actual distribution diverges significantly from this prior as parameters move away from their initial values. As a result, the optimized upper bound, measured by log-loss, becomes increasingly loose.
To address this issue, the authors propose an optimization procedure that involves recalculating the bound. The paper is well-written, and while the primary observation may not be novel, it is presented in a clear and engaging manner that could make it accessible and useful to a broad audience at this conference.
It is worth noting that the framework presented has close ties to curriculum learning, a connection that could be further explored with reference to relevant work such as [1], which should be cited for its relevance. Discussing this relationship could enhance the paper's quality.
Additionally, there is a substantial body of research focused on directly optimizing task losses, as seen in [2] and [3], which includes further relevant references. These should be discussed, particularly in relation to Section 3, where the optimization of the ROC curve is addressed.
In conclusion, the material in this paper is of interest to a wide audience at ICLR, tackling an intriguing problem with a sound approach. Therefore, I recommend accepting the paper and have increased my score from 7 to 8, reflecting its merit and potential contribution to the conference.