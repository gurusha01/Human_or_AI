Overall, this paper reads like a progress report from prominent researchers in the field, offering a clear and engaging presentation of interesting observations. However, the discussion appears disjointed and fails to coalesce into a significant advancement in the study of the Neural GPU model's capabilities.
A minor observation regarding Table 1 is that the terminology surrounding RNNs could be clarified, as several columns pertain to models that are, in fact, RNNs. The use of RNNs in applications such as translation and word2vec underscores the fact that these models can be characterized by input sequence length, input size, and the dimensions of input, output, and working memories at each step.
A fundamental question regarding the model's architecture is how inputs are presented and outputs are retrieved, particularly when dealing with a large number of "filters" (e.g., 512). If inputs and outputs are 1-hot encoded and processed using the same filters as intermediate layers, it should be possible to interpret the intermediate activation functions as digits and understand the filters as implementing a reliable algorithm, such as multiplication with carry. Examining the intermediate values may provide insight into why the model fails on certain pathological cases, as identified in Table 3.
The preliminary experiment on input alignment is noteworthy in two respects: it lays the groundwork for the effective use of an attentional mechanism and suggests that the model is not currently handling general expression evaluation in the manner of a correct algorithm. 
The abstract's claims regarding improvements to the Neural GPU's memory efficiency seem exaggerated, as the modifications described on page 6 (using tf.whileloop instead of unrolling the graph and utilizing swapmemory to leverage host memory when GPU memory is insufficient) appear to be good practice but not a remarkable breakthrough in efficiency. In fact, these changes may slow down training and inference when memory fits within the GPU.
The need to try multiple random seeds to achieve convergence raises questions about the Neural GPU's computational cost and whether it is worthwhile for learning well-understood algorithms, such as parsing and evaluating S-expressions. It may be more productive to allocate the computational resources required to train one of these models (with multiple seeds) to a traditional search through program space, such as sampling Lisp programs.
The discussion of curriculum strategies employed to obtain the presented results is interesting, as it highlights the extensive efforts required to train this type of model. However, it leaves the reviewer with the impression that, despite the stated extensions to the Neural GPU model, its practical utility remains unclear.