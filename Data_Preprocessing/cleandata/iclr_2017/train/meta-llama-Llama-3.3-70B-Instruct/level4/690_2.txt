The authors have conducted thorough data collection, but the results appear somewhat unremarkable:
- The first finding indicates that various architectures and batch sizes achieve similar GPU utilization rates, with all configurations managing to exploit the GPU to a comparable extent.
- Regarding the second finding, while the linear relationship observed in Figure 9 can be used to infer a hyperbolic relationship, its relevance is limited by the fact that the latest model generations, which cluster in the upper left corner of Figure 9, do not exhibit strong linear behavior. To conclusively demonstrate asymptotic hyperbolic behavior, a stronger linear relationship should be evident as models approach the upper left corner, which is not the case.
- The third finding appears to be a straightforward consequence of the first finding, as it simply states that slower models are more efficient when faster models consume the same amount of power.
- The fourth finding is analogous to the first, suggesting that inference time is proportional to the number of operations, given that all architectures are able to fully utilize the GPU.
A more intriguing observation might be that all tested models seem to utilize the same proportion of available computational resources on the GPU, contrary to expectations that more complex models would be limited by inter-dependencies. However, since the authors used an older GPU and did not evaluate actual GPU utilization, it is not surprising that all models were able to leverage the available computational power.
To increase the significance of these findings, it would be beneficial to relate them to compression techniques or test them on real-world production networks, providing a more practical context for the results.