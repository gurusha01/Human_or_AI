SUMMARY: This manuscript presents a series of experiments assessing the efficacy of various techniques for training a dialogue agent using reinforcement learning, building upon the foundation established in Weston 2016. The authors employ a standard memory network architecture, training it on both the bAbI and WikiMovies datasets, and conduct a comprehensive set of experiments to compare the performance of different training algorithms under diverse conditions.
STRENGTHS: The experimental design is thorough and well-executed, providing valuable insights into the model's performance that complement the findings of the 2016 paper. The authors' efforts to extend the existing work are appreciated, and the results offer a more nuanced understanding of the model's behavior.
WEAKNESSES: However, this work can be seen as an extension of the earlier paper, rather than a standalone contribution, as it does not introduce significant new machine learning concepts. Furthermore, the manuscript appears to conflate the distinction between training with adaptive sampling procedures and training in interactive environments more broadly. Notably, the comparison to the static exploration policy presented in the 2016 paper is lacking, which would have provided a more comprehensive understanding of the training algorithms' performance. The modifications made to the exploration policy, although well-studied, are relatively straightforward and do not substantially enhance the novelty of the work.
A primary concern is that the work's novelty is limited, and the additional data, while welcome, might be more suitably presented in a shorter format, such as an ACL short paper or technical report. As it stands, the manuscript does not demonstrate sufficient independence to warrant an ICLR submission.
REINFORCEMENT LEARNING: 
[Update: concerns in this section have been addressed by the authors.]
The manuscript attempts to draw a sharp distinction between the reinforcement learning condition and the non-RL condition presented in the 2016 paper. However, this distinction is not as clear-cut as suggested. The RBI objective can be viewed as a special case of vanilla policy gradient with a zero baseline and off-policy samples, making the version of RBI in this paper similar to that in the 2016 paper, but with a different exploration policy. Similarly, the change in FP is essentially a modification to the sampling policy. The distinction between fixed dataset and online learning is not particularly meaningful when the fixed dataset consists of synthetic data.
It is worth noting that certain variants of the exploration policy in the 2016 paper provide a stronger training signal than the RL setting presented here. However, the results in Figures 3 and 4 suggest that the completely random initial policy achieves an average reward comparable to or better than the other exploration policies in the 2016 paper.
To improve clarity, the manuscript could express the differences from the 2016 paper directly in terms of the exploration policies, rather than attempting to categorize the previous work as non-RL. The lack of direct comparisons to the training conditions in the earlier work is a notable omission, which may be a consequence of the manuscript's stance on the RL framework.
ON-POLICY VS OFF-POLICY:
The use of vanilla policy gradient methods, which typically require additional techniques (such as importance sampling or trust region methods) to utilize off-policy samples, is an interesting aspect of this work. The fact that these methods seem to work effectively in some experiments without such modifications warrants further discussion.
OTHER NOTES:
- The claim that batch size is related to off-policy learning is somewhat misleading, as there are numerous on-policy algorithms that require collecting a large batch of transitions from the current policy before performing an update.
- The experiments on fine-tuning to human workers are a compelling aspect of this work and could be explored in greater detail, rather than being relegated to a secondary role.