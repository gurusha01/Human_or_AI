This paper presents a straightforward approach to addressing action repetitions by representing actions as tuples (a, x), where 'a' denotes the chosen action and 'x' represents the number of repetitions. The authors report notable improvements over A3C and DDPG, with significant enhancements in certain games and moderate gains in others. The concept appears intuitive, and the extensive experimental support lends credibility to the idea.
Comments:
- A significant discrepancy exists between the A3C scores reported in this paper and those in the Mnih et al. publication (table S3). It is essential to clarify the source of this discrepancy, particularly if it arises from differences in training regimes, such as fewer iterations. The authors should verify whether replicating their experiment under the same settings as Mnih et al. yields similar results.
- The finding that FiGAR achieves its best results in games where few actions repeat is intriguing. This suggests that the performance overhead of FiGAR compared to A3C might be substantial, given that A3C utilizes an action repeat of 4, resulting in fewer gradient updates. Running A3C with a lower action repeat, thereby increasing its computation cost, could potentially lead to improved performance. Nevertheless, the automatic determination of the appropriate action repeat is an interesting aspect, even if the overall implication is that actions should not be repeated too frequently.
- The notation used is somewhat problematic, as 'r' is employed to denote both rewards and elements of the repetition set R (top of page 5), which may cause confusion.
- In the equation at the bottom of page 5, since the sum is not indexed over decision steps but rather time steps, it is necessary to modify the rewards r_k to represent the sum of rewards (appropriately discounted) between those time steps.
- The section discussing DDPG is unclearly written. The concept of "concatenating" loss seems unusual, and it appears that FiGAR corresponds to a loss that resembles Q(x, mu(x)) + R log p(x) (with a separate loss for learning the critic). It seems that REINFORCE should be applied to the repetition variable x (the second term of the sum), while reparametrization should be used for the action 'a' (the first term).
- The 'namethisgame' name in the tables appears to be intentional, but its purpose is unclear.
- A potential limitation of the method is that the agent must commit to an action for a fixed number of steps, regardless of subsequent events. The authors may want to consider a scheme in which the agent decides at each time step whether to maintain its current decision or not. This modification seems relatively straightforward and could potentially enhance the FiGAR approach.