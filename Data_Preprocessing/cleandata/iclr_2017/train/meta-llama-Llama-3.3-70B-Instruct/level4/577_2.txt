This manuscript presents a methodology utilizing a linear classifier to assess the informativeness of hidden activations across various neural network layers, without influencing the neural network's training process.
The investigation is well-justified in its aim to determine the usefulness of information (or quality of representations) at each layer. The findings align with existing knowledge, including: 1) the detrimental effect of excessive random layers (Fig 5a), 2) the benefits of training (Fig 5b), 3) the faster convergence of lower layers compared to higher layers (Fig 7), and 4) the challenges of training very deep networks and the potential remedy of skip links (Fig 8).
However, several concerns arise:
1. The choice of a linear classifier as a probe lacks sufficient justification. Further theoretical analysis or intuitive explanations are necessary to clarify why intermediate features with high linear classification accuracy are deemed effective.
2. The paper falls short in providing actionable insights for designing improved networks based on the observed results. Demonstrating how to create better networks would not only validate the analysis but also enhance its practical utility.
In summary, while the paper addresses an intriguing problem, the employed technique (linear classifier as a probe) is not innovative and requires more rigorous justification. Moreover, it is crucial to illustrate how the observations can inform the design of better neural networks, thereby underscoring the value of the analysis.