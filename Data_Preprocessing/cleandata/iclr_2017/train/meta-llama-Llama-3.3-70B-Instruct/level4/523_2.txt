REVIEW SUMMARY:
The authors propose an efficient approximation of the softmax function, leveraging the uneven distribution of words and empirical timings of matrix multiplies on GPUs to cluster vocabulary optimally. They demonstrate empirical results showcasing speedups over alternative methods while maintaining acceptable accuracy compared to the full softmax.
COMMENTS:
Given the primary objective of accelerating training, it is intriguing that the comparison is limited to the flat 2-level HSM, which offers a maximum speedup of O(sqrt(V)), rather than the deeper binary-tree HSM, which can achieve a speedup of O(lgV). 
The paper is generally well-structured, clear, and concise, with minor notation issues as highlighted by other reviewers. It contributes a valuable addition to the language modeling toolkit by building upon previous works focused on optimizing vocabulary clustering for improved speed-accuracy tradeoffs. The notable finding is that the proposed clustering objective not only enhances speed, as intended, but also preserves accuracy, albeit without speculative reasoning provided by the authors. A plausible explanation could be the flat region in the timing graph (Fig 1), allowing the head group V_h to encompass a substantial portion of the most frequent vocabulary words at a constant cost, thereby reducing approximation error and mitigating the perplexity impact compared to the full softmax.
However, due to the method's close relation to the speed-optimal approach by Zweig et al. (2013), albeit without explicit GPU tailoring, a direct comparison seems necessary. If the method's performance and accuracy advantages persist, this would warrant a revised rating of 7.