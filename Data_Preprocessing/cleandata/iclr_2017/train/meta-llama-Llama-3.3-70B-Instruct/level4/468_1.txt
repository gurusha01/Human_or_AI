This manuscript presents a method for quantizing neural networks, which reduces the storage requirements for network parameters by compressing them. The authors focus on compressing the parameters of already pruned networks, targeting the non-pruned components. The issue of network compression is a significant and relevant problem for the ICLR community, as it addresses a crucial need.
However, a major limitation of this work is its lack of substantial novelty. The paper heavily relies on the findings of Han 2015, with only minor extensions to address the limitations of that work. Although the proposed approach has not been previously presented, its foundation on existing research is evident.
The paper is well-organized and easy to follow, despite its extensive reliance on Han 2015. Nevertheless, its length exceeds that of Han 2015, suggesting potential redundancy. Notably, the experiments section begins on page 12, whereas in Han 2015, it starts on page 5, indicating that some introductory material could be efficiently condensed.
The experimental results demonstrate impressive compression performance compared to Han 2015, with minimal loss of accuracy. It would be beneficial for the authors to explain the absence of a comparison with Han 2015 on ResNet in Table 1.
Several points warrant further clarification:
1) The procedure illustrated in Figure 1 is not clearly attributed to the authors or existing literature, requiring clarification on its origin.
2) In Section 4.1, the authors approximate the Hessian matrix with a diagonal matrix. It would be helpful to understand how this approximation impacts the final compression ratio and what trade-offs are involved in making this simplification.
Minor typographical errors to be addressed in a revised version:
1) Page 2, Paragraph 3, 3rd line from the end: "fined-tuned" should be corrected to "fine-tuned".
2) Page 2, last paragraph, last line: "assigned for" should be corrected to "assigned to".
3) Page 5, line 2: The same correction as above applies.
4) Page 8, Section 5, Line 3: "explore" should be corrected to "explored".