Final review: Upon reevaluation, I concur with Reviewer 2 that the experimental setup is valid, which prompted me to increase my score by one. However, I still maintain that the paper is deficient in experimental rigor and the results lack conclusiveness. As a reader, I expect a paper to either provide novel insights or demonstrate a method that yields improved performance. This paper aims to achieve the latter but falls short due to inadequate and rigorous experimentation. In essence, the paper's limitations stem from a lack of comprehensive experiments and inconclusive results, leading me to doubt the usefulness of the proposed method, which is a critical criterion for a conference-level publication.
--
The paper presents a methodology for training a policy network in conjunction with a main network to selectively utilize a subset of data during training, aiming to achieve faster convergence with reduced data requirements.
Pros:
The paper is well-structured and easy to follow, with a clear explanation of the algorithm.
Cons:
A critical issue arises in Section 2, where the validation accuracy is utilized as a feature vector for training the NDF, thereby compromising the validity of the experiments since the training process leverages data from the validation set.
Furthermore, the experimentation is limited to a single dataset. For papers claiming improved convergence rates, it is essential to demonstrate consistency across multiple datasets and network architectures. This is particularly pertinent for larger datasets, as the proposed method intends to use less training data per iteration. Thus, scalability and effectiveness should be showcased on substantial datasets like Imagenet.
As previously discussed, claims of faster convergence necessitate a comparison of learning curves with established baselines such as Adam. The comparison to plain SGD is deemed unfair, as it is rarely used in practical scenarios. This comparison is crucial regardless of the black box optimizer employed. It is conceivable that Adam, used as a black box optimizer, could perform as well as or better than the combination of Adam with NDF, highlighting the need for more comprehensive baseline comparisons.