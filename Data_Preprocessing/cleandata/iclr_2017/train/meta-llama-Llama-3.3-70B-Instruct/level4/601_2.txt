Paper Summary: 
This manuscript introduces the NewsQA dataset, a comprehensive collection of 100,000 question-answer pairs derived from over 10,000 news articles sourced from CNN. The dataset's construction involves a multi-step process comprising article filtering, question collection, answer collection, and answer validation. The dataset examples are categorized based on answer types and the reasoning required to answer questions. A comparison of human and machine performance on the NewsQA dataset is presented, alongside a comparison with the SQuAD dataset.
Paper Strengths: 
-- The creation of diverse datasets is crucial for model development, and the NewsQA dataset, sourced from news articles, offers a unique set of challenges distinct from existing datasets like SQuAD.
-- With its substantial size, the NewsQA dataset is well-suited for training data-intensive deep learning models.
-- The inclusion of questions with null answers enhances the dataset's versatility.
-- The four-stage data collection process demonstrates meticulous planning and consideration.
-- The proposed BARB model exhibits performance comparable to a published state-of-the-art model while offering significant improvements in speed.
Paper Weaknesses: 
-- The human evaluation is limited, relying on the performance of two near-native English speakers on a mere 100 examples each, which may not accurately represent the entire dataset. Furthermore, the model's performance on these 200 examples is not reported.
-- To convincingly demonstrate the relative difficulty of the NewsQA dataset compared to SQuAD, the authors should consider calculating human performance using a consistent methodology across both datasets or on sizable, representative subsets. This approach is exemplified by the VQA dataset, which adopts the SQuAD method for human performance calculation.
-- An inconsistency is observed between the 86% of questions with answers agreed upon by at least two workers (Section 3.5) and the 4.5% of questions lacking agreement after validation (Section 4.1).
-- It is unclear whether the same article is presented to multiple questioners and whether measures are taken to prevent similar questions being asked about the same article.
-- The authors' decision to retain the same hyperparameters as SQuAD raises questions about the potential impact of hyperparameter tuning using a NewsQA validation set on model accuracy.
-- The 500 examples labeled for reasoning types may be insufficient to represent the entire dataset, and the model's performance on these examples is not provided.
-- The model whose performance is depicted in Figure 1 is not specified.
-- The background and expertise of the two "students" involved in the evaluation are not detailed.
-- The test set appears to be relatively small.
-- Consideration could be given to releasing the dataset in two versions: one with all answers collected during the third stage (prior to validation) and the current version, which includes the validation step.
Preliminary Evaluation: 
The NewsQA dataset represents a large-scale machine comprehension dataset sourced from news articles, offering a diverse range of challenges that can benefit state-of-the-art models. With enhanced human evaluation, this paper has the potential to make a valuable contribution as a poster presentation.