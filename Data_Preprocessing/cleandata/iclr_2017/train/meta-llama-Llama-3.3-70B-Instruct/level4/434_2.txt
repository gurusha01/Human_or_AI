This manuscript successfully adapts batch normalization to recurrent neural networks (RNNs), a domain where it has historically struggled or underperformed. The comprehensive experiments and diverse datasets presented unequivocally demonstrate the superiority of batch normalized LSTMs over conventional LSTMs. The authors provide a broad range of examples, encompassing character-level (PTB and Text8), word-level (CNN question-answering task), and pixel-level (MNIST and pMNIST) tasks. Furthermore, the included training curves clearly illustrate the potential reductions in training time, a crucial consideration in model development.
The pMNIST experiment convincingly highlights the benefits of batch normalization in the recurrent setting, particularly in establishing long-term dependencies. I also found the insight into gradient flow, specifically the effect of unit variance on tanh derivatives, to be particularly informative. The additional illustration of this concept using the "toy task" (Figure 1b), beyond just batch normalization, was extremely helpful.
In summary, I consider this paper a valuable contribution to the application of batch normalization, providing essential information for its effective utilization in recurrent settings.