Overview:
This manuscript proposes a novel biasing term for Stochastic Gradient Descent (SGD) that theoretically and empirically yields solutions with comparable or improved generalization error. However, this comes at an additional computational cost, as it requires estimating the gradient of the biasing term through stochastic gradient Langevin dynamics, effectively introducing an inner loop to the standard SGD algorithm for each minibatch.
Pros:
- The paper provides a comprehensive review of past research, distilling key results and theorems from the last two decades to propose a promising approach for enhancing the generalizability of deep neural networks.
- The manuscript is well-written, with clear and engaging presentation of results, including an interesting discussion on the role of Hessian eigenvalues in characterizing "flat" minima.
- The mathematical arguments presented suggest that Entropy-SGD (E-SGD) has a generalization error bounded below by that of SGD, motivating further investigation in this area.
Cons and suggested points for rebuttal:
(1) The claim in the abstract that Entropy-SGD leads to improved generalization and has the potential to accelerate training appears to be inconsistent with the experimental results presented. The discussion section notes that E-SGD results in comparable generalization error to SGD but with lower cross-entropy loss, which does not clearly support the claims made in the abstract.
(2) The assertion that E-SGD can accelerate training is not convincingly supported. While vanilla SGD requires a single forward pass through all minibatches for a parameter update, E-SGD necessitates multiple forward passes due to the inner-loop SGLD iterations, potentially leading to worse computational complexity. The authors' remark on defining an epoch based on the number of parameter updates does not adequately address the increased computational cost introduced by the SGLD iterations.
(3) The magnitude of the claims made in the paper could be tempered. For instance, the introduction suggests that optimizing solely the free energy term can yield similar generalization error to SGD on the original loss function. However, the reported results indicate that this is only true for MNIST when using the free energy term alone, and a non-zero rho value is used for CIFAR-10, suggesting that the original claim may be overstated.
(4) The paper's contribution to characterizing the optimization landscape using Hessian eigenvalues and associating low generalization error with flat local extrema is valuable and well-presented. While the authors have acknowledged similarities with "Flat Minima" by Hochreiter and Schmidhuber (1997), a more detailed comparison of the two works, including their differences and potential areas for future research, would be beneficial.
(5) The assumption regarding the eigenvalues made in section 4.4 and Appendix B lacks clarity on its implications for real-world applications. Specifically, the choice of c>0 and its relation to measurable dataset characteristics is not well-explained, making it difficult to understand how this result can be applied in practice.