This manuscript proposes a novel approach to compressing CNN weights by utilizing a recently developed neural network quantization technique, which reduces network weights to ternary values. 
However, given the research group's extensive recent publications on this topic, the marginal gains presented in this work, such as a minimal percentage improvement on ImageNet, seem underwhelming. 
Furthermore, the results on AlexNet, an older network architecture, are of limited relevance at this point, as the group has already demonstrated significant compressibility of such networks in previous work. 
A more comprehensive evaluation would have been facilitated by the release of the compression code and a detailed report on the computational resources required for compression, including metrics such as flops, processing time, number of iterations, and the need for the original dataset. This information is crucial for assessing the practicality and value of the proposed compression method.