This manuscript introduces Hyperband, a novel approach to hyperparameter optimization that leverages gradient descent or other iterative schemes for model training. Building upon the successive halving and random search framework established by Jamieson and Talwalkar, the authors address the inherent tradeoff between training a smaller number of models for an extended duration versus training a larger number of models for a shorter period. The core idea involves performing multiple rounds of successive halving, initiating with the most exploratory configuration and subsequently reducing the number of experiments exponentially while allocating exponentially more resources to each. Unlike recent studies on this topic, the proposed method does not rely on specific models of the underlying learning curves, thereby minimizing assumptions about the model's nature. The results suggest that this approach can yield significant speedups, often outperforming sequential methods by several factors.
In my opinion, this paper constitutes a valuable contribution to the hyperparameter optimization literature, offering a relatively straightforward implementation that appears effective for a wide range of problems. Hyperband seems to be a natural extension of the random search methodology to scenarios involving early stopping. Its utility is likely to be most pronounced in situations where a) random search is anticipated to perform well, and b) computational resources are limited, making it essential to achieve near-optimal performance. To further substantiate the findings, I would like to see the plots in Figure 3 extended to allow other methods to converge, thereby elucidating the gap between optimal and near-optimal performance.
I have some reservations regarding the use of random2x as a baseline. While it serves to demonstrate the benefits of parallelism over sequential methods, most other methods also have parallel extensions. If random2x is included, I believe it would be beneficial to also consider SMAC2x, Spearmint2x, TPE2x, and other parallel variants. Additionally, evaluating Hyperband against these baselines with increased parallelism (e.g., 3x, 10x) would provide further insight into its performance.