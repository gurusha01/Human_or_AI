This manuscript proposes a neural network regularization technique by combining the target distribution with the model's own predictions, similar to scheduled sampling, SEARN, and DAgger, which employ a mixture of target and model distributions during training. However, the manuscript's pseudocode may be misleading, as clarified in the pre-review questions, since the targets are generated online rather than from a lagged distribution.
The approach represents an incremental improvement over label softening/smoothing, recently revisited, resulting in limited novelty. Although the author argues that co-label similarity is better preserved, it is unclear whether this is causally related to regularization. A more comprehensive evaluation would involve comparing against a fixed soft label distribution and one with gradually reduced softening/temperature as the model approaches the target distribution.
While the idea is intriguing, its usefulness is not convincingly demonstrated. The MNIST dropout baselines fall short of existing results, such as Srivastava et al (2014), which achieved 1.06% error with a 3x1024 MLP and dropout. The CIFAR10 results are also far from the current state-of-the-art, making it challenging to assess the contribution. Furthermore, the SVHN benchmark yields poor accuracy, with reported error rates exceeding those of single-net performance over the past 3-4 years.
Concerns regarding data hygiene persist, including the reporting of minimum test loss/maximum test accuracy instead of using an unbiased model selection method, such as minimum validation set error. Additionally, the potential regularization effect of early stopping on a validation set, as described in Goodfellow et al (2013), is not considered. These limitations hinder the ability to draw conclusive insights into the proposed method's effectiveness, particularly in well-tuned settings.