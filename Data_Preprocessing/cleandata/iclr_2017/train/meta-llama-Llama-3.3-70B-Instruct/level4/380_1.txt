This manuscript tackles a significant limitation of generative adversarial networks, namely their inability to evaluate held-out data. Unlike existing approaches, such as BiGANs and ALI, which employ a separate inference network, the authors propose modifying the GAN objective function to enable the optimal discriminator to serve as an energy function. This novel objective requires estimating gradients of the generated data's entropy, a challenging task that the authors address through two methods: a nearest neighbors-based approach and a variational lower bound. The experimental results demonstrate that the learned discriminator/energy function effectively approximates the log probability of the data on toy datasets and provides a reliable quality metric for held-out data on more complex datasets.
However, the paper's main weaknesses lie in the practical limitations of the proposed methods, including the scalability of the nearest neighbors approximation and the accuracy of the variational approximation, which the authors acknowledge. Furthermore, given the close relationship between entropy estimation and density estimation, it is unclear whether any practical implementation of EGANs will ultimately be equivalent to a form of approximate density estimation, a problem that GANs were initially designed to avoid. Nevertheless, the paper's mathematical rigor and elegance make it a valuable contribution to the field.
Additionally, there are some minor issues with the writing, such as the incomplete sentence at the top of page 5, which starts with "Finally, let's whose discriminative power" and lacks clarity. Moreover, the title may be misleading, as it suggests a minor improvement to an existing training method rather than the introduction of a new training framework, which is the actual contribution of the paper.