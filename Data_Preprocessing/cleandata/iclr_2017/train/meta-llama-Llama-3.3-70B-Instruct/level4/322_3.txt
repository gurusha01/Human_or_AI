This manuscript presents a novel approach to dynamically adjusting the number of units in a network during training, employing a straightforward yet effective method. By introducing or removing units with zero input or output weights and utilizing a group sparsity norm for regularization, the authors encourage unit weights to approach zero. The primary theoretical contribution lies in demonstrating that, with suitable regularization, the loss is minimized by a network with a finite number of units. Although this result does not guarantee optimal performance on the training data, preliminary experiments suggest that over- or under-fitting may not occur.
A potential benefit of this approach is the potential to alleviate the burden of hyperparameter tuning, as the network can adaptively determine the required number of units. However, a drawback is that the proposed method does not entirely resolve the hyperparameter tuning issue, as several implicit hyperparameters still control the emergence of units, including the frequency of unit addition and the rate of weight decay. It is unclear whether tuning these hyperparameters will be more or less challenging than in conventional approaches. While the authors do not claim to have simplified the training process, it is somewhat disappointing that this does not appear to be the case.
The authors highlight the ability to train networks with fewer units, achieving comparable performance to parametrically trained networks. This is significant, as smaller networks can reduce runtime, power consumption, and memory footprint, particularly on mobile devices. Nevertheless, the authors do not experimentally compare their approach to existing methods for reducing network size, such as pruning trained networks, making it uncertain whether their approach is competitive with current state-of-the-art methods.
A further potential drawback of the proposed approach is that the same hyperparameters control both the number of units and the training time, potentially leading to slower training times compared to parametric approaches with fixed hyperparameters. Although many parametric approaches require time-consuming grid searches to select hyperparameters, experience with similar problems can often facilitate hyperparameter selection. The authors do not discuss how this issue will scale to larger networks, raising concerns about the practicality of their approach for large-scale networks due to potentially slow training times.
The experiments presented are helpful and encouraging but not exhaustive or entirely convincing. More comprehensive experiments on larger problems would be necessary to demonstrate the practicality and widespread applicability of this approach. 
In summary, this paper is well-written, interesting, and presents a potentially useful concept. The overarching vision of developing networks that can adapt and grow through lifelong learning is inspiring, and this type of research may be essential to realizing such a vision. However, the current results remain somewhat speculative, and further investigation is needed to fully establish the efficacy of this approach.