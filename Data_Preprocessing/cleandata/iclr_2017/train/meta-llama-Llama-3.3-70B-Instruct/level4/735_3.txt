This manuscript proposes a methodology for non-linear kernel dimensionality reduction, incorporating a trace norm regularizer within the feature space. The authors suggest an iterative minimization approach to achieve a local optimum of a relaxed formulation of the problem.
However, the paper is marred by errors and an unconvincing experimental evaluation. The comparisons are limited to outdated techniques and simplistic datasets, which fails to demonstrate the robustness and efficacy of the proposed approach, particularly in the presence of complex noise and outliers, as initially motivated.
The claim of achieving state-of-the-art performance is questionable, given that the oil dataset is not a standard benchmark and the comparisons are made against antiquated methods. A more comprehensive experimental evaluation should be conducted to assess the robustness of the approach.
Furthermore, the authors neglect to address the out-of-sample problem, a critical issue inherent to kernel-based methods when compared to Latent Variable Models (LVMs), which should be explicitly addressed in this context.
Several errors are present in the manuscript:
- The assertion in the final paragraph of Section 1 that the paper presents a closed-form solution to robust KPCA is incorrect. The proposed approach involves iterative solutions of closed-form updates and Levenberg-Marquardt optimization, which does not constitute a closed-form solution.
- The claim that the proposed approach can be easily generalized to incorporate other cost functions is also misleading. In general, such generalizations would require solving a more complex optimization problem, potentially without closed-form updates.
- The third paragraph of Section 2 incorrectly states that the paper introduces a novel energy minimization framework for solving problems of the form of equation (2). However, the authors ultimately solve a different problem that has undergone at least two relaxations, making it unclear how the solution to this relaxed problem relates to the original problem.
- The statement regarding Geiger et al.'s work is inaccurate. Their approach discovers the dimensionality of the latent space through a regularizer that encourages sparse singular values, rather than operating on a latent space with pre-defined dimensionality.
The authors' comment on LVMs, such as GPLVM, implying that the latent space is learned with clean training data, is misleading. Different noise models can be integrated within the GP framework, and the proposed approach assumes Gaussian noise, which is also a trivial case for GP-based LVMs.
The terminology used in the paper, such as "pre-training" and the notion that certain techniques lack a training phase, is unclear. For instance, KPCA is trained through a closed-form update, which still constitutes a training process.