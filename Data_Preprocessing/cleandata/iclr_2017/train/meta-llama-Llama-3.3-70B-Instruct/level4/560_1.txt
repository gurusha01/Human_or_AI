This manuscript examines the effect of orthogonal weight matrices on the learning dynamics of Recurrent Neural Networks (RNNs). The authors propose several novel optimization formulations that impose orthogonality on the recurrent weight matrix to varying degrees, yielding interesting insights. The experimental results reveal that enforcing exact orthogonality does not necessarily enhance learning, whereas implementing soft orthogonality or initializing with orthogonal weights can significantly improve learning outcomes. Although some of the proposed optimization methods are computationally expensive due to the requirement of matrix inversion, orthogonal initialization and certain soft orthogonality constraints are relatively efficient and may be applicable in practical scenarios.
The experiments are conducted to a high standard, providing valuable insights, and the writing is clear and concise. However, the experimental results are based on a fixed learning rate for different regularization strengths, which may impact learning speed. It would be beneficial to optimize the learning rate for each margin separately to investigate how soft orthogonality affects the stability of the learning process. For instance, Figure 5 demonstrates that a sigmoid function improves stability, but reducing the learning rate for the non-sigmoid Gaussian prior RNN might also yield well-behaved learning for weightings less than 1.
Figure 4 shows that singular values converge to approximately 1.05 rather than 1. It would be interesting to explore whether initializing with orthogonal matrices multiplied by 1.05 offers any noticeable advantages over standard orthogonal matrices, particularly on the T=10K copy task. The observation that larger margins and models without sigmoidal constraints on the spectrum perform well when initialized orthogonally suggests that deviation from orthogonality is not a significant issue in this task. This finding is consistent with the analysis presented in Saxe et al. (2013), which highlights the role of orthogonality as a preconditioner for optimization.
The results of this study contribute to the ongoing debate about the usefulness of orthogonality as an initialization technique, as proposed by Saxe et al. (2013), versus its role as a regularizer, as suggested by Arjovsky et al. (2015) and Henaff et al. (2015). The experiments indicate that initializing with orthogonal weights is sufficient to achieve an optimization speed advantage, while excessive regularization can harm performance. This distinction is also evident in Figure 2, which shows that models with no margin or a margin of 1 or 0.1 perform similarly in terms of training loss on MNIST, but a margin of 0.1 yields the best accuracy. This suggests that large or nonexistent margins enable fast optimization of the training loss, but among models with similar training loss, those with more nearly orthogonal weights perform better.
Overall, this paper presents a range of techniques and insights that are likely to be valuable in training RNNs. The authors' findings highlight the importance of orthogonal initialization and soft orthogonality constraints, while also underscoring the need for further exploration of the interplay between orthogonality, regularization, and optimization speed. It would be beneficial for the authors to explicitly discuss the initialization versus regularization dimension in the text to provide a clearer understanding of the contributions and implications of their work.