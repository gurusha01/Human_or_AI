The manuscript presents a neural network-based approach to image compression-decompression using an auto-encoder framework. Although the concept is intriguing and well-justified, the results indicate that it achieves comparable compression rates to JPEG-2000 in practice.
As the authors point out, the automatic learning of the compression scheme has inherent value, particularly for its potential application to signals with limited domain knowledge, beyond natural image compression. However, in its current form, I believe the paper is not suitable for publication due to the following concerns:
1. Given that the learned encoder performs similarly to, but not significantly better than, JPEG 2000, the paper should focus on comparing and contrasting the learned encoder with JPEG 2000. Specifically, it would be beneficial to investigate whether the encoder learns similar or distinct filters, and to analyze its performance on various textures, including showcasing the top and bottom 10 patches at different bit rates.
2. It is essential to demonstrate that the observed benefits stem from an improved coding scheme rather than just a better decoder, as initially suggested in my pre-review inquiry. A comparative analysis of a decoder trained on JPEG-2000 codes, as well as encoded random projections, would help clarify this aspect.
3. The fact that the proposed method performs similarly to or worse than JPEG-2000 undermines the justification for employing a deep auto-encoder. Since JPEG-2000 relies on a wavelet transform, which can be approximated using a simple sparse dictionary algorithm like K-SVD, I believe it is necessary for the method to either outperform JPEG-2000 or provide comparisons to, or at least discuss, a well-designed traditional or generative model-based baseline.