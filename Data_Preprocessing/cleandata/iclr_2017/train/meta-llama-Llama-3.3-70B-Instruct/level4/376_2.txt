CONTRIBUTIONS
This study employs large-scale experiments to assess the capacity and trainability of various RNN architectures, yielding insights into their information storage capabilities. The results indicate that RNNs can store between three and six bits of information per parameter across all architectures, with ungated RNNs exhibiting the highest per-parameter capacity. Furthermore, all architectures can store approximately one floating-point number per hidden unit. The trainability experiments reveal that ungated architectures, such as RNN and IRNN, are significantly more challenging to train than gated architectures, including GRU, LSTM, UGRNN, and +RNN. Notably, the paper introduces two novel RNN architectures, UGRNN and +RNN, with experiments suggesting that UGRNN has comparable per-parameter capacity to ungated RNN but is easier to train, and deep +RNN models exhibit improved trainability compared to existing architectures.
CLARITY
The paper is well-structured and easy to comprehend, presenting complex ideas in a clear and concise manner.
NOVELTY
To the best of my knowledge, this paper is the first to empirically quantify the number of bits of information that can be stored per learnable parameter in RNNs. The experimental approach, which involves measuring network capacity by identifying the dataset size and hyperparameters that maximize mutual information, is a notable innovation. Although the proposed UGRNN bears some resemblance to the minimal gated unit introduced by Zhou et al., it is distinct and offers a unique contribution to the field.
SIGNIFICANCE
My assessment of the paper's significance is mixed. While the experiments are intriguing, they do not reveal particularly surprising or unexpected findings about recurrent networks. The results may not substantially alter my understanding of RNNs or influence my future work. However, it is valuable to have intuitive results about RNNs confirmed through rigorous experiments, especially given the limited computational resources available to many researchers. The capacity experiments, which focus on modeling random data, may not be directly applicable to real-world tasks, where RNNs are typically used to model non-random data. The text8 experiments suggest that architectures may not vary significantly in their ability to model real-world data, despite differences in their capacity to model random data. Furthermore, the experimental results do not provide sufficient evidence to establish the significance of the proposed UGRNN and +RNN architectures, as their performance on real-world tasks is not substantially better than that of existing architectures like GRU or LSTM.
SUMMARY
While the experiments could have yielded more surprising insights, there is still value in experimentally verifying intuitive results. The proposed UGRNN and +RNN architectures show promise on synthetic tasks, but more convincing performance on real-world tasks is needed. Overall, the paper's contributions outweigh its limitations, and its ideas are likely to be of value to the community.
PROS
- The paper is the first to explicitly measure the bits per parameter that RNNs can store, providing a unique contribution to the field.
- The paper experimentally confirms several intuitive ideas about RNNs, including their ability to store approximately one number per hidden unit, the importance of comparing architectures based on parameter count, and the relative ease of training gated architectures.
- The study demonstrates that, with careful hyperparameter tuning, all RNN architectures can perform similarly on text8 language modeling tasks.
CONS
- The experiments do not reveal particularly surprising or unexpected findings about RNNs.
- The motivation behind the UGRNN and +RNN architectures is not well-established, and their utility is not convincingly demonstrated.
- The paper's focus on synthetic tasks, rather than real-world applications, may limit the significance of its findings.