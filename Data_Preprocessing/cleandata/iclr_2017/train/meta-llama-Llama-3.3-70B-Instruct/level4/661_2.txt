This manuscript introduces the Neural Graph Machine, which incorporates graph regularization into neural network hidden representations to enhance learning and account for graph structure. However, upon examination, the proposed model bears a strong resemblance to the work of Weston et al. (2012).
As clarified by the authors in their responses, several novel aspects are presented, including:
1. Demonstrating the effectiveness of graph-augmented training across various network architectures, such as feedforward, convolutional, and recurrent neural networks, and its applicability to diverse problems.
2. Showing that graph-augmented training enables the achievement of comparable performance with fewer layers, for instance, a 3-layer CNN with graph regularization matching the performance of a 9-layer CNN.
3. Illustrating the versatility of graph-augmented training across different types of graphs.
Nevertheless, these contributions appear to be extensions and applications of the graph-augmented training concept, rather than a fundamentally new model. In light of this, it seems more accurate to characterize this work as an empirical investigation of the Weston et al. (2012) model applied to various domains, rather than presenting it as a novel model with a distinct name, Neural Graph Machine. A more transparent approach would be to clearly position this work as an exploratory study of the existing model's capabilities, rather than implying the introduction of a new framework.