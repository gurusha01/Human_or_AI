The authors propose a method to learn subspaces across multiple views, ensuring that the neighborhoods of data points exhibit similarity across all views. This similarity is quantified by comparing the distributions of neighbors in pairs of views, which is motivated by its relevance to information retrieval tasks.
The concept of preserving neighborhood relationships across views for retrieval tasks is intriguing, and the ability to learn spaces with varying dimensionalities for different views is a notable advantage. However, the empirical validation provided appears to be preliminary and limited.
The paper is a revised version of the authors' ICLR 2016 submission, and while the revisions are appreciated, further work is necessary to make the paper suitable for publication. In its current state, it may be more appropriate for the workshop track.
A significant concern is that the experiments are conducted on extremely small datasets, such as 2000 examples in each of the train and test sets for the MNIST task, and do not involve real-world tasks. Although the authors acknowledge that efficiency is not their primary focus, it is unclear whether any conclusions drawn from these small-scale experiments can be generalized to more realistic and larger datasets. Given the substantial body of work on multi-view subspace learning and its applications to real tasks, it is challenging to consider this contribution significant without demonstrating its applicability in more realistic settings.
On a related note, the authors' claim that no other information retrieval-based approaches exist is somewhat exaggerated. For instance, the contrastive loss proposed by Hermann and Blunsom in their 2014 ACL paper "Multilingual models for compositional distributed semantics" is related to information retrieval and could be a relevant comparison.
The presentation of the paper could be improved, as there are several vague and confusing points. Examples include:
- The term "dependency" is used colloquially throughout the paper, which can be confusing in a technical context.
- Phrases such as "an information retrieval task of the analyst" are vague and grammatically unclear.
- The definition of "the probability that an analyst who inspected item i will next pick j for inspection" is not well-defined.
- The discussion of KL divergence and its relationship to the "cost of misses" could be more precise or potentially omitted, as KL divergence is already well-motivated in this context.
- The usage of C_{Penalty} (7) in relation to C (6) is unclear, and it would be helpful to clarify whether it is added to or used instead of C (6).
- The statement that CCA "iteratively finds component pairs" is misleading, as CCA can be defined as an iterative operation but is typically solved all at once.
- The method used for PCA "between Xi^1 and Xi^2" is not specified.
- The "nonlinear dimensionality algorithm" applied is not clearly identified.
- The task described in the context of image patches and stock prices is unclear.
Additional minor comments and corrections include:
- The font size in the figures is too small.
- "difference measures" should be corrected to "different measures".
- The phrase "...since, hence any two..." is grammatically incorrect.
- The phrase "between feature-based views and views external neighborhoods" is unclear.
- Several typos and minor errors are present throughout the paper.