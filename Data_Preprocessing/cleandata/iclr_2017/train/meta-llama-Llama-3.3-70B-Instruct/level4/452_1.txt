This manuscript demonstrates the effectiveness of enhancing deep reinforcement learning (RL) algorithms by enabling them to determine not only the action to take but also the frequency of its repetition, resulting in improved performance across multiple domains. The evaluation undertaken is comprehensive, illustrating the efficacy of this straightforward concept in both discrete and continuous action spaces.
Several points warrant further consideration:
- The presentation of Table 1 in a histogram format could enhance its interpretability.
- Conversely, representing Figure 3 in a tabular form might facilitate easier comprehension.
- The methodology behind the selection of the subset of Atari games could be elucidated.
- While the evaluation on Atari games, particularly those requiring extensive exploration like Freeway and Seaquest, shows promising improvements over A3C, a comprehensive assessment across all 57 games would be beneficial. This would enable a standardized comparison of overall performance using mean and median scores.
- A direct comparison with the STRAW model proposed by Vezhnevets et al., which addresses similar challenges as FiGAR, would provide valuable insights.
- The current implementation of FiGAR involves discarding frames between action decisions, potentially overlooking a tradeoff between action repetition and information loss. Exploring the separation of these effects, possibly by training a model that processes intermediate frames, could yield interesting results.
In summary, this work presents a simple yet effective extension to deep RL algorithms, likely to be adopted by many in the field.
Following the rebuttal and revision of the paper, I am revising my score to 8.