This paper presents an investigation into semi-supervised reinforcement learning, where the goal is to differentiate between labelled Markov Decision Processes (MDPs) that yield rewards and unlabelled MDPs lacking a reward signal. The approach is straightforward, focusing on concurrent learning of a policy utilizing the REINFORCE algorithm with entropy regularization, and a reward model that provides feedback on unlabelled MDPs, akin to inverse reinforcement learning. Experimental evaluations across various continuous domains yield noteworthy outcomes.
The paper's clarity and readability are commendable, rooted in a simple yet effective concept of simultaneous policy and reward model learning, resulting in an algorithm with intriguing characteristics. Although the proposed concept may seem intuitive, the authors are pioneering in testing such a model. While the experiments demonstrate promise, incorporating a mix of continuous and discrete problems could enhance their robustness, nonetheless, they remain convincing.