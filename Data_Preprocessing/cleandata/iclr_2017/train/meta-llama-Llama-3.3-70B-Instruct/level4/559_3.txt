Paper Summary 
This paper proposes a simplification of matching networks by utilizing a single prototype per class, calculated as the average of the embeddings of training class samples, and presents empirical comparisons with matching networks.
 Review 
The paper is well-written and effectively motivates its contributions. The objective of simplifying earlier work on matching networks is commendable, as it aims to advance the field of metric learning. However, it is unclear whether the proposed approach yields superior results compared to matching networks. While the concept of learning embeddings to optimize nearest neighbor classification is not new, the idea of averaging prototypes is intriguing, building upon the non-linear extension of Mensink et al's 2013 work. To strengthen the paper, I recommend enhancing the discussion of related work and consolidating the results section to clearly differentiate between the methods that are outperformed and those that are not.
The related work section could be expanded to include research on learning distance metrics for optimizing nearest neighbor classification, such as Weinberger et al's 2005 work and subsequent studies. Additionally, extensions of this concept using neural networks, like Min et al's 2009 work, pursue similar goals and should be considered. The paper cites siamese networks with pairwise supervision, but it would be beneficial to explore approaches with different learning objectives, such as learning to rank for web search with triplet supervision or global ranking losses. A suitable starting point for this could be Chris Burges' 2010 tutorial.
I have concerns that the reported results may not accurately represent the state of the art for all tasks. While the results on Omniglot are positive, it would be informative to include the superior results of matching networks on miniImageNet with fine-tuning and full contextual embeddings. Omitting this information could be misleading. Furthermore, on Cub 200, I believe the state-of-the-art performance is 50.1% using features from GoogLeNet, as reported by Akata et al in 2015. I would appreciate clarification on this point.
In conclusion, the paper has the potential for significant improvement, particularly in its discussion of related work and the presentation of prior empirical results, which should be more comprehensive and balanced.
 References 
Large Margin Nearest Neighbors. Weinberger et al, 2005
From RankNet to LambdaRank to LambdaMART: An Overview, Chris J.C. Burges, June 23, 2010
A Deep Non-linear Feature Mapping for Large-Margin kNN Classification, Min et al, 09