This manuscript proposes a method for transferring skills between tasks in a control environment, trained using reinforcement learning, by imposing a penalty (L2) to ensure similarity between the embeddings learned for two distinct tasks. The experiments, conducted in MuJoCo, involve two sets of tests: one using the state of joints/links (sections 5.2 and 5.3) and another using pixel data (section 5.4), demonstrating successful transfer between arms with varying link numbers and from a torque-driven to a tendon-driven arm.
A notable limitation of this work is the assumption that time alignment is straightforward due to the episodic nature and shared domain of the tasks. However, time alignment is a critical aspect of domain adaptation and transfer, which could be addressed through techniques like subsampling, dynamic time warping, or learning a matching function, such as a neural network.
In general, the comparison to Canonical Correlation Analysis (CCA) as a baseline is relevant, but given the experimental nature of the paper, an additional baseline involving random projections for the embedding functions "f" and "g" could further validate the findings by showing that poor performance in the "no transfer" model version is indeed due to over-specialization of these embeddings. It's also worth noting that the problem of learning invariant feature spaces is related to metric learning, as seen in works like [Xing et al. 2002]. Furthermore, the paper could benefit from drawing parallels with multi-task learning in machine learning, and in the context of knowledge transfer (section 4.1.1), annealing Î± might be a worthwhile consideration.
The experimental section feels somewhat rushed. Notably, the baseline performance being consistently zero (indicating no transfer) is not very informative, suggesting the need for a larger sample budget. Additionally, the absence of "CCA" and "direct mapping" results in Figure 7.b is puzzling. Another experimental concern is whether and how the authors controlled for the difference in training iterations for the embeddings when transfer was involved.
Overall, the exploration of transfer in reinforcement learning is a welcome contribution. While the experiments are intriguing and warrant publication, the paper could benefit from a more comprehensive approach.