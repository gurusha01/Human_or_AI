This manuscript examines the error surface of deep rectifier networks, providing specific instances where local minima occur. Experimental results demonstrate that training can become trapped in apparent local minima due to various factors, including dataset characteristics and initialization methods. The paper offers valuable insights and examples of how training can go awry, developing useful intuitions about the optimization process.
Although the constructed examples are artificial, they retain theoretical significance as simple illustrations of potential issues. The broader theoretical context of the paper, however, appears to target a misconception. The statement "the underlying easiness of optimizing deep networks... is tightly connected to the intrinsic characteristics of the data these models are run on" is not a new perspective, as prior works, such as Choromanska et al., have already considered the role of data characteristics in optimization. The loss function is inherently tied to both the dataset and model parameters, making it impossible to separate the error surface from dataset properties. The emerging structure of the error surface is necessarily related to the dataset and model parameters, contradicting the idea of "emerging structures due to high dimensional spaces" that are independent of these factors.
A primary concern with this paper is that it challenges a strawman argument: replica methods describe average behavior for infinite systems, so it is expected that specific finite-sized systems may exhibit poor optimization landscapes. The paper's surprise that training can be compromised by poor initialization is unwarranted, as initialization is known to be critical even in linear networks, with saddle points potentially slowing learning significantly (e.g., Saxe et al., 2014).
The proof of Proposition 5 appears to contain an error. Assuming cdfb(0) = 0 and cdfW(0) = 1/2, the probability of learning failure increases with the number of hidden units, contrary to the expected behavior. A corrected expression for the probability of failure would be p(fails) >= 1 - [1 - p(w<0)^h^2]^{k-1}, which suggests that the limit as k approaches infinity depends on the scaling of h with k, undermining the claim that "one does not have a globally good behaviour of learning regardless of the model size."
The paper also fails to adequately distinguish between local minima and saddle points. The results in Section 3.1, which claim to demonstrate training becoming stuck in a local minimum, are based on a fixed training budget and may actually reflect a saddle point. Furthermore, different activation functions, such as sigmoid or soft rectifier nonlinearities, may not suffer from the same issues as rectifiers. For instance, the XOR problem with two hidden nodes was thought to have local minima, but it was later shown that there are none (e.g., L. Hamey, 1995).
If the goal is to demonstrate that training does not converge for specific finite problems, simpler counterexamples can be constructed. For example, setting all hidden unit weights to zero would suffice. The authors' response to prereview questions implies that the "complete characterization" of the error surface is universally valid, but this is not the case, as even deep linear networks can be sensitive to initialization.
The explanation for Figure 2 seems counterintuitive, as simply scaling the input would not change the regions where each ReLU activates if the weight matrices are initialized with zero biases. A more plausible explanation is that the weaker scaling has not been compensated by the learning algorithm, which would converge if run for a longer duration. The response notes that training has been conducted for an order of magnitude longer than required for the unscaled input to converge, but the scaling factor is five orders of magnitude, and training does converge without issue for scaling up to four orders of magnitude. The use of Adam as the optimization algorithm may also depend on implementation details, such as the epsilon factor used to protect against division by zero.
Despite containing interesting results, this paper raises several technical concerns that need to be addressed.