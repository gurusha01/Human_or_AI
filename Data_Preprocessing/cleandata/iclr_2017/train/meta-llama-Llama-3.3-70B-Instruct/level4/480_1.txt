While a more concise title than the one I provided earlier would be ideal, the current title could be improved to better reflect the paper's content, with a potential alternative title striking a balance between the two. 
The proposed approach is intriguing, as it incorporates three key techniques - auxiliary tasks, skip and diagonal connections, and internal labels for data type - that are well-founded and logical.
However, some results were challenging to comprehend and interpret. The additional explanations provided in the discussion, such as those addressing my previous questions about Figures 4 and 5, were helpful. To enhance clarity, the paper could benefit from incorporating more such explanations.
A brief mention of the relationship between "diagonal" connections and other emerging terms, like skip connections, might be worthwhile. The term "skip" appears to be universally applicable, whereas "diagonal" is layout-dependent.
Regarding the comment on "leading to less over-segmentation of action bouts" in the discussion and Section 5.1, I would appreciate a more detailed explanation in the paper. My understanding of "per-bout" as "per-action event" may be incorrect, and given the audience, explicitly defining such terms would be beneficial.
The distinction between fly behaviors lasting minutes versus milliseconds is interesting, and I am curious about how classification accuracy relates to the time scale of the behavior. Investigating whether most errors occur with long-term behaviors could provide valuable insights, although it may only tell part of the story.
In response to the comment on scaling to human behavior, I agree that adding convolutional layers above the sensory input is a logical approach, but the gap between theoretical and practical effectiveness should not be overlooked. The presented system has been demonstrated to handle spatiotemporal trajectories, and claims should be tailored to this capability.
Depending on future revisions, I would consider adjusting my rating to a 7.
In Figure 4b, BESNet and BENet exhibit comparable performance, with BENet performing better in both filtered and unfiltered per-bout scores. Do the authors have any insights into this phenomenon, and could it be related to the minimal difference in log-likelihoods between RNN and BESNet in Figure 5b?