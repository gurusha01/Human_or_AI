This work proposes an augmentation of traditional gradient descent algorithms with a "Data Filter" that functions as a curriculum teacher, selectively choosing which examples the target network should learn from to achieve optimal performance. The filter is learned concurrently with the target network and is trained using Reinforcement Learning (RL) algorithms, which receive rewards based on the training state with respect to a pseudo-validation set (e.g., similar to Adagrad (Duchi et al., 2011)).
A stylistic suggestion is to adopt the more common citation style of "(Author, year)" instead of "Author (year)" when the author is not directly referenced in the sentence. For instance, "and its variants such as Adagrad (Duchi et al., 2011)" should be used consistently throughout the paper.
The paragraph discussing the dynamic determination of training instances after "seeing" the mini-batch Dt of M training instances requires clarification. Specifically, it should be explicitly stated that the forward-pass is performed first, followed by the computation of features, and then the decision on which examples to use for training and which to filter.
Several choices in this work are unclear:
- The decision to update the REINFORCE policy only at the end of the episode (Algorithm 2), while training the actor-critic at each step (Algorithm 3), seems inconsistent. Although REINFORCE is known to have high variance, this does not inherently mean it cannot be trained at each step, unless supported by experimental evidence that should be included in the paper.
- It is also unclear why REINFORCE is not trained with the same reward as the Actor-Critic model, and vice versa. The limitation of REINFORCE requiring the episode to be over is mentioned, but given the i.i.d. nature of the data, the episode duration could be flexibly defined, ranging from a single training step to the entire multi-epoch training process.
The experimental setting raises several concerns:
- Figure 2 appears to be the result of a single experiment per setup, potentially starting from different initial weights. Without proper statistical analysis, it is challenging to determine whether the results are significant or due to chance, which is a serious concern.
- The omission of state-of-the-art optimization methods like Adam and RMSProp from the experiments is surprising.
- The learning rates for the RL component and how they adapt to the supervised learning (SL) part are not clearly specified, and it is unclear if this was experimentally evaluated.
- The environment, namely the target network being trained, is non-stationary. Analyzing how the policy changes over time would be insightful. Figure 3 could result from either policy adaptation or a fixed policy with changing features, which might indicate a failure of the policy to adapt.
- The non-stationary nature of the environment and its impact on optimization, given the changing distribution of features as the target network progresses, is not adequately addressed in the paper.
- The selection of the "pseudo-validation" data targeted by the policy is unclear. It should be a subset of the training data, but the algorithms suggest that the same data is used for both policy and network training, leading to confusion.
In summary, while the idea presented is novel and interesting, and the paper is generally well-written, the methodology has several flaws. Clearer explanations and additional experiments or justifications for the experimental choices are necessary to strengthen the paper. Unless the authors can address these concerns, it might be beneficial to conduct more experiments and submit a more comprehensive paper rather than presenting this potentially powerful idea with weaker results.