This paper introduces Generative Adversarial Parallelization (GAP), a method for training N Generative Adversarial Networks (GANs) in parallel, where the assignments between the N generators and N discriminators are randomly shuffled every few epochs, forcing each generator to compete with multiple discriminators. The authors argue that this randomization mitigates the "mode collapsing behaviour" commonly observed in GANs.
I have three main concerns regarding this submission:
1) The proposed approach of selecting the best generator using the GAM metric after training the N GANs for a sufficient duration is problematic for two reasons. Firstly, a single GAN is unlikely to capture the full complexity of the data being modeled, as a single generator with limited capacity may either accurately describe a single mode or poorly describe multiple modes. Secondly, the GAM metric relies on discriminator scores, which can be flawed (e.g., focusing on artifacts), and mode collapsing may not always be undesirable. Instead, I suggest combining all generators into a mixture, which would require determining mixture weights, potentially using rejection sampling based on discriminator scores.
2) The authors should provide a theoretical or conceptual comparison with dropout, as the two methods share similarities. In GAP, each generator competes against all N discriminators, but with N-1 discriminators "dropped" at each epoch. Similar to dropout, which retains all neurons after training, effectively approximating a large ensemble of neural networks, GAP could be viewed as a form of ensemble method.
3) The qualitative results are unconvincing, with most figures only showcasing GAP results. It would be beneficial to include baseline samples for comparison, as the GAN and LAPGAN papers have presented similar samples. Furthermore, Figures 3 and 4 are not persuasive, with the generator in Figure 3 likely being under-parameterized.
As a minor comment, I recommend removing Figure 2, as it may be copyrighted, occupies significant space, and does not substantially contribute to the explanation. Additionally, the indices (i_t) in Algorithm 1 are undefined.
Overall, this paper presents interesting ideas but requires further development in terms of conceptual framework and experimental evaluation.