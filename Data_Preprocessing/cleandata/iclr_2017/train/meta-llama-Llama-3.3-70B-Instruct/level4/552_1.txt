My primary concern with this research is that it relies on a popular, yet unproven, hypothesis that the presence of gradients is sufficient to resolve long-term dependency issues. The conventional approach involves imposing orthogonal matrices, which, in the absence of nonlinearity, yields unitary Jacobians, thereby preventing vanishing or exploding gradients. However, this hypothesis is assumed to be true without empirical evidence to support its validity, particularly in real-world scenarios, rather than just synthetic data.
I have several reservations about this line of thinking: a) the representational power of the model is compromised when restricted to orthogonal matrices, limiting its ability to represent complex functions, such as those with intricate attractors, which can only be achieved with eigenvalues greater than 1. Moreover, this approach struggles to handle noise, as it attempts to preserve all input details, making it challenging to filter out irrelevant information. Ideally, the model should preserve only the necessary information for the task at hand, given its limited capacity. Unfortunately, the current focus on preserving gradients overlooks these potential side effects.
To address this, I would like to see a study demonstrating the benefits of Jacobians with eigenvalues of 1 in realistic scenarios, using complex datasets, to provide a more comprehensive understanding of this approach.