The proposed approach is straightforward yet sophisticated, leveraging the proven effectiveness of gradient-based optimization for deep non-linear function approximators and integrating it with established linear many-view CCA methods. A significant contribution of this work is the accurate derivation of gradients with respect to the non-linear encoding networks, which project multiple views into a shared space. This derivation appears to be correct, and the overall methodology seems promising, with potential applications to various similarly structured problems.
The paper is well-structured and clearly written, although a detailed description of the complete algorithm, including the update process for the joint embeddings G and U, would enhance its clarity. As someone without prior experience in CCA-style many-view techniques, it is challenging to fully assess the practical and empirical advancements presented. However, the experiments appear convincing, albeit mostly conducted on smaller and medium-sized datasets.
Some specific observations and suggestions include:
The color scheme or x-axis sign in Figure 3b seems inconsistent with Figure 4. 
Including a continuous, rainbow-colored representation for Figures 2, 3, and 4 could facilitate the identification of neighboring datapoints. More importantly, visualizing the average reconstruction error between individual network outputs and the learned representation during training would be beneficial. The mismatch between different views on a validation or test set could serve as a useful cross-validation metric. Given the method's sensitivity to regularization and hyperparameter selection due to its increased number of parameters compared to GCCA, and the use of different regularization parameters for different views, it would be valuable to identify a clear metric for optimizing these parameters.