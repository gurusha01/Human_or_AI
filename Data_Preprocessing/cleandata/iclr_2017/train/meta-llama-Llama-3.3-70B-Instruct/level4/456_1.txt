This study presents a novel regularization technique for learning domain-invariant representations using neural networks, which seeks to align the higher-order central moments of hidden activations between source and target domains. The authors evaluate their approach against MMD and two state-of-the-art domain adaptation algorithms on the Amazon review and office datasets, demonstrating comparable results.
The proposed concept is intuitive and straightforward, with empirical evidence suggesting its effectiveness. However, a significant limitation of this method is its assumption that hidden activations are independently distributed, which may not hold true, particularly for convolutional layers where neighboring activations exhibit dependence. This might explain why the authors initiate adaptation from the output of dense layers for image datasets. It would be beneficial to know if the authors have explored starting adaptation from lower levels and if they have strategies to relax this assumption. In such cases, MMD might have an advantage due to its lack of reliance on this assumption.
The visual representation in Figure 3 does not appear to strongly corroborate the performance enhancement reported in Table 2. The only class that seems to show the new regularization bringing the source and target domains closer is the mouse class, as highlighted by the authors. It is unclear whether the observed performance improvement is primarily attributed to this single class, warranting further clarification.