This paper introduces a novel learning framework, termed "compositional kernel machines" (CKM), which integrates kernel methods and sum-product networks (SPN). The CKM approach initially defines leaf kernels for elements of query and training examples, followed by recursive kernel definition, analogous to SPN. The authors demonstrate that CKM evaluation can be efficiently performed using techniques similar to those employed in SPN.
The concept presented in this paper is intriguing, as it explores the uncharted territory of combining kernel methods and deep networks, specifically SPN. Instance-based learning methods, such as SVM with kernels, have been successful in the past but have been largely superseded by deep learning methods like convnets in recent years.
However, despite the interesting idea, the paper is clearly in its preliminary stages. In its current form, the proposed CKM framework does not appear to offer any significant advantages over convnets. Several concerns need to be addressed:
1. The claim that CKM is faster to learn than convnet is unclear, as both methods utilize gradient descent during learning. Furthermore, the inference time of convnet is solely dependent on its network structure, whereas CKM's inference time is also influenced by the size of the training set, potentially limiting its scalability for large datasets. This may explain the need for specialized data structures and tricks, even for relatively simple datasets like NORB.
2. The leaf kernel's purpose is not well understood, particularly when dealing with raw pixel intensities. Comparing pixel intensities between query and training images may lead to unnecessary comparisons of background pixels, which could hinder recognition performance. A more detailed explanation of Section 3.1 would be beneficial, as the current presentation is dense and difficult to comprehend.
3. The design of the sum-product function's architecture is not clearly explained, and the example in Section 3.1 appears to be arbitrary. More guidance on how to design this architecture would be helpful.
4. The experimental section is the weakest part of the paper, with the NORB dataset being relatively small and simplistic by today's standards. Even on this dataset, the proposed method only marginally outperforms SVM (with unclear specifications regarding linear or kernel SVM) and significantly underperforms convnet. The proposed method only demonstrates improvement over convnet on synthetic datasets (NORB compositions and NORM symmetries).
In conclusion, while this paper presents some interesting ideas, it is still in its preliminary stages, and more work is necessary to demonstrate its advantages. Nevertheless, it is acknowledged that many important ideas in machine learning have initially seemed premature, only to develop and mature over time.