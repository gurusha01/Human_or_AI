This paper presents an intriguing approach that demonstrates empirical performance gains over baseline methods. However, upon closer examination, I have concerns regarding the technical sophistication of the work. The core idea can be distilled to estimating batch normalization mean and bias on a per-domain basis, rather than across the entire model. It is unclear whether this constitutes a novel contribution, as it appears to be a straightforward extension of the original batch normalization paper. In my opinion, this work may be more suited to a brief workshop presentation rather than a full conference paper.
Detailed comments:
Section 3.1: I disagree with the assertion that the primary purpose of batch normalization is to align the distribution of training data. While batch normalization does have this effect, its main function is to regulate the scale of gradients, enabling the training of deep models without encountering vanishing gradient issues. The observation that intermediate features from different datasets form distinct groups in a t-SNE embedding is not unique to batch normalization, as similar results can be obtained using AlexNet. Therefore, the premise in section 3.1 is inaccurate.
Section 3.3: I share the same concerns as another reviewer regarding the disconnect between the concept of AdaBN and the content of this section. Equation 2 simply demonstrates that the combined batch normalization and fully connected layer forms a linear transformation, which is a well-established fact in both the original batch normalization and the proposed approach. I do not believe this adds significant theoretical depth to the paper, and overall, the novelty of this work appears to be limited.
Experiments:
- Section 4.3.1 does not provide an accurate assessment of the proposed method's effectiveness. Instead, it verifies a straightforward fact: the source domain features are normalized to a Gaussian distribution, and the proposed method explicitly normalizes the target domain features to the same distribution. Consequently, it is expected that the KL divergence between these distributions would decrease. However, this does not directly relate to the final classification performance.
- Section 4.3.2 presents an interesting sensitivity analysis, suggesting that only a few images are required to account for domain shift in AdaBN parameter estimation. This implies that a single "whitening" operation may be sufficient to offset domain bias, as a single batch is enough to recover approximately 80% of the performance gain. A more comprehensive comparison between these approaches and a detailed analysis of the effect of each layer on the model would be beneficial, as the current analysis seems somewhat superficial.