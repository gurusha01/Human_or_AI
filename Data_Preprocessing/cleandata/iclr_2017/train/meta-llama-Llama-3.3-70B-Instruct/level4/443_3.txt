This manuscript introduces the neural noisy channel model, P(x|y), for input-to-output sequence pairs (x, y), building upon the authors' prior work on the segment-to-segment neural transduction (SSNT) model. A key distinction of the noisy channel model from traditional sequence-to-sequence models is that the entire output sequence y is not known beforehand. The SSNT model addresses this challenge through incremental alignment and prediction, but the current paper does not significantly advance beyond the SSNT framework. The SSNT model remains applicable by simply reversing the input and output sequences. The authors' assertion that a unidirectional LSTM must be used as an encoder, rather than a bidirectional LSTM, seems to be a minor distinction. However, the decoding algorithm presented in the appendix appears to be a novel contribution.
The experimental evaluation is thorough and robust, yet a crucial baseline result is missing from all experiments: the performance using direct + LM + bias, and ideally, direct + bias. Although mathematically, incorporating a language model (LM) into the direct model may not be intuitive, it has been shown to be effective in practice, as the LM can rescore and smooth predictions, as demonstrated in the Deep Speech 2 study by Baidu. The LM may also be a key factor in explaining why the noisy channel model outperforms the direct model in Table 3. Two minor clarifications are needed:
1. It is unclear whether the direct model used in the experiments is based on SSNT or a traditional sequence-to-sequence model.
2. While the O(|x|^2*|y|) training complexity is acceptable, further reducing the computational cost would be beneficial, as it remains prohibitively expensive for long input sequences, such as those encountered in paragraph or document-level modeling, or speech sequences.
The paper is well-written, and overall, it remains an interesting contribution, as channel models continue to captivate the general public.