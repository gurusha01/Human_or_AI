This paper presents a novel approach to utilizing "surprisal" as a top-down signal in Recurrent Neural Networks (RNNs), specifically by incorporating the error from the previous prediction as an additional input at the current timestep in a Long Short-Term Memory (LSTM) network.
The concept of surprise-driven feedback is intriguing, particularly for online prediction tasks, and appears to yield notable improvements. However, the paper in its current form exhibits several significant shortcomings.
- The overall quality of the paper's writing could be enhanced. Notably, sections 2.4 and 2.5 are predominantly composed of equations related to the forward and backward propagation of feedback RNN and feedback LSTM, without accompanying analysis. This omission makes it unclear what insights the author intends to convey in these sections. Furthermore, the feedback RNN is not evaluated in the experimental section, which raises questions about its inclusion.
- The experimental evaluation is restricted, focusing solely on the enwik8 dataset. To comprehensively assess the efficacy of the feedback LSTM, it is essential to test the concept on diverse datasets to determine if consistent improvements are observed.
- The author claims state-of-the-art performance on enwik8, but the hypernetwork, which is cited in the paper, achieves superior results (1.34 BPC, as per table 4 in the hypernetworks paper).
- The comparison is limited to methods that do not utilize the last prediction error as an extra signal. A more equitable comparison would be with dynamic evaluation, which backpropagates the prediction error through the network and adjusts the weights accordingly. Both feedback LSTM and dynamic evaluation leverage "extra" supervised information via prediction errors, albeit through different propagation methods.
In summary:
Pros:
- The idea is interesting and shows potential for improvement.
- The approach seems to enhance performance.
Cons:
- The paper's writing quality is subpar.
- The evaluation is weak, being confined to a single dataset.
- The comparison is restricted to approaches that do not incorporate the last-timestep error signal.