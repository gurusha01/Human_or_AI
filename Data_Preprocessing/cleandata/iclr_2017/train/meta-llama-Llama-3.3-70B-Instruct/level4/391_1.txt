This paper presents a weight pruning method for neural networks, specifically applied to a recurrent neural network (RNN) system trained on a speech recognition dataset, resulting in sparse solutions. The approach yields significant test-time computational savings with minimal impact on task performance, and in some cases, even improves evaluation performance.
The experiments utilize a state-of-the-art RNN system, and the methodology appears sound. The investigation of pruning effects on large networks is commendable, and the computational gains are substantial. However, the use of a private dataset for all experiments is unfortunate, and evaluation on a standard test set, such as HUB5 for conversational speech, would have been beneficial. Additionally, a comparison with other pruning approaches, like the work by Han et al., would have helped verify the relative merit of the proposed method.
While the single-stage training approach seems elegant, it may not provide significant time savings if multiple experiments are required to find optimal hyperparameter settings for the threshold adaptation scheme. The dense baseline would have been more convincing if it had incorporated model compression techniques, such as training on soft targets provided by a larger network.
The paper is well-written, although the table and figure captions could be more detailed. The discussion on potential future speed-ups and memory savings for sparse recurrent neural networks is interesting but not specific to the proposed pruning algorithm. The paper could better motivate the method's details, such as the necessity of ramping up the threshold after a certain period.
The concept of sparse neural networks, including recurrent neural networks, has been extensively researched, with similarities to the work by Han et al. The proposed method's novelty lies in its application to RNNs, which are typically more challenging to train than feedforward networks. Although the paper may not present groundbreaking ideas or scientific insights, it demonstrates the successful application of weight pruning to large, practical RNN systems without significant performance sacrifices.
The key advantages of the proposed method include its ability to substantially reduce the number of parameters in RNNs without compromising performance and its application to a state-of-the-art system for a practical task. However, the method's similarity to earlier work and lack of novelty, as well as the absence of comparisons with other pruning methods and the use of private data, are notable drawbacks.
Overall, the paper presents a worthwhile result, showing that weight pruning can be effectively applied to large RNN systems with minimal performance impact, using a simple heuristic. This finding is worth sharing, as improving scalability is a crucial driving force in neural network research. 
Pros:
- The proposed method effectively reduces parameters in RNNs without significant performance loss.
- The experiments utilize a state-of-the-art system for a practical application.
Cons:
- The proposed method bears strong resemblance to earlier work and offers limited novelty.
- There is no comparison with other pruning methods.
- The use of private data prevents result replication.