This manuscript proposes three enhancements to the conventional LSTM architecture commonly employed in neural NLP models, specifically Monte Carlo averaging, embed average pooling, and residual connections. Given the simplicity of implementing these modifications, the paper is likely to be of value to NLP researchers exploring deep learning. 
However, I have reservations regarding the experimental design and outcomes. The effectiveness of residual connections appears inconsistent; on the SST dataset, vertical residuals yield improvements while lateral residuals detract from performance, whereas the opposite is observed on the IMDB dataset. A more significant concern is the limited scope of tasks investigated, with the paper focusing solely on sentiment analysis. It is unclear why the authors have concentrated on text classification, as the proposed modifications could potentially benefit any NLP task utilizing an LSTM encoder. 
To strengthen the paper, it would be beneficial to include a broader range of tasks, such as question answering and machine translation, to demonstrate the generalizability of the modifications. Although the experiments presented are comprehensive and the analysis is insightful, the lack of diversity in tasks undermines the paper's ability to convincingly demonstrate the widespread applicability of the proposed enhancements. Consequently, in its current form, I do not consider the manuscript ready for publication.