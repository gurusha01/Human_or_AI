This paper proposes a novel approach by integrating LSTMs, trained on an extensive MIDI corpus, with a custom-designed reward function to fine-tune the model in a musically relevant manner. The concept of utilizing hand-crafted rewards is commendable and holds promise for practical applications, where musicians can define rules rather than melodies.
Although certain decisions appear somewhat arbitrary and oversimplified from a music theoretical standpoint, the results demonstrate an improvement over the note RNN baseline. However, it is unclear how representative these results are, and the possibility of cherry-picking cannot be ruled out. 
I remain skeptical about the scalability of this approach to more complex reward functions necessary for composing authentic music. The difficulties encountered by LSTMs in producing pleasant melodies from a relatively large corpus raise questions about their suitability. Are there alternative differentiable models, such as dilated convolution-based approaches, that might be more effective?
A concern with the paper is the reference to short melodies as "compositions," which is misleading given their simplicity and lack of polyphony. Collaboration with individuals having formal musical training could provide valuable feedback and enhance the paper's credibility. 
A notable strength of the paper is the authors' effort to provide insight into their methodology, as evidenced by Table 1. However, Figure 3 would have benefited from the inclusion of real melody excerpts using the same sound synthesis and sample setup. Additionally, a more in-depth discussion of the method's limitations would be beneficial.
In conclusion, while the paper's novelty may be limited, it serves as a useful documentation of achieving practical results through RL-based fine-tuning approaches. I appreciate the paper's idea and can envision its potential utility for musicians, making it a worthwhile contribution despite its shortcomings.