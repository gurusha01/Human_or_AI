This paper deserves credit for reemphasizing the significance of deep network initialization, challenging the prevailing notion that advanced architectures and refined gradient descent methods have rendered optimization issues, such as local minima and saddle points, obsolete. 
The authors present intriguing counter-examples demonstrating how inadequate initialization, combined with specific data, can lead to suboptimal solutions, although these examples appear to be artificially constructed and somewhat contrived. A more significant concern is that the paper overlooks widely adopted heuristics, including non-saturating activation functions (like leaky ReLU), batch normalization, and skip connections (as in ResNet), which can be seen as facilitating gradient flow and potentially mitigating the risk of getting stuck in poor solutions.
By highlighting potential initialization pitfalls in standard ReLU networks, the paper serves as a cautionary notice, but it falls short by not proposing novel solutions or workarounds and failing to conduct a comprehensive analysis of how current heuristic techniques (in architecture, initialization, and training) impact this issue. Expanding the scope of the analysis to include these commonly used techniques, particularly if it yields practically relevant insights, would substantially enhance the paper's value for readers.