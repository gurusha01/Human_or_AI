Review- Summary:
This paper proposes a parametric class for modeling nonlinearities in neural networks, employing a two-stage optimization approach to learn both network weights and nonlinearity weights.
Significance:
The paper presents an intriguing concept and compelling experimental results. However, the theoretical analysis is somewhat uninformative and detracts from the central idea. More comprehensive experimentation using diverse bases and comparing the results to wider networks (with a comparable number of parameters to the cosine basis used) would provide stronger support for the paper's claims.
Comments:
- Are the learned nonlinearity weights shared across all units and layers, or does each unit have its own unique nonlinearity?
- If the weights are indeed shared, it would be interesting to investigate whether an optimal nonlinearity exists.
- How does the learned nonlinearity differ when hidden units are normalized versus unnormalized, i.e., with or without batch normalization?
- Does normalization impact the conclusion that polynomial bases are ineffective?