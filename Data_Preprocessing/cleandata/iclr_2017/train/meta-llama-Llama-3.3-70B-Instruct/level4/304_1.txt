This paper emphasizes the significance of recursion handling in neural programming architectures, enabling robust generalization to out-of-domain test cases and efficient learning from limited training data. The majority of the paper builds upon the Neural Programmer Interpreters concept introduced by Reed & de Freitas at ICLR 2016, which learns from program traces, with the key distinction being the incorporation of recursive calls into the training traces for NPI models. The authors demonstrate the verification of correctness through evaluation on a small set of base cases and reduction rules, showcasing the NPI architecture's capability to perfectly infer complex problems like Bubblesort and the Tower of Hanoi.
The simplicity of the idea is notable, with the authors acknowledging that the primary modification lies in the execution traces provided to the training pipeline. This raises intriguing questions regarding the implications of this approach, such as whether the neural programming problem is effectively solved when execution traces are available, and if the problem was initially overly simplistic. For instance, applying this method to a larger input domain like MNIST digits, where the task involves sorting digits from highest to lowest, would essentially decouple digit recognition from program logic inference, reducing the problem's complexity to recognizing MNIST digits and learning to bubble sort symbols. A critical follow-up question is the identification of a scenario where access to execution traces does not facilitate inference using the proposed method, highlighting the need for further exploration of the limitations and potential applications of this approach.