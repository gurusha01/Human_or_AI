The authors propose a recurrent deep neural network that models human fixation locations in videos as a Gaussian mixture, trained using maximum likelihood estimation with real fixation data. In addition to assessing the model's fixation prediction capabilities, they also explore combining saliency predictions with C3D features for action recognition purposes.
Quality: A more comprehensive evaluation of the model's fixation prediction performance is needed. Notably, the center bias performance differs substantially between Table 1 and Table 2, with state-of-the-art models in Table 2 performing worse than the center bias in Table 1, raising questions about whether there are no better models than center bias. Furthermore, details on how central bias and human performance are modeled are lacking, including whether human performance is cross-validated.
The claim that the results are close to human performance, with a difference of only 3.2%, is problematic as this difference exceeds the difference between Central Bias and the proposed model in Table 1. Comparing AUC performance differences is also risky due to potential saturation issues.
Clarity: The explanation for Table 3 is confusing, and it is unclear why the CONV5 and FC6 models differ in their utilization of the saliency map. An evaluation of the CONV5 model using the saliency map multiplication method would be beneficial to distinguish between the effects of different saliency map usage and feature differences.
Other issues include the inaccurate citation of KÃ¼mmerer et al. (2015), whose model was actually trained on fixation data using maximum likelihood, contrary to the claim that it learns indirectly without explicit human gaze information.
Despite these issues, the paper presents an interesting contribution to spatio-temporal fixation prediction. Addressing the aforementioned evaluation concerns would significantly improve the paper's quality, potentially leading to a revised rating.