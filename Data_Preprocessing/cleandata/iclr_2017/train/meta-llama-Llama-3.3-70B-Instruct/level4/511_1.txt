This manuscript proposes an algorithm for approximating the solution of specific time-evolution partial differential equations (PDEs). The approach presented is a learning-based method that involves alternating between three key steps: 
1. selecting points in space-time for sampling
2. computing the PDE solution at these sampled points
3. constructing a space-time function through regression to match the PDE solutions at the sampled points, with the aim of generalizing to unsampled points.
The proposed algorithm is found to be intriguing and potentially beneficial in practical applications. Traditional grid-based simulations of PDEs often become prohibitively expensive due to the curse of dimensionality, making the learning of PDE solutions an attractive alternative for real-world scenarios. However, as noted by the authors, directly applying gradient descent to the regression loss is not viable due to the non-differentiability introduced by the "min" operator in the PDEs under consideration.
Consequently, the algorithm put forth is considered a novel and interesting strategy for learning PDE solutions in the presence of non-differentiability, a scenario that poses significant challenges for numerical PDE solvers. Although the manuscript motivates the problem through applications in control theory, involving time-evolution PDEs with "min" operators applied to spatial derivatives, it is believed that such problems may hold more direct relevance and interest for both the machine learning and deep learning communities, as exemplified by potential applications in these fields.