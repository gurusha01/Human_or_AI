This paper proposes a novel regularization term that guides the optimizer towards a flat local minimum with reasonably low loss, rather than a sharp region of low loss, motivated by empirical observations that local minima with good generalization performance tend to be flat. To achieve this, the authors introduce a regularization term based on free local energy and estimate its gradient using Monte Carlo methods with an SGLD sampler, as a closed-form solution is not tractable. The experiments provide evidence for the flatness of good local minima and compare the performance of the proposed method to the Adam optimizer.
The paper is well-structured and clearly written, making it an enjoyable read. The connection between free energy and the optimization framework is intriguing, and the motivation for pursuing flatness is well-supported by several experiments. However, there appears to be a potential error in equation (8), where the first term might should be represented as f(x') instead of f(x). Additionally, it is puzzling that the authors did not include experiment results on Recurrent Neural Networks (RNNs) in their performance evaluation, given that char-LSTM for text generation was already utilized in the flatness experiments. Including more experiments on diverse models and applications of deep architectures, such as RNNs and sequence-to-sequence models, would strengthen the authors' claims. Furthermore, the inconsistent use of terminology, such as free energy and free entropy, is somewhat confusing and could be clarified for better understanding.