This paper proposes a methodology for learning shared neural representations of temporal abstractions in hierarchical reinforcement learning (RL) using actor-critic methods, demonstrated through two tasks: a gridworld with objects and a simplified Minecraft environment. The concept of providing symbolic task descriptions and learning their corresponding implementations is intriguing, with empirical results showing promise. However, two significant limitations are evident. Firstly, the ideas presented are not novel, having been explored in previous work on symbolic specifications, actor-critic methods, and shared representations. Although related work is discussed, the unique contribution of this paper, beyond implementing existing concepts within a deep learning context, is unclear and needs to be explicitly stated. 
Secondly, the approach heavily relies on curriculum learning, as evident from the experiments. While the authors suggest that specifying tasks in simplified language is straightforward, designing an effective curriculum can be complex and task-dependent. The provided examples are relatively small-scale, and there is no indication of how to design curricula for more substantial problems. This sensitivity to curriculum design limits the potential applicability of the work. Furthermore, it is uncertain whether automatic supervision can be achieved without relying on prior domain knowledge.
Additional minor concerns include:
- The experimental setup lacks detailed description, making it difficult to understand the rationale behind specific choices (e.g., optimization, initial successful configurations). Despite the availability of GitHub code, the experiments as described are not fully reproducible.
- The approach's description is closely intertwined with specific algorithmic decisions, making it challenging to formalize the methodology in a more general sense. A more abstract formulation could facilitate clearer connections to prior work and enhance the overall clarity of the approach.