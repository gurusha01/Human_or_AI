The authors have presented a dynamic neural Turing machine (D-NTM) model, which addresses the limitation of rigid location-based memory access in the original NTM model. The paper makes two key contributions: 1) the introduction of learnable addressing to NTM, and 2) the incorporation of curriculum learning using a combination of discrete and continuous attention mechanisms. The proposed D-NTM model has been empirically evaluated on the Facebook bAbI task, demonstrating improved performance compared to the original NTM.
Strengths:
+ The paper provides a thorough comparison between feed-forward and recurrent controllers, offering valuable insights.
+ The results on curriculum learning with hybrid discrete and continuous attention are promising and noteworthy.
Weaknesses:
- The NTM baseline in Table 1 appears to be suboptimal, with an error rate of 31% compared to the 20% error rate reported in Graves et al. (2016) for the same model. Notably, the NTM baseline in the aforementioned study outperforms the proposed D-NTM with a GRU controller. It may be beneficial to replicate the results using the hyper-parameter settings from Table 2 in Graves et al. (2016), as this could potentially lead to enhanced performance for the D-NTM.
- Section 3 of the paper is difficult to follow, and the overall clarity of the manuscript requires improvement to facilitate better understanding.