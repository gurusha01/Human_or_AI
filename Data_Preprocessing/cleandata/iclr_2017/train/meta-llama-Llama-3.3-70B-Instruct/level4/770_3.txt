This manuscript tackles the challenging issue of generating rare or unseen words in language modeling, a problem exacerbated by the computational constraints imposed by Zipf's law, which leads to the limitation of vocabulary and the mapping of rare words to an unknown token. The ability to generate such words is particularly crucial in applications like question answering and machine translation. The proposed approach integrates knowledge from knowledge bases (KBs) into the language modeling framework, enabling the generation of potentially unseen words based on KB facts. Additionally, the authors introduce a new dataset that aligns words with Freebase facts and corresponding Wikipedia descriptions.
The model operates by first selecting a relevant KB fact based on the context provided by previously generated words and facts. It then determines whether to generate a word from the predefined vocabulary or to output a symbolic word directly from the KB. In the latter case, the model predicts the word's position within the fact description. 
While the paper is well-structured, some sections, particularly the notation in Section 3, could benefit from rewriting for enhanced clarity. The experimental design is robust, yielding promising results, with the heat maps providing valuable insights into the model's performance.
To further strengthen the paper's contributions, demonstrating the technique's applicability and improvement in practical tasks such as question answering would be beneficial, although the authors do suggest its potential utility in such areas. 
Several aspects of the model warrant further consideration. The terminology in Section 3, where an entity is referred to as a 'topic', might be misleading due to the abstract connotations of the term 'topic', whereas in this context, it specifically denotes a Freebase entity. 
The necessity of predicting a fact at every step before word generation is also questionable. Intuitively, a sentence typically describes a few key facts about an entity, suggesting that fact generation could potentially be optimized or made conditional through the introduction of a latent variable, which could also enhance the model's efficiency.
The hard decision required in Equation 2 to select a fact implies the need for comprehensive annotation of words with corresponding facts, a requirement that may not always be feasible, especially in domains like social media where such annotations might be sparse or unreliable.
Furthermore, the approach of learning position embeddings for copying knowledge words seems counterintuitive, raising questions about the structural consistency of knowledge word sequences. For instance, does the sequence follow a predictable pattern where certain positions consistently correspond to specific types of information, such as a last name?
Finally, a comparison with character-level language models, which inherently address the issue of unknown tokens, would provide a more comprehensive understanding of the proposed model's strengths and weaknesses.