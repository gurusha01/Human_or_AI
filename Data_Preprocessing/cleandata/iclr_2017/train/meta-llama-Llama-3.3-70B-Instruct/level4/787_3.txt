This manuscript presents SEM, a straightforward multilabel learning approach that employs a one-layer hidden network to model label probabilities as softmax(sigmoid(W^T X) + b). While this formulation is not innovative, nor is the use of adagrad for optimization, the paper's derivation of the gradient and proposed alternating adagrad steps are unconventional and their impact on performance is uncertain. The key factor contributing to the model's efficiency is the candidate label sampling method, which follows standard practice by sampling labels in proportion to their dataset frequency.
Despite the lack of novelty in both the model and training strategy, the reported results surpass the current state-of-the-art in terms of quality and efficiency, although claims of non-asymptotic efficiency are inherently difficult to verify due to the trade-off between implementation effort and performance. Overall, the paper's contribution seems insufficient to warrant acceptance, as it does not appear to meet the expected standards.