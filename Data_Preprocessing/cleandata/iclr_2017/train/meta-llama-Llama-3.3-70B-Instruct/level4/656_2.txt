This paper explores a novel hybrid architecture, combining a scattering network with a convolutional network, which reduces parameter count and ensures stability to deformations in the initial layers. Experimental results demonstrate reasonable performance, surpassing the network-in-network architecture in low-data scenarios.
A long-standing question in the field is why convolutional networks require re-learning low-level features during each training iteration. Theoretically, utilizing fixed features could conserve parameters and training time. This study appears to be the first to investigate this query. In my opinion, the findings suggest that incorporating scattering features in lower layers does not yield the same performance as learned CNN features, a result that is not immediately apparent and thus noteworthy. However, I disagree with the authors' assertion that the hybrid network exhibits superior generalization capabilities.
In the low-data regime, the hybrid network occasionally outperforms NiN, although this architecture is somewhat outdated and its capacity has not been optimized for the dataset size. When using the full dataset, the hybrid network is clearly outperformed by fully learned models. Additionally, the authors' comparison does not seem to involve identical architectures with and without scattering as the initial layers, further complicating the interpretation of the results.
The authors argue that the hybrid network possesses a theoretical stability advantage. Nevertheless, only the first layers of the hybrid network are stable, while the learned layers can still introduce instability. Moreover, if potentially unstable deep networks outperform stable scattering nets and partially stable hybrid nets, it raises questions about the significance of stability as defined in scattering network theory.
In conclusion, while the paper addresses a relevant question, I remain unconvinced that the hybrid network generalizes better than standard deep networks. The potential for faster computation during testing, which could be beneficial in low-power and mobile devices, is not thoroughly explored in the paper.
Minor comments:
- Section 3.1.2: "learni" appears to be a typo.