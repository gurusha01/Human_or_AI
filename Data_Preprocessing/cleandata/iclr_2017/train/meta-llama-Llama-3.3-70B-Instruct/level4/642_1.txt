This paper examines the effects of tailored number representations on the accuracy, speed, and energy efficiency of neural network inference, with a focus on standard computer vision architectures such as VGG and GoogleNet. The findings suggest that floating-point representations outperform fixed-point representations, and that 14-bit floating-point numbers are sufficient for the considered architectures, resulting in minimal accuracy loss.
The paper provides a comprehensive overview of floating-point and fixed-point representations, shedding light on a crucial yet underexplored aspect of deep learning. While there are areas for improvement, the overall contribution warrants a weak accept, pending the authors' addressing of the following concerns:
1. The paper's scope should be clarified to explicitly focus on neural network inference, with the title and abstract revised to include the term "inference" and a note that the findings may not apply to neural network training due to differing training dynamics.
2. The possibility of leveraging quantization techniques during training, potentially enabling the use of fewer bits at inference, is not discussed and should be considered.
3. The methodology for calculating running time and power consumption lacks clarity, particularly regarding whether all modules or only multiply-accumulate units are included, as well as the accuracy of these measurements across different designs and potential discrepancies between simulation and production. A more detailed explanation of the simulation setup is necessary.
4. The emphasis on "efficient customized precision search" seems unnecessary, as the benefits of optimized hardware configurations may outweigh the costs of increased simulation time. An exhaustive search could be parallelized, and weak configurations could be filtered out after evaluating a few examples, making the discussion of efficient search methods less relevant.
5. The paper should acknowledge and cite relevant NVIDIA documentation and research, particularly regarding the support for FP16 in the Pascal GP100 GPU.
Additional comments include:
- The sections discussing "efficient customized precision search" require further clarification.
- Future studies could investigate the impact of number representations on batch normalization and recurrent neural networks, offering a promising direction for further research.