It appears that the usefulness of a dataset has a relatively short lifespan in current research, as evidenced by the rapid progress made on the SQuAD dataset since its release a few months ago, with the top performance on the leaderboard now reaching 82%. This is noteworthy considering that the dataset was formally presented at EMNLP'16 just a month ago, and the reported machine performance at the time of submission was only 51%. A plausible explanation for this rapid progress is that the dataset may not be challenging enough.
The submitted paper, NewsQA, attempts to address this concern by introducing a dataset of comparable scale created using alternative QA collection strategies. Notably, the authors collect questions from turkers without requiring them to provide answers, promoting more diverse and challenging questions. Additionally, the questions are gathered without providing the content of the news articles, and the dataset utilizes a larger subset of the CNN/Daily corpus (12K/90K) compared to SQuAD (500/90K).
In summary, I believe that the NewsQA dataset represents an effort to create a more challenging, large-scale reading comprehension task, a currently popular research area lacking satisfactory datasets. Although it has its weaknesses, the dataset has potential value compared to existing ones.
However, the paper appears to have been prepared hastily, with several minor issues that the authors could have improved. This raises concerns about the dataset's quality. For instance, the human performance on SQuAD reported by the authors (70.5-82%) is lower than that reported by SQuAD (80.3-90.5%). Such discrepancies can occur due to variations in annotator carefulness, as humans have different levels of attention to detail and reading comprehension abilities. It would be beneficial for the authors to explain these differences and conduct a more rigorous measurement of human performance. If the human performance on NewsQA is only 74.9%, it may indicate that the dataset's difficulty stems from noise in the QA collection process rather than the actual comprehension and reasoning challenges, which could lead to low model performance due to incorrect human annotations.
I also question the design choice of not providing the news article when soliciting questions. Without sufficient context, people may ask generic questions. A potential solution could be to present news articles with randomly redacted sentences or phrases, allowing question generators to have some context without access to the full material.
Another approach to encourage turkers to ask more challenging questions is to engage an automatic QA system in real-time, requiring turkers to create QA pairs that existing state-of-the-art systems cannot answer correctly.