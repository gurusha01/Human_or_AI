The paper presents a methodology for incorporating recurrent layers into larger convolutional networks, which may be pre-trained, with the goal of combining the feature extraction capabilities of CNNs with the ability of RNNs to capture global context information. This approach is validated through experiments on two tasks: image classification using the CIFAR-10 dataset and semantic segmentation using the PASCAL VOC12 dataset.
On the positive side, the paper is well-structured and clearly written, aside from a few minor typos. The proposed concept is straightforward and has the potential to be adopted by other studies, offering a beneficial modification to existing systems without requiring a complete retraining. This practical importance is notable, as it allows for performance enhancement without the need for a full system overhaul. The evaluation is systematic, including a comprehensive ablation study that provides insight into the contributions of different components.
However, there are also negative aspects to consider. The novelty of the work is somewhat limited, and the validation could be more extensive. The idea of integrating a recurrent layer with a CNN is not entirely new, as a similar concept was proposed by Bell et al. in 2016. Although there are some technical differences, such as the application of recurrent layers in a cascading versus parallel manner, these differences are relatively minor. The initialization of the recurrent network with the CNN is a reasonable approach but does not significantly deviate from the original work by Bell et al., instead improving upon one of its limitations. This contribution, highlighted throughout the paper, was largely present in Bell et al.'s work, modulo minor adjustments.
Regarding the evaluation, the experiments on CIFAR-10 serve as an interesting proof of concept but do not fully demonstrate the method's potential. Notably, Wide Residual Networks, as reported by Sergey Zagoruyko and Nikos Komodakis at BMVC16, achieve better results on CIFAR-10 (with a 4% error rate) without utilizing recurrent layers, instead employing a wide, VGG-type, ResNet variant. The authors respond by explaining that Wide Residual Networks leverage the depth of the network to spread the receptive field across the entire image, thereby not requiring recurrence within layers to capture contextual information. In contrast, the authors demonstrate that a shallow CNN can capture contextual information across the whole image when a Long-term Recurrent Neural Network (L-RNN) is used. This exchange highlights the point that while recurrence may not be necessary for achieving high performance, it could be beneficial in certain scenarios, such as when maintaining a shallow network is desirable.
An evaluation on ImageNet could provide further insights into the merits of incorporating a recurrent layer. 
For semantic segmentation, a question arises regarding whether the observed performance boost is due to the unique properties of the recurrent layer or simply the result of adding extra parameters to a pre-trained network. The authors' response indicates that adding the L-RNN to a pre-trained network boosts performance more significantly than adding the same number of parameters as extra CNN layers, as it can model long-range dependencies. However, the paper does not include an experiment directly comparing the addition of recurrent parameters to the addition of non-recurrent parameters (e.g., through residual layers), which would be a useful sanity check.
Furthermore, the presentation of results in Table 3, comparing FCN-8s and FCN8s-LRNN, might be misleading, as it suggests a 10% boost from the LRNN. In reality, the "FCN8s" baseline in the paper performs better than the original FCN-8s by Long et al., as indicated in Table 2. 
Additionally, the source of the performance boost in Table 2 is not entirely clear. The authors mention that inserting the L-RNN after pool 3 and pool 4 in FCN-8s allows it to learn contextual information over a larger range than pure local convolutions. However, this advantage should theoretically apply to FCN-32s as well, given that it is a property of the recurrence rather than the 8/32 factor.
Several minor points also warrant mention: Figures 2b and 2c appear to be missing from the PDF. Figure 4 is overly complex, with approximately 30 boxes that may overwhelm the reader; a table might be a more effective way to present this information. In Appendix A, it is unclear whether other learning rate schedules (e.g., polynomial) were explored, and the performance under a standard training schedule (e.g., step) is not discussed. Lastly, in Appendix C, there is a grammatical error ("maps .. is" should be "maps ... are").