This paper presents an implementation of the delayed synchronized SGD method for deep neural network training on multiple GPUs. 
Key observations and suggestions include:
1) The manual implementation of delayed synchronization and state protection is a useful contribution, although it could be alternatively achieved using a dependency scheduler, eliminating the need for manual threading.
2) The technique of overlapping computation and communication, while implemented in this work, is not novel and has been previously incorporated in established frameworks such as TensorFlow, as discussed in Chen et al., and MXNet, which somewhat diminishes the claimed contribution.
3) The convergence accuracy is only partially reported, focusing on the initial iterations and solely for AlexNet; a more comprehensive analysis would involve including the full convergence curve for all compared networks to provide a clearer understanding of the proposed approach's effectiveness.
In summary, the paper proposes a variant of the delayed SyncSGD approach. However, the system's novelty is perceived as limited, primarily due to the existing implementation of similar techniques in other solutions. To strengthen the paper, the experimental section could be enhanced to more convincingly demonstrate the advantages and unique benefits of the proposed method.