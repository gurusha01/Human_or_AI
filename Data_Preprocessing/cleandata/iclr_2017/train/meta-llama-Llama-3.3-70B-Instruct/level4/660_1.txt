The paper presents an enhanced version of the Adam optimizer, termed Eve, which dynamically adjusts the learning rate by comparing consecutive cost function values during the training process. The authors provide empirical evidence of the Eve optimizer's effectiveness on various problems, including CIFAR convnets, logistic regression, and RNNs.
However, several concerns arise upon reviewing the paper:
- The proposed method appears to be sensitive to arbitrary shifts and scaling of the cost function, which may impact its robustness.
- A more comprehensive comparison with baseline methods could be achieved by incorporating additional exponential decay learning schedules between the lower and upper thresholds of dt. Upon examining Figure 2, it seems that 1/dt may simply represent an exponential decay, which warrants further investigation.
- The introduction of three new hyper-parameters, namely k, K, and \beta_3, adds complexity to the method.
In my opinion, the method has inherent limitations, and the paper offers relatively little novelty. The modification lacks theoretical justification, and it would be beneficial for the authors to discuss potential failure modes of the proposed approach. Furthermore, Section 3.2 is difficult to follow, and the writing quality and clarity of the method section could be improved to enhance overall readability.