CONTRIBUTIONS 
This manuscript presents a novel approach to learning semantic units from multimodal data, specifically audio and visual inputs, using a joint neural network architecture. The model processes both image and audio spectrograms, enabling the embedding of image and spoken language captions into a shared representation space. By measuring the affinity between image patches and audio clips, the authors generate audio-visual groundings, which facilitate the association of specific visual regions with specific audio segments. The experimental evaluation encompasses image search and annotation tasks, as well as acoustic word discovery.
NOVELTY+SIGNIFICANCE
As acknowledged in Section 1.2, multimodal learning has been extensively explored in the computer vision and natural language processing communities, particularly for image captioning and retrieval applications. This work offers incremental advancements by introducing a novel combination of input modalities, namely audio and images. However, it is essential to note that the authors' prior work (Harwath et al, NIPS 2016) has already investigated bidirectional image/audio retrieval, and the current submission's training procedure is largely identical, with minor differences in data and CNN architecture. The primary novelty of this submission lies in the methodology for associating image regions with audio subsequences, which, although useful, employs relatively standard techniques with limited innovation. The approach involves computing alignment scores between densely sampled image regions and audio subsequences and applying heuristics to associate clusters of image regions with clusters of audio subsequences.
MISSING CITATION
The field of multimodal learning is vast, spanning computer vision, natural language processing, and speech recognition. A notable omission is the reference to Ngiam et al.'s work on "Multimodal deep learning" (ICML 2011).
POSITIVE POINTS
- The manuscript improves upon prior work in bidirectional image/audio retrieval by utilizing more extensive data and an enhanced CNN architecture.
- The proposed method demonstrates efficient acoustic pattern discovery capabilities.
- The audio-visual grounding, combined with image and acoustic cluster analysis, successfully identifies audio-visual cluster pairs.
NEGATIVE POINTS
- The work exhibits limited novelty, particularly in comparison to the authors' previous work (Harwath et al, NIPS 2016).
- Although the clustering method yields good results, it lacks novelty and appears to rely heavily on heuristics.
- The proposed method involves numerous hyperparameters (e.g., patch size, acoustic duration, VAD threshold, IoU threshold, and the number of k-means clusters), and there is no discussion on how these parameters were set or the method's sensitivity to these choices.