In the realm of supervised learning, a notable breakthrough was achieved with the introduction of the semi-supervised learning framework, which leveraged the relatively weaker approach of unsupervised learning to deduce specific properties, such as distance metrics or smoothness regularizers. These inferred properties could then be utilized in conjunction with a limited set of labeled examples, typically under the assumption of manifold smoothness.
This manuscript endeavors to extend this concept to reinforcement learning, albeit with an analogy that is somewhat inconsistent. The equivalence between labels and reward functions is not directly applicable, as positive or negative rewards do not convey the same meaning as positive and negative labels. Nevertheless, the paper presents a valuable exploration of the notion of semi-supervised reinforcement learning, a field that undoubtedly warrants further investigation. The authors employ the term "labeled MDP" to describe the conventional MDP framework with an unknown reward function, while using the potentially misleading term "unlabeled MDP" to denote a scenario where the reward is unknown, which technically constitutes a controlled Markov process rather than a Markov decision process.
In traditional reinforcement learning transfer learning settings, an agent attempts to transfer knowledge from a source "labeled" MDP to a target "labeled" MDP, where both reward functions are known, but the learned policy is only available in the source MDP. In contrast, the semi-supervised reinforcement learning setting involves a target "unlabeled" controlled Markov process and a source that encompasses both a "labeled" MDP and an "unlabeled" controlled Markov process. The proposed approach involves using inverse reinforcement learning to infer the unknown "labels" and subsequently attempting to facilitate transfer. Additionally, the authors impose a restriction to linearly solvable MDPs due to technical considerations. Experimental results are presented using three relatively complex domains simulated with the Mujoco physics engine.
Although the work is intriguing, in the opinion of this reviewer, it falls short of providing a sufficiently general and straightforward notion of semi-supervised reinforcement learning that would garner widespread interest from the reinforcement learning community. This remains an open challenge to be addressed by future research, but in the meantime, the present work is sufficiently interesting, and the problem under investigation is undoubtedly worthy of study.