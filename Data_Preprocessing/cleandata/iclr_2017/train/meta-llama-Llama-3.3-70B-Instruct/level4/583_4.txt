This paper introduces a quantitative metric for assessing the out-of-class novelty of samples generated by generative models, with an evaluation of over 1000 models featuring diverse hyperparameters, supplemented by a human subject study on a selected subset.
However, the authors acknowledge the challenges associated with human subject studies but fail to provide specific details about their methodology, including the use of an in-house annotation tool. The lack of transparency regarding the number of participants, their backgrounds, and the sample size presented to each individual raises concerns about the subject diversity, potentially leading to biased results due to a limited number of subjects being exposed to a large number of samples and/or being experts in the field.
The paper's objective is to propose a universal metric for novelty, yet the experiments are confined to a single setting, involving the generation of Arabic digits and English letters. This limitation raises doubts about the metric's generalizability, as there is insufficient evidence to support its broad applicability.
Furthermore, the classification of English letters as "novel" in comparison to Arabic digits is debatable. The metric's effectiveness in distinguishing between various types of generated characters, such as Arabic or Indian letters, and random doodles is unclear. For instance, could a person unfamiliar with Arabic handwriting differentiate it from random doodles? The notion that English letters are more "novel" than random doodles is also questionable. In my view, these concerns would be better addressed through large-scale human subject studies focused on tasks with clear real-world implications, such as comparing the preference for generated paintings versus those created by artists.