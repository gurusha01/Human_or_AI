The paper presents a novel nonlinear regularizer for addressing ill-posed inverse problems, where the underlying latent variables are assumed to reside near a low-dimensional subspace within a reproducing kernel Hilbert space (RKHS) defined by a predetermined kernel. This approach can be viewed as an extension of the traditional linear low-rank assumption imposed on the latent factors. To relax the dimensionality constraint of the subspace, the authors employ a nuclear norm penalty on the Cholesky factor of the kernel matrix. The empirical results demonstrate the effectiveness of the proposed method on two linear inverse problems: missing feature imputation and estimating non-rigid 3D structures from 2D orthographic projections, outperforming the linear low-rank regularizer.
However, the paper's clarity, particularly in the introduction, could be improved. A more logical flow might involve first introducing the concept of ill-posed inverse problems and then motivating the need for a regularizer, which would naturally lead to the discussion of dimensionality reduction techniques. The current back-and-forth presentation between these concepts can be confusing at times.
The motivation behind relaxing the rank constraint in Equation 1 to a nuclear norm in Equation 2 is not entirely clear. This relaxation not only fails to yield a convex problem over S and C (Equation 5) but also increases computational complexity, as Algorithm 2 requires a full singular value decomposition (SVD) of K(S) at each iteration. The authors should provide a discussion on the pros and cons of this approach compared to an alternative method that fixes the rank of C, which could be selected using cross-validation similar to the selection of Ï„. This simpler objective raises an interesting question: Are there kernel functions that allow for scalable solutions?
The proposed alternating optimization approach, in its current form, is computationally intensive and appears challenging to scale to moderately sized datasets. Each iteration involves computing the kernel matrix over S and performing a full SVD of the kernel matrix (Algorithm 2). Furthermore, the empirical evaluations are limited: (i) the dataset used for feature imputation is outdated and non-standard, (ii) the comparison for structure estimation from motion on the CMU dataset is only made with linear low-rank regularization, and (iii) there is no discussion or study on the convergence of the alternating procedure (Algorithm 1).