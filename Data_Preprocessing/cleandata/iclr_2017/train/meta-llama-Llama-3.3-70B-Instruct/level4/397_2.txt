This manuscript presents a novel concept, the "variational lossy autoencoder", which intentionally restricts a powerful autoregressive conditional distribution on inputs x given the latent code z, forcing it to utilize z in a meaningful manner. The paper's key contributions include:
(1) Providing an insightful information-theoretical explanation for the tendency of VAE-type models to underutilize their latent representation when the conditional distribution on x given z is sufficiently powerful.
(2) Demonstrating how this insight can be leveraged to efficiently train VAEs with powerful autoregressive conditional distributions, ensuring they effectively utilize the latent code.
(3) Introducing a robust method for parametrizing the prior using an autoregressive flow transformation, equivalent to applying an inverse autoregressive flow transformation to the approximate posterior.
The information-theoretical explanation for VAEs' underutilization of their latent code constitutes a significant addition to the understanding of VAE-related approaches. However, the empirical evaluation of this intuition is somewhat lacking. The "crippling" method employed appears to be hand-crafted and task-dependent, and the qualitative assessment of the "lossyness" of the learned representation is limited to three datasets (MNIST, OMNIGLOT, and Caltech-101 Silhouettes) featuring black-and-white images with minimal texture. Although Figures 1a and 2a illustrate that reconstructions discard low-level information, a more comprehensive analysis using complex image datasets would be more convincing. It is unclear whether the authors have explored applying VLAE to such datasets.
The Caltech101 Silhouettes benchmark results should be interpreted with caution, as no comparison is made to other competitive approaches like IAF VAE, PixelRNN, and Conv DRAW, resulting in VLAE outperforming the state-of-the-art in only one of the four examined settings.
A crucial question relevant to this paper is whether a latent representation on top of an autoregressive model enhances density modeling performance. Although the paper briefly addresses this question, the only setting in which VLAE is compared to recent autoregressive approaches shows a marginal improvement over PixelRNN.
The proposal to transform the latent code using an autoregressive flow, equivalent to parametrizing the approximate posterior with an inverse autoregressive flow transformation, is also noteworthy. However, a key distinction between the two approaches is that the former allows for a potentially complex prior over the latent code, whereas the latter limits the prior to a simple, factorized distribution.
It is unclear whether having a very powerful prior is necessarily beneficial from a representation learning perspective, as it may not always result in an untangled and independent representation of the data distribution. The trade-off between the prior's ability to fit the data and its usefulness as a high-level representation warrants further discussion, and the authors' opinion on this matter would be valuable.
In conclusion, despite introducing interesting ideas, the paper's weaknesses in empirical evaluation prevent a recommendation for acceptance. However, following the authors' response, the rating has been revised to a 7.