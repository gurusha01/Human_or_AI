This paper presents a joint multimodal variational autoencoder, a directed graphical model designed to capture multimodal data through a latent variable framework. Essentially, the model extends the standard VAE by generating two data modalities independently from a shared latent representation. To address issues with missing input modalities or bidirectional inference between modalities, the authors propose modality-specific encoders trained to minimize the KL divergence between the latent variable distributions of joint and modality-specific recognition networks. The effectiveness of this approach is demonstrated on the MNIST and CelebA datasets, with favorable results in terms of test log-likelihoods, conditional image generation, and editing capabilities.
Given that the proposed method is a relatively straightforward extension of the VAE, it is expected to inherit the probabilistic inference capabilities of VAEs. For instance, in scenarios with missing data modalities, the model should be capable of inferring a joint representation and filling in the missing modalities through iterative sampling, as previously introduced by Rezende et al. (2014). However, the marginal improvements observed raise questions about the significance of the contribution made by the modality-specific encoders introduced in Section 3.3. Furthermore, the inference methods used to generate Figure 5 appear somewhat ad hoc, prompting curiosity about whether a more principled approach, such as iterative sampling, would yield comparable or even superior conditional image generation results. Notably, the paper lacks experimental results on joint image-attribute generation, which would further substantiate the model's capabilities.