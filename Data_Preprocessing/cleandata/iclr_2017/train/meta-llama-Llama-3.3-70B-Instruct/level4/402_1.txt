Given the authors' examination of Configuration Validation with Successive Thresholding (CVST), it is surprising that Hyperband does not exhibit superior performance in their experiment. Notably, despite being an anytime algorithm, Hyperband was run for a significantly shorter duration than CVST, yielding consistently worse mean results. This disparity raises questions, as the characteristics of the task are not presented, potentially allowing for comparable results within error bars of CVST simply by selecting a random configuration at no additional cost. A more equitable comparison would involve running Hyperband for the same duration as CVST.
Furthermore, the experiment with Hyperband utilized a different η value compared to other experiments, prompting inquiries about the extent of method tuning required for optimal performance. It would be beneficial to investigate the outcome of using the same η=4 as in other experiments to provide a more comprehensive understanding.
This paper explores Hyperband, an extension of successive halving introduced by Jamieson & Talwalkar (AISTATS 2016). Successive halving is a notable algorithm that evaluates multiple configurations, iteratively eliminating the worst half to efficiently explore various configurations within a limited budget.
Upon re-reading the paper, the intended contribution remains unclear. The primary improvement of Hyperband over successive halving lies in its theoretical worst-case bounds, which are no more than 5x worse than random search. However, this bound can be trivially achieved by allocating a fraction of the time to running random configurations to completion. The theoretical analysis, purportedly beyond the scope of this paper, raises questions about whether these results constitute the primary contribution or are part of a separate study, leaving the current paper as primarily an empirical investigation of the method. Clarification from the authors is necessary to resolve this ambiguity.
The experimental results fail to demonstrate a scenario where Hyperband outperforms successive halving with its most aggressive setting of bracket b=4. In every figure, successive halving with b=4 is at least as effective as, and sometimes substantially better than, Hyperband. This suggests that, in practice, successive halving with b=4 might be preferred over Hyperband. If the guarantee of not being more than 5x worse than random search is desired, it can be achieved by running random search on a fraction of the machines.
The experiments also compare Hyperband to Bayesian optimization methods but omit a comparison to the highly relevant Multi-Task Bayesian Optimization methods, which have dominated effective methods for deep learning in recent years. Notably, "Multi-Task Bayesian Optimization" by Swersky, Snoek, and Adams (2013) demonstrated 5x speedups for deep learning by starting with smaller datasets, with subsequent papers showing even larger speedups.
Given the existence of prominent work on multitask Bayesian optimization, the introduction's portrayal of Hyperband as a novel approach to hyperparameter optimization is misleading. A more nuanced introduction, acknowledging the importance of configuration evaluation in hyperparameter optimization, including Bayesian optimization, and highlighting the potential for large speedups, would be more accurate. This could be achieved by adding a paragraph to the introduction, providing a more down-to-earth perspective on the paper's contributions.
Regarding novelty, it is essential to acknowledge that approaches for adaptively deciding resource allocation for evaluations have been studied in the ML community for at least 23 years, as seen in Maron & Moore's "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation" (NIPS 1993). This context is crucial for understanding the positioning and contribution of Hyperband within the broader landscape of hyperparameter optimization techniques.