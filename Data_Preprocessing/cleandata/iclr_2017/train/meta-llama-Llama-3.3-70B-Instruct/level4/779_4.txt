This paper assesses various approaches to decreasing output vocabulary size, aiming to accelerate NMT decoding and training, which could be highly beneficial for practitioners. However, the primary contributions appear to be somewhat unrelated to the core focus of representation learning and neural networks, raising questions about the suitability of ICLR as the optimal platform for this research.
- Are the reported decoding times inclusive of the time required for the vocabulary reduction process?
- Beyond machine translation, could these strategies be applied to other areas, such as language modeling, where large vocabularies pose significant scalability issues?
- The proposed methodologies are valuable due to the challenges inherent in word-level models, but it seems intuitive (in my view) that adopting a character-level or even more granular abstraction could inherently mitigate the vast vocabulary problem.