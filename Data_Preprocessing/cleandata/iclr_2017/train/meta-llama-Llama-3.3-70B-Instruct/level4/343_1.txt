This paper presents a novel approach to character language modeling (CLMs) by introducing a domain-specific language (DSL) for representing CLMs. The experimental results exhibit mixed performance compared to neural CLM approaches when modeling Linux kernel data and Wikipedia text, although the proposed DSL models demonstrate slightly better compactness and query speed. However, the overall approach is challenging to comprehend and may be tailored to a specific sub-community, lacking sufficient clarification for the broader ICLR audience. A major concern is that the paper overlooks crucial issues, such as demonstrating the validity of the proposed DSL as a probabilistic model and explaining the training process used to fit the model to the data, which apparently does not rely on gradient-based methods. Furthermore, the experiments seem incomplete without providing samples generated from the model or analyzing the learned model to understand its capabilities. Ultimately, the paper fails to provide an in-depth description of the approach, making it difficult for readers to understand or replicate.
The majority of the model section focuses on describing the DSL without elaborating on how probabilities are calculated using this model or how training is conducted. The DSL description appears to involve discrete decisions rather than probabilities, raising questions about how probabilities are actually encoded. Although training may be discussed in previous works, a concise explanation of the training process and optimality measures is necessary, which is not adequately addressed in Section 2.5.
Given the distinct hypothesis space of this model compared to neural models or n-grams, examining samples generated from the model is essential. While the current experiments demonstrate the model's ability to score utterances reasonably well, it would be fascinating to investigate whether the model can generate more structured samples than neural approaches, such as those exhibiting long-range syntax constraints like brackets.