A novel approach to click prediction is introduced, where categorical input variables are transformed into a feature vector using a discriminative embedding scheme that distinguishes between genuine and fake samples. This embedded vector is then processed through a sequence of SUM and MULT gates, followed by K-max pooling to identify the most significant interactions. The process is repeated across multiple layers, ultimately feeding the final feature into a fully connected layer to generate the predicted click-through rate.
The authors assert that:
(1) The incorporation of gates and K-max pooling enables the modeling of complex interactions, yielding state-of-the-art results.
(2) Unlike existing methods, such as word2vec, which may not be directly applicable for feature embedding, the proposed approach leverages a discriminative scheme to learn feature representations by distinguishing between authentic and fake samples.
From a theoretical perspective, convolutional operations can be viewed as "sum" gates that capture interactions between input dimensions. The authors explicitly impose structure on these interactions through the use of gates. To validate the effectiveness of the proposed method, a comparison with a baseline model that utilizes convolutional and pooling layers without gates is essential. However, this critical baseline, which consists of an embedding vector followed by a series of convolutional and pooling layers, is currently lacking.
Furthermore, it is unclear whether the proposed model and the baseline models have a similar number of parameters. For instance, a comparison of the total number of parameters in the CCPM model and the proposed model would be informative.
In general, the paper does not introduce a fundamentally new concept. While this alone is not a reason for rejection, the comparison to established baselines is weak. To strengthen the paper, the authors are encouraged to conduct a more comprehensive comparison with existing methods to demonstrate the effectiveness of their approach.