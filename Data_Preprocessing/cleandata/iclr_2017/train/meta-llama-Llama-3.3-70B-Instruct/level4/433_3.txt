This paper extends previous research on the NICE model, proposing a method for creating deep feed-forward generative models, which are evaluated on multiple datasets. Although the results do not surpass the current state-of-the-art, the approach contributes to the development of an intriguing class of models. The paper is generally well-structured and easy to follow.
A key advantage of this model is its ability to perform efficient and exact inference and generation. To further demonstrate the practical implications of this benefit, it would be beneficial for the authors to provide concrete example applications where such capabilities are essential or highly valuable.
The authors assert that their technique can learn a semantically meaningful latent space with the same dimensionality as the input space, unlike variational autoencoders and GANs. However, the paper lacks evidence to support this claim, and a more in-depth analysis of the latent space's semantic meaningfulness would be necessary to substantiate it. Additionally, demonstrating the usefulness of the learned representations in downstream tasks would strengthen the argument.
The discussion surrounding the "fixed reconstruction cost of L2" remains unclear. The factorial Gaussian assumption does not inherently restrict the generative model; instead, it smooths an arbitrary distribution to a degree that can be made arbitrarily small, as expressed by the equation p(x) = \int p(z) N(x | f(z), \sigma^2) dz. The role of a loose lower bound in this context is not adequately explained in the paper, warranting further clarification.