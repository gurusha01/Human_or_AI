This paper proposes a compelling explanation for the presence of a foveal area in the primate retina, suggesting that optimizing receptive field location and size in a simulated eye that makes saccades can minimize classification error for images with variable-size and variable-location subimages. 
To strengthen this argument, the authors could utilize more realistic image data and establish a more direct correlation with the number, receptive field sizes, and eccentricities of retinal cells in species like the macaque. However, this would require identifying a biologically plausible loss function that supports their claim, posing a significant challenge.
Additionally, the authors could enhance their argument by discussing the timescales involved, as the density of the foveal center likely depends on the number of allowed saccades and the size of target sub-images, ultimately impacting overall classification accuracy.
The persistently high classification error rate of 24% for dataset 2 raises concerns about the model's performance, suggesting that it may not be functioning as intended. The paper's central argument assumes that the model can be trained to be a competent classifier, but if alternative training strategies or models yield better results, it prompts questions about why the human visual system does not employ similar mechanisms, given the same evolutionary pressures.
The comparison between the model with zooming capabilities and the translation-only model yields unexpected results, with the former outperforming the latter on dataset 1 and tying on dataset 2, despite the zooming model being seemingly better suited for the variable-size target images in dataset 2. This, combined with the high error rate on dataset 2, leads to speculation that one or both models may not be reaching their full potential, which could undermine the paper's primary claim.
While comparing this model to other attention models, such as spatial transformer networks or DRAW, may not be directly relevant to the paper's main point, it could address concerns about suboptimal training or model parameterization, providing a more comprehensive understanding of the results.