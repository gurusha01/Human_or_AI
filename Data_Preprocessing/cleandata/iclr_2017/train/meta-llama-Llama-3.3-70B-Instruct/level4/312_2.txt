This manuscript proposes a novel approach to discovering optimal neural network architectures utilizing an actor-critic framework. By representing deep neural networks (DNNs) as variable-length sequences, the method leverages reinforcement learning (RL) to identify the target architecture, where the architecture itself serves as the actor. Within this RL context, node selection is framed as an action, while the evaluation error of the resulting architecture is treated as the reward signal. A two-layer, auto-regressive LSTM is employed to function as both the controller and the critic. The efficacy of this method is demonstrated through its application to two distinct problems, with comparisons drawn against multiple human-designed architectures.
The paper is highly engaging and addresses a significant challenge in the field, namely the difficulty of manually selecting optimal architectures and the uncertainty surrounding how far from optimal hand-designed networks may be. The methodology presented is innovative, and the authors provide a comprehensive and detailed explanation, incorporating necessary improvements. The datasets used for testing effectively showcase the method's capabilities, and the comparison between generated and human-designed architectures yields intriguing insights. The manuscript is characterized by its clarity and accessibility, with a thorough review of related literature that highlights both the coverage and contrast with existing works.
To further enhance the paper, it would be beneficial to include data on the training time required and an analysis of the correlation between the resources needed for training and the resultant model's quality. Additionally, exploring the performance of human-bootstrapped models and their potential integration could offer valuable insights. Overall, the paper is excellent and presents a compelling contribution to the field, making it a highly interesting read.