The authors demonstrate a method for normalizing the hidden states of a Long Short-Term Memory (LSTM) network to preserve means and variances, and provide an analysis of the method's gradient behavior. The experimental results suggest that the proposed approach is comparable to other similar methods.
Key points to consider:
1) The writing quality is inconsistent, with some sections requiring improvement, as highlighted in the list of specific errors at the end of this review.
2) While the experimental results indicate slight improvements, the statistical significance of these improvements is difficult to determine, which may be attributed in part to the reliance on previously published results for the Penn Treebank (PTB) dataset. Furthermore, weight normalization appears to be a viable alternative, offering similar performance and runtime, but with arguably lower implementation complexity. The authors could have provided a more thorough comparison to help practitioners and researchers assess the value of the proposed method.
3) The analysis presented in Section 4 is commendable, and the authors are to be applauded for their effort in conducting this analysis.
Specific areas for improvement:
- "maintain" should be used correctly
- "requisites" is the correct spelling
- The indefinite article "an" should be used before "LSTM"
- The statement "The gradients of ot and ft are equivalent to equation 25" is incorrect, as gradients cannot be equivalent to an equation
- "because" is the correct spelling
- The equation on page 5 contains an error, with one instance of γx > γh being incorrect.