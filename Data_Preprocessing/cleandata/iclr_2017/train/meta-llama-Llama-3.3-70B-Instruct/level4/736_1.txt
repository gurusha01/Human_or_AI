This paper presents a straightforward domain adaptation approach by applying batch normalization separately to each domain.
Strengths:
The proposed technique is exceptionally simple, making it easily comprehensible and applicable.
Experimental results indicate that this method performs comparably to, or even surpasses, existing techniques on conventional domain adaptation tasks.
The analysis in section 4.3.2 reveals that only a minimal number of target domain samples are required for network adaptation.
Weaknesses:
The approach lacks novelty, as it can be argued that the method is overly simplistic and more of an intuitive extension of using batch normalization in domain adaptation rather than a distinct methodology. The alternative approach of utilizing source domain batch normalization statistics for target domain examples seems less intuitive. It appears that this alternative is employed in the Inception BN results presented in Tables 1 and 2.
The analysis in section 4.3.1 seems redundant, serving only as a sanity check, as the KL divergence between distributions should theoretically be zero when each distribution is normalized to N(0,1) through batch normalization.
Section 3.3 fails to convey a clear point, requiring further clarification.
Overall, while the paper's contribution may not be particularly novel, its simplicity is an advantage, especially given its competitive performance with prior work on standard benchmarks, aligning with the tradition of "Frustratingly Easy Domain Adaptation." If accepted, it is recommended that sections 4.3.1 and 3.3 be revised or removed for enhanced clarity in the final version.