This paper proposes an intriguing incremental approach to exploring novel convolutional network hierarchies, building upon a baseline network that has achieved satisfactory recognition performance. 
The authors present experimental results on the CIFAR-100 and ImageNet benchmarks, demonstrating the transformation of various ResNet models into higher-performing models with moderately increased computational requirements.
Although the baseline models employed in this study are not as robust as those reported in existing literature, the paper asserts a substantial reduction in error rates for both ImageNet and CIFAR-100 benchmarks.
The core concept of this paper involves re-expressing convolutions as multiple convolutions while expanding the filter count, a strategy that surprisingly yields improvements over the baseline model.
However, the paper lacks experimental evidence to support some fundamental aspects of network morphing, raising essential questions:
- How do the morphed networks' quality compare to those with identical topology trained from scratch?
- What is the relationship between the incremental training time after morphing and that of a network trained from scratch?
- What is the source of the additional computational cost in the morphed networks?
- Why do the baseline ResNet models' performance lag behind those reported in literature and on github (e.g., the github ResNet-101 model has a 6.1% top-5 recall, whereas the paper reports 6.6%)?
More evidence addressing these points is necessary to assess the validity of the paper's claims.
The paper is well-written and easily comprehensible, but the absence of crucial evidence and weaker baselines diminish its convincing power. 
I would consider revising my evaluation upward if the authors provided additional experimental evidence supporting the paper's primary message, particularly regarding the aforementioned points.