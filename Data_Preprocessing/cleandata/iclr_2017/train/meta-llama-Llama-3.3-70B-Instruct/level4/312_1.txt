The paper presents a methodology where, upon completing the training of the controller RNN, the best RNN cell is selected based on the lowest validation perplexity, followed by a grid search over hyperparameters such as learning rate, weight initialization, dropout rates, and decay epoch. The optimal cell identified through this process is then evaluated with multiple configurations and sizes to enhance its capacity.
To facilitate replication and build upon this work, it would be beneficial to include specific details regarding the hyperparameters used, such as the type of dropout (e.g., recurrent dropout, embedding dropout), as this information is crucial for precise replication and further research. Similar to the approach outlined in "Recurrent Neural Network Regularization" by Zaremba et al. (2014), providing a baseline set of hyperparameters would significantly contribute to advancing the field by reducing the need for trial and error in future studies.
This level of detail would also be advantageous for other experiments mentioned, such as the character language model. The paper addresses a critical aspect of automating architecture search, a technique that, despite being currently computationally intensive, is likely to become more efficient as technological advancements continue.
By exploring both conventional vision and text tasks across numerous benchmark datasets, the paper demonstrates potential gains from venturing beyond the standard RNN and CNN search spaces. Although applying the technique to additional datasets would be desirable, the current scope is sufficient to demonstrate the technique's competitiveness with, and potential to surpass, human architectural intuition. This approach also suggests a viable method for tailoring architectures to specific datasets without requiring manual engineering at each stage.
Overall, this is a well-crafted paper on a compelling topic, presenting robust results. Based on its merits, I strongly recommend its acceptance.