This manuscript presents CoopNets, a novel algorithm that leverages a Deep-Energy Model (DEM), referred to as the "descriptor", in conjunction with an auxiliary directed Bayes net, termed the "generator". The descriptor is trained using standard maximum likelihood estimation, with Langevin MCMC employed for sampling purposes. Conversely, the generator is trained to produce likely samples under the DEM through a single, feed-forward ancestral sampling step, thereby enabling the potential to bypass costly MCMC sampling and facilitating "cooperative training".
Although the underlying concept is intriguing and innovative, the experimental results fail to provide sufficient validation. Notably, two out of the three experiments lack a train/test split and neglect standard protocols for texture generation, as outlined in [R1], while also utilizing excessively small datasets. As a consequence, these experiments primarily demonstrate the model's propensity for overfitting. Furthermore, the third experiment, which involves in-painting tasks, is hindered by the absence of meaningful baselines, including VAEs, RBMs, and DEM, rendering it challenging to assess the benefits of the proposed approach.
In future revisions, the authors should consider experimentally addressing the following questions. Firstly, what are the implications of omitting the rejection step in Langevin MCMC, and how does this impact training with or without this step? Secondly, what influence does the generator exert on the burn-in process of the Markov chain, and how can this be illustrated through sample auto-correlation? Thirdly, what are the consequences of approximating the training of the generator using ({\tilde{Y}, \hat{X}) instead of ({\tilde{Y}, \tilde{X})? Conducting comparative experiments would provide valuable insights.
The manuscript would also benefit from a rewrite focused on enhancing clarity, rather than relying on exaggerated claims ("pioneering work" in reference to closely related, non-peer-reviewed research) and overly elaborate prose ("tale of two nets"). For instance, the authors fail to specify the exact form of the energy function, a glaring omission that warrants attention.
The strengths of the manuscript include:
+ An interesting and novel idea
However, the weaknesses comprise:
- Inadequate experimental protocols
- Absence of meaningful baselines
- Lack of diagnostic experiments
Reference:
[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of experts.