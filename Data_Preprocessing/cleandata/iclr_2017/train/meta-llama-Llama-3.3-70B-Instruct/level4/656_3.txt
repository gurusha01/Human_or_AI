I appreciate the thorough response and clarifications provided.
The manuscript presents a novel approach that utilizes a scattering transform as the foundation of a deep network, leveraging its desirable geometric properties, such as local invariance to deformations, which can be viewed as a form of regularization. The upper layers of the network are then trained for a specific supervised task, effectively creating a model that integrates a standard deep convolutional network with the scattering transform. The results on CIFAR 10 and 100 demonstrate that this method achieves competitive performance with state-of-the-art baselines.
I find the paper to be highly intriguing, as the concept of combining these representations appears to be a logical and promising direction. To my knowledge, this is the first study to successfully merge predefined, generic representations with modern CNN architectures, yielding performance comparable to high-performing methods. Although the current state-of-the-art (Resnets and variants) achieves substantially higher performance, I believe this work effectively conveys its key message.
The paper convincingly demonstrates that lower-level invariances can be derived from analytic representations (scattering transform), streamlining the training process by reducing the number of parameters required and enabling faster evaluation. The benefits of this hybrid approach become particularly significant in low-data regimes.
The author's argument that the scattering initialization prevents instabilities in the initial layers, due to the non-expansive nature of the operator, suggests that the model may be more robust to adversarial examples. An empirical evaluation of this aspect would be highly valuable, as it could have significant practical implications. Can this hybrid network be deceived by adversarial examples? If so, the use of scattering initialization would become even more appealing.