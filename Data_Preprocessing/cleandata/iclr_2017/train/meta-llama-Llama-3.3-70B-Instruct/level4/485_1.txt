The manuscript presents an examination of the capacity of deep networks utilizing ReLU functions to represent specific types of low-dimensional manifolds, namely "monotonic chains of linear segments," which essentially comprise sets of intersecting tangent planes. The authors propose a construction that efficiently models such manifolds within a deep network and provide a fundamental error analysis of the resulting construction.
Although the presented results appear to be novel, they are somewhat expected (1) given the existing understanding of the representational power of deep networks and (2) considering the selected deep network architecture and data structure are highly compatible. However, I have three primary concerns regarding the results presented in this paper:
(1) The past decade has seen significant research on learning data representations from sets of local tangent planes, including local tangent space analysis by Zhang & Zha (2002), manifold charting by Brand (2002), and alignment of local models by Verbeek, Roweis, and Vlassis (2003). Surprisingly, none of these studies are referenced in the related work section, despite their apparent relevance to the analysis presented. A comparison between these traditional techniques and the deep network trained to produce the embedding of Figure 6 would be intriguing, as it could provide insight into the inductive biases introduced by the deep network. Does the deep network learn better representations than non-parametric techniques due to its inductive biases, or does it learn worse representations due to the non-convex loss being optimized?
(2) It is challenging to see how the analysis extends to more complex data, where local linearity assumptions on the data manifold are invalid due to the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not purely ReLU networks. For instance, most modern networks employ a variant of batch normalization, which seems to invalidate the presented analysis.
(3) The error bound presented in Section 4 appears to be impractically loose, as the upper bound on the error is exponential in the total curvature, a quantity that is typically large in most practical settings. This is evident in the analysis of the Swiss roll dataset, where the authors acknowledge that the "bound for this case is very loose." The fact that the bound is already so loose for this relatively simple manifold raises concerns that the error analysis may not provide significant insights into the representational power of deep nets.
I would encourage the authors to address issue (1) in the revision of the paper. While issues (2) and (3) may be more challenging to address, it is essential that they are addressed for this line of research to have a meaningful impact on our understanding of deep learning.
Minor comments:
- The authors only reference fully supervised siamese network approaches in prior work, whereas their approach is unsupervised. It is worth noting that the authors are not the first to study unsupervised representation learners parametrized by deep networks, as other notable examples include deep autoencoders (Hinton & Salakhutdinov, 2006) and work on denoising autoencoders from Bengio's group, as well as parametric t-SNE (van der Maaten, 2009).
- What loss function do the authors use in their experiments? The description of using "the difference between the ground truth distance ... and the distance computed by the network" seems unusual, as it would encourage the network to produce infinitely large distances to achieve a loss of minus infinity. Is the difference squared?