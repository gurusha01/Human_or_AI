This paper presents a novel approach to iteratively refine the output of an existing machine translation system by detecting potential errors and suggesting corrections using an attention-based model, drawing inspiration from the presumed workflow of human translators.
The paper is engaging and creative, but I have reservations about the underlying methodology, which involves using a machine learning approach to identify and correct the predictions of another method or itself. Specifically, if the new method is superior, it is unclear why it should not be used as the primary approach from the outset. Furthermore, when a method is used to correct its own predictions, it is uncertain why it would be more likely to detect past mistakes and correct them, rather than introducing new errors, unless there is a compelling reason to believe that an iterative approach will converge to a better solution over multiple epochs.
This paper does not adequately address these concerns. Notably, the authors acknowledge that the probability of accurately identifying a word as a mistake is relatively low, at 62%, which, although better than a random baseline, lacks a more meaningful comparison, such as contrasting the existing system with a more powerful convolutional model. The oracle experiments are also unconvincing, as they merely demonstrate that improving a translation is straightforward when existing mistakes are known, but more challenging when they are not.
While I appreciate the paper's overall concept, to convincingly demonstrate the benefits of iterative improvement, I believe it is essential to include more robust baselines. Specifically, it would be necessary to show that an iterative refinement scheme can outperform a system closely matched to the attention-based model, both when used independently and in combination with a phrase-based machine translation (PBMT) system. Additionally, it is crucial to demonstrate that the PBMT system is not simply acting as a regularizer for the attention-based model.
Some minor comments include: the notation can be overly complex at times, such as the use of |F^i| to denote the length of the slice, which seems unnecessary since F is a matrix. Furthermore, in the discussion in section 4, there appears to be a mismatch between the training and test conditions; it is unclear whether this issue can be addressed.