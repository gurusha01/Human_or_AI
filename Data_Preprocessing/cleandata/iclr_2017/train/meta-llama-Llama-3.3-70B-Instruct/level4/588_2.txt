The thoughtful author responses have successfully addressed my primary concerns, and the provided GitHub links for data and code will greatly facilitate result reproduction. The revision has resolved several issues, including the addition of new results, prompting me to upgrade my rating from 5 to 6 and recommend paper acceptance.
This paper explores the application of deep neural networks for detecting and localizing extreme weather events in simulated weather data, drawing parallels with object detection in computer vision. The input data, akin to 2D "images" or 3D "videos," consists of multichannel spatial weather data, and the output comprises a bounding box for spatial-temporal localization and a class label for the weather event type. Unlike standard object detection, this task involves heterogeneous channels and scarce labeled data.
The proposed deep net approach, although straightforward, is reasonable and grounded in similar computer vision methods. While proposal-based systems, such as Faster-RCNN, are currently prevalent in vision, the proposed method serves as a suitable starting point. The lack of innovation in the detection system is mitigated by its valid application of computer vision concepts to the task at hand.
The authors propose both supervised and semi-supervised approaches, with the latter incorporating reconstruction loss as regularization. The losses employed are standard, but the terminology is somewhat confusing, as the "semi-supervised" loss utilizes all labels from the "supervised" loss and additional reconstruction loss, making it stronger.
The paper is generally easy to follow, but the notation requires improvement. For instance, the description of the loss above equation 5 is inaccurate, and the reference to "figure 4 and 4" is unclear. These minor issues should be addressed to enhance the paper's clarity.
The primary concern with the paper lies in its experimental results. The presentation of only one figure and table (figure 4 and table 4) is insufficient, and the metrics used are not defined. The comparison between 2D and 3D models, as well as supervised and semi-supervised approaches, raises questions about the consistency of the results. Furthermore, the absolute quality of the results and the number of events in the training and testing data are unclear. The experiments are sparse, and ablation studies and more in-depth discussions are lacking. To ensure reproducibility, it is essential to open-source the data or provide a means to replicate the experimental setting.
A minor point of contention is the use of both classification loss and "objectness" loss, which is unconventional. Typically, objectness loss is employed in two-stage object proposal systems, and its use alongside classification loss requires justification, potentially through experimental validation.
Overall, this is a borderline paper that applies computer vision techniques to a relatively unexplored domain. Although the algorithmic novelty is limited, the application of reconstruction loss to improve results in data-sparse settings is intriguing. However, the experimental results are inconclusive, and the validation is insufficient. With improved experiments and more meticulous writing, this paper has the potential to become a decent contribution.