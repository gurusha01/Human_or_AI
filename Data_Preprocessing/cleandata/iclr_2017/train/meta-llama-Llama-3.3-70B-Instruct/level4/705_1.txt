This manuscript builds upon the NIPS 2016 publication "Unsupervised learning of spoken language with visual context" by implementing the proposed future work, which involves acoustic segmentation and clustering to learn a lexicon of word-like units using the system's learned embeddings. The provided analysis is intriguing and aligns with a promising research trajectory.
However, my primary concern lies in the paper's novelty. The work appears to be a straightforward extension of an existing model, which, while acceptable, necessitates a more comprehensive analysis to justify its contribution. Currently, the authors seem to merely demonstrate the capabilities of the NIPS model with minor enhancements, rather than providing a more in-depth examination. To strengthen the analysis, I would suggest comparing various segmentation approaches in both audio and image modalities, such as exploring the outcomes when perfect segmentation in both modalities is achievable. Additionally, investigating the learned representations and evaluating their performance on multi-modal semantics tasks could yield more insightful results.
Overall, the paper is well-structured and explores an important research direction, emphasizing the need to examine what models learn. This line of inquiry is crucial, and the manuscript serves as a good example of the types of questions that should be posed. Nevertheless, I have reservations regarding the paper's novelty and the depth of the questions explored, which may impact its competitiveness for ICLR, potentially positioning it as a borderline submission.