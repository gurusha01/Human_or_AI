The authors propose an online learning approach for sum-product networks, which assumes Gaussian marginal distributions and learns both parameters and structure in an online setting. This is achieved through a recursive update procedure that reweights nodes contributing most to the likelihood of the current data point, while structure learning is facilitated by either merging independent product Gaussian nodes into multivariate leaf nodes or creating a mixture over the two nodes when the multivariate node would be too large.
Although the method shows promise by being applied to larger datasets in terms of datapoints, the number of variables remains relatively small. In contrast, current benchmarks for continuous density modeling with neural networks, such as NICE and Real-NVP, can be scaled to both large numbers of datapoints and variables. Similarly, intractable methods like GAN, GenMMN, and VAE also possess this scalability.
A significant concern for the ICLR audience is the reliance on SPN-specific datasets that are not commonly used in the deep learning generative modeling literature. Furthermore, using GenMMN as a baseline may not be effective in bridging the gap to the neural community, given its Parzen-window based likelihood evaluation. A more suitable approach might involve using a simple VAE-type model to establish a lower bound on the likelihood or employing a model like Real-NVP. Additionally, evaluating the method's scalability to large numbers of observations and instances, such as modeling MNIST, would be insightful.
While neural network density models can scale to large datasets, it is unclear whether this method exhibits similar "horizontal" scalability. The strengths of SPNs, including tractable marginal and conditional queries, are not evaluated or compared in this work. However, exploring applications like imputing unknown pixels or color channels in images could be a fruitful direction, as there is currently a lack of high-performing tractable models for such tasks.
Despite the disconnect from other ICLR generative modeling literature, the algorithm appears simple, intuitive, and convincingly outperforms the previous state of the art for online SPN structure learning. Using VAE as a baseline for continuous data could provide a more meaningful comparison to neural network approaches. Moreover, combining sum-product networks with deep latent variable models as an observation model or posterior could yield a powerful combination.
To increase the relevance of SPNs to the ICLR probabilistic modeling community, additional work may be necessary. As a non-expert in SPNs, it is unclear whether this paper adequately addresses this gap. Nevertheless, the proposed algorithm seems effective for online structure induction, and its scalability is an important aspect of recent representation learning research. While the paper is suitable for publication, incorporating the suggested additions could more effectively bridge the gap with other deep generative modeling literature.