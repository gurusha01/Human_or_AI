This paper presents a novel approach to accelerating attention-based memory addressing in memory-augmented networks, such as Neural Turing Machines (NTMs) and memory-augmented neural networks (memNNs), by utilizing a hierarchical softmax.
The proposed model constructs a hierarchical softmax over the input sequence and, at each time step, performs a discrete search to identify the most relevant input for predicting the next output. The corresponding embedding of the selected input is then used to update the state of a Long Short-Term Memory (LSTM) network, which subsequently generates the output. Additionally, the embedding of the utilized input is updated via a WRITE function, implemented as an LSTM that takes the hidden state of the other LSTM as input. The model's discrete component, namely the SEARCH operation, necessitates training using the REINFORCE algorithm. The experimental evaluation focuses on various algorithmic tasks, including search and sorting.
A key benefit of replacing the traditional softmax with a hierarchical softmax is the reduction in computational complexity from O(N) to O(log(N)) during inference. It would be interesting to investigate whether this complexity reduction enables the model to tackle problems that are significantly larger than those addressed using the full softmax. However, the authors only evaluate their approach on short sequences of up to 32 tokens, which is a relatively small scale.
The model relies on a complex search mechanism that can only be trained using REINFORCE, which may be effective for small and simple sequences but requires further investigation to determine its performance on larger problem sizes.
In summary, while the concept of replacing the softmax in attention mechanisms with a hierarchical softmax is intriguing, the current work is not entirely convincing. The approach may be perceived as unnatural, challenging to train, and potentially difficult to scale. Furthermore, the experimental section is somewhat lacking, which diminishes the overall impact of the paper.