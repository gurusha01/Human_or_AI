The authors' work involves evaluating various tasks that aim to identify whether a given test example falls outside the domain or is likely to be misclassified. This is achieved by analyzing the statistics of the softmax probability for the most likely class, which, although not a strong confidence measure on its own, exhibits distinct differences between out-of-domain and in-domain examples, allowing for their identification with a degree of certainty.
My observations and suggestions are as follows:
1. Given that the AUROC/AUPR metric is threshold-independent, as noted by the authors, it remains unclear whether the thresholds corresponding to a specific operating point (e.g., a true positive rate of 10%) would be consistent across different datasets. It would be valuable to assess the sensitivity of these thresholds to various test sets or splits of the test set, as this information is crucial for applying thresholds determined from a held-out set to unseen data, where threshold selection is necessary.
2. The performance is evaluated using AUROC/AUPR, with models being compared to a random baseline. However, it is challenging to gauge the improvement of the proposed classifier over the baseline solely based on differences in AUC/AUPR. Reporting the statistical significance of these differences would provide additional insight, although the differences appear substantial in most cases.
3. The evaluation methodology in the speech recognition experiments presented in Section 3.3 was not entirely clear. Specifically, in Table 9, it is unclear whether an "example" refers to an entire utterance or a single speech frame. Assuming each example represents an utterance, it would be helpful to clarify whether the softmax probabilities correspond to the probability of the entire phone sequence, potentially obtained through Viterbi decoding by multiplying local probability estimates.
4. The decision to exclude the blank symbol's logit in Section 3.3 warrants further explanation. What necessitates this exclusion?
5. As previously mentioned, comparing the proposed model's performance to a simple generative baseline, such as GMM-HMM, particularly in the context of speech recognition, would have been insightful. This comparison would serve as a meaningful indicator of the proposed model's capability to detect out-of-domain examples relative to the baseline.