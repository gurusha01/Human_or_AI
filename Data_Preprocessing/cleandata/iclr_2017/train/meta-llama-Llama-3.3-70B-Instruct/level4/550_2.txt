This study presents a novel regularization approach for denoising autoencoders, designed to enhance robustness in the encoding phase against input perturbations. The proposed regularization term is justified as a means to minimize the conditional entropy of the encoding given the input, thereby promoting a more stable representation. The modified denoising autoencoders are assessed on both synthetic datasets and the MNIST dataset, alongside standard autoencoders and denoising autoencoders. However, the work bears similarities to existing autoencoder extensions, such as contractive autoencoders, which are notably absent from the comparative analysis. Furthermore, the experimental section requires refinement, with additional details necessary to facilitate a clearer understanding of the presented figures and results.