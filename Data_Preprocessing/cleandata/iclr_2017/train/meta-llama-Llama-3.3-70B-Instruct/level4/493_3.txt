This paper presents an alternative approach to conditional maximum log likelihood for training discriminative classifiers, arguing that the traditional method provides an upper bound on the Bayes error that becomes less effective during training. Instead, the authors propose optimized bounds computed through an iterative algorithm, with extensions to regularized losses and a basic form of policy learning, as demonstrated through experiments on various datasets.
A notable aspect of this contribution is its re-examination of a widely accepted methodology for classifier training. The concept appears sound, and some results support its validity. However, this remains preliminary work, and further development of these ideas is desirable. Overall, the paper suffers from a lack of cohesion and depth, particularly in the section on policy learning, which feels disconnected from the rest of the paper, and the connection to reinforcement learning is not well-motivated in the provided examples, such as ROC optimization and uncertainty analysis. Furthermore, the experimental section requires revision, including the addition of legends to identify curves in the figures, which would facilitate a clearer understanding of the results.