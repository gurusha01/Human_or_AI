This paper presents a study on StarCraft micro-management tasks, which involve controlling individual units during battles, a challenging problem for current Deep Reinforcement Learning (DeepRL) methods due to the high-dimensional and variable action spaces. The complexity of these action spaces renders simple exploration strategies, such as epsilon-greedy, ineffective.
To address this issue, the authors propose a novel algorithm, ZO, which integrates concepts from policy gradient methods, deep networks trained via backpropagation for state embedding, and gradient-free optimization. The algorithm is clearly explained and compared to existing baseline methods, demonstrating superior performance due to its structured exploration capabilities facilitated by gradient-free optimization.
The paper is well-written and introduces a novel algorithm to tackle a highly relevant problem. Following the success of DeepRL in learning within large state spaces, such as visual environments, there is a growing interest in applying Reinforcement Learning (RL) to more structured state and action spaces. The tasks presented in this work offer interesting environments for exploring these challenges.
To facilitate comparisons and further research, it would be beneficial for the authors to provide the source code or detailed specifications for their tasks and algorithm. Additionally, clarifying the details of raw inputs and feature encodings in Section 5, potentially by sharing the source code for the encoder, would enhance the reproducibility and comparability of this work.
While the paper discusses various approaches, a comparison with value-based methods that aim to improve exploration by modeling uncertainty, such as Bootstrapped DQN, is lacking. Including such a comparison would provide valuable insights into how these alternative approaches, which also promise enhanced exploration, stack up against the proposed algorithm.
Furthermore, exploring the potential of action embedding models, including energy-based approaches, could offer additional perspectives on tackling the challenges posed by high-dimensional action spaces. Discussing these aspects would enrich the paper and contribute to a more comprehensive understanding of the proposed solution's strengths and limitations.