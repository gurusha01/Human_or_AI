The manuscript presents a novel application of tensor factorization to linear models, enabling the consideration of higher-order interactions between variables in classification and regression problems while maintaining computational efficiency, with a linear complexity in terms of dimension. This is achieved through the utilization of the TT format, initially proposed by Oseledets in 2011. Furthermore, the authors suggest employing a Riemannian optimization scheme to explicitly account for the geometry of the tensor manifold, thereby enhancing convergence speed.
Overall, the paper is well-structured and introduces an intriguing application of the TT tensor format to linear models, complemented by the use of Riemannian optimization. In my opinion, this is a compelling contribution, given its broad potential applications across various machine learning algorithms.
However, I have several concerns regarding the experimental section, which I believe does not meet the standards set by the rest of the paper. Specifically, the number of experiments conducted on real datasets is limited, and the role of dropout in these datasets, as well as comparisons with other algorithms, are not adequately explored. Additionally, the authors fail to explicitly address the issue of selecting the optimal rank for the experiments. The experimental section appears to be a collection of preliminary tests, where various aspects are examined but not in a comprehensive manner.
In my assessment, the paper falls between a weak acceptance and a weak rejection. I do not consider it suitable for full acceptance primarily due to the shortcomings in the experimental setup. Nevertheless, if supplementary experiments can validate the effectiveness of the proposed approach, I believe the paper could receive significantly higher scores.
Some minor observations and suggestions include:
- Formula 2: It is worth noting that the parameters of the model in (1) can be learned through the approach outlined in (2), as well as through alternative methods depending on the specific approach being used.
- The bound of the rank by 2r, preceding formula 9, is discussed in Lubich et al., 2015.
- Following formula 10, the cost of N projections is stated to be O(dr^2(r+N)), but should it not be O(Ndr^2(r+1)) instead? This is because each element in the summation has a rank of 1, and the cost for each is O(dr^2(r+TT_rank(Z)^2)), where TT-rank(Z) equals 1. Could you clarify this point?
- In section 6.2, the random initialization is mentioned to freeze convergence, which seems interesting but lacks motivation. Do you have any insights into this phenomenon?
- Section 6.3 discusses the adoption of dropout. Could you elaborate on the specific advantages it offers in the context of exponential machines and whether it was applied to real datasets?
- How do you determine the value of r_0 in your experiments? Is a validation set used for this purpose?
- In section 7, it is noticeable that the variable x1 x2 is not included among the variables. Could you explain the reasoning behind this omission?
- Section 8 contains a typo in the word "experiments."
- Section 8.1 features a sentence with a potential language error: "We simplicity, we binarized."
- Section 8.3 states that "we report that dropout helps," which seems like a general statement that has only been tested on a synthetic dataset.
- Finally, in section 8.5, it would be beneficial to provide additional results for this dataset, such as training and inference times, or comparisons with other algorithms, to further substantiate the findings.