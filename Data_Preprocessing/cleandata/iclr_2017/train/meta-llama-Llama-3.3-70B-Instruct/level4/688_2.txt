This paper addresses the challenge of reinforcement learning with a limited number of policy updates, presenting a well-motivated problem and an intriguing modification to the PoWER algorithm. The introduction of variational bounds on the value function, as seen in lemmas 3 and 4, is noteworthy and independently interesting. The authors also provide numerical results for the cartpole problem and an online advertising problem using real data, contributing to the paper's overall strength and clarity. However, my primary concern is whether the paper's focus and assumptions, particularly the log-concavity assumption that underpins the model, align with the scope of ICLR, as this assumption may limit the model's applicability to more complex scenarios where representation learning is essential.
Additional observations include:
- The numerical experiments section lacks comparative baselines, which, although acknowledged as challenging in this context, would have been enhanced by even a simple, well-justified baseline. Considering the relative simplicity of the cartpole problem and the privacy of the advertising dataset, generating a synthetic advertising dataset for comparison could have added value.
- The use of control variates as constant scalars was perplexing; if intended as constant baselines, it is unclear why they are treated as hyperparameters rather than being learned or estimated.
- The section on constrained optimization is interesting but appears somewhat disconnected from the rest of the paper. Its applicability to the online advertising problem is not explored in the experimental section, and a reference to the literature on constrained MDPs, which explores similar Lagrangian concepts, could further enrich this discussion.