This study provides a theoretical foundation for reutilizing input word embeddings in the output projection layer by introducing an additional loss function. This loss function aims to reduce the discrepancy between the predicted distribution and an estimated true data distribution, thereby effectively smoothing the input labels. Although this setup is promising, the estimation of the true data distribution appears to be tailored to justify the weight tying in equations 3.6 and 3.7.
The rationale behind using the same projection matrix, L (renamed as L' for clarity), in both equations 3.6 and 2.1 is not immediately clear. Alternatively, L' could be derived from word2vec embeddings trained on a large dataset or learned as an extra set of parameters. If L' is a newly learned matrix, the outcome in equation 4.5 seems to suggest using a separate matrix for the output projection layer, which is a common practice.
The experimental results are noteworthy and substantiate the approximate derivation presented in section 4, particularly the distance plots illustrated in figure 1.
Minor comments:
- In the abstract, the third line should read "where the model" instead of "where model".
- In section 7, the second line should be revised to "into the space" from "into space".
- Furthermore, it appears that the right-hand side of equation 3.5 should be $\sum \tilde{y{t,i}}(\frac{\hat{y}t}{\tilde{y{t,i}}} - ei)$, which warrants verification.