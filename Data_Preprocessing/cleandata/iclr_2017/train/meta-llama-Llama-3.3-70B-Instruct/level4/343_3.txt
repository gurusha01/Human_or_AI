This manuscript presents an innovative approach to language modeling, applicable to both programming languages and natural languages. The proposed method leverages a program synthesis algorithm to explore program space, utilizing count-based weight estimation. This diverges from traditional neural network-based methods, which rely on gradient descent and are consequently slow to estimate. In contrast, count-based methods, such as standard n-gram models, are limited by their simplicity, failing to capture large contexts and scaling poorly as context increases. The introduced approach employs MCMC to synthesize programs, learning context-sensitive probabilities through count-based estimation, thereby achieving both speed and the ability to model long-range contexts.
Experimental evaluations on a programming language dataset, specifically the Linux kernel corpus, demonstrate that this method significantly outperforms both LSTM and n-gram language models. Furthermore, experiments on the Wikipedia corpus indicate that the method is competitive with, although not superior to, state-of-the-art models. Notably, both estimation and query times are substantially improved compared to LSTM language models and are competitive with n-gram language models.
The suitability of this paper for ICLR is questionable due to the conference's emphasis on neural network-based approaches. Nevertheless, considering the importance of diversity and novelty, it is argued that such unconventional papers should be considered for acceptance to foster innovation. This work has the potential to inspire further research into the integration of program synthesis and machine learning techniques, a theme that garnered significant attention at NIPS 2016.
Strengths
1. The approach is novel and innovative.
2. The results presented are promising.
Weaknesses
1. Certain crucial algorithmic details are omitted from the manuscript. It is recommended that these be included, at the very least in an appendix, to ensure comprehensiveness.
Suggestions
1. The authors are encouraged to include n-gram results in the table for Wikipedia results to provide a more comprehensive comparison.