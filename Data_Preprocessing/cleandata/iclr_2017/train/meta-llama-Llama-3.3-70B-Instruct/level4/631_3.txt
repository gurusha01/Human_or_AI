This paper presents a novel approach to incorporating global context information into convolutional neural networks (CNNs) by utilizing a cascade of paired 1D recurrent neural networks (RNNs) as a module. The experimental evaluation focuses on image classification and semantic segmentation tasks.
The strengths of the paper include:
- The clarity and readability of the manuscript, making it easy to follow and understand.
- The provision of sufficient details, enabling potential reproduction of the results with or without access to the source code.
- The exploration of using 1D RNNs within CNNs, a topic that warrants further investigation in the literature.
However, several concerns are noted:
(1) The contributions of the paper are relatively minor compared to existing works, such as those by Bell et al.
(2) The actual implementation of the proposed L-RNN module falls short of the expectations set by the introduction.
(3) The image classification experiments are not convincing and fail to demonstrate the effectiveness of the proposed approach.
Regarding points (1) and (2), the introduction highlights two key differences between the proposed approach and Bell et al.: the treatment of the L-RNN module as a general block that can be inserted into any layer of a modern architecture, and the ability to formulate the L-RNN to be inserted into a pre-trained fully convolutional network (FCN) with zero recurrence matrices. However, the experimental sections do not fully realize these contributions, as the L-RNN module is only placed towards the end of the network, similar to Bell et al. The paper lacks a comparison to the design from Bell et al., raising questions about the advantages of the proposed design and its performance relative to existing methods. Furthermore, the introduction suggests integrating the L-RNN module earlier in the network, but this is not explored in the experiments.
The second difference, although more substantial, still does not constitute a significant improvement in the reviewer's opinion. While the method of integration proposed in the paper may be more elegant, it is noted that Bell et al. also integrate 1D RNNs into a pre-trained VGG-16 model.
Regarding point (3), the classification experiments on CIFAR-10 are deemed insufficient, as this dataset is not representative of more complex tasks, and methods that perform well on CIFAR-10 may not generalize to other tasks. The lack of results on ImageNet, a more challenging and representative dataset, raises concerns about the model's ability to learn generalizable features. Additionally, the presentation of the CIFAR experiments makes it difficult to assess the actual benefit of the L-RNN module, as a direct comparison of models with and without L-RNN is not provided.
A minor suggestion is to revise Figure 4, as the current version is difficult to read due to the pixelated rounded corners on the yellow boxes.