This paper demonstrates that Batch Normalization (BN), which is not directly applicable to Recurrent Neural Networks (RNNs), can be effectively utilized with Long Short-Term Memory (LSTM) networks by applying the operator separately to the hidden-to-hidden and input-to-hidden transitions. The authors provide experimental evidence suggesting that this approach yields improved generalization error and accelerated convergence.
The paper is well-structured and clearly conveys its central idea. 
However, several limitations are noted:
i) The experimental evaluation is restricted to a narrow range of datasets and statistical assumptions, notably excluding continuous data and only considering autoregressive generative modeling.
ii) The hyperparameter settings remain largely unchanged across experiments, raising concerns that they may have been selectively chosen to favor the proposed method. For instance, it is possible that adjusting the learning rate could have achieved comparable convergence speeds for standard LSTM, which is not investigated.
In conclusion, the experimental methodology is flawed and fails to provide sufficient evidence to support the claims made. A more comprehensive exploration of the hyperparameter space could help to alleviate these concerns and strengthen the findings.