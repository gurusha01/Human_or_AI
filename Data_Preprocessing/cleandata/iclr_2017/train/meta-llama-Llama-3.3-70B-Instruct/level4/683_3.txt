The submitted paper presents a methodology for incrementally expanding a residual network through the addition of layers based on a boosting criterion. 
A significant obstacle to publication is the lack of robust empirical validation. The tasks examined are relatively small in scale, even by 2016 standards, and the use of MNIST with a convolutional network is no longer a challenging or meaningful test. Furthermore, the paper fails to provide a comparison with existing literature, and the results on CIFAR-10 do not surpass simple, single-network baselines previously published (for instance, Springenberg et al, 2015 achieved 92% without data augmentation). It is likely that a basic ResNet result also outperforms the presented results. In contrast, the CIFAR-100 results are somewhat noteworthy, as they exceed typical expectations, which is not surprising given the expected performance of ensembles when labeled training data is scarce, with only a few hundred examples per label. However, the absence of simple data augmentation schemes, commonly employed in both CIFAR-10 and CIFAR-100, and other regularization techniques, makes the proposed iterative augmentation scheme seem unnecessarily complex.
A more compelling case for this method could be made if it were positioned as a solution for datasets with limited labeled data, where data augmentation is not straightforward (although, for most image-related applications, random crops and reflections are straightforward and effective). Nevertheless, this would require different benchmarks and comparisons to simpler methods, such as data augmentation, dropout (particularly in light of the ensemble interpretation), and other relevant techniques.