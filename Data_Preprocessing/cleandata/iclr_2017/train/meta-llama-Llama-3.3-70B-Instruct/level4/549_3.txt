I appreciate the authors' efforts in providing clarifications and responses to my initial concerns.
The revised presentation of the multi-stage model is now more transparent and easier to follow.
Strengths:
+ The formulation of the sparse coding problem using cosine loss enables a single-pass solution, which is a notable advantage.
+ The energy-based approach facilitates bi-directional coding, effectively integrating top-down and bottom-up information into the feature extraction process.
Weaknesses:
+ The evaluation process may incur significant computational costs, particularly in multi-class settings, which could make the approach less appealing and comparable to recurrent architectures in terms of computational expense.
+ Although the model demonstrates competitiveness and improvement over the baseline, the paper would benefit from additional comparisons (as mentioned in the text) to strengthen its claims. The experimental evaluation is limited, relying on a single database and baseline.
------
The primary motivation behind the sparse coding scheme is to enable feedforward inference. However, this property does not hold in the multi-stage setting, necessitating optimization (as clarified by the authors).
The concept of efficient bi-directional coding is intriguing. As the authors noted, this may not always be the case, given the need for multiple model evaluations during classification.
A potential approach could involve running the model without class-specific bias and evaluating only the top K predictions using the energy-based setting.
It would be beneficial to include a discussion on the trade-offs associated with using a model like the one proposed by Cao et al., such as computational costs and performance.
Restricting bi-directional coding to the top layers seems reasonable, as it allows for a good, class-agnostic representation of low-level features. However, this aspect could be explored in greater detail, such as empirically demonstrating the trade-offs. Currently, only one setting is reported.
The authors mention that the proposed coding method's derived architecture benefits from a spherical normalization scheme, leading to smoother optimization dynamics. I wonder if the baseline (or model) employs batch normalization. If not, it seems relevant to test this.
Minor comments:
I find figure 2 (d) confusing and would recommend omitting it, as it does not yield a valid function (as stated in the text).