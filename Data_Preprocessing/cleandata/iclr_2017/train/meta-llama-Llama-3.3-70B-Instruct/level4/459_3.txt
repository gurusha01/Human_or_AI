This paper presents a novel deep multi-task representation learning framework, leveraging tensor factorization and end-to-end knowledge sharing to learn cross-task sharing structures at every layer of a deep network. By doing so, it eliminates the need for a manually defined multi-task sharing strategy, as required in traditional approaches. The experimental results demonstrate that this framework can attain higher accuracy while reducing the number of design choices.
The application of factorization concepts to multi-task learning (MTL) is a notable aspect of this work, despite similar ideas being explored in other contexts previously. However, it is worth mentioning that the parameter savings achieved through this method stem from low-rank factorization, a technique that could also be used to reduce the weight size of each layer in conventional MTL approaches via Singular Value Decomposition (SVD).
It is also worth noting that the concept of neural network-based MTL has been explored earlier, particularly in the speech recognition community, prior to the 2014 and 2015 works cited in the paper. For example, a relevant study by Huang et al. in 2013 investigated cross-language knowledge transfer using a multilingual deep neural network with shared hidden layers, as presented at the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing.