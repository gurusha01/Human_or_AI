This manuscript explores various techniques for generating adversarial examples for generative models, including VAE and VAEGAN, focusing on three primary methods: classification-based adversaries utilizing a classifier on the hidden code, VAE loss that directly leverages the VAE loss, and the "latent attack" which seeks to find adversarial perturbations in the input to match a target input's latent representation.
The problem addressed in this paper is potentially valuable and intriguing to the scientific community, and to my knowledge, it is among the first to delve into adversarial examples for generative models. Notably, a concurrent study, "Adversarial Images for Variational Autoencoders," proposes a similar "latent attack" concept, employing both L2 distance and KL divergence, as highlighted in my initial review comments.
Regarding novelty and originality, I found the ideas presented in this paper to be not particularly groundbreaking. The three proposed attacks are standard and well-established methods applied to a new problem domain, without the development of novel algorithms specifically tailored for attacking generative models. Nonetheless, it is interesting to observe how these standard methods perform in this new context.
However, the paper's clarity and presentation are disappointing. The initial version introduced "classification-based adversaries" but only reported negative results. Subsequent revisions significantly altered the paper's core idea, adding a new co-author and proposing the "latent attack," which outperforms the initial approach. Unfortunately, the authors attempted to retain material from the first version, resulting in an overly lengthy 13-page paper with disparate claims and unrelated experiments. The justification for the paper's length, citing efforts to be thorough, is unconvincing.
In summary, while the paper investigates an interesting problem by applying and comparing standard adversarial methods to this domain, its novelty and presentation are limited, hindering its overall impact.