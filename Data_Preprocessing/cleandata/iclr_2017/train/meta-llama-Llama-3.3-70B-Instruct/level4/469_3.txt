This manuscript addresses the challenge of compressing trained convolutional neural networks (CNNs) to minimize memory usage and accelerate the forward pass. The primary contribution, as I interpret it, lies in the development of efficient convolutional routines for sparse convolutional weights under general sparsity conditions, distinguishing it from approaches that rely on structured sparsity. The methodology is assessed using AlexNet and GoogLeNet, and its performance is evaluated across multiple platforms. Notably, the authors have made their code publicly available, enhancing the paper's reproducibility. The manuscript is well-structured, effectively situating this research within the broader context of model reduction techniques.
A key suggestion for the authors is to include a succinct comparison of the speed and memory efficiency gains achieved by their method relative to existing approaches. While the paper presents sparsity levels attained through various pruning methods, it lacks a clear, direct comparison that would enable readers to understand the advancements of this work over previous studies. Providing such a comparison would significantly enhance the clarity and impact of the manuscript.