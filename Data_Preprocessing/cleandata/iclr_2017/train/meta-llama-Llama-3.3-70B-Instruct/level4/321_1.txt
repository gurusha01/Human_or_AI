This study presents a notable contribution to the field of hierarchical control, drawing parallels with the research conducted by Heess et al. The experimental results are robust, successfully completing benchmarks that have proven challenging for previous studies. However, the analysis of these experiments could be more comprehensive.
Several key points warrant discussion:
(1) The terminology 'intrinsic' motivation, as used in this context, may be misleading due to its established meaning in reinforcement learning (RL). The pre-training approach, which involves rewarding robots for locomotion speed or grasping abilities, appears to be closely tied to the specific tasks they are designed to perform later on. This is reminiscent of the pre-training tasks outlined by Heess et al., although they are not identical.
(2) The incorporation of Mutual Information regularization is a sophisticated approach that generally yields positive results. Nevertheless, it seems to offer limited benefits in more complex maze environments, such as mazes 1, 2, and 3. The authors acknowledge this finding, but a more in-depth interpretation or analysis of this outcome would be beneficial.
(3) The factorization between Sagent and Srest should be explicitly outlined in the paper to ensure clarity and replicability. While Duan et al. provide a clear specification of Sagent, a detailed description of Srest is also necessary. It is possible that this information may have been overlooked in the initial review.
(4) Further examination of the agent's switching behavior, as well as a more detailed analysis of the policies, including failure modes and the impact of switching time on performance, would provide valuable insights and enhance the overall quality of the study.