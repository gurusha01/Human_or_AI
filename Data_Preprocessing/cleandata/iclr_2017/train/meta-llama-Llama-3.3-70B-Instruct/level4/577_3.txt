The authors present a methodology for examining the predictive capabilities of intermediate layer activations by training linear classifiers and assessing their performance on the test set.
This paper is well-grounded in motivation, seeking to elucidate the progression of model training and offer insights into the design of deep learning architectures.
The decision to utilize linear probes appears to be driven by two primary factors:
- The convex nature of the problem
- The typical linearity of the final layer in the network
However, as noted by the authors in the penultimate paragraph of page 4, it is possible that intermediate features may not be useful for a linear classifier, which I consider a significant limitation of the paper. I find it lacking in justification for the relevance of the proposed analysis to architecture design. In fact, the example featuring a skip connection (Figure 8) seems to imply that skip connections are not beneficial, which contradicts the recent successes achieved with ResNet architectures.
Although the results are intriguing, they are not particularly unexpected, and I struggle to see their direct relevance to understanding deep models as claimed by the authors.