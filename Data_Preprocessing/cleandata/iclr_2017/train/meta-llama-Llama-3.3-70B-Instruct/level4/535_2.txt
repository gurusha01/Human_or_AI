The authors adapt the image captioning framework of Xu et al. 2015 for video captioning, modifying it to incorporate attention across multiple ConvNet layers rather than a single layer. Experimental results on YouTube2Text, M-VAD, and MSR-VTT datasets demonstrate the effectiveness of this approach over using individual layers. 
This work is reminiscent of a well-designed course project or workshop submission, with a logical model, sufficient description, and experiments that validate the benefits of multi-layer attention. However, from a technical standpoint, the paper lacks a significant impact, and its contribution to the field is unclear. Certain elements, such as the inclusion of hard attention, seem unnecessary and detract from the overall quality. 
To enhance the paper's value, the authors could delve deeper into the examination of multi-level features, presenting a more comprehensive analysis of the tradeoffs and choices involved in different approaches, while streamlining the content to focus on the essential aspects and omitting redundant elements like video features and hard attention.