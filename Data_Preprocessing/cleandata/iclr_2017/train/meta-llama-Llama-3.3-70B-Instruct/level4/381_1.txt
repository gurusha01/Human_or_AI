The authors present a weight pruning strategy aimed at reducing GFLOP computations, which is well-motivated by the Taylor expansion of the neural network function with respect to feature activations. This strategy effectively removes feature maps with both small activation and small gradient, as expressed in equation 7. 
Ideally, the gradient of the output with respect to the activation functions should be zero at the optimal point, but due to stochastic gradient evaluations, this is practically never achieved. A small variance in the gradient across mini-batches suggests that the specific network parameter is unlikely to change, indicating that it is closer to convergence. Parameters that are close to convergence and result in small activations are good candidates for pruning, which is essentially what equation 7 conveys. This explains why removing weights based solely on small activations is not as effective as the proposed strategy, as demonstrated by the paper's results. 
There are two key differences in weights removed by the activation criterion versus the Taylor expansion: 
1. Weights with high activations but very low gradients are removed by the Taylor expansion but not by activation alone. 
2. Weights with low activation but high gradients are removed by the activation criterion but not by the Taylor expansion. 
Analyzing which of these differences contributes more to the variation in weights removed by the Taylor expansion versus the activation criterion would be interesting. Intuitively, weights satisfying the first condition are important because they are converged and significantly contribute to the network's activation. A modified criterion, such as equation 7 plus λ feature activation (where λ is determined by cross-validation), might lead to even better results, albeit at the cost of additional parameter tuning. 
Another noteworthy comparison is with the optimal damage framework, where pruning is performed using second-order information, assuming first-order gradients are zero. Although the authors discuss this in the appendix and claim it is memory and computation inefficient, a simple calculation suggests this would only result in a 50% increase in memory and computation during pruning, without affecting testing efficiency. Thus, from a deployment standpoint, this comparison seems justified and should be considered. 
The authors' ultimate goal is to reduce GFLOPs, and recent papers have proposed using lower precision computation to achieve this. A comparison of GFLOPs between lower precision and pruning would be valuable, as these approaches are complementary, and combining them could lead to superior performance. However, when operating in the low-precision regime, it is unclear how much pruning can be performed, making an analysis of this tradeoff desirable, although not necessary. 
Regarding fine-tuning, the authors report results for AlexNet and VGG on different datasets (Flowers and Birds, respectively). It would be beneficial to see the results of both networks on both datasets to provide a more comprehensive comparison. 
The authors report a small drop in performance after pruning. However, if the original network was trained for N iterations and the pruned network underwent M fine-tuning iterations, the correct comparison would involve training the original network for N + M iterations. In figure 4, it is unclear whether the performance at 100% parameters reports accuracy after N + M iterations or N iterations alone. 
Overall, the paper is technically and empirically sound, proposing a novel pruning strategy based on Taylor expansion, feature normalization, and iterative fine-tuning. However, incorporating some of the suggested comparisons would strengthen the paper, potentially leading to a revised rating of accept.