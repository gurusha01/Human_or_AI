This paper examines the trade-offs of utilizing a sine activation function, initially revealing that even simple tasks can lead to complex loss functions that are challenging to optimize when sine activations are employed. The authors then conduct a comparative analysis of networks trained with various activations on the MNIST dataset, finding that the periodic nature of the sine activation does not appear to be essential for effective task learning. Furthermore, they investigate the application of sine activations in algorithmic tasks where periodicity proves beneficial.
The strengths of this paper include the insightful closed-form derivations of the loss surface and the balanced discussion of both the advantages and disadvantages of sine activations, which contributes to an educational tone. 
However, the study seems to be more of an exploratory investigation into the potential benefits of sine activations, and additional evidence is required to draw substantial conclusions. The results from the MNIST dataset suggest that truncated sine may be equally effective, and while the comparison with tanh reveals interesting differences in saturation utilization, the two appear to be relatively interchangeable. Moreover, the findings from the toy algorithmic tasks are difficult to interpret in a concrete manner, highlighting the need for further research to solidify the conclusions.