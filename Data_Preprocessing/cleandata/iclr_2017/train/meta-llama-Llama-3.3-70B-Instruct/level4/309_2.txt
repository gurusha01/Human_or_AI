This manuscript presents a novel approach to enhancing feature learning in deep reinforcement learning by incorporating auxiliary tasks into the primary policy's optimization problem. These tasks, which are domain-independent, focus on control, immediate reward prediction, and value function replay, with the exception of the latter, they are utilized solely to shape the features by sharing the CNN+LSTM feature extraction network. Experimental results demonstrate the effectiveness of this approach on Atari and Labyrinth problems, showcasing significantly improved data efficiency compared to A3C.
The paper is well-written, with sound ideas and convincing results, making it a clear candidate for acceptance. However, I have a few minor suggestions:
- It would be beneficial to discuss the additional computational cost associated with optimizing the auxiliary tasks and its impact on training speed.
- Clarifying in the abstract and introduction that the agent learns distinct policies for each task would improve readability, as the current phrasing may imply a single policy optimizing all rewards simultaneously.
- The concept of "feature control" lacks empirical validation, and the preliminary experiment in Figure 5 is not convincing, raising concerns about stability and convergence issues.
- Considering the agents' performance is still improving, it would be interesting to continue training them to observe their full potential.
- The choice of using a fixed value for the auxiliary task weights (lambda_*) instead of optimizing them as hyperparameters is unclear, and experiments validating this choice would be beneficial.
- It should be noted that the auxiliary tasks are not trained using traditional Q-Learning, as they are trained off-policy with multiple steps of empirical rewards.
Additionally, there are several minor issues that require attention:
- The policy gradient algorithm description should be corrected to reflect that it minimizes a loss function.
- Equation 1 requires lambdac to be within the sum, and the subscript of rt^(c) should be corrected to r_t+k^(c).
- Figure 2 is not referenced in the text, and Figure 1(d) should be referenced in Section 3.3.
- Several grammatical errors, such as "the features discovered in this manner is shared" and missing definitions, like L_PC, need to be addressed.
- The legend of Figure 3 should explain the "Clip" term for dueling networks, and more ablated versions on Atari would provide valuable insights.
- The text contains several inconsistencies, such as incorrect figure references and unclear descriptions, which should be corrected for clarity.
- The appendix mentions supplementary materials, but they are not provided, and the value of lambda_PC is not specified.