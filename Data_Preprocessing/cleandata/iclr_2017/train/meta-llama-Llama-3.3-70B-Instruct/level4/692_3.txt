This manuscript proposes a document classification approach utilizing a hierarchical attention-based framework. 
The core concept involves employing a bidirectional LSTM to obtain a global context vector, followed by another attention-based bidirectional LSTM that leverages the final hidden state from the initial pass to weight local context vectors, referred to as TS-ATT. 
A simplified variant, termed SS-ATT, is also introduced, which eliminates the first LSTM and utilizes the output of the second LSTM as the global context vector. 
Although experiments are conducted on three datasets, the outcomes largely fail to achieve state-of-the-art performance.
While the underlying idea is intriguing, the experimental results lack sufficient convincing power to justify the introduction of this novel model architecture. 
It is noteworthy that the Yelp 2013 dataset used in this study is significantly smaller than the original dataset presented by Tang et al. in 2015, which comprised approximately 300,000 documents. 
This discrepancy raises questions regarding the scalability of the proposed model to larger datasets, as the other datasets employed in this research are also relatively small. 
To provide a more comprehensive comparison, it would be beneficial to include the results from Tang et al. (2015) in Table 2, which reported an accuracy of 65.1% on the Yelp 2013 dataset, highlighting the substantial difference between their achievement and the results presented in this manuscript. 
Furthermore, it is suggested that phrases such as "Learning to Understand" be omitted when presenting the model to maintain a more formal tone. 
In light of these considerations, it appears that this submission might be more suitable for a workshop setting.
Minor observations include:
- A typo in "gloal," which should be corrected to "global."
- The emphasis on not requiring pre-trained embeddings, although a positive aspect, is not a significant advantage, as various models can function adequately without them.