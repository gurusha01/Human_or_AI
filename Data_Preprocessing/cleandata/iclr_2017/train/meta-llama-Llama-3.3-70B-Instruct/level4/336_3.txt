This paper presents a notable advancement in autoregressive generative models by introducing several key extensions to PixelCNNs, including the replacement of the softmax function with a logistic mixture model, the incorporation of dropout for regularization, the use of downsampling to increase the receptive field size, and the introduction of specific skip connections. The authors demonstrate that these modifications enable the PixelCNN to surpass the performance of the PixelRNN on the CIFAR-10 dataset, which was previously the state-of-the-art model. Furthermore, the authors investigate the performance of PixelCNNs with smaller receptive field sizes, providing valuable insights into the capabilities of these models.
The contributions of this paper are significant, as they address a crucial limitation of autoregressive models: their slow performance at test time. The more efficient architectures proposed in this work have the potential to mitigate this issue, making them a useful addition to the field of tractable image models.
However, a major concern is the lack of thorough discussion of related work. The use of mixture models in autoregressive image modeling is not a new concept, with previous studies employing them for multivariate conditional densities and incorporating downsampling to increase receptive field size, albeit in different ways (Domke, 2008; Hosseini et al., 2010; Theis et al., 2012; Uria et al., 2013; Theis et al., 2015). Notably, the logistic distribution is a special case of the Gaussian scale mixture (West, 1978). The primary distinction in this work appears to be the integration of the density to model integers, which, although a good idea, is not sufficiently supported by evidence or references. The authors claim that not integrating the density has been a problem for earlier models based on continuous distributions, but this assertion requires further elaboration, citation, or experimental validation, such as reporting the performance of PixelCNN++ without integration and instead adding uniform noise to make the variables continuous.
Another limitation of this study is the relatively small dataset used, consisting of 60,000 images, which may not be sufficient to fully capture the complexities of high-dimensional spaces. While the use of regularization techniques, such as dropout, may be beneficial for specialized content, it would be more convincing to demonstrate the model's capabilities on a larger dataset, such as the "80 million tiny images" dataset, which is a superset of CIFAR-10. Moreover, semi-supervised learning could be easily implemented in this context, given the tractable likelihood of the model, and could provide additional insights, particularly in the class-conditional case.
Finally, it would be informative to include a comparison of the test-time generation speeds of the different models, as this is a critical aspect of their performance and usability.