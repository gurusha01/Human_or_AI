The manuscript presents a regularization technique, applying a last-layer feature penalty to the final layer of a neural network. 
Notably, the provided equations imply a per-example weighting, but omitting this weight (alpha_i) yields comparable results.
This approach bears resemblance to both Batch Normalization and weight decay, with experimental evaluations focused on the "low-shot" setting.
However, the paper appears to convey two distinct narratives: the feature penalty as a variant of soft batch normalization, and its application to low-shot learning. It is unclear why the feature penalty is specifically tailored to low-shot learning rather than more traditional supervised tasks.
Regarding the Omniglot results, achieving 91.5% accuracy, it is observed that this is still approximately 2% lower than the performance of Matching Networks, which are referenced but not included in Table 1. This discrepancy warrants further explanation.
Overall, while the concept is straightforward, it feels preliminary, particularly given that Batch Normalization itself yields better performance than the feature penalty, and combining both techniques results in even superior outcomes. This raises questions about whether the explanation provided is comprehensive.
-- edits after revised version:
The authors have incorporated additional information into the manuscript, which is appreciated. However, the paper still feels overly lengthy, and it is hoped that it can be condensed to the promised 9 pages. Despite these improvements, the paper still falls short of acceptance in my view, primarily due to the following concerns:
- The results on Omniglot remain significantly behind the current state-of-the-art.
- The new experiments do not substantially clarify or refute the relationship with Batch Normalization.
- An explanation has been added for why the feature penalty is effective in the low-shot setting, attributing it to the control of VC dimension and thus mitigating overfitting with limited training examples. However, this discussion is somewhat rudimentary and does not offer profound insights beyond the obvious.
In light of the revisions, I have increased the score from 4 to 5, yet I still believe the manuscript does not meet the acceptance threshold.