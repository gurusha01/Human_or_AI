This paper contributes to the growing body of research on learning optimizers and algorithms, a topic that has garnered significant attention lately. The authors employ a guided policy search framework at the meta-level to train optimizers, opting to train on random objectives and evaluate transfer to a limited set of simple tasks.
As highlighted below, this contribution is indeed valuable.
However, the discussion on the use of reinforcement learning (RL) versus gradients at the meta-level, mentioned later, lacks clarity and conviction. The authors are encouraged to conduct a comparative experiment between the two approaches and present the results. This is a crucial question, as the scalability of this method may depend on it. In fact, demonstrating scalability to large domains and transfer to these domains is a key challenge in this field.
In summary, the concept is promising, but the experimental support is inadequate.
Notably, this work was published on arxiv shortly before the release of our own paper on learning to learn by gradient descent by gradient descent. Our research was motivated by neural art and the goal of replacing the lBFGS optimizer with a neural Turing machine, given their similar equation forms (see the appendix of our arxiv version). Ultimately, we developed an LSTM optimizer learned by stochastic gradient descent (SGD).
In contrast, this paper utilizes guided policy search to determine parameter updates (policy), emphasizing the importance of optimizer transfer to new tasks.
This paper undoubtedly makes a timely and valuable contribution to the learning to learn literature, which has experienced rapid growth in recent months.