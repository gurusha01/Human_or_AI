This paper presents a compelling approach to enhancing attention mechanisms by incorporating structure, wherein attention posterior probabilities are treated as structured latent variables. The authors demonstrate the efficacy of this method through experiments involving segmental attention, akin to semi-Markov models, and syntactic attention, similar to projective dependency parsing, across both synthetic tasks, such as tree transduction, and real-world applications, including neural machine translation and natural language inference. Although the improvement over simple attention in the latter tasks is modest, the results are promising. Overall, this is a strong paper that warrants acceptance.
The clarity of the paper, the novelty of the approach, and the thoroughness of the experiments all contribute to a convincing proof of concept. However, there appears to be untapped potential in the application of structured attention to neural machine translation. For instance, segmental attention could pave the way for neural phrase-based MT, while syntactic attention offers a means of integrating latent syntax into MT, both of which seem like fruitful avenues for further exploration. It would be particularly intriguing to investigate the impact of introducing some level of (semi-)supervision on these attention mechanisms, such as utilizing posterior marginals computed by an external parser, to determine if this enhances the learning or initialization of the network's attention components.
The paper also marks a significant milestone in the application of backpropagation of forward-backward/inside-outside algorithms, as introduced by Stoyanov et al. in 2011. As noted in Section 3.3, the forward step in structured attention corresponds to the computation of first-order moments (posterior marginals), while the backprop step corresponds to second-order moments (gradients of marginals with respect to log-potentials, essentially the Hessian of the log-partition function). This extension broadens the applicability of the proposed approach to arbitrary graphical models where these quantities can be efficiently computed. For example, it would be interesting to explore whether a generalized matrix-tree formula could facilitate backpropagation for non-projective syntax. On the other hand, the requirement for second-order statistics might introduce numerical instability in certain problems due to the use of the signed log-space field, a concern that warrants further investigation to determine if it was encountered in practice.
Minor comments and corrections include:
- In the last paragraph of Section 1, there is a repetition of "standard attention attention."
- The third paragraph of Section 3.2 contains a typo, "the on log-potentials."
- In Section 4.1, under Results, the statement "... as it has no information about the source ordering" could benefit from clarification regarding the intended meaning.