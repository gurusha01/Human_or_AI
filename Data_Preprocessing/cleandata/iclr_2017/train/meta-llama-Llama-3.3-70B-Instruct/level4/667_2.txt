I have re-examined the manuscript in light of the authors' response and my initial assessment remains unchanged.
---
The manuscript proposes a novel method for incorporating a directed acyclic graph (DAG) structure of the data into word/code embeddings, aiming to leverage domain knowledge and enhance the training of a recurrent neural network (RNN) with limited data. This approach is applied to codes of medical visits, where each code is part of an ontology represented by a DAG. The codes correspond to leaf nodes, and different codes may share common ancestors (non-leaf nodes) in the DAG. Rather than solely embedding the leaf nodes, the authors also embed the non-leaf nodes and combine the embeddings of the code and its ancestors using a convex sum, which can be viewed as an attention mechanism over the representation. The attention weights are dependent on the embeddings and the weights of a multi-layer perceptron (MLP), allowing the model to separate the learning of code embeddings and the interaction between codes. The code embeddings are pre-trained using GloVe and then fine-tuned.
The model is thoroughly evaluated on two medical datasets, with multiple variations designed to isolate the contribution of the DAG (GRAM or GRAM+ vs. RNN or RandomDAG) and the pretraining of embeddings (RNN+ vs. RNN, GRAM+ vs. GRAM). The results demonstrate that both components are essential for achieving optimal performance, and the evaluation methodology appears to be comprehensive.
The paper is well-written, and the authors provide a compelling argument for utilizing MLP attention instead of a simple dot product of embeddings.
My two primary concerns are:
1) The inclusion of a softmax function in equation 4 seems inconsistent with the use of multivariate cross-entropy loss, as the predicted visit can have multiple codes equal to 1, which is not typical of single-class cross-entropy scenarios.
2) The embedding dimension, denoted as m, is not specified, and its value could have a significant impact on the model's performance.