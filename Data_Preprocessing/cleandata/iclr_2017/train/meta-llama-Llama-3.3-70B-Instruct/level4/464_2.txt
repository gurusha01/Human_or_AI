This paper presents a novel approach to learning sentence composition using reinforcement learning, with the goal of generating parse trees that can enhance performance in downstream tasks. The proposed method leverages the shift-reduce framework, utilizing RL to learn the optimal policy for the SHIFT and REDUCE actions. Experimental results on four datasets (SST, SICK, IMDB, and SNLI) demonstrate that this approach surpasses traditional methods relying on predefined tree structures, such as left-to-right or right-to-left parsing.
The paper is well-structured and offers two significant strengths. Firstly, the concept of employing RL to learn parse trees tailored to specific downstream tasks is innovative and intriguing. The adoption of the shift-reduce framework is particularly astute, given the simplicity of the action set (shift and reduce). Secondly, the findings presented in the paper provide evidence supporting the importance of parse trees, which is noteworthy given the ongoing debate about the utility of syntax in natural language processing.
Several comments and suggestions arise from this review:
- The authors appear to be unaware of recent research utilizing RL to learn compositional structures, such as the work by Andreas et al. (2016), which could provide valuable context and comparison.
- Given that different composition functions (e.g., LSTM, GRU, or traditional recursive neural networks) possess distinct inductive biases, it would be interesting to investigate whether the learned tree structures are independent of the chosen composition function.
- Considering that RNNs are theoretically equivalent to Turing machines, it is worth exploring whether constraining the model's expressiveness (e.g., by reducing its dimensionality) could facilitate the discovery of more useful tree structures.
Reference: Andreas et al. (2016) - Learning to Compose Neural Networks for Question Answering, NAACL 2016. Additionally, the slow training speed of the model, which requires constructing a new computational graph for each sentence, raises questions about the implementation details (e.g., TensorFlow, Python). Dynet, a framework specifically designed for such tasks, could potentially offer improvements - was it considered as an alternative?