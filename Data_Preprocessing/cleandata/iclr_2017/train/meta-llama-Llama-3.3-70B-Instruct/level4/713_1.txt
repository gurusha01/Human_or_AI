This paper tackles the crucial problem of vanishing gradients and the search for an optimal activation function, proposing a novel approach that learns activation functions during training. While I find the research intriguing, I have concerns that the paper may be prematurely presented.
The extensive experimental section lacks a clear conclusion, and the authors' frequent use of tentative language, such as "maybe," "could mean," and "perhaps," suggests uncertainty. To warrant acceptance, the paper requires a definitive statement about performance, supported by robust evidence. Currently, this is lacking, leaving me uncertain whether this approach represents a significant breakthrough or an ineffective method.
The theoretical section could benefit from clarification to enhance understanding. Furthermore, it is essential to investigate how the performance is impacted, particularly in comparison to ReLU, which offers a significant advantage due to its simplicity and low computational cost. A comparison of the computational efficiency of PELU-s to existing activation functions, such as ReLU, would provide valuable insight into their practicality.