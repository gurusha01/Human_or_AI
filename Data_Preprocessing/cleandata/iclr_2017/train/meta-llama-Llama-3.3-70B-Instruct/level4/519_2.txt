This submission appears to be an extension of existing research, attempting to replicate the success of batch normalization for recurrent neural networks (RNNs). However, the experimental results presented are not entirely persuasive, as it remains unclear whether this approach offers substantial improvements over alternative methods, such as Layer Norm. Furthermore, the reported speedup, while notable, does not seem to be dramatically faster, lacking an order of magnitude difference. The accompanying theoretical analysis also fails to provide novel insights, leaving the work feeling more incremental than groundbreaking. Overall, although this is a solid, incremental contribution, its impact may not be sufficient to warrant presentation at ICLR.