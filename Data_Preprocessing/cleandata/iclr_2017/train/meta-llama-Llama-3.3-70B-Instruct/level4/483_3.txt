This paper presents a novel approach for estimating visual attention in videos, wherein a convolutional neural network (C3D) is utilized to extract visual features from the input clip, which are then fed into a Long Short-Term Memory (LSTM) network. The hidden state at each time step in the LSTM is employed to generate parameters for a Gaussian mixture model, ultimately yielding a visual attention map.
The concept outlined in this paper is sound and the writing is of high quality. While Recurrent Neural Networks (RNNs) and LSTMs have been extensively applied to vision problems involving discrete sequence outputs, their use in problems with continuous outputs, such as the one addressed in this paper, is less common.
The experimental results demonstrate the efficacy of the proposed method, notably outperforming state-of-the-art approaches in saliency prediction on the Hollywood2 dataset and improving upon baselines like C3D + SVM in action recognition tasks.
One notable omission in this paper is the lack of certain baseline comparisons. Specifically, it does not clearly illustrate the contribution of the "recurrent" component to the overall performance. Although the results in Table 2 indicate that the proposed method (RMDN) surpasses other state-of-the-art methods, this might be attributed to the use of robust C3D features, whereas other methods in the table rely on traditional handcrafted features. Given that saliency prediction can be viewed as a dense image labeling problem, similar to semantic segmentation, numerous methods have been proposed in recent years, including Fully Convolutional Neural Networks (FCN) and deconvnet. A meaningful baseline would involve applying FCN to each frame and comparing the results to the proposed method. If the proposed method still outperforms this baseline, it would confirm the effectiveness of the "recurrent" aspect.