This paper proposes a video sequence prediction method, building upon the work of Mathieu et al., with a key contribution being the decomposition of the predictor into two distinct networks, one focused on motion and the other on content.
Although the paper is engaging, its novelty is somewhat limited when compared to the referenced study. As also noted by AnonReviewer1, similarities exist with two-stream networks and subsequent research based on this foundational paper. Moreover, the concept of separating motion and content has been explored in other applications, such as pose estimation.
Details:
The paper's clarity relies on the reader's familiarity with fundamental frameworks like GANs, but its presentation falls short of being accessible to a broader audience.
For instance, losses (7) to (9) are reminiscent of those in the Mathieu et al. paper, but to ensure the paper is self-contained, these losses should be thoroughly explained, with an explicit mention that they are supplementary to the primary GAN loss. The adversarial aspect of the paper lacks a comprehensive introduction, which should include an explanation of L_Disc as the loss function for the discriminator network, as well as the roles of both the generator and discriminator.
In Equation (1), the variable c is not defined (are these motion vectors?), and it is also used interchangeably with the feature dimension c'. 
The residual nature of the layer in Equation (3) should be more clearly emphasized.
The manuscript contains several typos, missing articles, and prepositions, indicating a need for careful proofreading.
The use of VGG-sized networks on relatively small datasets, with the same architecture applied across all datasets, raises questions. Specifically, there is no mention of pre-training, and it is unclear whether overfitting issues were encountered during training.