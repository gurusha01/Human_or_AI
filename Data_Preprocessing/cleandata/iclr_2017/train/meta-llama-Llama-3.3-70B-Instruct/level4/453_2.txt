This paper proposes a second-order approach, referred to as Binarization using Proximal Newton Algorithm (BPN), for training neural networks with binary weights and activations, aiming to achieve model compression for deployment on systems with limited memory. By integrating the supervised loss function into the binarization process, BPN addresses a significant limitation of existing weight binarization methods, which often overlook the impact of binarization on the loss function. The method is well-explained and analytically related to previous weight binarization techniques.
The experimental evaluation is comprehensive, covering multiple datasets and architectures, and generally demonstrates the superior performance of BPN. However, in the context of feed-forward network experiments, only test errors are reported, which may not fully capture the optimization performance, particularly given that all baselines achieve near-perfect training accuracy. To more effectively demonstrate the superior optimization capabilities of BPN, considering a more challenging optimization problem, such as incorporating an explicit regularizer into the training objective or utilizing a data augmentation scheme, and monitoring the training objective rather than the test error could provide more direct evidence.
In contrast, the advantages of BPN become more pronounced in the subsequent experiments involving Long Short-Term Memory (LSTM) networks, where its superiority is more clearly evident.