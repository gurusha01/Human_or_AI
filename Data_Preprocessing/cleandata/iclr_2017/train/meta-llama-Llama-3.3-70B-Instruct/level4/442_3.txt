I appreciate the opportunity to review this paper, which presents a fascinating application of the information bottleneck principle to deep neural networks. As someone who is fond of this principle, I am delighted to see it being utilized in this context, and to the best of my knowledge, this is the first paper to apply it to train deep networks, building upon the conceptual foundations laid out in the original papers. However, I do have some concerns regarding the claim of independent work, which I will address later.
The paper's derivation of the variational lower bound is exceptionally clear, making it accessible even to those without an extensive background in variational inference. The explanation of the information bottleneck principle is also well-articulated. The experimental results appear promising and contribute positively to the paper's overall impact.
However, I found the model's presentation to be somewhat confusing. Typically, in variational inference and information maximization, "p" denotes the model, while "q" represents the inference engine, implying that the choice of inference method is independent of the modeling procedure. In contrast, the Variational Information Bottleneck (VIB) presented in this paper assumes p(x, y) to be the underlying data distribution, approximated by the empirical distribution. This implies that the model is actually q(y|z)p(z|x), which seems to contradict the conventional notation. Furthermore, the authors refer to p(y|x) as the predictive distribution in section 4.2.3, which is unclear in this context. I interpret it as p(y|x) = âˆ«q(y|z)p(z|x)dz, but this contradicts the earlier definition.
The connection made between VIB and variational auto-encoders (VAEs), including the warm-up training by tuning beta, is interesting. Nonetheless, even when the loss function formula matches the variational lower bound used in VAEs (with beta = 1), the underlying models differ significantly. For instance, r(z) in VIB is a variational approximation to p(z) and is not a component of the model, whereas in VAEs, it is the prior distribution defined within the modeling procedure. Similarly, p(z|x) in VIB is part of the model, but in VAEs, it is the approximate posterior that can be chosen independently.
In summary, I believe the presentation of the modeling procedure needs clarification. As a Bayesian, I find the current presentation uncomfortable, and I hope these points will be addressed in the revision. Specifically, in the section comparing VIB to VAE, it would be beneficial to clearly highlight the differences between the two and provide intuition for why the VIB interpretation might be preferred.
Additionally, I noticed a couple of typos: in equations 9-11, should q(y|z) be used instead of q(z|y)? And in Figure 2, the caption mentions "as beta becomes smaller," but perhaps it should say "larger"?
Regarding the claim of independent work, the authors mention that their manuscript presents an independent contribution relative to Chalk et al. 2016, which was published online in May 2016. Given the competitive nature of deep learning research, where similar ideas often emerge concurrently, I appreciate the authors' honesty. However, if this claim is not accurate, it could impact my recommendation for the manuscript's acceptance.