This paper explores the properties of shortcut-based architectures, such as ResNet, which have demonstrated empirical success across various domains, making their optimization a valuable area of study. While the paper presents some intriguing experiments, two primary concerns arise. 
Firstly, the distinction between linear and non-linear networks is crucial, as findings from linear networks should not be extrapolated to non-linear networks without sufficient evidence. This is particularly relevant when considering the Hessian, given that non-linear networks exhibit large condition numbers even in relatively straightforward optimization scenarios, as highlighted in the ICLR submission "Singularity of Hessian in Deep Learning". Therefore, the paper's claims regarding non-linear networks are not supported, and a single plot based on MNIST data is insufficient to suggest that non-linear networks behave similarly to linear ones.
Secondly, the justification for focusing on the Hessian at the zero initial point is unconvincing. The zero initial point is an isolated case that does not provide insight into the Hessian's behavior during the optimization process, rendering it an uninteresting point of study.