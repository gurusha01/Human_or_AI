Unfortunately, the paper lacks clarity, making it difficult to comprehend the proposed concept. At a high level, the authors appear to suggest a generalized version of the traditional layered neural architecture, which includes MLPs as a special case, by utilizing arbitrary nodes that exchange messages. The paper then demonstrates that their layer-free architecture can replicate the computations of a standard MLP, which seems to be a circular argument. Furthermore, the technical details of the method are perplexing: while the authors aim to deviate from layer-based matrix-vector products, Algorithm 4 still employs matrix-vector products for both the forward and backward passes. Although the implementation involves asynchronous communication between nodes, the computation's "locking" nature renders it equivalent to synchronous methods, undermining the distinction between the two approaches.