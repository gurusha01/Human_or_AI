This work is grounded in the notion that utilizing decorrelated neurons, which activate exclusively for either background or foreground regions, provides distinct information streams for subsequent decision-making processes. By doing so, it offers "complementary viewpoints" of the input data to later layers, effectively mimicking an ensembling or expert combination approach within the model itself, rather than relying on an ensemble of multiple networks.
To achieve this decorrelation, the authors propose a methodical approach aimed at delivering complementary inputs to the final classification layers. They accomplish this by dividing intermediate neurons into "foreground" and "background" subsets and introducing side-losses that penalize activations on background and foreground pixels, respectively.
The efficacy of this method is demonstrated through improved classification performance on a moderately sized dataset (a subset of ImageNet) using a ResNet with 18 layers, compared to a standard baseline without these specialized losses.
The paper's concept, which is both straightforward and ingenious, appears to yield positive results, making for an enjoyable read. However, several concerns arise, notably that the methodology seems tailored specifically to vision tasks. In the context of vision, it is well understood that feature masking during both training and testing phases can be beneficial, highlighting the potential limitations of this approach in terms of broader applicability.