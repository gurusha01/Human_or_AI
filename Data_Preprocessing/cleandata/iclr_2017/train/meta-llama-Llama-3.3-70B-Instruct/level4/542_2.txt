This manuscript investigates abstract hierarchical multiagent reinforcement learning (RL) utilizing policy sketches, which are high-level descriptions of abstract actions. The research builds upon prior work in hierarchical RL, incorporating novel elements through neural implementations of earlier studies on hierarchical learning and skill representations.
Policy sketches are defined as sequences of high-level symbolic labels from a fixed vocabulary, initially lacking semantic meaning. These sketches are eventually mapped to concrete policies, facilitating policy transfer and temporal abstraction. The learning process is based on a variant of the standard actor-critic architecture.
The authors provide experimental results using a conventional game-like domain, such as maze or Minecraft environments.
However, the paper has two main limitations. Firstly, the concept of policy sketches, although intriguing, is not fully developed and lacks substantial impact. A more detailed explanation of policy sketches in the context of abstract Semi-Markov Decision Process (SMDP) models would be beneficial to understand their contributions. Instead, the paper presents a specialized application of this idea within the proposed approach. Secondly, the experimental evaluation is insufficient, as it does not provide a comprehensive comparison with related work. For instance, Ghavamzadeh et al. explored the use of MAXQ-like abstractions in multiagent RL. A more thorough comparison with MAXQ-based multiagent RL approaches, where the value function is explicitly decomposed, would be valuable to assess the strengths and weaknesses of the proposed method.