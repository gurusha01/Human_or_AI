This paper presents a fascinating and well-timed contribution, offering multiple significant advancements. 
- It introduces a framework for addressing combinatorial perception and action-spaces, which can be generalized to any number of units and opponent units,
- It establishes foundational deep RL results on a range of Starcraft subdomains,
- It proposes a novel algorithm, combining elements of black-box optimization and REINFORCE, to facilitate consistent exploration.
As previously noted, the choice of "gradient of the average cumulative reward" seems questionable, as it prioritizes late rewards over early ones, potentially mismatching the intended objective. The authors mention that preliminary experiments showed minimal difference, but if so, it would be more appropriate to use the correct objective.
The description of DPQ is inaccurate; it does not involve collecting traces via deterministic policies, but rather follows a stochastic behavior policy to learn about deterministic policies off-policy. This characterization should be revised.
Additionally, the portrayal of gradient-free optimization is misleading, as recent studies (such as Koutnik et al's 2013 TORCS paper) have demonstrated that its scalability limitations can be overcome. This also raises questions about the methodology used in the preliminary experiments with direct exploration in the parameter space. Were best practices in neuroevolution followed, such as exploring recent NEAT variants that have been successfully applied to similar domains?
Regarding the specific results, the DQN transfer from m15v16 to m5v5, achieving a 96% win rate despite only reaching 13% on the training domain, seems anomalous. Is this an error, or can the authors provide an explanation for this discrepancy?