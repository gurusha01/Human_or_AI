The paper presents an attention-based approach for video description, utilizing three LSTMs and two attention mechanisms to predict words sequentially from a sequence of frames. 
In the frame-encoding LSTM (TEM), the first attention mechanism predicts spatial attention for each frame and computes a weighted average. The second LSTM (HAM) predicts attention over the hidden states of the encoder LSTM. The third LSTM, running in parallel to the second, generates the sentence one word at a time.
Strengths:
- The paper addresses a relevant and interesting problem.
- The proposed use of two layers of attention is novel, to my knowledge, for video description, although the paper's claims of novelty are overstated due to insufficient attribution.
- Experiments on MSVD and Charades datasets show performance comparable to related work on MSVD and improvements on Charades.
Weaknesses:
1. The claims of contribution and novelty are questionable:
1.1. The Hierarchical Attention/Memory (HAM) model's uniqueness is unclear, as it closely resembles models from Xu et al. and Yao et al., with the main difference being the use of an additional LSTM for decoding.
1.1.2. The paper's statement about memorizing previous attention is unclear, as the model only considers the last hidden state and does not have access to previous attention weights.
1.1.3. The authors' claim that attention is a function of all previous attentions and network states is misleading, as this dependency is inherent in any LSTM, but the model does not actually access previous network states.
1.1.4. The multi-layer attention in HAM is not clearly explained.
1.2. The paper criticizes CNN features for discarding low-level information but fails to demonstrate how its approach addresses this issue, as it operates on high-level VGG conv 5 features and lacks access to attention between frames.
2. Related work: The differences between HAM and previous models (Yao et al. and Xu et al.) should be clarified or properly cited.
3. Conceptual limitation: The model's independent attention mechanisms (spatial and temporal) cannot attend to different aspects of frames for different words, limiting its ability to generate nuanced descriptions.
4. Equation 11 contradicts Figure 1: The input of the previous word is unclear, with Equation 11 suggesting the use of softmax output, which is unusual and should be emphasized.
5. Clarity issues:
5.1. Inconsistent notation between Equations 2-5 and 6-9 should be addressed.
5.2. Figure 1 lacks detail and could be improved with additional figures or by omitting well-known equations.
6. Evaluation:
6.1. The claim of achieving state-of-the-art results is overstated, as the model does not outperform all previous methods, especially on the MSVD dataset.
6.2. Qualitative results of attention mechanisms are missing, making it difficult to understand their effectiveness.
6.3. Performance improvements over model ablations are not significant.
6.4. Human evaluation is feasible and should be conducted, even if only on a subset of the test data.
7. Several comments and issues raised by reviewers remain unaddressed or unclear.
8. Hyperparameters are inconsistent between ablation analysis and performance comparison, which could impact results.
Other minor points:
- Equation 10's handling of hm and hg is unclear, as the provided LSTM formulas only account for two inputs.
- The paper lacks a section 4.2.
- The statement in section 4.1 about the proposed architecture's capabilities is not unique to this approach.
Summary:
The paper's approach and results are not as novel or convincing as claimed, with several weaknesses and areas for improvement in clarity, visualization, and evaluation.