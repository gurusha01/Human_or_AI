This manuscript extends the work of Lenc and Vedaldi (2015) by demonstrating that CNN representations at the FC7 layer exhibit a certain degree of equivariance to various transformation classes, and that training with specific transformation groups enhances representation equivalence.
The authors have conducted an extensive experimental study, training over 30 networks with diverse forms of jitter, which is a notable effort. However, it is challenging to discern a primary takeaway from this work. While the authors have measured properties at a different layer than Lenc and Vedaldi (2015), it is difficult to identify novel insights beyond the established fact that jitter contributes to achieving invariance. The evaluation appears to be largely correct, but the paper seems to fall short of effectively addressing the task advertised in its title.
Several major concerns arise from the experiments on representation distances:
* The exclusive focus on FC7 is somewhat debatable, as it is followed only by a single classification layer (FC8) that outputs class likelihoods. Since FC8 is a linear projection, the equivalence map essentially re-projects the FC8 weights of the attached network to the original network's weights. Conducting similar experiments on multiple layers may yield more valuable insights, particularly given that the networks are already trained.
* The experiment on representation distance lacks the classification error on the testing dataset, which would clarify whether the representations are indeed compatible up to linear transformation.
* The K-NN experiment's methodology is unclear, specifically whether the measurement is performed per test set example and after training the equivalence map. A more transparent approach would be to demonstrate that networks trained on similar groups of jitter transformations exhibit improved compatibility on the target task.
* The proposed method does not consistently enhance equivariance across all tasks, particularly with small values of λ1 and λ2, which renders the loss equivalent to simple data jitter. This issue may be attributed to the selection of the FC7 layer.
In general, this paper presents some intriguing results on FC7 equivariance but fails to derive many novel observations from these experiments. Due to issues with the equivalence experiments and equivariance fine-tuning, I would not recommend accepting this manuscript in its current form. However, refining the experiments on already trained networks and restructing the manuscript to focus on investigative work may lead to a meaningful contribution to the field.
Additionally, several minor issues were identified:
* The experimental verification of the new criterion for equivariance mapping is lacking, and its effectiveness in achieving better results is unclear.
* The angles mentioned on pages 1 and 5 are missing units (degrees?).
* On page 3, the statement "In practice, it is difficult..." incorrectly refers to Mg being maximized/minimized, when in fact it is the loss over Mg that is being optimized.
* Page 4, footnote 2: the described approach, which involves halving the activations, does not constitute a true dropout, as this constant factor can be absorbed into the preceding or following weights.
* It is unclear whether the network architecture used for RVL-CDIP is identical to AlexNet.
* On page 7, Figure 3a+3b: setting the diagonal elements to white is potentially misleading and incorrect, as the distance between identical representations should be zero, which can also serve as a verification of the experiments' correctness.