This paper proposes a methodology for crafting adversarial input images targeting a convolutional neural network, leveraging only black-box access, which permits obtaining output values for selected inputs without revealing the network's parameters. Nonetheless, the concept of an adversarial example is somewhat diluted in this context, as it focuses on achieving k-misclassification, where the true label is excluded from the top-k output rankings, rather than misclassifying to a specific desired label.
A comparable black-box scenario was investigated by Papernot et al. in 2016, where black-box access was utilized to train a surrogate model of the network, which was then subjected to attack. In contrast, this work exploits black-box access through a local search approach, where the input is perturbed, and the subsequent change in output scores is analyzed. Perturbations that drive the scores toward k-misclassification are retained.
A significant concern regarding the paper's novelty stems from the fact that the employed greedy local search procedure bears a strong resemblance to gradient descent. Given the absence of network parameters, a numerical approximation is used, observing the change in output corresponding to a change in input, rather than relying on backpropagation. Consequently, the greedy local search algorithm, which is extensively discussed in the paper, is not particularly surprising, and the paper's technical contributions can be seen as fairly incremental.