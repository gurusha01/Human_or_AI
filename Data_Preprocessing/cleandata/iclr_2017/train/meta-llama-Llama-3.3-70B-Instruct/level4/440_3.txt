This manuscript explores the application of model-based policy search, leveraging Bayesian Neural Networks to learn an environmental model, with a notable emphasis on $\alpha$-divergence minimization as an alternative to the more conventional variational Bayes approach.
The capability of $\alpha$-divergence to capture bi-modality, however, is offset by its complexity, prompting the authors to dedicate a significant portion of the paper to developing tractable approximations. To this end, they adopt the methodology proposed by Hernandez-Lobato et al. (2016) as a surrogate for $\alpha$-divergence.
The paper provides a clear definition of the environment's/system's dynamics, as well as the policy parameterization (section 3), which could serve as a valuable reference for other researchers. The authors then utilize simulated roll-outs based on the learned model to generate samples of the expected return. Given the availability of an environmental model, stochastic gradient descent can be performed using automatic differentiation tools, without relying on policy gradient estimators.
The experimental results demonstrate the effectiveness of $\alpha$-divergence in capturing multi-modal structures, outperforming competing methods such as variational Bayes and Gaussian Processes. The proposed approach also yields favorable results in a real-world batch setting.
The manuscript is well-written and technically sound, integrating various recent tools into a coherent algorithm. However, the repeated reliance on approximations to original quantities may undermine the benefits of the initial problem formulation. Furthermore, the scalability and computational efficiency of this approach are uncertain, and it is unclear whether many problems would necessitate such complexity in their solution. As with other Bayesian methods, the proposed approach is likely to excel in low-sample regimes and may be preferable to other methods in the same class (e.g., VB, GP) in such scenarios.