This paper presents a meticulous experimental investigation on the CIFAR-10 task, leveraging data augmentation and Bayesian hyperparameter optimization to train a large number of high-quality deep convolutional network classification models from binary targets. An ensemble of the top 16 models serves as a teacher model in the distillation framework, where student models are trained to match the averaged logits from the teacher ensemble. The training of student models also employs data augmentation and Bayesian hyperparameter optimization. Both non-convolutional and convolutional student models of varying depths and parameter counts are trained. Additionally, convolutional models with the same architecture and parameter count as some convolutional students are trained using binary targets and cross-entropy loss. The experimental results demonstrate that convolutional students with only one or two convolutional layers are unable to match the performance of students with more convolutional layers, given the constraint that the number of parameters in all students remains constant.
The strengths of this paper include:
+ A thorough and well-designed study that utilizes the best existing tools to investigate whether deep convolutional models require both depth and convolution.
+ It builds upon the preliminary results in Ba & Caruana, 2014, providing a nice extension of their work.
The weaknesses of this paper include:
- Proving a negative result can be challenging, as the authors acknowledge. However, this study presents a convincing argument given the current state of deep learning theory and practice.
A minor correction is suggested for Section 2.2, where it should be stated that the logits represent unnormalized log-probabilities, excluding the log partition function.
Furthermore, the paper does not adhere to the ICLR citation style. According to the template, citations should be formatted as follows: "When the authors or the publication are included in the sentence, the citation should not be in parenthesis (as in "See Hinton et al. for more information"). Otherwise, the citation should be in parenthesis (as in "Deep learning shows promise to make progress towards AI (Bengio & LeCun, 2007))"."
There are also a few minor issues with English usage and typos that should be addressed in the final manuscript, including:
- "necessary when training student models with more than 1 convolutional layers" should be revised to "necessary when training student models with more than 1 convolutional layer"
- "remaining 10,000 images as validation set" should be revised to "remaining 10,000 images as the validation set"
- "evaluate the ensemble's predictions (logits) on these samples, and save all data" should be revised to "evaluated the ensemble's predictions (logits) on these samples, and saved all data"
- "more detail about hyperparamter optimization" should be revised to "more detail about hyperparameter optimization"
- "We trained 129 deep CNN models with spearmint" should be revised to "We trained 129 deep CNN models with Spearmint"
- "The best model obtained an accuracy of 92.78%, the fifth best achieved 92.67%" should be revised to "The best model obtained an accuracy of 92.78%; the fifth best achieved 92.67%"
- "the sizes and architectures of three best models" should be revised to "the sizes and architectures of the three best models"
- "clearly suggests that convolutional is critical" should be revised to "clearly suggests that convolution is critical"
- "similarly from the hyperparameter-opimizer's point of view" should be revised to "similarly from the hyperparameter-optimizer's point of view"