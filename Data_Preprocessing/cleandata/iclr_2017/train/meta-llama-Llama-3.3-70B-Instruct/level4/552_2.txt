This manuscript explores a recurrent network model that utilizes an update rule of the form h{t+1} = Rx R h{t}, where Rx represents an input x embedded into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix. Although this model presents an intriguing concept, it is not a novel approach, as the idea of leveraging matrices to represent input objects and multiplication to update states has been previously employed in the literature on embedding knowledge bases or logic, such as the work by Ilya Sutskever and Geoffrey Hinton on using matrices to model symbolic relationships, or the study by Maximillian Nickel et al. on Holographic Embeddings of Knowledge Graphs. In my opinion, the experiments and analysis conducted in this study do not significantly contribute to our understanding of this model. The experimental design is particularly underwhelming, as it only involves a highly simplified version of the copy task, which is already a simplistic problem. I am aware of several researchers who have experimented with this model in language modeling, and as another reviewer has noted, the model's inability to forget is a notable limitation. 
To strengthen their work, I believe the authors should demonstrate the model's utility in a more substantial task. In its current state, I do not think this paper merits acceptance. Several questions arise from this work: What is the rationale behind using a shared R instead of integrating it into each Rx? Are there any elegant methods to exploit the model's linearity in h or Rx?