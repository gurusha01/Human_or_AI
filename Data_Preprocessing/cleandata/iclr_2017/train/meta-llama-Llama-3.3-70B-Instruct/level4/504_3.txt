The paper presents a straightforward approach to learning reward functions for reinforcement learning from visual observations of expert trajectories, particularly in scenarios with limited training data. To achieve descriptive rewards under these challenging conditions, the method leverages a pre-trained neural network as a feature extractor, akin to task transfer techniques in computer vision, and represents the reward function as a weighted distance to features of automatically extracted "key-frames" from the provided expert trajectories.
The paper is well-structured and clearly explains the concepts involved, situating the presented approach within the context of inverse reinforcement learning (IRL) literature. The resulting algorithm is appealing due to its simplicity and has the potential to be useful in various real-world robotic applications. However, there are three primary concerns that need to be addressed to significantly strengthen the paper:
1) Although the recursive splitting approach for extracting "key-frames" seems reasonable and the feature selection is well-motivated, two baselines are missing from the experiments:
   - The impact of disabling feature selection and using the distance between all features should be investigated. Would this approach fail immediately, and if not, what trade-offs are involved?
   - A simpler baseline could involve using all frames from the recorded trajectories, calculating the distance to them in feature space, and weighting them according to their time, as proposed in the paper. The effectiveness of this approach should be evaluated.
2) While combining the extracted reward function with a simple RL method is understandable, the used simple controller may introduce significant bias in the experiments due to its requirement for initialization from an expert trajectory. As a result, the RL procedure starts close to a good solution, and the extracted reward function may only be queried in a small region around the observed initial set of images. Without additional experiments, it is unclear how well the presented approach will work with other RL methods for training the controller.
3) Given the limited number of available images, which precludes training a deep neural net directly for the task, it is natural to wonder how other baselines would perform. The effectiveness of using a random projection of images to form a feature vector, a distance measure based on raw images (e.g., L2 norm of image differences), or a distance measure based on the first principal components should be investigated. While occlusions may render these approaches ineffective, empirical evidence is needed to confirm this.
Minor issues:
- Page 1: The phrase "make use of ideas about imitation" could be rephrased for better clarity.
- Page 3: The sentence "We use the Inception network pre-trained ImageNet" should be revised to "pre-trained for ImageNet classification" for accuracy.
- Page 4: The definition of the transition function for the stochastic case appears to be incorrect.
- Page 6: The sentence "efficient enough to evaluate" is somewhat awkwardly phrased.
Additional comments:
- The paper is primarily empirical in nature, with little actual learning involved in obtaining the reward function and no theoretical advances required. While this is not inherently negative, it places greater importance on the empirical evaluation.
- Although the clear exposition is commendable, the approach ultimately simplifies to computing quadratic distances to features of pre-extracted "key-frames." While making connections to standard IRL approaches in Section 2.1 is beneficial, one could argue that this derivation is not strictly necessary.