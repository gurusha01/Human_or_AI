I would like to commend the authors for their efforts in creating and sharing an NLP data set, a valuable contribution to the field.
The authors' utilization of an algorithm to generate a data set with 500 clusters per language for semantic similarity raises several key points.
1. Given the algorithm's scalability, it is surprising that the released data set is relatively small, comparable in size to those from recent SemEval tasks. A significantly larger data set would have been expected, showcasing the algorithm's capabilities.
2. The authors' manual verification of a subset of clusters revealed ambiguities, highlighting the need for more thorough post-hoc filtering. Leveraging Mechanical Turk, as done in the creation of ImageNet, could have facilitated the filtering of all clusters, resulting in a more refined data set.
3. Evaluating data set papers poses a challenge, as the criteria for a "good" or publishable data set are not well-defined. With numerous medium-sized NLP data sets being released annually, often targeting specific tasks, it is essential to demonstrate the unique value of the proposed data set. The authors could have strengthened their case by comparing their data set to existing ones, particularly those focused on semantic similarity, to highlight its advantages.
In conclusion, two additional points are worth noting:
A. The paper's alignment with ICLR's focus is questionable, as the primary contribution is a new NLP data set, which, while potentially useful for evaluating word embeddings, does not provide significant insights into representation learning. For instance, the finding that GloVe is empirically less effective for semantic similarity than other embeddings warrants further explanation.
B. The concept of evaluating human interpretation of topic models by placing a word into a cluster and assessing its distinctness was first proposed by Jonathan Chang et al. in their 2009 NeurIPS paper, "Reading Tea Leaves: How Humans Interpret Topic Models," which deserves citation in this work.