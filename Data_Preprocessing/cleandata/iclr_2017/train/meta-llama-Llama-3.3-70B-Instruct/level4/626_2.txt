I have not yet had the opportunity to thoroughly review the revised version.
SUMMARY 
This manuscript examines the preimages of outputs of a feedforward neural network that utilizes ReLUs.
PROS 
The paper proposes an innovative approach to changing coordinates at individual layers, which is a notable concept.
CONS 
The manuscript appears to be somewhat unrefined and lacks sufficient contributions to be considered a complete paper.
COMMENTS 
- Upon reviewing the initial version, it is evident that the paper contains numerous typos and appears to be in a relatively unfinished state.
- Although the paper presents interesting ideas, it does not contribute a substantial number of results to warrant consideration as a conference paper; however, I would be willing to recommend it for the workshop track.
- The concepts of irreversibly mixed and other notions presented in this paper bear a close relationship to the ideas discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014], and it would be beneficial to cite this work and explore these connections further, particularly in regards to the local linear maps of ReLU networks.
- I am intrigued by the practical considerations involved in computing pre-images, as while the definition may be straightforward, the implementation and computation could be problematic.
DETAILED COMMENTS 
- On page 1, the statement "can easily be shown to be many to one" is generally applicable.
- On page 2, the expression "For each point x^{l+1}" is missing parentheses in the superscript.
- Following equation 6, the phrase "the mapping is unique" should be qualified with "when w1 and w2 are linearly independent."
- Equation 1 should be represented as a vector.
- Above equation 3, the sentence "collected the weights a_i into the vector w" and bias b is missing a period.
- On page 2, the phrase "... illustrate the preimage for the case of points on the lines ... respectively" should specify which case corresponds to which.
- In Figure 1, it is unclear whether this is a sketch or an actual illustration of a network; if the latter, the specific value of x and the weights depicted should be stated, and the arrows should be precisely defined and explained, including those in the gray area.
- On page 3, the statement "This means that the preimage is just the point x^{(l)}" should be clarified to indicate the points that W maps to x^{(l+1)}.
- On page 3, the first display equation contains an index i on the left but not on the right-hand side, and the quantifier on the right-hand side is unclear.
- The phrase "generated by the mapping ... w^i" should include a subscript.
- The phrase "get mapped to this hyperplane" should be revised to "get mapped to zero."
- The term "remaining" is used without clear reference to what it is remaining from.
- The suggestion to "use e.g. Grassmann-Cayley algebra" could be replaced with a recommendation to use elementary linear algebra.
- The statement "gives rise to a linear manifold with dimension one lower at each intersection" assumes that the hyperplanes are in general position.
- The phrase "is complete in the input space" could be rephrased to "forms a basis."
- The term "remaining kernel" is used without clear reference to what it is remaining from, and the meaning of "kernel" in this context should be specified, whether it refers to the nullspace, a matrix of orthonormal basis vectors of the nullspace, or something else.
- In Figure 3, the nullspaces of linear maps should be depicted as passing through the origin.
- The phrase "from pairwise intersections" could be replaced with the symbol \cap.
- The description "indicated as arrows or the shaded area" is unclear and should be revised for better understanding.
- There are several typos, including "peieces," "diminsions," "netork," and "me."