This manuscript presents a novel approach to compressing neural networks by quantizing parameters by minimizing the loss function.
The objective is to maximize network compression through parameter quantization while maintaining minimal impact on the expected loss, assuming that parameter pruning has already been performed.
The proposed method focuses solely on quantizing individual scalar parameters, differing from previous studies (Han et al. 2015a, Gong et al. 2014) by considering the effects of weight quantization on both the loss function used for training and the variable-length binary encoding of cluster centers for quantization.
However, the submitted manuscript exceeds the recommended 8-page limit, clocking in at 20 pages, which seems unwarranted given that the initial three sections (approximately five pages) contain generic and redundant information that could be condensed or omitted (including figures 1 and 2).
Although not strictly mandated by the submission guidelines, condensing the manuscript to 8 pages would enhance readability.
To account for the impact on the network's loss, the authors employ a second-order approximation of the loss function's cost, leading to a formulation that expresses the effect of weight quantization on the loss in terms of a weighted k-means clustering objective.
The weights in this objective are derived from the Hessian of the loss function at the original weights, which can be efficiently computed using a back-propagation algorithm similar to gradient computation, as demonstrated in cited literature.
Alternatively, the authors suggest utilizing the second-order moment term from the Adam optimization algorithm as an approximate Hessian.
In section 4.5, the authors argue that their approach naturally lends itself to quantizing weights across all layers simultaneously due to the Hessian weighting, which considers the variable impact of quantization errors on network performance across layers.
However, the statement regarding the inefficiency of optimizing layer-by-layer clustering jointly across all layers due to exponential time complexity with respect to the number of layers requires further clarification.
Section 5 introduces methods to incorporate the code length of weight quantization into the clustering process.
The authors first describe uniform quantization of the weight space, which is then optimized using their Hessian-weighted clustering procedure from section 4.
For non-uniform codeword lengths, they develop a modified Hessian-weighted k-means algorithm that accounts for the code length of each cluster, weighted by a factor lambda, allowing for different compression-accuracy trade-offs based on lambda values.
The experimental results in section 6 demonstrate the proposed method's advantage over the layer-wise compression technique by Han et al. (2015) and uncompressed models using three datasets (MNIST, CIFAR10, and ImageNet) with dataset-specific architectures.
The results highlight the benefits of the Hessian-weighted k-means clustering criterion and variable bitrate cluster encoding.
In conclusion, this work is interesting, although the technical novelty may be limited.
Notably, the proposed techniques appear applicable beyond deep convolutional networks to any model with a cost function that can be locally approximated quadratically.
It would be beneficial for the authors to discuss this broader applicability in their manuscript.