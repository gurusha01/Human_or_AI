Summary: Various pruning techniques exist to reduce the memory footprint of CNN models, differing in granularity, pruning ratio, and sparsity of representation. This work presents a method for selecting the optimal pruning mask from multiple trials, with experiments conducted on CIFAR-10, SVHN, and MNIST datasets.
Pros:
The method proposes a strategy for choosing the best pruning mask out of multiple trials, providing an analysis of different pruning techniques.
Cons & Questions:
The proposed approach involves selecting the best pruned network through multiple random pruning trials, which seems to contradict the claim of achieving this in one shot. Although this concern has been addressed, further clarification would be beneficial. Additionally, the approach has only been tested on smaller CNNs, and it would be useful to extend the evaluation to larger models like AlexNet, VGG, GoogLeNet, or ResNet - although the extension to VGG is a positive step. Moreover, given the ultimate goal of reducing model size for embedded systems, a comparison of the memory space saved (in MB) using the proposed technique versus other methods, such as Han et al. (2015), would provide valuable insights.
Misc:
A minor typo was found in the caption of Figure 6 a), where "Featuer" should be corrected to "Feature".