This study presents a model that leverages paragraph vectors to learn compact binary codes, enabling efficient document retrieval. The experimental results indicate that this approach outperforms semantic hashing. However, the methodology is relatively straightforward and lacks technical sophistication. When using a code size of 128, the loss in comparison to a continuous paragraph vector appears moderate.
For the sake of clarity, it would be beneficial to include the baseline numbers from the Salakhutdinov and Hinton paper directly in the manuscript, rather than requiring readers to refer to the external source. To facilitate a more straightforward comparison, the authors could provide precision values at recall levels of 12.5%, 25%, and 50% for both the proposed model and semantic hashing. Additionally, it is worth noting that the semantic hashing paper reports results on the RCV2 dataset, whereas this study uses RCV1, which is twice the size and limited to English documents, making the results non-comparable. It would be intriguing to investigate the number of binary bits required to match the performance of the continuous representation. A comparison with the continuous PV-DBOW model trained using bigrams would also provide a more equitable evaluation.
Figure 7 illustrates the loss incurred when utilizing the real-binary PV-DBOW. This suggests that if a user requires high-quality ranking after the retrieval stage and can afford the additional space and computational resources, using a standard PV-DBOW to obtain the continuous representation at that stage might be a better option.
Minor comments:
- The first line following the introduction should read "is the sheer" instead of "is sheer".
- On page 1, the fourth line from the bottom, "words embeddings" should be corrected to "word embeddings".
- In Table 1, the term "code size" for PV-DBOW requires clarification; does it refer to the number of elements in the continuous vector?
- On page 5, the fifth line from the bottom, "W" should be replaced with "We".
- The fifth line after Section 3.1 should be revised to "covers a wide" from "covers wide".