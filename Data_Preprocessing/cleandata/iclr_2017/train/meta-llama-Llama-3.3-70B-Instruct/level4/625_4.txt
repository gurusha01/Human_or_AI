This manuscript introduces a hierarchical Deep Reinforcement Learning (DRL) algorithm designed to tackle sequences of navigate-and-act tasks within a 2D maze environment. The agent is provided with a list of text-based sub-goals during both training and evaluation phases, with the objective of learning to leverage pre-learned skills to achieve these sub-goals. The authors successfully demonstrate the algorithm's ability to generalize across sequences of varying lengths and to novel combinations of sub-goals, showcasing its capacity to adapt when faced with new scenarios.
The paper exhibits high technical merit, presenting a compelling and complex integration of cutting-edge advancements in Deep Learning (DL) and DRL. Notably, the DRL agent is hierarchical, capable of learning skills and planning their utilization. These skills are acquired through a differential temporally extended memory network equipped with an attention mechanism, alongside a novel application of analogy making and parameter prediction.
However, the manuscript lacks a clear justification for the problem's significance and why it remains unsolved. Given the simplicity of the 2D maze domain, the reliance on deep networks seems unwarranted, as similar problems have been effectively addressed using less complex models. Furthermore, the authors overlook a substantial body of literature on planning with skills. Since all skills are pre-trained before the hierarchical agent's evaluation, the problem essentially reduces to a supervised learning task, as the reward is not significantly delayed when utilizing pre-trained skills. The demonstrated generalization appears limited, primarily involving the decomposition of sentences into constituent words (item, location, action).
The paper's readability is compromised by its oscillation between algorithmic descriptions and technical specifics, resulting in an overload of details that hinder a comprehensive understanding. Many implementation details could be relocated to an appendix to enhance clarity. To ensure the paper is self-contained, it is essential to introduce all relevant notations and explain the methods used, without assuming the reader's familiarity with them.
By addressing these concerns, the manuscript has the potential to make a more substantial contribution to the scientific community in future conferences.