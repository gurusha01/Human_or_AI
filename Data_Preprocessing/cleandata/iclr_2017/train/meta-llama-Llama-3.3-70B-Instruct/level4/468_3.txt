This paper presents two primary contributions: 
1) It demonstrates the effectiveness of uniform quantization when combined with variable-length (Huffman) coding, a finding that, although not highly novel, is noteworthy as it appears to be previously unexplored.
2) The paper enhances fixed-length quantization by introducing Hessian-weighted k-means, an improvement over the traditionally used vanilla k-means. The incorporation of Hessian weighting is well-justified, and the authors cleverly explain how to leverage an efficient approximation without additional cost when utilizing the Adam optimizer, a particularly ingenious aspect. A significant advantage of this approach, beyond its performance improvements, is the elimination of the need for per-layer compression rate tuning, as this is inherently achieved.
In summary, the paper is commendable for its contributions: while the first is not groundbreaking but confirms the efficacy of a previously untested combination, the second is innovative and yields positive results. The paper's clarity and the quality of its results are additional strengths. However, it could benefit from being more concise.
One minor point of clarification is needed regarding the storage of "additional bits for each binary codeword for layer indication" in the context of layer-by-layer quantization. It seems perplexing when considering an alternative approach where each layer's quantized weight values could be stored in separate arrays (e.g., q[0][:] for layer 0, q[1][:] for layer 1), with each layer having its own codebook. The only discernible overhead compared to joint quantization would be the storage of each layer's codebook, which is negligible. The mention of "additional bits" is not entirely clear, but this does not significantly impact the paper's overall validity; nonetheless, the authors may wish to provide further clarification on this point to avoid potential confusion among readers.