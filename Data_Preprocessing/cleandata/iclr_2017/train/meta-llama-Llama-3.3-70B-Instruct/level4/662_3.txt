This paper presents an extension of the Neural Turing Machine (NTM) model, incorporating a trainable memory addressing scheme. The authors explore two distinct approaches to memory addressing: continuous/differentiable and discrete/non-differentiable mechanisms.
The strengths of this work include:
* The introduction of a trainable addressing mechanism to the NTM framework, offering a significant enhancement.
* The investigation of discrete addressing, which provides an interesting contrast to traditional methods.
* The application of the proposed model to bAbI QA tasks, demonstrating its potential in question answering scenarios.
However, there are several areas that require improvement:
* A substantial performance gap exists between the proposed model and state-of-the-art models such as MemN2N and DMN+, indicating room for optimization.
* The unavailability of the code hinders reproducibility and further research.
* Additional experiments on diverse real-world tasks would strengthen the paper, providing a more comprehensive understanding of the model's capabilities.
Note: It appears that a footnote is missing from the original submission, which may contain important information or clarifications.