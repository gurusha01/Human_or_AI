The authors present a transfer learning approach for neural network-based models, tailored to various NLP tagging tasks. 
In the context of multi-task learning, the proposed methods do not appear to introduce significant novelty from a machine learning perspective, as they involve sharing components of a general NLP architecture, with the extent of sharing determined by the specific task at hand.
However, the originality of this work lies in the specific architectural design employed for NLP tagging tasks. 
The experimental results indicate that this approach yields promising performance when labeled data is scarce (Figure 2), and Table 3 demonstrates modest improvements when operating at full scale.
Nevertheless, the findings in Figure 2 are open to interpretation, as the authors seem to have maintained a fixed architecture size while varying the amount of labeled data; potentially, optimizing the architecture for each data size could have led to enhanced outcomes.
In conclusion, although the paper is well-written, the overall novelty is somewhat restricted, and the experimental section falls short of expectations.