This paper presents a synchronous parallel SGD approach that utilizes multiple backup machines, allowing the parameter server to update the model without waiting for responses from all machines, thereby reducing synchronization overhead. The concept appears to be sensible and straightforward.
However, my primary concern is that this method may only be applicable in a narrow set of scenarios, specifically when most learners (with the exception of a small subset) operate at a similar pace in returning results. If the efficiency of learners does not adhere to this distribution, I question the effectiveness of the proposed algorithm. To address this, I recommend two revisions:
- Conduct additional experiments to demonstrate the algorithm's performance under various efficiency distributions among learners.
- Assume a uniform efficiency distribution among learners and provide an analysis to show that the expected idle time using the proposed algorithm is negligible.