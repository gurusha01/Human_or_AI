This paper presents a novel approach to obtaining an ensemble of neural networks without incurring additional training costs, by capturing snapshots of the network during the training process. The network is trained using a cyclic learning rate schedule with a cosine function, and snapshots are taken at the lowest points of these cycles. The authors demonstrate that these snapshot ensembles yield improved performance over a single network on image classification tasks across various datasets.
Strengths:
1. The proposed method is straightforward and easy to replicate, thanks to the simplicity of the technique and the detailed experimental setup described in the paper.
2. The paper is well-written, providing a clear explanation of the method and comprehensive experimental results.
Recommendations for enhancement and additional comments:
1. Although comparing the proposed method to other techniques with a fixed computational budget is reasonable, a more comprehensive comparison to "true ensembles" (i.e., independently trained networks) would provide a clearer understanding of its effectiveness. Specifically, Table 4 should be expanded to include results from "true ensembles" for a more thorough evaluation.
2. The comparison to "true ensembles" is currently only provided for DenseNet-40 on CIFAR100 in Figure 4, where the snapshot ensemble achieves approximately 66% of the improvement of the "true ensemble" over the single baseline model. This contradicts the authors' claim in the abstract that the snapshot ensemble "almost matches" the results of more expensive independently trained ensembles, and this discrepancy should be addressed.
3. To better understand the diversity of snapshot ensembles, it would be beneficial to compare their diversity to that of other ensembling techniques, such as (1) "true ensembles" and (2) ensembles derived from dropout, as described by Gal et al. in 2016 (Dropout as a Bayesian Approximation).