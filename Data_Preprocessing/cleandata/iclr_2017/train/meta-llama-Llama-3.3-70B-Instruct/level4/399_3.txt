This manuscript presents a novel approach to scaling up network models by incorporating a Mixture of Experts (MoE) between recurrent neural network layers, which is shared across all time steps. By processing features from all time steps simultaneously, the effective batch size for the MoE is increased by a factor equal to the number of time steps, enabling each expert to operate on a sufficiently large sub-batch of inputs and maintaining computational efficiency even with sparse expert assignments. Additionally, a secondary technique is introduced that rearranges elements within a distributed model, further augmenting per-expert batch sizes.
The authors conduct experiments on language modeling and machine translation tasks, demonstrating substantial improvements with an increased number of experts compared to state-of-the-art (SoA) and computationally matched baseline systems.
One aspect that could be improved is the provision of more detailed plots and statistics regarding the actual computational load and system behavior. Although two loss terms are utilized to balance expert usage, their impact is not thoroughly explored in the experiments. It would be beneficial to investigate the effects of these loss terms and the increase in effective batch sizes, such as by analyzing loss trajectories during training and comparing the distributions of per-expert batch sizes.
Overall, the proposed system is well-described and yields promising results, leveraging a clever placement of the MoE to overcome potential drawbacks of sparse computation.
Minor suggestion: Figure 3 is informative, but it is unclear whether data points align between the left and right plots. For instance, the H-H line has three points on the left plot but five on the right. It would also be helpful if the colors used for corresponding lines matched between the two plots.