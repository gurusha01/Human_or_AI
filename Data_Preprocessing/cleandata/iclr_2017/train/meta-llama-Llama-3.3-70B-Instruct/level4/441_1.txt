The authors propose Variable Computation in Recurrent Neural Networks (VCRNN), a concept similar to Adaptive Computation Time (Graves et al., 2016). Essentially, VCRNN modifies the traditional RNN by updating only a subset of the state at each timestep. However, the experimental results are unconvincing, and the comparison to other relevant works and a basic LSTM baseline is limited.
=== Gating Mechanism ===
The VCRNN generates a vector $m_t$ at each timestep, which acts as a gating mechanism. This vector determines a subset of the RNN state, specifically the first D states (D-first), to be updated or not. The introduction of additional hyperparameters, epsilon and $\bar{m}$, is not thoroughly explained, and their values or selection process are not provided. The sensitivity and criticality of these hyperparameters are also unclear. This mechanism, although novel, appears somewhat cumbersome and unprincipled, as it only updates the D-first states rather than allowing for a more generalized solution where any subset of the state can be updated. A comparison to the soft-gating mechanisms used in GRUs, LSTMs, and Multiplicative RNNs (Wu et al., 2016) would be a valuable addition.
=== Variable Computation ===
The authors argue that VCRNN can reduce computational costs compared to traditional RNNs. While this may be theoretically true, it is unlikely to have a significant impact in practice, as the RNN sizes used in the comparison do not saturate modern GPU cores. As a result, the actual wallclock time may not differ substantially. The absence of reported wallclock numbers makes it challenging to support this claim.
=== Evaluation ===
This reviewer would have liked to see more citations and comparisons to other relevant works, as well as a stronger baseline, such as a stacked LSTM architecture. The lack of comparison to more advanced RNN architectures, like those presented in Chung et al. (2016), is notable. The PTB BPC results are underwhelming, and the VCRNN fails to outperform the basic vanilla RNN baseline. The authors' citation and comparison to only basic RNN architectures overlook the significant contributions made since then. For example, Chung et al. (2016) experimented with PTB BPC and cited a large number of important contributions.
One interesting experiment conducted by the authors is the visualization of per-character computation in VCRNN (Figure 2), which shows increased computation after space or word boundaries. However, it would be insightful to compare this to the behavior of GRUs or LSTMs, and to investigate the magnitude of change in the state vector after a space in these models.
=== Minor ===
* The paper would benefit from equation numbers, making it easier to refer to specific equations in discussions and reviews.
References
Chung et al., "Hierarchical Multiscale Recurrent Neural Networks," in 2016.
Graves et al., "Adaptive Computation Time for Recurrent Neural Networks," in 2016.
Wu et al., "On Multiplicative Integration with Recurrent Neural Networks," in 2016.