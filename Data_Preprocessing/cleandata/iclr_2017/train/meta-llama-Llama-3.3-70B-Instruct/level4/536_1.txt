SUMMARY 
This manuscript explores the relationship between the number of hidden units and training examples required to learn functions belonging to a specific class, characterized by a bounded variability of outputs in Boolean functions.
PROS
The paper brings forth intriguing results from theoretical computer science, shedding light on the efficient representation of functions with limited output variability using shallow feedforward networks equipped with linear threshold units, thus contributing to the understanding of neural network capabilities.
CONS
However, the analysis is constrained to shallow networks, lacking a deeper exploration of more complex architectures. Furthermore, while the paper compiles interesting existing results, it does not introduce substantial novel contributions. The presentation of key findings and conclusions is somewhat opaque, particularly in relation to how increased robustness correlates with a decreased number of required hidden units, as the terms and constants used do not clearly elucidate this relationship.
COMMENTS
- The abstract references the universal approximation theorem for neural networks, noting its limitation in providing bounds on the number of hidden-layer nodes or weights. On page 1, the paper directs readers to a review article; considering the inclusion of more recent references could enhance the manuscript's comprehensiveness.
Given the abstract's motivation, discussing works on classes of Boolean functions representable by linear threshold networks could provide valuable context. For example, the paper "Hyperplane Arrangements Separating Arbitrary Vertex Classes in n-Cubes" by Wenzel, Ay, and Paseman explores various function classes that can be represented by shallow linear threshold networks, offering bounds on the number of hidden units needed for different Boolean functions, including lower bounds for a universal approximator.
- Discussing learning complexity in terms of measures like the VC-dimension could offer additional insights into the manuscript's findings.
- The explanations regarding constants were appreciated. It's noted that keeping noise sensitivity constant, larger epsilon values are associated with smaller delta and 1/epsilon values. However, Theorem 2's description in terms of poly(1/epsilon, 1/delta) could still imply an increase. Similarly, in Lemma 1, reducing sensitivity at constant noise increases the bound on k, which warrants further clarification.
- The independence of descriptions from n appears related to defining noise sensitivity as an expectation over all inputs. This aspect deserves more discussion, potentially starting with examples of functions with bounded noise sensitivity beyond the linear threshold functions in Lemma 2. Reverse statements to Lemma 1, focusing on the noise sensitivity of juntas, even as simple examples, would be intriguing.
- On page 3, the statement "...variables is polynomial in the noise-sensitivity parameters" might be more accurately described as the inverse.
MINOR COMMENTS
On page 5, Proposition 1 is suggested to be more appropriately labeled as Lemma 1.