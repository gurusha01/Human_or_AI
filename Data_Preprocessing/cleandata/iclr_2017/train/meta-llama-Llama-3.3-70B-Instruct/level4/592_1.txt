This paper proposes an alternative to the traditional Gaussian prior in Variational Autoencoders (VAEs) by introducing a group sparse prior, which is accompanied by a modification to the approximate posterior function to generate group sparse samples. The development of novel generative models and inference processes in VAEs is a vibrant and crucial area of research. However, the motivation behind the specific prior choice in this paper seems lacking. Several conceptual claims appear to be incorrect, and the experimental results are unconvincing, potentially due to a comparison of log likelihoods in bits against competing algorithms in natural units (nats).
Some detailed concerns include:
In Table 1, the log likelihoods of competing techniques are reported in nats, whereas the reported log likelihood of the conditional VAE (cVAE) using 10,000 samples exceeds that of true data samples and even surpasses the log likelihood achievable by fitting a 10,000-component k-means mixture model to the data. This discrepancy raises skepticism about the results, as outperforming a 10,000-component k-means mixture on Parzen estimation should be highly improbable. However, if the cVAE log likelihood is assumed to be in bits and converted to nats by multiplying by the logarithm of 2, the result becomes plausible. It is essential to clarify the units used for log likelihood (bits or nats) in the table and ensure consistent comparison.
Reporting and comparing the variational lower bound on the log likelihood would be highly beneficial. Alternatively, using Annealed Importance Sampling (AIS) could provide a more accurate measure of the log likelihood. Even if the Parzen window results are correct, Parzen estimates of log likelihood are inherently poor due to the drawbacks of log likelihood evaluation and additional limitations.
The quality of MNIST samples appears to be visually non-competitive. Furthermore, it seems that the images represent the probability of activation for each pixel rather than actual samples from the model. It would be more accurate to display samples, and in any case, a clear description of what is shown in the figure is necessary.
Notably, there are no experiments conducted on non-toy datasets, which raises concerns about the model's applicability to real-world scenarios.
The authors' response to previous questions has addressed some concerns but has also raised additional issues:
1. The construction of minibatches to balance epitome assignment (Algorithm 1, line 4) is a positive aspect, as it ensures that all epitomes will be utilized.
2. The response does not adequately address why the conditional VAE (C_vae) would trade off between data reconstruction and being factorial, as the approximate posterior is factorial by construction.
3. The claim that a particular unit in C_vae must have all examples in the training set deactivated (KL term of zero) to have zero contribution from the KL term is incorrect. A standard VAE can set the variance to 1 and the mean to 0 (KL term of 0) for some examples and have non-zero KL for others.
4. The VAE loss is trained on a lower bound on the log likelihood, which includes a term resembling reconstruction error. Intuitively, overfitting should correspond to data samples becoming more likely under the generative model.
5/6. The concerns regarding Parzen window evaluation remain, particularly the treatment of a binary model's probability of activation as a sample in a continuous space.
6. The statement that the model can only be evaluated from its samples is not accurate. Training on a lower bound on the log likelihood provides an alternative method of quantitative evaluation, and techniques like AIS can compute the exact log likelihood.
7. Parzen window evaluation is not considered a better measure of model quality, even for sample generation, than log likelihood.