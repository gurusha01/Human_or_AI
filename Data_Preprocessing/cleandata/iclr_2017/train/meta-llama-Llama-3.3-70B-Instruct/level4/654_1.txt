The authors propose a novel approach to training probabilistic models by optimizing a stochastic variational lower-bound-type objective, which involves sampling and learning a transition-based inference to revert samples back to the data. This method's emphasis on transitions enables the learning of a raw transition operator, rather than solely relying on energy-based models. The objective's similarity to established, albeit less principled, training methods for Markov Random Fields, such as Contrastive Divergence, makes it intuitively appealing.
The algorithm's concept is intriguing and has the potential to make a valuable contribution to the literature. Nevertheless, in its current state, the submission is not suitable for publication. The experiments are qualitative in nature, and the generated samples do not clearly demonstrate high model quality. As noted by other reviewers, the mathematical analysis falls short of proving the tightness of the variational bound when using a learned transition operator. Additional evaluation, such as using annealed importance sampling to estimate held-out likelihoods, is necessary. If the analysis can be improved, the method's ability to directly parametrize a transition operator, a key strength, should be further explored in experiments and compared to traditional energy-based modeling approaches.
This idea shows promise, and other reviews have already highlighted important technical aspects that can help strengthen the paper for future submission, making it a worthwhile endeavor to revise and resubmit.