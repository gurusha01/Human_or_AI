This manuscript demonstrates that increasing the mini-batch size in a serial setting requires a larger number of samples to achieve the same convergence guarantee. A similar phenomenon is observed when utilizing multiple learners in asynchronous SGD, which is reminiscent of findings in convex optimization, such as those presented in "Better Mini-Batch Algorithms via Accelerated Gradient Methods" (NIPS 2011), where convergence rates follow the form O(1/\sqrt{bT}+1/T), resulting in only a \sqrt{b} time improvement when using bT samples. The paper extends this concept to the nonconvex case, primarily building upon the mathematical framework established by Ghadimi and Lan (2013). However, this behavior has been previously discussed and summarized in a deep learning textbook (chapter 8), which limits the novelty of the current work.
The theoretical findings suggest that using mini-batches may not be optimal, but in practice, mini-batches are often beneficial. As noted in the deep learning textbook, mini-batches enable the use of larger learning rates (although this paper assumes a constant learning rate across different mini-batch sizes). Furthermore, multicore architectures can execute mini-batches in parallel with minimal additional cost. Therefore, the practical implications of these results are also limited.
Additional comments include:
- In Equation (4), since the number of samples (S) processed is constant and S = MK, where M represents the mini-batch size and K is the number of mini-batches processed (as mentioned in the first paragraph of Section 2), when comparing two different mini-batch sizes (Ml and Mh), the corresponding K values should differ. Nevertheless, the same K is used on the left-hand side of Equation (4).
- For Figures 1 and 2, as the convergence speed is the primary focus, it would be more informative to display the training objective instead.