The authors present an extension of the conventional attention mechanism by incorporating a distribution over latent structures, such as alignments and syntactic parse trees, which are modeled as a graphical model with potentials derived from a neural network.
The paper is well-organized and easy to follow, with the proposed methods being evaluated on a range of problems, consistently outperforming baseline models that either lack attention or utilize simple attention mechanisms. Although the improvements achieved by the proposed approach are relatively modest compared to simple attention models in the two real-world tasks, the techniques presented are nonetheless noteworthy.
Key observations:
1. The Japanese-English Machine Translation example exhibits a relatively small performance difference between the Sigmoid attention model and the Structured attention model. It would be intriguing to analyze the attention alignments to determine if the structured models yield better alignments, potentially by comparing them to ground-truth alignments or human-annotated test examples, in addition to evaluating the BLEU metric.
2. The final experiment on natural language inference reveals an unexpected outcome, where the use of pretrained syntactic attention layers appears to degrade model performance rather than improve it. The authors' hypotheses regarding this phenomenon would be valuable in providing insight into this observation.
Minor suggestions:
1. A typographical error is present in Equation 1, where "p(z | x, q" should be corrected to "p(z | x, q)".
2. In Section 3.3, the sentence "Past work has demonstrated that the techniques necessary for this approach, … " could be revised to "Past work has demonstrated the techniques necessary for this approach, … " for improved clarity.