The authors suggest utilizing a layer-wise language model-like pretraining approach for encoder-decoder models, enabling the exploitation of separate source and target corpora in an unsupervised manner without requiring large parallel training corpora. This concept, although straightforward in principle, involves initially optimizing both the encoder and decoder using LSTM for language modeling tasks.
While the ideas presented are not novel, the paper effectively compiles and integrates several existing approaches that have been established for some time. The experimental validation provides valuable insights into the significance of initialization and the efficacy of different initialization methods in the encoder-decoder setting.
The regularizer proposed on page 3 resembles a typical multi-task objective function, particularly when used in an alternating manner. It would be intriguing to investigate whether comparable performance could be achieved by starting with this objective from random initialization.
The authors should consider acknowledging the pioneering work on encoder-decoder like-RNN models published in the 1990s.
Minor suggestions:
On page 2, Section 2.1, in the second paragraph, the phrase "can be different sizes" should be revised to "can be of different sizes" for improved clarity.