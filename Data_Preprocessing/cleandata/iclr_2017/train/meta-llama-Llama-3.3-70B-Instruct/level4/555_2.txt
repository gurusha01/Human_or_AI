This manuscript seeks to address the challenge of preselecting deep learning model architectures for novel domains by presenting a series of experiments conducted on small tasks utilizing feed-forward Deep Neural Networks (DNNs). The authors propose that a ranking algorithm can be derived from these experiments to inform the selection of model structures for new domains.
While the objective is intriguing, I find the conclusions drawn to be unconvincing and of limited practical utility for several key reasons:
1. The experiments were restricted to simple network architectures (feed-forward DNNs), which, although simplifying the search space, also diminishes the value and applicability of the findings. In reality, the optimal model architecture is often highly dependent on the specific task or domain, with the type of model (e.g., DNN, CNN, LSTM) frequently being more critical than the network size itself.
2. The experiments were performed with certain crucial hyperparameters (such as the learning rate schedule) held constant. However, it is well-established that the learning rate is frequently the most critical hyperparameter during training. Without adjusting these vital hyperparameters, the conclusions regarding the optimal model architecture lack conviction.
3. The experiments suggest that differences in training data are not significant, which is unlikely. Typically, larger models (in terms of the total number of parameters) are preferred when the training set is substantially larger, indicating that the logarithm of the data size could be an important feature. This oversight is likely due to the experiments not being conducted on large datasets.
Furthermore, I believe the title of the paper does not accurately capture its content and should be revised. Additionally, the paper cites Sainath et al. (2015) as the pioneering work leading to breakthroughs in speech recognition. However, significant advancements in Automatic Speech Recognition (ASR) occurred earlier. Notably, the first paper incorporating all three key components was published in 2010 by Yu, Deng, and Dahl, and a more detailed paper was published in 2012 by Dahl, Yu, Deng, and Acero.
In conclusion, this paper presents preliminary results that, while interesting, are not sufficiently developed for publication. The work requires further refinement to address the limitations and inaccuracies identified.