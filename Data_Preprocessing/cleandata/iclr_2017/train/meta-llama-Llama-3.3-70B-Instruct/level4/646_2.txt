This paper presents a two-stage encoding approach for narratives in bAbI-like settings, utilizing a GRU to encode sentences word by word, conditioned on a sentence-level GRU that maintains a sentence-level encoding. These encodings are then employed 
to modify the bAbI tasks, necessitating the formulation of a question to accurately solve the problem.
However, I remain unconvinced by the paper's findings:
1. The proposed architecture fails to demonstrate significant improvement over DMN+, and in my opinion, bears resemblance to DMN+. It is unclear what specific limitation of DMN+ the authors' architecture addresses. 
2. Several existing studies, such as "Dialog-based Language Learning" by Weston and "Learning End-to-End Goal-Oriented Dialog" by Bordes and Weston, have explored similar concepts, and in my view, have done so more rigorously and convincingly. In the current work, the correct answer to the question appears to be provided independently of the agent's inquiry, allowing any model capable of outputting "unknown" and subsequently inputting the additional response to gain an advantage. Essentially, any architecture designed to solve bAbI tasks can be adapted to accommodate this functionality. The enc-dec* accuracies reported in Appendix A demonstrate that such a module can be appended to existing models. Moreover, standard models can be trained to generate questions as a sequence of words. I suspect that, in the authors' setting, questions could be generated by simply enumerating all questions that occur during training and applying a softmax function, rather than generating them word by word.