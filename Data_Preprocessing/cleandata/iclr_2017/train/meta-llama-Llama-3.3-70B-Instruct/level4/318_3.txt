This paper presents an innovative extension of the Gated Graph Sequence Neural Network, enabling the model to generate complex graph transformations. The core concept revolves around developing a method that can construct or modify a graph structure as an internal representation to solve problems, with a specific focus on question-answering tasks. The author introduces five distinct differentiable transformations that can be learned from a training set in a supervised manner, where the graph state is provided at each time step. A specific instantiation of the model is showcased, which takes a sequence as input and iteratively updates an internal graph state to produce a final prediction, yielding promising results for question-answering tasks, such as BaBi.
The approach outlined in this paper is noteworthy, as the proposed model successfully maintains a complex graph representation of its current state while remaining differentiable and amenable to learning through gradient-descent techniques. This can be viewed as a successful integration of continuous and symbolic representations, offering a more general framework than recent attempts to incorporate symbolic elements into differentiable models, such as Memory Networks and Neural Turing Machines. The key advantage lies in the model's ability to evolve its state shape, which is not fixed. However, concerns arise regarding the training methodology, which relies on providing the graph state at each time step, a approach that may be limited to specific tasks and not readily applicable to more complex problems. Additionally, the paper's density and comprehensive content may be more suited to a journal format, making it challenging to read and digest within the confines of a conference submission.