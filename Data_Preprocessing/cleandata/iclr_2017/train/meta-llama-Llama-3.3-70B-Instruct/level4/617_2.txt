The authors propose techniques to accelerate gradient descent by exploiting asynchronicity at the layer level, achieving speedups of up to 1.7x relative to synchronous training. However, their baseline comparison is suboptimal. A more significant concern is the omission of parameter-server based approaches, which have become a standard, resulting in a lack of comparison to current state-of-the-art methods. Furthermore, the paper fails to provide wall-time measurements, a critical metric. Due to these limitations, the submission is not suitable for acceptance at ICLR in its current form.