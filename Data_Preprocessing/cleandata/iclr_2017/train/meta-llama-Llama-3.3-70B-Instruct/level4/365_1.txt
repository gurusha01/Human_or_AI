The proposed method suggests compressing deep network weight matrices by introducing a novel density-diversity penalty, coupled with a computational optimization technique involving weight sorting, and a weight-tying strategy. 
This density-diversity penalty incorporates an additional cost term, comprising the l2-norm of the weights, representing density, and the l1-norm of all pairwise weight differences within a layer.
Typically, the most frequently occurring value in the weight matrix is set to zero, promoting sparsity. 
As the diversity penalty causes weights to converge to identical values, they are tied and subsequently updated using the average gradient.
The training process alternates between two phases: 1) training with the density-diversity penalty and untied weights, and 2) training without this penalty but with tied weights.
Experiments conducted on the MNIST and TIMIT datasets demonstrate that the method achieves impressive compression rates without compromising performance.
The paper is well-presented, featuring innovative ideas and appearing to be at the forefront of compression research. The weight-tying approach may have significant implications beyond compression, potentially enabling the discovery of regularities in data.
However, the result tables are somewhat perplexing.
Minor issues include:
p1: An English error is noted in the phrase "while networks that consist of convolutional layers".
p6-p7: Tables 1, 2, and 3 are confusing, as the proposed method (DP) seems to perform worse than the baseline (DC) in several instances. Specifically, DP exhibits lower sparsity and higher diversity than DC in Table 1 overall, Table 2 overall for FC, and Table 3 overall. This inconsistency suggests a potential inversion of the sparsity values, possibly reporting the fraction of non-modal values instead of the actual sparsity.