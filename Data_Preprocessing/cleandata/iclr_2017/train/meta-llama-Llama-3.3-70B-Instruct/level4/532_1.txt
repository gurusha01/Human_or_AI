This paper presents a straightforward narrative, with a coherent and logical concept. However, the experimental results are somewhat difficult to evaluate, and the inclusion of additional baselines would be beneficial for a more comprehensive assessment. To achieve faster training convergence, it is crucial to consider the optimization of the stochastic gradient descent (SGD) algorithm, particularly with regards to the learning rate schedule, which is not mentioned in the paper. Furthermore, testing the approach on diverse datasets is essential to validate its robustness. The effectiveness of filtering training data may be task-specific, and therefore, requires further investigation to determine its generalizability.