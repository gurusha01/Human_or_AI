This manuscript proposes a novel approach to training deep generative models with discrete latent variables by leveraging the reparameterization trick, and demonstrates its effectiveness on a specific DBN-like architecture, achieving state-of-the-art performance in density modeling on MNIST and similar datasets.
The paper is exceptionally well-structured and clearly written, with a thorough and precise exposition that is rare in our field. The inclusion of detailed appendices justifying various design choices is particularly commendable, setting a high standard for clarity and transparency.
The reported log-likelihood results are impressive, especially considering that most competing algorithms rely on continuous latent variables. However, the experiments could be further strengthened by isolating the contributions of the architecture and the inference algorithm. For instance, comparing the performance of a similar architecture trained with VIMCO or applying the algorithm to a previously published discrete architecture would provide valuable insights.
One potential concern is the variance of the gradients in the general algorithm formulation. As noted in my comment on the "variance of the derivatives of F^{-1}", the response is convincing, but it would be beneficial to highlight this issue in the paper, along with potential "engineering principles" for the smoothing distribution, to raise awareness among users. 
Another concern relates to the number of sequential operations required for inference in the RBM model, which appears to be a general Boltzmann machine. The q distribution is formulated as an autoregressive model, where variables are processed sequentially. Although Section 3 and Appendix A discuss the possibility of grouping variables, the solution involves decomposing the joint into conditionals and applying CDFs sequentially, which may lead to computational expenses.
A minor suggestion is to add a reference to Appendix A in the second paragraph of Section 3 to enhance clarity and readability.