This paper presents a novel approach to initializing the weights of a deep neural network using a marginal Fisher analysis model, leveraging a similarity metric. 
The strengths of this paper include:
The authors have conducted an extensive series of experiments to test their proposed method, albeit on relatively small datasets.
However, there are some notable weaknesses:
A key limitation is the lack of a baseline comparison, such as a discriminatively trained convolutional network on a standard dataset like CIFAR-10.
Additionally, the computational cost of calculating the association matrix A in equation 4 is unclear.
Overall, this is a decent paper that introduces a new concept and combines it with existing techniques like greedy-layerwise stacking, dropout, and denoising auto-encoders.
Nevertheless, similar ideas have been explored in the past, approximately 3-5 years ago, as seen in papers like SPCANet.
The primary innovation of this paper lies in the application of marginal Fisher Analysis as a new layer, which is an acceptable contribution.
However, the absence of robust baselines to demonstrate the superiority of this approach is a significant concern.
Specifically, a comparison with a convolutional or fully connected network trained from scratch with proper initialization would be beneficial to assess the effectiveness of the proposed method.
To strengthen the paper, the authors should provide unequivocal evidence that initializing layers with MFA yields better results than using random weight matrices.