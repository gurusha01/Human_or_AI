This paper tackles the challenge of predicting learning curves, distinguishing itself from existing research by (1) training a neural network that can generalize across different hyperparameter configurations and (2) utilizing a Bayesian neural network in conjunction with Stochastic Gradient Hamiltonian Monte Carlo (SGHMC). 
The authors successfully demonstrate the efficacy of their proposed approach in both extrapolating partially observed curves and predicting entirely unobserved learning curves across a variety of architectures, including fully connected (FC) networks, convolutional neural networks (CNN), logistic regression (LR), and variational autoencoders (VAE). This shows significant promise for applications in Bayesian optimization, and an additional experiment assessing the comparative advantages of this method would be highly valuable.
Consideration of learning rate decays could further enhance the methodology. One potential strategy involves running the algorithm on a randomly selected subset of the data and then extrapolating from this subset.
Beyond the mean squared error (MSE) and log likelihood (LL) evaluation metrics used, it might be beneficial to explore other assessment measures. In practical scenarios, identifying the most promising run is of paramount importance. Therefore, evaluating the accuracy with which each method identifies the best run could provide additional insights.
Minor suggestions include:
- Increasing the font size for figure legends and axes to improve readability, as the current font size is too small and difficult to read in hard copy.
- Fig 6 appears to have inconsistencies in the number of lines depicted. It would be helpful to clarify whether lines are overlapping in certain cases, which could affect the interpretation of the results.