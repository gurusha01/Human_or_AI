Review- Strengths
- The investigation of the relationship between ReLU DNN and simplified SFNN is a noteworthy aspect of this work
- The use of a small-scale task, such as MNIST, provides an experimental demonstration of the proposed training methods' effectiveness
- The multi-stage training approach proposed is straightforward to implement, although it lacks a rigorous theoretical foundation
Weaknesses
- The absence of results on large-scale, real-world tasks with substantial training datasets is a significant limitation
- The scalability of the proposed learning methods when dealing with larger training data remains unexplored and unclear
- When the hidden layers are stochastic, the model exhibits similarities with deep Bayes networks and deep generative models, particularly in terms of uncertainty representation (as discussed in "Deep Discriminative and Generative Models for Pattern Recognition", a book chapter in "Pattern Recognition and Computer Vision", November 2015). A more in-depth discussion of these connections, especially regarding the application of uncertainty representation to enhance pattern recognition via supervised learning and the incorporation of domain knowledge, such as "explaining away", would be beneficial
- A comparison with variational autoencoder models and their training procedures, which also involve stochastic hidden layers, would provide valuable insights and is currently lacking in the manuscript