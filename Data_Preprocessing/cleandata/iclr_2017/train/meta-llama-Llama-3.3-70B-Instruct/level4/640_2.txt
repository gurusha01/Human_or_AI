This study seeks to enhance the representation of multi-sense words by leveraging multilingual context, with experiments on word sense induction and word similarity in context demonstrating an improvement over the baseline.
From a computational linguistics standpoint, it is notable that languages less similar to English yield greater benefits. However, several issues with this work are apparent:
- The paper's complexity and lack of clear differentiation from the baseline model [1] make it challenging to discern its novelty. A dedicated discussion paragraph should explicitly compare and contrast this work with [1].
- The proposed model is essentially a variant of previous work [1], so the experimental design should be modified to isolate and quantify the contributions of each component to the improvement. Currently, the MONO setup has not been exposed to the same training data, making it uncertain whether the proposed model's superiority is due to its design or differences in data exposure or computational resources. To address this, a suggested baseline involves converting multilingual data into monolingual data using alignment and then training the baseline model [1] on this pseudo-monolingual data.
- While the paper provides valuable benchmarks for intrinsic evaluation, demonstrating improvement in a downstream task would strengthen the message and convey the impact more effectively.