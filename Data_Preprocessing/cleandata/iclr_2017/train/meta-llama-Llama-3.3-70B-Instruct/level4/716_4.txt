This paper proposes a lightweight network for semantic segmentation by integrating multiple acceleration techniques. However, as I previously inquired, the authors fail to demonstrate how their proposed techniques, such as factorizing filters into alternating 1-D convolutions, utilizing low-rank kernels, or incorporating newer inception network architectures, offer anything novel beyond existing knowledge.
I struggled to discern the primary contribution of this paper, as these concepts are well-established and have already proven effective in detection tasks. If applying these techniques to semantic segmentation is sufficient for acceptance, it sets a precedent where subsequent papers could be accepted for applying them to other tasks, such as normal estimation or saliency estimation, without adding significant value.
The authors acknowledge that improvements from classification architectures can be readily applied to object segmentation, which is exactly what they have done by basing their network on current state-of-the-art models. They claim to focus on in-depth discussions of segmentation-specific choices, which they consider the most important contributions. However, I found the discussion to be lacking in depth, with statements such as "these gave a significant accuracy boost" or "this helped a lot" being more akin to informal remarks than rigorous analysis.
The authors' evaluation is also limited, comparing their approach only to [1,2] (SegNet) in terms of accuracy and speed, without providing a clear justification for this choice. Notably, their results show that [1] requires approximately 1 second per frame, whereas Deeplab v2, without DenseCRF, achieves 5-8 frames per second. If novelty is not a primary concern, and performance or speed are the key metrics, I remain unconvinced by the authors' presentation.