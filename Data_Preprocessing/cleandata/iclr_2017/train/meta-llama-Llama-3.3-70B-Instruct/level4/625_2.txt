This paper introduces a novel architecture and accompanying algorithms for learning to perform multiple tasks described in natural language. The proposed system is hierarchical and closely related to the options framework, but instead of learning a discrete set of options, it learns a mapping from natural language instructions to an embedding that dynamically defines an option. This approach offers a fresh perspective on options, which has only been partially explored in linear settings. The use of policy distillation is particularly relevant in this context and could be a valuable takeaway for reinforcement learning (RL) readers, even those not specifically interested in natural language processing (NLP) applications.
The paper does not present a straightforward, end-to-end recipe for learning with this architecture. Instead, it skillfully combines several recent advances, including generalized advantage estimation, analogy-making regularizers, L1 regularization, memory addressing, matrix factorization, and policy distillation. While an in-depth analysis would be desirable, it is understandable that this would be a challenging task. For instance, the statement "while the parameters of the subtask controller are frozen" suggests the use of a two-timescale stochastic gradient descent, and it is unclear how the semi-Markov decision process (SMDP) structure is handled in the gradient updates when moving to the "temporal abstractions" setting.
I believe this approach has the potential to scale up to very large domains, although the paper does not currently provide empirical evidence to support this claim. While it is tempting to suggest larger experiments, it is also valuable that the system performs well in a "toy" domain. The characterization in figure 3 provides insight into the importance of the analogy regularizer and the need for hierarchy.
Overall, the proposed architecture is likely to inspire other researchers and is worthy of presentation at ICLR. It also introduces novel elements, such as subtask embeddings, which could be useful beyond the deep learning and NLP communities, extending into more traditional RL communities.
Regarding parameterized options, the concept was not originally explored by Sutton et al. (1999) but was later introduced in works such as Comanici and Precup (2010) and Levy and Shimkin (2011). Konidaris has also contributed to the area of "parameterized skills" (da Silva, Konidaris, and Barto, 2012; Masson, Ranchod, and Konidaris, 2015). It is essential to distinguish between two flavors of "parameterized options": those where policies and termination functions are represented by function approximators, and those where options take parameters as inputs and act accordingly, as in Konidaris' work. In this paper, the embedding of subtask arguments serves as input to the options, behaving as parameters in the sense of Konidaris.
In the related work section, it is notable that there are no references to S.R.K. Branavan's work, which used control techniques from RL to interpret natural instructions and achieve goals (e.g., "Reinforcement Learning for Mapping Instructions to Actions"). While Branavan's work focuses on different aspects, there are algorithmic and architectural similarities that should be discussed or compared in experiments.
The paper may also benefit from considering "Learning Shared Representations for Value Functions in Multi-task Reinforcement Learning" (Borsa, Graepel, and Shawe-Taylor) under the section on zero-shot task generalization.
Minor issues include clarifying the meaning of "instructions" in the abstract and defining "zero-shot" more explicitly, potentially with a citation. The terminology in section 3 could be revised to avoid implying a multi-agent setting, and the second sentence of section 6 could be split for better readability.