Here is a paraphrased version of the review:
Paper Summary:
The authors explore the concept of identity re-parametrization in both linear and nonlinear settings.
Detailed Comments:
— Linear Residual Network:
The authors demonstrate that for a linear residual network, every critical point corresponds to a global optimum, which is a notable result given the non-convex nature of the problem. The simplicity of the re-parametrization technique leading to this outcome is particularly interesting.
— Nonlinear Residual Network:
The authors propose a construction that utilizes a residual network to map points to their corresponding labels. This construction involves an initial random projection, followed by a residual block that clusters the data based on their labels, and a final layer that maps these clusters to the labels.
1. In Equation 3.4, there appears to be a dimensionality mismatch between $qj$ in $\mathbb{R}^k$ and $ej$ in $\mathbb{R}^r$. Could the authors provide clarification on this point?
2. While the proposed construction seems sound, it is unclear what specific benefits the residual network architecture provides in this context. Could a similar construction be achieved without the identity mapping? The authors should discuss this point and provide intuition on how the identity mapping facilitates optimization, as is evident from a spectral perspective in the linear case.
3. The existence of a residual network that can overfit the data provides some insight into why residual networks may outperform other architectures. However, it is essential to understand what this existence result implies about the representation power of residual networks. For instance, a simple linear model can overfit the data and achieve fast convergence rates under certain assumptions (e.g., Tsybakov's noise condition).
4. What implications does the proposed construction have on the number of layers required in the network?
5. The approach of clustering activations independently of the labels is a traditional method for pretraining networks. It is possible to use the resulting centroids as weights for the subsequent layer, which is related to the Nystrom approximation (see, for example, ...).