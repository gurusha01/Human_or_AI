The authors present a novel approach to language modeling, wherein a program is first generated from a domain-specific language (DSL) and then the count-based parameters of that program are learned. The key advantages of this method include its innovative nature, which deviates significantly from the conventional LSTM-based approaches that have been prevalent lately. Additionally, the proposed model is expected to be substantially faster during query time. The empirical results demonstrate strong performance in modeling code, although a noticeable gap exists between the synthesis method and neural methods on the Hutter task. A comprehensive description of the language syntax is also provided.
However, several areas require improvement:
- The MCMC-based synthesis procedure lacks clarity, which is a critical aspect, as optimizing the efficiency of this procedure is essential.
- Given that this work builds upon research from the programming languages literature, it would be beneficial to expand the related work section and contextualize this study more effectively.
- More concise and persuasive examples illustrating human interpretability would be valuable.
Further observations include:
- The training time evaluation in Table 1 would benefit from additional details, such as specifying whether training was conducted on a GPU or CPU, and providing CPU specifications, among other relevant information.