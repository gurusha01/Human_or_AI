*Edited the score 6->7.
This paper proposes a hierarchical reinforcement learning approach utilizing stochastic neural networks, incorporating an information-theoretic measure of option identifiability as an additional reward to learn a diverse mixture of sub-policies. A notable finding is the comparison with a strong baseline that combines intrinsic rewards with sparse rewards, demonstrating that this supposedly smooth reward is insufficient to solve tasks. The authors argue that the difficulty of long-term credit assignment and the benefits of hierarchical abstraction contribute to this outcome. However, another possible explanation lies in the diversity requirement imposed during sub-policy training, which is assumed to be absent in the baseline case. This insight may inform improvements to the baseline and the development of new end-to-end hierarchical policy learning methods, similar to hierarchical REPS or option-critic. The visualizations provided are also commendable.
The paper presents a promising direction, but could be further strengthened by addressing the following points:
1) The diversification of sub-policies is limited, as both concatenation and bilinear integration allow for only minimal differentiation through the first hidden weight. While this is not a significant issue in the tested tasks, which require similar locomotion policies with minimal diversification, it may become more apparent in tasks that demand more diverse ideal sub-policies, such as manipulation tasks. It would be interesting to see the application of this method to more challenging, non-locomotion domains.
2) The hierarchical policies have limitations, as the manager network is trained while the sub-policies are fixed, and the time steps for sub-policies are also fixed. This requires the "intrinsic" rewards and learned sub-policies to be highly effective in solving downstream tasks. Further discussion and results on handling such cases, ideally connecting to end-to-end hierarchical policy learning, would be beneficial.
3) The intrinsic/unsupervised rewards appear to be domain-specific or supervised, which may be unavoidable due to the limitations mentioned earlier.
Some last-minute questions arise: What is the intuition behind the strong baseline in section 7.3 performing poorly and failing to solve the task? Is the "CoM maze reward" the baseline in question? Does the baseline account for the extra experience pre-trained policies have received? What is the distinction between multi-policy and stochastic neural networks (SNNs)? In figure 1, is it correct that (1) concatenation refers to using different biases for the first hidden layer based on categorical values, and (2) bilinear integration refers to using different weight matrices to connect observations to the first hidden layer?