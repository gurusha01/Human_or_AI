This study introduces a novel approach to training reinforcement learning (RL) agents by incorporating auxiliary tasks, with the hypothesis that this will enable models to learn more robust features. The authors propose two pseudo-control tasks, which involve controlling changes in pixel intensity and the activation of latent features, as well as a supervised regression task that predicts immediate rewards following a sequence of events. The latter task is learned offline using a skewed sampling of an experience replay buffer to balance the likelihood of seeing a reward or not.
The results show that agents trained with these auxiliary tasks perform significantly well on discrete-action-continuous-space RL tasks, achieving baseline performance in substantially fewer iterations, approximately 10 times less. This approach differs from traditional "passive" unsupervised or model-based learning methods, which may force the model to learn a potentially useless representation of the input or an impossible task-modelling objective due to partial observability. Instead, learning to control local and internal features of the environment complements the learning of the optimal control policy.
The proposed approach is innovative and offers an interesting alternative to unsupervised learning, leveraging the agent's potential control over the environment. The tasks are explained at a high level, making it easy to understand the intuition, but additional details, such as explicitly mentioning L_PC before the appendix, would be beneficial for clarity. The methodology is sound, and the use of top-3 measurements ensures that the best hyperparameters for each method are found, assuming they are within the explored intervals.
However, one limitation of the paper is the lack of in-depth experimental analysis of the effect of the auxiliary tasks beyond their significant impact on performance. Further investigation into the effects of these tasks, particularly the pixel and feature control tasks, which seem to have the most substantial impact, would be valuable. For instance, in the Labyrinth environment, A3C+PC outperforms other methods, except for UNREAL, suggesting that a more detailed examination of this aspect would be worthwhile, potentially measuring more than just performance on RL tasks.