This manuscript proposes a video captioning framework that incorporates both soft and hard attention mechanisms, leveraging a C3D network as the encoder and a recurrent neural network (RNN) as the decoder, with experimental evaluations conducted on the YouTube2Text, M-VAD, and MSR-VTT datasets. Although the concepts of employing soft and hard attention for image captioning and soft attention for video captioning have been previously explored, the novelty of this work lies in its specific architectural design and the application of attention across various layers of the convolutional neural network (CNN).
The presentation of the work is clear, and the experimental results effectively demonstrate the advantages of utilizing attention across multiple layers. Nevertheless, considering the existing body of research in captioning, the contribution and the ensuing insights presented in this paper are somewhat incremental, which may not be sufficiently substantial for a conference like ICLR. To enhance the manuscript, additional experiments and a more in-depth analysis of the primary contribution would be beneficial. However, given the current state of the paper, I would suggest resubmitting it to a more appropriate venue for publication.