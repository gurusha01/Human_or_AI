Review- Summary:
This manuscript presents a novel approach to training recurrent neural networks by incorporating surprisal-driven feedback, wherein the network's next-step prediction error is utilized as an additional input. The authors demonstrate the efficacy of this method on language modeling tasks.
Contributions:
The key contribution of this work lies in the introduction of surprisal-driven feedback, which leverages the model's errors from preceding time-steps as feedback.
Questions:
One aspect that remains unclear is whether the authors employed ground-truth labels from the test set for the surprisal feedback component of the model. It is assumed that the authors utilized these labels, as they claim to incorporate misprediction errors as supplementary input.
Criticisms:
The manuscript suffers from poor writing quality, and the authors should reconsider the organization of the paper. Many of the equations related to backpropagation through time (BPTT) presented in the main text are unnecessary and could be relegated to the appendix. The provided justification is unconvincing, and the experimental results are limited, with only a single dataset being evaluated. Although the authors claim to have achieved state-of-the-art (SOTA) results on the enwiki8 dataset, this assertion is incorrect, as other studies, such as those using HyperNetworks, have reported better results (e.g., 1.34). A significant limitation of the proposed technique is its reliance on ground-truth labels for the test set, which restricts its applicability to a narrow range of tasks and essentially precludes its use in most conditional language modeling tasks.
High-level Review:
    Pros: 
        - The proposed modification to the model is straightforward and appears to yield improved results, making it an intriguing approach.
    Cons:
       - The requirement for test-set labels is a significant drawback.
       - The writing quality of the paper is subpar.
       - The assumption of access to ground-truth labels during testing is overly restrictive.
       - The experimental results are insufficient and lack diversity.