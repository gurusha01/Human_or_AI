This paper proposes a novel soft-target regularization technique, building upon the analysis of co-label similarity effects (Hinton et al., 2015), which iteratively trains the network using a weighted average of the exponential moving average of past labels and hard labels as the target argument of loss. The authors claim that this approach prevents the disappearance of co-label similarity after early training and yields a competitive regularization to dropout without sacrificing network capacity.
However, to establish a fair comparison with dropout, it is essential to carefully tune the dropout hyperparameters. The current results, which show superior performance for specific dropout values (Table 2), do not convincingly demonstrate an advantage, as dropout may perform better after reasonable tuning with cross-validation.
The experimental setup has several limitations. The baseline architectures used are not state-of-the-art, resulting in significantly lower accuracy. The absence of data augmentation in the experiment setup may also impact the results. Furthermore, the number of epochs is set to a small value of 100 without convergence tests, which raises concerns about the significance of the method. 
An alternative approach to calculating co-label similarities could be using softmax results at the final layer instead of predicted labels. The advantage of the proposed method over dropout is not clear in Figure 4, where dropout is set to 0.2 without cross-validation.
The idea of regularizing by enforcing training steps to maintain co-label similarities is intriguing but not particularly novel, and the results are not significant.
The strengths of the paper include providing an investigation of regularization on co-label similarity during training. However, the empirical results do not support the intuitive claims regarding the proposed procedure, and the iterative version can be unstable in practice.
Several concerns need to be addressed. Firstly, it is crucial to report misclassification error results on test data to provide conclusive comparisons. Secondly, the reason for using specific updates in equations (3) and (4) needs to be explained, and the choice of hyperparameters, such as β and γ, should be justified through cross-validation.
The setting of nt and nb is not satisfactory, and cross-validation of these parameters is essential. The choice of nt = {1,2} seems unreasonably small and may cause unstable results. It is also unclear why all nb and n_t values are equal, and results for other values should be reported.
The proposed method appears to be an iterative extension of distillation without using a larger teacher model. It would be interesting to compare the results with a two-step version of distillation to determine if the proposed method offers any advantages.
Additionally, the tuning process for λ in weight decay should be explained. The argument that a frozen set of hyperparameters was used to demonstrate the effectiveness of SoftTarget regularization without extensive grid search is not valid. A common hyperparameter tuning procedure, such as random search or Bayesian optimization, should be employed to ensure a fair comparison.
Overall, while the idea of soft-target regularization is interesting, the results are not convincing, and several limitations and concerns need to be addressed to establish the significance and effectiveness of the proposed method.