This manuscript introduces a straightforward approach to enhancing the training of deep neural networks by incorporating gradient noise. Although it was initially published on arXiv over a year ago, the field of deep neural network training has undergone significant advancements since then, including the development of batch normalization for RNNs, layer normalization, and normalization propagation, none of which are referenced or compared in this work.
Notably, the authors claim that applying batch normalization to recurrent networks has shown limited promise in improving generalization for recurrent architectures, citing Laurent et al. (2015). However, this assertion is inaccurate, as subsequent studies, such as Cooijmans et al. (2016), have demonstrated the effectiveness of batch normalization for RNNs.
The proposed methodology is remarkably simple and bears resemblance to various training strategies previously discussed in the literature. Consequently, its contribution would be modest at best, unless substantiated by robust empirical evidence supporting this specific variant. Nevertheless, the absence of empirical comparisons with other existing training strategies and algorithms in the literature is a notable limitation.
Regrettably, this manuscript is now substantially outdated. Given the current state of research, publishing it at ICLR 2017 would not be suitable.