This manuscript proposes a novel approach to learning sequence models for music by integrating likelihood and reward-based learning. The concept of combining these two paradigms has been well-established in the machine learning literature, dating back to the Expectation-Maximization (EM) formalism introduced by Attias (2003) for fixed horizons, and subsequently extended by Toussaint and Storkey (2006) to general horizon settings, Toussaint et al. (2011) to Partially Observable Markov Decision Processes (POMDPs), and further generalized by Kappen et al. (2012) and Rawlik et al. (2012). These foundational works demonstrated that any probabilistic or data-driven objective can be unified with the reinforcement learning signal, effectively creating a unified reward/likelihood framework. Consequently, the optimal control target under this unification can be represented as p(b=1|\tau)Ep(A,S) \prodt \pi(at|st), which encompasses both the probability of receiving a reward and the probability of policy actions under the known data-derived distribution, thus introducing the log p(at|st) term into equation (9).
The interpretation of the secondary objective as a prior is an alternative approach within the stochastic optimal control (SOC) setting, although it may not be the most intuitive one given the fundamental principle of SOC, which aims to align control objectives with inference objectives. Notably, the SOC off-policy objective still retains the Kullback-Leibler (KL) term, which distinguishes it from the approach presented in this paper.
While the discussion on optimal control is commendable, further elaboration on the historical context and the mechanism of reward augmentation in SOC would be beneficial. This would enable a more direct comparison between SOC off-policy methods and the Deep Q-Network (DQN) approach. The motivation behind objective (3) is reasonable, but its clarity could be enhanced by leveraging the unification argument mentioned earlier. The paper then employs a Differentiable Consensus Network (DCN) to pursue an alternative approach to variational SOC for achieving the specified objective.
An intriguing aspect worthy of discussion is the choice of Epi \log p(at|s_t), which implies that the policy must "cover" the model. However, in generation tasks, a well-trained model often suffers from underfitting, leading to actions that, over time, navigate the state into regions of the space not supported by the data. As a result, the model's confidence wanes, and its behavior becomes increasingly random. This approach, in contrast to a KL(p||pi) term (whose implementation is not straightforward), may not mitigate this issue without a strong signal to counteract the tails of the distribution. In the context of music, which typically involves a smaller discrete alphabet, this problem may be less pronounced compared to real-valued policy densities with exponentially decaying tails. Further discussion on the implications of this issue and the critical role of the balancing parameter c, as evident from Figure 2, where a high reward signal was necessary to align the log p signal appropriately, would be valuable.
Overall, this paper demonstrates the value of augmenting a sequence model with an additional reward constraint in the music setting. It shows that DQN can be used to learn this signal, although it does not compare the learning of this signal through other techniques directly. Instead, for comparator techniques, it reverts to treating p(a|s) as a "prior" term rather than a reward term, leaving some question as to whether DQN is the most suitable approach. An additional interesting point for discussion is whether the music theory reward could be approximated by a differentiable model, potentially mitigating the need for a reinforcement learning approach altogether.