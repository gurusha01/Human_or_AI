The Squeezenet paper, published in February 2016, presents a set of pragmatic engineering proposals for reducing parameter memory in CNNs for object recognition, specifically on the ImageNet dataset. These suggestions are well-founded and yield a significant compression of approximately 50x compared to AlexNet, and potentially up to 500x when combined with the work of Han (2015), making the results noteworthy and worthy of publication.
The fact that the arXiv version of the paper has already garnered attention and inspired extensions by other researchers demonstrates its potential impact and justifies its publication in a permanent venue.
However, there are some limitations to consider. The architecture's effectiveness has only been evaluated on ImageNet, leaving uncertainty about its applicability to other domains, such as audio or text recognition. Furthermore, similar to many architecture-focused papers, the ideas presented lack rigorous mathematical or theoretical underpinnings, relying instead on empirical validation and intuitive reasoning.
Overall, I believe this paper merits presentation at ICLR, as it contributes meaningfully to the ongoing research on deep learning architectures, aligning with the conference's main themes.