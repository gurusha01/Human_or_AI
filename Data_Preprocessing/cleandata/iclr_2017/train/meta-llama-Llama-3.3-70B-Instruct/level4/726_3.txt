The authors introduce "information dropout", a dropout variant grounded in information theory, where a dropout layer imposes a constraint on the information flow, quantified via a variational bound. 
However, the theoretical justification for this information bottleneck remains ambiguous. While Bayesian frameworks provide a theoretical foundation for parameter noise, the incorporation of activation noise lacks a clear motivation. Although the information bottleneck effectively restricts the information transmitted, a rigorous rationale for its potential to enhance generalization is absent.
The empirical evaluations are unconvincing. Notably, the CIFAR-10 results fall short of those reported in the original paper proposing the utilized network architecture (Springenberg et al). Furthermore, the VAE results on MNIST are subpar.