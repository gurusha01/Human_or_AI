This paper proposes a supervised deep learning approach that incorporates layer-wise reconstruction loss, in addition to supervised loss, and utilizes class-conditional semantic additive noise to enhance representation learning. The total correlation measure and insights from auto-encoders are leveraged to derive the layer-wise reconstruction loss, which is then combined with the supervised loss. The introduction of a class-conditional additive noise model, when combined with supervised loss, yields consistent improvements over the baseline model, as demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets with varying numbers of training examples per class.
However, the derivation of Equation (3) from total correlation appears to be somewhat ad hoc. Furthermore, under the assumption of a graphical model between X, Y, and Z, the estimation of H(X|Z) and H(Z|Y) should be more rigorously derived. The current approach, which involves encoding Z and Y from X and then decoding from the encoded representation, lacks clear justification.
It is unclear whether Ïƒ in Equation (8) is a trainable parameter or a hyperparameter. If it is trainable, the training process is not specified; if it is not, the method for setting its value is not provided. Additionally, it is unclear whether j corresponds to one of the classes. The proposed feature augmentation bears resemblance to simply adding Gaussian noise to the pre-softmax neurons, which raises concerns about its novelty. In fact, the proposed method seems to be similar to Gaussian dropout (Wang and Manning, ICML 2013), but applied to different layers. Moreover, a relevant reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied a synthetic noise process to the loss layer is missing.
To strengthen the experiments, it is recommended that they be conducted multiple times with different random subsets, and the authors should report the mean and standard error. Overall, the proposed method appears to lack robust justification and has limited novelty, which raises concerns about its contribution to the field.