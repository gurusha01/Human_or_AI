The authors have identified certain drawbacks of current deep architectures, notably their difficulty in optimizing on smaller or medium-sized datasets, and have proposed a novel approach involving the stacking of marginal Fisher analysis (MFA) to construct deep models. This proposed method has been evaluated on several small to medium-sized datasets and compared to various feature learning techniques. Additionally, the authors have incorporated established deep learning techniques, including backpropagation, denoising, and dropout, to enhance performance.
However, the paper's innovative contribution is somewhat limited, as MFA has been previously established. A significant omission is the lack of theoretical or empirical justification for the stacking of MFA components. Furthermore, the comparison does not include deep architectures that necessitate backpropagation across multiple layers, which was a primary issue the authors aimed to address; instead, all compared methods were learned layer by layer. It remains unclear whether a randomly initialized deep model, such as a Deep Belief Network (DBN) or Convolutional Neural Network (CNN), would perform poorly on these datasets. The authors have also not provided a clear explanation for the selection of specific model architectures and hyperparameters used for each dataset. The paper's writing requires substantial improvement, with many details omitted, such as the application of dropout in the MFA context.