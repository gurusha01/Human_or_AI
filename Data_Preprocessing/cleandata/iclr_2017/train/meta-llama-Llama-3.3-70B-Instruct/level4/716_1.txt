Paper summary: This work introduces ENet, a novel convolutional neural network architecture for semantic labeling, which achieves performance comparable to the existing SegNet while being approximately 10 times faster and using about 10 times less memory.
Review summary: Although the results appear intriguing, the paper is lacking in detailed experimental results and may not be of significant interest to the ICLR audience due to its limitations.
Pros:
* The proposed architecture is 10 times faster
* It requires about 10 times less memory
* The design rationale is described in detail
Cons:
* The reference baseline used for comparison has relatively low quality, with cityscapes results showing 58.3 IoU, whereas the state-of-the-art achieves approximately 80 IoU, making the results less compelling.
* The paper fails to provide experimental evidence to support the design rationale, which is crucial for validating the claims made.
Quality: The work is interesting but seems incomplete. Given that the model is 10 times faster and smaller, it would be beneficial to explore the possibility of building a larger model to achieve improved results. The paper's focus on speed over quality, using a weak baseline, limits its appeal to the ICLR audience.
Clarity: The overall text is somewhat clear, but the description of the model in section 3 could be improved for better understanding.
Originality: The work appears to be a compilation of practitioner's wisdom applied to a specific task, resulting in limited originality.
Significance: Establishing a new set of "best practices" can be interesting, but such work must excel in all aspects. Prioritizing speed over quality may restrict the impact of this work.
Minor comments:
* The text is generally well-written in proper English, but sentence constructions often seem unsound, as noted in specific examples below.
* To enhance the chances of acceptance, the authors are encouraged to explore larger models and demonstrate that the collected wisdom can be applied to achieve both high speed and high quality, with a suitable trade-off curve. Focusing solely on one end of the quality-speed curve may limit the paper's scope.
* Section 1: The statement "mobile or battery-powered devices require rates > 10 fps" should be more specific, including the energy budget, e.g., "> 10 fps && < X Watt."
* The term "rules" might be too strong; "guidelines" could be more appropriate.
* Phrases like "is of utmost importance" could be simplified to "is of importance."
* The sentence "Presents a trainable network... therefore we compare to... the large majority of inference the same way" lacks a logical connection and could be rephrased for clarity.
* "Scen-parsing" should be corrected to "scene-parsing."
* The description of the encoder and decoder as "separate" might be arguable.
* The relevance of "Unlike in Noh" should be made explicit or removed.
* "Real-time" is vague and could be specified as "X fps @ Y W."
* "Other existing architectures" could be simplified to "Other architectures."
* Section 3: The BN layer includes a bias term, and it would be beneficial to discuss whether good results can be achieved without any bias term.
* Table 1: The initial layer being a downsampling one, resulting in half the size of the input, could be explained.
* Section 4: The meaning of "settle to recurring pattern" in the context of non-linear operations is unclear.
* Section 4: Dimensionality changes are described as "computationally expensive," but this should be relative to something specific.
* Section 4: The claim that a technique "speeds-up ten times" needs experimental validation, especially when it does not provide the same results.
* Section 4: The issue found with using ResNet for semantic labeling could be more accurately described as an "issue" or "miss-match" rather than a "problem."
* Section 4: The term "asymmetric" for nx1 filters could be misleading; "rectangular filters" might be more appropriate.
* Section 4: The increase in variety due to factorizing filters is not clearly explained and seems counterintuitive.
* Section 4: The term "much better" in the context of regularization needs definition.
* Section 5.1: The statement "640x360 is adequate for practical applications" could be more specific, as it might only apply to certain applications.
* Section 5.2: "Very quickly" is vague and should be quantified.
* Section 5.2: There are minor typos, such as "Haver" instead of "have," and inconsistencies in capitalization.
* Section 5.2: The use of class weighting for class balancing should be clarified.
* Section 5.2: The description of Cityscapes should be in the present tense.
* Section 5.2: The weighting by the average object size should be clearly explained.
* Section 5.2: The fastest model in the Cityscapes benchmark should be specified as the fastest in the public Cityscapes benchmark.