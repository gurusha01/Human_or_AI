This proposal has potential to improve the efficiency of training recurrent networks, but I would appreciate more comprehensive experimental results to support the claims. Despite posing several questions earlier, I have yet to receive a response. Additional concerns and inquiries I have include:
- Does the proposed model retain its ability to universally approximate functions, given sufficiently large hidden dimensions and a suitable number of layers?
- More broadly, how does the expressiveness of this model compare to its equivalent without orthogonal matrices, particularly when controlling for the number of parameters?
- The experimental section is underwhelming, with a limited number of distinct input/output sequences and acknowledged instability during training, which makes the notion of "success" unclear. Although the authors acknowledge the need for expansion in this area, it appears this has not been adequately addressed.