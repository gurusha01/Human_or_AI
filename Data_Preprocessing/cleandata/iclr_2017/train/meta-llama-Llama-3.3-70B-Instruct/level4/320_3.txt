1) Summary
This manuscript presents a novel approach to visual servoing, specifically target tracking, by leveraging spatial feature maps derived from convolutional networks pre-trained on general image classification tasks. The authors integrate bilinear models of one-step visual feature map dynamics at multiple scales with a reinforcement learning algorithm to learn an effective servoing policy. This policy is optimized by minimizing a regularized weighted average of distances to predicted features, as modeled by the visual dynamics.
2) Contributions
* The paper provides controlled simulation experiments that quantify the effectiveness of pre-trained deep features for visual servoing, demonstrating clear performance advantages over various baselines, including those utilizing ground truth bounding boxes.
* A principled method for learning multi-scale visual feature weights is proposed, utilizing an efficient trust-region fitted Q-iteration algorithm to address the issue of distractors.
* The approach exhibits good sample efficiency, attributed to the choice of Q-function approximator and the model-based one-step visual feature dynamics.
* An open-source virtual city environment is introduced to benchmark visual servoing, offering a valuable resource for the community.
3) Suggestions for improvement
- Enhanced benchmarking: 
While the environment is more realistic than a simple synthetic setup, the experiments would greatly benefit from more complex and diverse visual conditions, including clutter, distractors, varied appearances, and motions. Incorporating a larger number of realistic 3D car models, obtained from sources like Google SketchUp, and increasing the number of distractor cars would significantly enhance the environment's realism and diversity. This is crucial for assessing the approach's robustness to visual variations.
- End-to-end training and representation learning: 
Although the current improvements in synthetic experiments are notable, investigating the impact of end-to-end training, including fine-tuning the convolutional network, would be valuable. This could lead to better generalization in more challenging visual conditions and provide insights into the benefits of deep representation learning for visual servoing, a relevant aspect for ICLR. Currently, the method lacks representation learning, despite its potential for straightforward adaptation.
- Reproducibility: 
While the formalism and algorithms are clearly explained, the multitude of practical tricks and implementation details presented throughout the paper and appendix can be overwhelming. To improve clarity, it would be beneficial to summarize the most critical implementation details and provide a link to an open-source implementation of the method for comprehensive reference.
- Typographical corrections: 
Page 2: "learning is a relative[ly] recent addition" and "be applied [to] directly learn" require correction.
4) Conclusion
Despite the limitations of the experiments, this paper is well-founded and interesting, particularly considering the authors' thorough response to pre-review questions and the subsequent revisions. This suggests that the authors are capable of addressing the proposed suggestions for improvement, potentially leading to an even stronger manuscript.