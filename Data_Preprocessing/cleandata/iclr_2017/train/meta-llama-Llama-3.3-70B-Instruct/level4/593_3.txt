This paper proposes a modified version of the semi-supervised variational auto-encoder (VAE) framework, incorporating structure through observed variables within the recognition network. 
I believe that the presentation of inference using auxiliary variables could be simplified, as it unnecessarily complicates the discussion. While these auxiliary variables facilitate a unified implementation, they are not essential for modeling purposes, and the same model can be achieved without them, resulting in a minimal extension of VAE where part of the generating space is observed. The observed variables require the posterior to condition on this additional information, which is accomplished in a manner similar to Kingma et al. 2014. I find it surprising that the experimental results show significant deviations between the two methods, and it would be beneficial for the authors to provide an explanation for the superiority of their approach. Additionally, I question whether the experimental setup for Fig. 5 (bottom) is identical to that of Kingma et al. 2014, particularly regarding the use of CNNs for feature extraction.
Similarly, I have concerns regarding the comparison with Jampani et al. 2015, specifically whether the supervision rate is consistent for a fair comparison. 
The experiment in section 4.3 is noteworthy, as it demonstrates a useful property of the proposed approach. The discussion on the supervision rate and the pre-review answer provides valuable insights into successful training protocols for semi-supervised learning.
However, the paper does not fully meet my expectations based on the title and introduction. The title suggested a method for interpreting general deep generative models, but the approach is actually a semi-supervised variant of VAE. While naturally incorporating labeled examples can disentangle the latent space, this is a general property of semi-supervised probabilistic models, not unique to this approach. Furthermore, the introduction implied a more general approximation scheme for the variational posterior, similar to Ranganath et al. 2015, which is not the case here.
In summary, the contributions of this paper lie in defining a slight variant of the semi-supervised VAE and formulating it in a way that facilitates easier automation in software. However, methodologically, there is limited contribution to the current literature. The authors' plan to extend the framework to the probabilistic programming setting seems promising and potentially useful. 
As a minor note, the repeated citation of Kingma's papers as Kingma et al. 2014 in the main text causes confusion, and I suggest using distinct citations, such as Kingma et al. 2014a, to clarify references.