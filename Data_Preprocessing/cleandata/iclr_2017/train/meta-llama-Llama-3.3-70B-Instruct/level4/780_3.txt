This paper's central argument is that, considering specific architectural features of multi-GPU systems, such as bi-directional PCI-E communication and the presence of two independent DMA engines on recent GPU devices, and given the communication patterns required by synchronous SGD trainers for deep neural networks, it is logical to design communication collectives like broadcast, reduce, and allreduce specifically for synchronous SGD training on multi-GPU systems. These communication patterns are characterized by large, dense messages of fixed length. The paper details the implementation of these collectives using a linear pipelining (LP) scheme on a logical ring topology. 
A comparison is made between the LP collectives and two alternative approaches: collectives based on a minimal spanning tree (MST) topology and those based on bidirectional exchange (BE). Initially, a theoretical comparison is conducted using a standard cost model from the high-performance computing community. By incorporating assumptions about multi-GPU system architecture (notably very low latency for messages) and the communication characteristics of synchronous SGD training (very large messages), the paper concludes that LP collectives should be less costly than BE collectives by a factor of 2 and less costly than MST collectives by a factor of log(p), where p represents the number of GPUs in use.
Empirically, the paper measures the time required to perform each collective on a 4-device (k40m) system as a function of message size and the time required for each collective with a 200 MB message length as a function of the number of devices. These measurements consistently show that LP-based collectives are the fastest. Furthermore, experiments training AlexNet and GoogLeNet on a 4-device system using three different synchronous SGD algorithms with different collective implementations demonstrate that LP collectives reduce communication costs without impacting computation costs, as expected. The convergence of training loss over time for both DNN architectures shows that using LP collectives leads to faster training.
However, while theory suggests that LP collective costs should be invariant to the number of devices, empirical results indicate that this invariance does not hold when moving from 4 to 5 devices due to messages having to traverse the QPI. It would be beneficial for the authors to discuss other practical considerations that might affect the scaling of LP collectives.
Certain sentences require clarification. For instance, the sentence referencing Worringen (2003) and discussing pipeline collective models in shared memory environments for CPU data is unclear, particularly the part following "but communications of different MPI processes." This sentence lacks comprehensibility and needs revision.
The claim that MST collectives are only suitable for high-frequency, short messages because they have the smallest latency term is misleading. It overlooks the importance of how the cost scales with message size (the bandwidth term). If MST collectives have a superior bandwidth term, they could also be preferable for large messages, not just short ones.
There appears to be a mistake in the condition "Let's take an appropriate block size b to ensure n/b ≪ α," as it should likely be "b/n ≪ α" given that n is greater than b.
The inconsistency in weight estimates across devices is attributed to precision issues with float multiplications in Gradient Update. However, this inconsistency might more accurately be attributed to the non-commutative nature of floating-point addition, as gradients are accumulated in different orders across devices.
Finally, it is recommended that the term "sub-gradients" be replaced with "partial gradients" throughout the paper, as "sub-gradient" has a specific meaning in the optimization literature that differs from its usage here.