This paper presents a neural network pruning approach that eliminates neurons with highly correlated operations, a concept that, although not entirely new, offers a fresh perspective by focusing on neuron removal rather than individual weight elimination. While the idea has potential, the manuscript requires enhancements in both its experimental and theoretical underpinnings to warrant publication:
1. Experimental Validation: The authors' assertion that network accuracy does not degrade upon pruning lacks substantial evidence. The absence of accuracy degradation data in the tables, coupled with the brief mention that networks did not degrade, is unconvincing. Figure 5 provides some insight but introduces inconsistencies with Table 2, where the parameter range (40k-600k) does not align with the range depicted in the figure (12k-24k). Without more detailed information, claims of achieving 50% neuron removal without accuracy degradation remain unsubstantiated.
2. Theoretical Foundation: The theoretical proofs presented do not align with the experimental conditions and are based on unrealistic assumptions. Specifically, the proofs demonstrate that in bias-free networks, constant output leads to correlated neurons generating the output offset. However, this overlooks the purpose of biases in neural networks and fails to explain the beneficial effect of noise injection. The proof suggests that deterministic auxiliary neurons should suffice, contradicting the observed benefits of noisy outputs. An alternative interpretation could be that noisy outputs introduce gradient noise, a concept explored in concurrent research (e.g., a recent ICLR submission), highlighting the need for a more nuanced theoretical explanation that accounts for the role of noise and biases in the pruning process.