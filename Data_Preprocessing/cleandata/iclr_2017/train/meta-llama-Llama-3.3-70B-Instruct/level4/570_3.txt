This paper presents a neural architecture designed to address non-factoid questions, showcasing an improvement over existing neural models in answer sentence selection. The experiments utilize a Japanese love advice corpus, with a notable highlight being the model's public deployment and subsequent rating as twice as effective as human contributors.
However, assessing the novelty of the contribution proved challenging. The authors claim their model bridges the gap between answer selection and generation, yet it does not actually perform generation. Instead, the model closely resembles the QA-LSTM of Tan et al., 2015, with the addition of terms in the objective function to handle conclusion and supplementary sentences. The answer structure is constrained to a predefined template, limiting the model's ability to learn sentence ordering. Another contribution is the "word embedding with semantics" described in section 4.1, which essentially adapts the paragraph vector model using titles and categories instead of paragraphs.
Although the paper demonstrates a model with real-world applicability, the technical contributions may not be sufficiently novel for publication in ICLR.
Additional comments include:
- The model's reliance on a fixed template hinders evaluation on standard non-factoid QA datasets like InsuranceQA. Allowing the model to learn the template could enable evaluation on diverse datasets.
- Table 4's examples do not clearly demonstrate the superiority of the proposed model over QA-LSTM in terms of answer quality.
- It is unclear whether the construction model's knowledge of conclusion and supplementary sentences gives it an unfair advantage over the vanilla QA-LSTM, or if QA-LSTM also possesses this distinction.