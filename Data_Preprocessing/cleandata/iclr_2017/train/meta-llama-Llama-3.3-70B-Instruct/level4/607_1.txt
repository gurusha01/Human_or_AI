The authors present a hierarchical attention model for video captioning, comprising three primary components: a temporal modeler (TEM) that generates a sequential video representation, a hierarchical attention/memory mechanism (HAM) that applies soft attention to this representation, and a decoder that produces a caption.
Regarding the terminology used, it appears that the authors refer to the output of the Bahdanau et al (2015) attention mechanism, implemented using an LSTM or equivalent RNN, as a hierarchical memory mechanism. While this terminology may be justified in the context of acknowledging the implicit memory in LSTM state vectors, it may overstating the significance of the paper's contribution.
The ablation study in Table 1 is a notable aspect of the paper, as it provides a thorough analysis of the model's components. However, the results suggest that the value of the contributions is not entirely clear, particularly with regards to the TEM, which seems to have a weak case.
The quantitative evaluation in Table 2 presents a narrow set of features to compare with existing literature. Given the variability in models and training datasets, it would be more convincing if the authors focused on achieving the best possible results, including fine-tuning the frame model if necessary. As an application paper, the incorporation of elements that significantly improve performance would be justified.
Ultimately, in its current form, the paper does not demonstrate a sufficient contribution to warrant publication in ICLR.