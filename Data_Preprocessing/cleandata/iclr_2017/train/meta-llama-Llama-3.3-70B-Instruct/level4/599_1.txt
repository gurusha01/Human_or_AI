This manuscript introduces a modified gated RNN, termed GRU-D, designed to handle time series data with substantial missing input values. The approach tackles this issue from two angles: firstly, by utilizing a learned convex combination of forward imputation (using the previous available value) and mean imputation (using the mean value) to address missing inputs directly; secondly, by incorporating a dampening mechanism into the recurrent layer, akin to a second reset gate, but with parameters that are time-elapsed sensitive since the last available value for each attribute.
Strengths
------------
- The task of handling missing values for time series classification is clearly defined.
- The inclusion of numerous baselines provides a comprehensive framework for testing the new model.
- The proposed model innovatively deals with missing values through a machine learning-based approach, learning new dampening parameters.
- The extensive testing on various datasets is a significant strength of the paper.
Weaknesses
-------------
- The manuscript would benefit from a thorough proofread to eliminate typos.
- Section A.2.3, which discusses important related works, would be more appropriately placed in the main article, potentially replacing imprecise model diagrams if space is a concern.
- The paper lacks reference to methods from the statistical literature.
Two key points underpin my assessment:
1. Although the results are promising, they fall short of expectations. The paper fails to convincingly demonstrate that GRU-D outperforms GRU-simple (without intervals) in handling missing inputs. GRU-simple, presented as a primary baseline, includes extraneous parameters (intervals) that, according to Table 5, may hinder the model's performance more than they help. This raises questions about the fairness of the comparison, especially since GRU-D does not significantly outperform GRU-simple (without intervals) in the table where it is included.
2. My primary concern lies with several unsubstantiated claims throughout the paper. The relationship between the presence rate of data in the dataset and diagnostics may merely indicate that medical analyses were conducted based on patient condition, suggesting an expert system would always be reactive rather than proactive. The introduction's final sentence sets high expectations that the paper does not fulfill. The assertion that simply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values is unsupported and later disproven. The conclusion that GRU models will continue to improve with more data, based on their performance increase from a subsample to the whole dataset, overlooks that non-GRU models started with better results. Lastly, the claim to capture informative missingness by incorporating masking and time intervals into the GRU architecture is questionable, given that the authors also concatenate the mask to the input, similar to GRU-simple (without intervals).
Given these concerns, while the work presented is above average, I would not accept the paper without a reframing of the findings to focus on the actual contribution, which I believe is the novel parametrization of the imputation method choice.