This manuscript tackles the crucial problem of assessing automatic dialogue responses, a task complicated by the limitations of existing evaluation methods (such as BLEU, which relies on N-gram overlap) that often fail to align with human-annotated quality standards. The authors propose a novel approach that leverages LSTM encoding to capture dialogue context, reference responses, and model-generated responses, incorporating a scoring mechanism. This method essentially involves training one dialogue model to evaluate the performance of another, introducing a layer of meta-evaluation. Nevertheless, the efficacy of this proposed metric is contingent upon the availability of a sufficiently robust initial dialogue model, which may not always be the case, potentially rendering the new evaluation metric ineffective.