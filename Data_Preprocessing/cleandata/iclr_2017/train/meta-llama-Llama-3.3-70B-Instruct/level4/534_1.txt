Hello Authors,
I would like to extend my congratulations on the acceptance of your paper. Upon re-examining the revised manuscript, I have identified a few aspects that may benefit from consideration and revision prior to the camera-ready deadline.
* The reference to KLIEP has been introduced following Equation (16), but it appears that this procedure is actually an instance of least-squares importance estimation. Conversely, Equation (14) bears a stronger resemblance to KLIEP, with the primary distinction being the utilization of the unnormalized form of the KL-divergence. It seems that the KLIEP reference would be more appropriately positioned in conjunction with Equation (14).
Regarding the practical application of Equation (14), it is noteworthy that the KLIEP paper formulates the procedure as a constrained optimization problem, wherein the objective is to maximize 
$$
E{p^*} log r\phi(x)
$$
subject to the constraint 
$$
E{q\theta} r_\phi(x) = 1
$$
Upon comparing this constrained optimization to the proposed solution, it becomes apparent that introducing a Lagrange multiplier to handle the constraint yields an unconstrained optimization problem. This involves seeking stationary points of 
$$
\ell(\phi, \lambda) = E{p^*} log r\phi(x) - \lambda E{q\theta} (r_\phi(x) - 1)
$$
I believe that solving this unconstrained optimization problem is feasible, potentially through stochastic gradient descent, and it does not involve the problematic cross-entropy term. I would appreciate clarification on this point.
The authors have presented a comprehensive and accurate overview of the emerging theory surrounding GANs from a likelihood ratio estimation and divergence minimization perspective. The manuscript is well-written and engaging, making it a valuable resource for individuals seeking to become involved in GAN research. However, as a reviewer, I found it challenging to pinpoint the precise novelty of the submission, beyond the articulate presentation of existing views. The paper's statement, "But it has left us unsatisfied since we have not gained the insight needed to choose between them," resonates with my sentiment, as the manuscript appears to be a well-crafted 'unifying review' that lacks a singular, novel insight.
In summary, my assessment is mixed: I consider this to be a high-quality paper that I enjoyed reading, but I was somewhat disappointed by the absence of a groundbreaking idea or novel insight, which is often expected in conference presentations. This sentiment is reflected in my scoring, and I remain open to being convinced otherwise.
Detailed comments:
* The connection between Equation (13) and KLIEP, as introduced by Sugiyama and colleagues, warrants discussion.
* The sections pertaining to Equations (13) and (14) appear somewhat disconnected from the overall narrative, which had established the notion that GANs are concerned with estimating likelihood ratios and utilizing these ratios to enhance the generator. These paragraphs seem to suggest an alternative formulation, but one that was not successfully developed in a practical manner.
* A typo is present in the spelling of Csiszar divergence.
* Equation (15) is recognized as Least Squares Importance Estimation, as proposed by Kanamori et al. (2009). A variant of this approach employs the kernel trick to identify a function from an RKHS that optimally represents the likelihood ratio between two distributions in a least squares sense. It would be intriguing to explore the relationship between this function and the witness function commonly used in MMD, as well as compare their properties for simple distributions.
* The authors could further emphasize the significant contributions of Sugiyama and collaborators to density ratio estimation, albeit with a different objective in mind.
* Regarding likelihood ratio estimation, some methods directly approximate the ratio (e.g., least-squares importance estimation), while others focus on approximating the log of this quantity (e.g., logistic regression, denoising autoencoders). An unbiased estimate of the ratio will yield a biased estimate of the logarithm, and vice versa. It seems that estimating the log of the ratio directly may be more useful, and in general, estimating the convex function of the ratio used to define the f-divergence appears to be a viable approach. I would appreciate the authors' commentary on this aspect.
* The hypothesis testing angle appears to be overstated in the paper. I am uncertain about the additional insight gained by incorporating hypothesis testing terminology, as the work does not truly engage with hypothesis testing concepts or methodologies. In contrast, Sutherland et al. (in review for ICLR) effectively borrow concepts from two-sample testing to optimize hyperparameters of the divergence used.