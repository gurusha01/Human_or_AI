This study presents a method for computing embeddings of symbolic expressions, such as boolean expressions or polynomials, in a way that semantically equivalent expressions are positioned close to each other in the embedded space. The proposed approach utilizes recursive neural networks, where the architecture is designed to mirror the parse tree of a given symbolic expression. To optimize the model parameters, the authors create a dataset of expressions with known semantic equivalence relationships and minimize a loss function, using a max-margin loss function to ensure that equivalent expressions are closer to each other than non-equivalent ones. Additionally, the authors employ a "subexpression forcing" mechanism, which, as understood, promotes the embeddings to maintain a form of compositionality.
The results are demonstrated on several symbolic expression datasets created by the authors, and the proposed method is shown to outperform baseline methods convincingly. Notably, the PCA visualization illustrates that negating an expression corresponds roughly to negating its embedding in the vector space, similar to the semantic relationships observed in word2vec and glove-style embeddings.
However, the paper's setting appears somewhat artificial, as it is unclear what real-world scenario would involve having a training set of known semantic equivalences while still benefiting from using a neural network for predictions. The authors have also simplified the problem by assuming distinct variables refer to different entities in the domain, which, although understandable, limits the applicability of the proposed method. For instance, the method would not be suitable for an "equation search engine" without a reliable way to canonicalize variable names.
Other notable points include:
* The problem of determining expression equivalence is potentially undecidable, as related to the "word problem for Thue systems." The authors' method for determining ground truth equivalence in their training sets, which involves simplifying expressions into a canonical form and grouping, may not be generally possible. It is unclear whether equivalent expressions in the training data could have been mapped to different canonical forms, and whether constructing and comparing truth tables might have been a more feasible approach.
* The "COMBINE" operation uses a connection that resembles a residual connection but is not identical, as it involves a weight matrix multiplied by the lower-level features. A true residual connection would have used an identity connection, which could have helped mitigate gradient explosion. The reason for using this modified connection instead of an identity connection is not clear.
 In Table 3, the first tf-idf entry, "a + (c+a)  c," appears to be equivalent to "a + (c * (a+c))".
* The vertical spacing between the Figure 4 caption and the body of text is insufficient, making it seem like the caption continues into the text.