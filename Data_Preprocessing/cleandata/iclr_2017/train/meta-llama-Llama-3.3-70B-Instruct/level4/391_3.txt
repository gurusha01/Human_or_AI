Updated review: 18 Jan. 2017
I appreciate the authors' inclusion of a comparison to the sparsity method proposed by Yu et al. in 2012. Although the comparison is reasonable, it would be more transparent if the authors explicitly stated that the most relevant comparison for the results in Table 4 is the "RNN Sparse 1760" result in Table 3.
I have revised my evaluation to reflect my assessment of the revised manuscript, while also retaining the original review to maintain a record of the paper's development.
This manuscript presents three primary contributions: (1) a method for training sparse Recurrent Neural Networks (RNNs) by masking weights below a specified threshold to zero, with a threshold schedule that postpones pruning until a certain number of iterations have been completed and gradually increases the threshold during training; (2) experimental results on an internal Baidu task using the Deep Speech 2 network architecture, demonstrating that applying sparsification to a large model can yield a trained model with improved performance and fewer non-zero parameters compared to a dense baseline model; and (3) timing experiment results using the cuSPARSE library, indicating potential for faster model evaluation with sufficiently sparse models, although the current cuSPARSE implementation may not be optimal.
Strengths:
+ The manuscript is generally well-organized and easy to follow.
+ The authors address a crucial, practical problem in deep learning: reducing computational and memory costs for model deployment.
Weaknesses:
- The manuscript would benefit from a comparison to "distillation" approaches (e.g.,