This paper presents a novel approach to training neural networks by utilizing a hypernetwork to generate the model parameters of the primary network. The authors successfully demonstrate that this method can achieve competitive results on image classification tasks while reducing the total number of model parameters. Notably, the hyperLSTM with non-shared weights exhibits exceptional performance compared to traditional LSTM and its variants on several language model benchmarks, which is particularly noteworthy.
--pros
A key strength of this work is its demonstration of the feasibility of generating neural network model parameters using another network, as evidenced by several large-scale experiments. The underlying concept is highly inspiring, and the experimental results are robust and well-founded.
--cons
To enhance the paper's impact, it would benefit from a more focused approach. Specifically, the primary advantage of the hypernetwork method is unclear. While the paper argues that competitive results can be achieved with a smaller number of trainable model parameters, the computational complexity remains equivalent to that of the standard main network for static networks, such as ConvNet, and is even higher for dynamic networks like LSTMs. The improvements observed in hyperLSTMs over conventional LSTM and its variants appear to stem primarily from an increase in model parameters.
--minor question
The ConvNet and LSTM architectures used in the experiments lack a large softmax layer, which can exceed 100K in many word-level tasks for language models or machine translation. It is unclear whether the hypernetwork would encounter challenges in generating a large number of weights for such cases and whether this would significantly slow down the training process.