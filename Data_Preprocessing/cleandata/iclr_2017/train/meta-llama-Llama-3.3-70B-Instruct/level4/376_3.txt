The authors conduct a comprehensive examination of various existing and two novel RNN architectures to gain a deeper understanding of their ability to store task information in their parameters and activations.
The experimental design appears to be well-founded, as it considers multiple tasks of varying complexities, employs a principled hyperparameter tuning methodology, and utilizes a substantial number of tuning iterations, which is currently only feasible with the computational resources available to some large industrial research groups. This approach enables a more generalized comparison between different architectures, while controlling for the effects of hyperparameters.
The descriptions of the models and objectives are clear, but the explanations of the experiments and presentation of the results are occasionally unclear, even with the supplementary details provided in the appendix. However, these issues can be readily addressed through editing. For instance, in the memory task, the scaling of the inputs and outputs is not specified, making it challenging to interpret the squared error scores in Figure 2c. Additionally, the term "unrollings" in Figure 2b is unclear, and it would be beneficial to clarify whether it refers to a time lag with additional hidden state updates between the presentation of the input sequence and the generation of the output. A more detailed description of how and when predictions are computed in the perceptron capacity task would also be helpful, given its centrality to the paper. The large number of graphs can make it difficult to identify the most relevant results, and consider relocating some of the more obvious findings, such as Figure 1(b-d), to the appendix to make room for more detailed task descriptions.
While novelty is not the primary focus of this paper, as it primarily investigates existing architectures, the use of mutual information to obtain bits per parameter scores in highly non-linear parameterized functions is a new approach. The paper also proposes two new architectures that appear to have practical value, contributing to the ongoing research effort to leverage larger computational resources to better understand architectures designed when such resources were not available. The paper's originality lies in its ability to provide new insights into the properties of RNNs, making it a valuable contribution to the field.
The paper offers several interesting new insights into the properties of RNNs, including the importance of gated units for maintaining trainability in networks with many layers, and the potential use of vanilla RNNs for simpler tasks where high capacity per parameter is required due to hardware constraints. The proposed +RNN may also have practical value, and the hyperparameter robustness results provide insight into the popularity of certain architectures when hyperparameter tuning time is limited. The extensive results and hyperparameter analysis will be useful to many researchers who intend to use RNNs in the future. Overall, this paper would be a valuable addition to the ICLR conference, but would benefit from some improvements to the text.
Pros:
* Comprehensive analysis
* Well-designed experiments
* Novel approach to quantifying capacity in neural networks
* Practical value of the results and suggested analysis of other architectures
* Useful insights into the relative merits of different RNN architectures
Cons:
* Difficulty in isolating the most important findings due to redundant plots
* Missing relevant experimental details