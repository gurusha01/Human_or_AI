Paper summary
This paper revisits the concept of utilizing a binary classifier for two-sample testing, where the sample is divided into separate training and test sets. A classifier is trained on the training set, and its accuracy on the test set serves as the test statistic. If the accuracy exceeds chance level, it is inferred that the two samples originate from different distributions, leading to the rejection of the null hypothesis.
The paper provides a theoretical result regarding the asymptotic approximate test power, implying that the test is consistent if the classifier performs better than random guessing. Experimental results on toy problems, GAN evaluation, and causal discovery demonstrate the test's effectiveness. Furthermore, when a neural network is used as the classifier, examining the first linear filter layer reveals the most activated features, providing an interpretable visual indicator of the differences between the two samples.
Review summary 
The paper is well-structured and easy to comprehend. Although the idea of using a binary classifier for two-sample testing is not novel, as acknowledged in the paper, the main contributions lie in the analysis of asymptotic test power, the application of modern deep neural networks as classifiers, and the empirical studies on various tasks. The empirical results are convincing, and while the paper could benefit from a more in-depth discussion on why the method performs well in practice, the overall contributions have the potential to initiate a new research direction in model criticism of generative models and visualization of model failures. I recommend acceptance.
Major comments / questions 
My primary concern regards Theorem 1 (asymptotic test power) and its underlying assumptions. However, I understand that these can be addressed as discussed below.
* Under the null hypothesis, the test statistic (the sum of 0-1 classification results) indeed follows a Binomial distribution with parameters (nte, 1/2), as stated. Nevertheless, under the alternative hypothesis, the terms in the sum are independent but not identically distributed Bernoulli random variables, since each term depends on a data point zi that can originate from either distribution P or Q. Consequently, the statement in Section 3.1, "...the random variable nte \hat{t} follows a Binomial(nte, p)..." is inaccurate, as p is dependent on z_i. Instead, it should follow a Poisson binomial distribution.
* In the same paragraph, due to the same reason, the alternative distribution of Binomial(nte, p=p_{risk}) is likely incorrect. I assume this is mentioned to invoke the Moivre-Laplace theorem and obtain asymptotic normality. However, this statement is not necessary, as only the asymptotic normality of the Binomial distribution is required for the proof. A variant of the central limit theorem (instead of the Moivre-Laplace theorem) for independent, non-identical variables can still be used to conclude the asymptotic normality of the Poisson binomial distribution, given certain conditions. For example, see...