The authors have introduced a methodology for assessing the generation of out-of-distribution novelty, suggesting that a model trained on MNIST digits can be considered capable of generating novel samples if it produces outputs that resemble letters, as judged by a separate model trained on both MNIST and letter datasets. Empirical experiments were conducted to support this claim.
However, the concept of novelty itself is inherently challenging to define, and the proposed metric raises several concerns. A straightforward combination of MNIST and letter datasets does not accurately represent the natural distribution of handwritten digits and letters, which implies that a model trained on this combined dataset may struggle to effectively distinguish between the two. As a result, the proposed out-of-class count and out-of-class max metrics lack validity. Upon examining the "novel" samples presented in Fig. 3, it is apparent that they are, in fact, digits. It appears that the samples may have been quantized to binary, and if they were instead quantized to 8-bit, the resulting images would likely resemble digits even more closely.