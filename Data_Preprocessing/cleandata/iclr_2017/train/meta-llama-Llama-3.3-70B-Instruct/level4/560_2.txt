This manuscript is well-grounded in its motivation and aligns with recent research exploring the application of orthogonal weight matrices in recurrent neural networks (RNNs). The use of orthogonal weights mitigates the issue of vanishing or exploding gradients, but it remains uncertain whether enforcing orthogonality compromises the representational power or trainability of the model. Therefore, an empirical study examining the effects of deviating from orthogonality is a worthwhile pursuit.
The manuscript is well-structured and clearly written. The primary formulation for investigating soft orthogonality constraints, which involves representing weight matrices in their Singular Value Decomposition (SVD) factorized form to exert explicit control over singular values, is both elegant and intuitive. Although this approach may not be ideal from a computational efficiency perspective due to the requirement of maintaining and updating multiple orthogonal weight matrices, it appears to be a novel investigation. 
However, the experimental component of the study is somewhat underwhelming. The evaluation is limited to two tasks: a copy task utilizing an RNN architecture without transition non-linearities and the sequential/permuted sequential MNIST task. While these tasks are reasonable initial evaluations, they are simplistic and do not provide substantial insights into the practical implications of the proposed approach. An assessment in a more realistic context, such as a language modeling task, would be highly beneficial.
Moreover, although examining pure RNNs is sensible for understanding the effects of orthogonality, it seems somewhat theoretical given that Long Short-Term Memory (LSTM) networks also offer a mechanism for capturing long-term dependencies. In the tasks where the proposed approach was directly compared to an LSTM, the LSTM significantly outperformed it. It would be intriguing to explore the impact of the proposed soft orthogonality constraint in other architectures, such as deep feed-forward networks or when integrated within an LSTM, although the latter seems unlikely to yield benefits.
In summary, the manuscript addresses a pertinent question with a well-motivated methodology and presents interesting findings on select toy datasets. As such, it has the potential to make a valuable contribution. Nonetheless, the significance of this work is tempered by the constrained experimental settings, both in terms of datasets and network architectures.