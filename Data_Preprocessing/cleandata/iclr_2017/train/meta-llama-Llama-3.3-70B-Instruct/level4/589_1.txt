This paper presents a framework that integrates graph convolutional networks with Recurrent Neural Networks (RNNs) to address graph-structured input problems. The approach is built around two primary concepts: (i) utilizing a graph convolutional layer to extract features that are subsequently input into an RNN, and (ii) substituting matrix multiplications with graph convolution operations. The application of concept (i) to language modeling achieves a lower perplexity on the Penn Treebank (PTB) dataset compared to traditional LSTM models. Concept (ii) demonstrates superior performance over the combination of LSTM and CNN on the moving-MNIST dataset.
However, upon closer examination, both proposed models and their underlying ideas align closely with current trends in combining disparate architectural elements, lacking a significant degree of novelty. For example, the notion of replacing matrix multiplications with graph convolutions can be seen as a minor extension of the work by Shi et al.
The experimental methodology outlined in section 5.2 for the PTB dataset raises concerns. Instead of leveraging the provided development set for hyperparameter tuning, the authors opted to use a pre-existing configuration tailored for a different model, which may not be optimal for the proposed architecture.
The strengths of the paper include its noteworthy experimental results. However, it is also marked by several weaknesses: the ideas presented are somewhat straightforward and do not significantly advance the state-of-the-art, and the experimental approach for the PTB dataset is questionable due to the improper use of the development set.