The authors contend that traditional ancestral sampling methods employed in stochastic autoencoders, such as Variational Autoencoders and Adversarial Autoencoders, are limited by the requirement that the encoder distribution must match the prior distribution of the latent variables. As an alternative, they suggest a Markov Chain Monte Carlo approach that eliminates the need for a simple parametric form of the prior.
However, the paper suffers from clarity issues, particularly with regards to notation. The authors' use of notation appears to be either fundamentally flawed or based on a misunderstanding of probability distribution manipulations. For instance, they imply that both Q(Z|X) and Q(X|Z) are parametrized, which would only be possible in trivially simple models or energy-based models, without indicating that they are referring to the latter. Another potential point of confusion is the statement that the ratio of distributions Q(Z|X)/P(Z) equals 1, which seems to be intended as a ratio of marginals, Q(Z)/P(Z) = 1. This suggests a confusion regarding the representation of Q and P, as the standard notation in VAEs uses P for the decoder distribution and Q for the encoder distribution, which does not appear to be the case here.
The empirical results are limited to qualitative assessments, comprising samples and reconstructions from a single dataset, CelebA. Moreover, the quality of the samples does not approach that of state-of-the-art models. The interpolations presented in Figures 1 and 3 appear to resemble pixel-space interpolations for both the VAE model and the proposed DVAE, raising concerns about the effectiveness of the proposed method.