The paper presents SampleRNN, a hierarchical recurrent neural network model for raw audio, which is trained end-to-end and evaluated on three datasets covering speech and music using log-likelihood and human judgment of unconditional samples. The results indicate that the proposed model compares favorably to the baselines.
Notably, the subsequence length used for truncated backpropagation through time (BPTT) significantly impacts performance, yet a subsequence length of 512 samples (~32 ms) yields good results, despite the modeled features spanning much longer timescales. This outcome is intriguing and warrants further discussion to fully understand its implications.
The authors have also attempted to reimplement WaveNet, a fully convolutional alternative model of raw audio, although they were unable to replicate the exact model architecture from the original paper. Instead, they built a model with a receptive field of approximately 250ms, which is commendable given their computational resources. However, the description of the WaveNet model architecture is more detailed than that of the proposed SampleRNN architecture, making it challenging to find specific details such as the value of "r" used for different tiers or the number of units per layer. A comparison of computational cost, training time, and number of parameters between the models would be highly informative.
The results in Table 1 are surprising, as a vanilla RNN (LSTM) substantially outperforms the proposed model in terms of likelihood, which is unexpected given the larger receptive field of the WaveNet model. Similarly, Figure 3 shows the vanilla RNN outperforming the WaveNet reimplementation in human evaluation on the Blizzard dataset, raising questions about the implementation. The authors' discussion of these results and their expectations would be valuable.
Additionally, Table 1 and Figure 4 show the 2-tier SampleRNN outperforming the 3-tier model in terms of likelihood and human rating, respectively, which is counterintuitive given the expected importance of longer-range temporal correlations in music. This result warrants further discussion to understand the underlying reasons.
Overall, this paper presents an interesting approach to modeling long sequences with long-range temporal correlations, and the results are convincing, although the comparison with baselines raises some questions. It would be interesting to explore the model's performance in conditional generation, where it can be more objectively compared to models like WaveNet.
Other notable points include the use of r separate linear projections for upsampling the output of the models, which lacks motivation. The choice of upsampling method and its advantages over alternatives like linear interpolation or nearest neighbor upsampling should be clarified. Furthermore, the use of 8-bit linear PCM, in contrast to WaveNet's 8-bit mu-law encoding, may impact audio fidelity, and it is unclear whether the authors explored this alternative. Finally, the reference to prior work on discretization of the input and the use of a softmax to model this discretized input should be moved to avoid implying that this is a novel observation.