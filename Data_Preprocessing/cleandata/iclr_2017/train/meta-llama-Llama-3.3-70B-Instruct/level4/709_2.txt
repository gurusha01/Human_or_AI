Review - strengths:
This paper presents a novel approach to initializing the encoder and decoder of a seq2seq model using pre-trained language model weights, eliminating the need for parallel data. The pre-trained weights are then fine-tuned jointly with parallel labeled data, supplemented by an additional language modeling loss. The results demonstrate that this pre-training strategy enhances training efficiency and improves the generalization capabilities of seq2seq models. A key advantage of this method is its ability to utilize separate source and target corpora, diverging from conventional approaches that rely on large parallel training datasets.
weaknesses:
The objective function introduced on page 3 appears to be largely empirical and lacks a direct connection to the mechanism by which non-parallel data contributes to improved prediction outcomes. To strengthen the paper, the authors should compare and discuss their objective function in relation to the expected cross-entropy objective, as proposed in Chen et al.'s 2016 work, "Unsupervised Learning of Predictors from Unpaired Input-Output Samples" (arXiv:1606.04646), which has a more direct link to enhancing prediction results. Furthermore, the pre-training procedure outlined in this paper bears similarities to the DNN pre-training method described in Dahl et al.'s 2011 and 2012 studies. The authors should provide comparisons and clarify the conceptual superiority of their proposed approach, if they believe it to be so.