SUMMARY.
This paper presents a machine reading approach for cloze-style question answering, utilizing a bidirectional Gated Recurrent Unit (GRU) to encode both the query and the document. The encoded representations are then combined using a Gated Attention (GA) mechanism, which calculates the compatibility between each word in the document and the query as a probability distribution. For each document word, a gate is computed, weighting the query representation based on word compatibility, and applied to the GRU-encoded document word. The resulting word vectors undergo re-encoding with a bidirectional GRU, a process repeated for multiple hops. After k hops, a log-linear model calculates the probability of each word being part of the answer, taking as input the final word representations and the concatenated query representation before and after the cloze token. The probability of a candidate answer is determined by a linear combination of single word probabilities.
The proposed model is evaluated on four distinct datasets, demonstrating state-of-the-art performance on three out of four benchmarks.
----------
OVERALL JUDGMENT
The paper's primary contribution lies in its introduction of the gated attention mechanism, a concept that is both straightforward and intriguing. The paper is well-structured, and the ablation study effectively highlights the benefits of the gated attention mechanism. The GA reader model outperforms previous state-of-the-art models on three benchmarks and shows promise on the CBT dataset. However, it would be beneficial to include a discussion on the model's relatively poor performance on the CBT dataset.
----------
DETAILED COMMENTS
Minor: The introduction incorrectly states that Weston et al. (2014) employed an attention mechanism.