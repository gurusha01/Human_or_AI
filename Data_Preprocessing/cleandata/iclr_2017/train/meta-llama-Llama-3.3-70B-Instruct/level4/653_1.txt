This manuscript provides a theoretical foundation for the faster convergence of stochastic gradient descent (SGD) and averaged stochastic gradient descent (ASGD) when utilizing small mini-batches, particularly in scenarios involving a limited number of learners. The analysis reveals a potential inefficiency in accelerating gradient descent methods through parallelization that leverages hardware capabilities. Overall, the paper presents a compelling narrative, effectively bridging the gap between algorithmic design and hardware characteristics.
However, upon closer examination, I have reservations regarding the accuracy of Lemma 1. Specifically, it appears that the factor $Df / S$ should be adjusted to $Df / (S*M)$ to ensure correctness. I would appreciate it if the authors could address this discrepancy and verify the subsequent theorem to confirm its validity in light of this potential correction.