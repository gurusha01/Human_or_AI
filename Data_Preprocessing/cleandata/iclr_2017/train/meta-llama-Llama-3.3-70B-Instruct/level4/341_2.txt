This paper introduces a novel problem setup for imitation learning, where an agent attempts to replicate a trajectory demonstrated by an expert in a different state or observation space, despite sharing the same underlying Markov Decision Process (MDP) dynamics. The authors propose a solution that combines recent work on domain confusion losses with a generative adversarial network (GAN)-based inverse reinforcement learning (IRL) method.
The problem setup is deemed relevant, and the authors' argument that it provides a more natural formulation for imitation learning, potentially increasing its applicability, is convincing. However, several issues detract from the paper's overall quality. 
One major concern is the paper's rushed writing style, particularly evident in the experiments section, and the introduction's unsubstantiated claims. For instance, the statement that advancements in this area would significantly improve robotics by enabling easy skill transfer to robots seems exaggerated, given the method's reliance on accurate simulators and its susceptibility to GAN training instability. Furthermore, grammatical errors and inconsistent sentence tenses hinder the reading experience.
The concept of third-person imitation learning is novel and intriguing, but the authors' decision to extend a specific GAN-based IRL approach, rather than exploring the adaptability of various IRL algorithms to this setting, raises questions. It is unclear why other existing IRL algorithms, such as behavioral cloning, could not be modified with domain confusion losses to achieve similar results. A more comprehensive discussion on the adaptability of different algorithms and empirical validation of the proposed approach's superiority would strengthen the paper.
The lack of empirical comparison with existing IRL algorithms in the proposed setting is another significant concern. The authors fail to investigate how supervised learning or other IRL methods would perform in this context, leaving important questions unanswered. Additionally, the experimental evaluation, while well-structured around specific questions, seems to overlook crucial aspects, such as the cost of TRPO training and the sensitivity of results to various hyperparameters.
The experimental methodology also lacks essential details, including the process of obtaining expert trajectories, the specifics of the pendulum experiment domains, and the variance of results in Figure 5. Given the challenges of training GANs, it would be beneficial to discuss the frequency of training failures and the reusability of hyperparameters across experiments.
Overall, while the paper presents an interesting problem setup and a potentially promising solution, it falls short due to its rushed writing style, unsubstantiated claims, and lack of comprehensive experimentation and analysis.