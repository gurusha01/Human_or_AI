This study leverages the stochastic nature of neural network outputs under randomized augmentation and regularization techniques to generate targets for unlabeled data in a semi-supervised setting. By applying stochastic augmentation and regularization to a single image multiple times per epoch and promoting output consistency (Π-model), or by maintaining a weighted average of past epoch outputs and penalizing deviations from this running mean (temporal ensembling), the approach yields ensemble predictions that are likely more accurate than the current network, making them suitable targets for unlabeled data. Both methods demonstrate impressive performance on semi-supervised tasks, with some results showing remarkable robustness to label noise.
The paper is well-written, providing sufficient details for reproducibility, along with a publicly available code base. The core idea is intriguing and appears to result in higher semi-supervised accuracy than previous work. The discussion on the impact of different data augmentation choices is also noteworthy. 
However, I find it surprising that a standard supervised network can achieve 30% accuracy on SVHN with 90% random training labels, which would only yield 19% correctly labeled data. Although the remaining 81% may not provide a consistent training signal, this result seems counterintuitive. Unfortunately, the GitHub repository for this experiment is not available for further examination.
The resistance of Π-model and temporal ensembling to label noise is more plausible, given the significant weights assigned to the consistency constraint. The authors should consider including a discussion on w(t) in the main paper, particularly in light of the substantial difference in w_max in the incorrect label tolerance experiment. 
It would be beneficial for the authors to comment on the scalability of their approach for larger problems, such as ImageNet, which would require substantial storage or increased training time. Additionally, a discussion on the sensitivity of this approach to the amount and location of dropout layers in the architecture would be valuable.
Preliminary rating: I consider this to be a very interesting paper with quality results and clear presentation.
Minor note: In the 2nd paragraph of page one, 'without neither' should be corrected to 'without either'.