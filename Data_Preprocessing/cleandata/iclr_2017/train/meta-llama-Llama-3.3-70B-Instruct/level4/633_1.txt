The authors present a novel approach that integrates an energy-based model, or descriptor, with a generator network, enabling them to mutually enhance each other's performance. Specifically, the generator's output is utilized as the initial point for the descriptor's inference process, and conversely, the refined samples produced by the descriptor are used to update the generator by serving as target images.
This concept is intriguing, but a significant limitation lies in the lack of convincing empirical evidence to support the benefits of this architectural design. For instance, a quantitative assessment of how initializing the descriptor with generator samples improves performance is expected. Furthermore, the sole quantitative experiment on reconstruction is compared to somewhat outdated models, which may not provide a comprehensive understanding of the proposed model's capabilities. Given the similarity between the proposed model and the one introduced by Kim & Bengio in 2016, a comparison to this model would be anticipated by readers to contextualize the advancements.
Minor Comments
- The analysis of convergence may be questionable due to the biased nature of samples generated by Stochastic Gradient Langevin Dynamics (SGLD) with a fixed step size, which could potentially impact the validity of the conclusions drawn.
- Additional clarification on the derivation of Equation 8 would be beneficial, particularly in explaining how the conditional probability p(x|y) depends on the generator's weights, W_G.