This manuscript explores the concept of merging multiple layers, typically a convolutional layer and a layer such as LRN or pooling, into a single convolutional layer through retraining, demonstrating the potential for creating simpler and faster models with minimal accuracy loss. The underlying idea is sound. However, several concerns need to be addressed:
- The introduction of the 'Deeprebirth layer' concept is misleading, as it initially appears to represent a novel architectural contribution. Instead, it is revealed to be essentially a convolutional layer, with its specific implementation varying depending on whether serial or parallel pooling layers are fused. While naming techniques can be beneficial, in this case, assigning a name to a layer that encompasses multiple, non-innovative architectural elements obscures the clarity of the argument.
- Alternative methods for achieving operator fusion without retraining exist, and certain deep learning frameworks like Theano and the forthcoming TensorFlow XLA have implemented these approaches. Incorporating a baseline that utilizes such methods would have been valuable, particularly given that the primary energy savings from operator fusion stem from the elimination of intermediate memory writes.
- Batch normalization can be integrated into convolutional layers without retraining by adjusting the weights. It is unclear whether this optimization was applied in the baseline figures presented in Table 7.
- At the time of review, the authors have not provided sufficient details to ensure the reproducibility of their research, notably regarding the relationship between the depth of the fused layers and the original layers in each experiment.
- The retraining process lacks specific information, such as the duration (in epochs) and whether knowledge distillation was considered as an approach.
The experiments presented are intriguing, but significant improvements are necessary for this paper to be considered suitable for publication.
- Making the implementation open-source would enhance the paper's utility, although this is not a strict requirement.
Overall, while the idea has potential, the current presentation and methodology require substantial refinement.