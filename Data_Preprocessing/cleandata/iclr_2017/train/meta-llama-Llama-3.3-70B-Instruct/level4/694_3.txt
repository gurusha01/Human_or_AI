UPDATE: Having reviewed the authors' responses, I maintain my initial stance. The updates provided by the authors are appreciated, but my overall assessment remains unchanged. The authors have successfully applied a modern deep learning approach to a task that had not been previously addressed in this manner. However, I believe that simply applying deep learning to an unexplored task, without providing novel insights, analysis, or unexpected results, does not constitute a sufficient contribution to ICLR. This perspective may be subject to disagreement from the program chairs.
I have withdrawn my suggestion to consider this paper for a workshop presentation, as the focus of the workshop track this year differs from my initial recommendation.
ORIGINAL REVIEW:
The authors demonstrate the effectiveness of an LSTM+CNN+CTC network in achieving excellent lipreading performance on the GRID corpus. This outcome is noteworthy, as it exemplifies the potential of deep learning methods in yielding impressive results when applied to established tasks for the first time. The engineering effort invested in this work appears substantial and solid. Nevertheless, this achievement alone does not meet the novelty threshold for publication in ICLR. To enhance the paper's suitability for publication, revisions are necessary to better acknowledge prior work and eliminate vague motivational language. Specific areas requiring revision include:
- The claim of being the first to achieve sentence-level lipreading is inaccurate, as prior work has addressed this task, albeit often using non-public data. The paper should be revised to discuss and contextualize this prior work, and ideally, the title should be adjusted accordingly.
- The comparison between the performance of human lipreaders and the proposed system needs to be qualified. Given the unnatural grammar of the task for humans, the results may indicate that the machine learning model can better leverage the strong constraints inherent in the task, rather than making a general statement about the superiority of LipNet over human lipreaders.
- The paper contains unnecessary motivational statements. Invoking Easton and Basala (1982) to justify modeling context in linguistic sequence prediction tasks is not required, as prior work using older sequence models (e.g., HMMs) has also modeled context. Furthermore, the McGurk effect does not demonstrate the crucial role of lipreading in human communication.
- It is noteworthy that the Baseline-2D model, even without spatial convolution, achieves exceptionally good performance. This raises questions about the emphasized importance of spatiotemporal feature extraction in the conclusion.
Additional minor comments and suggestions:
- Citations for LSTMs, CTC, and other relevant concepts should be provided upon their first mention.
- The justification for upsampling is not entirely clear.
- The term "lip-rounding vowels" is unclear, as it seems to encompass most English vowels.
- Consideration should be given to maintaining the vowel visemes V1-V4 as separate entities rather than collapsing them into a single category. Since the full viseme set from Neti et al. is listed, it would be beneficial to explain the rationale behind modifying it.
- The comment regarding the confusion between /aa/ and /ay/ being specific to British speakers is puzzling, as this relationship exists in other English dialects as well.
- The discussion of confusions within bilabial stops and alveolar stops does not align with the actual confusion data presented in Fig. 3(b,c). For instance, there appears to be no confusion between /m/ and /b/ or between /m/ and /p/.
- The term "lipreading actuations" is unclear, and "actuations" seems out of context.
- Minor typographical corrections are needed, such as "palato-alvealoar" to "palato-alveolar" and "Articulatorily alveolar" to "Alveolar".