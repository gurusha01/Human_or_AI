Upon re-examining the paper, I remain unclear about the authors' primary objective. 
The CCA objective is nondifferentiable due to the sum of singular values (trace norm) of T, which suggests that the authors aim to address this issue, as indicated by the title and section 3. However, several key points require clarification:
- It is essential to explicitly state whether the authors have reformulated the CCA objective or introduced a new objective, as this distinction is crucial.
- The connection between the retrieval objective and the "CCA layer" is unclear. Multiple integration methods, such as combination or bi-level optimization, can be envisioned, but the paper lacks a detailed explanation, particularly in section 3, where equations would provide valuable insight.
- Although the CCA objective is technically nondifferentiable, it has not significantly hindered training in practice (e.g., large minibatches have been shown to work effectively despite the theoretical need for batch training). The authors must provide a justification for why the original gradient computation poses a problem for their specific goals. Given their response to my previous question, which mentioned the continued use of SVD of T, it is uncertain whether the proposed method offers a computational efficiency advantage.
Regarding the paper's structure, introducing the retrieval objective earlier, prior to the experiments section, would improve clarity. Additionally, I reiterate my suggestion from previous comments to include a comparison with contrastive loss, which would strengthen the paper.