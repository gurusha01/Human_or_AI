This manuscript presents an enhanced variant of matching networks, exhibiting improved scalability in relation to the support set of a few-shot classifier. By learning an embedding function that consolidates items within each class in the support set (eq. 1), rather than treating each support point in isolation, the authors introduce a notable methodological advancement. This approach is coupled with episodic few-shot training, where random partitions of the training set classes are sampled, thereby closely aligning training and testing conditions.
While the underlying concept is relatively simple and builds upon an extensive body of prior research in zero-shot and few-shot learning, the proposed methodology appears to be novel and yields state-of-the-art performance on multiple benchmark datasets. To further strengthen the manuscript, I suggest incorporating a more detailed explanation of the training algorithm, potentially in the form of pseudocode. Currently, the description of the training process lacks clarity, which could be improved with additional specification.