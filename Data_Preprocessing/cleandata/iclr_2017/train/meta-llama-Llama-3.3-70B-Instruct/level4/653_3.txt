This manuscript investigates the impact of mini-batch size on the convergence of Stochastic Gradient Descent (SGD) in a general non-convex setting, with subsequent implications for the influence of the number of learners on Asynchronous Stochastic Gradient Descent (ASGD). The problem tackled is pertinent, and the theoretical exposition is clear and well-structured. However, the experimental assessment appears somewhat constrained. To strengthen the paper, it would be beneficial to conduct experiments on a broader range of datasets and neural architectures, as well as provide a more comprehensive evaluation. For instance, the current experiment with N=16 is relatively small-scale. Additionally, comparing the behavior of the proposed method with other widely used techniques, such as momentum SGD or Elastic Averaging Stochastic Gradient Descent (EASGD) in a distributed setting, would enhance the manuscript. While this additional evaluation may not be directly within the paper's scope, incorporating these experiments would require minimal extra effort and would significantly contribute to the paper's completeness.