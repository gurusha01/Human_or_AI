This manuscript presents a relational network (RN) framework for capturing relationships between input entities, such as objects, through a two-stage architecture. Initially, a lower-level module processes all possible pairs of input entities, and the outputs are then aggregated using a simple summation across all pairs to form the input to a higher-level module. In its basic form, both modules are implemented as multi-layer perceptrons (MLPs).
The approach is intriguing and offers a fresh perspective on understanding entity relationships. The central concept is well-articulated and motivated, leveraging pooling techniques to induce invariance for relation learning. By extending traditional pooling structures (e.g., spatial or temporal average/max pooling), the method focuses on pairwise relationships, with potential for extension to higher-order interactions, albeit with considerations for scalability.
Empirical evaluations on scene description and image tasks demonstrate the effectiveness of relation networks, outperforming MLP baselines that struggle to model structured dependencies inherent in these tasks. It would be insightful to explore whether incorporating pooling operators (e.g., across-object max pooling within an MLP) or employing data augmentation techniques via permutation could enhance the performance of MLPs on these tasks. Nonetheless, the proposed model exhibits novelty and efficacy in handling relational information, showing promise for tackling higher-level reasoning tasks.