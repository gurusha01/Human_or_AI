This paper presents a novel approach to learning word decomposition, or word-to-subword sequence mapping, in conjunction with an attention-based sequence-to-sequence model. Notably, the proposed decomposition is dynamic, conditioning on the acoustic input, and probabilistic, allowing one word to map to multiple sub-word sequences. The authors contend that this dynamic approach can more accurately capture acoustic patterns, drawing an analogy to learning pronunciation mixture models for HMM-based speech recognition, where probabilistic word-to-pronunciation mappings also depend on acoustic input, as seen in previous works such as those by McGraw et al., Lu et al., and Singh et al. 
Placing this work within the context of prior research in the HMM framework would provide valuable insights. The paper is well-written and theoretically sound. However, the experimental study could be strengthened, for instance, by including a word-level baseline, given that the proposed approach falls between character-level and word-level systems. The WSJ si284 dataset's vocabulary size of 20K, although not excessively large for the softmax layer, especially in a closed vocabulary task, might still yield competitive results with a word-level system. 
It would be beneficial to clarify the computational bottleneck of the proposed method, considering that downsampling the data by a factor of 4 using an RNN still resulted in a convergence time of approximately 5 days, which seems costly, particularly given the use of a single sample for gradient computation. Table 2 may be somewhat misleading, as the inclusion of a language model with CTC and seq2seq models, as in Bahdanau et al., achieves results close to the best numbers reported, while the improvement from adding a language model in this work may be minimal. Lastly, the phrase "O(5) days to converge" is somewhat unusual.