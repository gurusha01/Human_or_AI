Updated review: The authors have made a commendable effort in addressing and incorporating the feedback provided by reviewers. Notably, they have conducted additional experiments and introduced a new, more robust baseline, namely the ConvNet -> LSTM baseline, which was suggested by multiple reviewers. However, I still harbor two concerns that I previously mentioned: the necessity of tuning the architecture of each model independently, including the number of hidden units, and the importance of exploring a pure time series forecasting baseline without trend preprocessing. In light of the authors' revisions, I am inclined to revise my evaluation from a clear rejection to a borderline acceptance.
This manuscript focuses on time series prediction problems where the targets include the slope and duration of upcoming local trends, a setting of significant interest in various real-world applications, such as financial markets, where decisions are often driven by local changes and trends. The primary challenge lies in distinguishing between genuine changes and trends, and noise. The authors propose a hybrid architecture, TreNet, comprising four components: (1) trend extraction through preprocessing, (2) an LSTM that captures long-term dependencies from the extracted trends, (3) a ConvNet that processes a local window of raw data at each time step, and (4) a feature fusion layer that combines the outputs of the LSTM and ConvNet. The TreNet outperforms competing baselines, including those based on its constituent parts, on three univariate time series datasets.
Strengths:
- The problem setting is intriguing and can be argued to differ from other sequential modeling problems in deep learning, representing a thoughtful, task-driven approach to machine learning.
- Assuming the authors' premises are valid, the proposed architecture appears intuitive and well-designed.
Weaknesses:
- Although the problem setting is interesting, the authors fail to provide a compelling argument for their formulation of the machine learning task. The trend targets are extracted from raw data using a deterministic algorithm, rather than being provided directly. This raises the question of whether the problem could be more straightforwardly formulated as a plain time series forecasting task, where the next steps are predicted and then converted into trends using the extractor.
- The proposed architecture, while interesting, lacks justification, particularly in the choice to feed extracted trends and raw data into separate LSTM and ConvNet layers that are combined only at the end by a shallow MLP. A more intuitive approach might involve feeding the ConvNet's output into the LSTM, potentially augmented by trend inputs. Without a solid rationale, this choice seems arbitrary.
- Natural baselines for experiments include the raw->ConvNet->LSTM and {raw->ConvNet, trends}->LSTM architectures.
- The paper assumes, rather than demonstrates, the value of extracted trends and durations as inputs. It is plausible that a sufficiently powerful ConvNet->LSTM architecture could learn to detect these trends in raw data, given enough training data.
- Obvious omitted baselines include raw->LSTM and {raw->ConvNet, trends}->MLP. The authors propose a complex architecture without adequately demonstrating the value of each component. The baselines presented are unnecessarily weak.
A general point of uncertainty is the validity of using the same LSTM and ConvNet architectures in both baselines and TreNet. While this appears to be an apples-to-apples comparison, it could potentially disadvantage either approach due to hyperparameter tuning. A more thorough approach might involve optimizing each architecture independently.
Regarding related work and baselines, it is reasonable to limit the scope of in-depth analysis and experiments to a set of representative baselines, especially in a conference paper. However, the authors overlook a significant body of work on financial time series modeling using probabilistic models and related techniques, which frames the "separate trends from noise" problem as treating observations as noisy. An example of this is the work by J. Hernandez-Lobato, J. Lloyds, and D. Hernandez-Lobato on Gaussian process conditional copulas with applications to financial time series, presented at NIPS 2013.
While I appreciate the research direction, I currently believe the work is not suitable for inclusion at ICLR. My approach to interactive review is to remain open-minded and willing to adjust my score. However, a substantial revision is unlikely, and I would encourage the authors to use the feedback to prepare for a future conference submission, such as ICML.