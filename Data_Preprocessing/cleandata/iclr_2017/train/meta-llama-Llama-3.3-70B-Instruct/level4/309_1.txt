This manuscript presents a methodology for incorporating unsupervised auxiliary tasks into a deep reinforcement learning (RL) framework, such as A3C. The authors introduce several auxiliary control and reward tasks, which they evaluate in both Labyrinth and Atari environments. The proposed UNREAL agent demonstrates substantial performance improvements over A3C, achieving not only better outcomes but also faster learning. This constitutes a valuable contribution to the conference, although the outcome is somewhat anticipated, as the addition of relevant auxiliary tasks typically enhances feature learning and overall performance. Essentially, this paper serves as a proof-of-concept for this intuitive idea.
The manuscript is well-structured and accessible to readers familiar with deep RL, making it easy to comprehend the authors' arguments and findings.
To further contextualize the results, could the authors provide details regarding the computational resources required for training the UNREAL agent?
The architecture of the UNREAL agent is notably complex. Would the authors be willing to make the source code for their model publicly available to facilitate replication and further research?
--------------------------------------------------------
After rebuttal:
The review remains unchanged.