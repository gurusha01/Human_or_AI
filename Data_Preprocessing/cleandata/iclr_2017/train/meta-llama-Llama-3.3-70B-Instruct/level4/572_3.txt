After the rebuttal:
The paper presents a compelling collection of results, largely generated after the initial submission, although the novelty is somewhat restricted and the presentation falls short of expectations.
At this point, my primary concern is the discrepancy between the title and the content. The authors explicitly target deterministic encoder-decoder models, as outlined in section 3.2, which differ significantly from generative models, despite many generative models utilizing this architecture. A minor experiment involving sampling is intriguing but does not alter the paper's overall focus. This inconsistency is unacceptable. A potential resolution could involve simply replacing "generative models" with "encoder-decoder networks" in the title, which would incline me towards recommending acceptance.
------
Initial review:
This paper explores three approaches for generating adversarial examples for deep encoder-decoder generative networks, trained as VAE or VAE-GAN, and provides a comparative analysis of these methods. While the concept of adversarial examples in discriminative models is well-known and extensively studied, I am unaware of prior work on adversarial examples for generative networks, making this research novel. However, there is a concurrent study by Tabacof et al. that should be cited. The paper has undergone significant improvements since its initial submission, yet I have several comments regarding presentation and experimental evaluation, leaving me undecided and potentially open to revising my rating during the discussion phase.
Detailed comments:
1) The paper exceeds the recommended 8-page limit, spanning 13 pages. Given the workload of reviewers, it is essential for authors to ensure their paper is concise and readable. Large sections should be condensed or relocated to the appendix. The excuse "in our attempts to be thorough, we have had a hard time keeping the length down" is unconvincing, as authors must prioritize brevity.
2) I deliberately avoided using the term "generative model" because it is unclear whether the described attacks truly target generative models. The authors train encoder-decoders as generative models (VAE or VAE-GAN) but then remove stochasticity (sampling) and the prior on latent variables, effectively treating the models as deterministic encoder-decoders. It is unsurprising that a deterministic deep network can be easily deceived; a more interesting investigation would explore whether the probabilistic aspects of generative models enhance their robustness to such attacks. I would appreciate clarification from the authors on this point and adjustments to the paper's claims if necessary.
3) The paper's motivation lies in potential attacks on a data channel utilizing a generative network for compression. However, the attack scenario described in section 3.1 is unconvincing, occupying substantial space without adding significant value. Experiments on natural images are necessary to assess the proposed attack's feasibility in realistic scenarios. Moreover, I am unaware of practical applications of VAEs to image compression, making an attack on JPEG a more practical target.
4) The experiments are limited to MNIST and, in the latest version, SVHN. While generative models for general natural images are lacking, evaluating generative models on face datasets is common, making this an additional natural domain for testing the proposed approach.
Smaller remarks:
1) The usage of "Oracle" in section 3.2 seems inappropriate, as an oracle typically has access to part of the ground truth, which is not the case here.
2) The beginning of section 4 states, "All three methods work for any generative architecture that relies on a learned latent representation z"; a more accurate phrase would be "are technically applicable to."
3) Section 4.1 contains a typo: "confidentally" should be corrected.