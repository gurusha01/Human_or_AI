The authors have neglected to address or rectify the concerns raised in the pre-review comments, prompting me to reiterate them here:
I strongly advise against making unsubstantiated claims, such as: 
"The working procedure of this model is analogous to how human beings read a text and then answer a related question."
In reality, human cognition does not employ an LSTM-like model for reading comprehension. I request that you provide a citation from a reputable neuroscience paper to support this assertion, which I believe is unlikely. Therefore, I recommend omitting such statements from future drafts to maintain scientific integrity.
Upon examining your experiments, I notice that they focus on straightforward classification tasks, with comparisons to basic models like NB-SVM. In light of this, I suggest revising the title, abstract, and introduction to better reflect the scope and complexity of your work, avoiding exaggerated claims like "Learning to Understand" in the title.
Furthermore, your attention-level approach bears resemblance to the dynamic memory networks proposed by Kumar et al., who also conducted experiments on sentiment analysis. It would be enlightening to explore the distinctions between your model and theirs, as well as compare their performance.
Additional reviewers have pointed out further omissions in related work and the need to contextualize this paper within the current literary landscape.
Given the lack of effort to address the pre-review questions and feedback, I have reservations about the paper's readiness for publication within the allotted timeframe.