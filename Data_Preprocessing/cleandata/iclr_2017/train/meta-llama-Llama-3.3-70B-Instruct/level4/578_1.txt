This manuscript presents an empirical examination of the invariance, equivariance, and equivalence properties of representations learned by convolutional networks under various data augmentation techniques. The authors also propose additional loss terms that can enhance the invariance or equivariance of a representation.
While the concept of assessing invariance, equivariance, and equivalence of representations is not novel, as evidenced by prior work (Lenc & Vedaldi), the authors undertake a systematic investigation into the impact of data augmentation on these properties. However, the significance, novelty, or usefulness of the findings are not clearly established. The observation that data augmentation increases invariance or that training with consistent augmentation yields more similar representations than training with disparate augmentations is somewhat expected.
Regarding the method proposed to enhance invariance and equivariance, although it is plausible that a representation with increased invariance or equivariance may generalize better, it is unclear why one would prioritize enhancing these properties without demonstrable performance improvements. The paper lacks evidence that training for increased invariance or equivariance substantially enhances performance. Furthermore, the introduced loss (eq. 6) would significantly increase computational complexity, which diminishes the potential utility of this technique.
Several minor issues were noted:
- The notation $R^{nxn}$ should be corrected to $R^{n \times n}$.
- A typo in eq. 2, "equivaraince," should be corrected.
- In section 3.3, the formatting of argmax is improper.
- The attribution of data augmentation's importance to Krizhevsky et al. may not be accurate, as its significance was recognized prior to their work.
- The claim relating CNNs' behavior to the idea of whether they collapse (invariance) or linearize (equivariance) view manifolds of 3D objects contains an inaccuracy. Equivariance does not imply the linearization of manifolds; a linear representation can generate nonlinear manifolds, as exemplified by a 2D rotation matrix creating a circular manifold.
- The term "equivariance" in eq. 2 should be replaced with "non-equivariance," as a low value indicates equivariance, while a high value suggests non-equivariance.
- The use of the term "paradigm" in the context of eq. 2 is unconventional.
- In the definition of $x'ij$, it appears that one of the $gj$ should be inverted to correctly undo the transformation rather than applying it twice.