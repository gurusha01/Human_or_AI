Review- Summary:
This paper presents a theoretical analysis of ResNets using a spin glass model, revealing that they initially behave like an ensemble of shallow networks, but as training progresses, the scaling parameter C increases, causing them to act like an ensemble of increasingly deeper networks, as evidenced by the evolution of weight magnitudes for specific path lengths.
Clarity:
The paper's dense notation and potential notation overloading make it challenging to follow, and including summaries of key proofs in the main text could enhance readability.
Specific Comments:
- The derivation of the sequence beta in the proof of Lemma 2 is unclear, as its connection to equation 11 is not explicitly stated.
- The modified ResNet structure, which involves skipping multiple layers, differs from the standard architecture; it is uncertain whether the analysis applies when only one layer is skipped, as the primary impact of skipping layers appears to be on the number of paths of a given length.
- The new experimental results demonstrating the increase in scale in practice are noteworthy, although it is questionable whether Theorems 3 and 4 provide a theoretical justification for this phenomenon, particularly in light of the simplifying assumptions introduced in Section 4.2.