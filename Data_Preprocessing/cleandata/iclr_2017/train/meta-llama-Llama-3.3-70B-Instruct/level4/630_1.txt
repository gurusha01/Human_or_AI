Summary: This manuscript presents a novel approach to document representation using a read-again attention mechanism with a copy mechanism for abstractive summarization tasks. The proposed model processes each sentence in the input document twice, generating a hierarchical representation instead of relying on a bidirectional RNN. During decoding, it leverages the document representation obtained through the read-again mechanism and identifies out-of-vocabulary (OOV) words in the source document. The authors demonstrate improvements on the DUC 2004 dataset and provide an analysis of their model under various configurations.
Contributions:
The primary contribution of this work is the introduction of the read-again attention mechanism, which enables the model to read the same sentence twice, resulting in an enhanced document representation.
Writing Quality:
The manuscript requires significant refinement. Numerous typos and unclear explanations of the model architecture are present, making some sections appear redundant and poorly organized.
Pros:
- The proposed model offers a straightforward extension to existing summarization models, such as the one presented in [2].
- The results outperform the baseline models.
Cons:
- The improvements achieved are modest.
- The justifications provided are insufficient.
- The manuscript needs substantial rewriting, as several sections lack clarity and precision, and the overall structure could be improved. Additionally, the tone is occasionally informal.
- The paper is heavily focused on applications, with limited contributions to the broader field of machine learning.
Question:
- How does the training speed of the proposed model compare to that of a regular LSTM?
Criticisms:
A similar approach to the read-again mechanism has been explored in [1] within the context of algorithmic learning. The application of this mechanism to summarization tasks may not be considered a significant contribution. The justification for the additional gating alpha_i in the read-again stage is weak and unclear. Although the authors adopt the pointer mechanism for unknown or rare words, as suggested in [2], it is unclear whether the gains are due to the read-again mechanism or the pointing mechanism. The paper's focus on applications results in limited contributions to the machine learning community. It may be beneficial to explore the read-again mechanism in other tasks, such as neural machine translation (NMT), to assess its generalizability. The writing quality of the manuscript is subpar and requires significant improvement.
Minor Comments:
Several corrections are recommended:
- On page 4, replace "... better than a single value ..." with "... scalar gating ..."
- On page 4, rephrase "... single value lacks the ability to model the variances among these dimensions" to "... scalar gating couldn't capture the ..."
- On page 6, modify "... where h0^2 and h0^'2 are initial zero vectors ..." to "... h0^2 and h0^'2 are initialized to a zero vector in the beginning of each sequence ..."
Inconsistencies are present, such as references to both "Tab. 1" and "Table 2". Improved naming of models in Table 1 and a more suitable location for the table would enhance the manuscript's clarity.
References:
[1] Zaremba, W., & Sutskever, I. (2015). Reinforcement learning neural Turing machines. arXiv preprint arXiv:1505.00521.
[2] Gulcehre, C., et al. (2016). Pointing the Unknown Words. arXiv preprint arXiv:1603.08148.