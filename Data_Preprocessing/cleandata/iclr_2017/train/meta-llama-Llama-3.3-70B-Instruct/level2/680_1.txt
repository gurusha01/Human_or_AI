The paper proposes a novel architecture for neural machine translation, which learns morphology by using two recurrent networks and a hierarchical decoder that translates at the character level. The main claim of the paper is that this architecture can avoid the large vocabulary issue and is more efficient in training than word-based models. The authors also claim that their model is able to learn morphology and can translate misspelled or nonce words.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and well-defined problem in neural machine translation, which is the out-of-vocabulary issue.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing word-level models and the potential benefits of character-level models.
The supporting arguments for the decision are as follows:
- The paper provides a clear and detailed description of the proposed architecture, including the word encoder and the hierarchical decoder.
- The authors provide experimental results that demonstrate the effectiveness of their model, including a comparison with other character-level models and a word-level model.
- The paper also provides an analysis of the learned morphology, which shows that the model is able to capture meaningful representations of words and their relationships.
Additional feedback to improve the paper includes:
- Providing more details on the training procedure, such as the hyperparameter settings and the optimization algorithm used.
- Including more examples of the translations produced by the model, to give a better sense of its performance and limitations.
- Discussing potential applications of the proposed architecture beyond neural machine translation, such as speech recognition or text summarization.
Some questions that I would like the authors to answer to clarify my understanding of the paper are:
- How does the model handle out-of-vocabulary words that are not seen during training?
- Can the authors provide more details on the computational resources required to train the model, and how it compares to other character-level models?
- How does the model perform on languages with more complex morphology, such as Arabic or Russian?