Summary of the Paper's Claims and Contributions
The paper proposes a framework for detecting action patterns from motion sequences and modeling the sensory-motor relationship of animals using a generative recurrent neural network. The network has a discriminative part for classifying actions and a generative part for predicting motion, with lateral connections allowing higher levels of the network to represent high-level behavioral phenomena. The authors claim that their framework can be trained with partially labeled sequences, learn to represent high-level information such as writer identity and fly gender without supervision, and generate realistic motion trajectories.
Decision and Reasons
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. Novelty and Significance: The paper proposes a novel framework that combines action classification and motion prediction in a single model, which is a significant contribution to the field of behavior analysis.
2. Strong Experimental Results: The authors provide extensive experimental results on two types of data (fly behavior and online handwriting) that demonstrate the effectiveness of their framework in improving action classification performance, generating realistic motion trajectories, and discovering high-level information.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of behavior analysis and the limitations of existing approaches. The authors also provide a detailed description of their framework, including the architecture and training procedure, which is well-supported by theoretical and empirical evidence. The experimental results are thorough and well-presented, with clear comparisons to baseline methods and ablation studies to demonstrate the importance of different components of the framework.
Additional Feedback and Suggestions
To further improve the paper, I suggest the following:
1. Provide more details on the hyperparameter tuning process: The authors mention that they performed a rough parameter sweep on a subset of the training data, but it would be helpful to provide more details on the specific hyperparameters that were tuned and the range of values that were explored.
2. Include more visualizations of the simulated motion trajectories: While the paper provides some visualizations of the simulated motion trajectories, it would be helpful to include more examples to demonstrate the quality and diversity of the generated trajectories.
3. Discuss potential applications and limitations of the framework: The authors mention that their framework can be applied to more complex data with appropriate tuning of hyperparameters and abstraction of visual input, but it would be helpful to discuss potential applications and limitations of the framework in more detail.
Questions for the Authors
1. Can you provide more details on the computational resources required to train the model, and how the training time scales with the size of the dataset?
2. How do you plan to extend the framework to handle more complex data, such as human motion capture with 1st person video as sensory input?
3. Can you provide more insights into the learned representations of the model, such as the types of features that are captured by the discriminative and generative parts of the network?