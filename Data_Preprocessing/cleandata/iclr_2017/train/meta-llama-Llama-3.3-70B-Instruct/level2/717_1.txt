The paper proposes a framework to analyze the activity of individual neurons in a convolutional neural network (CNN) by quantifying their inherent selectivity to specific properties, such as color and class labels. The authors introduce two selectivity indexes, one for color and one for class labels, which allow ranking neurons according to their response to these properties. The paper also proposes a method to visualize the activity of individual neurons, called the Neuron Feature (NF), which provides a weighted average of the images that maximally activate a neuron.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and relevant question in the field of CNNs, which is understanding the internal representations of these networks.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and the contributions of the proposed framework.
The paper supports its claims with empirical results, including experiments on a VGG-M network trained on ImageNet, which demonstrate the effectiveness of the proposed selectivity indexes and NF visualization method. The results show that color is strongly entangled at all levels of the CNN representation and that class selective neurons are more prevalent in deeper layers. The paper also provides a thorough discussion of the implications of the results, including the possibility of localist and distributed neural codes.
To improve the paper, I suggest the following:
- Provide more details on the implementation of the NF visualization method and the selectivity indexes, including the choice of hyperparameters and the computational resources required.
- Consider adding more experiments to demonstrate the robustness of the proposed framework to different CNN architectures and datasets.
- Provide more insights into the potential applications of the proposed framework, such as understanding the behavior of CNNs in different tasks or improving their interpretability.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
- How do the proposed selectivity indexes relate to existing methods for understanding CNN representations, such as feature importance or saliency maps?
- Can the NF visualization method be used to understand the behavior of CNNs in other tasks, such as object detection or segmentation?
- How do the authors plan to extend the proposed framework to other properties, such as shape or texture, and what are the potential challenges and limitations of doing so?