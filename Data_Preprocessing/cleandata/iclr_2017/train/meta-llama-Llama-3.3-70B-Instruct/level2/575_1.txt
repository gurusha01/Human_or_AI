This paper proposes a differentiable version of Canonical Correlation Analysis (CCA), which enables the computation of CCA to be cast as a layer within a multi-view neural network. The authors demonstrate the effectiveness of this approach in cross-modality retrieval experiments on two public image-to-text datasets, surpassing both Deep CCA and a multi-view network with freely-learned projections.
I decide to accept this paper, with the main reasons being that the approach is well-motivated, the paper is well-placed in the literature, and the results are scientifically rigorous. The authors provide a clear and concise overview of the background and related work, and their proposed method is a significant improvement over existing approaches.
The paper supports its claims with thorough experiments and analysis, including investigations on the learned representations and the influence of running average statistics. The results show that the proposed differentiable CCA layer is superior to optimizing a network for maximally-correlated projections, or not using CCA at all.
To further improve the paper, I suggest that the authors provide more details on the computational complexity of the proposed method and its scalability to larger datasets. Additionally, it would be interesting to see more comparisons with other state-of-the-art methods in cross-modality retrieval.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the choice of batch size affect the performance of the proposed method? Can the authors provide more insights into the learned representations and how they differ from those obtained with Deep CCA? How does the proposed method handle cases where the input data is noisy or has missing values?
Overall, the paper is well-written, and the proposed method has the potential to be a useful building block for many multi-modality tasks. With some additional details and analysis, the paper could be even stronger.