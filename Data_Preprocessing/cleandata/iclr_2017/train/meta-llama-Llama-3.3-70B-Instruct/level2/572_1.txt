This paper explores the vulnerability of deep generative models, specifically Variational Autoencoders (VAEs) and VAE-GANs, to adversarial examples. The authors propose three classes of attacks on these models: a classifier-based attack, an attack using the VAE loss function, and an attack on the latent space. They demonstrate the effectiveness of these attacks on several datasets, including MNIST, SVHN, and CelebA.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of deep learning, and the approach is well-motivated and well-placed in the literature. Secondly, the paper provides a thorough evaluation of the proposed attacks, including a comparison of the three attack methods and an analysis of the results on different datasets.
The paper provides strong support for its claims, including a clear explanation of the attack methodology and a thorough evaluation of the results. The authors also provide a detailed analysis of the limitations of their approach and suggest potential avenues for future work. The paper is well-written and easy to follow, with clear and concise explanations of the technical details.
One potential area for improvement is the discussion of the implications of the results. While the authors provide some motivation for why an attacker might want to attack generative models, they could further explore the potential consequences of such attacks in real-world applications. Additionally, the authors could provide more detail on the potential defenses against these attacks, and how they might be implemented in practice.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend this work to more complex datasets, such as CIFAR-10 and ImageNet? What are the potential implications of these attacks for real-world applications, such as image compression and generation? How might the authors' approach be used to improve the robustness of generative models to adversarial examples?
Overall, this is a well-written and well-researched paper that makes a significant contribution to the field of deep learning. With some additional discussion of the implications and potential defenses, it has the potential to be a highly impactful paper.