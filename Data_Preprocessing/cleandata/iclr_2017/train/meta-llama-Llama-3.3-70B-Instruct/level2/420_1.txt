This paper proposes a novel neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. The authors claim that this model outperforms existing memory-augmented neural language models on two corpora, and that a simpler model based on the concatenation of recent output representations is on par with more sophisticated models.
I decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to improving neural language models. The authors provide a clear and thorough explanation of their methodology, and the results are convincing, with significant improvements over baseline models.
The paper is well-structured and easy to follow, with a clear introduction, methodology, and results section. The authors provide a thorough review of related work, and their approach is well-placed in the literature. The results are supported by experiments on two different corpora, and the authors provide a detailed analysis of the performance of their models.
One potential limitation of the paper is that the authors do not provide a clear explanation of why their model is able to outperform other attention-based models. They suggest that this may be due to the fact that their model is able to separate the key and value representations, but further analysis is needed to fully understand the reasons behind this improvement.
To improve the paper, I would suggest that the authors provide more detailed analysis of the performance of their models, including a breakdown of the results by part of speech and a comparison with other state-of-the-art models. Additionally, the authors could provide more insight into the limitations of their approach and potential avenues for future work.
Some questions I would like the authors to answer include: How do the results vary depending on the size of the attention window? Can the authors provide more insight into why their model is able to outperform other attention-based models? How do the authors plan to address the limitation of their model being unable to exploit long-range dependencies?
Overall, this is a well-written and well-executed paper that presents a significant contribution to the field of neural language models. With some additional analysis and insight, it has the potential to be a highly impactful paper.