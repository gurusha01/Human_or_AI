The paper "Gated-Attention Reader for Answering Cloze-Style Questions" presents a novel approach to answering cloze-style questions over documents. The authors propose a Gated-Attention (GA) Reader, which integrates a multi-hop architecture with a novel attention mechanism based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader.
Summary of Claims: The paper claims to achieve state-of-the-art results on three benchmark datasets for cloze-style question answering, outperforming previous models by a significant margin. The authors attribute this success to the GA module, which enables the reader to build query-specific representations of tokens in the document for accurate answer selection.
Decision: Accept
Reasons: The paper presents a well-motivated approach, building on existing work in attention mechanisms and multi-hop architectures. The authors provide a clear and detailed explanation of the GA module and its components, as well as an extensive evaluation of the model on multiple benchmark datasets. The results demonstrate a significant improvement over previous models, and the ablation study provides evidence for the effectiveness of the GA module.
Supporting Arguments: The paper provides a thorough analysis of the GA Reader, including an ablation study that demonstrates the importance of the GA module, as well as the effectiveness of multiplicative gating over other operations. The authors also provide visualization of the attention distributions at intermediate layers of the reader, which provides insight into the reading process employed by the model.
Additional Feedback: To further improve the paper, the authors could consider providing more details on the hyperparameter tuning process, as well as the computational resources required to train the model. Additionally, it would be interesting to see an analysis of the GA Reader's performance on other tasks, such as language modeling or text classification.
Questions for the Authors: 
1. Can you provide more details on the hyperparameter tuning process, including the range of values considered for each hyperparameter and the criteria used to select the optimal values?
2. How do you plan to address the potential issue of overfitting, given the large number of parameters in the GA Reader?
3. Have you considered applying the GA Reader to other tasks, such as language modeling or text classification, and if so, what results have you obtained?