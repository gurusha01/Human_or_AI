The paper "Interior Gradients" proposes a novel method for quantifying feature importance in deep neural networks. The authors argue that traditional methods, such as gradients, often fail to capture feature importance due to saturation in the network. To address this issue, they introduce the concept of interior gradients, which involve examining the gradients of counterfactual inputs constructed by scaling down the original input.
The paper claims to contribute a new method for feature attribution that is easy to implement, applicable to a wide range of deep networks, and satisfies desirable axioms such as sensitivity and implementation invariance. The authors demonstrate the effectiveness of their method on various networks, including the Inception network, a ligand-based virtual screening network, and an LSTM-based language model.
Based on the provided guidelines, I will evaluate the paper as follows:
Summary of the paper's claims: The paper proposes a new method for feature attribution called interior gradients, which involves examining the gradients of counterfactual inputs constructed by scaling down the original input. The authors claim that this method is easy to implement, applicable to a wide range of deep networks, and satisfies desirable axioms such as sensitivity and implementation invariance.
Decision: Accept
Reasons for the decision: The paper presents a well-motivated and well-executed approach to feature attribution in deep neural networks. The authors provide a clear and concise explanation of the problem, propose a novel solution, and demonstrate its effectiveness on various networks. The paper also provides a thorough analysis of the method's properties and limitations, including its sensitivity and implementation invariance.
Supporting arguments: The paper's approach is well-supported by theoretical analysis and empirical evaluations. The authors provide a clear and concise explanation of the method's properties and limitations, including its sensitivity and implementation invariance. The empirical evaluations demonstrate the effectiveness of the method on various networks, including the Inception network, a ligand-based virtual screening network, and an LSTM-based language model.
Additional feedback: One potential area for improvement is the paper's discussion of the method's limitations. While the authors provide a thorough analysis of the method's properties and limitations, they could further discuss the potential limitations of the method in practice, such as its computational cost and its applicability to very large networks. Additionally, the authors could provide more detailed comparisons with other feature attribution methods, such as DeepLift and Layer-wise relevance propagation (LRP).
Questions for the authors: Can you provide more detailed comparisons with other feature attribution methods, such as DeepLift and Layer-wise relevance propagation (LRP)? How do you plan to address the potential limitations of the method in practice, such as its computational cost and its applicability to very large networks? Can you provide more detailed analysis of the method's properties and limitations, including its sensitivity and implementation invariance?