This paper presents a critical examination of the widely-held assumption that the error surface of deep neural networks is well-behaved, allowing for efficient optimization using gradient-based methods. The authors argue that this assumption may not hold for finite-sized models and datasets, and provide theoretical and empirical evidence to support their claim.
The paper's main contribution is the construction of counterexamples that demonstrate the existence of bad local minima in the error surface of deep neural networks. These counterexamples are based on specific architectures, datasets, and initialization schemes, and are shown to be robust to various optimization algorithms and hyperparameters.
The authors also provide a theoretical analysis of the conditions under which bad local minima can occur, and show that they are more likely to arise when the data is structured in a way that creates "blind spots" in the model's representation. They also demonstrate that bad initialization can lead to poor optimization outcomes, even for simple datasets like MNIST.
The paper's results have important implications for the design and training of deep neural networks. They suggest that the choice of initialization scheme, architecture, and optimization algorithm can have a significant impact on the quality of the learned model, and that careful consideration of these factors is necessary to avoid poor optimization outcomes.
I decide to accept this paper because it presents a well-motivated and well-executed challenge to a widely-held assumption in the field of deep learning. The paper's theoretical and empirical contributions are significant, and the results have important implications for the design and training of deep neural networks.
To further improve the paper, I would suggest that the authors provide more detailed analysis of the conditions under which bad local minima are likely to occur, and explore the relationship between the structure of the data and the optimization outcomes. Additionally, it would be interesting to see more extensive empirical evaluations of the paper's results, using a wider range of datasets and architectures.
Some questions I would like the authors to answer in their response include:
* Can you provide more insight into the relationship between the structure of the data and the optimization outcomes? How do the "blind spots" in the model's representation arise, and how can they be avoided?
* How do the results of the paper relate to other work on the optimization of deep neural networks? Are there any connections to be made with other areas of research, such as optimization algorithms or regularization techniques?
* Are there any potential applications of the paper's results to other areas of machine learning, such as reinforcement learning or natural language processing?