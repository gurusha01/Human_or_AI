The paper "L-SR1: A New Second Order Method for Training Deep Neural Networks" presents a novel second-order optimization method, L-SR1, which addresses the limitations of existing second-order methods in deep learning. The authors claim that L-SR1 overcomes the challenges of handling saddle points and poor conditioning of the Hessian matrix, making it a practical and effective method for training deep neural networks.
I decide to accept this paper, with the primary reason being that the authors provide a well-motivated approach that is grounded in the literature, and they demonstrate the effectiveness of L-SR1 through extensive experiments on various datasets and network architectures. The paper is well-written, and the authors provide a clear and concise explanation of the L-SR1 algorithm, its advantages, and its limitations.
The supporting arguments for this decision include the fact that the authors provide a thorough analysis of the related work, highlighting the strengths and weaknesses of existing second-order methods. They also demonstrate the superiority of L-SR1 over other first-order and second-order methods, including L-BFGS, Nesterov's Accelerated Gradient Descent, and Adam. Additionally, the authors provide a detailed analysis of the hyperparameters of L-SR1, showing that it is relatively robust and requires minimal tuning.
To further improve the paper, I suggest that the authors provide more insights into the theoretical properties of L-SR1, such as its convergence rate and stability. They could also explore the application of L-SR1 to other domains, such as natural language processing or computer vision, to demonstrate its broader applicability. Furthermore, the authors could provide more details on the computational resources required to implement L-SR1, as well as its scalability to larger networks and datasets.
Some questions I would like the authors to answer include: (1) How does L-SR1 handle non-convex optimization problems with multiple local minima? (2) Can the authors provide more insights into the choice of hyperparameters, such as the trust-region radius and the memory size? (3) How does L-SR1 compare to other second-order methods, such as the Hessian-free approach, in terms of computational efficiency and accuracy? (4) Can the authors provide more details on the implementation of L-SR1, including any optimizations or approximations used to reduce computational costs?