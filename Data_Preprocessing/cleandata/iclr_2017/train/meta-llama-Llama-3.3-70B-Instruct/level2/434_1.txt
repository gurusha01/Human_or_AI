The paper proposes a reparameterization of Long Short-Term Memory (LSTM) networks that incorporates batch normalization, which has been shown to be beneficial in feed-forward neural networks. The authors demonstrate that batch-normalizing the hidden states of recurrent neural networks can greatly improve optimization, leading to faster convergence and better generalization on various tasks, including language modeling and question-answering.
I decide to accept this paper, with the main reasons being the novelty of the approach and the thorough evaluation of the proposed method on multiple tasks. The paper presents a well-motivated and well-placed approach in the literature, and the results show significant improvements over the baseline LSTM model.
The supporting arguments for this decision include the fact that the paper provides a clear and detailed explanation of the proposed method, including the reparameterization of the LSTM network and the initialization of the batch normalization parameters. The authors also provide a thorough evaluation of the method on multiple tasks, including sequential MNIST, character-level language modeling on Penn Treebank, and question-answering on the CNN corpus. The results show that the proposed method consistently outperforms the baseline LSTM model, demonstrating its effectiveness.
Additional feedback to improve the paper includes providing more analysis on the impact of batch normalization on the gradient flow in recurrent neural networks, as well as exploring the application of the proposed method to other types of recurrent neural networks, such as Gated Recurrent Units (GRUs). It would also be helpful to provide more details on the hyperparameter searches performed for each task, as well as the sensitivity of the results to the choice of hyperparameters.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the proposed method handle variable-length sequences, and what are the implications of using separate statistics for each time step? How does the initialization of the batch normalization parameters affect the performance of the model, and what are the optimal values for the initialization parameters? What are the potential limitations of the proposed method, and how can they be addressed in future work?