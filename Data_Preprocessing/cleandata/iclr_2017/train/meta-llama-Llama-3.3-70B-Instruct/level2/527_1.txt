The paper introduces a novel recurrent neural network architecture called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors claim that mLSTM is more expressive and better suited for autoregressive density estimation, particularly for sequences of discrete mutually exclusive elements. The paper demonstrates the effectiveness of mLSTM through a series of character-level language modelling experiments, showing improvements over LSTM and its deep variants, especially for more complex tasks.
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear explanation of the limitations of traditional RNNs and LSTMs, and demonstrate how mLSTM addresses these limitations through its flexible input-dependent transition functions. The experimental results are thorough and well-presented, showing the effectiveness of mLSTM on a range of character-level language modelling tasks.
The paper supports its claims through a combination of theoretical analysis and empirical experiments. The authors provide a detailed explanation of the mLSTM architecture and its components, and demonstrate its effectiveness through experiments on several benchmark datasets. The results show that mLSTM outperforms LSTM and its deep variants, especially for more complex tasks, and achieves state-of-the-art performance on the Hutter prize dataset when combined with dynamic evaluation.
To further improve the paper, I suggest that the authors provide more analysis on the computational complexity of mLSTM and its potential applications to other sequence modelling tasks. Additionally, it would be helpful to include more visualizations and plots to illustrate the performance of mLSTM on different tasks and datasets.
Some questions I would like the authors to answer include: How does the performance of mLSTM compare to other state-of-the-art RNN architectures, such as gated recurrent units (GRUs) and recurrent highway networks (RHNs)? Can mLSTM be applied to other sequence modelling tasks, such as speech recognition and machine translation? How does the choice of hyperparameters, such as the number of hidden units and the learning rate, affect the performance of mLSTM?