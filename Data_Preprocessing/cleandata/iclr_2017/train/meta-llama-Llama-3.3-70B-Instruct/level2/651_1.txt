This paper proposes the Gaussian Error Linear Unit (GELU), a novel neural network activation function that outperforms existing nonlinearities such as ReLU and ELU across various tasks in computer vision, natural language processing, and automatic speech recognition. The GELU is derived from a stochastic regularizer, the SOI map, which probabilistically applies the identity or zero map to a neuron's input. The authors demonstrate that the GELU, as the expected transformation of the SOI map, can match or exceed the performance of models using ReLUs or ELUs, and that the SOI map itself can compete with nonlinearities aided by dropout.
I decide to accept this paper, with the primary reason being the thorough empirical evaluation of the GELU against existing nonlinearities, which shows consistent performance improvements across multiple tasks. The paper is well-motivated, and the authors provide a clear understanding of the relationship between the GELU and the SOI map, as well as its connections to existing stochastic regularizers and nonlinearities.
The supporting arguments for this decision include the comprehensive experiments conducted on various datasets, including MNIST, CIFAR-10/100, and TIMIT, which demonstrate the effectiveness of the GELU. Additionally, the authors provide a probabilistic interpretation of the GELU, which bridges the gap between nonlinearities and stochastic regularizers. The paper also acknowledges the limitations of the proposed approach and suggests potential avenues for future work.
To further improve the paper, I suggest that the authors provide more insights into the theoretical properties of the GELU, such as its curvature and non-monotonicity, and how these properties contribute to its performance. Additionally, it would be interesting to see a more detailed analysis of the SOI map's behavior and its relationship to other stochastic regularizers.
Some questions I would like the authors to address include: (1) How does the GELU's performance change when using different optimizers or learning rate schedules? (2) Can the authors provide more intuition on why the GELU outperforms existing nonlinearities in certain tasks? (3) Are there any potential applications or modifications of the GELU that the authors envision for future work?