This paper proposes a spatiotemporal attentional model, called Recurrent Mixture Density Network (RMDN), that learns to predict human fixations in videos. The model uses a combination of 3D convolutional features and a Long Short-Term Memory (LSTM) network to capture both spatial and temporal information in videos. The authors evaluate their model on two datasets, Hollywood2 and UCF101, and demonstrate state-of-the-art performance in saliency prediction and action recognition.
The main claims of the paper are: (1) the proposed RMDN model outperforms existing methods in saliency prediction, and (2) the predicted saliency maps can be used to improve action recognition accuracy. The authors support these claims with extensive experiments and comparisons to existing methods.
I decide to accept this paper because it presents a novel and well-motivated approach to spatiotemporal attentional modeling, and the experimental results demonstrate significant improvements over existing methods. The paper is well-written, and the authors provide a clear and detailed description of their model and experiments.
The key reasons for my decision are: (1) the paper presents a significant improvement over existing methods in saliency prediction, with a large margin of improvement over the state-of-the-art; and (2) the authors demonstrate the effectiveness of their model in improving action recognition accuracy, which is a practically useful application of their work.
To further improve the paper, I suggest that the authors provide more analysis of the limitations of their model and potential avenues for future work. For example, they could discuss the potential impact of using different types of features or architectures, or explore the application of their model to other tasks beyond action recognition.
Some questions I would like the authors to answer are: (1) How do the authors plan to address the potential issue of overfitting to the training data, given the large number of parameters in their model? (2) Can the authors provide more insight into the interpretability of their model, such as visualizations of the attentional weights or analysis of the learned features? (3) How do the authors plan to extend their model to handle more complex or dynamic scenes, such as those with multiple objects or actions?