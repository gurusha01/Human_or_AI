The paper "Variational Lossy Autoencoder" presents a novel approach to learning representations by combining Variational Autoencoders (VAEs) with neural autoregressive models. The authors propose a method to control what kind of information is included in the learned representation, allowing for lossy compression of data. The paper is well-motivated, and the approach is well-placed in the literature.
The specific question tackled by the paper is how to design a VAE model that can learn a lossy compression of observed data, while still maintaining a good representation of the data. The authors address this question by introducing two complementary improvements to VAEs: using autoregressive models as prior distributions and using autoregressive decoders to model local statistics.
The approach is well-motivated, and the authors provide a clear explanation of the limitations of traditional VAEs and how their approach addresses these limitations. The use of autoregressive models as prior distributions and decoders allows for more flexible and expressive models, which can capture complex dependencies in the data.
The paper supports its claims with extensive experiments on several datasets, including MNIST, OMNIGLOT, and CIFAR10. The results show that the proposed approach, called Variational Lossy Autoencoder (VLAE), achieves state-of-the-art results on density estimation tasks and can learn lossy compressions of data that preserve global statistics.
I decide to accept this paper because it presents a novel and well-motivated approach to learning representations, and the experimental results demonstrate the effectiveness of the approach. The two key reasons for this choice are: (1) the paper presents a clear and well-motivated approach to addressing the limitations of traditional VAEs, and (2) the experimental results demonstrate the effectiveness of the approach on several datasets.
To improve the paper, I would suggest providing more details on the implementation of the autoregressive models and the hyperparameter tuning process. Additionally, it would be interesting to see more visualizations of the learned representations and lossy compressions, to gain a better understanding of what kind of information is being preserved and discarded.
Some questions I would like the authors to answer are: (1) How do the authors choose the receptive field size of the PixelCNN decoder, and what is the effect of different receptive field sizes on the learned representations? (2) Can the authors provide more details on the optimization technique used to train the VLAE model, and how it compares to other optimization techniques used in VAEs? (3) How do the authors plan to extend this approach to other forms of data, such as audio and video, and what are the potential challenges and limitations of doing so?