The paper proposes a novel approach to reduce the computational complexity of deep convolutional neural networks (CNNs) by pruning feature maps and kernels. The authors claim that their approach can achieve significant reductions in computational cost while maintaining the network's performance. The main contributions of the paper are the proposal of a simple and generic strategy for selecting the best pruning mask and the demonstration of the effectiveness of combining feature map and kernel pruning.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and relevant problem in the field of deep learning, which is reducing the computational complexity of CNNs.
2. The approach proposed by the authors is well-motivated and supported by experimental results, which show that the proposed technique can achieve significant reductions in computational cost while maintaining the network's performance.
The supporting arguments for the decision are as follows:
* The paper provides a clear and concise introduction to the problem of reducing computational complexity in CNNs and motivates the need for pruning techniques.
* The authors propose a simple and generic strategy for selecting the best pruning mask, which is based on randomly generating a pool of pruning masks and selecting the one that results in the least performance degradation.
* The paper provides extensive experimental results on several benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN, which demonstrate the effectiveness of the proposed approach.
* The authors also provide a detailed analysis of the results and discuss the implications of their findings, including the potential benefits of combining feature map and kernel pruning.
Additional feedback to improve the paper includes:
* Providing more details on the computational cost of the proposed approach and how it compares to other pruning techniques.
* Discussing the potential limitations of the proposed approach and how it can be improved or extended to other types of neural networks.
* Providing more insights into the relationship between the pruning ratio and the number of random evaluations required to select the best pruning mask.
Questions to be answered by the authors include:
* How does the proposed approach compare to other pruning techniques in terms of computational cost and performance?
* Can the proposed approach be extended to other types of neural networks, such as recurrent neural networks or graph neural networks?
* How does the pruning ratio affect the number of random evaluations required to select the best pruning mask, and are there any strategies to reduce the number of evaluations while maintaining the performance of the pruned network?