This paper explores the concept of sample importance in training deep neural networks using stochastic gradient descent. The authors define sample importance as the change in parameters induced by a sample and analyze its impact on different layers and training stages. The main claims of the paper are that easy samples shape parameters closer to the output, while hard samples impact parameters closer to the input, and that mixing hard and easy samples in each batch is beneficial for training.
I decide to accept this paper with some minor revisions. The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear definition of sample importance and demonstrate its usefulness in understanding the training process of deep neural networks. The empirical analysis on two standard datasets provides interesting insights into the behavior of easy and hard samples during training.
The supporting arguments for my decision are as follows. Firstly, the paper addresses a specific question/problem, which is the importance of samples in training deep neural networks. The approach is well-motivated, and the authors provide a clear definition of sample importance. The empirical analysis is thorough, and the results are interesting and insightful. The paper also provides a good discussion of the implications of the results and potential future extensions.
However, I have some minor suggestions for improvement. Firstly, the paper could benefit from a clearer summary of the main contributions and implications of the results. Additionally, some of the figures and tables could be improved for better clarity and readability. Finally, the authors could provide more discussion on the potential applications of the sample importance concept in real-world scenarios.
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to extend the sample importance analysis to different deep learning structures, such as Convolutional Neural Networks and Recurrent Neural Networks? (2) Can the authors provide more insights into the computational cost of calculating the sample importance, and how it can be efficiently computed for large datasets? (3) How do the authors think the sample importance concept can be used in practice to improve the training of deep neural networks, and what are the potential benefits and challenges of using this concept in real-world applications?