Summary of the Paper
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) for regularizing deep neural networks and achieving better optimization performance. The DSD training flow consists of three steps: dense, sparse, and re-dense. The authors claim that DSD training can improve the performance of various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, caption generation, and speech recognition.
Decision
I decide to accept this paper with minor revisions. The paper presents a well-motivated and well-executed idea, and the experimental results demonstrate the effectiveness of the proposed DSD training strategy.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of optimizing deep neural networks and motivates the need for a new training strategy. The authors propose a simple yet effective approach to regularize neural networks by pruning and re-densifying the connections. The experimental results are thorough and demonstrate the superiority of DSD training over conventional training methods on various tasks and datasets.
Additional Feedback
To further improve the paper, I suggest the authors provide more insights into the theoretical aspects of DSD training and its relationship to existing regularization techniques. Additionally, it would be helpful to include more visualizations of the weight distributions and the pruning process to better understand the effects of DSD training on the network.
Questions for the Authors
1. Can you provide more details on the choice of sparsity ratio and its effect on the performance of DSD training?
2. How does DSD training compare to other regularization techniques, such as dropout and weight decay, in terms of performance and computational cost?
3. Can you provide more examples of the captions generated by NeuralTalk with and without DSD training to demonstrate the qualitative improvements?