The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors empirically study its performance on the CIFAR-10 and CIFAR-100 datasets and demonstrate new state-of-the-art results. They also show its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-motivated question of improving the anytime performance of SGD, which is a crucial aspect of deep learning.
2. The approach is well-placed in the literature, building upon existing restart techniques in gradient-free and gradient-based optimization.
The paper provides strong empirical evidence to support its claims, including experiments on multiple datasets and comparisons with existing state-of-the-art methods. The authors also provide a clear and concise explanation of their approach, making it easy to understand and implement. Additionally, the paper discusses the limitations of the approach and provides potential directions for future work.
To further improve the paper, I suggest the authors consider the following:
- Provide more theoretical analysis of the proposed warm restart technique, including its convergence properties and relationships to existing optimization methods.
- Investigate the application of the warm restart technique to other optimization algorithms, such as AdaDelta and Adam.
- Consider alternative network structures, such as those proposed by Zhang et al. (2016) and Huang et al. (2016b), to further improve the results.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
- How do the authors choose the hyperparameters, such as the initial learning rate and the restart period, and how sensitive are the results to these choices?
- Can the authors provide more insight into the effect of the warm restart technique on the optimization trajectory, including the behavior of the learning rate and the model's parameters?
- How do the authors plan to extend the warm restart technique to other domains, such as natural language processing and reinforcement learning?