This paper presents multiple approaches for improving the performance of gradient descent when utilizing multiple compute resources. The authors propose a new approach, asynchronous layer-wise gradient descent, which maximizes overlap of layer-wise backpropagation with gradient synchronization. They also implement and evaluate other baseline techniques using the Machine Learning Toolkit for Extreme Scale (MaTEx) on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system.
I decide to accept this paper with minor revisions. The main reason for this decision is that the paper presents a well-motivated approach to improving the performance of gradient descent on multiple compute resources, and the experimental evaluation demonstrates a significant speedup of up to 1.7x compared to synchronous gradient descent.
The paper is well-structured, and the authors provide a clear explanation of the background, related work, and their proposed approach. The experimental evaluation is thorough, and the results are presented in a clear and concise manner. The authors also discuss the limitations of their approach and provide suggestions for future work.
To improve the paper, I suggest that the authors provide more details on the implementation of their approach, particularly with regards to the use of MPI and the parallelization of the gradient averaging. Additionally, it would be helpful to include more analysis on the tradeoff between the speedup and the accuracy of the model.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on how the delayed gradient approach is implemented, and how the number of iterations to delay the gradient updates is determined?
* How do you handle the case where the mini-batch size is smaller than the number of compute devices, and how does this affect the performance of the approach?
* Can you provide more analysis on the convergence of the approach, particularly with regards to the effect of the delayed gradient updates on the accuracy of the model?
Overall, this is a well-written paper that presents a significant contribution to the field of deep learning. With some minor revisions to address the above questions and provide more details on the implementation, I believe this paper is ready for publication.