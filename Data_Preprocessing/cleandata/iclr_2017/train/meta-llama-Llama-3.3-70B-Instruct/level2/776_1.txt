Summary of the Paper's Claims and Contributions
The paper presents a novel approach to machine translation decoding, which allows for iterative improvements to an initial guess translation. The authors propose two models, a single attention model and a dual attention model, that predict word substitutions to improve the translation. The models are trained on bilingual text and can be applied to any initial guess translation. The paper claims that this approach can improve translation accuracy by up to 0.4 BLEU on WMT15 German-English translation, with an average of only 0.6 substitutions per sentence.
Decision and Reasons
I decide to Accept this paper, with the main reasons being:
1. The paper presents a well-motivated and novel approach to machine translation decoding, which addresses a significant limitation of existing models.
2. The authors provide a thorough evaluation of their approach, including experiments on a large dataset and comparisons to baseline models.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed approach, including the architecture of the single and dual attention models. The authors also provide a detailed analysis of the results, including examples of system outputs and plots of accuracy versus the number of allowed substitutions. The use of oracles to evaluate the performance of the models is also a nice touch, as it provides a upper bound on the potential improvement of the approach.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors consider the following:
* Providing more details on the training procedure, including the optimization algorithm and hyperparameter settings.
* Evaluating the approach on other language pairs and datasets to demonstrate its generalizability.
* Comparing the approach to other iterative decoding methods, such as those used in automatic post-editing.
* Considering the use of other evaluation metrics, such as METEOR or TER, to provide a more comprehensive picture of the approach's performance.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the implementation of the dual attention model, including the architecture and training procedure?
* How do you select the initial guess translation, and what is the impact of this choice on the performance of the approach?
* Have you considered using other types of substitutions, such as deletions or insertions, in addition to word substitutions?