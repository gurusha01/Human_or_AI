The paper "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from Multiple Source Tasks" proposes a novel deep neural network architecture, A2T, for transfer learning that avoids negative transfer while enabling selective transfer from multiple source tasks in the same domain. The authors claim that their architecture can effectively transfer knowledge from prior source tasks to solve a new target task, addressing two crucial issues in transfer learning: negative transfer and selective transfer.
I decide to accept this paper, with the key reason being that the authors provide a well-motivated approach to addressing the challenges of transfer learning, and their empirical evaluations demonstrate the effectiveness of their architecture. The paper is well-structured, and the authors provide a clear explanation of their methodology and results.
The supporting arguments for my decision include:
1. The authors provide a thorough review of related work in transfer learning, highlighting the limitations of existing approaches and motivating the need for their proposed architecture.
2. The A2T architecture is well-designed, and the authors provide a clear explanation of how it works, including the attention mechanism and the base network.
3. The empirical evaluations demonstrate the effectiveness of A2T in avoiding negative transfer and performing selective transfer from multiple source tasks, using various algorithms and simulated worlds.
To improve the paper, I suggest that the authors provide more details on the training process, including the hyperparameters used and the computational resources required. Additionally, it would be helpful to include more visualizations of the attention mechanism and the base network to better understand how they work together.
Some questions I would like the authors to answer include:
1. How do the authors plan to extend their architecture to transfer learning across different domains, where the state and action spaces may differ?
2. Can the authors provide more insights into the attention mechanism, including how it learns to assign weights to the different source tasks and how it adapts to new tasks?
3. How do the authors plan to apply their architecture to real-world problems, such as robotics or computer vision, where transfer learning is crucial?