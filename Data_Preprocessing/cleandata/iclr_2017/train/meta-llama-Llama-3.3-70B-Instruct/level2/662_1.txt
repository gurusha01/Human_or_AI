The paper proposes a dynamic neural Turing machine (D-NTM) that extends the traditional neural Turing machine (NTM) by introducing a learnable and location-based addressing scheme. This allows the D-NTM to learn sophisticated location-based addressing strategies, including both linear and nonlinear ones. The authors evaluate the D-NTM on various tasks, including episodic question-answering, sequential MNIST, and algorithmic tasks, and demonstrate its effectiveness in outperforming traditional NTM and LSTM baselines.
I decide to accept this paper for the following reasons:
1. The paper tackles a specific question/problem of improving the addressing scheme in neural Turing machines, which is a well-motivated and well-placed problem in the literature.
2. The approach is well-motivated, and the authors provide a clear explanation of the limitations of traditional NTM and how the proposed D-NTM addresses these limitations.
3. The paper provides extensive experimental results, including comparisons with other models and ablation studies, which demonstrate the effectiveness of the proposed D-NTM.
Some supporting arguments for the decision include:
* The authors provide a clear and concise explanation of the proposed D-NTM and its components, including the learnable addressing scheme and the discrete attention mechanism.
* The experimental results demonstrate the effectiveness of the D-NTM in various tasks, including episodic question-answering and sequential MNIST.
* The authors provide a thorough analysis of the results, including comparisons with other models and ablation studies, which helps to understand the strengths and limitations of the proposed D-NTM.
Some additional feedback to improve the paper includes:
* Providing more details on the implementation of the D-NTM, including the hyperparameter settings and the training procedure.
* Including more visualizations or examples to illustrate the learned addressing scheme and the discrete attention mechanism.
* Discussing potential applications of the D-NTM beyond the tasks evaluated in the paper, such as text summarization or visual question-answering.
Some questions I would like the authors to answer include:
* How does the learnable addressing scheme in the D-NTM compare to other addressing schemes, such as content-based addressing or location-based addressing?
* Can the D-NTM be applied to other tasks that require external memory, such as reinforcement learning or natural language processing?
* How does the discrete attention mechanism in the D-NTM affect the interpretability of the model, and can it be used to provide insights into the decision-making process of the model?