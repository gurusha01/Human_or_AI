The paper proposes a novel extension to the Generative Adversarial Network (GAN) framework, called Generative Multi-Adversarial Network (GMAN), which introduces multiple discriminators to improve the training process. The authors claim that GMAN can be reliably trained with the original, untampered minimax objective, and demonstrate its effectiveness on various image generation tasks.
I decide to accept this paper, with the main reason being that the authors provide a well-motivated and well-placed extension to the GAN framework, which is supported by theoretical and empirical evidence. The paper is well-written, and the authors provide a clear and concise explanation of their contributions, including the GMAN framework, the generative multi-adversarial metric (GMAM), and the experimental results.
The supporting arguments for this decision include:
* The authors provide a thorough review of the GAN framework and its limitations, which motivates the need for an extension like GMAN.
* The GMAN framework is well-designed, and the authors provide a clear explanation of how it works, including the different variants of GMAN, such as GMAN-max, GMAN-λ, and GMAN∗.
* The experimental results demonstrate the effectiveness of GMAN on various image generation tasks, including MNIST, CIFAR-10, and CelebA, and show that GMAN outperforms standard GANs in terms of convergence rate and image quality.
* The authors provide a thorough analysis of the results, including an evaluation of the GMAM metric, which provides a way to compare the performance of different GMAN variants.
Additional feedback to improve the paper includes:
* Providing more details on the implementation of GMAN, including the architecture of the generator and discriminator networks, and the hyperparameters used in the experiments.
* Including more visualizations of the generated images, to provide a better understanding of the quality of the images produced by GMAN.
* Discussing the potential applications of GMAN, beyond image generation, and exploring its potential use in other domains, such as natural language processing or reinforcement learning.
* Providing more analysis on the robustness of GMAN to mode collapse, and exploring ways to further improve its robustness.
Questions to the authors include:
* Can you provide more details on the computational cost of training GMAN, compared to standard GANs?
* How do you plan to extend GMAN to other domains, beyond image generation?
* Can you provide more analysis on the effect of the number of discriminators on the performance of GMAN?
* How do you plan to address the potential issue of overfitting in GMAN, particularly when using multiple discriminators?