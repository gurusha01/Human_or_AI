This paper proposes a novel approach to answering non-factoid questions, which are complex and open-ended questions that require more than a simple fact-based answer. The authors introduce a Neural Answer Construction Model that incorporates semantic biases behind questions into word embeddings and simultaneously learns the optimum combination of answer sentences as well as the closeness between questions and sentences.
The paper claims to tackle two major problems with current methods: (1) they cannot understand the ambiguous use of words in questions, and (2) they can only select from existing answers and cannot generate new ones. The proposed model addresses these issues by using word embeddings with document semantics and a joint neural network to learn sentence selection and combination.
I decide to accept this paper with minor revisions. The reasons for this decision are:
1. The paper tackles a significant problem in the field of question answering, and the proposed approach is novel and well-motivated.
2. The experimental results demonstrate the effectiveness of the proposed model, with a 20% higher accuracy in answer construction compared to the current best answer selection method.
3. The paper provides a clear and detailed explanation of the proposed model, including the architecture and training procedure.
However, I have some minor concerns that need to be addressed:
1. The paper could benefit from a more detailed analysis of the results, including a breakdown of the performance on different types of questions and a discussion of the limitations of the proposed model.
2. The paper assumes that the abstract scenario of the answer is predefined, which may not always be the case in real-world applications. The authors should discuss how to handle cases where the scenario is not predefined.
3. The paper uses a specific dataset and evaluation metric, which may not be representative of all non-factoid question answering tasks. The authors should discuss the potential applicability of the proposed model to other datasets and tasks.
To improve the paper, I suggest the following:
1. Provide more details on the dataset and evaluation metric used, including the size of the dataset and the criteria for selecting the questions and answers.
2. Discuss the potential limitations of the proposed model, including the assumption of a predefined abstract scenario and the reliance on a specific dataset and evaluation metric.
3. Consider adding more visualizations or examples to illustrate the proposed model and its output, which would help to make the paper more accessible to a broader audience.
Some questions I would like the authors to answer:
1. How do the authors plan to handle cases where the abstract scenario of the answer is not predefined?
2. Can the proposed model be applied to other types of question answering tasks, such as factoid question answering or multi-step question answering?
3. How does the proposed model handle out-of-vocabulary words or words with multiple meanings, which are common in non-factoid questions?