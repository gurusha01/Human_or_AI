This paper proposes an automatic dialogue evaluation model (ADEM) that learns to predict human-like scores for input responses. The model is trained on a dataset of human response scores and uses a hierarchical recurrent neural network (RNN) to predict scores. The authors show that ADEM's predictions correlate significantly with human judgements at both the utterance and system-level, and that it can generalize to evaluating new models unseen during training.
I decide to accept this paper for the following reasons:
1. The paper tackles a specific and well-defined problem in the field of dialogue systems, namely the evaluation of dialogue responses.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing word-overlap metrics and the need for a more accurate evaluation model.
3. The paper provides strong empirical evidence to support the claims, including utterance-level and system-level correlations with human judgements, as well as generalization to new models.
Some supporting arguments for this decision include:
* The paper provides a thorough analysis of the limitations of existing word-overlap metrics, such as BLEU, and demonstrates the need for a more accurate evaluation model.
* The authors propose a novel approach to dialogue evaluation, using a hierarchical RNN to predict human-like scores, and provide a clear explanation of the model architecture and training procedure.
* The experimental results are comprehensive and well-presented, including correlations with human judgements, generalization to new models, and a qualitative analysis of the model's strengths and weaknesses.
To improve the paper, I suggest the following additional feedback:
* The authors could provide more details on the dataset collection process, including the criteria used to select the Twitter responses and the instructions given to the human evaluators.
* The paper could benefit from a more detailed analysis of the model's failures, including examples of responses where the model disagrees with human evaluators and an explanation of the possible reasons for these disagreements.
* The authors could also explore the potential applications of ADEM in dialogue system development, such as using the model as a reward function for reinforcement learning or as a metric for evaluating the performance of different dialogue models.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can the authors provide more details on the pre-training procedure used to train the VHRED model, including the dataset used and the hyperparameters selected?
* How do the authors plan to address the issue of human biases in the evaluation model, such as the tendency to give higher scores to shorter responses?
* Can the authors provide more examples of the model's output, including responses where the model agrees and disagrees with human evaluators, to illustrate the model's strengths and weaknesses?