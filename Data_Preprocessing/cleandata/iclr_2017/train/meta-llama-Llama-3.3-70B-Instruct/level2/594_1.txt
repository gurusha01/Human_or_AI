This paper proposes low-dimensional parametrizations for passthrough neural networks based on low-rank or low-rank plus diagonal decompositions of the n Ã— n matrices that occur in the hidden layers. The authors claim that these parametrizations can reduce the data complexity and memory requirements of the network while preserving its memory capacity. They evaluate their approach on several tasks, including sequential randomly-permuted MNIST classification, memory tasks, addition tasks, and character-level language modeling, and report competitive results.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific question of reducing the data complexity and memory requirements of passthrough neural networks, which is a well-motivated problem.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the state passthrough principle and its relation to existing architectures such as LSTMs, GRUs, and Highway Networks.
3. The paper provides a thorough evaluation of the proposed approach on several tasks, including a near state-of-the-art result on sequential randomly-permuted MNIST classification.
The supporting arguments for this decision include:
* The paper provides a clear and concise explanation of the proposed approach, including the low-rank and low-rank plus diagonal parametrizations.
* The experimental evaluation is thorough and well-designed, with a clear comparison to existing state-of-the-art models.
* The results are competitive and demonstrate the effectiveness of the proposed approach in reducing the data complexity and memory requirements of passthrough neural networks.
Additional feedback to improve the paper includes:
* Providing more details on the optimization procedure used for the memory tasks, including the specific hyperparameters and the rationale behind the modifications made to the original optimizer implementation.
* Exploring the application of the proposed approach to other tasks and domains, such as word-level language modeling and neural machine translation.
* Investigating the use of alternative parametrizations, such as non-linear activation functions, and their potential benefits and limitations.
Questions to the authors include:
* Can you provide more details on the numerical instability issues encountered in the memory tasks and how they were addressed?
* How do you plan to extend the proposed approach to other tasks and domains, and what are the potential challenges and limitations?
* Can you provide more insights into the trade-offs between the low-rank and low-rank plus diagonal parametrizations, and how they affect the performance of the model on different tasks?