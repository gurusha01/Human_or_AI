The paper presents a method to prune convolutional neural networks (CNNs) while improving inference speed, which is a significant challenge in the field of deep learning. The authors propose a high-performance sparse convolution design that takes advantage of arbitrary sparsity patterns and outperforms dense convolution even with a moderate sparsity. They also develop a performance model that projects speedup over different sparsity levels and on different computing platforms, providing guidelines for useful sparsity ranges.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and relevant problem in the field of deep learning, which is the reduction of computational cost and memory usage in CNNs while maintaining their accuracy.
2. The approach is well-motivated, and the authors provide a clear and concise explanation of their method, including the performance model and the guided sparsity learning algorithm.
The paper supports its claims with extensive experiments on various platforms, including Intel Atom, Xeon, and Xeon Phi processors, demonstrating significant speedups over dense convolution. The authors also provide a comprehensive review of related work, highlighting the advantages and limitations of their approach compared to existing methods.
To further improve the paper, I suggest the authors provide more details on the implementation of their sparse convolution design and the performance model, including any potential limitations or challenges they encountered during the development process. Additionally, it would be interesting to see more results on the application of their method to other CNN architectures and datasets.
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
- How do the authors plan to extend their performance model to cover other FLOP-reduction methods, such as FFT, Winograd, and tensor factorization?
- Can the authors provide more insights into the trade-offs between model size, inference speed, and accuracy in their approach, and how these trade-offs can be balanced in practice?
- How do the authors envision their method being used in real-world applications, such as mobile devices or data centers, and what are the potential benefits and challenges of deploying their approach in these scenarios?