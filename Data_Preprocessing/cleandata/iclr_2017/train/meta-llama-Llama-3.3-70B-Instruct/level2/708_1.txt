This paper claims to contribute to the field of adversarial attacks on deep neural networks by presenting simple and effective black-box adversarial attacks. The authors propose two approaches: a random pixel perturbation method and a greedy local-search method. The paper demonstrates the effectiveness of these approaches in generating adversarial images that can fool state-of-the-art convolutional neural networks (CNNs) without requiring knowledge of the network architecture or parameters.
I decide to accept this paper with the following reasons:
1. The paper tackles a specific and relevant problem in the field of deep learning, namely the vulnerability of CNNs to adversarial attacks.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the differences between the proposed methods and existing approaches.
3. The paper provides extensive experimental results that support the claims made by the authors, demonstrating the effectiveness of the proposed methods in generating adversarial images.
The supporting arguments for my decision are as follows:
* The paper provides a clear and concise explanation of the problem and the proposed approaches, making it easy to understand the contributions of the paper.
* The experimental results are thorough and well-presented, with a comparison to existing methods (e.g., the fast-gradient sign method) that demonstrates the advantages of the proposed approaches.
* The paper discusses the limitations of the proposed methods and potential counter-measures, which shows that the authors have considered the broader implications of their work.
Additional feedback to improve the paper:
* It would be helpful to provide more details on the computational resources required to run the experiments, as this could be an important consideration for practitioners.
* The paper could benefit from a more detailed discussion of the potential applications and implications of the proposed methods, beyond just the vulnerability of CNNs to adversarial attacks.
* It would be interesting to see an extension of the proposed methods to other types of machine learning models, such as recurrent neural networks or natural language processing models.
Questions to the authors:
* Can you provide more details on the choice of hyperparameters for the greedy local-search method, and how these were tuned?
* How do you think the proposed methods could be adapted to defend against adversarial attacks, rather than just generating them?
* Have you considered the potential for using the proposed methods to improve the robustness of CNNs, for example by using adversarial training or data augmentation?