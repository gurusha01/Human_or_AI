The paper proposes a technique to reduce the parameters of a Recurrent Neural Network (RNN) by pruning weights during the initial training of the network. The authors claim that their approach can achieve a sparsity of 90% with a small loss in accuracy, and that the pruned models can be deployed on mobile devices and back-end server farms due to their small size and increased computational efficiency.
I decide to accept this paper with some minor revisions. The main reason for this decision is that the paper presents a well-motivated approach to reducing the size of RNNs, which is a significant challenge in deploying these models on resource-constrained devices. The authors provide a clear and detailed description of their pruning technique, and the experimental results demonstrate the effectiveness of their approach in reducing the size of the model while maintaining accuracy.
The supporting arguments for this decision include the fact that the paper is well-written and easy to follow, with a clear structure and concise language. The authors provide a thorough review of related work, and their approach is well-placed in the literature. The experimental results are convincing, and the authors provide a detailed analysis of the performance of their pruned models.
However, I do have some minor suggestions for improvement. Firstly, the authors could provide more details on the hyperparameters used in their experiments, and how they were tuned. Secondly, the authors could provide more analysis on the trade-off between model size and accuracy, and how their approach compares to other techniques for reducing the size of RNNs.
Some questions I would like the authors to answer include: How do the authors plan to extend their approach to other types of neural networks, such as convolutional neural networks? How do the authors plan to investigate the use of their pruning technique in combination with other techniques, such as quantization and knowledge distillation? What are the potential applications of the authors' approach in real-world scenarios, such as speech recognition and language modeling?
Overall, I believe that this paper presents a significant contribution to the field of neural networks, and that the authors' approach has the potential to make a significant impact in the deployment of RNNs on resource-constrained devices. With some minor revisions, I believe that this paper is ready for publication.