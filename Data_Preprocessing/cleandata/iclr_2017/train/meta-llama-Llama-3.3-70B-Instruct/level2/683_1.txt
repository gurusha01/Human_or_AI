This paper presents a novel ensemble method called Boosted Residual Networks, which combines the strengths of Residual Networks and Deep Incremental Boosting. The authors claim that their approach outperforms existing methods, including Deep Incremental Boosting without shortcut connections, AdaBoost with Residual Networks, and single Residual Networks. 
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by experimental results on benchmark datasets, including MNIST, CIFAR-10, and CIFAR-100. The authors provide a clear and detailed explanation of their method, including the iterative algorithm used to construct the ensemble, and demonstrate its effectiveness in improving accuracy and reducing training time.
The paper is well-structured, and the authors provide a thorough background on Residual Networks, Deep Incremental Boosting, and ensemble methods. The experimental results are comprehensive, and the authors provide a detailed comparison with existing methods. The use of a distilled version of the Boosted Residual Network and a bagged version of the Residual Network adds to the paper's strength.
To further improve the paper, I suggest that the authors provide more analysis on the computational complexity of their method and compare it with existing ensemble methods. Additionally, it would be interesting to see more experiments on larger datasets and more complex network architectures. 
Some questions I would like the authors to answer include: How do the authors plan to extend their method to larger, state-of-the-art networks? What are the potential applications of Boosted Residual Networks in real-world problems? How does the choice of hyperparameters, such as the number of epochs and the learning rate, affect the performance of the method? 
Overall, the paper presents a significant contribution to the field of ensemble methods and deep learning, and with some additional analysis and experimentation, it has the potential to be a strong publication.