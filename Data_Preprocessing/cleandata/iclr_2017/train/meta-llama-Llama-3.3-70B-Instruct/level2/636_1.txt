This paper proposes a novel weight initialization technique that corrects for the variance introduced by dropout in neural networks. The authors derive a new initialization method that takes into account the dropout rate and the nonlinearity's effect on the neuron output variance. They also propose a simple technique to re-estimate the Batch Normalization variance parameters after training, which improves the accuracy of the network.
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear and thorough derivation of their initialization method, and the experiments demonstrate the effectiveness of their approach on various architectures and datasets.
The paper tackles the specific question of how to adjust for the variance introduced by dropout in neural networks, and the approach is well-placed in the literature. The authors provide a comprehensive review of previous weight initialization techniques and highlight the limitations of existing methods. The paper supports its claims with empirical results, including experiments on MNIST, CIFAR-10, and CIFAR-100, which demonstrate the superiority of the proposed initialization method.
One potential limitation of the paper is that the authors do not provide a detailed analysis of the computational cost of their approach. While they mention that their method is less computationally expensive than Batch Normalization, it would be helpful to provide a more detailed comparison of the computational costs of different methods.
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the sensitivity of their approach to different hyperparameters. Additionally, it would be helpful to include more experiments on larger datasets and more complex architectures to further demonstrate the effectiveness of their approach.
Some questions I would like the authors to answer include: How do the authors plan to extend their approach to more complex architectures, such as recurrent neural networks or transformers? How do the authors think their approach will interact with other regularization techniques, such as weight decay or early stopping? What are the potential limitations of their approach, and how do they plan to address them in future work?