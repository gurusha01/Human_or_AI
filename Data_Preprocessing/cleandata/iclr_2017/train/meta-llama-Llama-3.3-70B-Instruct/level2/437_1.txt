This paper introduces a hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, a state-of-the-art method in reinforcement learning. The authors analyze the computational traits of A3C and propose a system of queues and a dynamic scheduling strategy to leverage the GPU's computational power. The hybrid CPU/GPU implementation, named GA3C, achieves a significant speedup compared to a CPU implementation, with a maximum speedup of 45x for larger DNNs.
I decide to accept this paper, with the main reason being that it presents a well-motivated and well-executed approach to improving the computational efficiency of A3C. The authors provide a thorough analysis of the computational aspects of A3C and demonstrate the effectiveness of their proposed GA3C architecture through extensive experiments.
The paper supports its claims with a comprehensive evaluation of GA3C, including an analysis of the effect of resource utilization on training speed, the impact of training batch size on learning stability and convergence speed, and a comparison with the original A3C algorithm. The authors also provide a detailed discussion of the trade-offs involved in configuring GA3C and propose a dynamic adjustment mechanism to optimize the system's performance.
To further improve the paper, I suggest that the authors provide more insight into the potential applications of GA3C beyond the Atari games domain. Additionally, it would be interesting to see a more detailed comparison with other GPU-accelerated reinforcement learning algorithms.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend GA3C to more complex domains, such as robotics or autonomous driving? What are the potential limitations of GA3C, and how do the authors plan to address them in future work? How does the dynamic adjustment mechanism proposed in the paper affect the overall performance of GA3C, and are there any potential drawbacks to using this approach? 
Overall, the paper presents a significant contribution to the field of reinforcement learning, and I believe that it has the potential to inspire further research in this area. With some minor revisions to address the questions and suggestions above, I am confident that the paper will be a valuable addition to the conference proceedings.