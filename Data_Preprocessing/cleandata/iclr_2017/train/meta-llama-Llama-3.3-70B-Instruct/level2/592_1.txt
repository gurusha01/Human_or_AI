This paper proposes a novel extension of variational autoencoders (VAEs) called epitomic variational autoencoders (eVAEs) to address the problem of model over-pruning in VAEs. The authors claim that eVAEs can overcome this issue by learning multiple shared subspaces that have learned specializations, allowing for increased utilization of the model capacity to model greater data variability.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and grounded in the literature, with a clear explanation of the problem of model over-pruning in VAEs and how eVAEs address this issue. Secondly, the experimental results demonstrate the effectiveness of eVAEs in overcoming over-pruning and improving generative performance, with qualitative and quantitative comparisons to VAEs and other state-of-the-art models.
The paper provides a thorough analysis of the problem of model over-pruning in VAEs and presents a well-structured solution. The authors provide a clear explanation of the eVAE model, including its architecture, training procedure, and theoretical justification. The experimental results are comprehensive and demonstrate the advantages of eVAEs over VAEs and other models.
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process for eVAEs, as well as a more detailed analysis of the computational complexity of the model. Additionally, it would be helpful to include more visualizations of the learned subspaces and their specializations.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors choose the epitome size K, and what is the effect of different values of K on the performance of eVAEs? How do the authors ensure that the learned subspaces are diverse and do not overlap excessively? What are the potential applications of eVAEs beyond image generation, and how can they be extended to other domains?