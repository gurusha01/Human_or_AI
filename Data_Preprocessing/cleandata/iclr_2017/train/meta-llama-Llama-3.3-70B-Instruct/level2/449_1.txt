The paper "FractalNet: Ultra-Deep Neural Networks without Residuals" introduces a novel design strategy for neural network macro-architecture based on self-similarity, where repeated application of a simple expansion rule generates deep networks with fractal structural layouts. The authors claim that their FractalNet architecture can match the performance of standard residual networks on both CIFAR and ImageNet classification tasks, demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.
I decide to accept this paper, with the key reason being that the approach is well-motivated and the paper supports its claims with thorough experiments and analysis. The authors provide a clear and concise explanation of their design strategy, and the experimental results demonstrate the effectiveness of FractalNet in achieving competitive performance with residual networks.
The paper provides strong evidence that path length is fundamental for training ultra-deep neural networks, and that residuals are incidental. The authors also introduce a novel regularization protocol, drop-path, which is shown to be effective in preventing co-adaptation of parallel paths in fractal networks. The paper's analysis of the internal behavior of fractal networks and its connection to phenomena engineered into other networks, such as deep supervision and student-teacher learning, is also well-done.
To further improve the paper, I suggest that the authors provide more details on the implementation of drop-path and its hyperparameter tuning. Additionally, it would be interesting to see more experiments on the anytime property of fractal networks, and how it can be used in practice. I would also like to see more discussion on the potential applications of FractalNet, and how it can be used in real-world scenarios.
Some questions I would like the authors to answer include: How do the authors plan to extend FractalNet to other types of neural networks, such as recurrent neural networks or graph neural networks? How do the authors think FractalNet can be used in conjunction with other techniques, such as attention or transfer learning, to further improve its performance? What are the potential limitations of FractalNet, and how can they be addressed in future work?