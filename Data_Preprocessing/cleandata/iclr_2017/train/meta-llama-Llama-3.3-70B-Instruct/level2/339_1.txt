The paper proposes a novel extension to neural network language models, called the Neural Cache Model, which adapts the prediction to the recent history by storing past hidden activations as memory and accessing them through a dot product with the current hidden activation. The authors claim that this approach outperforms recent memory-augmented networks on several language model datasets.
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by experimental results. The authors provide a clear explanation of the Neural Cache Model and its relationship to existing memory-augmented neural networks and cache models. The experimental results demonstrate the effectiveness of the proposed approach on various language modeling datasets, including the challenging LAMBADA dataset.
The supporting arguments for this decision include the fact that the paper provides a thorough analysis of the related work, including cache models and memory-augmented neural networks. The authors also provide a detailed explanation of the Neural Cache Model, including its architecture and training procedure. The experimental results are comprehensive and demonstrate the superiority of the proposed approach over baseline models.
To further improve the paper, I suggest that the authors provide more analysis on the computational cost of the Neural Cache Model and its scalability to larger datasets. Additionally, it would be interesting to see more experiments on the effect of different cache sizes and interpolation parameters on the performance of the model.
Some questions I would like the authors to answer include: How does the Neural Cache Model handle out-of-vocabulary words? Can the authors provide more insights on the relationship between the Neural Cache Model and other memory-augmented neural networks, such as pointer networks? How does the authors' approach compare to other methods for adapting language models to dynamic environments? 
Overall, the paper presents a significant contribution to the field of natural language processing, and with some additional analysis and experimentation, it has the potential to be a strong publication.