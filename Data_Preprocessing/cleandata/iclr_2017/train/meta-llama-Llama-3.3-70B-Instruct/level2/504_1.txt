This paper presents a novel approach to learning reward functions for reinforcement learning (RL) from visual demonstrations. The authors propose a method that leverages pre-trained deep models to extract features from demonstration videos and learn a reward function that can be used to train an RL agent. The key insight is that the features learned by the deep model can be used to identify the key intermediate steps of a task, and a reward function can be learned to encourage the agent to follow these steps.
The paper makes several significant contributions, including: (1) a method for learning reward functions from visual demonstrations, (2) a technique for discovering intermediate steps in a task, and (3) a demonstration of the effectiveness of the approach on a real-world robotic task.
My decision is to accept this paper, with two key reasons for this choice: (1) the paper presents a novel and well-motivated approach to learning reward functions from visual demonstrations, and (2) the experimental results demonstrate the effectiveness of the approach on a real-world robotic task.
The paper is well-written and easy to follow, with clear explanations of the methodology and experimental results. The authors provide a thorough discussion of the related work and clearly motivate the need for their approach. The experimental results are impressive, demonstrating the ability of the learned reward function to guide an RL agent to successfully complete a complex task.
One potential limitation of the paper is that the approach relies on pre-trained deep models, which may not always be available or suitable for a given task. However, the authors demonstrate that their approach can be effective even with a small number of demonstrations, which suggests that it may be possible to learn effective reward functions even in the absence of large amounts of labeled data.
To improve the paper, I would suggest that the authors provide more details on the implementation of the PI2 reinforcement learning algorithm, as well as more analysis of the learned reward functions. Additionally, it would be interesting to see more experiments demonstrating the robustness of the approach to variations in the demonstration data and the environment.
Some questions I would like the authors to answer include: (1) How sensitive is the approach to the choice of pre-trained deep model, and are there any specific models that work better than others? (2) Can the approach be extended to learn reward functions from demonstrations in other modalities, such as audio or tactile data? (3) How does the approach compare to other methods for learning reward functions, such as inverse reinforcement learning or learning from demonstrations? 
Overall, this is a strong paper that presents a novel and effective approach to learning reward functions from visual demonstrations. With some additional analysis and experimentation, the paper has the potential to make a significant contribution to the field of reinforcement learning.