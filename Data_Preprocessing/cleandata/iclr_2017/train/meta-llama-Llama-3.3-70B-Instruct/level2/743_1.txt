The paper under review presents empirical evidence for the presence of universality in the halting time of optimization algorithms applied to random systems, including spin glasses and deep learning. The authors claim that the fluctuations of the halting time follow a distribution that remains unchanged even when the distribution on the landscape is changed, after centering and scaling. They identify two main universality classes: a Gumbel-like distribution and a Gaussian-like distribution.
I decide to accept this paper with minor revisions. The main reason for this decision is that the paper presents a well-motivated and well-placed approach in the literature, and the empirical evidence provided is convincing. The authors demonstrate universality in various algorithms, including the conjugate gradient algorithm, gradient descent, and stochastic gradient descent, and provide a clear definition of universality.
The supporting arguments for this decision include the fact that the paper provides a clear and concise introduction to the concept of universality, and the authors demonstrate a good understanding of the relevant literature. The empirical results presented are thorough and well-organized, and the authors provide a clear explanation of their methodology. The paper also raises important questions about the conditions that lead to universality and the connection between the universal regime and the structure of the landscape.
To improve the paper, I suggest that the authors provide more details on the theoretical foundations of universality and its implications for algorithm development and tuning. Additionally, the authors could provide more discussion on the limitations of their approach and the potential applications of their results.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) Can you provide more details on the choice of ensembles and the model used in the experiments? (2) How do you plan to extend this work to provide theoretical estimates for the mean and standard deviation of the halting time? (3) Can you discuss the potential implications of your results for the development of new optimization algorithms and the tuning of existing ones? 
Overall, the paper presents a significant contribution to the field of optimization and machine learning, and with minor revisions, it has the potential to be a strong publication.