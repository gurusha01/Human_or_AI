This paper claims to investigate the capacity and trainability of various Recurrent Neural Network (RNN) architectures, including vanilla RNNs, LSTMs, GRUs, and two novel architectures, the Update Gate RNN (UGRNN) and the Intersection RNN (+RNN). The authors experimentally show that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, and that the capacity is approximately 5 bits per parameter. They also find that the per-task parameter capacity bound determines performance for several tasks, and that gated architectures are easier to train than vanilla RNNs.
Based on the results presented, I decide to accept this paper. The two key reasons for this choice are: (1) the paper provides a thorough and well-motivated experimental setup to investigate the capacity and trainability of RNN architectures, and (2) the results are scientifically rigorous and provide new insights into the behavior of RNNs.
The paper is well-organized, and the authors provide a clear and concise explanation of their methods and results. The experimental setup is thorough, and the authors use a Gaussian Process tuner to optimize the hyperparameters of the RNNs, which allows for a fair comparison between architectures. The results are presented in a clear and easy-to-understand manner, and the authors provide a detailed analysis of the capacity and trainability of each architecture.
One potential limitation of the paper is that the authors only investigate a limited number of tasks, and it is unclear whether the results generalize to other tasks and domains. Additionally, the authors do not provide a detailed analysis of the computational resources required for each architecture, which could be an important consideration in practice.
To improve the paper, I suggest that the authors provide more details on the computational resources required for each architecture, and that they investigate the generalizability of their results to other tasks and domains. Additionally, it would be interesting to see a more detailed analysis of the trade-offs between capacity and trainability, and how these trade-offs vary depending on the specific task and architecture.
Some questions I would like the authors to answer are: (1) How do the results change when using different optimization algorithms or hyperparameter tuning methods? (2) Can the authors provide more details on the implementation of the UGRNN and +RNN architectures, and how they differ from existing architectures? (3) How do the results generalize to other tasks and domains, such as speech recognition or natural language processing?