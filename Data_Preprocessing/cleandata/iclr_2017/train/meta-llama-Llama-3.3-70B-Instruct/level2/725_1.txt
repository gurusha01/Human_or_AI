This paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by introducing an intermediate model, called Simplified-SFNN, which approximates certain SFNNs by simplifying their upper latent units above stochastic ones. The authors establish a connection between three models: DNN → Simplified-SFNN → SFNN, leading to an efficient training procedure for stochastic models utilizing pre-trained parameters of DNNs.
The main claim of the paper is that the proposed Simplified-SFNN model can be used to efficiently train large-scale SFNNs, which is a notoriously hard task. The authors support this claim by providing theoretical analysis and experimental results on various tasks, including multi-modal learning and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100, and SVHN datasets.
The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear and concise introduction to the background and related work, and the proposed method is well-explained. The experimental results are impressive, showing that the proposed approach can outperform baseline DNNs and other stochastic models.
However, there are some limitations to the paper. The authors assume that the stochastic layers are not consecutive, which might not always be the case in practice. Additionally, the paper focuses on fully-connected networks, and it is not clear how the approach would work for convolutional neural networks.
Based on the provided information, I would accept this paper. The paper presents a novel and well-motivated approach to training SFNNs, and the experimental results are impressive. The limitations of the paper are acknowledged, and the authors provide potential directions for future work.
To improve the paper, I would suggest the following:
* Provide more details on how the approach would work for convolutional neural networks.
* Investigate the case where stochastic layers are consecutive.
* Provide more analysis on the computational complexity of the proposed approach.
* Compare the proposed approach with other stochastic models, such as Bayesian neural networks.
Some questions I would like the authors to answer:
* How do the authors plan to extend the approach to convolutional neural networks?
* What are the potential applications of the proposed approach in real-world tasks?
* How does the proposed approach compare to other stochastic models in terms of computational complexity and performance?