This paper proposes a novel approach to address the issue of catastrophic failures in deep reinforcement learning (DRL) agents. The authors introduce the concept of "intrinsic fear," which involves training a separate danger model to predict the likelihood of a catastrophe within a short number of steps. This model is used to penalize the Q-learning objective, shaping the reward function away from catastrophic states.
The paper claims to contribute to the development of safer DRL agents by mitigating the "Sisyphean curse," where agents may periodically relive catastrophic mistakes due to the use of function approximation. The authors demonstrate the effectiveness of their approach on several toy problems, including Adventure Seeker and Cart-Pole, and provide preliminary results on the Atari game Seaquest.
I decide to accept this paper because it addresses a critical issue in DRL and proposes a novel solution that shows promise in mitigating catastrophic failures. The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear explanation of the problem, and the experimental results demonstrate the effectiveness of their approach.
The main reason for my decision is that the paper provides a significant contribution to the field of DRL by addressing a critical safety issue. The authors' approach is novel and shows promise in mitigating catastrophic failures, which is a major concern in real-world applications of DRL. The paper is also well-written, and the authors provide a clear explanation of their approach and the experimental results.
To improve the paper, I suggest that the authors provide more detailed analysis of the results, including a comparison with other state-of-the-art methods. Additionally, the authors could provide more discussion on the limitations of their approach and potential future directions. Some questions that I would like the authors to answer include: How does the choice of fear radius and fear factor affect the performance of the intrinsic fear model? Can the authors provide more insight into the behavior of the danger model and how it affects the Q-learning objective? How does the intrinsic fear model perform in more complex domains, such as robotics or autonomous driving?
Overall, I believe that this paper makes a significant contribution to the field of DRL and has the potential to impact the development of safer DRL agents. With some revisions to address the limitations and provide more detailed analysis, this paper could be even stronger.