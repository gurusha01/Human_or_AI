This paper proposes a novel "density-diversity penalty" regularizer that can be applied to fully-connected layers of neural networks during training, resulting in significantly fewer parameters and distinct values, allowing for high compression of the trained weight matrices without appreciable loss in performance. The main claims of the paper are that the proposed density-diversity penalty can achieve high sparsity and low diversity in the trained weight matrices, leading to significant compression of the models, and that the approach outperforms the state-of-the-art "deep compression" method on fully-connected layers.
I decide to accept this paper, with the key reasons being that the approach is well-motivated, the results are impressive, and the paper is well-written. The authors provide a clear and concise explanation of the proposed density-diversity penalty and its optimization using a "sorting trick", and demonstrate the effectiveness of the approach on two separate tasks, computer vision and speech recognition.
The supporting arguments for this decision include the fact that the paper tackles a specific and important problem in the field of deep learning, namely the compression of neural networks, and that the proposed approach is novel and innovative. The authors also provide a thorough analysis of the related work and demonstrate that their approach outperforms the state-of-the-art method.
Additional feedback to improve the paper includes suggesting that the authors provide more details on the hyperparameter tuning process, and that they consider applying the density-diversity penalty to other types of neural network layers, such as convolutional layers. It would also be interesting to see the authors explore the use of other forms of regularization to reduce the diversity of the trained weight matrices.
Questions that I would like the authors to answer include: How did the authors choose the value of the hyperparameter Î»j, and how sensitive is the approach to this value? Can the authors provide more details on the computational cost of the sorting trick, and how it compares to the computational cost of the "deep compression" method? How do the authors plan to extend the density-diversity penalty to recurrent models and convolutional layers, and what are the potential challenges and benefits of doing so?