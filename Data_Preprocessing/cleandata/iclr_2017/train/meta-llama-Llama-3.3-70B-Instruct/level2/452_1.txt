This paper proposes a novel framework, Fine Grained Action Repetition (FiGAR), which enables Deep Reinforcement Learning (DRL) algorithms to learn temporal abstractions by predicting the number of time steps for which an action should be repeated. The framework is generic and can be applied to various DRL algorithms, including policy gradient methods and actor-critic methods. The authors demonstrate the efficacy of FiGAR by extending three popular DRL algorithms: Asynchronous Advantage Actor Critic (A3C), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradients (DDPG).
The paper claims that FiGAR can improve the performance of DRL algorithms by learning temporal abstractions, which can lead to better control policies in various domains. The authors support their claims with empirical results on several domains, including Atari 2600 games, Mujoco simulated physics tasks, and the TORCS car racing domain. The results show that FiGAR can significantly improve the performance of the baseline algorithms in many cases.
The approach is well-motivated, and the authors provide a clear explanation of the framework and its components. The paper also discusses related work and highlights the differences between FiGAR and other approaches. The experimental setup is well-designed, and the results are thoroughly analyzed.
However, there are some limitations to the paper. The authors acknowledge that the ability to stop an action repetition is important, especially in stochastic environments, but they do not address this issue in the current implementation. Additionally, the paper could benefit from more detailed analysis of the learned temporal abstractions and their impact on the performance of the algorithms.
To improve the paper, the authors could provide more insights into the learned temporal abstractions and their relationship to the underlying dynamics of the environments. They could also explore the application of FiGAR to more complex domains, such as robotic control or multi-agent systems. Furthermore, the authors could investigate the use of FiGAR in combination with other techniques, such as hierarchical reinforcement learning or transfer learning.
Overall, the paper presents a significant contribution to the field of reinforcement learning, and the proposed FiGAR framework has the potential to improve the performance of DRL algorithms in various domains. With some additional analysis and experimentation, the paper could be even stronger.
Decision: Accept
Reasons:
1. The paper proposes a novel and well-motivated framework for learning temporal abstractions in DRL algorithms.
2. The authors provide thorough empirical results on several domains, demonstrating the efficacy of FiGAR.
3. The paper is well-written, and the authors provide a clear explanation of the framework and its components.
Additional feedback:
* Provide more insights into the learned temporal abstractions and their relationship to the underlying dynamics of the environments.
* Explore the application of FiGAR to more complex domains, such as robotic control or multi-agent systems.
* Investigate the use of FiGAR in combination with other techniques, such as hierarchical reinforcement learning or transfer learning.
* Address the issue of stopping action repetitions in stochastic environments.
Questions for the authors:
* Can you provide more details on the learned temporal abstractions and their impact on the performance of the algorithms?
* How do you plan to address the issue of stopping action repetitions in stochastic environments?
* Have you considered applying FiGAR to more complex domains, such as robotic control or multi-agent systems?