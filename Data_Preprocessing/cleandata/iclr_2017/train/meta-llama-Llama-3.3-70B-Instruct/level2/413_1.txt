This paper introduces Sigma-Delta networks, a novel approach to reducing computation in deep neural networks by taking advantage of temporal redundancy in data. The authors propose a method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, which can cut computational costs by at least an order of magnitude for video data.
The paper claims to achieve this by introducing two modifications to traditional deep neural networks: temporal-difference communication and rounding. The authors demonstrate the effectiveness of their approach through experiments on toy problems, Temporal-MNIST, and a deep convolutional network on video data.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, drawing on concepts from spiking neural networks, discretization, and sparse coding. Secondly, the paper provides a clear and thorough explanation of the Sigma-Delta network architecture and its optimization method, along with convincing experimental results that demonstrate the potential for significant computational savings.
The supporting arguments for this decision include the paper's clear and well-structured presentation, the thorough evaluation of the approach through multiple experiments, and the potential impact of the work on reducing computational costs for deep neural networks. The authors also provide a detailed analysis of the trade-off between error and computation, and propose a method for jointly optimizing these two objectives.
To improve the paper, I suggest that the authors provide more discussion on the potential limitations and challenges of implementing Sigma-Delta networks in practice, particularly on modern computing hardware. Additionally, it would be helpful to see more detailed comparisons with other related work, such as binarized neural networks and sparse coding approaches.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to address the issue of high-level feature instability in convolutional networks, which was observed in the experiments? Can the Sigma-Delta network architecture be extended to other types of neural networks, such as recurrent neural networks or graph neural networks? How do the authors envision the Sigma-Delta network approach being used in real-world applications, such as video processing or autonomous driving?