This paper presents a theoretical analysis of the nonlinear dynamics of two-layered bias-free ReLU networks. The authors derive a close-form expression for the expected gradient of the loss function and use it to study the convergence of gradient descent. They show that for a single ReLU node, the dynamics converges to the optimal solution with high probability if the initialization is proper. For multiple ReLU nodes, they prove that when the teacher parameters form an orthonormal basis, a symmetric initialization leads to convergence to a saddle point, while a certain symmetry-breaking initialization converges to the optimal solution without getting trapped in local minima.
The paper is well-organized, and the authors provide a clear and concise introduction to the problem and their approach. The theoretical analysis is rigorous, and the results are supported by simulations. The paper also provides a good review of the related work and highlights the contributions of the authors.
I decide to accept this paper because it presents a significant contribution to the understanding of the dynamics of deep neural networks. The results are novel and provide new insights into the behavior of ReLU networks. The paper is well-written, and the authors provide a clear and concise explanation of their approach and results.
To further improve the paper, I suggest that the authors provide more details on the simulations and the experimental setup. Additionally, it would be interesting to see how the results generalize to more complex networks and different activation functions. The authors may also want to consider discussing the implications of their results for the design of neural networks and the choice of initialization methods.
Some specific questions that I would like the authors to address in their response are:
* Can the authors provide more details on the simulation setup and the parameters used in the experiments?
* How do the results generalize to more complex networks, such as multilayer networks with different activation functions?
* What are the implications of the results for the design of neural networks and the choice of initialization methods?
* Can the authors provide more insights into the behavior of the dynamics for different initialization methods and network architectures?