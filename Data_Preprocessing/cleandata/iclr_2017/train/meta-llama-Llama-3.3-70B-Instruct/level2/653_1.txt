This paper analyzes the effect of mini-batch size on the convergence of Stochastic Gradient Descent (SGD) and its asynchronous variant, Asynchronous Stochastic Gradient Descent (ASGD). The authors provide theoretical justifications for the practical observation that smaller mini-batches converge faster than larger mini-batches in SGD, and that increasing the number of learners in ASGD can lead to slower convergence.
The paper's main claims are well-supported by theoretical analysis and experimental evidence. The authors provide a clear and concise explanation of their methodology and results, making it easy to follow their arguments. The use of mathematical proofs and lemmas adds to the paper's rigor and credibility.
I decide to accept this paper because it provides a significant contribution to the understanding of SGD and ASGD convergence. The paper's results have important implications for the design of deep learning systems, particularly in the context of distributed computing. The authors' analysis of the trade-offs between mini-batch size and convergence rate provides valuable insights for practitioners seeking to optimize their systems.
One potential limitation of the paper is that it focuses primarily on the theoretical analysis of SGD and ASGD, with limited discussion of the practical implications of their results. Additionally, the authors could provide more context about the related work in the field and how their contributions fit into the broader landscape of deep learning research.
To improve the paper, I suggest that the authors provide more detailed explanations of their experimental setup and results. For example, they could include more information about the specific deep learning models and datasets used in their experiments, as well as more detailed plots and tables to illustrate their findings. Additionally, the authors could discuss potential avenues for future research, such as exploring the application of their results to other optimization algorithms or distributed computing architectures.
Some questions I would like the authors to answer include: How do the results of this paper relate to other work on distributed optimization algorithms, such as distributed SGD with momentum or adaptive learning rates? Can the authors provide more insight into the practical implications of their results, such as how to choose the optimal mini-batch size or number of learners for a given deep learning task? Are there any potential limitations or biases in the experimental setup that could affect the validity of the results?