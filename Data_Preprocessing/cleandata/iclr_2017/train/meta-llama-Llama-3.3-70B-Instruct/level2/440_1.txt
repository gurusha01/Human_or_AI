This paper presents a novel approach to policy search in stochastic dynamical systems using model-based reinforcement learning with Bayesian neural networks (BNNs) that include stochastic input variables. The authors claim that their method can capture complex statistical patterns in the transition dynamics, such as multi-modality and heteroskedasticity, which are usually missed by alternative modeling approaches.
I decide to accept this paper with the following key reasons:
1. The paper tackles a specific and well-defined problem in the field of reinforcement learning, which is policy search in stochastic dynamical systems.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and how the proposed method addresses these limitations.
3. The paper provides a thorough evaluation of the proposed method on several benchmark problems, including a challenging problem that has not been solved by model-based approaches before.
The supporting arguments for my decision are as follows:
* The paper provides a clear and concise introduction to the problem of policy search in stochastic dynamical systems and the limitations of existing methods.
* The authors propose a novel approach that uses BNNs with stochastic input variables to capture complex statistical patterns in the transition dynamics.
* The paper provides a thorough evaluation of the proposed method on several benchmark problems, including the Wet-Chicken benchmark, a gas turbine control problem, and an industrial benchmark.
* The results show that the proposed method outperforms existing methods, including Gaussian processes and variational Bayes, on several metrics, including test log-likelihood and policy performance.
Additional feedback to improve the paper:
* The paper could benefit from a more detailed explanation of the α-divergence minimization method and how it is used to train the BNNs.
* The authors could provide more insight into the choice of hyperparameters, such as the number of hidden layers and units, and how they were tuned.
* The paper could benefit from a more detailed comparison with other model-based reinforcement learning methods, such as PILCO.
Questions to the authors:
* Can you provide more details on how the α-divergence minimization method is used to train the BNNs, and how it compares to other methods, such as variational Bayes?
* How did you choose the hyperparameters, such as the number of hidden layers and units, and how did you tune them?
* Can you provide more insight into the computational complexity of the proposed method, and how it compares to other model-based reinforcement learning methods?