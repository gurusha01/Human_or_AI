This paper presents a novel approach to automating algorithm design by learning an optimization algorithm using reinforcement learning. The authors formulate the problem of learning an optimization algorithm as a policy search problem in a Markov decision process, where the policy corresponds to the optimization algorithm. They use guided policy search to learn the policy, which is parameterized using a neural network.
The paper claims to contribute a method for learning optimization algorithms that can outperform hand-engineered algorithms in terms of convergence speed and final objective value. The authors evaluate their approach on various convex and non-convex optimization problems, including logistic regression, robust linear regression, and neural network classification. The results show that the learned optimizer converges faster and/or reaches better optima than popular hand-engineered algorithms.
I decide to accept this paper, with two key reasons for this choice: (1) the paper presents a novel and well-motivated approach to automating algorithm design, and (2) the experimental results demonstrate the effectiveness of the proposed method on various optimization problems.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of learning optimization algorithms. The related work section is comprehensive, and the authors clearly distinguish their approach from existing work on meta-learning and program induction. The experimental results are thorough and well-presented, and the authors provide a detailed analysis of the performance of the learned optimizer on various optimization problems.
To improve the paper, I suggest that the authors provide more details on the implementation of the guided policy search algorithm, including the choice of hyperparameters and the number of iterations used in the experiments. Additionally, the authors could provide more insights into the behavior of the learned optimizer, such as visualizations of the optimization trajectories or analysis of the learned policy.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How do the authors choose the hyperparameters for the guided policy search algorithm, and how do they affect the performance of the learned optimizer? (2) Can the authors provide more insights into the behavior of the learned optimizer on different optimization problems, such as visualizations of the optimization trajectories or analysis of the learned policy? (3) How does the proposed approach compare to other methods for automating algorithm design, such as genetic programming or program induction? 
Overall, the paper presents a significant contribution to the field of machine learning and optimization, and I believe it has the potential to inspire further research in this area.