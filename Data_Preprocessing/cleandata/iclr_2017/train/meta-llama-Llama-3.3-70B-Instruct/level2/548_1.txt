This paper introduces a novel dynamic normalization technique, Charged Point Normalization (CPN), which enables gradient-based optimization algorithms to escape saddle points in high-dimensional non-convex optimization problems. The authors provide a thorough analysis of the problem of saddle points and the behavior of first-order gradient descent algorithms around these points. They also present empirical results on various neural network architectures, demonstrating the effectiveness of CPN in escaping saddle points.
I decide to accept this paper, with the main reason being that the authors provide a well-motivated and well-placed approach in the literature, and the paper supports its claims with correct and scientifically rigorous results. The introduction of CPN is a significant contribution to the field, and the authors provide a clear and concise explanation of the technique.
The paper is well-organized, and the authors provide a thorough analysis of the problem and the proposed solution. The empirical results are convincing, and the authors demonstrate the effectiveness of CPN on various datasets and neural network architectures. The discussion on the theoretical properties of first-order gradient descent algorithms around saddle points is also insightful.
One potential limitation of the paper is the lack of a comprehensive study on the behavior of CPN with respect to its hyper-parameters. The authors acknowledge this limitation and note that the selection of hyper-parameters was kept simple. However, this does not detract from the overall contribution of the paper.
To improve the paper, I suggest that the authors provide more details on the implementation of CPN and its computational complexity. Additionally, it would be interesting to see a more detailed analysis of the trade-off between exploration and exploitation in CPN, as well as a comparison with other optimization algorithms that escape saddle points.
Some questions I would like the authors to answer include: How does the choice of the Ï† function affect the performance of CPN? Can the authors provide more insights into the behavior of CPN on different types of optimization problems? How does CPN compare to other optimization algorithms that escape saddle points, such as those using second-order information? 
Overall, the paper is well-written, and the authors provide a significant contribution to the field. With some minor revisions to address the limitations and provide more details on the implementation and analysis, the paper has the potential to be a strong contribution to the conference.