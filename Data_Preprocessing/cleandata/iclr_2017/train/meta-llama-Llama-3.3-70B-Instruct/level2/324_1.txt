The paper presents a method to prune filters in Convolutional Neural Networks (CNNs) to reduce computation costs without introducing irregular sparsity. The authors propose a simple yet effective approach to prune filters based on their weight magnitudes, which achieves significant reductions in FLOP (up to 34% for VGG-16 and 38% for ResNet-110) while maintaining close to the original accuracy.
I decide to accept this paper with the following key reasons:
1. The paper tackles a specific and relevant problem in the field of CNNs, which is reducing computation costs without sacrificing accuracy.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and the advantages of the proposed method.
The supporting arguments for this decision are:
* The paper provides a thorough analysis of the sensitivity of each layer to pruning, which is useful for understanding the robustness of the network.
* The authors demonstrate the effectiveness of their approach on various architectures, including VGG-16 and ResNet-110, and on different datasets, including CIFAR-10 and ImageNet.
* The paper provides a comparison with other pruning methods, including random filter pruning and activation-based feature map pruning, which shows the superiority of the proposed approach.
Additional feedback to improve the paper:
* It would be helpful to provide more details on the computational resources used for the experiments, such as the GPU model and the batch size.
* The authors could consider providing more visualizations, such as plots of the filter weights and feature maps, to help illustrate the pruning process.
* It would be interesting to see an analysis of the effect of pruning on the network's robustness to adversarial attacks.
Questions to the authors:
* Can you provide more insights on why the `1-norm works better than the `2-norm for filter pruning in some cases?
* How do you plan to extend this work to other types of neural networks, such as Recurrent Neural Networks (RNNs) or Generative Adversarial Networks (GANs)?
* Have you considered using other pruning criteria, such as the filter's output variance or the feature map's activation statistics?