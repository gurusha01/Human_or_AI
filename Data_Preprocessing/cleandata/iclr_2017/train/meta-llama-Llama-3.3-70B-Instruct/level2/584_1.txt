The paper introduces a Joint Many-Task (JMT) model that predicts increasingly complex NLP tasks at successively deeper layers. The model is trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. The authors propose an adaptive training and regularization strategy to grow the model in its depth, avoiding catastrophic interference between tasks. The model achieves state-of-the-art results on chunking, dependency parsing, semantic relatedness, and textual entailment, and performs competitively on POS tagging.
I decide to accept this paper for the following reasons: 
1. The paper tackles a specific question/problem of jointly training multiple NLP tasks in a single model, which is a well-motivated and well-placed problem in the literature.
2. The approach is well-motivated, and the authors provide a clear explanation of their model and training strategy.
3. The paper supports its claims with extensive experimental results, including comparisons with published results and ablation studies.
The supporting arguments for my decision are as follows:
* The paper provides a clear and concise overview of the JMT model and its components, including the use of shortcut connections, successive regularization, and character n-gram embeddings.
* The experimental results demonstrate the effectiveness of the JMT model in achieving state-of-the-art results on multiple NLP tasks.
* The ablation studies provide valuable insights into the importance of different components of the model, such as the shortcut connections and character n-gram embeddings.
To improve the paper, I suggest the following additional feedback:
* The authors could provide more analysis on the limitations of their model, such as its performance on out-of-vocabulary words and its ability to handle tasks with different linguistic hierarchies.
* The authors could also provide more discussion on the potential applications of their model, such as its use in low-resource languages or its integration with other NLP models.
* The authors could consider adding more visualizations or illustrations to help explain the architecture and components of the JMT model.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to handle tasks with different linguistic hierarchies, such as tasks that require a different order of operations or tasks that involve multiple languages?
* How do the authors evaluate the performance of their model on out-of-vocabulary words, and what strategies do they use to handle such words?
* Can the authors provide more details on the computational resources and training time required to train the JMT model, and how they plan to make the model more efficient and scalable?