This paper proposes ParMAC, a parallel, distributed framework for training nested, nonconvex models using the method of auxiliary coordinates (MAC). The authors claim that ParMAC achieves high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance, and streaming data processing. 
I decide to accept this paper with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, addressing the need for parallel, distributed optimisation algorithms for nonconvex problems in machine learning. Secondly, the paper provides a thorough analysis of the parallel speedup and convergence of ParMAC, along with a comprehensive experimental evaluation on large-scale datasets.
The supporting arguments for the decision include the fact that the paper clearly outlines the main claims and provides a detailed explanation of the ParMAC algorithm, including its advantages and limitations. The authors also demonstrate the effectiveness of ParMAC through experiments on large-scale datasets, achieving nearly perfect speedups and good retrieval precision. Additionally, the paper discusses the convergence properties of ParMAC and its relationship to expectation-maximisation (EM) algorithms.
To further improve the paper, I suggest that the authors provide more details on the implementation of ParMAC, including the specific optimisation algorithms used for the W and Z steps, and the tuning of hyperparameters. It would also be helpful to include more comparisons with other parallel, distributed optimisation algorithms, such as Google's DistBelief and parameter-server-based approaches. Furthermore, the authors could explore the application of ParMAC to other nested models, such as deep neural networks, and investigate the use of GPUs or distributed convex optimisation algorithms to further improve performance.
Some questions I would like the authors to answer include: How does the choice of the penalty parameter schedule affect the convergence of ParMAC? Can the authors provide more insights into the trade-offs between the number of machines, dataset size, and computation/communication times in ParMAC? How does ParMAC handle non-differentiable layers, and are there any plans to extend the algorithm to handle more complex models?