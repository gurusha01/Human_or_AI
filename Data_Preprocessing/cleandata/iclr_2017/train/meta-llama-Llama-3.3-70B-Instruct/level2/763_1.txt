This paper proposes a novel approach to modeling relational time series, which are multiple time series that are correlated both within and between series. The authors introduce a dynamic state space model, called Relational Dynamic model with Gaussian representations (RDG), that represents latent factors as Gaussian distributions in a latent space and learns the series dynamics in this latent space. The model has two main components: a decoding component that maps the latent representations to the observations, and a dynamic component that models the time dynamics in the latent space. The authors also introduce a structural regularization term that encourages the model to learn similar representations for time series that are interdependent.
The paper claims to contribute a new dynamical model for relational time series inspired by representation learning, and a stochastic component for modeling the uncertainties at the observation and dynamic levels. The authors evaluate their model on four datasets and compare it to several baselines, including Auto-Regressive, Feed Forward Neural Network, Recurrent Neural Network, Kalman Filter, and Dynamic Factor Graph. The results show that the proposed model outperforms the baselines on most datasets and is competitive with the state-of-the-art models.
I decide to accept this paper because it tackles a specific and important problem in the field of time series forecasting, and the approach is well-motivated and well-placed in the literature. The paper supports its claims with empirical results on several datasets, and the model is shown to be effective in modeling the uncertainty associated with its predictions.
The strengths of the paper include the novelty of the approach, the thorough evaluation of the model on several datasets, and the comparison to several baselines. The paper is also well-written and easy to follow. However, there are some limitations to the paper, such as the assumption of a static graph structure, which may not be realistic in many applications. Additionally, the paper could benefit from more analysis of the model's performance on different types of datasets and more comparison to other state-of-the-art models.
To improve the paper, I suggest that the authors provide more details on the implementation of the model, such as the choice of hyperparameters and the optimization algorithm used. Additionally, the authors could provide more analysis of the model's performance on different types of datasets and more comparison to other state-of-the-art models. The authors could also consider extending the model to handle more complex graph structures, such as dynamic graphs or graphs with multiple types of edges.
I would like to ask the authors to clarify the following points: (1) How did the authors choose the hyperparameters for the model, such as the size of the latent space and the number of layers in the neural network? (2) How did the authors handle missing values in the datasets, and what was the impact of missing values on the model's performance? (3) Can the authors provide more details on the computational complexity of the model and the time it takes to train the model on large datasets?