Summary of the Paper's Claims and Contributions
The paper proposes a novel hierarchical deep reinforcement learning (RL) architecture for zero-shot task generalization in RL. The architecture consists of two interacting neural controllers: a meta controller that reads instructions and communicates subtasks to a subtask controller, which executes the given subtask. The authors claim that their architecture can generalize well to unseen instructions and longer sequences of instructions. They also propose a regularizer that encourages the subtask controller to learn subtask embeddings that capture correspondences between similar subtasks, and a differentiable neural network architecture in the meta controller that learns temporal abstractions.
Decision: Accept
I decide to accept this paper because it presents a well-motivated and well-executed approach to zero-shot task generalization in RL. The authors provide a clear and detailed description of their architecture and training procedures, and their experimental results demonstrate the effectiveness of their approach.
Supporting Arguments
1. Well-motivated approach: The authors provide a clear motivation for their approach, highlighting the importance of zero-shot task generalization in RL and the limitations of existing methods.
2. Effective architecture: The proposed hierarchical architecture is well-designed and effective, as demonstrated by the experimental results.
3. Strong experimental results: The authors provide a thorough evaluation of their approach, including comparisons to baseline methods and ablation studies.
4. Good writing and organization: The paper is well-written and easy to follow, with clear and concise descriptions of the architecture, training procedures, and experimental results.
Additional Feedback
To further improve the paper, I suggest that the authors:
1. Provide more analysis of the learned policies: While the authors provide some visualization of the learned policies, it would be helpful to have more detailed analysis of the policies learned by the meta controller and subtask controller.
2. Investigate the scalability of the approach: The authors demonstrate the effectiveness of their approach on a 2D grid world and a 3D visual environment, but it would be interesting to see how the approach scales to more complex environments and tasks.
3. Compare to other zero-shot learning methods: While the authors compare their approach to baseline RL methods, it would be helpful to compare their approach to other zero-shot learning methods, such as those used in computer vision and natural language processing.
Questions for the Authors
1. How do the authors plan to extend their approach to more complex environments and tasks?
2. Can the authors provide more details on the training procedures and hyperparameter tuning?
3. How do the authors plan to address the potential limitations of their approach, such as the need for a large amount of training data and the potential for overfitting?