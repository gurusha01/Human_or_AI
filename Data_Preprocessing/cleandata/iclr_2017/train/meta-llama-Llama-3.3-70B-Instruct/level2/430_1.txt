The paper "Latent Sequence Decompositions" presents a novel framework for sequence-to-sequence models, where the decomposition of the target sequence into tokens is learned during training, rather than being fixed a priori. The authors argue that this approach can lead to more efficient and effective modeling of sequences, particularly in tasks such as speech recognition.
The main claim of the paper is that the proposed Latent Sequence Decompositions (LSD) framework can learn a distribution of sequence decompositions that are a function of both the input and output sequences. The authors support this claim through a series of experiments on the Wall Street Journal speech recognition task, where they demonstrate that the LSD model outperforms a baseline character-level sequence-to-sequence model, achieving a word error rate (WER) of 12.9% compared to 14.8% for the baseline.
The approach is well-motivated, and the authors provide a clear and detailed explanation of the LSD framework, including the probabilistic formulation and the training algorithm. The experiments are also well-designed, and the results are convincing.
My decision is to accept this paper, with the main reason being that the authors have presented a novel and well-motivated approach to sequence-to-sequence modeling, and have demonstrated its effectiveness through a series of experiments.
One potential limitation of the paper is that the authors do not provide a detailed analysis of the learned decompositions, and it is not clear how the model is using the different tokens in the vocabulary. To address this, I would suggest that the authors provide more visualization and analysis of the learned decompositions, to help understand how the model is working.
Additionally, I would like to see more discussion of the relationship between the LSD framework and other approaches to sequence-to-sequence modeling, such as word piece models and subword modeling. How does the LSD framework compare to these approaches, and what are the advantages and disadvantages of each?
Some questions I would like the authors to answer are:
* Can you provide more visualization and analysis of the learned decompositions, to help understand how the model is working?
* How does the LSD framework compare to other approaches to sequence-to-sequence modeling, such as word piece models and subword modeling?
* What are the advantages and disadvantages of the LSD framework, and how does it relate to other approaches in the field?
Overall, I think this is a strong paper that presents a novel and well-motivated approach to sequence-to-sequence modeling. With some additional analysis and discussion, I believe it has the potential to make a significant contribution to the field.