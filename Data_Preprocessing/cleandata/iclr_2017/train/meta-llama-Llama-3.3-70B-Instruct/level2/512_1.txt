This paper proposes a principled framework for nonparametrically learning activation functions in deep neural networks. The authors introduce a class of nonparametric models for activation functions using a Fourier basis expansion and demonstrate that networks with these nonparametric activation functions generalize well. They also provide a theoretical justification for their choice of nonparametric activation functions and demonstrate that feed-forward networks with these activation functions have vanishing generalization error as the sample size increases.
I decide to accept this paper with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of deep learning, namely the learning of activation functions in neural networks. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of current methods and the potential benefits of the proposed approach. Secondly, the paper provides a thorough theoretical analysis of the proposed method, including a proof of the generalization bound for feed-forward networks with nonparametric activation functions.
The supporting arguments for these reasons are as follows. The paper provides a clear and concise introduction to the problem of learning activation functions in neural networks, and motivates the need for a nonparametric approach. The authors then introduce their proposed method, which is based on a Fourier basis expansion, and provide a detailed analysis of its properties and advantages. The theoretical analysis is rigorous and well-supported, and provides a clear understanding of the conditions under which the proposed method is guaranteed to work.
In addition to the theoretical analysis, the paper also provides an experimental evaluation of the proposed method on several benchmark datasets, including MNIST and CIFAR-10. The results demonstrate the effectiveness of the proposed method in improving the generalization performance of neural networks, and provide a comparison with other state-of-the-art methods.
To improve the paper, I would suggest the following additional feedback. Firstly, the authors could provide more details on the implementation of the proposed method, including the specific hyperparameters used and the computational resources required. Secondly, the authors could provide a more detailed analysis of the results, including a discussion of the limitations of the proposed method and potential avenues for future work.
Some questions I would like the authors to answer to clarify my understanding of the paper are as follows. How do the authors choose the hyperparameters for the Fourier basis expansion, and what is the effect of different choices on the performance of the proposed method? How do the authors handle the case where the activation functions are not well-approximated by a Fourier basis expansion, and what are the implications for the generalization bound? What are the potential applications of the proposed method beyond the benchmark datasets considered in the paper, and how might it be used in practice?