This paper proposes a novel approach to reduce the memory requirements and power consumption of deep neural networks (DNNs) by introducing sparsely-connected networks. The authors claim that their approach can reduce the number of connections in fully-connected networks by up to 90% while improving the accuracy performance on three popular datasets (MNIST, CIFAR10, and SVHN).
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and relevant problem in the field of DNNs, which is the high memory requirements and power consumption of fully-connected layers.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and the benefits of the proposed sparsely-connected networks.
The paper provides strong support for its claims through extensive experimental results on three datasets, demonstrating the effectiveness of the proposed approach in reducing memory requirements and improving accuracy. The use of linear-feedback shift registers (LFSRs) to generate random binary streams for forming the sparse weight matrix is a novel and efficient technique. The proposed VLSI architecture for the sparsely-connected network is also well-designed, with a significant reduction in silicon area and power consumption compared to conventional fully-connected networks.
To further improve the paper, I suggest that the authors provide more details on the training algorithm and the hyperparameter tuning process, as well as a more comprehensive comparison with other state-of-the-art methods. Additionally, it would be helpful to include more analysis on the robustness of the proposed approach to different types of noise and adversarial attacks.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How did the authors choose the value of p for the stochastic number generator (SNG) unit, and what is the effect of different values of p on the sparsity degree and accuracy of the network?
* Can the authors provide more details on the implementation of the proposed VLSI architecture, including the specific hardware components and the clock frequency used?
* How does the proposed approach compare to other methods for reducing memory requirements and power consumption in DNNs, such as pruning, quantization, and knowledge distillation?