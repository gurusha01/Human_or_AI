This paper proposes a novel framework for end-to-end multi-task representation learning in deep neural networks. The authors generalize matrix factorization-based multi-task ideas to tensor factorization, allowing for flexible knowledge sharing in fully connected and convolutional DNN layers. The approach is well-motivated, and the authors provide a clear explanation of the methodology and its advantages over existing methods.
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in multi-task learning, and (2) the approach is well-motivated and supported by experimental results.
The paper provides a thorough review of related work, and the authors demonstrate a good understanding of the field. The methodology is clearly explained, and the experimental results show that the proposed approach outperforms single task learning and comparable or better performance than the best results from exhaustive search of user-defined MTL architectures.
To further improve the paper, I suggest that the authors provide more analysis on the learned sharing structure, such as visualizing the learned weights and analyzing the similarity between tasks. Additionally, it would be interesting to see more experiments on different datasets and tasks to demonstrate the generalizability of the approach.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How do the authors determine the rank of the tensor factorization, and what is the effect of different rank values on the performance? (2) Can the authors provide more details on the computational cost of the proposed approach compared to existing methods? (3) How do the authors plan to extend the approach to more complex tasks, such as multi-modal learning or transfer learning?
Overall, the paper is well-written, and the authors provide a clear and concise explanation of the methodology and results. With some additional analysis and experiments, the paper has the potential to make a significant contribution to the field of multi-task learning.