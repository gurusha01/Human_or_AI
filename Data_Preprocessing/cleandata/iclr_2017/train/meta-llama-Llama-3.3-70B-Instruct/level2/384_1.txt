This paper proposes two end-to-end neural network models for machine comprehension, specifically designed to tackle the challenges of the Stanford Question Answering Dataset (SQuAD) and the Human-Generated MAchine Reading COmprehension (MSMARCO) dataset. The models combine match-LSTM and Pointer Network to handle the special properties of these datasets, where answers can be any sequence of tokens from the given text. The authors claim that their models outperform the best results obtained by previous methods, including logistic regression and manually crafted features.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper proposes a novel and well-motivated approach to machine comprehension, which is a significant problem in natural language processing. The use of match-LSTM and Pointer Network is well-justified, and the authors provide a clear explanation of how these components work together to address the challenges of the SQuAD and MSMARCO datasets. Secondly, the experimental results demonstrate the effectiveness of the proposed models, with significant improvements over previous methods on both datasets.
The supporting arguments for this decision include the fact that the paper provides a thorough analysis of the results, including ablation studies and visualizations of the attention mechanism. The authors also discuss the limitations of their models and provide suggestions for future work, demonstrating a clear understanding of the strengths and weaknesses of their approach. Additionally, the paper is well-written and easy to follow, with clear explanations of the technical details and a thorough review of related work.
To improve the paper, I would suggest that the authors provide more details on the training process, including the hyperparameter settings and the optimization algorithm used. Additionally, it would be helpful to include more examples of the predictions made by the models, to give a better sense of their strengths and weaknesses. Some questions I would like the authors to answer include: How do the models perform on questions that require multi-sentence reasoning? Can the authors provide more insight into why the boundary model outperforms the sequence model on the MSMARCO dataset? How do the models handle out-of-vocabulary words, and are there any plans to incorporate subword modeling or other techniques to address this issue?