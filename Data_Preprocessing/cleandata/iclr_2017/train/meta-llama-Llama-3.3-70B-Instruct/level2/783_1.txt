This paper proposes a novel approach to distributed training of deep learning models, which combines the benefits of synchronous and asynchronous stochastic optimization. The authors introduce a method called synchronous stochastic optimization with backup workers, which mitigates the straggler effect in synchronous optimization while avoiding the staleness of gradients in asynchronous optimization.
The paper claims to demonstrate that this approach converges faster and to better test accuracies than asynchronous training. The authors provide empirical evidence to support their claims, including experiments on the Inception and PixelCNN models. They also provide a detailed analysis of the trade-offs between dropping stragglers and waiting for more gradients, and show that their approach can be used to minimize the running time to convergence.
I decide to accept this paper, with the main reason being that the approach proposed by the authors is novel and well-motivated, and the empirical evidence provided is convincing. The paper is well-written, and the authors provide a clear and detailed explanation of their approach and the experiments they conducted.
One of the key strengths of the paper is the thorough analysis of the trade-offs between synchronous and asynchronous optimization, and the identification of the straggler effect as a key bottleneck in synchronous optimization. The authors provide a clear and detailed explanation of how their approach addresses this bottleneck, and the empirical evidence they provide demonstrates the effectiveness of their approach.
To further improve the paper, I would suggest that the authors provide more details on the implementation of their approach, including the specific hardware and software configurations used in their experiments. Additionally, it would be helpful to see more comparisons with other related work, such as the "softsync" approach proposed by Zhang et al. (2015b).
Some questions I would like the authors to answer include: How do the authors plan to extend their approach to other types of deep learning models, such as recurrent neural networks or generative adversarial networks? How do the authors plan to address the potential issue of increased communication overhead in their approach, particularly in cases where the number of workers is large? What are the potential applications of this approach beyond distributed training of deep learning models, such as in other areas of machine learning or optimization? 
Overall, I believe that this paper makes a significant contribution to the field of distributed machine learning, and I look forward to seeing the authors' future work on this topic.