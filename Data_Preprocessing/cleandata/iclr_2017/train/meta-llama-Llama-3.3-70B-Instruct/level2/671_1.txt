The paper "Dynamic Recurrent Acyclic Graphical Neural Networks" presents a novel framework for constructing recurrent neural architectures, which allows for explicit structure in the input and output representations. The authors introduce the Transition-Based Recurrent Unit (TBRU), a modular unit that can be combined to form various neural architectures, including sequence-to-sequence models, attention mechanisms, and recursive tree-structured models.
I decide to accept this paper, with the primary reason being that it presents a well-motivated and innovative approach to modeling recurrent neural networks. The authors provide a clear and concise explanation of their framework, and the experimental results demonstrate the effectiveness of their approach on two NLP tasks: dependency parsing and extractive sentence summarization.
The paper supports its claims through a combination of theoretical analysis and empirical evaluations. The authors provide a detailed description of their framework, including the TBRU and its components, and demonstrate how it can be applied to various NLP tasks. The experimental results show that the proposed approach outperforms existing methods, including seq2seq with attention, on both tasks.
One potential limitation of the paper is that the authors do not provide a comprehensive comparison with other state-of-the-art methods, particularly in the area of dependency parsing. Additionally, the paper could benefit from a more detailed analysis of the computational complexity of the proposed approach, particularly in comparison to attention mechanisms.
To improve the paper, I suggest that the authors provide more detailed explanations of the TBRU and its components, particularly the recurrence function and the transition system. Additionally, the authors could provide more insights into the interpretability of the proposed approach, particularly in terms of understanding how the explicit structure in the input and output representations contributes to the improved performance.
Some questions I would like the authors to answer include: (1) How do the authors plan to extend their framework to other NLP tasks, such as machine translation and question answering? (2) Can the authors provide more details on the computational complexity of their approach, particularly in comparison to attention mechanisms? (3) How do the authors plan to address the potential limitations of their approach, particularly in terms of the need for explicit structure in the input and output representations?