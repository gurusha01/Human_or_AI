This paper proposes a novel approach to alleviate the adverse effects of polysemy when using a lexicon to improve vector-space word representations. The authors introduce a fuzzy paraphrase set, where each paraphrase is annotated with a degree of reliability, and use this set to learn word vectors. The approach is shown to outperform prior works in several benchmarks, including the MEN Dataset and Word Analogical Reasoning Task.
I decide to accept this paper, with two key reasons for this choice: 
1. The paper tackles a specific and well-motivated problem in natural language processing, namely the issue of polysemy in word vector representations. The approach is well-placed in the literature, building on existing work on word vector representations and lexicon-based methods.
2. The paper provides a thorough evaluation of the proposed approach, including experiments on different corpora, benchmarks, and parameter settings. The results demonstrate the effectiveness of the approach in improving word vector quality, and the authors provide a detailed analysis of the results.
The supporting arguments for the decision include the fact that the paper provides a clear and well-written introduction to the problem and the proposed approach, and that the experimental results are comprehensive and well-analyzed. The authors also provide a thorough discussion of the limitations of the approach and potential avenues for future work.
Additional feedback to improve the paper includes suggesting that the authors provide more details on the computational resources required to train the model, and that they consider exploring other applications of the fuzzy paraphrase set, such as in text classification or sentiment analysis. 
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
- How do the authors plan to extend the approach to handle out-of-vocabulary words or words with limited context?
- Can the authors provide more details on the bilingual similarity measure used to estimate the membership function, and how it is computed?
- How do the authors plan to make the approach more efficient and scalable for large-scale corpora and applications?