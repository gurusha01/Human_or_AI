The paper proposes a novel video captioning model, Adaptive SpatioTemporal representation with dynAmic abstRaction (ASTAR), which utilizes a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. The model employs a novel attention mechanism with spatiotemporal alignment to adaptively and sequentially focus on different layers of CNN features and local spatiotemporal regions of the feature maps at each layer. The authors claim that their approach achieves state-of-the-art performance on the YouTube2Text benchmark.
I decide to accept this paper, with the primary reason being the novelty and effectiveness of the proposed attention mechanism. The paper provides a clear and well-motivated approach to addressing the limitations of previous video captioning models, which often compress rich video content into a single compact feature vector. The authors demonstrate the importance of leveraging intermediate convolutional layer features and achieve state-of-the-art results on the YouTube2Text benchmark.
The supporting arguments for this decision include the thorough evaluation of the proposed model on a widely-used benchmark, the clear presentation of the attention mechanism, and the comparison with previous state-of-the-art models. The paper also provides a comprehensive review of related work and demonstrates a good understanding of the field.
To further improve the paper, I suggest that the authors provide more details on the training process, such as the optimization algorithm used, the batch size, and the number of epochs. Additionally, it would be helpful to include more visualizations of the attention weights to better understand the behavior of the model. I would also like to see more analysis on the impact of the attention mechanism on the performance of the model, such as ablation studies or sensitivity analysis.
Some questions I would like the authors to answer include: How did the authors select the hyperparameters for the model, such as the number of layers and the dimensionality of the feature vectors? How does the model perform on other video captioning benchmarks, such as MSVD or MSR-VTT? Can the authors provide more insights into the computational cost of the model and its potential applications in real-world scenarios?