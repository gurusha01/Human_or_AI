The paper proposes a novel approach to boosting generative models, where models are trained in sequence to correct earlier mistakes. The authors introduce a meta-algorithm that can leverage many existing base learners, including recent latent variable models, and allows the ensemble to leverage discriminative models trained to distinguish real from synthetic data during sample generation.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of generative models, which is to improve the performance of these models by leveraging the strengths of boosting algorithms. Secondly, the approach is well-motivated and well-placed in the literature, drawing on existing work in supervised learning and generative models.
The paper provides a clear and concise overview of the proposed approach, including the theoretical conditions under which incorporating a new model to the ensemble will improve the fit. The authors also provide empirical evidence demonstrating the effectiveness of boosting on density estimation, sample generation, and unsupervised feature learning on real and synthetic datasets. The results show that the proposed approach can outperform baseline models without incurring significant computational overhead.
To further improve the paper, I would suggest that the authors provide more detailed analysis of the computational efficiency of the proposed approach, particularly in comparison to other ensemble methods. Additionally, it would be helpful to see more extensive experiments on larger and more complex datasets, such as natural images.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend the proposed approach to more sophisticated models and complex datasets? What are the potential limitations of the proposed approach, and how do the authors plan to address them? How does the proposed approach compare to other ensemble methods, such as bagging and stacking, in terms of performance and computational efficiency?
Overall, the paper presents a novel and well-motivated approach to boosting generative models, with promising empirical results and a clear potential for future extensions and applications.