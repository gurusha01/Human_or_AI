This paper proposes a novel approach to compressing deep neural networks (DNNs) using homologically functional hashing (HFH). The authors claim that HFH can achieve high compression ratios with little loss in prediction accuracy, outperforming existing methods such as HashedNets. 
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, addressing a significant problem in the field of DNN compression. The authors provide a clear and concise overview of the existing methods and their limitations, highlighting the need for a new approach. Secondly, the paper provides extensive experimental results demonstrating the effectiveness of HFH on various datasets, including MNIST, CIFAR-10, and ImageNet ILSVRC-2012.
The supporting arguments for this decision include the fact that the authors provide a thorough analysis of the properties and benefits of HFH, including its ability to reduce value collisions and improve value reconstruction. The experimental results show that HFH can achieve significant compression ratios while maintaining a high level of accuracy, making it a promising approach for practical applications. Additionally, the authors discuss the potential for combining HFH with other compression techniques to achieve even better results.
To further improve the paper, I suggest that the authors provide more details on the implementation of HFH, including the choice of hash functions and the architecture of the reconstruction network. It would also be helpful to include more analysis on the computational complexity and memory requirements of HFH, as well as its potential applications in real-world scenarios. 
Some questions I would like the authors to answer include: How do the authors plan to extend HFH to more complex network architectures, such as recurrent neural networks or transformers? Can the authors provide more insights into the trade-offs between compression ratio, accuracy, and computational complexity in HFH? How do the authors envision HFH being used in conjunction with other compression techniques, such as pruning or quantization, to achieve even better results? 
Overall, I believe that this paper presents a significant contribution to the field of DNN compression and has the potential to make a substantial impact in the community. With some additional clarification and analysis, I am confident that this paper can be even stronger and more effective in conveying its message.