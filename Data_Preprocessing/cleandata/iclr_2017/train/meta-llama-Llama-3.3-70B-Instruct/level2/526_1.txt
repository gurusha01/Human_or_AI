This paper presents a significant contribution to the field of deep learning by providing the first convergence guarantee applicable to modern convolutional neural networks (CNNs). The authors introduce the neural Taylor approximation, a straightforward application of Taylor expansions to neural networks, and the associated Taylor loss. They use this tool to analyze the convergence of neural networks and provide a guarantee that matches a lower bound for convex nonsmooth functions.
The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear and concise overview of the problem and the key concepts, making it easy to follow. The technical contributions are sound, and the empirical analysis provides strong evidence to support the theoretical claims.
I decide to accept this paper with minor revisions. The reasons for this decision are:
1. The paper tackles a specific and important problem in deep learning, providing a convergence guarantee for modern CNNs.
2. The approach is well-motivated, and the technical contributions are sound and well-supported by empirical analysis.
To further improve the paper, I suggest the following:
* Provide more discussion on the implications of the convergence guarantee and how it can be used in practice.
* Consider adding more experiments to demonstrate the effectiveness of the proposed approach on different datasets and architectures.
* Clarify the relationship between the Taylor loss and the Taylor approximation to the loss, and provide more intuition on why the Taylor loss is a better choice.
Some questions I would like the authors to address in their revision are:
* Can you provide more insight into how the neural Taylor approximation can be used to improve the optimization of neural networks in practice?
* How does the convergence guarantee provided in this paper relate to other convergence guarantees in the literature, and what are the key differences?
* Are there any potential limitations or drawbacks to using the neural Taylor approximation, and how can they be addressed?