This paper presents a novel approach to multitask deep reinforcement learning guided by policy sketches. The authors propose a framework that associates each subtask with its own modular subpolicy, and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The approach is evaluated on a maze navigation game and a 2-D Minecraft-inspired crafting game, and the results show that it outperforms standard baselines that learn task-specific or shared monolithic policies.
The main claims of the paper are that the proposed approach can learn hierarchical policies from minimal supervision, and that it can generalize to new tasks unseen at training time. The authors support these claims with extensive experiments, including multitask learning, ablations, and zero-shot and adaptation learning.
I decide to accept this paper because it presents a well-motivated and well-executed approach to multitask reinforcement learning. The paper is well-written, and the authors provide a clear and concise explanation of their approach and its advantages. The experimental results are thorough and demonstrate the effectiveness of the proposed approach.
The key reasons for my decision are:
1. The paper presents a novel and well-motivated approach to multitask reinforcement learning.
2. The experimental results are thorough and demonstrate the effectiveness of the proposed approach.
Supporting arguments for my decision include:
* The paper provides a clear and concise explanation of the proposed approach and its advantages.
* The authors provide extensive experiments to evaluate the proposed approach, including multitask learning, ablations, and zero-shot and adaptation learning.
* The results show that the proposed approach outperforms standard baselines that learn task-specific or shared monolithic policies.
Additional feedback to improve the paper includes:
* Providing more details on the implementation of the subpolicies and the critic.
* Discussing the potential limitations of the proposed approach and future directions for research.
* Providing more examples of tasks and sketches to illustrate the flexibility and generality of the proposed approach.
Questions I would like the authors to answer include:
* Can the proposed approach be applied to more complex tasks and environments?
* How does the proposed approach compare to other approaches to multitask reinforcement learning?
* Can the proposed approach be used for transfer learning and few-shot learning?