The paper proposes a new regularization method, called Central Moment Discrepancy (CMD), for domain-invariant representation learning in the context of domain adaptation with neural networks. The authors claim that CMD is a metric on the set of probability distributions on a compact interval and that convergence in CMD implies convergence in distribution. They also provide experimental results showing that CMD outperforms other state-of-the-art methods, such as Maximum Mean Discrepancy (MMD) and Domain Adversarial Neural Networks (DANN), on two benchmark datasets, Office and Amazon reviews.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and well-defined problem in the field of domain adaptation, which is a crucial aspect of machine learning.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and how CMD addresses these limitations.
3. The paper provides a thorough theoretical analysis of CMD, including proofs that it is a metric and that convergence in CMD implies convergence in distribution.
The supporting arguments for the decision are as follows: 
1. The experimental results demonstrate the effectiveness of CMD in achieving state-of-the-art performance on two benchmark datasets, which suggests that the method is practically useful.
2. The paper provides a detailed analysis of the computational complexity of CMD, which is shown to be linear with respect to the number of samples, making it more efficient than MMD-based approaches.
3. The authors also provide a parameter sensitivity analysis, which shows that the accuracy of CMD is not sensitive to the choice of the parameter K, making it easier to use in practice.
Additional feedback to improve the paper includes: 
1. Providing more insights into the relationship between CMD and other distribution matching methods, such as MMD and KL-divergence.
2. Investigating the use of CMD for training generative models, which could be an interesting direction for future research.
3. Considering the use of other kernels or parameter selection procedures for MMD, which could provide a more comprehensive comparison with CMD.
Some questions that I would like the authors to answer to clarify my understanding of the paper include: 
1. Can you provide more details on the choice of the parameter K and how it affects the performance of CMD?
2. How does CMD handle cases where the distributions have different supports or are not compactly supported?
3. Can you provide more insights into the computational complexity of CMD and how it compares to other methods, such as MMD and DANN?