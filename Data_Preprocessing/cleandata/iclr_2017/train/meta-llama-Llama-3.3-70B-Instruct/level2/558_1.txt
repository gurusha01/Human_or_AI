This paper proposes a novel approach to exploration in reinforcement learning (RL) by extending classic count-based methods to high-dimensional state spaces through hashing. The authors demonstrate that their method, which uses a hash function to discretize the state space and assigns a bonus reward based on the state-visitation count, can achieve near state-of-the-art performance on various deep RL benchmarks.
The main claims of the paper are: (1) a simple generalization of classic count-based exploration can reach near state-of-the-art performance on high-dimensional and/or continuous deep RL benchmarks, and (2) the proposed method is fast, flexible, and complementary to most existing RL algorithms.
I decide to accept this paper because it presents a well-motivated and well-executed approach to exploration in RL, and the results demonstrate significant improvements over baseline methods. The paper is well-written, and the authors provide a clear and concise explanation of their method and its advantages.
The approach is well-motivated by the need for efficient exploration in high-dimensional state spaces, and the authors provide a thorough analysis of the importance of hash function design and granularity. The experiments are well-designed and demonstrate the effectiveness of the proposed method on a range of tasks, including continuous control and Atari 2600 games.
One potential limitation of the paper is that the authors do not provide a detailed analysis of the computational complexity of their method, which could be an important consideration for large-scale RL applications. Additionally, the authors could provide more discussion on the potential applications of their method beyond the benchmarks presented in the paper.
To improve the paper, I suggest that the authors provide more details on the implementation of their method, including the specific hash functions used and the hyperparameter settings. Additionally, the authors could provide more analysis on the robustness of their method to different hyperparameter settings and the potential for overfitting.
Some questions I would like the authors to answer are: (1) How do the authors plan to extend their method to more complex RL tasks, such as those with multiple agents or partial observability? (2) Can the authors provide more insight into the design of the hash function and how it affects the performance of the method? (3) How does the authors' method compare to other exploration strategies, such as entropy regularization or curiosity-driven exploration?