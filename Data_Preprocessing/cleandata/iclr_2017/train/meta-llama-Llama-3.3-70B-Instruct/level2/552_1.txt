The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The authors claim that their architecture can learn long-term dependencies up to 5,000 time steps, outperforming existing approaches.
I decide to accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The authors provide a clear and concise explanation of the problem, and their proposed solution is theoretically sound. The use of orthogonal matrices to parameterize the recurrent transition is a novel and interesting approach, and the experimental results demonstrate the effectiveness of the DORNN architecture.
The supporting arguments for this decision include the fact that the paper is well-written, easy to follow, and provides a clear explanation of the methodology and results. The authors also provide a thorough discussion of the limitations of their approach and potential future directions, which demonstrates a good understanding of the topic and a willingness to improve.
Additional feedback to improve the paper includes providing more experimental results on more complex tasks, such as language modeling or image captioning, to demonstrate the generalizability of the DORNN architecture. The authors could also consider comparing their approach to other existing methods, such as LSTMs or GRUs, to provide a more comprehensive evaluation of their approach.
Questions I would like the authors to answer include: How do the authors plan to address the issue of training instability for longer sequence lengths? Can the authors provide more insight into the choice of parameterization for the orthogonal matrices, and how it affects the performance of the DORNN architecture? How do the authors plan to extend their approach to more complex tasks, and what are the potential challenges and limitations of their approach?