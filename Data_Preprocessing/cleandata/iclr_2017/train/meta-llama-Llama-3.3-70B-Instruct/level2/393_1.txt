This paper proposes a novel approach to attention mechanisms in deep neural networks, introducing structured attention networks that incorporate graphical models to generalize simple attention. The authors claim that this approach allows for extending attention beyond the standard soft-selection approach, enabling the model to attend to partial segmentations or subtrees. They experiment with two classes of structured attention networks: linear-chain conditional random fields (CRFs) and graph-based parsing models.
The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear and concise overview of the background and related work, highlighting the limitations of standard attention mechanisms. The technical machinery and computational techniques for backpropagating through models of this form are well-explained, and the experiments demonstrate the effectiveness of the approach on various tasks, including tree transduction, neural machine translation, question answering, and natural language inference.
The results show that structured attention networks outperform baseline attention models on these tasks, and the analysis of learned representations reveals interesting structural properties. The paper also discusses the potential applications of structured attention networks, including learning latent labelers or parsers through attention on other tasks.
However, there are some limitations to the approach. The additional complexity in computing the attention distribution increases runtime, and the authors note that structured attention was approximately 5Ã— slower to train than simple attention for the neural machine translation experiments. Furthermore, the approach requires careful initialization and tuning of hyperparameters to achieve good performance.
To improve the paper, I would suggest providing more detailed analysis of the learned representations and the structural properties they capture. Additionally, the authors could explore ways to reduce the computational complexity of the approach, such as using approximate inference methods or pruning the graphical models.
Overall, I would accept this paper, as it presents a novel and well-motivated approach to attention mechanisms in deep neural networks, with promising results on various tasks. The paper is well-written, and the technical contributions are significant.
Decision: Accept
Reasons:
1. The paper proposes a novel approach to attention mechanisms in deep neural networks, introducing structured attention networks that incorporate graphical models to generalize simple attention.
2. The approach is well-motivated, and the technical machinery and computational techniques for backpropagating through models of this form are well-explained.
3. The experiments demonstrate the effectiveness of the approach on various tasks, including tree transduction, neural machine translation, question answering, and natural language inference.
Additional feedback:
* Provide more detailed analysis of the learned representations and the structural properties they capture.
* Explore ways to reduce the computational complexity of the approach, such as using approximate inference methods or pruning the graphical models.
* Consider providing more visualizations of the attention distributions and the learned representations to help illustrate the results.
Questions for the authors:
* Can you provide more details on the initialization and tuning of hyperparameters for the structured attention networks?
* How do you plan to address the increased computational complexity of the approach in future work?
* Can you provide more insights into the types of structural properties that the learned representations capture, and how they relate to the tasks being performed?