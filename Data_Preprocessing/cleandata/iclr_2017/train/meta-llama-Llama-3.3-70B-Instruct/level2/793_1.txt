The paper "Surprisal-Driven Recurrent Networks" introduces a novel approach to recurrent neural networks by incorporating a feedback signal based on the discrepancy between past predictions and current observations. The authors claim that this feedback mechanism, which they term "surprisal-driven," improves the generalization capabilities of Long-Short Term Memory (LSTM) networks, particularly in the context of temporal data prediction. The paper presents a thorough mathematical formulation of the proposed architecture, including forward and backward passes for the Back Propagation Through Time (BPTT) algorithm.
Based on the content of the paper, I decide to Accept this paper. The two key reasons for this choice are: (1) the paper presents a well-motivated and novel approach to incorporating feedback in recurrent neural networks, and (2) the experimental results demonstrate a significant improvement in performance on the enwik8 character-level prediction task, outperforming other state-of-the-art approaches.
The paper provides a clear and concise introduction to the problem of temporal data prediction and the limitations of existing recurrent neural network architectures. The authors effectively motivate the need for a feedback mechanism and provide a thorough explanation of their proposed approach. The mathematical formulation of the surprisal-driven recurrent network is well-presented, and the experimental results are convincing.
To further improve the paper, I suggest that the authors provide more discussion on the potential applications of their approach beyond character-level prediction tasks. Additionally, it would be beneficial to include more analysis on the computational complexity of the proposed architecture and its potential scalability to larger datasets.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) How do the authors plan to extend their approach to more complex temporal data prediction tasks, such as speech or image sequence prediction? (2) Can the authors provide more insight into the choice of hyperparameters, such as the sequence length for BPTT and the batch size, and how these parameters affect the performance of the proposed architecture? (3) How do the authors envision the surprisal-driven feedback mechanism interacting with other techniques, such as attention or regularization, to further improve the performance of recurrent neural networks?