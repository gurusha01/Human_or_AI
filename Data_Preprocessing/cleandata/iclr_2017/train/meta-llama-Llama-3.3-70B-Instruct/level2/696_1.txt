This paper presents a comprehensive analysis of neural architectures for reading comprehension tasks, specifically focusing on the emergence of predication structure in the hidden state vectors of certain neural readers. The authors propose that the hidden state vectors can be viewed as a concatenation of a predicate vector and a constant symbol vector, representing the atomic formula P(c). They provide empirical evidence to support this claim, showing that the hidden state vectors can be decomposed into a direct sum of statement and entity representations.
The paper also introduces a new class of neural readers, called pointer annotation readers, which add features to the input to mark the occurrences of candidate answers. The authors demonstrate that these models perform well on the Who-did-What dataset, even without anonymization, and that the addition of linguistic features to the input embeddings can further improve performance.
The main claims of the paper are well-supported by the experimental results, and the authors provide a clear and detailed explanation of their methodology and findings. The paper also provides a thorough review of existing datasets and models for reading comprehension, making it a valuable resource for researchers in the field.
Based on the conference guidelines, I would answer the three key questions as follows:
1. The specific question/problem tackled by the paper is the analysis of neural architectures for reading comprehension tasks, with a focus on the emergence of predication structure in the hidden state vectors of certain neural readers.
2. The approach is well-motivated, including being well-placed in the literature, as it builds on existing work on neural readers and provides a new perspective on the internal workings of these models.
3. The paper supports its claims, including determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. The experimental results are well-presented and provide strong evidence for the authors' claims.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of natural language processing and provides a thorough and well-supported analysis of neural architectures for reading comprehension tasks.
Additional feedback:
* The paper could benefit from a clearer summary of the main contributions and findings in the introduction.
* The authors could provide more details on the experimental setup and hyperparameter tuning for the pointer annotation readers.
* The paper could include more discussion on the potential applications and implications of the proposed approach, beyond the specific task of reading comprehension.
Questions for the authors:
* Can you provide more insight into the choice of datasets and models used in the experiments?
* How do you plan to extend this work to other natural language processing tasks, such as question answering or text classification?
* Can you provide more details on the implementation of the pointer annotation readers, including the specific features used and the training procedure?