This paper proposes a novel approach to accelerate the training of deep neural networks by learning the weight evolution pattern from a simple network. The authors use a neural network, called the introspection network, to predict the future values of weights based on their past values, and then use this network to update the weights of other neural networks during training. The results show that this approach can significantly reduce the training time and improve the accuracy of various neural networks on different datasets, including MNIST, CIFAR-10, and ImageNet.
I decide to accept this paper because it presents a well-motivated and well-executed idea that has the potential to improve the efficiency of deep learning. The paper is well-written, and the experiments are thorough and convincing. The authors provide a clear explanation of their approach and demonstrate its effectiveness on various datasets and network architectures.
The main strengths of this paper are:
1. Novelty: The idea of using a neural network to predict weight evolution and accelerate training is new and interesting.
2. Thorough experiments: The authors provide a comprehensive set of experiments that demonstrate the effectiveness of their approach on various datasets and network architectures.
3. Improvement over baselines: The results show that the proposed approach outperforms existing methods, including Adam and SGD, in terms of training time and accuracy.
However, there are some limitations and open questions that the authors acknowledge, such as:
1. Determining optimal jump points: The authors note that the choice of jump points is crucial, but they do not provide a clear method for determining the optimal jump points.
2. Generalization to RNNs and non-image tasks: The authors only demonstrate their approach on image classification tasks, and it is unclear whether it will generalize to other types of tasks or recurrent neural networks.
To improve the paper, I suggest that the authors:
1. Provide more analysis on the introspection network: The authors could provide more insight into how the introspection network works and why it is effective.
2. Investigate the robustness of the approach: The authors could investigate the robustness of their approach to different hyperparameters, network architectures, and datasets.
3. Explore the application of the approach to other tasks: The authors could explore the application of their approach to other tasks, such as natural language processing or speech recognition.
Overall, this is a well-written and well-executed paper that presents a novel and interesting approach to accelerating deep learning. With some additional analysis and experimentation, this approach has the potential to make a significant impact in the field. 
Some questions I would like the authors to answer are:
1. How do the authors plan to determine the optimal jump points for the introspection network?
2. Can the authors provide more insight into how the introspection network works and why it is effective?
3. How do the authors plan to extend their approach to other types of tasks, such as natural language processing or speech recognition?