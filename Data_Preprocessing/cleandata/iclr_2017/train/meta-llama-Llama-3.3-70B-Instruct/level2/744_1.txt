This paper proposes a novel layer augmentation technique that facilitates the optimization of deep networks by making identity mappings easy to learn. The authors introduce a scalar parameter to control each gate, allowing for easier optimization and increased performance. The technique is applied to both plain and residual layers, resulting in improved performance and robustness to layer removal.
I decide to accept this paper, with the main reasons being the well-motivated approach and the strong empirical results. The authors provide a clear and concise explanation of the problem and the proposed solution, and the experimental results demonstrate the effectiveness of the technique.
The paper supports its claims through a combination of theoretical analysis and empirical results. The authors provide a thorough discussion of the related work and the motivation behind the proposed technique, and the experimental results are well-presented and easy to follow. The results show that the proposed technique outperforms existing methods, including ResNets, and that it can be used for layer pruning, effectively removing large numbers of parameters from a network without harming its performance.
One potential limitation of the paper is the lack of a more detailed analysis of the trade-offs between the proposed technique and existing methods. For example, the authors could provide a more detailed comparison of the computational cost and memory requirements of the proposed technique versus existing methods.
To improve the paper, I suggest that the authors provide more insight into the interpretation of the k parameter and its relationship to the level of refinement performed by each layer. Additionally, the authors could explore the application of the proposed technique to other domains, such as natural language processing or speech recognition.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can the authors provide more details on the initialization of the k parameter and its impact on the optimization process?
* How does the proposed technique compare to other methods for layer pruning, such as sparse coding or dropout?
* Can the authors provide more insight into the relationship between the k parameter and the level of abstraction of the learned representations? 
Overall, the paper is well-written and provides a clear and concise explanation of the proposed technique and its benefits. The experimental results are strong, and the paper has the potential to make a significant contribution to the field of deep learning.