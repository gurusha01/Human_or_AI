This paper presents a thorough analysis of deep neural networks using sinusoidal activation functions, which have been largely ignored and regarded as difficult to train. The authors provide a formal characterization of the challenges in training these networks, particularly the presence of infinitely many shallow local minima in the loss landscape. They also investigate the conditions under which these networks can learn successfully, showing that the periodicity of the sinusoids is often ignored in favor of the monotonic segment around zero.
The paper makes several key contributions, including a detailed analysis of the loss surface, experiments on various networks and datasets, and a demonstration of the potential benefits of using sinusoidal activation functions on certain tasks. The authors also provide a comprehensive review of related work and acknowledge the limitations of their study.
Based on the content and quality of the paper, I decide to accept this paper. The main reasons for this decision are:
1. The paper tackles a specific and interesting question, providing a thorough analysis of the challenges and opportunities in using sinusoidal activation functions.
2. The authors provide a clear and well-motivated approach, including a detailed analysis of the loss surface and experiments on various networks and datasets.
3. The paper presents several key contributions, including a demonstration of the potential benefits of using sinusoidal activation functions on certain tasks.
To further improve the paper, I suggest the following:
* Provide more discussion on the implications of the results for practical applications, such as image and speech recognition.
* Consider adding more experiments on different datasets and tasks to further demonstrate the benefits of using sinusoidal activation functions.
* Provide more details on the initialization and training procedures used in the experiments, to facilitate reproducibility.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more intuition on why the periodicity of the sinusoids is often ignored in favor of the monotonic segment around zero?
* How do the results generalize to deeper networks and more complex tasks?
* Are there any potential applications of the sinusoidal activation functions in other areas, such as natural language processing or reinforcement learning?