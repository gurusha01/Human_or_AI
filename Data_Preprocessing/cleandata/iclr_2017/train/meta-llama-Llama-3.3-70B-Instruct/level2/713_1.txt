The paper proposes a novel activation function, Parametric Exponential Linear Unit (PELU), which learns a parameterization of the Exponential Linear Unit (ELU) function. The authors claim that PELU improves the performance of Convolutional Neural Networks (CNNs) by providing more control over bias shift and vanishing gradients. The paper presents several experiments on CIFAR-10/100 and ImageNet datasets, demonstrating that networks trained with PELU consistently outperform those trained with ELU.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-defined problem in the field of deep learning, namely improving the performance of CNNs by learning a parameterization of the ELU activation function.
2. The approach is well-motivated and grounded in the literature, with a clear explanation of the limitations of existing activation functions and the potential benefits of the proposed PELU.
The paper provides strong support for its claims, including:
* A thorough analysis of the PELU function and its properties, including its ability to reduce bias shift and vanishing gradients.
* Extensive experimental results on multiple datasets and network architectures, demonstrating the effectiveness of PELU in improving network performance.
* A comparison with other activation functions, including ReLU and ELU, which shows that PELU achieves better results in most cases.
Additional feedback to improve the paper includes:
* Providing more insight into the optimization process of PELU, including the choice of hyperparameters and the effect of different learning rates and weight decay values.
* Investigating the application of PELU to other network architectures, such as recurrent neural networks, and to other tasks, such as object detection.
* Comparing PELU with other parametric activation functions, such as Parametric ReLU (PReLU) and Adaptive Piecewise Linear (APL) unit, to better understand its strengths and weaknesses.
Questions to the authors:
* Can you provide more details on the choice of hyperparameters for the experiments, including the learning rate, weight decay, and batch size?
* How do you think PELU could be applied to other network architectures and tasks, and what potential benefits or challenges do you foresee?
* Have you considered comparing PELU with other parametric activation functions, and if so, what were the results?