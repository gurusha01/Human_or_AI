This paper presents two novel Recurrent Neural Network (RNN) based architectures for extractive summarization of documents, namely the Classifier and Selector architectures. The authors propose a scoring function that captures the notions of salience, redundancy, and positional importance of sentences, allowing for interpretable models. The paper evaluates the performance of these models on two datasets, Daily Mail and DUC 2002, and compares them to state-of-the-art models.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of natural language processing, and the approach is well-motivated and grounded in the literature. Secondly, the paper provides a thorough evaluation of the proposed models, including a comparison to state-of-the-art models and an analysis of the results.
The paper supports its claims through a series of experiments, including a comparison of the Classifier and Selector architectures, an evaluation of the impact of document structure on the performance of the models, and an analysis of the learned importance weights of the abstract features. The results show that the Classifier architecture outperforms the Selector architecture on the Daily Mail corpus, but the Selector architecture performs better when the document structure is destroyed. The paper also provides a qualitative analysis of the model's predictions, including a visualization of the system's output and a display of the learned importance weights.
To improve the paper, I suggest that the authors provide more details on the experimental settings, such as the hyperparameter tuning process and the evaluation metrics used. Additionally, the authors could provide more analysis on the results, including a discussion of the limitations of the models and potential avenues for future work.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How did the authors tune the hyperparameters of the models, and what was the effect of different hyperparameter settings on the performance of the models? How do the authors plan to address the issue of domain adaptation, given that the models perform well on the Daily Mail corpus but poorly on the DUC 2002 corpus? What are the potential applications of the proposed models, and how do they compare to other state-of-the-art models in terms of interpretability and performance?