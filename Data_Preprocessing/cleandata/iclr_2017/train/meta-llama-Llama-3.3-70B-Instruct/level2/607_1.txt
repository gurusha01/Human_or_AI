This paper proposes a novel memory-based attention model for video description, which utilizes memories of past attention when reasoning about where to attend to in the current time step. The model is designed to capture the temporal structure of the video and learn to attend to the important parts of the video. The authors evaluate their model on the MSVD and Charades datasets and achieve state-of-the-art results.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and well-defined problem in the field of video captioning, which is a challenging task due to the complexity of modeling temporal relationships between video frames and captions.
2. The approach is well-motivated and grounded in the literature, with a clear explanation of the limitations of existing attention mechanisms and the benefits of the proposed memory-based attention model.
The paper provides a thorough evaluation of the proposed model, including ablation studies and comparisons to state-of-the-art models. The results demonstrate the effectiveness of the memory-based attention model in improving the performance of video captioning. The paper also provides qualitative examples of generated captions, which illustrate the model's ability to capture complex temporal relationships between video frames and captions.
To further improve the paper, I suggest the authors provide more details on the hyperparameter optimization process and the sensitivity of the model to different hyperparameter settings. Additionally, it would be helpful to include more analysis on the types of errors made by the model and how they relate to the limitations of the proposed approach.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on the implementation of the memory-based attention model, including the specific architecture and hyperparameter settings used?
* How do you handle cases where the video has a large number of frames, and the model needs to attend to a subset of frames to generate a caption?
* Can you provide more analysis on the types of errors made by the model, and how they relate to the limitations of the proposed approach?