The paper proposes a novel method for regularizing Recurrent Neural Networks (RNNs) called zoneout, which stochastically preserves hidden units' activations. The authors claim that zoneout improves performance across tasks, outperforming many alternative regularizers and achieving results competitive with state of the art on several datasets.
Based on the provided information, I decide to accept this paper. The main reasons for this decision are: 
1. The approach is well-motivated and placed in the literature, with a clear explanation of how zoneout differs from existing regularization methods such as dropout.
2. The paper provides extensive empirical evidence to support the claims, including experiments on multiple datasets and tasks, and comparisons with other regularization methods.
The supporting arguments for this decision include the fact that the authors have performed a thorough evaluation of zoneout on various tasks, including character-level and word-level language modeling, and classification on permuted sequential MNIST. The results show that zoneout consistently outperforms other regularization methods, including dropout and recurrent dropout. Additionally, the authors have provided a clear explanation of the benefits of zoneout, including the introduction of stochasticity and the improvement of gradient flow.
To further improve the paper, I suggest that the authors provide more analysis on the effect of zoneout on different types of RNNs, such as LSTMs and GRUs. Additionally, it would be interesting to see more experiments on the combination of zoneout with other regularization methods, such as dropout and recurrent batch normalization.
Some questions I would like the authors to answer include: 
* How does zoneout affect the interpretability of the RNN models?
* Can zoneout be applied to other types of neural networks, such as feedforward networks?
* How does the choice of zoneout probability affect the performance of the model?
Overall, the paper provides a significant contribution to the field of RNN regularization, and the results have the potential to be useful in a variety of applications. With some additional analysis and experimentation, the paper could be even stronger.