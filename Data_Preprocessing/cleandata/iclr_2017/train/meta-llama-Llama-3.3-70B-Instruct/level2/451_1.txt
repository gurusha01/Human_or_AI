This paper presents a comprehensive study on the loss surface of deep neural networks, focusing on the topological and geometrical aspects of the optimization landscape. The authors provide new theoretical results that quantify the amount of uphill climbing required to progress to lower energy configurations in single hidden-layer ReLU networks and prove that this amount converges to zero with overparametrization under mild conditions.
The paper's main claims are: (1) the loss surface of deep linear networks has a radically different topology from that of deep half-rectified ones, and (2) the energy landscape in the non-linear case is fundamentally controlled by the interplay between the smoothness of the data distribution and model over-parametrization. The authors support these claims through a combination of theoretical analysis and empirical experiments.
The theoretical contributions of the paper are significant, particularly the proof that half-rectified single-layer networks are asymptotically connected. The authors also introduce a dynamic programming algorithm, Dynamic String Sampling, to efficiently approximate geodesics within each level set, providing a tool to verify the connectedness of level sets and estimate the geometric regularity of these sets.
The empirical results show that the level sets of various neural network models, including CNNs and LSTMs, remain connected throughout the learning phase, suggesting a near-convex behavior. However, the level sets become exponentially more curvy as the energy level decays, in accordance with what is observed in practice with very low curvature attractors.
I decide to accept this paper because it presents a significant improvement over existing approaches to understanding the loss surface of deep neural networks. The paper's theoretical and empirical contributions are well-motivated, and the results are scientifically rigorous. The authors provide a clear and well-structured presentation of their work, making it easy to follow and understand.
To improve the paper, I suggest that the authors provide more details on the implementation of the Dynamic String Sampling algorithm and its computational complexity. Additionally, the authors could discuss the potential limitations of their approach and possible extensions to other types of neural network models.
Some questions I would like the authors to answer are: (1) How do the results of the paper relate to existing work on the optimization of deep neural networks, such as the use of stochastic gradient descent and its variants? (2) Can the authors provide more insight into the role of over-parametrization in the connectedness of the level sets? (3) How do the results of the paper generalize to other types of neural network models, such as recurrent neural networks and generative adversarial networks?