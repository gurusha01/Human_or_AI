The paper "NewsQA: A Challenging Machine Comprehension Dataset" presents a new large-scale dataset for machine comprehension, consisting of over 100,000 question-answer pairs based on news articles from CNN. The authors claim that their dataset, NewsQA, is more challenging than existing datasets, requiring reasoning mechanisms such as synthesis of information across different parts of an article.
I decide to accept this paper, with the key reason being that it presents a well-motivated and well-designed dataset that addresses the limitations of existing machine comprehension datasets. The authors provide a thorough analysis of the dataset, demonstrating its challenge and usefulness as a machine comprehension benchmark.
The paper supports its claims through a variety of statistics and experiments, including a comparison with human performance and several strong neural models. The results show a significant performance gap between humans and machines, indicating that NewsQA is a challenging dataset that requires further research.
Additional feedback to improve the paper includes providing more details on the crowdworkers' instructions and the validation process, as well as exploring the use of other evaluation metrics beyond F1 score. It would also be interesting to see a more in-depth analysis of the types of questions that are most challenging for the models, and how the dataset can be used to improve the performance of machine comprehension models.
Some questions I would like the authors to answer include: How did the authors ensure the quality of the crowdworkers' questions and answers? What are the potential biases in the dataset, and how can they be addressed? How can the dataset be used to improve the performance of machine comprehension models, and what are the potential applications of the dataset?
Overall, the paper presents a significant contribution to the field of machine comprehension, and the dataset has the potential to spur further advances in the development of literate artificial intelligence.