This paper proposes a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM), which achieves a significant improvement in memory access complexity, reducing it from Θ(n) to Θ(log n). The authors demonstrate the effectiveness of HAM by augmenting an LSTM network with it and training it to learn algorithms for various tasks such as merging, sorting, and binary searching. The results show that the LSTM+HAM model outperforms strong baseline models, including encoder-decoder LSTM and LSTM with attention, and generalizes well to longer input sequences.
I decide to accept this paper for the following reasons:
1. The paper tackles a specific and well-defined problem in the field of neural networks, namely, improving the efficiency of memory access in neural networks.
2. The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing memory architectures and how HAM addresses these limitations.
3. The paper provides strong empirical evidence to support the claims, including experiments on various tasks and comparisons with baseline models.
The supporting arguments for my decision are:
* The paper presents a novel and innovative solution to the problem of memory access in neural networks, which has the potential to improve the performance of neural networks on tasks that require large memory.
* The authors provide a thorough analysis of the related work and demonstrate a good understanding of the existing literature in the field.
* The experimental results are convincing, and the authors provide a detailed description of the experimental setup and the results.
Additional feedback to improve the paper:
* It would be helpful to provide more insights into the algorithms learned by the LSTM+HAM model, such as the example provided in Appendix A.
* The authors could consider providing more analysis on the computational complexity of the HAM module and its implications for large-scale neural networks.
* It would be interesting to see how the HAM module performs on more complex tasks, such as natural language processing or computer vision tasks.
Questions to the authors:
* Can you provide more details on how the HAM module is initialized and updated during training?
* How do you plan to extend the HAM module to more complex tasks, such as natural language processing or computer vision tasks?
* Can you provide more analysis on the trade-offs between the HAM module and other memory architectures, such as NTM or DMN?