This paper proposes a novel technique for collective operations, referred to as Linear Pipelining (LP), to reduce the cost of communication in parallel training of neural networks. The authors claim that LP provides an O(log p) speedup over Minimal Spanning Tree (MST) collectives and up to a 2 times speedup over Bidirectional Exchange (BE) collectives, where p is the number of GPUs. The paper also demonstrates the effectiveness of LP in training large-scale neural networks, achieving up to 2.19x speedup over BE-based algorithms.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and important problem in parallel training of neural networks, which is the reduction of communication overhead.
2. The approach is well-motivated and placed in the literature, with a clear understanding of the limitations of existing collective algorithms such as MST and BE.
The supporting arguments for the decision are:
* The paper provides a thorough theoretical analysis of the proposed LP collectives, demonstrating their superiority over MST and BE collectives in terms of communication cost.
* The experimental results demonstrate the effectiveness of LP collectives in reducing the total training time of large-scale neural networks, without affecting the convergence properties of SGD.
* The paper provides a comprehensive evaluation of the proposed LP collectives, including scalability experiments and comparisons with existing collective algorithms.
Additional feedback to improve the paper includes:
* Providing more details on the implementation of the LP collectives, such as the block size and the overlap of communications and computations.
* Discussing the potential limitations of the LP collectives, such as the assumption of dense, long, and fixed-length messages, and the potential impact on other types of neural network training.
* Considering the extension of the LP collectives to other types of parallel training, such as distributed training or asynchronous training.
Questions to the authors include:
* Can you provide more details on the block size and the overlap of communications and computations in the LP collectives?
* How do you plan to address the potential limitations of the LP collectives, such as the assumption of dense, long, and fixed-length messages?
* Have you considered extending the LP collectives to other types of parallel training, such as distributed training or asynchronous training?