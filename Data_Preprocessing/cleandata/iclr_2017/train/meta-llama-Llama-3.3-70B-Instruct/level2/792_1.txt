Summary
The paper introduces a new regularization technique called SoftTarget regularization, which aims to reduce over-fitting in deep neural networks without sacrificing their capacity. The method uses a weighted average of the real labels and an exponential average of past soft-targets to guide the learning process. The authors demonstrate the effectiveness of SoftTarget regularization on various neural network architectures and datasets, including MNIST, CIFAR-10, and SVHN.
Decision
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by experimental results. The authors provide a clear explanation of the method and its advantages over existing regularization techniques.
Supporting Arguments
The paper tackles a specific question/problem, which is the issue of over-fitting in deep neural networks. The approach is well-placed in the literature, and the authors provide a thorough review of existing regularization techniques. The experimental results demonstrate the effectiveness of SoftTarget regularization, and the authors provide a detailed analysis of the results. The method is also shown to be applicable to various neural network architectures and datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the hyper-parameter tuning process for SoftTarget regularization. The authors mention that the hyper-parameters were tuned experimentally, but it would be helpful to provide more details on the tuning process and the sensitivity of the results to the hyper-parameters. Additionally, the authors could provide more discussion on the relationship between SoftTarget regularization and other regularization techniques, such as Dropout and Batch Normalization.
Questions for the Authors
I would like the authors to clarify the following points:
1. How did the authors choose the hyper-parameters for SoftTarget regularization, and what was the sensitivity of the results to these hyper-parameters?
2. Can the authors provide more insights into the relationship between SoftTarget regularization and other regularization techniques, such as Dropout and Batch Normalization?
3. How do the authors plan to address the issue of reducing the number of hyper-parameters introduced by SoftTarget regularization, as mentioned in the future work section?