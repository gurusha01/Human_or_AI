The paper proposes a novel reparametrization of the Long Short-Term Memory (LSTM) network, called Normalized LSTM, which preserves the means and variances of the hidden states and memory cells across time. This approach is motivated by the need to address the vanishing and exploding gradient problems in recurrent neural networks. The authors claim that their method achieves similar or better performance than other recurrent normalization approaches, such as Recurrent Batch Normalization and Layer Normalization, while requiring fewer computations.
I decide to accept this paper, with the main reason being that the approach is well-motivated and the experimental results demonstrate its effectiveness. The paper provides a clear and detailed explanation of the proposed method, including the mathematical derivations and the initialization scheme for the weight matrices. The experiments on character-level language modeling and image generative modeling tasks show that the Normalized LSTM performs similarly or better than other state-of-the-art approaches, while being computationally faster.
The supporting arguments for this decision include the fact that the paper provides a thorough analysis of the gradient propagation in the Normalized LSTM, which is essential for understanding the behavior of the network. The authors also provide a clear comparison with other normalization techniques, such as Batch Normalization and Layer Normalization, and demonstrate the advantages of their approach. Additionally, the paper provides a detailed description of the experimental setup and the results, which makes it easy to reproduce and verify the findings.
To further improve the paper, I would suggest that the authors provide more insights into the theoretical aspects of the Normalized LSTM, such as the convergence properties and the stability of the network. Additionally, it would be interesting to see more experiments on other tasks and datasets, to further demonstrate the effectiveness and robustness of the proposed approach. Some questions that I would like the authors to answer include: How does the Normalized LSTM perform on tasks with longer sequences or more complex dependencies? Can the authors provide more insights into the initialization scheme for the weight matrices and its impact on the training process? How does the Normalized LSTM compare to other state-of-the-art approaches, such as gated recurrent units (GRUs) or other variants of LSTMs?