This paper presents a novel approach called layerwise origin-target synthesis (LOTS) that can be used for multiple purposes, including visualizing the internal representation of an input at any layer of a deep neural network (DNN), assessing the stability of the captured internal feature representations, and generating a large number of diverse adversarial examples for each input. The authors demonstrate the effectiveness of LOTS on two well-known DNNs, LeNet and VGG Face, and compare it with other adversarial generation techniques.
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach that addresses an important problem in the field of deep learning, namely the vulnerability of DNNs to adversarial examples. The paper is well-written, and the authors provide a clear and concise explanation of the LOTS algorithm and its applications.
The approach is well-supported by experiments, which demonstrate the effectiveness of LOTS in visualizing internal representations, assessing stability, and generating adversarial examples. The comparison with other adversarial generation techniques is also thorough and well-done. The paper reflects a good understanding of the field and the relevant literature, and the authors provide a clear and concise explanation of the contributions and limitations of their work.
One potential limitation of the paper is that the computational cost of the LOTS algorithm is not thoroughly discussed. The authors mention that the algorithm can be modified to perform better, but this would come at a higher computational cost. It would be helpful to have a more detailed analysis of the computational cost and how it compares to other adversarial generation techniques.
To improve the paper, I would suggest that the authors provide more details on the implementation of the LOTS algorithm, including the specific architectures and hyperparameters used in the experiments. Additionally, it would be helpful to have more visualizations of the internal representations and adversarial examples generated by LOTS, to provide a better understanding of the algorithm's capabilities.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the choice of target image affect the quality of the adversarial examples generated by LOTS? Can the LOTS algorithm be used to generate adversarial examples for other types of inputs, such as speech or text? How does the computational cost of LOTS compare to other adversarial generation techniques, and are there any potential optimizations that could be made to reduce the cost?