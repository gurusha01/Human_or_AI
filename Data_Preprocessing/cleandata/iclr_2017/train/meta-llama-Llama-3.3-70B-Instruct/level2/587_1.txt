This paper proposes a novel acceleration framework, DeepRebirth, to speed up the deployment of deep neural networks on mobile devices. The authors claim that their approach can achieve significant speed-up and memory saving while maintaining model accuracy. The main contributions of this paper are: (1) optimizing non-tensor layers to accelerate deep learning models on CPUs, (2) proposing a generic framework that can be applied to different deep learning architectures, and (3) demonstrating state-of-the-art speeding up on popular deep learning models with negligible accuracy loss.
I decide to accept this paper with the following reasons: (1) the approach is well-motivated and addresses a significant problem in deploying deep learning models on mobile devices, and (2) the experimental results demonstrate the effectiveness of the proposed framework in achieving significant speed-up and memory saving while maintaining model accuracy.
The supporting arguments for the decision are: (1) the paper provides a clear and concise introduction to the problem and the proposed solution, (2) the experimental results are comprehensive and demonstrate the effectiveness of the proposed framework on different deep learning models and processors, and (3) the paper provides a thorough analysis of the related work and demonstrates the advantages of the proposed framework over existing approaches.
To further improve the paper, I provide the following feedback: (1) the paper could benefit from a more detailed analysis of the computational complexity of the proposed framework, (2) the authors could provide more insights into the trade-off between speed-up and accuracy loss, and (3) the paper could be improved by providing more visualizations and illustrations of the proposed framework and its applications.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) how does the proposed framework handle the case where the non-tensor layers are not adjacent to each other, (2) can the authors provide more details on the fine-tuning process of the merged layers, and (3) how does the proposed framework compare to other acceleration frameworks in terms of computational complexity and memory usage?