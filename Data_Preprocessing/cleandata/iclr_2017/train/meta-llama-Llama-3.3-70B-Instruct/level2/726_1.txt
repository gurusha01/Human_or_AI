This paper introduces Information Dropout, a novel dropout method that generalizes existing dropout techniques by incorporating the Information Bottleneck principle. The authors claim that their approach can learn optimal representations of the data that are invariant to nuisances, leading to improved generalization performance. 
I decide to accept this paper, primarily because it tackles a specific question/problem - how to prevent overfitting to nuisances in neural networks - and proposes a well-motivated approach. The paper is well-placed in the literature, drawing connections to various existing dropout methods and the Information Bottleneck principle. 
The authors provide a clear and thorough explanation of their method, including a detailed derivation of the Information Dropout cost function and its relationship to existing dropout techniques. The experimental results demonstrate the effectiveness of Information Dropout in improving generalization performance, particularly on smaller models. 
To further improve the paper, I suggest that the authors provide more insights into the choice of hyperparameters, such as the value of Î², and how it affects the performance of Information Dropout. Additionally, it would be interesting to see more comparisons with other regularization techniques, such as weight decay or early stopping, to better understand the strengths and weaknesses of Information Dropout.
Some questions I would like the authors to answer include: (1) How does the choice of noise distribution affect the performance of Information Dropout? (2) Can the authors provide more intuition on why Information Dropout is able to adapt the noise to the structure of the network and the individual sample at test time? (3) Are there any plans to explore the application of Information Dropout to other domains, such as natural language processing or reinforcement learning? 
Overall, the paper presents a well-motivated and well-executed approach to preventing overfitting to nuisances in neural networks, and I believe it has the potential to make a significant contribution to the field.