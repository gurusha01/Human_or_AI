This paper introduces an innovative architecture for an open-vocabulary neural language model, which allows the model to consider any word, whether seen during training or not, by computing word representations on-the-fly using a convolutional network followed by a pooling layer. The authors tackle the challenge of designing such a model by deriving the training objective from Noise Contrastive Estimation (NCE), enabling the model to handle a potentially infinite vocabulary.
I decide to accept this paper, with the main reason being that it presents a well-motivated approach to addressing the limitation of traditional word-based language models, which rely on a finite vocabulary. The authors provide a clear and detailed description of their model architecture and training objective, and the experimental results show promising improvements in BLEU score, especially when using the CWE model.
The paper supports its claims through a series of experiments on the machine translation task of IWSLT2016 from English to Czech, demonstrating the effectiveness of the proposed open-vocabulary models in re-ranking outputs of translation systems. The authors also provide a thorough analysis of the limitations and challenges of their approach, including the issue of "contamination" of character n-gram representations by frequent short words.
To further improve the paper, I suggest that the authors provide more detailed explanations of the hyperparameter settings and the optimization process, as well as more extensive analysis of the learned representations and their implications for language modeling. Additionally, it would be interesting to see more comparisons with other state-of-the-art language models and a more thorough discussion of the potential applications of the proposed approach beyond machine translation.
Some specific questions I would like the authors to address include: (1) How do the authors plan to address the issue of "contamination" of character n-gram representations, and what are the potential consequences of this issue for the performance of the model? (2) Can the authors provide more insights into the learned representations and how they relate to the linguistic properties of the words, such as morphology and syntax? (3) How do the authors plan to extend their approach to other languages and tasks, and what are the potential challenges and limitations of doing so?