This paper presents a novel approach to deep learning by implementing a matrix library and deep learning framework in JavaScript, allowing it to run on web browsers and ordinary personal computers without the need for specialized hardware or software installation. The authors claim to have achieved significant performance gains using GPGPU via WebCL, enabling the training of large-scale convolutional neural networks such as VGGNet and ResNet.
I decide to accept this paper, with the primary reason being the novelty and potential impact of the proposed approach. The authors have demonstrated the feasibility of training large-scale CNNs using web browsers as computing clients, which could democratize access to deep learning capabilities.
The paper provides a clear and well-motivated approach, with a thorough review of related work and a detailed description of the matrix library and deep learning framework implementation. The experimental results demonstrate the effectiveness of the proposed approach, with significant speedups achieved through distributed training using web browsers.
However, there are some areas that require further improvement. The authors acknowledge the limitations of their approach, including the slower matrix multiplication necessary for convolution and the lack of inclusion of WebCL in web browsers. To address these limitations, the authors could explore alternative GPGPU interfaces, such as WebGL or asm.js, and investigate strategies to optimize matrix multiplication for convolution.
Additional feedback includes the suggestion to provide more detailed analysis of the communication overhead and its impact on the distributed training performance. The authors could also explore more advanced parallelization methods to further improve the scalability of their approach.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the limitation of WebCL not being included in web browsers, and what alternative GPGPU interfaces are being considered?
2. What strategies are being explored to optimize matrix multiplication for convolution, and how do the authors expect to improve the performance in this area?
3. Can the authors provide more detailed analysis of the communication overhead and its impact on the distributed training performance, and how do they plan to mitigate this overhead in future work?