The paper proposes an extension of Pixel Convolutional Neural Networks (PixelCNN) for text-to-image synthesis with controllable object locations. The model conditions on text and spatial structure, such as part keypoints and segmentation masks, to generate images. The authors demonstrate the effectiveness of their approach on three datasets: Caltech-UCSD Birds (CUB), MPII Human Pose (MHP), and Common Objects in Context (MS-COCO).
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the field of image synthesis, and the approach is well-motivated and placed in the literature. Secondly, the paper provides a thorough evaluation of the proposed model, including quantitative results and comparisons to previous works.
The paper's contributions are well-supported by the experiments, which demonstrate the ability of the model to generate high-quality images that respect the given text and spatial constraints. The use of segmentation masks and keypoints as conditioning variables is a novel and effective approach, allowing for more control and interpretability over the generated images. The quantitative results, including the negative log-likelihood of image pixels conditioned on text and structure, provide a principled and meaningful benchmark for evaluating the model's performance.
To further improve the paper, I suggest that the authors provide more details on the training process, including the hyperparameter settings and the optimization algorithm used. Additionally, it would be interesting to see more examples of the generated images, particularly for the MHP dataset, which is the most challenging of the three datasets. The authors may also consider providing more analysis on the limitations of the model, such as the inability to perfectly disentangle location and appearance, and potential directions for future work to address these limitations.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to address the issue of keypoints being predictive of the category of bird, which can limit the model's ability to disentangle location and appearance? Can the authors provide more details on the character-level text encoder and how it is trained end-to-end with the image generation network? How do the authors plan to extend the model to generate higher-resolution images, and what are the potential challenges and limitations of doing so?