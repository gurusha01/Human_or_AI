The paper proposes a novel neural network architecture called Layer-RNN (L-RNN) that combines traditional convolutional layers with recurrent neural network (RNN) modules to learn contextual information adaptively. The authors claim that their approach can learn long-range dependencies at multiple levels, and can be seamlessly inserted into pre-trained convolutional neural networks (CNNs) to boost performance.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, with a clear explanation of how it differs from existing methods. Secondly, the experimental results demonstrate the effectiveness of the L-RNN module in improving performance on both image classification and semantic segmentation tasks.
The paper provides a thorough evaluation of the L-RNN module, including ablation studies and comparisons to state-of-the-art methods. The results show that the L-RNN module can be used to improve performance on both shallow and deep networks, and can be used to learn contextual information at multiple levels. The authors also provide a detailed analysis of the results, including discussions of the benefits and limitations of the approach.
To improve the paper, I suggest that the authors provide more details on the training process, including the hyperparameters used and the computational resources required. Additionally, it would be helpful to include more visualizations of the results, such as examples of the semantic segmentation outputs.
I have several questions that I would like the authors to answer to clarify my understanding of the paper. Firstly, how do the authors choose the number of RNN modules to use in the L-RNN architecture? Secondly, how do the authors initialize the recurrence matrix V in the RNN modules? Thirdly, can the authors provide more details on the fine-tuning process, including the learning rate schedule and the number of epochs used?
Overall, the paper presents a novel and effective approach to learning contextual information in neural networks, and demonstrates its potential for improving performance on a range of computer vision tasks. With some additional details and clarifications, the paper has the potential to make a significant contribution to the field.