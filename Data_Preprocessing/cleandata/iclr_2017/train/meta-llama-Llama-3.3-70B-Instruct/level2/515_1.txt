This paper introduces Exponential Machines (ExM), a novel predictor that models all interactions of every order between features. The authors propose representing the exponentially large tensor of parameters in a compact multilinear format called Tensor Train (TT-format), which regularizes the model and allows for efficient learning and inference. The paper also develops a stochastic Riemannian optimization procedure to train the model, which outperforms the stochastic gradient descent baseline.
The main claims of the paper are: (1) ExM can model all interactions of every order between features, (2) the TT-format representation of the tensor of parameters regularizes the model and allows for efficient learning and inference, and (3) the stochastic Riemannian optimization procedure outperforms the stochastic gradient descent baseline.
The support for these claims is provided through a series of experiments on various datasets, including synthetic data, UCI datasets, and the MovieLens 100K dataset. The results show that ExM achieves state-of-the-art performance on synthetic data with high-order interactions and is competitive with other methods on the other datasets.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed method and its advantages. The experimental evaluation is thorough, and the results are convincing. The paper also provides a detailed discussion of related work and highlights the differences between ExM and other methods.
My decision is to accept this paper. The main reasons for this decision are: (1) the paper proposes a novel and interesting method for modeling interactions between features, (2) the experimental evaluation is thorough and convincing, and (3) the paper provides a clear and concise explanation of the proposed method and its advantages.
Some additional feedback to improve the paper: (1) the authors may want to consider providing more details on the computational complexity of the stochastic Riemannian optimization procedure, (2) the paper could benefit from a more detailed discussion of the limitations of the proposed method, and (3) the authors may want to consider providing more examples of how the proposed method can be applied to real-world problems.
Questions to the authors: (1) Can you provide more details on how the TT-rank is chosen in practice? (2) How does the proposed method handle missing or noisy data? (3) Can you provide more examples of how the proposed method can be applied to real-world problems, such as recommender systems or natural language processing?