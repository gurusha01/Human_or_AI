This paper presents a novel approach to reinforcement learning (RL) by extending recent work on video frame prediction to enable joint prediction of future states and rewards. The authors propose a network architecture and training procedure for joint state and reward prediction, which is evaluated on five Atari games. The results demonstrate accurate cumulative reward prediction up to roughly 200 frames, with best results achieved in Freeway and Q*bert.
I decide to accept this paper, with the main reason being that it presents a significant step towards data-efficient RL in high-dimensional environments without prior knowledge. The approach is well-motivated, and the results are impressive, especially considering the complexity of the environments.
The paper is well-structured, and the authors provide a clear explanation of their approach, including the network architecture and training procedure. The evaluation is thorough, with both quantitative and qualitative analyses of the results. The authors also provide a detailed error analysis, which helps to identify potential limitations of the approach.
One potential limitation of the approach is the assumption of ternary rewards, which may not be applicable to all environments. Additionally, the authors note that the model underestimates cumulative reward, which could be improved with further modifications to the optimization objective or the network architecture.
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process, including the selection of the reward weight λ and the gradient clipping threshold. Additionally, it would be interesting to see a comparison with other state-of-the-art RL approaches, such as model-free methods or other model-based approaches.
Some questions I would like the authors to answer include:
* How did the authors select the value of λ, and what is the sensitivity of the results to this hyperparameter?
* Can the authors provide more details on the gradient clipping technique used, and how it affects the optimization process?
* How do the authors plan to extend this approach to more complex environments, such as those with non-deterministic state transitions or high-dimensional action spaces?
Overall, this paper presents a significant contribution to the field of RL, and I believe it has the potential to inspire further research in this area. With some minor revisions to address the limitations and provide more details on the methodology, this paper is ready for publication.