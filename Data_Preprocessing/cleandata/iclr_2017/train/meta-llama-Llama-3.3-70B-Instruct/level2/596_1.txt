The paper proposes Implicit ReasoNets (IRNs), a novel neural network architecture designed to perform large-scale inference implicitly through a search controller and shared memory. The main claim of the paper is that IRNs can outperform previous approaches on the knowledge base completion task, specifically on the FB15k benchmark, by more than 5.7%. The authors also demonstrate the effectiveness of IRNs on a shortest path synthesis task.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and well-defined problem in the field of knowledge base completion, which is a crucial task in many downstream applications such as question answering and information extraction.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of previous approaches and how IRNs address these limitations.
The supporting arguments for the decision are as follows:
* The paper provides a thorough analysis of the behavior of IRNs, including the effect of different memory sizes and maximum inference steps on the performance of the model.
* The experimental results demonstrate the effectiveness of IRNs on both the FB15k and WN18 datasets, with significant improvements over previous approaches.
* The paper also provides a detailed analysis of the inference process of IRNs, including the ability to perform multi-step inference and the use of attention mechanisms to access relevant information in the shared memory.
Additional feedback to improve the paper includes:
* Providing more details on the construction of the graph for the shortest path synthesis task, such as the specific parameters used and the rationale behind the design choices.
* Discussing potential limitations of the IRN architecture, such as the reliance on a shared memory and the potential for overfitting to the training data.
* Exploring the application of IRNs to other tasks that require modeling structured relationships between instances, such as natural language processing and computer vision.
Questions to be answered by the authors include:
* Can the authors provide more insight into the learned representations in the shared memory, such as the types of patterns or structures that are captured by the memory vectors?
* How do the authors plan to address the potential limitations of the IRN architecture, such as the reliance on a shared memory and the potential for overfitting to the training data?
* Can the authors discuss the potential applications of IRNs to other domains, such as natural language processing and computer vision, and how the architecture can be adapted to these domains?