This paper presents a comprehensive study on adversarial training for large models on the ImageNet dataset. The authors successfully scale adversarial training to ImageNet and demonstrate its effectiveness in increasing robustness to adversarial examples. The paper makes several key contributions, including recommendations for scaling adversarial training, observations on the transferability of adversarial examples, and the discovery of a "label leaking" effect.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper presents a significant improvement over existing approaches to adversarial training, demonstrating its effectiveness on a large-scale dataset like ImageNet. Secondly, the paper provides a thorough analysis of the results, including an examination of the transferability of adversarial examples and the discovery of the label leaking effect.
The paper's contributions are well-supported by extensive experiments, and the authors provide a clear and detailed explanation of their methods and results. The paper also raises important questions about the security of machine learning models and the potential for adversarial attacks. The authors' recommendations for scaling adversarial training and their observations on the transferability of adversarial examples are particularly noteworthy.
To improve the paper, I suggest that the authors provide more discussion on the potential limitations of their approach and the potential for future work. For example, the authors could explore the use of other adversarial example generation methods or examine the effectiveness of their approach on other datasets. Additionally, the authors could provide more analysis on the label leaking effect and its implications for the design of adversarial example generation methods.
I would like the authors to answer the following questions to clarify my understanding of the paper: 
1. Can you provide more details on the computational resources required for adversarial training on ImageNet, and how this compares to traditional training methods?
2. How do you think the label leaking effect could be mitigated in practice, and what implications does this have for the design of adversarial example generation methods?
3. Have you explored the use of other adversarial example generation methods, such as iterative methods, and if so, what were the results? 
Overall, this is a well-written and comprehensive paper that makes significant contributions to the field of adversarial training. With some additional discussion and analysis, it has the potential to be an excellent paper.