This paper introduces Attentive Recurrent Comparators (ARCs), a novel class of neural networks that learn to estimate the similarity of a set of objects by cycling through them and making observations. The authors claim that ARCs outperform traditional Siamese neural networks and achieve state-of-the-art performance on the Omniglot dataset for one-shot classification.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-defined problem, namely, estimating the similarity of objects, and proposes a novel solution that is well-motivated and grounded in the literature.
2. The approach is well-supported by empirical results, including experiments on various visual tasks and a thorough analysis of the model's performance on the Omniglot dataset.
The supporting arguments for these reasons are as follows:
The paper provides a clear and concise introduction to the problem of similarity estimation and the limitations of traditional approaches. The authors then propose ARCs as a solution, which is based on the idea of cycling through objects and making observations conditioned on previous context. The model is well-described, and the authors provide a thorough analysis of its performance on various tasks.
The results presented in the paper are impressive, with ARCs achieving state-of-the-art performance on the Omniglot dataset for one-shot classification. The authors also provide a detailed analysis of the model's behavior, including qualitative and quantitative studies, which helps to understand how the model works and what factors affect its performance.
Additional feedback to improve the paper includes:
- Providing more details on the computational cost of ARCs compared to traditional Siamese neural networks.
- Exploring the application of ARCs to other modalities, such as natural language processing or speech recognition.
- Investigating the use of more advanced attention mechanisms or recurrent neural network architectures to further improve the performance of ARCs.
Questions to the authors:
- Can you provide more details on the hyperparameter tuning process for ARCs, and how the authors selected the optimal hyperparameters for each experiment?
- How do the authors plan to address the potential computational cost of ARCs, and what strategies can be employed to reduce the computational overhead?
- Are there any plans to apply ARCs to other tasks or datasets, and what are the potential challenges and opportunities in doing so?