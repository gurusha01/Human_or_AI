This paper presents a novel approach to sequence-to-sequence transduction, introducing a hard attention mechanism that combines traditional statistical alignment methods with the power of recurrent neural networks. The authors claim that their model achieves state-of-the-art results on the task of morphological inflection generation, outperforming previous neural and non-neural approaches.
I decide to accept this paper, with the main reason being the significant improvement in performance on the CELEX dataset, where the model surpasses the previous state-of-the-art results. Additionally, the paper provides a thorough analysis of the learned alignments and representations, shedding light on the features that the model extracts to solve the task.
The approach is well-motivated, and the authors provide a clear explanation of the limitations of previous soft attention models and the benefits of their hard attention mechanism. The experimental results are extensive and well-presented, with a thorough comparison to previous state-of-the-art models. The analysis of the learned alignments and representations provides valuable insights into the model's behavior and strengths.
To further improve the paper, I suggest that the authors provide more details on the training procedure, including the hyperparameter settings and the optimization algorithm used. Additionally, it would be interesting to see more analysis on the robustness of the model to different types of noise or errors in the input data.
Some questions I would like the authors to answer include: How do the authors plan to extend their model to other tasks that require a monotonic align-and-transduce approach? What are the potential limitations of the hard attention mechanism, and how can they be addressed? How does the model perform on datasets with more complex morphological phenomena, such as vowel harmony or consonant harmony?
Overall, the paper presents a significant contribution to the field of natural language processing, and the results are well-supported by extensive experiments and analysis. With some minor revisions to address the suggested questions and areas for improvement, the paper is ready for publication.