This paper presents a novel model for multimodal learning based on gated neural networks, specifically the Gated Multimodal Unit (GMU). The GMU is designed to learn the fusion of multiple sources of information, such as text and images, and can be used as an internal unit in a neural network architecture. The authors evaluate the GMU on a multilabel scenario for genre classification of movies using the plot and poster, and demonstrate that it outperforms single-modality approaches and other fusion strategies.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in multimodal learning, and proposes a novel and well-motivated approach to address it. The GMU model is based on a clear and intuitive idea, and the authors provide a thorough explanation of its architecture and training procedure. Secondly, the paper provides a comprehensive evaluation of the GMU model, including experiments on a large and publicly available dataset, and demonstrates its effectiveness in improving the performance of genre classification.
The paper provides strong support for its claims, including a thorough literature review, a clear explanation of the GMU model, and a comprehensive evaluation of its performance. The authors also provide additional insights into the behavior of the GMU model, including an analysis of the activations of the gate units and a qualitative exploration of test examples. The paper is well-written, well-organized, and easy to follow, making it a pleasure to read.
To further improve the paper, I would suggest that the authors provide more details on the hyperparameter tuning procedure, and consider adding more baseline models for comparison. Additionally, it would be interesting to see more analysis on the interpretability of the learned features, and how the GMU model can be used in other multimodal learning tasks.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How did the authors choose the hyperparameters for the GMU model, and what was the effect of different hyperparameter settings on the performance of the model? How does the GMU model compare to other multimodal fusion approaches, such as attention-based models or graph-based models? Can the GMU model be used in other multimodal learning tasks, such as image captioning or visual question answering, and if so, how would it need to be modified?