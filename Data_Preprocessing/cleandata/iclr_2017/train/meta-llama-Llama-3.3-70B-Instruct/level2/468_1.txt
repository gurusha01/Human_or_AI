This paper proposes novel network quantization schemes to minimize the performance loss due to quantization in deep neural networks, given a compression ratio constraint. The main claims of the paper are: (1) the Hessian-weighted distortion measure is a relevant objective function for network quantization to minimize the quantization loss locally, and (2) the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory, leading to two efficient heuristic solutions, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm.
The support for these claims is provided through a thorough analysis of the quantitative relation of quantization errors to the neural network loss function, and experimental results on three exemplary convolutional neural networks (LeNet, ResNet, and AlexNet). The results show that the proposed network quantization schemes provide considerable gain over the conventional method using k-means clustering, in particular for large and deep neural networks.
The usefulness of the ideas presented in the paper is evident, as they can be applied to reduce the storage requirements of deep neural networks, making them more suitable for deployment on resource-limited devices. The paper also demonstrates a good understanding of the field, with correct use of terms and evidence of understanding of relevant literature.
The novelty of the work lies in the proposal of Hessian-weighted k-means clustering and the connection to the ECSQ problem, which allows for the optimization of network quantization under a compression ratio constraint. The paper provides sufficient details for reproducibility, including a clear description of the proposed algorithms and experimental setup.
However, there are some limitations to the work. For example, the paper assumes that the loss function has reached a local minimum after training, and that the Hessian matrix can be approximated as a diagonal matrix. Additionally, the computational cost of Hessian computation can be high, although the paper proposes an alternative metric to avoid this cost.
Based on the evaluation of the paper, I would like to ask the authors to clarify the following points: (1) How sensitive are the results to the choice of Hessian approximation, and are there any alternative approximations that could be used? (2) Can the proposed network quantization schemes be applied to other types of neural networks, such as recurrent neural networks or generative adversarial networks? (3) How do the proposed schemes compare to other network compression techniques, such as network pruning or knowledge distillation?
Overall, I would recommend accepting the paper, as it presents a significant contribution to the field of network quantization and compression. The proposed schemes have the potential to be widely adopted, and the paper provides a thorough analysis and experimental evaluation of the ideas. 
My decision is to accept the paper with minor revisions to address the above questions and provide more clarity on the limitations and potential extensions of the work. 
Additional feedback to improve the paper includes: (1) providing more details on the computational cost of Hessian computation and the proposed alternative metric, (2) discussing the potential applications of the proposed schemes to other types of neural networks, and (3) comparing the proposed schemes to other network compression techniques.