This paper proposes a novel method for training neural networks, inspired by continuation and smoothing techniques, to address the challenge of optimizing highly non-convex neural network objectives. The authors introduce the concept of "mollification" of the loss function, which involves smoothing the loss function using a kernel, and then gradually reducing the amount of smoothing during training. This approach allows the optimization to start with a simpler, more convex objective function and gradually transition to the original, more complex objective function.
The paper claims to improve the optimization of deep neural networks, particularly in cases where the loss function has a highly non-convex landscape. The authors demonstrate the effectiveness of their approach on various tasks, including image classification, language modeling, and predicting character embeddings from characters.
I decide to accept this paper, with two key reasons for this choice: 
1. The paper presents a well-motivated and novel approach to addressing the challenge of optimizing highly non-convex neural network objectives. The idea of mollifying the loss function and gradually reducing the amount of smoothing during training is innovative and has the potential to improve the optimization of deep neural networks.
2. The paper provides a thorough evaluation of the proposed approach on various tasks, including image classification, language modeling, and predicting character embeddings from characters. The results demonstrate the effectiveness of the approach in improving the optimization of deep neural networks, particularly in cases where the loss function has a highly non-convex landscape.
The paper supports its claims with a range of experiments, including comparisons with other optimization methods, such as batch normalization and residual connections. The results show that the proposed approach can achieve better performance than these methods in some cases.
To further improve the paper, I suggest that the authors provide more analysis on the choice of the mollification kernel and the annealing schedule, as these seem to be critical components of the approach. Additionally, it would be interesting to see more comparisons with other optimization methods, such as stochastic gradient descent with momentum and Adam.
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
* Can you provide more intuition on why the mollification approach works, and how it relates to other optimization methods?
* How do you choose the hyperparameters for the mollification kernel and the annealing schedule, and are there any guidelines for selecting these hyperparameters in practice?
* Have you considered applying the mollification approach to other optimization problems, such as reinforcement learning or generative models? 
Overall, I believe that this paper presents a significant contribution to the field of deep learning, and I look forward to seeing further developments and applications of the proposed approach.