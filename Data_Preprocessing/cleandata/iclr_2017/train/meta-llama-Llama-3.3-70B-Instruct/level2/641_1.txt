This paper proposes two methods for wild variational inference, which allows for the training of general inference networks without requiring tractable density functions. The authors demonstrate the effectiveness of these methods by applying them to adaptively estimate the step sizes of stochastic gradient Langevin dynamics (SGLD). The paper is well-written, and the ideas are clearly presented.
The main claims of the paper are that the proposed methods, amortized SVGD and KSD variational inference, can be used to train inference networks with intractable density functions, and that these methods can be used to adaptively estimate the step sizes of SGLD, leading to improved performance. The paper provides a clear motivation for the work, discussing the limitations of traditional variational inference methods and the need for more flexible and powerful methods.
The support for the claims is provided through a combination of theoretical analysis and empirical results. The authors provide a detailed analysis of the proposed methods, including their connection to Stein's identity and Stein discrepancy. The empirical results demonstrate the effectiveness of the proposed methods on several examples, including a toy Gaussian mixture model and a Bayesian logistic regression example.
The ideas presented in the paper are practically useful, as they provide a new approach to variational inference that can be used in a wide range of applications. The methods proposed in the paper have the potential to improve the performance of Bayesian inference algorithms, particularly in cases where the posterior distribution is complex and difficult to sample from.
The paper reflects common knowledge in the field, and the authors demonstrate a clear understanding of the relevant literature. The references provided are comprehensive and relevant, and the authors engage with the existing work in the field in a meaningful way.
The novelty of the paper lies in the proposal of two new methods for wild variational inference, which can be used to train inference networks with intractable density functions. The authors demonstrate the effectiveness of these methods and provide a clear analysis of their connection to existing work in the field.
The paper is complete, and the authors provide sufficient details for reproducibility. The limitations of the paper are acknowledged, and the authors discuss potential avenues for future work.
My decision is to accept the paper, with the main reason being that the paper proposes two new and effective methods for wild variational inference, which have the potential to improve the performance of Bayesian inference algorithms.
Additional feedback:
* The paper could benefit from a more detailed discussion of the computational cost of the proposed methods, particularly in comparison to existing methods.
* The authors may want to consider providing more examples of the application of the proposed methods to real-world problems.
* The paper could benefit from a more detailed analysis of the theoretical properties of the proposed methods, particularly in terms of their convergence and stability.
Questions for the authors:
* Can you provide more details on the computational cost of the proposed methods, particularly in comparison to existing methods?
* How do you plan to extend the proposed methods to more complex models and applications?
* Can you provide more examples of the application of the proposed methods to real-world problems?