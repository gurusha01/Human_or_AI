This paper presents the design and development of a new large-scale dataset, MS MARCO, for machine reading comprehension (RC) and open-domain question answering (QA). The dataset aims to overcome the weaknesses of previous publicly available datasets by using real anonymized user queries, extracting context passages from real web documents, and providing human-generated answers. The authors claim that MS MARCO is the most comprehensive real-world dataset of its kind, with 100,000 queries and corresponding answers, and plan to release one million queries in the future.
I decide to accept this paper, with the main reason being that it presents a significant contribution to the field of RC and QA by providing a large-scale, real-world dataset that addresses the shortcomings of existing datasets. The paper is well-motivated, and the approach is well-placed in the literature, with a clear understanding of the limitations of current datasets and the need for a more realistic and comprehensive dataset.
The paper provides strong support for its claims, with a detailed description of the dataset structure, building process, and distribution, as well as experimental results that demonstrate the effectiveness of the dataset in evaluating machine reading comprehension models. The authors also acknowledge the limitations of their work and provide a clear direction for future research, including the plan to enrich the test set with multiple answers and develop more advanced datasets to assess human-like reading comprehension.
To improve the paper, I suggest that the authors provide more details on the data collection process, such as the criteria used to select the queries and passages, and the guidelines provided to the human judges for generating answers. Additionally, it would be helpful to include more analysis on the characteristics of the dataset, such as the distribution of query types and answer lengths, and the challenges posed by the dataset to machine reading comprehension models.
I would like to ask the authors to clarify the following points: (1) How do they plan to ensure the quality and consistency of the human-generated answers, and what measures are in place to handle cases where the answers are incorrect or incomplete? (2) How do they plan to address the issue of multiple answers to a single question, and what evaluation metrics will they use to compare the performance of models on single vs. multiple answers? (3) What are the potential applications of the MS MARCO dataset beyond machine reading comprehension and question answering, and how do they envision the dataset being used in other areas of natural language processing research?