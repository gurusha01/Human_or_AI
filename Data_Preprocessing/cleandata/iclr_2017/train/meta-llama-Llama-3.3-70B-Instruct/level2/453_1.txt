This paper proposes a novel binarization algorithm for deep neural networks, which directly considers the effect of binarization on the loss during the binarization process. The authors formulate this as an optimization problem using the proximal Newton algorithm with a diagonal Hessian approximation. The proposed method, called Loss-Aware Binarization (LAB), outperforms existing binarization schemes and achieves comparable performance to the original full-precision network.
I decide to accept this paper for the following reasons:
1. The paper tackles a specific and relevant problem in the field of deep learning, which is the reduction of computational costs and memory usage in neural networks.
2. The approach is well-motivated and placed in the context of existing literature, with a clear explanation of the limitations of current binarization methods.
3. The proposed method is supported by theoretical analysis and empirical evaluations on various datasets, including MNIST, CIFAR-10, and SVHN, as well as recurrent neural networks.
The supporting arguments for my decision are:
* The paper provides a clear and concise introduction to the problem of binarization in deep neural networks and the limitations of existing methods.
* The proposed LAB method is well-explained, and the use of proximal Newton algorithm with diagonal Hessian approximation is justified.
* The experimental results demonstrate the effectiveness of the proposed method, with significant improvements over existing binarization schemes.
To further improve the paper, I suggest the authors:
* Provide more detailed analysis of the computational costs and memory usage of the proposed method compared to existing binarization schemes.
* Investigate the applicability of the proposed method to other types of neural networks, such as convolutional neural networks and recurrent neural networks with attention mechanisms.
* Consider providing more visualizations of the results, such as plots of the convergence of the LAB method and comparisons of the binarized weights and activations.
Questions I would like the authors to answer:
* How does the proposed LAB method handle the case where the Hessian matrix is not positive semi-definite?
* Can the authors provide more insights into the choice of the diagonal Hessian approximation and its impact on the performance of the LAB method?
* How does the proposed method compare to other regularization techniques, such as dropout and weight decay, in terms of reducing overfitting and improving generalization?