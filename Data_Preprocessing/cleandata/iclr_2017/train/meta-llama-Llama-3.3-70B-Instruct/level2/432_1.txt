This paper presents a novel technique called PGQL, which combines policy gradient with Q-learning in a reinforcement learning setting. The authors establish a connection between the fixed points of regularized policy gradient algorithms and the Q-values of the resulting policy, allowing them to estimate the Q-values from the policy. They then propose an auxiliary update to the policy gradient, which reduces the Bellman residual evaluated on a transformation of the policy. This update can be performed off-policy, using stored experience.
The paper claims to improve data efficiency and stability compared to actor-critic or Q-learning alone. The authors provide numerical examples, including a grid world and the full suite of Atari games, where they achieve performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.
Based on the provided information, I decide to accept this paper. The main reasons for this decision are:
1. The paper tackles a specific question/problem in reinforcement learning, namely, improving the efficiency and stability of policy gradient methods.
2. The approach is well-motivated, building on existing work in policy gradient and Q-learning, and provides a clear connection between the two.
3. The paper supports its claims with numerical examples, including a comprehensive evaluation on the Atari games suite, demonstrating improved performance over existing methods.
To further improve the paper, I suggest the authors:
* Provide more detailed analysis of the theoretical guarantees of PGQL, particularly in the case of function approximation.
* Investigate the sensitivity of PGQL to hyperparameters, such as the regularization penalty and the weighting parameter η.
* Compare PGQL to other state-of-the-art methods in reinforcement learning, such as deep deterministic policy gradients (DDPG) and twin delayed deep deterministic policy gradients (TD3).
Questions for the authors:
* Can you provide more insight into the choice of the weighting parameter η and its effect on the performance of PGQL?
* How does PGQL handle exploration-exploitation trade-offs, particularly in cases where the reward signal is sparse or delayed?
* Are there any plans to extend PGQL to more complex domains, such as continuous control tasks or multi-agent environments?