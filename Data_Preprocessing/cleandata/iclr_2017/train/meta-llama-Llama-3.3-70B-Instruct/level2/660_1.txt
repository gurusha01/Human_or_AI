The paper proposes a novel method for improving stochastic gradient descent (SGD) algorithms by incorporating feedback from the objective function. The authors introduce a modified version of the Adam algorithm, called Eve, which uses a running average of the relative changes in the objective function to adaptively tune the learning rate. The paper claims that Eve outperforms state-of-the-art methods for training deep learning models, including Adam, Adagrad, and RMSProp.
Based on the provided information, I decide to accept this paper. The main reasons for this decision are:
1. The paper tackles a specific and well-motivated problem in the field of deep learning optimization, which is a crucial aspect of training deep neural networks.
2. The approach is well-placed in the literature, and the authors provide a clear and concise review of related work, including the limitations of existing methods.
The supporting arguments for this decision are:
* The paper provides a clear and detailed description of the proposed method, including the mathematical formulation and the algorithmic implementation.
* The authors conduct a thorough experimental evaluation of Eve, comparing it with several state-of-the-art methods on a variety of tasks, including image classification, language modeling, and question answering.
* The results show that Eve consistently outperforms other methods, achieving lower training losses and better performance on test sets.
Additional feedback to improve the paper:
* The authors could provide more theoretical analysis of the proposed method, including convergence guarantees and bounds on the optimization error.
* The paper could benefit from more detailed visualizations of the optimization process, including plots of the learning rate, the objective function, and the gradient norms.
* The authors may want to consider additional experiments on larger-scale problems, such as training deep neural networks on large datasets, to further demonstrate the effectiveness of Eve.
Questions to the authors:
* Can you provide more insight into the choice of hyperparameters for Eve, including the values of Î²3, k, and K?
* How does the proposed method handle non-convex objective functions, and are there any guarantees on the convergence to a global optimum?
* Are there any plans to extend the proposed method to other optimization algorithms, such as SGD with momentum or Nesterov acceleration?