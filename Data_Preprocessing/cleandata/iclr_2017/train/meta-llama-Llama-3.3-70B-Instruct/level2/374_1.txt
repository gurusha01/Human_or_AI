The paper presents a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on word properties, which is a significant improvement over existing approaches that use concatenation or scalar weighting. The authors also extend this idea to model the interaction between documents and queries for reading comprehension tasks. The paper claims to achieve new state-of-the-art results on several reading comprehension datasets, including the Children's Book Test and Who Did What datasets.
I decide to accept this paper with the following reasons: 
1. The paper tackles a specific and well-defined problem in natural language processing, which is combining word-level and character-level representations for reading comprehension tasks.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and how the proposed fine-grained gating mechanism addresses these limitations.
The supporting arguments for this decision are as follows: 
The paper provides a thorough analysis of the existing methods for combining word-level and character-level representations and clearly explains the limitations of these methods. The proposed fine-grained gating mechanism is well-motivated and addresses these limitations by dynamically combining the two representations based on word properties. The paper also provides extensive experimental results on several reading comprehension datasets, which demonstrate the effectiveness of the proposed approach.
Additional feedback to improve the paper includes: 
1. Providing more details on the hyperparameter tuning process and the sensitivity of the model to different hyperparameters.
2. Including more visualizations and analysis of the gate values to provide a better understanding of how the fine-grained gating mechanism works.
3. Discussing potential applications of the fine-grained gating mechanism beyond reading comprehension tasks.
Questions to the authors include: 
1. How do the authors plan to extend the fine-grained gating mechanism to combine other levels of representations, such as phrases and sentences?
2. Can the authors provide more details on the computational cost of the proposed approach and how it compares to existing methods?
3. How do the authors plan to integrate the fine-grained gating mechanism with other natural language processing tasks, such as language modeling and machine translation?