This paper proposes two techniques to improve the training of deep latent Gaussian models (DLGMs) on sparse, high-dimensional data. The first technique involves optimizing the local variational parameters using an iterative optimizer, which helps to avoid poor local optima. The second technique uses tf-idf features to incorporate global information into the inference process. The paper also introduces a method to interpret the learned generative model using Jacobian vectors, which can be used to extract distributed representations of features.
The paper claims to improve the training of DLGMs on sparse data and to provide a method for interpreting the learned model. The claims are supported by experiments on several datasets, including text and medical data. The results show that the proposed techniques can improve the performance of DLGMs on sparse data and that the Jacobian vectors can be used to extract meaningful representations of features.
I decide to accept this paper because it presents a clear and well-motivated approach to improving the training of DLGMs on sparse data, and the experimental results demonstrate the effectiveness of the proposed techniques. The paper also provides a novel method for interpreting the learned generative model, which is a valuable contribution to the field.
The key reasons for my decision are:
1. The paper presents a clear and well-motivated approach to improving the training of DLGMs on sparse data.
2. The experimental results demonstrate the effectiveness of the proposed techniques.
3. The paper provides a novel method for interpreting the learned generative model.
To improve the paper, I suggest that the authors provide more details on the implementation of the proposed techniques, such as the choice of hyperparameters and the optimization algorithms used. Additionally, it would be helpful to provide more analysis on the interpretability of the Jacobian vectors and how they can be used in practice.
Some questions I would like the authors to answer are:
1. How do the proposed techniques compare to other methods for training DLGMs on sparse data?
2. Can the Jacobian vectors be used for other tasks, such as feature selection or dimensionality reduction?
3. How do the authors plan to extend the proposed techniques to other types of data, such as images or audio?