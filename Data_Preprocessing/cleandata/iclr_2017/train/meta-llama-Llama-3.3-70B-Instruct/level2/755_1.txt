This paper provides a theoretical explanation for the great performance of ResNet via the study of deep linear networks and some nonlinear variants. The authors show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones.
I decide to accept this paper with the following reasons:
1. The paper tackles a specific question/problem: why ResNet is easy to train, and provides a clear and well-motivated approach to address it.
2. The approach is well-placed in the literature, and the authors provide a thorough review of related work, including the original ResNet paper and other attempts to improve it.
3. The paper supports its claims with both theoretical analysis and extensive experiments, which demonstrate the effectiveness of the proposed approach.
The theoretical analysis is sound and well-explained, and the experiments are well-designed and provide strong evidence for the claims made in the paper. The authors also provide a clear and concise explanation of the results, making it easy to follow and understand.
To further improve the paper, I would suggest the following:
- Provide more intuition on why the 2-shortcut is special and how it affects the optimization process.
- Consider adding more experiments to demonstrate the effectiveness of the proposed approach on other datasets and tasks.
- Provide more details on the implementation of the experiments, such as the specific architectures used and the hyperparameters tuned.
Some questions I would like the authors to answer:
- Can you provide more insight into how the condition number of the Hessian affects the optimization process?
- How does the proposed approach relate to other techniques for improving the training of deep neural networks, such as batch normalization and gradient clipping?
- Are there any potential limitations or drawbacks to using the proposed approach, and how might they be addressed?