The paper "Third-Person Imitation Learning" presents a novel approach to imitation learning, where an agent learns to perform a task by observing a demonstration from a third-person perspective. The authors propose a method that uses domain confusion and generative adversarial networks to learn a domain-agnostic representation of the agent's observations, allowing it to learn from third-person demonstrations.
The main claim of the paper is that their approach can successfully learn to perform tasks in simple environments, such as a pointmass, reacher, and inverted pendulum, using only third-person demonstrations. The authors support this claim with experimental results, which show that their approach can learn reasonable policies for these tasks and outperform several baselines.
I decide to accept this paper, with the main reason being that it presents a novel and well-motivated approach to imitation learning. The authors provide a clear and well-written introduction to the problem of third-person imitation learning and motivate their approach with a thorough review of related work.
The paper is well-organized, and the authors provide a clear explanation of their method, including the game formulation and the optimization procedure. The experimental results are thorough and well-presented, and the authors provide a detailed analysis of the results, including comparisons to several baselines.
One potential limitation of the paper is that the authors only evaluate their approach on simple environments, and it is unclear how well it will generalize to more complex tasks. Additionally, the authors could provide more details on the hyperparameter selection and the sensitivity of the approach to different hyperparameters.
To improve the paper, I suggest that the authors provide more details on the architecture parameters and the training procedure, including the learning rate and the number of iterations. Additionally, the authors could provide more analysis on the learned feature representations and how they relate to the task being performed.
Some questions I would like the authors to answer include: How do the learned feature representations change as the agent learns to perform the task? Can the authors provide more insight into how the domain confusion loss affects the learning process? How does the approach generalize to more complex tasks, such as tasks with multiple objects or tasks that require longer-term planning?
Overall, the paper presents a novel and well-motivated approach to imitation learning, and the authors provide a clear and thorough explanation of their method and experimental results. With some additional analysis and details, the paper has the potential to make a significant contribution to the field of reinforcement learning and imitation learning.