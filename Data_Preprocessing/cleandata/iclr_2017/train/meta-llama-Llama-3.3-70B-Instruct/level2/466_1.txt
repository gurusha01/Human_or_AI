This paper presents a theoretical analysis of the optimization landscape of deep linear residual networks and the representational power of residual networks. The authors provide a simple proof that arbitrarily deep linear residual networks have no spurious local optima, and show that residual networks with ReLU activations have universal finite-sample expressivity. They also experiment with a simple residual architecture on image classification benchmarks, achieving competitive results.
The main claims of the paper are: (1) deep linear residual networks have no spurious local optima, and (2) residual networks with ReLU activations have universal finite-sample expressivity. The authors support these claims with theoretical proofs and empirical experiments.
I decide to accept this paper because it presents a significant contribution to the understanding of deep learning, providing a theoretical foundation for the success of residual networks. The paper is well-motivated, well-placed in the literature, and the results are correct and scientifically rigorous.
The approach is well-motivated, as the authors identify a key principle in deep learning, namely, that each layer of a deep artificial neural network should be able to easily express the identity transformation. The paper is also well-placed in the literature, as it builds on existing work on residual networks and provides a new perspective on their optimization landscape and representational power.
The results are correct and scientifically rigorous, as the authors provide detailed proofs and empirical experiments to support their claims. The paper is well-written, and the authors provide a clear and concise explanation of their results.
To improve the paper, I suggest that the authors provide more discussion on the implications of their results for practice, and how they can be used to improve the design of deep learning models. Additionally, the authors may want to consider providing more experimental results to further demonstrate the effectiveness of their approach.
Some questions I would like the authors to answer are: (1) How do the results of this paper relate to other work on optimization landscapes of neural networks? (2) Can the authors provide more insight into why their simple residual architecture achieves competitive results on image classification benchmarks? (3) How can the results of this paper be used to improve the design of deep learning models in practice?