The paper proposes a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the RNN language model to improve the encoding and decoding of factual knowledge. The model predicts whether a word has an underlying fact or not and generates words either from the vocabulary or by copying from the fact description. The authors introduce a new dataset, WikiFacts, and evaluate the model using the standard perplexity and a new metric, Unknown-Penalized Perplexity (UPP).
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-motivated question of improving language models' ability to encode and decode factual knowledge.
2. The approach is well-placed in the literature, building upon existing work on RNN language models and knowledge graphs.
The paper provides a clear and well-structured presentation of the proposed model, including the architecture, training, and evaluation. The experiments demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities, and the introduction of the UPP metric provides a more nuanced evaluation of language models for knowledge-related tasks.
To further improve the paper, I suggest the authors provide more analysis on the limitations of the model, such as its reliance on the quality of the knowledge graph and the potential for overfitting to the training data. Additionally, it would be helpful to include more comparisons with other state-of-the-art language models and to explore the applicability of the NKLM to other tasks, such as question answering and dialogue modeling.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How does the model handle cases where the knowledge graph is incomplete or noisy?
* Can the authors provide more details on the implementation of the copy mechanism and how it is trained?
* How does the model adapt to changes in the knowledge graph, and what are the implications for real-world applications?