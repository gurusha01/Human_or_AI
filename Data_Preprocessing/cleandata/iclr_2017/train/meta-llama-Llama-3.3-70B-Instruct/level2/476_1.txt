This paper claims to provide the first empirical demonstration that deep convolutional models need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. The authors argue that previous research showing that shallow feed-forward nets can learn complex functions learned by deep nets is not applicable to problems like CIFAR-10, where multiple layers of convolution are required to train accurate teacher models.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper presents a well-motivated approach, including a thorough review of the literature and a clear explanation of the methodology used. The authors also provide a comprehensive evaluation of their results, including a comparison with previous work and an analysis of the limitations of their approach. Secondly, the paper supports its claims with empirical results, including experiments on CIFAR-10 and a detailed analysis of the performance of shallow and deep convolutional models.
The supporting arguments for these reasons include the fact that the authors use Bayesian hyperparameter optimization to thoroughly explore the space of architectures and learning hyperparameters, which ensures that the results are robust and reliable. Additionally, the authors provide a detailed analysis of the performance of shallow and deep convolutional models, including a comparison of the accuracy of models with different numbers of convolutional layers and parameters. The results show that shallow models, even when trained with distillation, are not able to achieve accuracy comparable to deep convolutional models, and that multiple layers of convolution are required to train accurate models on CIFAR-10.
To improve the paper, I would suggest that the authors provide more insight into why convolution is critical for this problem, and why shallow models are not able to achieve comparable accuracy even when trained with distillation. Additionally, it would be interesting to see more experiments on other datasets, such as ImageNet, to determine whether the results are generalizable to other problems.
Some questions I would like the authors to answer include: Can you provide more details on the hyperparameter optimization process, including the specific hyperparameters that were optimized and the ranges of values that were explored? How do the results change when using different distillation methods, such as using the intermediate representations learned by the teacher as hints to guide training deep students? Can you provide more insight into why the student models perform consistently worse when trained using dropout, and whether this is related to the regularization provided by the soft targets from the teacher model?