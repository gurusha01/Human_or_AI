This paper claims to show that for a large class of piecewise smooth functions, the number of neurons needed by a shallow network to approximate a function is exponentially larger than the corresponding number of neurons needed by a deep network for a given degree of function approximation. The authors provide theoretical results and proofs to support their claims, including upper bounds on the size of multilayer neural networks for approximating univariate and multivariate functions, as well as lower bounds on the network size for certain classes of functions.
I decide to accept this paper because it provides a clear and well-motivated approach to understanding the advantages of deep neural networks over shallow ones. The authors' use of rectifier linear units (ReLUs) and binary step units (BSUs) as activation functions is well-justified, and their analysis of the multiplication of two bits using a ReLU is a nice touch. The paper is well-organized, and the proofs are thorough and easy to follow.
The supporting arguments for my decision include the fact that the authors provide a comprehensive review of existing literature on neural networks and function approximation, and their results are consistent with existing knowledge in the field. The paper also provides a clear and concise summary of the main contributions, which are easy to understand and evaluate. Additionally, the authors provide a detailed analysis of the limitations of their results and suggest potential avenues for future research.
To improve the paper, I suggest that the authors provide more numerical examples or simulations to illustrate their results and make them more concrete. Additionally, the authors could provide more discussion on the implications of their results for practical applications of neural networks, such as image recognition or natural language processing. Some questions I would like the authors to answer include: How do the results change if different activation functions are used? Can the authors provide more insight into the trade-offs between depth and width in neural networks? How do the results relate to other existing work on neural networks and function approximation?
Overall, I think this is a well-written and well-researched paper that makes a significant contribution to our understanding of neural networks and function approximation. With some minor revisions to address the suggestions above, I believe the paper would be even stronger and more effective in communicating its results to the reader.