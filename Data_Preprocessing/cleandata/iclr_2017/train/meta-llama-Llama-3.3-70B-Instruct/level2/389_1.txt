This paper proposes a novel model for unconditional audio generation, called SampleRNN, which generates one audio sample at a time using a hierarchical structure of recurrent neural networks (RNNs) and autoregressive multilayer perceptrons. The model is designed to capture long-term dependencies in audio data and is evaluated on three datasets of different nature.
The main claims of the paper are: (1) the proposed model can generate high-quality audio samples that are preferred by human listeners, (2) the hierarchical structure of the model allows it to capture long-term dependencies in audio data, and (3) the model can be used for unconditional audio generation tasks such as speech synthesis and music generation.
Based on the evaluation, I decide to accept this paper. The main reasons for this decision are: (1) the paper presents a well-motivated approach to unconditional audio generation, and (2) the experimental results demonstrate the effectiveness of the proposed model in generating high-quality audio samples.
The paper provides a clear and detailed description of the proposed model, including the architecture and the training procedure. The experimental results are also well-presented, with a thorough evaluation of the model on three datasets. The paper also provides a good discussion of the related work and the potential applications of the proposed model.
One potential limitation of the paper is that the model is evaluated only on three datasets, and it is not clear how well the model would perform on other datasets. Additionally, the paper could benefit from a more detailed analysis of the computational complexity of the proposed model.
To improve the paper, I suggest that the authors provide more details on the computational complexity of the model and evaluate the model on a wider range of datasets. Additionally, the authors could provide more insights into the interpretability of the model, such as visualizations of the learned representations or analysis of the generated samples.
Some questions that I would like the authors to answer are: (1) How does the model perform on datasets with different sampling rates or bit depths? (2) Can the model be used for conditional audio generation tasks, such as text-to-speech synthesis? (3) How does the model compare to other state-of-the-art models for audio generation, such as WaveNet or PixelRNN?