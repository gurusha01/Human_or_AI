This paper presents a novel approach to reinforcement learning in the context of StarCraft micromanagement scenarios. The authors propose a zero-order optimization algorithm that combines backpropagation and direct exploration in the policy space, which outperforms traditional Q-learning and policy gradient methods. The paper is well-structured, and the authors provide a clear explanation of the problem, the proposed approach, and the experimental results.
The main claim of the paper is that the proposed zero-order optimization algorithm can learn effective strategies for micromanagement scenarios in StarCraft, which is a challenging problem due to the large state and action spaces. The authors support this claim by presenting experimental results that show the superior performance of their algorithm compared to Q-learning and policy gradient methods.
The paper is well-motivated, and the authors provide a thorough review of the related work in the field. The proposed approach is novel and addresses the limitations of traditional reinforcement learning methods in the context of StarCraft micromanagement. The experimental results are convincing, and the authors provide a detailed analysis of the learned strategies.
However, there are some limitations to the paper. The authors only evaluate their approach on a limited set of scenarios, and it is unclear how well the algorithm will perform on more complex scenarios or in other domains. Additionally, the authors do not provide a detailed analysis of the computational complexity of their algorithm, which could be an important consideration for large-scale applications.
Overall, I would accept this paper for publication. The authors present a novel and effective approach to reinforcement learning in the context of StarCraft micromanagement, and the experimental results are convincing. However, I would suggest that the authors provide more detailed analysis of the computational complexity of their algorithm and evaluate their approach on a wider range of scenarios to demonstrate its generality.
Some potential questions for the authors to answer:
* How does the computational complexity of the proposed algorithm compare to traditional Q-learning and policy gradient methods?
* Can the authors provide more detailed analysis of the learned strategies, such as visualizations or heatmaps of the unit movements and actions?
* How well does the proposed algorithm perform on more complex scenarios, such as those with multiple types of units or larger maps?
* Can the authors provide more discussion on the potential applications of their approach to other domains, such as robotics or autonomous vehicles?