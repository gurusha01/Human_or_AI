This paper proposes a novel training objective for neural networks, called Neural Graph Machines, which combines the strengths of neural networks and label propagation for semi-supervised learning on graphs. The authors claim that their approach can efficiently leverage both labeled and unlabeled data, and can be trained using stochastic gradient descent.
I decide to accept this paper, with the main reason being that the approach is well-motivated and supported by extensive experiments on various tasks and datasets. The authors provide a clear and concise introduction to the background and related work, and their proposed objective function is inspired by the label propagation objective, which is a well-established technique in semi-supervised learning.
The experiments demonstrate the efficacy of the proposed approach on multi-label classification, text classification, and semantic intent classification tasks, and show that it outperforms baseline methods, including a two-stage approach using graph embeddings and a linear classifier. The authors also provide a thorough analysis of the results and discuss the implications of their approach.
To further improve the paper, I suggest that the authors provide more details on the construction of the graphs used in the experiments, and discuss the sensitivity of the approach to the choice of hyperparameters. Additionally, it would be interesting to see more experiments on directed graphs and multiple graphs from different domains.
Some questions I would like the authors to answer include: How do the authors plan to extend their approach to handle multiple graphs from different domains? Can they provide more insights on the choice of hyperparameters and their impact on the performance of the approach? How do the authors plan to address the scalability of their approach to very large graphs?
Overall, I believe that this paper makes a significant contribution to the field of semi-supervised learning on graphs, and has the potential to inspire further research in this area. With some minor revisions to address the above suggestions, I think the paper is ready for publication.