Summary of the Paper's Contributions
The paper proposes two techniques to improve the training of deep latent Gaussian models (DLGMs) on sparse, high-dimensional data. The first technique involves optimizing the local variational parameters using an iterative optimizer, which helps to avoid poor local optima. The second technique uses tf-idf features to incorporate global information into the inference process. Additionally, the paper introduces a method to interpret the learned generative model by extracting distributed representations of features via a simple linearization of the model, known as Jacobian vectors.
Decision and Reasons
I decide to Accept this paper, with two key reasons:
1. The paper tackles a specific and important problem in deep generative modeling, namely, the difficulty of training DLGMs on sparse, high-dimensional data. The proposed techniques address this problem and demonstrate improved performance on several datasets.
2. The paper introduces a novel and interesting method for interpreting the learned generative model, which has the potential to provide valuable insights into the structure of the data.
Supporting Arguments
The paper provides a clear and well-motivated approach to addressing the challenges of training DLGMs on sparse data. The experimental results demonstrate the effectiveness of the proposed techniques, including the optimization of local variational parameters and the use of tf-idf features. The introduction of Jacobian vectors as a method for interpreting the learned generative model is a significant contribution, as it provides a way to extract meaningful representations of features from the model.
Additional Feedback and Questions
To further improve the paper, I would like to see more discussion on the following points:
* How do the proposed techniques compare to other methods for training DLGMs on sparse data, such as using different priors or regularization techniques?
* Can the Jacobian vectors be used for other tasks, such as feature selection or dimensionality reduction?
* How do the results change when using different architectures for the generative model or the inference network?
I would also like the authors to clarify the following points:
* How do the tf-idf features affect the inference process, and why are they particularly useful for sparse data?
* Can the optimization of local variational parameters be applied to other types of deep generative models, such as variational autoencoders or generative adversarial networks?