Summary
The paper explores the use of sinusoidal activation functions in deep neural networks, which have been largely ignored due to their perceived difficulty in training. The authors provide a formal analysis of the loss surface of these networks, identifying the challenges in training them, and demonstrate that they can learn successfully in certain scenarios. They also show that sinusoidal activation functions can outperform traditional monotonic functions in specific tasks, such as learning algorithms.
Decision
I decide to Accept this paper, with the main reasons being:
1. The paper tackles a specific and interesting question about the use of sinusoidal activation functions in deep neural networks, which has been largely overlooked in the literature.
2. The authors provide a thorough and well-motivated analysis of the loss surface of these networks, which sheds light on the challenges in training them.
Supporting Arguments
The paper is well-structured and easy to follow, with a clear introduction to the problem and a thorough analysis of the loss surface of sinusoidal activation functions. The authors provide a detailed explanation of the challenges in training these networks, including the presence of infinitely many and shallow local minima. They also demonstrate that, despite these challenges, sinusoidal activation functions can learn successfully in certain scenarios, such as simple classification tasks. Furthermore, they show that these functions can outperform traditional monotonic functions in specific tasks, such as learning algorithms.
Additional Feedback
To improve the paper, I suggest that the authors provide more intuition on why sinusoidal activation functions are beneficial for certain tasks, such as learning algorithms. Additionally, they could explore the use of sinusoidal activation functions in other models, such as recurrent neural networks, and investigate the effect of different initialization schemes on the training of these networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into why sinusoidal activation functions are beneficial for learning algorithms, and how they can be used in other tasks that require periodic or oscillatory behavior?
2. How do the results of the paper generalize to other types of neural networks, such as recurrent neural networks or convolutional neural networks?
3. What are the implications of the paper's findings for the design of neural network architectures, and how can sinusoidal activation functions be used in practice to improve the performance of these networks?