Summary
The paper proposes a novel activation function, Parametric Exponential Linear Unit (PELU), which learns a parameterization of the Exponential Linear Unit (ELU) function. The authors demonstrate that PELU improves the performance of Convolutional Neural Networks (CNNs) on various image classification tasks, including CIFAR-10, CIFAR-100, and ImageNet. The paper provides a thorough analysis of the proposed approach, including its motivation, definition, and optimization.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in deep learning, namely improving the performance of CNNs by learning a parameterization of the ELU activation function, and (2) the approach is well-motivated and supported by thorough experiments and analysis.
Supporting Arguments
The paper provides a clear and concise motivation for the proposed approach, highlighting the limitations of existing activation functions and the potential benefits of learning a parameterization of ELU. The authors also provide a thorough analysis of the proposed approach, including its definition, optimization, and experimental evaluation. The experimental results demonstrate the effectiveness of PELU in improving the performance of CNNs on various image classification tasks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the interpretation of the learned parameters and their impact on the network's behavior. Additionally, it would be interesting to see more experiments on other network architectures and tasks, such as recurrent neural networks and object detection. The authors may also consider providing more details on the computational cost and memory requirements of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insights into the interpretation of the learned parameters a and b, and how they affect the network's behavior?
2. How do you plan to extend the proposed approach to other network architectures and tasks, such as recurrent neural networks and object detection?
3. Can you provide more details on the computational cost and memory requirements of the proposed approach, and how it compares to existing activation functions?