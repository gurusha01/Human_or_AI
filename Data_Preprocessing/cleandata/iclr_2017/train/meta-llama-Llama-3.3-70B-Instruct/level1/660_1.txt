Summary
The paper proposes a novel method to improve stochastic gradient descent (SGD) by incorporating feedback from the objective function. The method, called Eve, modifies the Adam algorithm by adaptively tuning the learning rate based on the relative changes in the objective function. The authors conduct experiments on various deep learning models, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), and demonstrate that Eve outperforms state-of-the-art methods.
Decision
I decide to Accept this paper with some minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and important problem in deep learning optimization, and the proposed method is well-motivated and simple to implement.
2. The experimental results are thorough and demonstrate the effectiveness of the proposed method on various tasks.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of deep learning optimization and the limitations of existing methods. The proposed method is well-motivated by the idea of incorporating feedback from the objective function to adaptively tune the learning rate. The experimental results are extensive and demonstrate the superiority of Eve over other state-of-the-art methods on various tasks, including CNNs and RNNs.
Additional Feedback
To further improve the paper, I suggest the authors:
1. Provide more theoretical analysis of the proposed method and its effects on the optimization process.
2. Conduct additional experiments on larger-scale problems to further demonstrate the strength of the approach.
3. Consider providing more details on the hyperparameter tuning process and the sensitivity of the method to different hyperparameter settings.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more intuition on why the proposed method is effective in practice, and how it relates to the underlying geometry of the optimization problem?
2. How do you plan to address the issue of hyperparameter tuning in future work, and what are the potential limitations of the proposed method in terms of hyperparameter sensitivity?
3. Are there any plans to extend the proposed method to other optimization algorithms beyond Adam, and what are the potential challenges and opportunities in doing so?