Summary of the Paper's Contributions
The paper proposes a novel approach to compressing deep neural networks (DNNs) using homologically functional hashing (HFH). HFH utilizes multiple low-cost hash functions to map entries in a DNN's weight matrix to a shared compression space, and then employs a small reconstruction network to recover the original entries. This approach includes the recently proposed HashedNets as a special case and demonstrates improved compression ratios with minimal loss in prediction accuracy on several benchmark datasets.
Decision: Accept
The paper tackles a specific and relevant problem in the field of deep learning, namely, reducing the memory and energy consumption of DNNs while maintaining their predictive performance. The approach is well-motivated, building upon existing work on HashedNets and feature hashing. The paper provides a clear and detailed explanation of the HFH structure, training procedure, and property analysis, making it easy to follow and understand.
Supporting Arguments
1. Well-motivated approach: The paper clearly explains the limitations of existing DNN compression methods, such as HashedNets, and proposes a novel approach to address these limitations.
2. Effective compression: The experimental results demonstrate that HFH achieves high compression ratios with minimal loss in prediction accuracy, outperforming HashedNets on several benchmark datasets.
3. Theoretical analysis: The paper provides a thorough analysis of the properties of HFH, including value collision, value reconstruction, and feature hashing, which helps to understand the strengths and limitations of the approach.
Additional Feedback
To further improve the paper, the authors could consider providing more detailed analysis on the computational complexity and memory requirements of HFH, as well as exploring the application of HFH to other types of neural networks, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.
Questions for the Authors
1. How does the choice of hash functions affect the performance of HFH, and are there any specific hash functions that are more suitable for this approach?
2. Can the authors provide more insights into the trade-off between compression ratio and prediction accuracy, and how to balance these two competing objectives?
3. Are there any plans to explore the application of HFH to other domains, such as natural language processing and computer vision, and how might the approach need to be adapted for these domains?