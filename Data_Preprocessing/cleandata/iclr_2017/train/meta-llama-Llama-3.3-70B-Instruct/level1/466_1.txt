This paper tackles the specific question of understanding the optimization landscape of deep linear residual networks and the representational power of residual networks. The approach is well-motivated, building upon existing literature on residual networks and batch normalization. The authors provide a simple proof that arbitrarily deep linear residual networks have no spurious local optima and demonstrate the universal finite-sample expressivity of residual networks with ReLU activations.
I decide to accept this paper, with the primary reason being the significant theoretical contributions to the understanding of residual networks. The paper provides a thorough analysis of the optimization landscape of deep linear residual networks, showing that there are no bad critical points, and demonstrates the representational power of residual networks.
The supporting arguments for this decision include the clarity and rigor of the theoretical analysis, the simplicity and elegance of the proofs, and the significance of the results. The paper also provides empirical evidence to support the theoretical claims, demonstrating the effectiveness of all-convolutional residual networks on standard image classification benchmarks.
To further improve the paper, I would suggest providing more intuition and discussion on the implications of the theoretical results, particularly in the context of deep learning practice. Additionally, it would be helpful to include more detailed comparisons with existing works on residual networks and optimization landscapes.
Some questions I would like the authors to address include: How do the results on deep linear residual networks generalize to non-linear residual networks? What are the implications of the universal finite-sample expressivity of residual networks for deep learning practice? How do the authors' results relate to existing works on optimization landscapes and residual networks?
Overall, this paper makes significant contributions to the understanding of residual networks and has the potential to impact deep learning practice. With some minor revisions to address the above questions and provide more intuition and discussion, the paper would be even stronger.