Summary
The paper presents a novel framework called Latent Sequence Decompositions (LSD) for sequence-to-sequence models. LSD learns to decompose output sequences into a sequence of latent tokens, which are a function of both the input and output sequences. The authors argue that traditional sequence-to-sequence models rely on fixed deterministic decompositions, which can lead to sub-optimal performance. The LSD framework is evaluated on the Wall Street Journal speech recognition task and achieves a significant improvement over the baseline character model.
Decision
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and well-motivated problem in sequence-to-sequence modeling, which is the limitation of fixed deterministic decompositions.
2. The approach is well-placed in the literature, and the authors provide a clear and thorough overview of related work.
Supporting Arguments
The paper provides a clear and concise introduction to the problem and the proposed solution. The authors motivate the need for a more flexible decomposition of output sequences and provide a detailed description of the LSD framework. The experimental results are impressive, with the LSD model achieving a significant improvement over the baseline character model. The use of a convolutional network on the encoder further improves the performance, achieving a state-of-the-art result on the Wall Street Journal speech recognition task.
Additional Feedback
To improve the paper, I suggest the authors provide more analysis on the learned decompositions and their relationship to the input and output sequences. Additionally, it would be interesting to see more experiments on other sequence-to-sequence tasks, such as machine translation, to demonstrate the generalizability of the LSD framework. The authors may also want to consider providing more details on the implementation of the LSD framework, such as the choice of hyperparameters and the training procedure.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the learned decompositions and how they relate to the input and output sequences?
2. How do you choose the hyperparameters for the LSD framework, such as the size of the token vocabulary and the number of layers in the encoder and decoder?
3. Have you considered applying the LSD framework to other sequence-to-sequence tasks, such as machine translation, and if so, what were the results?