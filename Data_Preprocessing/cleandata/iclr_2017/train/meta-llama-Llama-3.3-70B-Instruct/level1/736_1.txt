Summary of the Paper's Claims and Contributions
The paper proposes a novel approach called Adaptive Batch Normalization (AdaBN) for domain adaptation in deep neural networks (DNNs). The authors claim that AdaBN can increase the generalization ability of a DNN by modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network. The paper demonstrates the effectiveness of AdaBN on standard domain adaptation datasets, including Office and Caltech-Bing, and achieves state-of-the-art performance. Additionally, the authors show that AdaBN can be applied to practical problems, such as cloud detection in remote sensing images.
Decision and Key Reasons
Based on the review, I decide to Accept this paper. The two key reasons for this choice are:
1. The approach is well-motivated and placed in the literature, addressing a significant problem in domain adaptation.
2. The paper provides extensive experimental results, demonstrating the effectiveness of AdaBN on various datasets and settings.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of domain adaptation and the limitations of existing methods. The authors propose a simple yet effective approach, AdaBN, which modifies the statistics of the Batch Normalization layer to adapt to the target domain. The experimental results are thorough and demonstrate the superiority of AdaBN over other state-of-the-art methods. The paper also provides an empirical analysis of the adaptation effect of different BN layers and the sensitivity of the method to the target domain size.
Additional Feedback and Questions
To further improve the paper, I suggest the authors provide more insights into the theoretical aspects of AdaBN and its relationship to other domain adaptation methods. Additionally, it would be interesting to see more experiments on other datasets and applications.
Some questions I would like the authors to answer are:
* How does AdaBN compare to other methods that use Maximum Mean Discrepancy (MMD) or domain confusion loss for domain adaptation?
* Can the authors provide more details on the computational efficiency of AdaBN compared to other methods?
* How does AdaBN perform in scenarios where the target domain has limited or no labeled data?