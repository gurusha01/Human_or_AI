Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to compressing deep neural networks using a variant of "soft weight-sharing" (Nowlan & Hinton, 1992). The authors claim that their method can achieve competitive compression rates by learning a mixture of Gaussians prior model over the weights, which encourages both quantization and pruning in a single re-training procedure. The paper also provides a theoretical foundation for the approach by relating it to the minimum description length (MDL) principle.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The two key reasons for this decision are:
1. The paper tackles a specific and relevant problem in the field of deep learning, namely, compressing neural networks for mobile devices.
2. The approach is well-motivated and grounded in theoretical foundations, including the MDL principle and variational Bayesian inference.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of compressing neural networks and motivates the need for a new approach. The authors also provide a thorough review of related work and demonstrate the effectiveness of their approach through experiments on several benchmark datasets. The use of Bayesian optimization to tune hyperparameters and the proposal of a scalable solution for large models are also notable strengths of the paper.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors:
* Provide more detailed visualizations of the compressed filters, such as those shown in Figure D, to help illustrate the effectiveness of the approach.
* Consider providing more theoretical analysis of the approach, such as bounds on the compression rate or guarantees on the accuracy of the compressed model.
* Investigate the applicability of the approach to other types of neural networks, such as recurrent neural networks or transformers.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the initialization of the mixture model components, such as how the means and variances are set?
* How do you choose the number of components in the mixture model, and is there a way to automatically determine this hyperparameter?
* Have you considered applying the approach to other types of data, such as images or speech, and if so, what are the potential challenges and opportunities?