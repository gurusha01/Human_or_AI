Summary of the Paper
The paper proposes a novel multimodal learning model called Joint Multimodal Variational Autoencoder (JMVAE), which can exchange multiple modalities bidirectionally. The model assumes that all modalities are independently conditioned on a joint latent variable, allowing for the extraction of a high-level representation that contains all information of modalities. The authors also propose an additional method, JMVAE-kl, to prevent generated samples from collapsing when some modalities are missing. The paper demonstrates the effectiveness of the JMVAE model on MNIST and CelebA datasets, showing that it can obtain appropriate joint representations and high log-likelihoods.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in multimodal learning, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of multimodal learning and the limitations of existing approaches. The proposed JMVAE model is well-motivated, and the authors provide a thorough explanation of the model architecture and training procedure. The experimental results on MNIST and CelebA datasets demonstrate the effectiveness of the JMVAE model in obtaining high log-likelihoods and generating samples that are comparable to or better than existing methods.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the JMVAE model, such as the specific architectures used for the encoders and decoders. Additionally, it would be helpful to include more visualizations of the generated samples and reconstructed images to better illustrate the quality of the results. Finally, the authors may want to consider exploring the application of the JMVAE model to more complex multimodal datasets, such as those with three or more modalities.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
1. Can you provide more details on the choice of hyperparameters, such as the learning rate and number of epochs, used in the experiments?
2. How do you plan to extend the JMVAE model to handle more complex multimodal datasets, such as those with three or more modalities?
3. Can you provide more insights into the trade-off between the reconstruction error and the regularization term in the JMVAE model, and how it affects the quality of the generated samples?