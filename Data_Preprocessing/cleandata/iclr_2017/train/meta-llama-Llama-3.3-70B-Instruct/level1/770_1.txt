Summary of the Paper's Contributions
The paper proposes a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with a recurrent neural network (RNN) language model. The NKLM predicts whether a word is based on a fact or not and generates words either from the vocabulary or by copying from the fact description. The model is trained on a new dataset, WikiFacts, and outperforms the traditional RNN language model in terms of perplexity and generates named entities that are not observed during training.
Decision and Key Reasons
I decide to Accept this paper with the following key reasons:
1. The paper tackles a specific and well-motivated problem of incorporating factual knowledge into language models, which is a significant limitation of current language models.
2. The approach is well-placed in the literature, and the authors provide a clear and thorough explanation of the model, its components, and the training process.
Supporting Arguments
The paper provides a thorough analysis of the limitations of traditional language models in encoding and decoding factual knowledge. The authors propose a novel model that addresses these limitations by incorporating symbolic knowledge from a knowledge graph. The model is evaluated on a new dataset, WikiFacts, and the results show significant improvements in perplexity and the ability to generate named entities. The authors also introduce a new evaluation metric, Unknown-Penalized Perplexity (UPP), which provides a more accurate assessment of the model's performance.
Additional Feedback and Questions
To further improve the paper, I would like the authors to:
1. Provide more details on the WikiFacts dataset, such as the size of the dataset and the process of aligning Wikipedia descriptions with Freebase facts.
2. Discuss the potential applications of the NKLM in other knowledge-related language tasks, such as question answering and dialogue modeling.
3. Consider relaxing the assumption that the true topic of a given description is known and explore ways to make the model search for proper topics on-the-fly.
Some questions I would like the authors to answer:
1. How does the NKLM handle cases where the knowledge graph is incomplete or outdated?
2. Can the NKLM be applied to other types of knowledge graphs, such as domain-specific graphs?
3. How does the NKLM compare to other models that incorporate external knowledge, such as memory-augmented neural networks?