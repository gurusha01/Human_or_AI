Summary
The paper studies the expressive power of deep neural networks, focusing on the relationship between network depth and expressivity. The authors introduce three measures of expressivity: neuron transitions, activation patterns, and dichotomies, and demonstrate that all three measures exhibit an exponential dependence on network depth. They prove that this exponential dependence is due to the growth of trajectory length, a quantity that characterizes the complexity of the input-output map. The authors also explore the consequences of their results for trained networks, showing that earlier layers have greater influence on expressivity and that training trades off between stability and expressivity of the input-output map.
Decision
I decide to Accept this paper, with the main reasons being:
1. The paper tackles a specific and well-motivated question about the expressive power of deep neural networks, which is a fundamental problem in the field.
2. The approach is well-placed in the literature, building on existing work on neural network expressivity and introducing new measures and techniques to analyze this problem.
3. The paper provides a clear and rigorous theoretical analysis, supported by empirical results and simulations, which demonstrates the exponential dependence of expressivity on network depth.
Supporting Arguments
The paper's contributions are significant, as they provide new insights into the relationship between network depth and expressivity. The authors' use of trajectory length as a key quantity to characterize expressivity is innovative and well-motivated. The theoretical analysis is rigorous and well-supported by empirical results, which demonstrate the validity of the authors' claims. The paper also raises important questions about the design of neural network architectures and the trade-offs between stability and expressivity.
Additional Feedback
To further improve the paper, I suggest that the authors:
1. Provide more detailed explanations of the theoretical results, particularly the proof of Theorem 1, which may be challenging for non-experts to follow.
2. Include more experimental results to support the claims made in the paper, particularly on larger datasets and more complex network architectures.
3. Discuss the implications of their results for practical applications, such as image classification and natural language processing.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more intuition about why trajectory length is a good measure of expressivity, and how it relates to other measures of expressivity in the literature?
2. How do the results in the paper depend on the specific choice of activation function, and are there any implications for the design of new activation functions?
3. Can you provide more details about the experimental setup and the hyperparameters used in the simulations, to facilitate replication of the results?