Summary of the Paper's Contributions
The paper proposes a novel approach to distributed training of deep learning models, which combines the benefits of synchronous and asynchronous stochastic optimization. The authors introduce a method called synchronous stochastic optimization with backup workers, which mitigates the effects of stragglers while avoiding the staleness of gradients. The approach is empirically validated on several models, including Inception and PixelCNN, and is shown to converge faster and to better test accuracies than asynchronous training.
Decision and Key Reasons
Based on the review of the paper, I decide to Accept the paper. The two key reasons for this decision are:
1. The paper tackles a specific and important problem in distributed deep learning, namely the trade-off between synchronous and asynchronous stochastic optimization.
2. The approach proposed by the authors is well-motivated, and the empirical results demonstrate its effectiveness in improving the convergence speed and test accuracy of deep learning models.
Supporting Arguments
The paper provides a clear and thorough analysis of the weaknesses of both synchronous and asynchronous stochastic optimization. The authors demonstrate that synchronous optimization can be slow due to the need to wait for the slowest worker, while asynchronous optimization can suffer from staleness of gradients. The proposed approach addresses these limitations by introducing backup workers, which can mitigate the effects of stragglers while avoiding staleness.
The empirical results presented in the paper are convincing and demonstrate the effectiveness of the proposed approach. The authors show that their method can converge faster and to better test accuracies than asynchronous training on several models, including Inception and PixelCNN.
Additional Feedback and Questions
To further improve the paper, I would like to see more discussion on the theoretical implications of the proposed approach. Specifically, I would like to know more about the conditions under which the approach is guaranteed to converge, and how the choice of backup workers affects the convergence rate.
I also have a few questions for the authors:
* How do the authors choose the number of backup workers, and what is the effect of this choice on the convergence speed and test accuracy?
* Can the authors provide more insight into the relationship between the staleness of gradients and the convergence speed of asynchronous optimization?
* How does the proposed approach compare to other methods for mitigating stragglers, such as softsync or asynchronous optimization with gradient clipping?