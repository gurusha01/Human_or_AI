Summary
The paper proposes a novel extension of Stochastic Gradient Variational Bayes (SGVB) to perform posterior inference for the weights of Stick-Breaking processes, allowing for the definition of a Stick-Breaking Variational Autoencoder (SB-VAE). The SB-VAE is a Bayesian nonparametric version of the variational autoencoder with a latent representation of stochastic dimensionality. The authors experimentally demonstrate that the SB-VAE and its semi-supervised variant learn highly discriminative latent representations that often outperform the Gaussian VAE's.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and interesting problem of extending SGVB to Bayesian nonparametric processes, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear and well-structured introduction to the background material, including Variational Autoencoders, Stochastic Gradient Variational Bayes, and Stick-Breaking Processes. The authors then propose a novel modification to the VAE, using the Kumaraswamy distribution as an approximate posterior, and demonstrate its effectiveness in experiments. The results show that the SB-VAE's latent space better captures class structure and provides beneficial regularization for semi-supervised learning.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of assembling the stick segments and how it affects the overall performance of the SB-VAE. Additionally, it would be interesting to see more experiments on different datasets and tasks to further demonstrate the effectiveness of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors: (1) How do the authors plan to extend the proposed approach to full Dirichlet processes with non-trivial base measures? (2) Can the authors provide more insights on how the SB-VAE's latent space captures class structure and provides beneficial regularization for semi-supervised learning? (3) How does the choice of the concentration parameter Î±0 affect the performance of the SB-VAE, and are there any guidelines for selecting this parameter in practice?