The paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by introducing an intermediate model, called Simplified-SFNN, which approximates certain SFNNs by simplifying their upper latent units above stochastic ones. The authors establish a connection between three models: DNN → Simplified-SFNN → SFNN, leading to an efficient training procedure for stochastic models utilizing pre-trained parameters of DNNs.
The paper tackles the specific question of developing efficient training methods for large-scale SFNNs, which is a well-motivated problem given the advantages of SFNNs over deterministic deep neural networks (DNNs). The approach is well-placed in the literature, building upon existing work on SFNNs and DNNs.
The paper supports its claims through theoretical analysis and empirical results on various tasks, including multi-modal learning and classification. The authors provide a rigorous proof of the connection between DNNs and Simplified-SFNNs, and demonstrate the effectiveness of their approach through experiments on several datasets, including MNIST, TFD, CIFAR-10, CIFAR-100, and SVHN.
Based on the analysis, I decide to Accept this paper. The key reasons for this choice are:
1. The paper proposes a novel and well-motivated approach to training SFNNs, which addresses a significant challenge in the field.
2. The authors provide a rigorous theoretical analysis of their approach, including a proof of the connection between DNNs and Simplified-SFNNs.
3. The empirical results demonstrate the effectiveness of the approach on various tasks and datasets.
To further improve the paper, I provide the following feedback:
* The authors could provide more details on the computational complexity of their approach, including the time and memory requirements for training Simplified-SFNNs.
* The paper could benefit from a more detailed comparison with existing methods for training SFNNs, including a discussion of the advantages and limitations of each approach.
* The authors may want to consider exploring the application of their approach to other tasks and domains, such as natural language processing or computer vision.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the choice of hyper-parameters, such as the number of samples used for estimating the expectations in Simplified-SFNNs?
* How do the authors plan to extend their approach to more complex SFNN architectures, such as those with multiple stochastic layers or recursive connections?
* Can you provide more insights into the regularization effect of Simplified-SFNNs, including a discussion of how the stochastic nature of the model affects the learning process?