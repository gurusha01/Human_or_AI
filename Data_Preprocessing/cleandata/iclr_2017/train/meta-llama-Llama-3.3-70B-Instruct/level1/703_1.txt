Summary
The paper presents Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs) that exploits the variable per-layer precision requirements of DNNs to deliver execution time proportional to the precision used per layer. TRT uses hybrid bit-serial/bit-parallel functional units and achieves significant performance and energy efficiency improvements over state-of-the-art bit-parallel accelerators. The paper evaluates TRT on a set of convolutional neural networks (CNNs) for image classification and demonstrates its ability to trade off accuracy for additional improvements in execution performance and energy efficiency.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and important problem in the field of DNN acceleration, and (2) the approach is well-motivated and supported by thorough evaluations and analysis.
Supporting Arguments
The paper provides a clear and detailed explanation of the TRT architecture and its advantages over existing bit-parallel accelerators. The evaluation results demonstrate significant performance and energy efficiency improvements over state-of-the-art accelerators, and the paper provides a thorough analysis of the trade-offs between accuracy, performance, and energy efficiency. The authors also provide a comprehensive review of related work and discuss the limitations and potential applications of TRT.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation and synthesis of the TRT architecture, including the specific technology node and design constraints used. Additionally, it would be interesting to see more results on the application of TRT to other types of neural networks and machine learning algorithms. The authors may also want to consider discussing the potential applications of TRT in edge computing and IoT devices, where energy efficiency and performance are critical.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the bit-serial loading of weights into processing units and how it facilitates the processing of fully-connected layers?
2. How do you plan to extend TRT to support other types of neural networks and machine learning algorithms?
3. What are the potential applications of TRT in edge computing and IoT devices, and how do you see TRT enabling new use cases in these domains?