Summary
The paper proposes a framework for multitask deep reinforcement learning guided by policy sketches. The approach associates each subtask with a modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating that it outperforms standard baselines and naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.
Decision
I decide to Accept this paper with minor revisions. The paper tackles a specific and well-motivated problem, and the approach is well-placed in the literature. The authors provide a clear and detailed explanation of their method, and the experimental results demonstrate the effectiveness of their approach.
Supporting Arguments
The paper addresses a key challenge in reinforcement learning, which is the ability to learn policies that can generalize across multiple tasks. The authors' approach is well-motivated, as it leverages the structural information provided by policy sketches to guide the learning process. The experimental results demonstrate that the approach outperforms standard baselines and can adapt to new tasks with minimal additional learning.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the implementation of the subpolicies and the critic. Additionally, it would be helpful to include more analysis on the learned subpolicies and how they are combined to form task-specific policies. Furthermore, the authors could provide more discussion on the potential applications of their approach and how it can be extended to more complex tasks.
Questions for the Authors
1. Can you provide more details on how the subpolicies are implemented and how the parameters are tied across shared subpolicies?
2. How do the authors ensure that the learned subpolicies are reusable and interpretable?
3. Can you provide more analysis on the learned subpolicies and how they are combined to form task-specific policies?
4. How do the authors plan to extend their approach to more complex tasks and larger state and action spaces?