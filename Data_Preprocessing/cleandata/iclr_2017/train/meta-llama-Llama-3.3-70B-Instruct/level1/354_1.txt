The paper "Snapshot Ensembling: A Simple Method for Ensembling Neural Networks without Additional Training" proposes a novel approach to obtain ensembles of neural networks without incurring any additional training cost. The authors achieve this by training a single neural network, converging to several local minima along its optimization path, and saving the model parameters. They leverage recent work on cyclic learning rate schedules to obtain repeated rapid convergence.
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated and placed in the literature, addressing the problem of high training costs associated with traditional ensembling methods. Secondly, the paper provides strong empirical evidence to support its claims, demonstrating the effectiveness of Snapshot Ensembling on several benchmark datasets and comparing favorably with state-of-the-art single models and traditional network ensembles.
The authors provide a clear and concise explanation of their method, which is simple yet effective. They also conduct a thorough analysis of the diversity of model ensembles, showing that the cyclic learning rate schedule creates snapshots that are not only accurate but also diverse with respect to model predictions. The experimental results are impressive, with Snapshot Ensembles achieving lower error rates than state-of-the-art single models on several datasets, including CIFAR-10, CIFAR-100, and SVHN.
To further improve the paper, I suggest that the authors provide more insights into the choice of hyperparameters, such as the number of cycles and the learning rate schedule. Additionally, it would be interesting to see more analysis on the trade-off between model diversity and optimization, as well as the potential applications of Snapshot Ensembling in other domains.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors choose the number of cycles and the learning rate schedule? What is the effect of varying the number of cycles and the learning rate schedule on the performance of Snapshot Ensembling? How do the authors plan to combine Snapshot Ensembling with traditional ensembles in future work? What are the potential limitations and challenges of applying Snapshot Ensembling to other domains or tasks?