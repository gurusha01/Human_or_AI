The paper "L-SR1: A New Second Order Method for Training Deep Neural Networks" presents a novel second-order optimization method, L-SR1, which addresses the challenges of training deep neural networks using second-order methods. The authors propose a limited-memory symmetric rank-one (SR1) update with a trust region approach, which overcomes the limitations of traditional quasi-Newton methods such as L-BFGS. The paper provides a thorough analysis of the method, including its motivation, algorithmic details, and experimental results on several benchmark datasets.
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in the field of deep learning, namely the challenge of training deep neural networks using second-order methods. The authors provide a clear and well-motivated approach to addressing this problem, which is well-placed in the literature. Secondly, the paper provides a thorough and rigorous evaluation of the proposed method, including experimental results on several benchmark datasets, which demonstrate the effectiveness of L-SR1 compared to other first and second-order methods.
The paper supports its claims through a combination of theoretical analysis and empirical results. The authors provide a detailed description of the L-SR1 algorithm, including its limited-memory implementation and trust region approach. The experimental results demonstrate the effectiveness of L-SR1 on several benchmark datasets, including MNIST and CIFAR10, and show that it outperforms other second-order methods such as L-BFGS. The results also demonstrate the robustness of L-SR1 to hyperparameter tuning and its ability to handle large mini-batch sizes, which is an important consideration for distributed training of deep neural networks.
To further improve the paper, I would like to see more analysis on the computational complexity of L-SR1 and its scalability to larger datasets. Additionally, it would be interesting to see a comparison of L-SR1 with other recent stochastic second-order methods, such as those proposed in [Byrd et al., 2016] and [Mokhtari & Ribeiro, 2015]. I would also like the authors to clarify the following points: (1) How does the choice of trust region radius affect the performance of L-SR1? (2) Can the authors provide more insight into the behavior of L-SR1 on residual networks, particularly with regards to the bouncing behavior of the test loss? (3) How does L-SR1 handle non-convex optimization problems, and are there any guarantees on its convergence?