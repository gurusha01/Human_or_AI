This paper presents a novel approach to transferring skills between morphologically different agents using invariant feature spaces. The authors formulate a problem where two agents learn multiple skills by sharing information and introduce a method that uses the skills learned by both agents to train invariant feature spaces. These feature spaces can then be used to transfer other skills from one agent to another.
I decide to accept this paper for several reasons. Firstly, the approach is well-motivated and placed in the literature, building upon existing work in transfer learning and reinforcement learning. The authors provide a clear problem formulation and assumptions, making it easy to understand the context and limitations of their method. Secondly, the paper supports its claims with empirical results, demonstrating the effectiveness of the proposed method in transferring skills between simulated robotic arms with different numbers of links and actuation mechanisms.
The key strength of this paper is its ability to learn a common feature space that can be used to transfer information between agents with different morphologies. The authors demonstrate that their method can accelerate learning in the target domain, even in the presence of sparse rewards. The use of deep neural networks to represent the mappings between the state spaces of the two agents is a significant contribution, as it allows for a more expressive and flexible mapping than traditional methods.
To improve the paper, I would suggest that the authors provide more details on the reinforcement learning algorithm used to learn the policies in the target domain. Additionally, it would be interesting to see more experiments on transferring skills between agents with more significant morphological differences, such as between a robotic arm and a legged robot. The authors could also explore the use of their method in more complex tasks, such as manipulation tasks that require multiple skills to be transferred.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) How do the authors choose the proxy tasks used to learn the common feature space? (2) Can the authors provide more details on the architecture of the neural networks used to represent the mappings between the state spaces? (3) How do the authors handle situations where the two agents have different action spaces or dynamics? 
Overall, this paper presents a significant contribution to the field of transfer learning and reinforcement learning, and I believe it has the potential to impact the development of more efficient and flexible learning algorithms for autonomous agents.