The paper presents a novel approach to mine full-fledged logical theories from relational embeddings, which are low-dimensional representations of knowledge bases. The authors propose to cast the semantics of logical operators into the space of embeddings, allowing for the mining of rules that support all propositional logic connectives, including conjunction, disjunction, and negation. The approach is based on a sparse reconstruction procedure, which is solved using compressed sensing algorithms. The paper claims to offer two key advantages: automatic modeling of inter-dependency between rules and support for all propositional logic connectives.
I decide to accept this paper, with the key reason being that the approach is well-motivated and well-placed in the literature. The authors provide a clear and concise overview of the background material, including relational embeddings, knowledge bases, and theory learning. They also discuss the limitations of existing approaches, such as classical Inductive Logic Programming (ILP) methods and previous embedding-based methods.
The paper supports its claims through empirical analysis on four publicly available knowledge bases, demonstrating the advantages of the proposed approach over existing methods. The results show that the proposed method, Feature Rule Miner (FRM), outperforms two variants of the kNN-based theory miner of Yang et al. (2015) in terms of F-score and per-rule recall. The authors also provide an example of mined rules, which illustrates the power of negation in the proposed approach.
To improve the paper, I suggest that the authors provide more details on the implementation of the compressed sensing algorithms and the choice of hyperparameters. Additionally, it would be helpful to include more discussion on the scalability of the approach and its potential applications to larger knowledge bases. Some questions I would like the authors to answer include: How do the authors plan to address the issue of complete enumeration of candidate rule bodies, which is exponential in rule length? How do the authors choose the threshold value τ for converting the coefficients α into a logic theory? What are the potential applications of the proposed approach to other areas, such as natural language processing or computer vision? 
Overall, the paper presents a novel and well-motivated approach to mining logical theories from relational embeddings, with promising empirical results. With some additional details and discussion, the paper has the potential to make a significant contribution to the field of knowledge base completion and theory learning.