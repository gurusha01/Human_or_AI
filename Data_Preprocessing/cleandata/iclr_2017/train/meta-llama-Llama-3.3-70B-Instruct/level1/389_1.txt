Summary
The paper proposes a novel model, SampleRNN, for unconditional audio generation based on generating one audio sample at a time. The model combines memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure to capture underlying sources of variations in temporal sequences over long time spans. The authors demonstrate the effectiveness of their model on three datasets of different nature and show that it outperforms competing models, including WaveNet, in terms of human evaluation.
Decision
I decide to accept this paper with minor revisions. The two key reasons for this choice are: (1) the paper tackles a specific and well-motivated problem in audio generation, and (2) the approach is well-supported by empirical results and human evaluation.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of audio generation and the limitations of existing approaches. The proposed model, SampleRNN, is well-designed and effectively addresses the challenges of modeling long-term dependencies in audio data. The authors provide extensive experimental results, including comparisons with competing models, and demonstrate the effectiveness of their approach through human evaluation. The paper also provides a thorough analysis of the contributions of each component of the model, which helps to understand the strengths and limitations of the approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the WaveNet model, as the comparison with WaveNet is a key aspect of the paper. Additionally, it would be helpful to include more visualizations of the generated audio samples to better illustrate the quality of the results. Finally, the authors may want to consider providing more discussion on the potential applications of their approach beyond audio generation, such as speech synthesis or music synthesis.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the hyperparameter search process and how the optimal hyperparameters were selected?
* How do you plan to address the issue of mode collapse in the generated audio samples, which is a common problem in generative models?
* Can you provide more insights on the computational resources required to train the SampleRNN model, and how it compares to other competing models?