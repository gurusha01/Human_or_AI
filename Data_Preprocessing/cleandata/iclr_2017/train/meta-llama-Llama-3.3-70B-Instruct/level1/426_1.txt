Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to learning bilingual word vectors offline, leveraging a linear transformation between two vector spaces that should be orthogonal. The authors introduce a new "inverted softmax" method for identifying translation pairs and demonstrate its effectiveness in improving the precision of word translations. They also show that orthogonal transformations are more robust to noise, enabling the use of pseudo-dictionaries compiled from identical character strings in both languages. The paper further extends the method to retrieve sentence translations from a corpus of sentences, achieving a high precision of 68%.
Decision and Key Reasons
Based on the review, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and well-motivated problem in the field of bilingual word vectors, providing a clear and concise introduction to the topic and its significance.
2. The approach is well-supported by theoretical insights and empirical results, demonstrating the effectiveness of the proposed method in improving translation precision and robustness to noise.
Supporting Arguments
The paper provides a thorough analysis of the problem, including a clear explanation of the limitations of existing approaches and the benefits of using orthogonal transformations. The authors also provide a detailed description of their method, including the use of singular value decomposition (SVD) to obtain the orthogonal transformation and the introduction of the "inverted softmax" method. The experimental results are comprehensive and well-presented, demonstrating the effectiveness of the proposed approach in various settings, including the use of expert training dictionaries and pseudo-dictionaries.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors consider providing more context on the potential applications of their approach, such as machine translation and language-agnostic text classification. Additionally, it would be helpful to include more details on the computational complexity of the proposed method and its scalability to larger datasets.
Some questions I would like the authors to address include:
* How do the authors plan to extend their approach to handle more diverse language pairs, where the number of identical character strings may be limited?
* Can the authors provide more insights into the relationship between the proposed method and other approaches to bilingual word vector learning, such as CCA and online learning methods?
* How do the authors plan to evaluate the quality of the learned bilingual word vectors, beyond precision and recall metrics?