The paper proposes a novel framework, Fine Grained Action Repetition (FiGAR), which enables Deep Reinforcement Learning (DRL) algorithms to learn temporal abstractions by predicting the number of time steps for which an action should be repeated. This framework is generic and can be applied to various DRL algorithms, including policy gradient methods and actor-critic methods.
The authors demonstrate the efficacy of FiGAR by extending three popular DRL algorithms: Asynchronous Advantage Actor Critic (A3C), Trust Region Policy Optimization (TRPO), and Deep Deterministic Policy Gradients (DDPG). They show that FiGAR can improve the performance of these algorithms in various domains, including Atari 2600 games, Mujoco simulated physics tasks, and the TORCS car racing domain.
I decide to Accept this paper for the following reasons:
1. The paper tackles a specific and well-defined problem in the field of reinforcement learning, which is learning temporal abstractions in DRL algorithms.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and how FiGAR addresses these limitations.
3. The paper provides empirical evidence to support the claims, with experiments demonstrating the effectiveness of FiGAR in improving the performance of various DRL algorithms in different domains.
To further improve the paper, I would like to provide the following feedback:
* The authors could provide more details on the hyperparameter tuning process for FiGAR, as well as the sensitivity of the results to different hyperparameter settings.
* The paper could benefit from a more detailed analysis of the learned temporal abstractions, including visualizations or examples of the learned policies.
* The authors could also explore the application of FiGAR to more complex domains, such as those with high-dimensional state or action spaces.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How does the choice of action repetition set W affect the performance of FiGAR, and are there any guidelines for selecting W in practice?
* Can the authors provide more insight into the learned policies, including the types of temporal abstractions that are learned and how they relate to the underlying dynamics of the environment?
* Are there any potential limitations or challenges of applying FiGAR to more complex domains, and how might these be addressed in future work?