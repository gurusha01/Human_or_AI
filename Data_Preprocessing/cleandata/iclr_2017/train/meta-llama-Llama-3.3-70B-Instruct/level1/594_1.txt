Summary
The paper proposes low-dimensional parametrizations for passthrough neural networks based on low-rank or low-rank plus diagonal decompositions of the matrices in the hidden layers. The authors argue that these parametrizations can reduce the number of parameters and memory requirements of the network while preserving its memory capacity. They experimentally evaluate their approach on various tasks, including sequential MNIST classification, language modeling, and memory tasks, and report competitive results.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated problem in deep learning, namely reducing the number of parameters and memory requirements of passthrough neural networks; and (2) the authors provide a thorough experimental evaluation of their approach on various tasks, demonstrating its effectiveness.
Supporting Arguments
The paper is well-motivated, as it addresses a significant challenge in training deep neural networks, particularly those with large state sizes. The authors provide a clear and concise overview of the problem and the proposed solution, making it easy to follow. The experimental evaluation is thorough, covering various tasks and comparing the results to state-of-the-art models. The authors also provide detailed descriptions of their models and experimental settings, making it possible to reproduce their results.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the trade-offs between the number of parameters, memory requirements, and computational complexity of their approach. Additionally, it would be interesting to see more comparisons with other low-rank or sparse neural network architectures. The authors may also consider providing more insights into the interpretability of their low-rank plus diagonal parametrization and its potential applications in other domains.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the computational complexity of your low-rank and low-rank plus diagonal parametrizations?
2. How do you choose the rank of the low-rank matrices, and what is the effect of different ranks on the performance of the network?
3. Have you considered applying your approach to other types of neural networks, such as convolutional neural networks or transformers?