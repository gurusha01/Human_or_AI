Summary
The paper proposes a new weight initialization technique that adjusts for the variance introduced by dropout, leading to faster and more accurate convergence in neural networks. The authors also introduce a simple method to re-estimate the Batch Normalization variance parameters after training, which improves test performance. The paper demonstrates the effectiveness of these techniques on various architectures, including fully connected and convolutional neural networks, and achieves state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision
I decide to Accept this paper, with the main reasons being the clear motivation and well-placed literature review, as well as the thorough empirical evaluation of the proposed techniques.
Supporting Arguments
The paper tackles a specific and important problem in deep learning, namely the variance introduced by dropout, and proposes a well-motivated solution. The authors provide a clear and concise derivation of their new weight initialization technique and demonstrate its effectiveness through experiments on various architectures. The paper also introduces a simple and general method to re-estimate the Batch Normalization variance parameters, which is shown to improve test performance. The empirical evaluation is thorough and well-designed, with a clear presentation of the results.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the theoretical implications of their proposed techniques and how they relate to existing work on variance stabilization. Additionally, it would be interesting to see more experiments on other datasets and architectures to further demonstrate the generality of the proposed methods.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more intuition on why the proposed weight initialization technique is effective in reducing the variance introduced by dropout?
2. How do the proposed techniques relate to existing work on variance stabilization, such as Batch Normalization and weight normalization?
3. Can you provide more details on the computational cost of the proposed methods, particularly the re-estimation of the Batch Normalization variance parameters?