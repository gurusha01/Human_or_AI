Summary
The paper proposes a transfer learning approach for sequence tagging tasks, leveraging the generality of deep neural networks to improve performance on target tasks with limited annotations. The authors design three neural network architectures for cross-domain, cross-application, and cross-lingual transfer, and demonstrate significant improvements on various datasets under low-resource conditions.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated and supported by thorough experiments. The paper tackles a specific and important problem in natural language processing, and the proposed transfer learning approach is novel and effective.
Supporting Arguments
The paper provides a clear and well-structured presentation of the problem, approach, and experiments. The authors demonstrate a good understanding of the related work and provide a thorough comparison with state-of-the-art systems. The experimental results are convincing, showing significant improvements on various datasets and achieving new state-of-the-art results on some benchmarks. The paper also provides a detailed analysis of the factors that affect the performance of the transfer learning approach, including label abundance, task relatedness, and parameter sharing.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the learned representations and how they transfer across tasks. Additionally, it would be interesting to see more analysis on the limitations of the approach and potential future directions. Some specific questions I would like the authors to address include: How do the learned representations change when transferring across tasks? Are there any specific tasks or domains where the approach does not work well? How can the approach be extended to other NLP tasks beyond sequence tagging?
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: Can you provide more details on the hyperparameter tuning process and how the hyperparameters were chosen? How do you handle out-of-vocabulary words in the target task, especially when transferring across domains or languages? Are there any plans to release the code and models used in the paper to facilitate reproducibility and further research?