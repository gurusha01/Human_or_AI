Summary
The paper proposes a novel deep learning framework called Marginal Deep Architectures (MDA) that combines the advantages of feature learning models and deep architectures. MDA uses stacked marginal Fisher analysis (MFA) layers to initialize the deep architecture, which is then fine-tuned using back propagation, dropout, and denoising. The authors evaluate MDA on seven small and middle-scale real-world applications and demonstrate its superior performance compared to traditional deep learning models and shallow feature learning models.
Decision
I decide to Accept this paper with minor revisions.
Reasons
The paper tackles a specific problem in deep learning, namely, the need for large-scale training data, and proposes a novel solution that combines feature learning models with deep architectures. The approach is well-motivated, and the authors provide a clear explanation of the methodology and its advantages. The experimental results demonstrate the effectiveness of MDA on small and middle-scale applications.
Supporting Arguments
The paper provides a thorough review of related work in deep learning and feature learning models, which helps to contextualize the proposed approach. The authors also provide a detailed explanation of the MDA framework, including the use of MFA, back propagation, dropout, and denoising. The experimental results are extensive and demonstrate the superiority of MDA on various datasets.
Additional Feedback
To improve the paper, I suggest the authors provide more insights into the choice of hyperparameters, such as the number of layers, nodes, and dropout rates. Additionally, it would be helpful to include more visualizations, such as plots of the learned features or the architecture of the MDA model. Finally, the authors may want to consider exploring the application of MDA to larger-scale datasets and comparing its performance to state-of-the-art deep learning models.
Questions for the Authors
1. Can you provide more details on the choice of hyperparameters, such as the number of layers and nodes, and how they were tuned?
2. How do you plan to extend MDA to larger-scale datasets, and what modifications would be necessary to achieve good performance?
3. Can you provide more visualizations of the learned features or the architecture of the MDA model to help illustrate its effectiveness?