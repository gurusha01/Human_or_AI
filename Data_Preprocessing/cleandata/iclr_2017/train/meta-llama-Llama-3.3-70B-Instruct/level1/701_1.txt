Summary
The paper presents a novel approach to transfer learning, called the neuro-modular approach, which combines pre-trained neural networks with new, untrained networks to learn the shift in distributions between data sets. This approach allows the network to retain the original representational power of the pre-trained network while learning new features that complement the pre-trained network. The authors demonstrate the effectiveness of this approach on several datasets, including MNIST, CIFAR-10, CIFAR-100, Stanford Cars, and IMDB sentiment, and show that it outperforms traditional fine-tuning methods, particularly when the amount of training data is small.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The paper provides a clear and concise explanation of the neuro-modular approach and demonstrates its effectiveness on several datasets.
Supporting Arguments
The paper tackles a specific and important problem in machine learning, namely, transfer learning with small amounts of data. The approach is well-motivated, building on existing work in transfer learning and modular neural networks. The authors provide a clear and concise explanation of the approach and demonstrate its effectiveness on several datasets, including image classification and text classification tasks. The results show that the neuro-modular approach outperforms traditional fine-tuning methods, particularly when the amount of training data is small.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the training procedures and hyperparameter settings used for each experiment. Additionally, it would be helpful to include more visualizations of the learned features and representations to provide a better understanding of how the neuro-modular approach works. Finally, the authors may want to consider exploring more efficient architectures and training strategies, as mentioned in the conclusion.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the pre-trained networks are selected and how the new networks are initialized?
2. How do you determine the optimal number of modules and the architecture of each module?
3. Can you provide more insights into the learned features and representations of the neuro-modular approach, and how they differ from those learned by traditional fine-tuning methods?