Summary
The paper proposes a novel layer augmentation technique that facilitates the optimization of deep networks by making identity mappings easy to learn. The technique adds shortcut connections with a linear gating mechanism, allowing layers to degenerate into identity mappings by optimizing only one parameter. The authors demonstrate the effectiveness of their approach on various architectures, including plain and residual networks, and achieve state-of-the-art results on CIFAR-10 and CIFAR-100.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by thorough experiments. The paper provides a clear and concise explanation of the proposed technique, and the results demonstrate its effectiveness in improving the performance and robustness of deep networks.
Supporting Arguments
The paper tackles a specific and important problem in deep learning, namely the difficulty of optimizing deep networks. The approach is well-motivated, building upon the ideas of Highway Neural Networks and Residual Networks. The authors provide a thorough analysis of the proposed technique, including its theoretical intuition and experimental evaluation. The results demonstrate the effectiveness of the approach in improving the performance and robustness of deep networks, including plain and residual networks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the interpretation of the k parameter and its relationship to the level of refinement performed by each layer. Additionally, it would be interesting to see more experiments on larger datasets, such as ImageNet, to further demonstrate the effectiveness of the proposed technique. Furthermore, the authors could provide more discussion on the potential applications of the proposed technique, such as layer pruning and knowledge distillation.
Questions for the Authors
1. Can you provide more insight into the relationship between the k parameter and the level of refinement performed by each layer?
2. How do you plan to extend the proposed technique to larger datasets, such as ImageNet?
3. Can you provide more discussion on the potential applications of the proposed technique, such as layer pruning and knowledge distillation?
4. How does the proposed technique compare to other methods for improving the optimization of deep networks, such as batch normalization and gradient clipping?