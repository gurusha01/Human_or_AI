Summary of the Paper
The paper proposes a neural network architecture and statistical framework for modeling frames in videos using principles inspired by computer graphics pipelines. The approach explicitly represents "sprites" or percepts inferred from the maximum likelihood of the scene and infers their movement independently of their content. The authors introduce a family of architectures called Perception Updating Networks (PUNs) that can optimize a variational lower bound, which decouples sprites and their dynamics in a video.
Decision
I decide to Accept this paper with minor revisions.
Reasons for the Decision
The paper tackles a specific and interesting problem in video modeling, and the approach is well-motivated and grounded in the literature. The authors provide a clear and detailed explanation of their statistical framework and architecture, and the experimental results demonstrate the effectiveness of their approach.
Supporting Arguments
The paper provides a thorough review of the literature on video modeling and generative models, and the authors clearly explain how their approach differs from existing methods. The experimental results on synthetic datasets and the Moving MNIST benchmark demonstrate the ability of the PUNs to generate interpretable and realistic videos. The authors also provide a detailed analysis of the strengths and limitations of their approach, including the importance of architectural constraints in decoupling the "what" and "where" representations.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the computational resources required to train the PUNs. Additionally, it would be helpful to include more visualizations of the learned sprites and their dynamics, as well as a more detailed comparison with other state-of-the-art video modeling approaches.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the spatial transformer network (STN) and how it is used in the PUNs?
2. How do you handle cases where the sprites or objects in the video are occluded or partially visible?
3. Can you provide more information on the computational resources and training time required to train the PUNs on the Moving MNIST benchmark?