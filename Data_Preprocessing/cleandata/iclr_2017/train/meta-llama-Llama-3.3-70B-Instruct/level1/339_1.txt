Summary
The paper proposes a novel extension to neural network language models, called the Neural Cache Model, which adapts the prediction to the recent history by storing past hidden activations as memory and accessing them through a dot product with the current hidden activation. This approach is a simplified version of memory-augmented networks and is shown to be efficient and scalable to large memory sizes. The authors demonstrate the effectiveness of their approach on several language model datasets and the LAMBADA dataset, achieving significant performance gains over recent memory-augmented networks.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear explanation of the Neural Cache Model and its relationship to existing memory-augmented neural networks and cache models. The experimental results demonstrate the effectiveness of the approach in improving language modeling performance, particularly on the LAMBADA dataset.
Supporting Arguments
The paper tackles a specific question of adapting neural network language models to the recent history, which is a well-known limitation of traditional neural networks. The approach is well-placed in the literature, drawing on existing work on memory-augmented neural networks and cache models. The authors provide a clear and concise explanation of the Neural Cache Model, making it easy to understand and replicate. The experimental results are thorough and demonstrate the effectiveness of the approach on several datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the computational efficiency of the Neural Cache Model compared to other memory-augmented neural networks. Additionally, it would be interesting to see more experiments on the sensitivity of the model to hyperparameters, such as the cache size and the interpolation parameter. Finally, the authors may want to consider discussing potential applications of the Neural Cache Model beyond language modeling, such as in dialogue systems or machine translation.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the Neural Cache Model handle out-of-vocabulary words, and are there any plans to extend the model to handle this case?
* Can the authors provide more details on the training procedure, such as the number of epochs and the batch size used in the experiments?
* How does the Neural Cache Model compare to other memory-augmented neural networks in terms of computational efficiency and scalability?