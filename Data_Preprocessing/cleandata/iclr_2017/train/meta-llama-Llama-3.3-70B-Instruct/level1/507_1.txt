Summary
The paper proposes a language-agnostic approach to generating sets of semantically similar clusters of entities along with sets of "outlier" elements, which can be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. The authors create a gold-standard dataset, WikiSem500, using their methodology and evaluate multiple state-of-the-art embeddings on this dataset. The results show a correlation between performance on this dataset and performance on sentiment analysis.
Decision
I decide to Accept this paper with minor revisions. The main reasons for this decision are that the paper tackles a specific and well-motivated problem, the approach is well-placed in the literature, and the paper supports its claims with empirical results.
Supporting Arguments
The paper clearly identifies the limitations of existing word similarity tasks and proposes a novel approach to address these limitations. The authors provide a thorough review of related work and demonstrate a good understanding of the current state of research in this area. The evaluation of the proposed approach is rigorous and well-designed, and the results are convincing. The paper also provides a clear and detailed description of the methodology used to create the WikiSem500 dataset, which is a significant contribution to the field.
Additional Feedback
To improve the paper, I suggest that the authors provide more analysis of the results and discuss the implications of their findings in more detail. Additionally, it would be helpful to include more examples of the clusters and outliers in the WikiSem500 dataset to illustrate the quality and diversity of the data. Finally, the authors may want to consider evaluating their approach on other downstream tasks, such as question answering or text classification, to further demonstrate its effectiveness.
Questions for the Authors
1. Can you provide more details on how you selected the specific languages (English, Spanish, German, Chinese, and Japanese) for the WikiSem500 dataset?
2. How do you plan to address the issue of out-of-vocabulary words in the WikiSem500 dataset, and what are the implications of this issue for the evaluation of word embeddings?
3. Can you discuss the potential applications of the WikiSem500 dataset beyond the evaluation of word embeddings, such as in natural language processing tasks or information retrieval?