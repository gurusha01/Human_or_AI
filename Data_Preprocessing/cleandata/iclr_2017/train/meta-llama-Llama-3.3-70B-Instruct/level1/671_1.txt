Summary
The paper presents a novel framework for constructing recurrent neural architectures, called Dynamic Recurrent Acyclic Graphical Neural Networks (DRAGNN). The framework is based on a new generic unit, the Transition-Based Recurrent Unit (TBRU), which allows connections between nodes in an unrolled compute graph to be specified dynamically. The authors demonstrate the effectiveness of DRAGNN on two NLP tasks: dependency parsing and extractive sentence summarization, showing that it outperforms traditional seq2seq models with attention mechanisms.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The paper provides a clear and concise explanation of the framework, and the experiments demonstrate the effectiveness of DRAGNN in improving the accuracy and efficiency of dependency parsing and extractive sentence summarization.
Supporting Arguments
The paper tackles a specific question of how to represent input and output structures in deep learning models, and the approach is well-placed in the literature. The authors provide a thorough review of related work and clearly explain how DRAGNN differs from existing approaches. The empirical results are convincing, showing that DRAGNN outperforms traditional seq2seq models with attention mechanisms on two NLP tasks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of DRAGNN, such as the specific hyperparameters used in the experiments. Additionally, it would be helpful to include more visualizations of the DRAGNN architecture and the TBRU units to facilitate understanding. The authors may also consider providing more analysis on the computational cost of DRAGNN compared to traditional seq2seq models.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the TBRU units are trained, and how the recurrence function r(s) is learned?
2. How do you handle the case where the input structure is not explicitly mapped to the output structure?
3. Can you provide more insights on how DRAGNN can be applied to other NLP tasks, such as machine translation and question answering?