This paper proposes a novel approach to exploration in reinforcement learning (RL) by extending classic count-based methods to high-dimensional state spaces through hashing. The authors demonstrate that a simple generalization of count-based exploration can achieve near state-of-the-art performance on various high-dimensional and continuous deep RL benchmarks.
The specific question tackled by the paper is how to balance exploration and exploitation in high-dimensional state spaces, where most states will only occur once. The approach is well-motivated, building on the idea of count-based exploration methods, which have been shown to perform near-optimally in small discrete Markov decision processes (MDPs). The authors provide a clear and thorough explanation of the methodology, including the use of locality-sensitive hashing (LSH) to convert continuous, high-dimensional data to discrete hash codes.
The paper supports its claims through extensive experiments on various deep RL benchmarks, including rllab and Atari 2600 games. The results demonstrate that the proposed method, TRPO-SimHash, can achieve significant performance gains over baseline methods, such as TRPO with Gaussian control noise. The authors also provide a detailed analysis of the factors that contribute to good performance, including the choice of hash function, granularity, and the importance of encoding relevant information for solving the MDP.
Based on the results and analysis presented in the paper, I decide to Accept this paper. The key reasons for this choice are:
1. The paper proposes a novel and well-motivated approach to exploration in high-dimensional state spaces, which addresses a significant challenge in RL.
2. The experimental results demonstrate the effectiveness of the proposed method, achieving near state-of-the-art performance on various benchmarks.
To further improve the paper, I provide the following feedback:
* The authors could provide more discussion on the relationship between the proposed method and other exploration strategies, such as intrinsic motivation and curiosity-driven exploration.
* The paper could benefit from more analysis on the robustness of the proposed method to hyperparameter changes and the choice of hash function.
* The authors may want to consider providing more visualizations or examples to illustrate the effectiveness of the proposed method in different scenarios.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How does the choice of hash function affect the performance of the proposed method, and are there any guidelines for selecting a suitable hash function for a given problem?
* Can the authors provide more insight into the trade-off between exploration and exploitation in high-dimensional state spaces, and how the proposed method balances these two competing objectives?
* Are there any potential limitations or challenges of applying the proposed method to more complex or real-world problems, and how might these be addressed in future work?