This paper proposes a novel approach to network quantization, a technique used to reduce the memory requirements of deep neural networks. The authors tackle the specific problem of minimizing the performance loss due to quantization, given a compression ratio constraint. They analyze the quantitative relation between quantization errors and the neural network loss function, identifying the Hessian-weighted distortion measure as a relevant objective function for network quantization.
The approach is well-motivated, building upon existing work in network compression and quantization. The authors provide a clear and thorough explanation of their methodology, including the derivation of the Hessian-weighted distortion measure and the proposal of Hessian-weighted k-means clustering for network quantization. They also establish a connection between the network quantization problem and the entropy-constrained scalar quantization (ECSQ) problem in information theory, leading to the development of two efficient heuristic solutions: uniform quantization and an iterative algorithm similar to Lloyd's algorithm.
The paper supports its claims through extensive experiments on various neural network architectures, including LeNet, ResNet, and AlexNet. The results demonstrate the effectiveness of the proposed network quantization schemes in achieving significant compression ratios while maintaining acceptable performance. The authors also provide a thorough analysis of the results, discussing the benefits and limitations of their approach.
I decide to Accept this paper, with the primary reason being the novelty and significance of the proposed approach to network quantization. The paper provides a thorough and well-motivated analysis of the problem, and the experimental results demonstrate the effectiveness of the proposed solutions.
To further improve the paper, I suggest the authors provide more insight into the computational complexity of their approach, particularly in terms of Hessian computation and the iterative algorithm for ECSQ. Additionally, it would be beneficial to explore the applicability of their approach to other neural network architectures and tasks, such as natural language processing and speech recognition.
Some questions I would like the authors to address in their response include:
* Can you provide more details on the computational complexity of Hessian computation and the iterative algorithm for ECSQ?
* How do you plan to extend your approach to other neural network architectures and tasks?
* Have you considered exploring other objective functions for network quantization, such as those based on gradient norms or other measures of parameter importance?