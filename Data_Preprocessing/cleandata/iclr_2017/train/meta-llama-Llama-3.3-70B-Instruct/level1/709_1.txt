Summary of the Paper
The paper presents a novel unsupervised pretraining method to improve sequence-to-sequence (seq2seq) learning. The approach involves pretraining two language models in the source and target domains and initializing the embeddings, first LSTM layers, and softmax of a seq2seq model with the weights of the language models. The authors demonstrate the effectiveness of their method on machine translation and abstractive summarization tasks, achieving state-of-the-art results on WMT'14 and WMT'15 English-to-German translation tasks and surpassing the supervised learning baseline on the CNN/Daily Mail summarization task.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by strong empirical results. The paper provides a clear and concise explanation of the method, and the experiments demonstrate significant improvements over baseline models.
Supporting Arguments
The paper tackles a specific and important problem in seq2seq learning, namely the issue of overfitting when the amount of supervised training data is small. The approach is well-motivated, building on the idea of using pretraining to improve the performance of neural networks. The authors provide a thorough analysis of the method, including ablation studies and comparisons to other related work. The results are impressive, with significant improvements over baseline models on both machine translation and summarization tasks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the computational resources required to train the models. Additionally, it would be interesting to see more analysis on the types of errors made by the pretrained models versus the non-pretrained models. Some questions I would like the authors to answer include: How do the pretrained models perform on other languages and tasks? Can the method be applied to other types of neural network architectures? How does the method compare to other unsupervised pretraining methods, such as autoencoders or generative adversarial networks?