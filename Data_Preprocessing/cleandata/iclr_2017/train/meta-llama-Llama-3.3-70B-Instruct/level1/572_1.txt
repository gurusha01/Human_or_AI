The paper "Adversarial Examples for Generative Models" explores methods of producing adversarial examples on deep generative models such as the Variational Autoencoder (VAE) and the VAE-GAN. The authors propose three classes of attacks on these architectures and demonstrate them against networks trained on MNIST, SVHN, and CelebA datasets. 
I decide to accept this paper with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem of generating adversarial examples for generative models, which is a novel and important area of research. Secondly, the approach is well-motivated and placed in the literature, with a clear explanation of the background and related work on adversarial examples and generative models.
The paper supports its claims with extensive experimental results, demonstrating the effectiveness of the proposed attacks on different datasets and models. The authors also provide a thorough analysis of the results, discussing the strengths and weaknesses of each attack method and the implications of their findings. The evaluation metrics used, such as ASignore-target and AStarget, provide a clear measure of the attack's success.
To improve the paper, I would suggest that the authors provide more discussion on the potential defenses against these attacks and the implications of their findings for the development of more robust generative models. Additionally, it would be interesting to see more analysis on the transferability of the attacks across different models and datasets.
I have several questions for the authors to clarify their work: 
1. Can you provide more details on the computational resources required to generate the adversarial examples, and how this might impact the feasibility of these attacks in practice?
2. How do the attacks perform on other generative models, such as Generative Adversarial Networks (GANs) or Normalizing Flows?
3. Can you discuss potential applications of these attacks, such as in data augmentation or adversarial training, and how they might be used to improve the robustness of generative models?