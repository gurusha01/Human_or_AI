Summary of the Paper
The paper introduces NewsQA, a large-scale machine comprehension dataset consisting of over 100,000 question-answer pairs based on news articles from CNN. The dataset is designed to require reasoning and synthesis of information, making it a challenging benchmark for machine comprehension models. The authors collect the dataset through a four-stage process involving crowdworkers and provide a thorough analysis of the dataset's characteristics, including answer types and reasoning types required to solve the questions.
Decision
I decide to Accept this paper, with the main reasons being the high quality of the dataset and the thorough analysis provided. The paper presents a well-motivated approach to creating a challenging machine comprehension dataset, and the results demonstrate a significant performance gap between humans and machines, indicating the potential for future research.
Supporting Arguments
The paper is well-written, and the authors provide a clear motivation for creating the NewsQA dataset. The dataset collection process is well-designed, and the analysis of the dataset's characteristics is thorough and informative. The results demonstrate the challenging nature of the dataset, and the comparison with human performance highlights the potential for future research. The paper also provides a detailed description of the baseline models and their performance, which will be useful for future researchers.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the crowdworkers' instructions and the validation process. Additionally, it would be helpful to include more examples of the questions and answers in the dataset to illustrate the types of reasoning required. The authors may also consider providing more analysis on the performance of the baseline models, such as error analysis or ablation studies, to better understand the challenges of the dataset.
Questions for the Authors
I would like the authors to clarify the following points:
1. How did the authors ensure the quality of the crowdworkers' work, and what measures were taken to prevent biases in the dataset?
2. Can the authors provide more details on the validation process, such as how the validation workers were selected and what criteria were used to evaluate the answers?
3. How do the authors plan to maintain and update the NewsQA dataset, and will it be made available for future research?