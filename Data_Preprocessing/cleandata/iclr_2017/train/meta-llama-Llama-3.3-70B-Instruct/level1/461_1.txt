1. The paper claims to present a simple and efficient method for training deep neural networks in a semi-supervised setting, where only a small portion of training data is labeled. The authors introduce self-ensembling, which forms a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs and under different regularization and input augmentation conditions.
2. I decide to accept this paper with two key reasons for this choice: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
3. The approach is well-motivated as it extends the idea of ensemble methods, which have been shown to generally yield better predictions than a single network. The authors also provide a clear explanation of how their method works and how it relates to previous work in semi-supervised learning. The paper supports its claims with impressive results on two standard semi-supervised learning benchmarks, reducing the classification error rate significantly. The results are also scientifically rigorous, as the authors provide a detailed description of their experimental setup and training parameters.
Additional feedback to improve the paper includes: 
- Providing more analysis on the hyperparameters used in the experiments, such as the choice of α in temporal ensembling and the ramp-up period for the unsupervised loss component.
- Investigating the use of other types of regularization and input augmentation techniques to further improve the results.
- Exploring the application of self-ensembling to other domains, such as natural language processing or speech recognition.
- Providing more visualizations, such as plots of the network's performance over time, to help illustrate the effectiveness of the method.
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
- Can you provide more details on how the ensemble predictions are combined in temporal ensembling, and how the momentum term α affects the results?
- How do the authors choose the hyperparameters, such as the number of epochs and the learning rate, and are these choices specific to the datasets used in the experiments?
- Are there any limitations or potential drawbacks to using self-ensembling, such as increased computational cost or sensitivity to hyperparameters?