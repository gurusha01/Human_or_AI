Summary
The paper proposes a novel attention-based framework for sentiment analysis, inspired by human reading behavior. The model uses a bidirectional LSTM (Bi-LSTM) network to extract a global context representation, which is then used as attention to selectively focus on important local contexts. The authors also introduce a simplified single-scan approach, which achieves comparable results with reduced model complexity. The experimental results demonstrate the effectiveness of the proposed model, outperforming existing related models on several benchmark datasets.
Decision
I decide to Accept this paper, with two key reasons: (1) the proposed model is well-motivated and grounded in the literature, addressing a significant problem in sentiment analysis; and (2) the experimental results are convincing, demonstrating the model's effectiveness on multiple datasets.
Supporting Arguments
The paper provides a clear and thorough introduction to the problem of sentiment analysis and the limitations of existing models. The proposed model is well-designed, and the use of global context attention to incorporate local contexts is a novel and effective approach. The experimental results are comprehensive, including comparisons with multiple baseline models and attention visualization to illustrate the model's behavior.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points: (1) provide more detailed analysis of the attention weights and their correlation with the model's performance; (2) explore the applicability of the proposed model to other NLP tasks, such as question answering or text classification; and (3) discuss potential limitations and future directions for the model, including the use of pre-trained embeddings and the impact of hyperparameter tuning.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the attention weights change across different datasets and tasks, and what insights can be gained from these changes? (2) Can the authors provide more details on the hyperparameter tuning process and the sensitivity of the model to different hyperparameters? (3) How does the proposed model handle out-of-vocabulary words and rare events, and are there any plans to incorporate subword modeling or other techniques to address these issues?