Summary
This paper presents a comprehensive analysis of state-of-the-art deep neural networks (DNNs) submitted to the ImageNet challenge, evaluating their performance in terms of accuracy, memory footprint, parameters, operations count, inference time, and power consumption. The authors aim to provide insights into designing efficient neural networks for practical applications, considering the often-limited resources in actual deployments. The paper highlights key findings, including the hyperbolic relationship between accuracy and inference time, the reliability of operations count in estimating inference time, and the impact of energy constraints on maximum achievable accuracy and model complexity.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and relevant problem in the field of computer vision, namely the lack of consideration for resource utilization in DNNs. Secondly, the approach is well-motivated and well-placed in the literature, providing a comprehensive analysis of multiple state-of-the-art DNN architectures.
Supporting Arguments
The paper provides a thorough analysis of the trade-offs between accuracy, computational requirements, and power consumption, which is essential for designing efficient neural networks for practical applications. The authors' findings, such as the hyperbolic relationship between accuracy and inference time, are well-supported by experimental results and provide valuable insights for the community. The paper also demonstrates the effectiveness of the proposed ENet architecture in terms of parameters space utilization, which is a significant contribution to the field.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed explanations of the experimental setup and the measurement tools used to collect the data. Additionally, it would be helpful to include more discussions on the implications of the findings for real-world applications and potential future research directions. Some questions I would like the authors to answer include: How do the results generalize to other computer vision tasks and datasets? What are the potential applications of the proposed ENet architecture in resource-constrained environments? How do the authors plan to extend their work to other types of neural networks, such as recurrent neural networks or generative models?