Summary
The paper presents multiple approaches for improving the performance of gradient descent when utilizing multiple compute resources. The proposed approaches include synchronous gradient descent, layer-wise gradient descent, and asynchronous gradient descent. The authors implement these approaches using Caffe, a high-performance deep learning library, and evaluate them on both an Intel Sandy Bridge cluster and an NVIDIA DGX-1 system. The results show that the asynchronous gradient descent approach can achieve a speedup of up to 1.7x compared to synchronous gradient descent while maintaining equivalence to the sequential algorithm.
Decision
I decide to Accept this paper with some minor revisions. The main reason for this decision is that the paper presents a well-motivated approach to improving the performance of gradient descent on multiple compute resources, and the results demonstrate a significant speedup. The paper is also well-organized and easy to follow.
Supporting Arguments
The paper tackles a specific question/problem, which is how to improve the performance of gradient descent on multiple compute resources. The approach is well-motivated, and the authors provide a clear explanation of the background and related work. The paper also presents a thorough evaluation of the proposed approaches, including a comparison with synchronous gradient descent and an analysis of the tradeoff between speedup and accuracy.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the implementation of the asynchronous gradient descent approach, including the specific techniques used to delay the gradient updates and the communication protocol used to synchronize the gradients. Additionally, it would be helpful to include more results on the convergence of the proposed approaches, including the number of iterations required to reach convergence and the final accuracy achieved.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
1. Can you provide more details on the specific techniques used to delay the gradient updates in the asynchronous gradient descent approach?
2. How do you handle the communication of gradients between compute devices in the asynchronous gradient descent approach?
3. Can you provide more results on the convergence of the proposed approaches, including the number of iterations required to reach convergence and the final accuracy achieved?
4. How do you plan to extend this work to other deep learning frameworks and applications?