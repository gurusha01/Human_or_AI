Summary
The paper introduces a novel design strategy for neural network macro-architecture based on self-similarity, called FractalNet. The authors propose a recursive expansion rule to generate deep networks with a fractal structure, which contains interacting subpaths of different lengths without any pass-through or residual connections. The paper claims that FractalNet matches the performance of standard residual networks on CIFAR and ImageNet classification tasks, demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
Supporting Arguments
The paper provides a clear and concise introduction to the concept of FractalNet and its motivation. The authors thoroughly review related work, including residual networks, deep supervision, and student-teacher learning, and demonstrate how FractalNet implicitly recapitulates many properties of these architectures. The experimental results are impressive, showing that FractalNet matches the performance of residual networks on various datasets, including CIFAR and ImageNet. The paper also introduces a novel regularization protocol, drop-path, which is shown to be effective in preventing co-adaptation of subpaths in fractal architectures.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the internal behavior of FractalNet, such as visualizing the feature representations learned by the network or analyzing the gradient flow during training. Additionally, it would be interesting to explore the application of FractalNet to other domains, such as natural language processing or speech recognition.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the implementation of the drop-path regularization protocol, such as the probability of dropping a path or the schedule of dropping paths during training?
2. How do you think the fractal structure of FractalNet affects the optimization process, and what are the implications for the convergence of the network?
3. Have you explored the use of FractalNet for other tasks, such as object detection or segmentation, and if so, what are the results?