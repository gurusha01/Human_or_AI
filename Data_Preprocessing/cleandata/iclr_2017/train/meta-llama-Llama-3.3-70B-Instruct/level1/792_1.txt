Summary
The paper introduces a new regularization technique called SoftTarget regularization, which aims to reduce over-fitting in deep neural networks without sacrificing their capacity. The method uses a weighted average of the real labels and an exponential average of past soft-targets to guide the learning process. The authors demonstrate the effectiveness of SoftTarget regularization on various neural network architectures, including fully connected networks and convolutional neural networks, and show that it outperforms other regularization techniques such as Dropout and Batch Normalization.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by experimental results. The authors provide a clear explanation of the problem of over-fitting and how SoftTarget regularization addresses it, and the results show that the method is effective in reducing over-fitting and improving the performance of neural networks.
Supporting Arguments
The paper provides a thorough analysis of the problem of over-fitting and how SoftTarget regularization addresses it. The authors also provide a detailed explanation of the method and its implementation, making it easy to understand and replicate. The experimental results are comprehensive and demonstrate the effectiveness of SoftTarget regularization on various datasets and architectures. Additionally, the authors provide insights into the phenomenon of co-label similarities and how it relates to over-fitting, which is a novel and interesting contribution.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the hyper-parameters introduced by SoftTarget regularization and how they affect the performance of the method. Additionally, it would be interesting to see more comparisons with other regularization techniques, such as weight decay and DropConnect. Finally, the authors may want to consider providing more details on the computational cost of SoftTarget regularization and how it compares to other methods.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the hyper-parameters of SoftTarget regularization, such as β and γ, affect the performance of the method, and how were they chosen?
* Can the authors provide more insights into the relationship between co-label similarities and over-fitting, and how SoftTarget regularization addresses this phenomenon?
* How does SoftTarget regularization compare to other regularization techniques in terms of computational cost and complexity?