Summary
The paper proposes a technique to reduce the parameters of Recurrent Neural Networks (RNNs) by pruning weights during the initial training of the network. This approach achieves a significant reduction in model size, with a 90% reduction in parameters, while maintaining a small loss in accuracy. The authors demonstrate the effectiveness of their technique on various RNN architectures, including vanilla RNNs and Gated Recurrent Units (GRUs), and show that it can lead to improved accuracy and reduced inference time.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by thorough experiments. The authors provide a clear and concise explanation of their technique, and the results demonstrate a significant reduction in model size and improved accuracy.
Supporting Arguments
The paper tackles a specific and relevant problem in the field of deep learning, namely the reduction of model size and inference time for RNNs. The approach is well-placed in the literature, building on existing work on pruning and quantization techniques. The authors provide a thorough evaluation of their technique, including experiments on various RNN architectures and datasets, and demonstrate its effectiveness in reducing model size and improving accuracy.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the selection of the threshold function. Additionally, it would be interesting to see a comparison of the proposed technique with other pruning and quantization methods, such as L1 regularization and knowledge distillation. The authors may also consider providing more insights into the interpretability of the sparse models and the potential applications of this technique in other domains.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the selection of the threshold function and the hyperparameter tuning process?
2. How do you plan to extend this technique to other domains, such as language modeling and computer vision?
3. Can you provide more insights into the interpretability of the sparse models and the potential applications of this technique in real-world scenarios?