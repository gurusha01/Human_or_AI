The paper "Discrete Variational Autoencoders" presents a novel approach to training probabilistic models with discrete latent variables using the variational autoencoder (VAE) framework. The authors introduce a method to backpropagate through discrete latent variables by augmenting the latent representation with continuous random variables, allowing for efficient training of models with discrete latent variables.
The specific question tackled by the paper is how to train probabilistic models with discrete latent variables using the VAE framework, which is challenging due to the difficulty of backpropagating through discrete variables. The authors address this problem by introducing a continuous auxiliary latent representation that allows for backpropagation, while maintaining the original discrete latent space.
The approach is well-motivated, as it builds upon the VAE framework and addresses a significant limitation of existing methods. The paper provides a clear and detailed explanation of the methodology, including the introduction of a hierarchical approximating posterior and the use of a spike-and-exponential transformation to map discrete latent variables to continuous latent variables.
The paper supports its claims through extensive experimental results, demonstrating the effectiveness of the discrete VAE on several benchmark datasets, including permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes. The results show that the discrete VAE achieves state-of-the-art performance on these datasets, outperforming existing methods such as importance-weighted autoencoders and ladder variational autoencoders.
To improve the paper, I would suggest providing more details on the implementation of the discrete VAE, including the architecture of the neural networks used to parameterize the distributions and the hyperparameter settings used in the experiments. Additionally, it would be helpful to provide more analysis on the properties of the discrete VAE, such as its ability to capture complex dependencies between latent variables and its robustness to overfitting.
Some questions I would like the authors to answer include:
* How do the authors choose the number of continuous latent variables and the architecture of the neural networks used to parameterize the distributions?
* How do the authors ensure that the discrete VAE is robust to overfitting, particularly when using a large number of latent variables?
* Can the authors provide more insight into the properties of the spike-and-exponential transformation used to map discrete latent variables to continuous latent variables, and how it affects the performance of the discrete VAE?
Overall, the paper presents a significant contribution to the field of probabilistic modeling and deep learning, and provides a novel approach to training models with discrete latent variables using the VAE framework. With some additional details and analysis, the paper could be even more convincing and provide a more comprehensive understanding of the discrete VAE methodology. 
Decision: Accept 
Reasons: 
1. The paper tackles a significant problem in probabilistic modeling and deep learning, and provides a novel approach to training models with discrete latent variables using the VAE framework.
2. The approach is well-motivated and builds upon existing methods, and the paper provides a clear and detailed explanation of the methodology.
3. The paper supports its claims through extensive experimental results, demonstrating the effectiveness of the discrete VAE on several benchmark datasets.
Additional feedback: 
* Provide more details on the implementation of the discrete VAE, including the architecture of the neural networks used to parameterize the distributions and the hyperparameter settings used in the experiments.
* Provide more analysis on the properties of the discrete VAE, such as its ability to capture complex dependencies between latent variables and its robustness to overfitting.
* Consider providing more insight into the properties of the spike-and-exponential transformation used to map discrete latent variables to continuous latent variables, and how it affects the performance of the discrete VAE.