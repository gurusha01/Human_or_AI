Summary
The paper introduces a technique called dynamic batching, which enables efficient batching for training and inference with dynamic computation graphs (DCGs). DCGs are neural networks that compute over graph structures, such as parse trees or molecular graphs, and are commonly used in natural language processing and cheminformatics. The authors also present a high-level library, TensorFlow Fold, which simplifies the creation of dynamic graph models using dynamic batching. The library provides a set of combinators that allow users to define models in a concise and compositional way.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in the field of deep learning, namely the efficient batching of dynamic computation graphs. Secondly, the approach is well-motivated and well-placed in the literature, building on existing work on neural networks and graph structures.
Supporting Arguments
The paper provides a clear and detailed explanation of the dynamic batching technique, including a greedy algorithm for scheduling operations and a TensorFlow implementation. The authors also demonstrate the effectiveness of dynamic batching through experimental results, showing significant speedups over manual batching and other existing methods. Additionally, the TensorFlow Fold library is shown to be a useful tool for defining and training dynamic graph models, with applications in sentiment analysis, question-answering, and molecular property prediction.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed comparisons with other existing methods for batching dynamic computation graphs, such as the SPINN architecture. Additionally, it would be helpful to see more examples of how the TensorFlow Fold library can be used to define and train complex models, such as those with recursive or nested structures. Finally, the authors may want to consider providing more detailed information about the computational resources required to run the experiments, such as the specific hardware and software configurations used.
Questions for the Authors
I have several questions for the authors to help clarify my understanding of the paper. Firstly, can you provide more details about how the dynamic batching algorithm handles cases where the input graphs have different sizes or topologies? Secondly, how does the TensorFlow Fold library handle cases where the model requires multiple inputs or outputs, such as in multi-task learning or reinforcement learning? Finally, are there any plans to release the TensorFlow Fold library as an open-source tool, and if so, what is the expected timeline for release?