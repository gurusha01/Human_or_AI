Summary of the Paper's Claims and Contributions
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) for regularizing deep neural networks and achieving better optimization performance. The DSD training flow consists of three steps: dense, sparse, and re-dense. The authors claim that DSD training can improve the performance of various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, caption generation, and speech recognition. The paper presents experimental results showing significant performance gains on several benchmark datasets, including ImageNet, Flickr-8K, and WSJ'93.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The two key reasons for this decision are:
1. The paper tackles a specific and important problem in deep learning, namely, the difficulty of training large neural networks due to overfitting and saddle points.
2. The authors propose a well-motivated and novel approach, DSD training, which is supported by theoretical analysis and extensive experimental results demonstrating its effectiveness.
Supporting Arguments
The paper provides a clear and well-structured presentation of the DSD training flow, including the dense, sparse, and re-dense steps. The authors also provide a thorough analysis of the related work, highlighting the differences between DSD and other regularization techniques, such as dropout and model compression. The experimental results are extensive and demonstrate the effectiveness of DSD training on various benchmark datasets. The paper also provides additional insights into the benefits of DSD training, including its ability to escape saddle points, achieve better minima, and reduce variance.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more detailed analysis of the computational cost and memory requirements of DSD training, as well as its potential applications to other domains, such as natural language processing and reinforcement learning. I also have the following questions:
* How does the choice of sparsity ratio affect the performance of DSD training?
* Can the authors provide more insights into the theoretical analysis of DSD training, particularly with regards to its ability to escape saddle points and achieve better minima?
* How does DSD training compare to other optimization techniques, such as stochastic gradient descent and Adam, in terms of convergence rate and stability?