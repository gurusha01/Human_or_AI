Summary
The paper proposes ParMAC, a parallel, distributed framework for training nested, nonconvex models using the method of auxiliary coordinates (MAC). MAC introduces auxiliary coordinates to decouple nested terms in the objective function, creating parallelism that can be exploited in a distributed system. The authors analyze the parallel speedup and convergence of ParMAC and demonstrate its effectiveness with an MPI-based implementation for training binary autoencoders.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and important problem in machine learning, namely, parallelizing the training of nested, nonconvex models, and (2) the approach is well-motivated and supported by theoretical analysis and empirical results.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of training nested models and the limitations of existing approaches. The authors motivate the use of MAC and its parallelization, ParMAC, and provide a detailed analysis of the parallel speedup and convergence properties. The empirical results demonstrate the effectiveness of ParMAC in training binary autoencoders on large datasets, achieving nearly perfect speedups and good retrieval precision.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the implementation of ParMAC, including the choice of hyperparameters and the tuning process. Additionally, it would be helpful to compare ParMAC with other distributed optimization algorithms, such as those used in Google's DistBelief system. The authors may also consider providing more insights into the trade-offs between parallelism, communication overhead, and convergence speed.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors choose the number of epochs and the penalty parameter schedule in the MAC algorithm?
2. Can the authors provide more details on the MPI implementation, including the communication protocol and the synchronization mechanism?
3. How do the authors plan to extend ParMAC to other types of nested models, such as deep neural networks with multiple layers?