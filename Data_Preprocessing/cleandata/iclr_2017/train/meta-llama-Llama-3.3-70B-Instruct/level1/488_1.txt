Summary
The paper proposes an extension of adversarial and virtual adversarial training to the text domain, applying perturbations to word embeddings in a recurrent neural network rather than to the original input. The authors demonstrate that this approach achieves state-of-the-art results on multiple benchmark semi-supervised and supervised text classification tasks, including sentiment classification and topic classification. They also provide visualizations and analysis showing that the learned word embeddings have improved in quality and that the model is less prone to overfitting.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated problem, extending adversarial and virtual adversarial training to the text domain, which is a significant contribution to the field. Secondly, the paper provides strong empirical evidence to support its claims, demonstrating state-of-the-art results on multiple benchmark tasks and providing detailed analysis and visualizations to illustrate the effectiveness of the proposed approach.
Supporting Arguments
The paper is well-placed in the literature, building on existing work on adversarial and virtual adversarial training, and providing a clear and concise overview of the relevant background and related work. The authors also provide a detailed and thorough explanation of their proposed approach, including the mathematical formulation and implementation details. The experimental evaluation is comprehensive and well-designed, including a range of benchmark tasks and comparison to state-of-the-art baselines.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more detailed analysis of the learned word embeddings, including quantitative metrics and comparisons to other embedding methods. Additionally, it would be interesting to see more exploration of the hyperparameter space, including the effect of different norm constraints and optimization algorithms on the performance of the proposed approach. Finally, I would like to see more discussion of the potential applications and limitations of the proposed approach, including its potential use in other sequential tasks such as machine translation and question answering.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the implementation of the recurrent language model pre-training, including the specific architecture and hyperparameters used?
* How did you select the norm constraint for the adversarial and virtual adversarial perturbations, and what is the effect of varying this hyperparameter on the performance of the proposed approach?
* Can you provide more insight into the qualitative analysis of the learned word embeddings, including any interesting patterns or trends that emerged from the visualization and analysis?