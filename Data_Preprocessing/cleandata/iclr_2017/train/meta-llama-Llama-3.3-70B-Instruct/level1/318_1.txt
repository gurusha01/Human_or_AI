Summary
The paper introduces the Gated Graph Transformer Neural Network (GGT-NN) model, which extends the Gated Graph Sequence Neural Network (GGS-NN) model by allowing graph-structured intermediate representations. The GGT-NN model can learn to construct and modify graphs in sophisticated ways based on textual input and use the graphs to produce a variety of outputs. The model is evaluated on the bAbI tasks, a set of simple natural-language tasks, and achieves high accuracy on most tasks. Additionally, the model is applied to two rule discovery tasks, a 1-dimensional cellular automaton and an arbitrary 2-symbol 4-state Turing machine, and successfully learns the rules governing the evolution of the graph structure over time.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and the results are impressive, demonstrating the effectiveness of the GGT-NN model on a variety of tasks.
Supporting Arguments
The paper tackles a specific question/problem by introducing a new model that can handle graph-structured data and textual input. The approach is well-motivated, building on existing work on GGS-NNs and graph neural networks. The results are impressive, with the model achieving high accuracy on most bAbI tasks and successfully learning the rules governing the evolution of the graph structure over time in the two rule discovery tasks. The paper also provides a clear and detailed explanation of the model and its components, making it easy to understand and follow.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the computational complexity of the GGT-NN model and its scalability to larger graphs and more complex tasks. Additionally, it would be interesting to see more comparisons with other models and architectures that can handle graph-structured data and textual input. Finally, the authors may want to consider providing more visualizations and examples of the graphs produced by the model to help illustrate its capabilities and limitations.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the GGT-NN model handle graphs with varying sizes and structures?
* Can the model be applied to more complex tasks, such as those involving multiple graphs or graphs with multiple types of nodes and edges?
* How does the model's performance compare to other models and architectures that can handle graph-structured data and textual input?
* Are there any plans to release the code and data used in the paper to facilitate reproducibility and further research?