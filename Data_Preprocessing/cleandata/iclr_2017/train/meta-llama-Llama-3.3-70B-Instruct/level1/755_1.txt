Summary
The paper provides a theoretical explanation for the great performance of ResNet via the study of deep linear networks and some nonlinear variants. The authors show that with or without nonlinearities, by adding shortcuts that have depth two, the condition number of the Hessian of the loss function at the zero initial point is depth-invariant, which makes training very deep models no more difficult than shallow ones. The paper also provides extensive experiments to support the theoretical results, demonstrating that initializing the network to small weights with 2-shortcuts achieves significantly better results than random Gaussian initialization, orthogonal initialization, and shortcuts of deeper depth.
Decision
I decide to Accept this paper with minor revisions. The paper tackles a specific and important question in the field of deep learning, and the approach is well-motivated and well-placed in the literature. The theoretical results are sound and well-supported by experiments.
Supporting Arguments
The paper provides a clear and concise explanation of the problem and the proposed solution. The theoretical analysis is rigorous and well-supported by mathematical derivations. The experiments are well-designed and provide strong evidence for the claims made in the paper. The paper also provides a good discussion of the implications of the results and potential future directions.
Additional Feedback
To further improve the paper, I suggest that the authors provide more intuition and explanation for the theoretical results, particularly for the non-expert reader. Additionally, the authors may want to consider providing more comparisons with other related work in the field, such as other architectures that use shortcut connections. Finally, the authors may want to consider providing more details on the experimental setup and the hyperparameters used in the experiments.
Questions for the Authors
I have a few questions for the authors to clarify some points in the paper:
1. Can you provide more intuition on why the condition number of the Hessian is depth-invariant for 2-shortcut networks?
2. How do the results in the paper relate to other architectures that use shortcut connections, such as Highway Networks?
3. Can you provide more details on the experimental setup and the hyperparameters used in the experiments?
4. Have you considered extending the results in the paper to other types of neural networks, such as recurrent neural networks or convolutional neural networks?