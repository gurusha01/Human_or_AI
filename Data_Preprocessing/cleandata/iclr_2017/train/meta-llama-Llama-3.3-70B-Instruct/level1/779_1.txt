Summary
The paper presents a comprehensive analysis of vocabulary selection techniques for neural machine translation, aiming to reduce the computational cost of decoding and training while maintaining accuracy. The authors explore various selection methods, including word co-occurrence counts, bilingual embeddings, word alignments, phrase pairs, and discriminative SVM classifiers. They evaluate these methods on two language pairs, WMT15 English-German and WMT16 English-Romanian, and demonstrate significant speed-ups in decoding time (up to 90%) and training time (up to 25%) with minimal loss in accuracy.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a well-motivated and important problem in neural machine translation, and (2) the authors provide a thorough and well-structured evaluation of various selection methods, demonstrating significant efficiency gains without compromising accuracy.
Supporting Arguments
The paper is well-placed in the literature, building upon previous work on vocabulary selection and neural machine translation. The authors provide a clear and concise introduction to the problem, and their experimental setup is well-designed and thoroughly evaluated. The results are impressive, demonstrating significant speed-ups in decoding and training times while maintaining accuracy. The authors also provide a detailed analysis of the trade-off between speed and accuracy, which is essential for understanding the limitations and potential applications of their approach.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points: (1) provide more insight into the computational cost of each selection method, to help readers understand the trade-offs between different approaches; (2) evaluate the impact of vocabulary selection on other neural machine translation architectures, such as transformer-based models; and (3) consider applying their approach to other natural language processing tasks, such as language modeling or text classification.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the selection methods perform on out-of-vocabulary words, and what strategies can be employed to handle such cases? (2) Can the authors provide more details on the computational resources used for training and decoding, to help readers estimate the potential speed-ups in their own setups? (3) How do the results change when using different evaluation metrics, such as ROUGE or METEOR, instead of BLEU?