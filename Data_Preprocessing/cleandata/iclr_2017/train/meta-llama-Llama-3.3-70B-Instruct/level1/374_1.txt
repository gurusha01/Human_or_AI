Summary
The paper proposes a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on word properties, which improves the performance on reading comprehension tasks. The authors also extend the idea of fine-grained gating to model the interaction between documents and queries. The approach achieves new state-of-the-art results on several datasets, including the Children's Book Test and Who Did What datasets.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with thorough experiments and analysis.
Supporting Arguments
The paper tackles a specific question of how to effectively combine word-level and character-level representations, which is a crucial problem in natural language processing. The approach is well-motivated, as it addresses the limitations of previous methods, such as concatenation and scalar gating. The paper also provides a thorough analysis of the results, including visualization and sampling of gate values, which helps to understand the effectiveness of the fine-grained gating mechanism.
Additional Feedback
To further improve the paper, I suggest the authors to provide more details on the hyperparameter tuning process and the computational resources used for the experiments. Additionally, it would be interesting to see more analysis on the types of words or tokens that benefit the most from the fine-grained gating mechanism. Some questions I would like the authors to answer include: (1) How do the results change when using different types of word embeddings or character-level representations? (2) Can the fine-grained gating mechanism be applied to other NLP tasks, such as language modeling or text classification? (3) How does the approach perform on out-of-vocabulary words or tokens with rare morphological structures?