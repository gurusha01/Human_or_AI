Summary
This paper investigates the generalization ability of the Neural GPU, a model that can learn algorithms such as multi-digit binary addition and binary multiplication. The authors show that the performance of the Neural GPU can be improved by carefully designing a curriculum and increasing the model size. They also find that the model's generalization is incomplete, as it fails on highly structured test cases, and that larger models generalize better.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated question, namely, how to improve the generalization ability of the Neural GPU. Secondly, the authors provide a thorough empirical study of the parameters that affect the model's generalization, including the importance of curriculum learning and model size.
Supporting Arguments
The paper is well-motivated, as it addresses a significant problem in the field of neural networks, namely, the ability to learn algorithms that generalize to inputs of arbitrary length. The authors provide a clear and concise overview of the related work, including the Neural GPU and other neural network architectures that can learn algorithms. The empirical study is thorough and well-designed, with a clear description of the experimental setup and results. The authors also provide insightful analysis of the model's failure modes, including the observation that the model fails on highly structured test cases.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the implementation of the Neural GPU, including the specific hyperparameters used and the computational resources required. Additionally, it would be helpful to include more visualizations of the model's performance, such as plots of the training and test error over time. Finally, the authors may want to consider discussing the potential applications of the Neural GPU, including its potential use in areas such as computer vision and natural language processing.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* Can you provide more details on the curriculum learning approach used in the paper, including the specific schedule and criteria used to determine when to increase the difficulty of the training data?
* How do you think the Neural GPU could be modified to improve its generalization ability, including the use of additional sources of information or architectural modifications?
* Can you provide more insight into the failure modes of the Neural GPU, including the types of highly structured test cases that cause the model to fail, and how these failure modes could be addressed in future work?