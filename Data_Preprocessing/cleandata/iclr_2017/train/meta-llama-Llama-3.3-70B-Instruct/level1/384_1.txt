Summary
The paper proposes two end-to-end neural network models for machine comprehension, specifically designed for the Stanford Question Answering Dataset (SQuAD) and the Human-Generated MAchine Reading COmprehension (MSMARCO) dataset. The models combine match-LSTM and Pointer Network to handle the special properties of these datasets, where answers can be any sequence of tokens from the given text. The authors claim that their models outperform existing approaches, including logistic regression and manually crafted features, and achieve state-of-the-art performance on the MSMARCO dataset.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in natural language processing, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of machine comprehension and the datasets used. The authors motivate their approach by highlighting the limitations of existing methods and the advantages of their proposed models. The experimental results demonstrate the effectiveness of the models, particularly the boundary model, which outperforms the sequence model and achieves state-of-the-art performance on the MSMARCO dataset. The authors also provide additional analyses and visualizations to support their claims and provide insights into the strengths and weaknesses of their models.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of their models, including the hyperparameter settings and the training procedures. Additionally, it would be helpful to include more comparisons with other existing approaches, such as Memory Networks, to provide a more comprehensive understanding of the strengths and weaknesses of the proposed models. Finally, the authors may want to consider exploring the application of their models to other machine comprehension datasets to demonstrate their generalizability.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the match-LSTM and Pointer Network components, including the hyperparameter settings and the training procedures?
2. How do you plan to address the limitation of your models in handling multi-sentence reasoning, as mentioned in the conclusion?
3. Can you provide more insights into the differences between the sequence and boundary models, and why the boundary model performs better on the MSMARCO dataset?