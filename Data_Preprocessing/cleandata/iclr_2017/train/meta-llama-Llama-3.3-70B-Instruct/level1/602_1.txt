Summary
The paper proposes a novel model, the Gated-Attention (GA) Reader, for answering cloze-style questions over documents. The GA Reader integrates a multi-hop architecture with a novel attention mechanism, which enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The model achieves state-of-the-art results on three benchmark datasets, demonstrating the effectiveness of the proposed gated-attention mechanism.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and well-motivated problem in the literature, and (2) the approach is well-supported by experimental results and ablation studies.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of answering cloze-style questions over documents. The proposed GA Reader model is well-placed in the literature, building on existing work on multi-hop architectures and attention mechanisms. The experimental results demonstrate the effectiveness of the GA Reader, with significant improvements over competitive baselines on three benchmark datasets. The ablation studies provide further evidence for the importance of the gated-attention mechanism and the multi-hop architecture.
Additional Feedback
To further improve the paper, I suggest the authors provide more analysis on the attention visualization results, to gain a deeper understanding of how the model is using the attention mechanism to arrive at the final answer. Additionally, it would be interesting to see more experiments on the effectiveness of the GA Reader on other tasks, such as language modeling, as mentioned in the conclusion.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the implementation of the character composition model, and how it is used to generate orthographic embeddings of tokens?
* How do you select the number of hops (K) in the multi-hop architecture, and what is the effect of varying K on the final performance?
* Can you provide more analysis on the attention visualization results, to gain a deeper understanding of how the model is using the attention mechanism to arrive at the final answer?