Summary
The paper proposes a novel memory-based attention model for video description, which utilizes memories of past attention when reasoning about where to attend to in the current time step. This allows the model to consider the entire sequence of video frames while generating each word, enabling it to capture higher-order interactions involved in video description. The model consists of three components: Temporal Model (TEM), Hierarchical Attention/Memory (HAM), and Decoder. The authors evaluate their model on the MSVD and Charades datasets, achieving state-of-the-art results.
Decision
I decide to Accept this paper, with two key reasons: (1) the approach is well-motivated and grounded in the literature, and (2) the paper provides strong empirical evidence to support its claims, demonstrating significant improvements over existing methods on challenging datasets.
Supporting Arguments
The paper tackles a specific and important problem in video description, namely capturing higher-order interactions between video frames and concepts. The authors provide a clear and well-structured presentation of their approach, which is grounded in the literature on attention mechanisms and sequence-to-sequence models. The experimental evaluation is thorough and well-designed, with a clear comparison to existing methods and a detailed analysis of the contributions of each component of the proposed model.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the interpretability of the memory-based attention mechanism, such as visualizations of the attention weights or analysis of the learned memories. Additionally, it would be interesting to see an extension of the model to other sequence learning problems, as mentioned in the conclusion. Some questions I would like the authors to answer include: (1) How do the learned memories evolve over time, and what do they represent? (2) Can the authors provide more details on the hyperparameter optimization process, and how the chosen hyperparameters affect the performance of the model? (3) How does the model perform on videos with multiple actions or events, and can it capture the relationships between them?