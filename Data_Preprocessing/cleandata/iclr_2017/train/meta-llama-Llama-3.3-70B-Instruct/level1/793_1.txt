Summary of the Paper
The paper introduces a novel approach to recurrent neural networks, called surprisal-driven recurrent networks, which incorporates top-down feedback by monitoring the discrepancy between predictions and observations. This feedback signal, also known as surprisal, is used to improve the generalization capabilities of Long-Short Term Memory (LSTM) networks. The authors demonstrate the effectiveness of their approach on the enwik8 character-level prediction task, achieving a state-of-the-art result of 1.37 BPC.
Decision
I decide to Accept this paper, with two key reasons: (1) the approach is well-motivated and grounded in the literature, and (2) the results demonstrate a significant improvement over existing state-of-the-art methods.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of top-down feedback in recurrent neural networks, and motivates the use of surprisal as a feedback signal. The authors also provide a thorough description of their approach, including the mathematical formulation of the feedback architecture and the backward pass for the Back Propagation Through Time (BPTT) algorithm. The experimental results are impressive, demonstrating a significant improvement over existing methods on the enwik8 dataset.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the interpretation of the surprisal signal and its relationship to the LSTM architecture. Additionally, it would be interesting to see an analysis of the computational cost of the proposed approach compared to existing methods. Finally, the authors may want to consider exploring the application of their approach to other temporal data tasks, such as speech recognition or natural language processing.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to address the open question of what the feedback signal should constitute and how it should interact with lower-level neurons? (2) Can the authors provide more details on the implementation of the Adagrad algorithm with a modification taking into consideration only recent window of gradient updates? (3) How do the authors plan to explore the addition of regularization to further improve the performance of their approach?