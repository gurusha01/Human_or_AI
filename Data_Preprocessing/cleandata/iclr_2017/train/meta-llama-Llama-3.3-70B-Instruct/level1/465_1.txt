This paper claims to contribute to the understanding of transferability of adversarial examples in deep neural networks, specifically by conducting an extensive study on large models and a large-scale dataset, ImageNet. The authors propose novel ensemble-based approaches to generate transferable adversarial examples and demonstrate their effectiveness in generating both non-targeted and targeted transferable adversarial examples.
I decide to accept this paper with two key reasons: (1) the paper tackles a specific and important problem in the field of deep learning, and (2) the approach is well-motivated and supported by thorough experiments and analysis.
The paper supports its claims through a series of experiments and analysis, including the evaluation of different approaches for generating adversarial examples, the study of geometric properties of models, and the demonstration of the effectiveness of the proposed ensemble-based approaches. The results show that the proposed approaches can generate transferable targeted adversarial examples with a high success rate, and that both non-targeted and targeted adversarial examples can successfully attack a black-box image classification system, Clarifai.com.
To further improve the paper, I would like to see more discussion on the implications of the results, particularly in terms of the security and robustness of deep neural networks. Additionally, it would be helpful to see more analysis on the geometric properties of the models and how they relate to the transferability of adversarial examples.
I have several questions that I would like the authors to clarify: (1) How do the authors plan to extend their work to other types of deep neural networks, such as recurrent neural networks or generative adversarial networks? (2) Can the authors provide more insight into the geometric properties of the models and how they affect the transferability of adversarial examples? (3) How do the authors plan to address the potential negative consequences of their work, such as the potential for malicious use of transferable adversarial examples?