Summary
This paper presents a novel model for multimodal learning based on gated neural networks, called the Gated Multimodal Unit (GMU). The GMU is designed to learn an intermediate representation based on a combination of data from different modalities, and it outperforms single-modality approaches and other fusion strategies in a multilabel movie genre classification task. The paper also introduces a new dataset, MM-IMDb, which is the largest publicly available multimodal dataset for genre prediction on movies.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and well-motivated problem in multimodal learning, and (2) the approach is well-supported by experimental results on a large and publicly available dataset.
Supporting Arguments
The paper clearly defines the problem of multimodal learning and motivates the need for a novel approach. The GMU model is well-explained and supported by theoretical and empirical results. The experimental evaluation is thorough and includes a comparison with other fusion strategies, demonstrating the effectiveness of the GMU model. The introduction of the MM-IMDb dataset is also a significant contribution, as it provides a large and publicly available resource for multimodal learning research.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the interpretability of the learned features and the gates' activations. Additionally, it would be interesting to explore the application of the GMU model to other multimodal tasks and datasets. The authors may also consider providing more details on the hyperparameter tuning process and the computational resources required for training the models.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more insights on how the GMU model handles cases where one modality is missing or noisy?
* How do you plan to explore the interpretability of the learned features and the gates' activations in future work?
* Have you considered applying the GMU model to other multimodal tasks, such as visual question answering or image captioning?