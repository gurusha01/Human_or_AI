Summary
The paper proposes a novel approach to reinforcement learning by extending recent work on video frame prediction to enable joint prediction of future states and rewards using a single latent representation. The authors demonstrate the feasibility of using a learned dynamics and reward model for accurate planning in complex, initially unknown environments. The approach is evaluated on five Atari games, achieving accurate cumulative reward prediction up to 200 frames.
Decision
I decide to Accept this paper, with two key reasons: (1) the approach is well-motivated and placed in the literature, addressing a key open question in reinforcement learning; and (2) the empirical results demonstrate the effectiveness of the approach in predicting cumulative rewards in multiple Atari games.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of reinforcement learning in complex environments, highlighting the limitations of model-free approaches and the potential benefits of model-based techniques. The authors also provide a thorough review of related work, demonstrating a strong understanding of the literature. The proposed approach is well-described, and the empirical results are convincing, with detailed error analysis and visualizations that support the claims made in the paper.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points: (1) provide more discussion on the potential applications of the approach beyond Atari games; (2) investigate the use of other regularization techniques, such as dropout and max norm regularization, to improve the prediction performance of the network; and (3) consider extending the approach to non-deterministic state transitions using variational autoencoder schemes.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to integrate the proposed approach with model-free techniques to achieve effective interactive learning and planning in complex environments? (2) Can the authors provide more details on the computational resources required to train and evaluate the network, and how this might impact the scalability of the approach? (3) How do the authors plan to address the issue of cumulative reward overestimation, as observed in the Freeway game, in future work?