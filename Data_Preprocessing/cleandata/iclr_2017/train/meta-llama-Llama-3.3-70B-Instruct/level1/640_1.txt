This paper presents a novel approach to learning multi-sense word embeddings by leveraging multilingual distributional information. The authors propose a multi-view Bayesian non-parametric algorithm that can effectively combine the benefits of crosslingual training and Bayesian non-parametrics. The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing bilingual approaches and the benefits of using multilingual information.
I decide to Accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The authors demonstrate that their multilingual approach can achieve comparable performance to a state-of-the-art monolingual model trained on five times more data.
The paper supports its claims through extensive experiments on word sense induction and contextual word similarity tasks. The results show that the multilingual approach outperforms bilingual and monolingual approaches, and the authors provide a detailed analysis of the effects of language family distance and window size on the results.
To further improve the paper, I would suggest that the authors provide more details on the parameter tuning process and the sensitivity of the results to the choice of hyperparameters. Additionally, it would be interesting to see more qualitative examples of the learned sense embeddings and how they compare to existing word sense induction approaches.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on how the multilingual approach handles cases where the same word has different translations in different languages?
* How do you plan to extend the approach to model polysemy in foreign languages, and what are the potential challenges and benefits of doing so?
* Can you provide more insights into the computational efficiency of the approach, and how it compares to existing bilingual and monolingual approaches?