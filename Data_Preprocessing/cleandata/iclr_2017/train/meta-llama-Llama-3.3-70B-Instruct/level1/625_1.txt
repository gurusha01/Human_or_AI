Summary of the Paper's Contributions
The paper proposes a novel hierarchical deep reinforcement learning architecture for zero-shot task generalization in instruction execution problems. The architecture consists of two interacting neural controllers: a meta controller that reads instructions and communicates subtasks to a subtask controller, which learns to perform the given subtasks. The paper introduces analogy-making regularization to encourage the subtask controller to learn subtask embeddings that capture correspondences between similar subtasks, enabling generalization to unseen instructions. Additionally, the paper proposes a differentiable neural network architecture in the meta controller that learns temporal abstractions, allowing the agent to operate at a larger time-scale and update subtask arguments only when needed.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The two key reasons for this decision are:
1. The paper tackles a well-motivated and challenging problem in reinforcement learning, and the proposed architecture is well-placed in the literature.
2. The paper provides thorough empirical evaluations and analysis, demonstrating the effectiveness of the proposed architecture and techniques in generalizing to unseen instructions and longer sequences of instructions.
Supporting Arguments
The paper provides a clear and well-structured presentation of the problem, architecture, and experiments. The introduction effectively motivates the problem and provides a concise overview of the related work. The architecture is well-described, and the analogy-making regularization and temporal abstraction techniques are novel and well-explained. The experiments are thorough and well-designed, demonstrating the effectiveness of the proposed architecture and techniques in various settings.
Additional Feedback and Questions
To further improve the paper, I suggest the authors consider the following:
* Provide more details on the hyperparameter tuning process and the sensitivity of the results to different hyperparameter settings.
* Investigate the applicability of the proposed architecture and techniques to other domains and tasks, such as natural language processing or computer vision.
* Consider adding more visualizations or examples to illustrate the behavior of the agent and the learned policies.
Some questions I would like the authors to address in their response:
* How do the authors plan to extend the proposed architecture and techniques to more complex and realistic environments, such as 3D visual environments with partial observability?
* Can the authors provide more insights into the learned subtask embeddings and how they capture correspondences between similar subtasks?
* How do the authors plan to address the potential limitations and challenges of the proposed architecture, such as the need for careful hyperparameter tuning and the potential for overfitting?