Summary of the Paper
The paper proposes a novel model for Interactive Question Answering (IQA) called Context-aware Attention Network (CAN). CAN uses Gated Recurrent Unit (GRU) recurrent networks as encoders for statements and questions, and another GRU as a decoder for outputs. The model employs context-dependent word-level attention and question-guided sentence-level attention to accurately understand when to output an answer or when to generate a supplementary question for additional input. The authors also introduce a new dataset, ibAbI, for IQA tasks and demonstrate the effectiveness of their approach through extensive experiments on both QA and IQA datasets.
Decision
I decide to Accept this paper with minor revisions.
Reasons for the Decision
The paper tackles a specific and well-motivated problem in the field of Question Answering, which is the ability to handle uncertain situations where additional user input is required to answer a question. The approach is well-placed in the literature, building upon existing sequence-to-sequence models with attention and memory. The paper provides a clear and detailed description of the proposed model, including the architecture, training procedure, and experimental setup. The results demonstrate significant improvements over state-of-the-art models on both QA and IQA datasets.
Supporting Arguments
The paper provides a thorough analysis of the limitations of previous models and motivates the need for a context-aware attention mechanism. The authors also provide a detailed description of the proposed model, including the use of GRUs, attention mechanisms, and the interactive mechanism. The experimental results are extensive and demonstrate the effectiveness of the proposed approach. The paper also provides a qualitative analysis of the interactive mechanism, which shows the usefulness of the model for solving different types of QA problems.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the ibAbI dataset, including the size of the dataset, the types of questions and answers, and the evaluation metrics used. Additionally, the authors could provide more analysis on the results, including the performance of the model on different types of questions and the effect of the interactive mechanism on the model's performance. Finally, the authors could consider providing more comparisons with other state-of-the-art models, including those that use different attention mechanisms or architectures.
Questions for the Authors
1. Can you provide more details on the ibAbI dataset, including the size of the dataset and the types of questions and answers?
2. How does the model perform on different types of questions, such as those that require multiple hops or those that have ambiguous answers?
3. Can you provide more analysis on the effect of the interactive mechanism on the model's performance, including the number of supplementary questions generated and the accuracy of the answers provided?