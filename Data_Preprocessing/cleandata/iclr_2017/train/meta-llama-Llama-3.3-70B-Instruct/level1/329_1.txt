Summary
The paper proposes an augmented training procedure for generative adversarial networks (GANs) that addresses the shortcomings of the original GAN framework. The authors introduce a denoising auto-encoder to estimate and track the distribution of discriminator features, which are used to propose high-level targets for the generator. This new loss is combined with the original adversarial loss, resulting in a hybrid criterion that improves the quality of generated samples. The authors demonstrate the effectiveness of their approach on several datasets, including CIFAR-10, STL-10, and ImageNet, and show that it outperforms existing methods in terms of Inception score and visual quality.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear and thorough explanation of their method, and the experiments demonstrate a significant improvement in the quality of generated samples.
Supporting Arguments
The paper tackles a specific and important problem in the field of GANs, namely the difficulty of training GANs to produce high-quality samples on diverse datasets. The authors provide a thorough review of the literature and clearly motivate their approach, which is based on the idea of using a denoising auto-encoder to estimate the distribution of discriminator features. The experiments are well-designed and demonstrate the effectiveness of the proposed method, with significant improvements in Inception score and visual quality.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the denoising auto-encoder, including the architecture and hyperparameters used. Additionally, it would be helpful to include more visual examples of the generated samples, as well as a more detailed analysis of the failure cases. Finally, I would like to see more discussion on the potential limitations and future directions of the proposed method.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How did you choose the hyperparameters for the denoising auto-encoder, and what is the sensitivity of the results to these hyperparameters?
* Can you provide more details on the architecture and implementation of the denoising auto-encoder, including the number of layers and units used?
* How do you plan to address the potential issue of mode collapse, which is a common problem in GAN training?
* Can you provide more visual examples of the generated samples, including examples of failure cases and cases where the proposed method outperforms existing methods?