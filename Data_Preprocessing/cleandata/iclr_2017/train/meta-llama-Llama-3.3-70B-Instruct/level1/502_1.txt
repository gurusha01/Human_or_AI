Summary
The paper proposes an automatic dialogue evaluation model (ADEM) that learns to predict human-like scores for input responses. ADEM is trained on a new dataset of human response scores and uses a hierarchical recurrent neural network (RNN) to predict scores. The model is shown to correlate significantly with human judgements at both the utterance and system-level, outperforming traditional word-overlap metrics such as BLEU. ADEM is also able to generalize to evaluating new models, whose responses were unseen during training, without a drop in performance.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in dialogue research, namely the evaluation of dialogue responses. Secondly, the approach is well-motivated and supported by experimental results, demonstrating the effectiveness of ADEM in predicting human-like scores.
Supporting Arguments
The paper provides a clear and thorough explanation of the problem and the proposed solution. The experimental results are convincing, showing that ADEM outperforms traditional word-overlap metrics and can generalize to new models. The use of a hierarchical RNN and semi-supervised learning is well-justified, and the results demonstrate the effectiveness of this approach. Additionally, the paper provides a thorough analysis of the results, including a failure analysis and an examination of the correlation between ADEM scores and human judgements.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points:
* Provide more details on the dataset collection process, including the criteria used to select the Twitter responses and the instructions given to the human evaluators.
* Consider using additional evaluation metrics, such as precision and recall, to provide a more comprehensive understanding of ADEM's performance.
* Provide more analysis on the generalizability of ADEM to other dialogue domains, such as task-oriented dialogue systems.
* Consider using more advanced techniques, such as attention mechanisms or graph-based models, to further improve the performance of ADEM.
Questions for the Authors
I would like the authors to clarify the following points:
* How did the authors select the specific models (TF-IDF, Dual Encoder, HRED, and human-generated responses) used to generate the candidate responses?
* Can the authors provide more details on the hyperparameter tuning process for ADEM, including the range of values considered for each hyperparameter?
* How do the authors plan to address the issue of generic responses, which are often rated highly by humans but may not be desirable in a dialogue system?