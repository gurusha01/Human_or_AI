Summary
The paper proposes a neural attention model with a learnable retinal sampling lattice, trained on a visual search task requiring the classification of an object in a cluttered scene. The model learns to create an eccentricity-dependent layout with a high-acuity region at the center, similar to the primate retina. The authors explore the properties of this layout and its dependence on task constraints, providing insights into the functionality of the high-acuity region and its relationship to zoom.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and well-motivated question, providing a clear contribution to the understanding of neural attention models and their relationship to biological vision; (2) the approach is well-supported by empirical results, demonstrating the emergence of an eccentricity-dependent sampling lattice and its dependence on task constraints.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem, highlighting the importance of understanding the design of the primate retina and its potential relationship to neural attention models. The authors propose a novel approach, using a learnable retinal sampling lattice, and demonstrate its effectiveness in learning an eccentricity-dependent layout. The results are well-supported by empirical evidence, including visualizations of the learned layouts and quantitative analyses of their properties.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points: (1) provide more discussion on the implications of their results for our understanding of biological vision and the design of neural attention models; (2) explore the potential applications of their approach to more complex and naturalistic visual scenes; (3) consider additional experiments to further investigate the relationship between the high-acuity region and zoom.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) Can you provide more details on the initialization of the kernel filters and how it affects the learned layouts? (2) How do the results change when using different datasets or task constraints? (3) Can you discuss the potential limitations of their approach and how they might be addressed in future work?