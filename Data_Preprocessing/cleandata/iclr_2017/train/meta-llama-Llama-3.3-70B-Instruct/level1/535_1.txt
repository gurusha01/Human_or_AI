Summary
The paper proposes a novel video captioning model, called Adaptive SpatioTemporal representation with dynAmic abstRaction (ASTAR), which utilizes a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. The model employs a novel attention mechanism with spatiotemporal alignment to adaptively and sequentially focus on different layers of CNN features and local spatiotemporal regions of the feature maps at each layer. The authors claim that their approach achieves state-of-the-art performance on the YouTube2Text benchmark.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, addressing the limitations of previous models in video captioning, and (2) the paper supports its claims with experimental results on a benchmark dataset, demonstrating the effectiveness of the proposed attention mechanism.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of video captioning and the limitations of previous models. The authors motivate their approach by highlighting the importance of adaptively selecting and weighting feature abstraction and location within a layer-dependent feature map. The experimental results on the YouTube2Text benchmark demonstrate the effectiveness of the proposed model, outperforming previous state-of-the-art results. The use of a single model without averaging over an ensemble of models is also a notable achievement.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the attention mechanism, such as the choice of hyperparameters and the effect of different attention weights on the model's performance. Additionally, it would be interesting to see a more detailed analysis of the results, such as a comparison of the proposed model's performance on different types of videos or a qualitative evaluation of the generated captions.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the choice of hyperparameters for the attention mechanism, such as the number of filters and the pooling dimensions?
* How do you handle the case where the input video has a variable length, and how does the model adapt to different video lengths?
* Can you provide a more detailed comparison of the proposed model's performance with other state-of-the-art models on the YouTube2Text benchmark, including a discussion of the strengths and weaknesses of each model?