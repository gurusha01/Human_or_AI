The paper under review tackles the important question of defining context in word embedding models, a crucial aspect of natural language processing. The authors claim to provide the first systematic investigation of different context types and representations for learning word embeddings, conducting comprehensive experiments across 21 datasets and 4 tasks. This contribution has the potential to serve as a valuable guideline for the community in choosing the appropriate context for word embedding models.
Based on the provided abstract, I am inclined to accept this paper, with the primary reason being the well-motivated approach that addresses a significant open question in the field. The authors' systematic investigation and comprehensive experiments demonstrate a rigorous scientific approach.
The paper's strength lies in its thorough evaluation of different context types and representations, providing insights into context selection. This is a well-placed contribution in the literature, as the number of word embedding models is growing, and a systematic understanding of context is essential. The authors' decision to publish the code alongside the paper is also commendable, as it facilitates reproducibility and enables the community to build upon their work.
To further improve the paper, I would suggest that the authors provide more details on the specific context types and representations evaluated, as well as the tasks and datasets used in the experiments. Additionally, it would be helpful to discuss the implications of the findings and how they can be applied in practice. Questions I would like the authors to answer include: What specific context types and representations were evaluated, and how were they selected? How do the results generalize across different tasks and datasets? What are the potential limitations of the study, and how might they be addressed in future work? Answering these questions would provide additional evidence to support the claims made in the paper and increase confidence in the assessment.