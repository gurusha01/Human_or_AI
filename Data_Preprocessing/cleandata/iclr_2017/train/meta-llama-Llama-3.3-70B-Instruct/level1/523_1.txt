The paper proposes an approximate strategy, called adaptive softmax, to efficiently train neural network-based language models over very large vocabularies. The approach exploits the unbalanced word distribution to form clusters that minimize the expectation of computational complexity, making it particularly suited for graphical processing units. The authors claim that their approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.
I decide to accept this paper, with the key reasons being that the approach is well-motivated and the paper supports its claims with thorough experiments and analysis. The authors provide a clear and detailed explanation of their method, including the computational complexity analysis and the optimization strategy. The experimental results demonstrate the effectiveness of the adaptive softmax approach, showing significant speed-ups and comparable perplexity to the full softmax.
The paper is well-organized, and the authors provide a thorough review of related work, highlighting the differences and advantages of their approach. The experiments are well-designed, and the results are clearly presented, making it easy to understand the benefits of the adaptive softmax. The authors also provide additional insights and analysis, such as the importance of keeping a short-list of frequent words in the root node and reducing the capacity of rare words.
To further improve the paper, I would suggest that the authors provide more details on the implementation of the adaptive softmax, such as the specific optimization algorithms used and the hyperparameter tuning process. Additionally, it would be interesting to see more analysis on the trade-offs between efficiency and accuracy, and how the approach performs on different types of datasets and models.
I would like to ask the authors to clarify the following points: (1) How do the authors determine the optimal number of clusters and the cluster sizes? (2) Can the authors provide more details on the computational complexity analysis and how it is used to optimize the adaptive softmax? (3) How does the approach perform on datasets with different vocabulary sizes and distributions? (4) Are there any plans to apply the adaptive softmax to other domains or models, such as computer vision or reinforcement learning?