Summary
The paper proposes a novel LSTM parametrization, called Normalized LSTM, which preserves the means and variances of the hidden states and memory cells across time. This approach is motivated by the need to address the vanishing and exploding gradient problems in recurrent neural networks. The authors derive the Normalized LSTM by analytically compensating for the variance of the cell and hidden states, and propose a scheme to initialize the weight matrices. The paper also investigates the impact of the Normalized LSTM on the gradient flow and presents experimental results on character-level language modeling and image generative modeling tasks.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in recurrent neural networks, and (2) the approach is well-motivated and supported by theoretical and empirical evidence.
Supporting Arguments
The paper provides a clear and concise overview of the problem and the proposed solution. The authors demonstrate a good understanding of the literature and provide a thorough analysis of the Normalized LSTM. The experimental results are convincing, showing that the proposed approach performs similarly or better than other recurrent normalization approaches, while being faster to execute. The paper also provides a detailed analysis of the gradient flow, which is an important aspect of understanding the behavior of recurrent neural networks.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the computational complexity of the Normalized LSTM and compare it to other approaches. Additionally, it would be interesting to see more experiments on other tasks and datasets to further demonstrate the effectiveness of the proposed approach. The authors may also want to consider discussing the potential limitations and future directions of their work.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on how the variance estimates of the cell and hidden states are computed and updated during training?
* How do you initialize the rescaling parameters γx, γh, and γc, and what is the effect of different initialization schemes on the performance of the Normalized LSTM?
* Have you considered applying the Normalized LSTM to other types of recurrent neural networks, such as GRUs or bidirectional LSTMs?