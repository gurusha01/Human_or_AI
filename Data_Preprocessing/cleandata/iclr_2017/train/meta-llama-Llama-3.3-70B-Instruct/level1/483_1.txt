This paper proposes a spatiotemporal attentional model that learns where to look in a video directly from human fixation data. The model, called Recurrent Mixture Density Network (RMDN), uses a mixture of Gaussians to express the probability of saliency for each pixel and leverages deep 3D convolutional features and a Long Short-Term Memory (LSTM) network to model temporal consistency.
The paper claims to contribute to the field of computer vision by providing a novel approach to saliency prediction in videos, which can be used to improve action recognition accuracy. The authors demonstrate the effectiveness of their model on two challenging datasets, Hollywood2 and UCF101, and show that it outperforms state-of-the-art methods for saliency prediction.
Based on the provided information, I decide to accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and relevant problem in the field of computer vision, namely saliency prediction in videos, and provides a well-motivated approach to address it.
2. The paper provides extensive experimental results that demonstrate the effectiveness of the proposed model, including comparisons with state-of-the-art methods and ablation studies to analyze the impact of different design choices.
The paper supports its claims through a thorough evaluation of the proposed model, including quantitative and qualitative results on two datasets. The authors also provide a detailed description of the model architecture, training procedure, and implementation details, which facilitates reproducibility and understanding of the results.
To further improve the paper, I would suggest the following:
* Provide more insights into the interpretability of the saliency maps generated by the model, and how they relate to human fixations and action recognition.
* Consider adding more comparisons with other state-of-the-art methods for action recognition, to further demonstrate the effectiveness of the proposed approach.
* Provide more details on the computational resources required to train and test the model, and how it can be optimized for real-time applications.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to extend the model to handle more complex scenarios, such as videos with multiple objects or actions?
* Can the authors provide more insights into the role of the LSTM network in modeling temporal consistency, and how it compares to other recurrent neural network architectures?
* How do the authors plan to address the potential issue of overfitting, given the relatively small size of the training datasets used in the experiments?