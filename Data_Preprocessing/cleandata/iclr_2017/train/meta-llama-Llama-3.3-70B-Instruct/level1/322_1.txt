The paper proposes a novel framework for automatically determining the optimal size of a neural network for a given task without prior information. The authors introduce nonparametric neural networks, a non-probabilistic framework for conducting optimization over all possible network sizes, and prove its soundness when network growth is limited via an `p penalty. They also develop a novel optimization algorithm, Adaptive Radial-Angular Gradient Descent (AdaRad), and demonstrate its effectiveness in experiments.
The specific question tackled by the paper is how to automatically find a good network size during a single training cycle, without requiring an expensive global search and training many networks from scratch. The approach is well-motivated, as it addresses a significant limitation of current neural network training methods, which often rely on manual tuning of hyperparameters or expensive grid searches.
The paper provides a thorough analysis of the proposed framework, including a proof of its soundness and an evaluation of its performance on several benchmark datasets. The results show that nonparametric neural networks can achieve competitive performance with parametric networks, while requiring fewer parameters and less training time.
However, there are some limitations to the approach. For example, the paper notes that nonparametric neural networks may not perform well on datasets with a large number of classes or complex relationships between inputs and outputs. Additionally, the authors acknowledge that the choice of hyperparameters, such as the regularization parameter λ, can significantly affect the performance of the framework.
To improve the paper, I would suggest providing more detailed analysis of the limitations of the approach and potential avenues for future research. Additionally, it would be helpful to include more comparisons with other state-of-the-art methods for neural network training and hyperparameter tuning.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors choose the value of λ, and what is the effect of different values of λ on the performance of the framework?
* Can the authors provide more insight into why nonparametric neural networks may not perform well on datasets with a large number of classes or complex relationships between inputs and outputs?
* How does the AdaRad algorithm compare to other optimization algorithms, such as Adam or RMSprop, in terms of convergence rate and stability?
Overall, I would accept the paper, as it presents a novel and well-motivated framework for automatically determining the optimal size of a neural network, and demonstrates its effectiveness in experiments. However, I would suggest that the authors address the limitations and potential avenues for future research in more detail. 
Decision: Accept 
Reasons: 
1. The paper tackles a significant problem in neural network training, and the approach is well-motivated and novel.
2. The paper provides a thorough analysis of the proposed framework, including a proof of its soundness and an evaluation of its performance on several benchmark datasets.
3. The results show that nonparametric neural networks can achieve competitive performance with parametric networks, while requiring fewer parameters and less training time.
However, I would like the authors to address the limitations and potential avenues for future research in more detail, and provide more comparisons with other state-of-the-art methods for neural network training and hyperparameter tuning. 
Additional feedback: 
* Provide more detailed analysis of the limitations of the approach and potential avenues for future research.
* Include more comparisons with other state-of-the-art methods for neural network training and hyperparameter tuning.
* Consider providing more insight into the choice of hyperparameters, such as the regularization parameter λ, and the effect of different values of λ on the performance of the framework.