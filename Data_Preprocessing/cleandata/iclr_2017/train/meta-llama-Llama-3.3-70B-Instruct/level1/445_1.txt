Summary
The paper proposes a novel approach to training conversational agents using reinforcement learning, where the agent learns from feedback provided by a teacher in an online fashion. The authors explore two types of feedback: explicit numerical rewards and textual feedback, and demonstrate the effectiveness of their approach using a simulator and real experiments with Mechanical Turk. The paper contributes to the development of end-to-end learning systems that can improve their performance through online interactions with humans.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in the field of conversational agents, and (2) the approach is well-motivated and supported by thorough experiments.
Supporting Arguments
The paper provides a clear and well-structured presentation of the problem, related work, and the proposed approach. The authors demonstrate a good understanding of the limitations of existing methods and propose a novel solution that addresses these limitations. The experiments are thorough and well-designed, and the results demonstrate the effectiveness of the proposed approach. The paper also provides a detailed analysis of the results and discusses the implications of the findings.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points:
* Provide more details on the implementation of the simulator and the Mechanical Turk experiments, such as the number of participants and the instructions provided to them.
* Discuss the potential limitations of the proposed approach, such as the reliance on high-quality feedback from the teacher and the potential for bias in the feedback.
* Consider providing more comparisons with other related work, such as the use of deep reinforcement learning methods for conversational agents.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the architecture of the MemN2N model used in the experiments, such as the number of layers and the dimensionality of the embeddings?
* How did you handle the case where the teacher provides inconsistent or noisy feedback, and what were the implications of this on the performance of the agent?
* Can you discuss the potential applications of the proposed approach to other domains, such as customer service or language learning?