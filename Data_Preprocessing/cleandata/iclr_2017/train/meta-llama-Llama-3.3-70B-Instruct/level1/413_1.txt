Summary
The paper introduces Sigma-Delta networks, a novel approach to reducing computation in deep neural networks by taking advantage of temporal redundancy in data. The authors propose a method to convert any pre-trained deep network into an optimally efficient Sigma-Delta network, which communicates discretized changes in activation between layers. The paper demonstrates the effectiveness of this approach on various datasets, including Temporal-MNIST and video data, showing significant reductions in computational cost.
Decision
I decide to Accept this paper, with two key reasons for this choice:
1. The paper tackles a specific and important problem in deep learning, namely reducing computation in neural networks, which is crucial for efficient processing of temporal data.
2. The approach is well-motivated and supported by theoretical analysis and experimental results, demonstrating the effectiveness of Sigma-Delta networks in reducing computation while maintaining accuracy.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of computational waste in deep neural networks and motivates the need for a more efficient approach. The authors propose a novel solution, Sigma-Delta networks, which is based on communicating discretized changes in activation between layers. The paper provides a thorough analysis of the approach, including theoretical results and experimental evaluations on various datasets. The results demonstrate significant reductions in computational cost, making this approach promising for efficient processing of temporal data.
Additional Feedback
To further improve the paper, I suggest the authors provide more detailed analysis of the trade-off between accuracy and computation, as well as more extensive experiments on larger datasets. Additionally, it would be helpful to discuss the potential applications of Sigma-Delta networks in real-world scenarios, such as video processing and autonomous systems.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the optimization method used to convert pre-trained networks into Sigma-Delta networks?
2. How do you plan to extend this approach to other types of neural networks, such as recurrent neural networks?
3. What are the potential limitations of Sigma-Delta networks, and how do you plan to address them in future work?