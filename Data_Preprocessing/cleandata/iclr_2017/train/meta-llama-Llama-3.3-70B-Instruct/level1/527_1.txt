Summary
The paper introduces a novel recurrent neural network architecture called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM is more expressive for autoregressive density estimation and demonstrate its effectiveness in a range of character-level language modeling tasks, outperforming standard LSTM and its deep variants. The paper also explores the application of mLSTM to multilingual learning and dynamic evaluation, showing promising results.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear explanation of the limitations of existing RNN architectures and demonstrate how mLSTM addresses these limitations.
Supporting Arguments
The paper provides a thorough analysis of the strengths and weaknesses of existing RNN architectures, including LSTM and mRNN. The authors motivate the need for a hybrid architecture that combines the benefits of both, and provide a clear explanation of how mLSTM achieves this. The empirical results demonstrate the effectiveness of mLSTM in a range of tasks, including character-level language modeling and multilingual learning. The paper also provides a detailed analysis of the results, including an investigation of the impact of surprising inputs on the performance of mLSTM.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the interpretability of the mLSTM architecture. How do the factorized hidden weights and input-dependent transition functions contribute to the overall performance of the model? Are there any specific applications or tasks where mLSTM is particularly well-suited? Additionally, it would be interesting to see a more detailed comparison of mLSTM with other state-of-the-art RNN architectures, such as gated recurrent units (GRUs) and recurrent highway networks (RHNs).
Questions for the Authors
1. Can you provide more insight into the training process for mLSTM? How did you initialize the factorized hidden weights, and what learning rate schedule did you use?
2. How do you think mLSTM could be applied to word-level language modeling tasks? Would the architecture need to be modified in any way?
3. Can you provide more details on the dynamic evaluation process used in the experiments? How did you adapt the weights of the model during evaluation, and what were the hyperparameters used for the RMSprop optimizer?