Summary
The paper proposes Trained Ternary Quantization (TTQ), a method to reduce the precision of weights in neural networks to ternary values, achieving significant model compression while maintaining or even improving accuracy. The authors introduce two trainable scaling coefficients for each layer, which are learned during training using back-propagation. The method is evaluated on CIFAR-10 and ImageNet datasets, showing improved accuracy over previous ternary weight networks and even surpassing full-precision models in some cases.
Decision
I decide to Accept this paper, with two key reasons: (1) the approach is well-motivated and placed in the literature, addressing the important problem of model compression for mobile devices, and (2) the paper provides strong empirical evidence supporting the claims, with thorough experiments on multiple datasets and comparisons to previous work.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed method, including a detailed description of the quantization process and the training procedure. The authors also provide a thorough analysis of the trade-off between sparsity and accuracy, showing that the proposed method can achieve a good balance between model compression and performance. The experimental results are impressive, with the proposed method outperforming previous ternary weight networks and even full-precision models in some cases.
Additional Feedback
To further improve the paper, I suggest the authors provide more insights into the learned ternary weights and scaling coefficients, such as visualizations or analyses of the learned patterns. Additionally, it would be interesting to see more experiments on other datasets and models, to further demonstrate the generality and effectiveness of the proposed method. I also have some questions for the authors: (1) How do the learned scaling coefficients relate to the layer-wise thresholds, and can they be used to inform the choice of threshold values? (2) Can the proposed method be applied to other types of neural networks, such as recurrent or graph neural networks? (3) Are there any plans to release the implementation of the proposed method as open-source code, to facilitate further research and applications?