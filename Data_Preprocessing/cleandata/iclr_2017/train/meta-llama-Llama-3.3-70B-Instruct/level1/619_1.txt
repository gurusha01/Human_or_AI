Summary
The paper proposes a simple yet effective technique for training deep neural networks by adding annealed Gaussian noise to the gradient. This approach is shown to be complementary to advanced stochastic optimization algorithms such as Adam and AdaGrad, and can help avoid overfitting and achieve lower training loss. The authors demonstrate the effectiveness of this technique on a variety of complex models, including state-of-the-art deep networks for question answering and algorithm learning.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in deep learning, namely the optimization of complex neural networks, and (2) the approach is well-motivated and supported by thorough experiments on various models.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of optimizing deep neural networks, and motivates the use of annealed Gaussian noise as a simple and effective solution. The authors provide a thorough analysis of the related work, and demonstrate the effectiveness of their approach on a range of complex models, including End-To-End Memory Networks, Neural Programmer, and Neural Random Access Machines. The experiments are well-designed and provide strong evidence for the benefits of adding gradient noise, including improved performance, robustness to initialization, and reduced overfitting.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the theoretical foundations of their approach, and explore the relationship between gradient noise and other optimization techniques, such as stochastic gradient descent and simulated annealing. Additionally, it would be interesting to see more experiments on the sensitivity of the approach to hyperparameter tuning, and to explore the application of gradient noise to other domains, such as natural language processing and computer vision.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) Can you provide more intuition on why annealed Gaussian noise is more effective than fixed noise or other types of noise? (2) How do you choose the hyperparameters for the noise schedule, and are there any guidelines for tuning these parameters in practice? (3) Have you explored the application of gradient noise to other optimization algorithms, such as stochastic gradient descent with momentum or Nesterov acceleration?