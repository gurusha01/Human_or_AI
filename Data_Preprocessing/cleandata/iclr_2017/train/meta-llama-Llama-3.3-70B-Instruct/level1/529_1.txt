This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning (RL) to refine a pre-trained Recurrent Neural Network (RNN) for sequence generation tasks. The approach uses a pre-trained RNN to supply part of the reward value in an RL model, allowing the model to learn from data while incorporating domain-specific constraints.
I decide to accept this paper for several reasons. Firstly, the paper tackles a specific and well-defined problem in sequence generation, namely the difficulty of ensuring multi-step generated sequences have coherent global structure. The approach proposed is well-motivated and builds upon existing work in RL and sequence learning. The paper provides a clear and detailed explanation of the RL Tuner framework, including its connection to stochastic optimal control and KL control.
The paper supports its claims with empirical results on music generation tasks, demonstrating that the RL Tuner approach can produce more pleasing and coherent melodies than a baseline RNN model. The results are impressive, with the RL Tuner models outperforming the baseline model on several metrics, including music theory rules and user preferences.
To further improve the paper, I would suggest providing more analysis on the trade-off between the influence of data and heuristic rewards, as controlled by the parameter c. Additionally, it would be interesting to see more examples of generated melodies and a more detailed discussion on the limitations of the approach.
Some questions I would like the authors to clarify include: (1) How did the authors choose the specific music theory rules used in the reward function, and are there any plans to explore more complex or nuanced rules? (2) Can the RL Tuner approach be applied to other sequence generation tasks, such as language modeling or text generation, and if so, what modifications would be necessary? (3) How does the RL Tuner approach compare to other methods for incorporating domain knowledge into sequence generation models, such as generative adversarial networks (GANs) or variational autoencoders (VAEs)?