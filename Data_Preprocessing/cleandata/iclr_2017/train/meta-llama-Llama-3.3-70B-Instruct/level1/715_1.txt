The paper proposes a novel approach to reduce the computational complexity of deep convolutional neural networks (CNNs) by pruning feature maps and kernels. The authors argue that the increasing depth and width of neural networks demand higher computational power, which can be a major obstacle in deploying deep learning models on resource-limited devices. The proposed approach aims to address this issue by inducing sparsity in the network, which can lead to significant computational savings.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper provides extensive experimental evaluations to support the claims. The authors provide a clear and concise overview of the related work, highlighting the limitations of existing pruning techniques and the benefits of their proposed approach. The experimental results demonstrate the effectiveness of the proposed approach in reducing computational complexity while maintaining the network's performance.
The paper provides supporting arguments for the decision by demonstrating the benefits of the proposed approach over existing pruning techniques. The authors show that their approach can achieve higher pruning ratios with less computational overhead, making it more suitable for real-time inference on resource-limited devices. The experimental results are extensive and well-presented, covering various datasets and network architectures.
To further improve the paper, I would like to provide some additional feedback. Firstly, it would be helpful to provide more insights into the selection of the pruning ratio and the number of random pruning masks to try. The authors mention that the sufficient number of random pruning masks to try depends on the pruning ratio, but a more detailed analysis of this relationship would be beneficial. Secondly, the authors could provide more discussion on the potential applications of the proposed approach, such as deploying deep learning models on edge devices or in real-time systems.
I would like to ask the authors to clarify the following points: (1) How do the authors determine the optimal pruning ratio for a given network and dataset? (2) Can the proposed approach be extended to other types of neural networks, such as recurrent neural networks or graph neural networks? (3) How does the proposed approach compare to other model compression techniques, such as quantization or knowledge distillation? Answering these questions would provide more clarity on the proposed approach and its potential applications.