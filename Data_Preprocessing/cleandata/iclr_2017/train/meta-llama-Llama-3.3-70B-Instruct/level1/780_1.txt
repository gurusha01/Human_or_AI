Summary
The paper proposes a new technique for collective operations, referred to as Linear Pipelining (LP), to reduce the cost of communication in parallel training of neural networks. The authors claim that LP demonstrates up to 2x higher bandwidth than Bidirectional Exchange (BE) techniques and provides an O(logP) speedup over Minimal Spanning Tree (MST) collectives. The paper also presents a theoretical analysis and experimental results to support the claims, showing that LP can reduce communication bottlenecks in practice while preserving the convergence properties of Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD).
Decision
I decide to Accept this paper with the following key reasons:
1. The paper tackles a specific and important problem in parallel training of neural networks, which is the reduction of communication overhead.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and the benefits of the proposed LP technique.
Supporting Arguments
The paper provides a thorough analysis of the problem and the proposed solution, including a theoretical analysis of the cost model and experimental results to support the claims. The authors also provide a detailed explanation of the LP technique and its implementation, which demonstrates a good understanding of the underlying architecture and the requirements of parallel training of neural networks. The experimental results show significant speedups over existing methods, which demonstrates the effectiveness of the proposed approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the LP technique, including any optimizations or trade-offs made to achieve the reported performance. Additionally, it would be helpful to include more experiments to evaluate the scalability of the proposed approach and its applicability to different types of neural networks. Some questions I would like the authors to answer include:
* How does the block size affect the performance of the LP technique, and what is the optimal block size for different types of neural networks?
* How does the proposed approach handle faults or failures in the parallel training process, and what are the implications for the convergence properties of BSP-SGD?
* Can the LP technique be applied to other types of parallel algorithms, such as asynchronous SGD or distributed optimization methods?