Summary
The paper proposes a novel acceleration framework, DeepRebirth, which aims to speed up the deployment of deep neural networks on mobile devices by optimizing non-tensor layers and their neighboring units. The framework merges non-tensor layers with their neighboring sparse tensor layers to form a single dense layer, reducing the execution time and memory intensity. The authors demonstrate the effectiveness of DeepRebirth on various deep learning models, including GoogLeNet, AlexNet, and ResNet, achieving significant speed-up and energy savings with negligible accuracy loss.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and addresses a significant problem in deploying deep learning models on mobile devices, and (2) the paper provides extensive experimental results demonstrating the effectiveness of DeepRebirth on various models and hardware platforms.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of deploying deep neural networks on mobile devices, highlighting the limitations of existing approaches. The authors propose a novel solution, DeepRebirth, which is guided by the key observation that non-tensor layers are the major obstacles to real-time mobile CPU execution. The paper provides a detailed description of the DeepRebirth framework, including the merging of non-tensor layers with their neighboring sparse tensor layers. The experimental results demonstrate the effectiveness of DeepRebirth in achieving significant speed-up and energy savings with negligible accuracy loss.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the fine-tuning process of the merged layers and the impact of different hyperparameters on the performance of DeepRebirth. Additionally, it would be interesting to see a comparison of DeepRebirth with other state-of-the-art model compression techniques, such as pruning and quantization. The authors may also consider providing more insights into the trade-offs between speed-up, energy savings, and accuracy loss.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the merging process of non-tensor layers with their neighboring sparse tensor layers?
2. How do you determine the optimal number of layers to merge, and what is the impact of merging too many or too few layers on the performance of DeepRebirth?
3. Can you provide more insights into the energy savings achieved by DeepRebirth, and how it compares to other energy-saving techniques, such as dynamic voltage and frequency scaling?