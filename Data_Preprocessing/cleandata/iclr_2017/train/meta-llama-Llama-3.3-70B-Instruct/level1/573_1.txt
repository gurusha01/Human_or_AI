Summary
The paper presents a general problem setting for training and testing the ability of agents to gather information efficiently. The authors propose a collection of tasks that require searching for fragments of information in a partially-observed environment to achieve a goal. They combine deep architectures with techniques from reinforcement learning to develop agents that solve these tasks. The agents learn to search actively and intelligently for new information, reducing their uncertainty and exploiting acquired information.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a well-motivated and important problem in artificial intelligence, and (2) the approach is well-supported by empirical results and rigorous analysis.
Supporting Arguments
The paper provides a clear and well-structured presentation of the problem setting, related work, and the proposed approach. The authors demonstrate the effectiveness of their approach through a series of experiments on various tasks, including cluttered MNIST, BlockWorld, CelebA, and Hangman. The results show that the agents learn to search efficiently and intelligently, outperforming baseline models and achieving state-of-the-art performance in some cases. The paper also provides a thorough analysis of the approach, including the use of intrinsic and extrinsic rewards, and the design of the model architectures.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the computational resources required to train the models. Additionally, it would be interesting to see more analysis on the learned policies and the exploration-exploitation trade-off in the agents' behavior. Some questions I would like the authors to answer include: (1) How do the agents' policies change as the number of questions increases? (2) Can the authors provide more insights into the learned representations and how they relate to the task-specific goals? (3) How do the results generalize to more complex and realistic environments?