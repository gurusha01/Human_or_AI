Summary
The paper proposes a general class of language models that treat reference as an explicit stochastic latent variable, allowing models to create mentions of entities and their attributes by accessing external databases and internal state. The authors develop a language model that has a specific module for generating referring expressions (REs) and demonstrate its efficacy in three tasks: dialogue modeling, recipe generation, and coreference-based language modeling.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated problem in language modeling, and (2) the approach is well-supported by experimental results on three tasks, showing improved performance over baseline models.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of reference in language modeling, highlighting the limitations of previous works that neglect to model REs explicitly. The proposed framework is well-motivated and placed in the literature, drawing on existing work on attention mechanisms, copying mechanisms, and coreference resolution. The experimental results demonstrate the effectiveness of the proposed model in three tasks, with improved performance over baseline models that do not explicitly incorporate reference decisions.
Additional Feedback
To further improve the paper, I suggest the authors provide more detailed analysis of the results, including error analysis and discussion of the limitations of the proposed model. Additionally, it would be helpful to include more examples of the generated text to illustrate the quality of the output. The authors may also consider exploring other evaluation metrics, such as human evaluation, to assess the accuracy and fluency of the generated text.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the table attention mechanism, including the specific architecture and hyperparameters used?
2. How do you handle out-of-vocabulary words in the recipe generation task, where the ingredient list may contain words not seen during training?
3. Can you discuss the potential applications of the proposed model beyond the three tasks evaluated in the paper, such as question answering or text summarization?