Summary of the Paper's Contributions
The paper proposes a novel approach to reduce the memory requirements and power consumption of deep neural networks (DNNs) by introducing sparsely-connected networks. The authors demonstrate that by randomly removing up to 90% of the connections in fully-connected networks, they can improve the accuracy performance on three popular datasets (MNIST, CIFAR10, and SVHN). Additionally, they propose an efficient hardware architecture based on linear-feedback shift registers to reduce the memory requirements of the proposed sparsely-connected networks, which can save up to 90% of memory and 84% of energy compared to conventional implementations.
Decision and Key Reasons
I decide to Accept this paper with minor revisions. The two key reasons for this choice are:
1. The paper tackles a specific and important problem in the field of deep learning, namely reducing the memory requirements and power consumption of DNNs.
2. The approach is well-motivated and supported by experimental results, which demonstrate the effectiveness of the proposed sparsely-connected networks and hardware architecture.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of memory requirements and power consumption in DNNs, and motivates the need for sparsely-connected networks. The authors propose a novel approach to forming the sparse weight matrix using linear-feedback shift registers, which is both efficient and effective. The experimental results demonstrate the superiority of the proposed approach over state-of-the-art methods, including binarized and ternarized networks. The hardware architecture proposed in the paper is also efficient and can save significant amounts of memory and energy.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more details on the implementation of the stochastic number generator (SNG) unit and its impact on the overall performance of the sparsely-connected network. Additionally, it would be interesting to see more results on the robustness of the proposed approach to different types of noise and attacks.
Some questions I would like the authors to answer are:
* How do the authors plan to extend the proposed approach to other types of neural networks, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs)?
* What are the potential applications of the proposed sparsely-connected networks and hardware architecture in real-world scenarios, such as edge computing and IoT devices?
* How do the authors plan to address the potential issue of overfitting in the proposed sparsely-connected networks, especially when the number of connections is significantly reduced?