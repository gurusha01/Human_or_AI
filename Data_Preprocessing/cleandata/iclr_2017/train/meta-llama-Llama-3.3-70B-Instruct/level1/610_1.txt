Summary of the Paper's Contributions
The paper presents a comprehensive analysis of different defensive strategies against various adversary models in the context of deep neural networks (DNNs). The authors evaluate the robustness of the general robust adversarial retraining framework (RAD) with state-of-the-art defensive algorithms, such as distillation, autoencoder stacked with classifier (AEC), and an improved version, IAEC. The experimental results show that RAD performs significantly better than the other defensive algorithms in terms of classification error and robustness against different adversary models.
Decision and Reasons
Based on the review of the paper, I decide to Accept the paper with minor revisions. The reasons for this decision are:
1. The paper tackles a specific and important problem in the field of deep learning, namely, the vulnerability of DNNs to adversarial examples.
2. The approach is well-motivated, and the authors provide a thorough analysis of different defensive strategies against various adversary models.
3. The paper supports its claims with extensive experimental results, which demonstrate the effectiveness of the proposed RAD framework.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of adversarial examples and the existing defensive strategies. The authors also provide a detailed analysis of the RAD framework and its advantages over other defensive algorithms. The experimental results are thorough and well-presented, and they demonstrate the effectiveness of RAD in defending against different adversary models.
Additional Feedback
To improve the paper, I suggest the following:
1. Provide more details on the implementation of the RAD framework, such as the choice of hyperparameters and the training procedure.
2. Consider adding more adversary models to the evaluation, such as the Carlini and Wagner attack.
3. Provide more insights into the tradeoff between robustness and accuracy on normal data, and how to balance these two competing objectives.
Questions for the Authors
1. Can you provide more details on the choice of hyperparameters for the RAD framework, and how they were tuned?
2. How do you plan to extend the RAD framework to defend against more sophisticated adversary models, such as the Carlini and Wagner attack?
3. Can you provide more insights into the vulnerability of the IAEC algorithm, and how to improve its robustness against different adversary models?