This paper presents LipNet, a novel end-to-end deep learning model for lipreading that maps variable-length sequences of video frames to text sequences. The authors tackle the specific question of sentence-level lipreading, which is a notoriously difficult task for humans, especially in the absence of context. 
I decide to accept this paper, with the key reasons being that the approach is well-motivated and well-placed in the literature, and the paper supports its claims with rigorous empirical evaluations. The authors provide a thorough review of existing work on lipreading and clearly explain how their approach addresses the limitations of previous methods. The use of spatiotemporal convolutions, recurrent neural networks, and the connectionist temporal classification loss allows LipNet to learn effective representations of visual speech and achieve state-of-the-art performance on the GRID corpus.
The empirical results demonstrate the effectiveness of LipNet, with a 95.2% sentence-level word accuracy on the GRID corpus, outperforming experienced human lipreaders and previous state-of-the-art models. The authors also provide additional analyses, such as saliency visualizations and phoneme confusion matrices, to interpret the learned representations of LipNet and provide insights into its strengths and weaknesses.
To further improve the paper, I would suggest that the authors provide more details on the training process, such as the number of epochs, batch size, and learning rate schedule. Additionally, it would be interesting to see more comparisons with other state-of-the-art models, such as those using attention mechanisms or transfer learning. The authors could also consider evaluating LipNet on other datasets, such as those with more varied speaking styles or environments, to demonstrate its robustness and generalizability.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How did the authors choose the hyperparameters for LipNet, and what was the effect of varying these hyperparameters on the model's performance? How does LipNet handle cases where the speaker's mouth is partially occluded or the video quality is poor? Are there any plans to extend LipNet to handle multi-speaker conversations or more complex linguistic structures?