Summary
The paper presents empirical evidence for the universality of halting times in optimization algorithms applied to random systems, including spin glasses and deep learning. The authors demonstrate that the fluctuations of the halting time follow a distribution that remains unchanged even when the distribution on the landscape is changed, after centering and scaling. They identify two main universality classes: a Gumbel-like distribution and a Gaussian-like distribution, which appear in various algorithms, including conjugate gradient, gradient descent, and stochastic gradient descent.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and interesting question about the universality of halting times in optimization algorithms, which is well-motivated and well-placed in the literature. Secondly, the paper provides empirical evidence and supporting arguments for the claims, including a clear definition of universality and a quantitative way to test for it.
Supporting Arguments
The paper provides a clear and concise introduction to the concept of universality in optimization algorithms, including a definition and a discussion of the literature. The authors also provide a detailed description of the algorithms and systems used in the experiments, including the conjugate gradient algorithm, gradient descent, and stochastic gradient descent. The empirical results are presented in a clear and concise manner, with supporting figures and tables. The paper also discusses the implications of the results and provides suggestions for future work.
Additional Feedback
To improve the paper, I suggest that the authors provide more theoretical analysis and justification for the observed universality classes. Additionally, it would be helpful to include more details about the experimental setup and the parameters used in the simulations. Furthermore, the authors could discuss the potential applications and implications of the results in more detail.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details about the theoretical justification for the observed universality classes?
2. How do the authors plan to extend the results to more general classes of optimization algorithms and systems?
3. What are the potential applications and implications of the results in practice, and how can they be used to guide algorithm development and tuning?