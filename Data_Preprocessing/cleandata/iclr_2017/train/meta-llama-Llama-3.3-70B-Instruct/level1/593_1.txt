Summary
The paper introduces a new formulation of semi-supervised learning in variational autoencoders (VAEs) that allows for the incorporation of domain knowledge in characterizing certain aspects of variation in the data. The approach enables flexible specification of probabilistic encoders as directed graphical models via a stochastic computation graph, containing both continuous and discrete latent variables, with conditional distributions parameterized by neural networks. The authors demonstrate the effectiveness of their framework in learning disentangled representations, performing classification and regression tasks, and generating new data samples.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in deep generative models, namely incorporating domain knowledge to learn disentangled representations, and (2) the approach is well-motivated and supported by a thorough analysis of related work, as well as a comprehensive set of experiments that demonstrate its effectiveness.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of learning disentangled representations in VAEs, and motivates the need for incorporating domain knowledge to address this challenge. The authors provide a thorough review of related work, highlighting the limitations of existing approaches and the benefits of their proposed framework. The experimental results are impressive, demonstrating the ability of the framework to learn disentangled representations, perform classification and regression tasks, and generate new data samples that are comparable to or even surpass those obtained by state-of-the-art methods.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of their framework, including the specific architectures used for the recognition and generative models, as well as the hyperparameter settings used in the experiments. Additionally, it would be helpful to include more visualizations of the learned representations, to provide a clearer understanding of the disentanglement achieved by the framework. Finally, the authors may want to consider exploring the application of their framework to more complex datasets, such as those involving multiple modalities or high-dimensional data.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors: (1) Can you provide more details on the stochastic computation graph framework used to specify the probabilistic encoders, and how it is implemented in practice? (2) How do you handle the case where the supervision rate is very low, and the model is mostly trained on unsupervised data? (3) Have you explored the use of other types of neural network architectures, such as convolutional or recurrent networks, in the recognition and generative models?