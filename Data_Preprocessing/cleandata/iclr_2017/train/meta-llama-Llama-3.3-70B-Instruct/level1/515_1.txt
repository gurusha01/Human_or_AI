Summary
The paper introduces Exponential Machines (ExM), a predictor that models all interactions of every order between features. The key idea is to represent the exponentially large tensor of parameters in a compact multilinear format called Tensor Train (TT). The authors propose a stochastic Riemannian optimization procedure to train the model and demonstrate its effectiveness on several datasets, including synthetic data with high-order interactions and a recommender system dataset.
Decision
I decide to Accept this paper with minor revisions.
Reasons
The paper tackles a specific and important problem in machine learning, namely modeling high-order interactions between features. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods. The authors provide a rigorous theoretical framework for their method, including a proof of the inference complexity and a discussion of the advantages of the TT-format over other tensor factorizations.
Supporting arguments
The paper provides a thorough experimental evaluation of the proposed method, including a comparison with other approaches such as logistic regression, random forest, and kernel SVM. The results demonstrate the effectiveness of ExM in modeling high-order interactions and its ability to outperform other methods on certain datasets. The authors also provide a detailed discussion of the related work and the advantages of their approach over other methods.
Additional feedback
To improve the paper, I suggest the authors provide more details on the implementation of the stochastic Riemannian optimization procedure, including the choice of hyperparameters and the computational complexity of the algorithm. Additionally, it would be helpful to include more visualizations of the results, such as plots of the training and test losses, to provide a better understanding of the performance of the method.
Questions for the authors
1. Can you provide more details on the choice of hyperparameters for the stochastic Riemannian optimization procedure, such as the learning rate and the mini-batch size?
2. How does the computational complexity of the algorithm scale with the size of the dataset and the number of features?
3. Can you provide more visualizations of the results, such as plots of the training and test losses, to provide a better understanding of the performance of the method?