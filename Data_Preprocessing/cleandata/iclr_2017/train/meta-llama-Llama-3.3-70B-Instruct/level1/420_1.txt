Summary
The paper proposes a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. The authors demonstrate that this model outperforms existing memory-augmented neural language models on two corpora, but surprisingly, it mainly utilizes a memory of the five most recent output representations. This leads to the finding that a much simpler model based on the concatenation of recent output representations is on par with more sophisticated memory-augmented neural language models.
Decision
I decide to Accept this paper with the following key reasons:
1. The paper tackles a specific and well-motivated problem in neural language modeling, namely the limitation of conventional attention mechanisms in capturing long-range dependencies.
2. The authors propose a novel key-value attention mechanism and demonstrate its effectiveness through experiments on two corpora.
Supporting Arguments
The paper is well-organized, and the authors provide a clear and concise explanation of their proposed model and its components. The experiments are thorough, and the results are convincing, showing that the proposed model outperforms existing memory-augmented neural language models. The authors also provide a detailed analysis of the results, highlighting the surprising finding that the model mainly utilizes a short memory of recent output representations.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into why the model is unable to effectively utilize longer-range dependencies. Additionally, it would be interesting to see more experiments on other corpora and tasks to demonstrate the generalizability of the proposed model. The authors may also want to consider providing more details on the implementation of the model, such as the hyperparameter settings and the training procedure.
Questions for the Authors
1. Can you provide more insights into why the model is unable to effectively utilize longer-range dependencies, and what potential modifications could be made to address this issue?
2. How do you plan to encourage the model to attend over a longer history, as mentioned in the conclusion?
3. Can you provide more details on the implementation of the model, such as the hyperparameter settings and the training procedure?