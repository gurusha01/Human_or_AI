Summary of the Paper's Contributions
The paper proposes a novel approach to handwritten word recognition based on open-bigrams, which are pairs of letters that are not necessarily next to each other. The authors train Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to predict open-bigrams rather than characters and use a simple cosine similarity measure to decode words. The approach is evaluated on two public databases of handwritten words (Rimes and IAM) and shows comparable results to conventional decoding methods based on sequential character recognition.
Decision: Accept
I decide to accept this paper because it tackles a specific and interesting problem in handwritten word recognition, and the approach is well-motivated and supported by experimental results. The paper provides a clear and detailed description of the proposed method, and the experiments are well-designed and evaluated.
Supporting Arguments
The paper provides a thorough review of related work in handwritten word recognition and clearly explains the motivation behind the proposed approach. The authors demonstrate the effectiveness of the open-bigram representation in capturing the global letter order and show that it can be used for word recognition. The experimental results are promising, and the comparison to conventional decoding methods provides a good baseline for evaluation.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the error patterns of the open-bigram decoder and compare them to the errors made by conventional decoding methods. Additionally, it would be interesting to see how the proposed approach performs on other datasets or languages. The authors may also consider providing more details on the training process of the LSTM-RNNs and the hyperparameter tuning.
Questions for the Authors
1. Can you provide more insights into the choice of the open-bigram orders (d = 0 to 3) and how they affect the performance of the decoder?
2. How do you plan to address the issue of out-of-vocabulary words, which may not have a corresponding open-bigram representation?
3. Can you provide more details on the computational complexity of the proposed approach compared to conventional decoding methods?