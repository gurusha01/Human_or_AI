Summary
The paper proposes a novel neural network activation function, the Gaussian Error Linear Unit (GELU), which is the expected transformation of a stochastic regularizer. The GELU nonlinearity is compared to ReLU and ELU activations and demonstrates performance improvements across various tasks, including computer vision, natural language processing, and automatic speech recognition. The authors also introduce the Stochastic 0-I Map (SOI Map), a stochastic regularizer that can replace traditional nonlinearities, and show that it can achieve comparable performance to nonlinearities aided by dropout.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated and supported by empirical evidence. The authors provide a clear and concise explanation of the GELU and SOI Map, and the experimental results demonstrate the effectiveness of the proposed activation function.
Supporting Arguments
The paper tackles a specific question/problem by proposing a novel activation function that bridges the gap between stochastic regularizers and nonlinearities. The approach is well-motivated, as it combines intuitions from dropout, zoneout, and ReLUs, and is supported by empirical evidence from various experiments. The results show that the GELU outperforms previous nonlinearities, and the SOI Map can achieve comparable performance to nonlinearities aided by dropout.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the theoretical properties of the GELU and SOI Map, such as their curvature and non-monotonicity. Additionally, it would be interesting to see more experiments on the robustness of the GELU to different hyperparameters and architectures. The authors may also consider exploring other stochastic regularizers and their potential applications in neural networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the probabilistic interpretation of the GELU and how it relates to the SOI Map?
2. How do the authors plan to explore the design space of other stochastic regularizers, and what potential applications do they envision?
3. Can you provide more insights into the trade-offs between using the GELU and other nonlinearities, such as ReLUs and ELUs, in terms of computational cost and performance?