This paper proposes a novel approach to tackle micromanagement scenarios in the real-time strategy game StarCraft using reinforcement learning. The authors introduce a new algorithm, called zero-order (ZO) reinforcement learning, which combines direct exploration in the policy space and backpropagation. The algorithm is designed to address the challenges of large action spaces and delayed rewards in micromanagement scenarios.
The paper claims to contribute to the field of reinforcement learning by providing a new benchmark for reinforcement learning algorithms and introducing a novel algorithm that outperforms existing methods, such as Q-learning and REINFORCE, in micromanagement scenarios.
I decide to accept this paper with minor revisions. The reasons for this decision are:
1. The paper tackles a specific and challenging problem in the field of reinforcement learning, which is micromanagement in real-time strategy games.
2. The authors propose a novel algorithm that addresses the challenges of large action spaces and delayed rewards in micromanagement scenarios.
3. The experimental results show that the proposed algorithm outperforms existing methods, such as Q-learning and REINFORCE, in micromanagement scenarios.
The supporting arguments for this decision are:
* The paper provides a clear and concise introduction to the problem of micromanagement in real-time strategy games and the challenges of applying reinforcement learning to this domain.
* The authors provide a detailed description of the proposed algorithm, including the mathematical formulation and the implementation details.
* The experimental results are thorough and well-presented, and they demonstrate the effectiveness of the proposed algorithm in micromanagement scenarios.
To improve the paper, I suggest the following additional feedback:
* The authors could provide more details on the hyperparameter tuning process and the sensitivity of the algorithm to different hyperparameter settings.
* The authors could provide more insights into the learned policies and the strategies that the algorithm discovers in different micromanagement scenarios.
* The authors could discuss the potential applications of the proposed algorithm to other domains and the potential challenges and limitations of applying the algorithm to more complex scenarios.
Some questions that I would like the authors to answer to clarify my understanding of the paper are:
* Can the authors provide more details on the implementation of the ZO algorithm, including the specific architecture of the neural network and the optimization procedure?
* How do the authors handle the exploration-exploitation trade-off in the ZO algorithm, and what are the implications of this trade-off for the performance of the algorithm?
* Can the authors provide more insights into the learned policies and the strategies that the algorithm discovers in different micromanagement scenarios, and how do these policies and strategies relate to human play?