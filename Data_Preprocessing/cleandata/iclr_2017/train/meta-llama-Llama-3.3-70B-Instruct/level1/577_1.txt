Summary
This paper proposes a novel method to understand the roles and dynamics of intermediate layers in neural networks. The authors introduce the concept of linear classifier probes, which are used to measure the performance of an optimal linear classifier trained on the inputs of a given layer. This approach provides a new perspective on the information contained in each layer and allows for the visualization of the model's behavior during training. The paper demonstrates the effectiveness of this method on various models, including a simple MNIST convnet and the Inception v3 model.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and provides a new and interesting perspective on understanding neural networks. The paper is also well-written and easy to follow, with clear explanations and examples.
Supporting Arguments
The paper tackles a specific and important problem in the field of neural networks, namely the lack of interpretability of these models. The authors provide a clear and well-motivated approach to addressing this problem, and the results are convincing and well-presented. The use of linear classifier probes is a simple yet effective way to measure the performance of each layer, and the visualization of the model's behavior during training is a valuable tool for understanding the dynamics of the network.
Additional Feedback
One potential area for improvement is the scalability of the method to larger models and datasets. The authors mention that the Inception v3 experiment was challenging due to the large number of features, and it would be interesting to see how the method can be adapted to handle such cases. Additionally, it would be useful to see more comparisons with other methods for understanding neural networks, such as feature importance or layer-wise relevance propagation.
Questions for the Authors
1. How do the authors plan to address the scalability issue with larger models and datasets?
2. Can the authors provide more comparisons with other methods for understanding neural networks?
3. How do the authors think the linear classifier probes can be used in practice, for example, in model design or debugging?