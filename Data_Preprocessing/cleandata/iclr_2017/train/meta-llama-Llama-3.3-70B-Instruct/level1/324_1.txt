Summary
The paper proposes a method to prune filters in Convolutional Neural Networks (CNNs) to reduce computation costs without introducing irregular sparsity. The approach prunes filters with relatively low weight magnitudes and achieves significant reductions in FLOP (up to 34% for VGG-16 and 38% for ResNet-110) on CIFAR-10 without significant loss in accuracy. The authors also conduct sensitivity analysis on convolutional layers in ResNets, which improves the understanding of these networks.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
Supporting Arguments
The paper tackles a specific question of reducing computation costs in CNNs, which is a significant problem in the field. The approach is well-motivated, as it builds upon previous work on model compression and pruning. The authors provide a clear and concise explanation of their method, which is easy to follow. The results are impressive, with significant reductions in FLOP achieved without sacrificing accuracy. The sensitivity analysis on ResNets is also a valuable contribution, as it provides insights into the behavior of these networks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational resources used for the experiments, such as the specific GPU model and the batch size used. Additionally, it would be interesting to see more results on other datasets and networks, such as ImageNet and ResNet-50. The authors may also consider providing more analysis on the trade-off between accuracy and computation cost, as this is an important consideration in many applications.
Questions for the Authors
I would like the authors to clarify the following points: (1) How do the authors determine the optimal pruning ratio for each layer, and is this process automated or manual? (2) Can the authors provide more insights into why the `1-norm works better than the `2-norm for filter pruning, and are there any theoretical justifications for this? (3) How do the authors plan to extend this work to other types of neural networks, such as Recurrent Neural Networks (RNNs) or Generative Adversarial Networks (GANs)?