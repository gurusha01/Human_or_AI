Summary of the Paper
The paper introduces an architecture for an open-vocabulary neural language model, which computes word representations on-the-fly using a convolutional network followed by a pooling layer. This approach allows the model to consider any word, including out-of-vocabulary words, and is particularly useful for morphologically-rich languages such as Czech. The model is trained using Noise Contrastive Estimation (NCE) and is evaluated on a machine translation task, where it is used to re-rank the output of a translation system.
Decision
I decide to accept this paper, with the main reason being that it tackles a specific and important problem in natural language processing, namely the limitation of traditional word-based language models in handling out-of-vocabulary words. The paper proposes a novel approach to address this issue and provides promising experimental results.
Supporting Arguments
The paper is well-motivated, and the authors provide a clear explanation of the limitations of traditional word-based language models. The proposed approach is well-placed in the literature, and the authors provide a thorough review of related work. The experimental results are promising, with a gain of up to 0.7 BLEU points in the machine translation task. The paper also provides a detailed analysis of the results and discusses the challenges and limitations of the approach.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the training process, including the hyperparameter settings and the optimization algorithm used. Additionally, it would be helpful to include more analysis on the learned representations and how they compare to traditional word embeddings. The authors may also want to consider evaluating the model on other tasks, such as language modeling or text classification, to further demonstrate its effectiveness.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the convolutional network architecture used to compute the word representations?
* How did you choose the hyperparameters for the NCE training, such as the number of negative samples and the learning rate?
* Can you provide more analysis on the learned representations, such as visualizations or comparisons to traditional word embeddings?
* Have you considered evaluating the model on other tasks, such as language modeling or text classification, and if so, what were the results?