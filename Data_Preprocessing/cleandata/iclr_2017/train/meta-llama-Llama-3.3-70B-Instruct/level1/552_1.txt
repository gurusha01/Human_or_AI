Summary
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN architecture uses a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation to update the hidden state, ensuring that the forward hidden state activation norm and backward gradient norm are preserved. The authors demonstrate the effectiveness of their approach on a simplified memory copy task, achieving dependencies of up to 5,000 timesteps.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by theoretical guarantees. The authors provide a clear and concise explanation of the problem, and their proposed solution is novel and interesting.
Supporting Arguments
The paper tackles a specific and important problem in the field of RNNs, and the authors provide a thorough analysis of the existing literature. The proposed DORNN architecture is well-motivated, and the authors provide theoretical guarantees that it does not suffer from vanishing or exploding gradients. The experimental results demonstrate the effectiveness of the approach on a simplified memory copy task, and the authors provide a clear discussion of the implications and limitations of their work.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed comparisons with existing approaches, such as the Unitary Evolution Recurrent Neural Network (uRNN) and the Long Short-Term Memory (LSTM) network. Additionally, it would be interesting to see the authors explore more complex tasks and datasets to demonstrate the scalability and effectiveness of their approach. The authors may also want to consider providing more details on the optimization process and the choice of hyperparameters.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the choice of the rotation plane parameterization and how it compares to other orthogonal matrix representations?
2. How do you plan to address the potential limitations of the DORNN architecture, such as the linearity of the input-dependent transition and the sensitivity to the randomly initialized planes?
3. Can you provide more insights into the training process and the optimization of the DORNN architecture, including the choice of hyperparameters and the convergence of the training process?