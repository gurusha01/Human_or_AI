Summary of the Paper's Claims and Contributions
The paper proposes a general framework for training policies in deep reinforcement learning, particularly for tasks with sparse rewards or long horizons. The framework involves learning useful skills in a pre-training environment using a proxy reward signal, which requires minimal domain knowledge about the downstream tasks. The skills are learned using Stochastic Neural Networks (SNNs) with an information-theoretic regularizer to encourage diversity of behaviors. The pre-trained skills are then used to train a high-level policy for each downstream task, allowing for faster learning and improved exploration. The authors demonstrate the effectiveness of their approach on several tasks, including maze navigation and object gathering, and show that it outperforms existing methods.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The paper tackles a significant problem in deep reinforcement learning, namely learning in tasks with sparse rewards or long horizons.
2. The proposed framework is well-motivated and builds upon existing work in hierarchical reinforcement learning and intrinsic motivation.
3. The experimental results demonstrate the effectiveness of the approach on several tasks, including maze navigation and object gathering.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed framework, including the pre-training environment, the SNN architecture, and the information-theoretic regularizer. The authors also provide a thorough analysis of the experimental results, including comparisons to existing methods and ablation studies. The use of SNNs with an information-theoretic regularizer is a novel contribution, and the authors demonstrate its effectiveness in learning diverse skills.
Additional Feedback and Suggestions
To further improve the paper, I suggest the following:
1. Provide more details on the hyperparameter tuning process, particularly for the SNN architecture and the information-theoretic regularizer.
2. Consider adding more tasks to the experimental evaluation, including tasks with more complex dynamics or higher-dimensional state and action spaces.
3. Provide more analysis on the failure modes of the approach, particularly for unstable robots like Ant, and discuss potential solutions to address these limitations.
Questions for the Authors
1. Can you provide more details on the design of the proxy reward signal, and how it is tailored to the specific downstream tasks?
2. How do you select the number of skills to learn in the pre-training environment, and what is the impact of this choice on the downstream task performance?
3. Can you discuss potential extensions of the approach to more complex tasks, such as tasks with multiple goals or tasks that require cooperation between multiple agents?