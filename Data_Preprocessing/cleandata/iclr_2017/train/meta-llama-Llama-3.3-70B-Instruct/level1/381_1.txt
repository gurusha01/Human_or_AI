The paper proposes a novel approach to pruning convolutional neural networks (CNNs) by interleaving greedy criteria-based pruning with fine-tuning by backpropagation. The authors introduce a new criterion based on Taylor expansion to approximate the change in the cost function induced by pruning network parameters. The approach is evaluated on various datasets, including Birds-200, Flowers-102, and ImageNet, and demonstrates superior performance compared to other pruning criteria.
I decide to accept this paper for the following reasons:
1. The paper tackles a specific and relevant problem in the field of deep learning, namely, efficient inference in CNNs through pruning.
2. The approach is well-motivated and grounded in the literature, with a clear explanation of the limitations of existing methods and the advantages of the proposed approach.
3. The paper provides extensive experimental results, including comparisons with other pruning criteria and evaluation on various datasets, which demonstrate the effectiveness of the proposed approach.
Supporting arguments for the decision include:
* The paper provides a thorough analysis of the oracle ranking and the importance of per-layer normalization, which highlights the challenges of pruning CNNs and the need for careful consideration of the pruning criterion.
* The experimental results demonstrate the superiority of the Taylor criterion over other pruning criteria, including the oracle-loss and oracle-abs criteria, and show that the approach can achieve significant reductions in computational cost while maintaining good generalization performance.
* The paper also provides a detailed comparison with other pruning methods, including weight regularization and Optimal Brain Damage, which demonstrates the advantages of the proposed approach in terms of computational efficiency and accuracy.
Additional feedback to improve the paper includes:
* Providing more insight into the computational cost of the proposed approach, including the time and memory requirements for pruning and fine-tuning.
* Investigating the applicability of the approach to other types of neural networks, such as recurrent neural networks and fully connected networks.
* Exploring the potential for combining the proposed approach with other techniques, such as knowledge distillation and quantization, to further improve the efficiency and accuracy of CNNs.
Questions to the authors include:
* Can you provide more details on the implementation of the Taylor criterion, including the computation of the gradient and activation terms?
* How do you select the hyperparameters for the pruning procedure, such as the learning rate and the number of fine-tuning updates between pruning iterations?
* Have you explored the use of other optimization algorithms, such as Adam and RMSProp, for fine-tuning the pruned network?