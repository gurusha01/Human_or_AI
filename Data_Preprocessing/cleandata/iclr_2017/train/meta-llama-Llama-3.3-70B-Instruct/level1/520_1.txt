Summary
The paper proposes an extension of Pixel Convolutional Neural Networks (PixelCNN) for text-to-image synthesis with controllable object locations. The model conditions on text and spatial structure, such as part keypoints and segmentation masks, to generate images. The authors demonstrate the effectiveness of their approach on three datasets: Caltech-UCSD Birds (CUB), MPII Human Pose (MHP), and Common Objects in Context (MS-COCO). The results show that the model can generate diverse and interpretable images that match the text and structure constraints.
Decision
I decide to Accept this paper with minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and well-motivated problem in the field of text-to-image synthesis, and the approach is well-placed in the literature.
2. The paper provides a clear and thorough explanation of the model and its components, and the results are well-supported by quantitative and qualitative evaluations.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of text-to-image synthesis and the motivation for using PixelCNN as a base model. The authors also provide a thorough explanation of the model architecture and its components, including the character-level text encoder and the image generation network. The results are well-supported by quantitative evaluations, including negative log-likelihood values, and qualitative evaluations, including visual examples of generated images.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the training procedure, including the hyperparameter settings and the optimization algorithm used. Additionally, the authors could provide more analysis on the failure cases, such as the instances where the model fails to generate images that match the text and structure constraints. Finally, the authors could consider providing more comparisons to other state-of-the-art models in the field of text-to-image synthesis.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
1. Can you provide more details on the character-level text encoder and how it is trained end-to-end with the image generation network?
2. How do you handle cases where the text and structure constraints are inconsistent or ambiguous?
3. Can you provide more analysis on the computational efficiency of the model and its potential applications in real-world scenarios?