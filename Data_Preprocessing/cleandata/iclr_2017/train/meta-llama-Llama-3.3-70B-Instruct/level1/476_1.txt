This paper claims to provide the first empirical demonstration that deep convolutional models need to be both deep and convolutional, even when trained with methods such as distillation that allow small or shallow models of high accuracy to be trained. The authors tackle the specific question of whether shallow models can be trained to be as accurate as deep convolutional models using a similar number of parameters.
I decide to accept this paper, with the key reason being that the approach is well-motivated and well-placed in the literature. The authors provide a thorough review of previous research on deep learning and model compression, and their methodology is sound. They use Bayesian hyperparameter optimization to thoroughly explore the space of architectures and learning hyperparameters, and their results are rigorously evaluated.
The paper supports its claims with empirical results, demonstrating that shallow models, even when trained via distillation, are not as accurate as deep convolutional models. The authors show that convolution is critical for this problem, and that adding depth to the student models without adding convolution does not significantly close the "convolutional gap". The results are consistent with previous research, and the authors provide a clear and detailed explanation of their methodology and results.
To improve the paper, I would suggest that the authors provide more insight into why extra layers are required to train accurate models from the original training data. Additionally, it would be interesting to see more results on the effect of increasing model size on the accuracy of the convolutional students and MLPs. Some questions I would like the authors to answer include: What is the relationship between the number of parameters and the accuracy of the models? How does the number of convolutional layers affect the accuracy of the models? Can the authors provide more details on the hyperparameter optimization process and the ranges of hyperparameters used? 
Overall, the paper is well-written and provides a significant contribution to the field of deep learning. The results are rigorous and well-evaluated, and the authors provide a clear and detailed explanation of their methodology and results. With some additional insight and results, this paper has the potential to be a strong contribution to the conference.