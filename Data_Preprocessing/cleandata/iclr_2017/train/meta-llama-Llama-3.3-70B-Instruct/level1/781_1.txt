Summary
The paper presents a novel approach for learning compact and intuitive distributed representations with binary encoding. The authors propose a dynamic partition model, which extends traditional partition models by allowing for dynamically adapting partitionings of variables into expert supports. The model is learned using an expectation-maximization (EM) algorithm, where the experts and their levels of expertise are updated iteratively. The authors demonstrate the effectiveness of their approach on various datasets, including synthetic data, MNIST digits, Weizmann horses, and Caltech motorcycles.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated question, namely, learning compact and intuitive distributed representations with binary encoding. The authors provide a clear and concise introduction to the problem and the limitations of existing approaches. Secondly, the paper presents a well-supported claim, with a thorough derivation of the dynamic partition model and its learning algorithm, as well as extensive experimental evaluations on various datasets.
Supporting Arguments
The paper provides a clear and well-structured presentation of the dynamic partition model and its learning algorithm. The authors demonstrate the effectiveness of their approach on various datasets, including synthetic data, MNIST digits, Weizmann horses, and Caltech motorcycles. The experimental results show that the dynamic partition model outperforms existing approaches, such as products of experts, autoencoders, and sparse dictionaries. The paper also provides a thorough discussion of related work and the advantages of the proposed approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational complexity of the learning algorithm and the scalability of the approach to larger datasets. Additionally, it would be interesting to see more visualizations of the learned experts and their responsibilities for different datasets. The authors may also consider providing more discussion on the potential applications of the dynamic partition model in other domains, such as natural language processing or computer vision.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors determine the number of experts to use for each dataset?
* Can the authors provide more details on the initialization of the experts and their levels of expertise?
* How do the authors handle cases where the number of variables is very large, and the dynamic partition model may not be able to capture the underlying structure of the data?