Summary
This paper explores the impact of customized numeric representations on the accuracy and efficiency of deep neural networks (DNNs). The authors investigate the use of unconventional narrow-precision floating-point representations and propose a novel technique to efficiently search for the optimal precision configuration. They evaluate their approach on several large-scale DNNs, including GoogLeNet and VGG, and demonstrate an average speedup of 7.6Ã— with less than 1% degradation in inference accuracy.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated and supported by rigorous experimental evaluations. The authors provide a thorough analysis of the trade-offs between accuracy and efficiency and demonstrate the effectiveness of their proposed technique in finding optimal precision configurations.
Supporting Arguments
The paper tackles a specific and important question in the field of deep learning, namely the impact of numeric representations on DNN performance. The authors provide a comprehensive review of related work and clearly motivate their approach. The experimental evaluations are thorough and well-designed, and the results demonstrate the potential for significant performance improvements with minimal accuracy degradation. The proposed search technique is also novel and effective, allowing for efficient exploration of the large design space.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of their search technique and the computational resources required to perform the experiments. Additionally, it would be interesting to see a more detailed analysis of the sources of accuracy degradation and how they can be mitigated. Finally, the authors may want to consider exploring the applicability of their approach to other types of neural networks and applications.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation of the search technique and how it is integrated with the Caffe framework?
2. How do you plan to extend this work to other types of neural networks and applications, such as recurrent neural networks or natural language processing tasks?
3. Can you provide more insights into the sources of accuracy degradation and how they can be mitigated, particularly for very large DNNs?