Summary
The paper explores the concept of sample importance in deep neural networks, which refers to the change in parameters induced by a sample during training. The authors define a quantitative measurement of sample importance and conduct empirical analysis on two standard datasets, MNIST and CIFAR-10. They find that easy samples shape parameters closer to the output, while hard samples impact parameters closer to the input, and that mixing hard and easy samples in each batch is beneficial for training.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and interesting question about sample importance in deep neural networks, and (2) the approach is well-motivated and supported by empirical analysis.
Supporting Arguments
The paper provides a clear and well-structured introduction to the concept of sample importance, and the authors' hypothesis about the relationship between sample importance and layer parameters is intuitive and well-supported by literature. The empirical analysis is thorough and provides interesting insights into the behavior of easy and hard samples during training. The results on batch construction and sample importance are also surprising and contradict some existing results on curriculum learning.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points: (1) provide more discussion on the implications of their results for deep learning practice, (2) explore the relationship between sample importance and other concepts, such as attention mechanisms or adversarial examples, and (3) consider extending their analysis to more complex deep learning structures, such as convolutional neural networks or recurrent neural networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to use the sample importance measurement in practice, for example, to improve training efficiency or robustness? (2) Can the authors provide more intuition about why mixing hard and easy samples in each batch is beneficial for training, and how this relates to other concepts, such as curriculum learning or self-paced learning? (3) How do the authors think their results will generalize to more complex deep learning structures or tasks, such as image segmentation or natural language processing?