Summary
The paper analyzes the nonlinear dynamics of two-layered bias-free ReLU networks in the form of $g(x;w) = \sum{j=1}^{K} \sigma(wj^T x)$, where $\sigma$ is the ReLU nonlinearity. The authors assume that the input $x$ follows a Gaussian distribution and the output is generated by a teacher network with parameters $w^$. They show that for $K=1$, the nonlinear dynamics can be written in closed form and converges to $w^$ with at least $(1-\epsilon)/2$ probability, if the initialization is proper. For $K \geq 2$, they prove that when the teacher parameters $\{w_j^\}$ form an orthonormal basis, a symmetric initialization yields convergence to a saddle point, while a certain symmetry-breaking initialization yields global convergence to $w^$ without local minima.
Decision
I decide to Accept this paper, with the following key reasons:
1. The paper tackles a specific and important question in the field of deep learning, namely the convergence of nonlinear neural networks.
2. The approach is well-motivated and placed in the literature, with a clear understanding of the limitations of previous works.
3. The paper provides rigorous theoretical analysis and supporting empirical evidence, including simulations that verify the theoretical results.
Supporting Arguments
The paper provides a thorough analysis of the nonlinear dynamics of ReLU networks, including a closed-form expression for the expected gradient update. The authors also provide a detailed proof of the convergence results, including the use of Lyapunov functions and Poincare-Bendixson theorem. The simulations provided in the paper demonstrate the effectiveness of the proposed approach and verify the theoretical results.
Additional Feedback
To further improve the paper, I suggest that the authors:
* Provide more intuition and explanation for the symmetry-breaking initialization and its relation to the convergence results.
* Discuss the implications of the results for deeper neural networks and more complex architectures.
* Consider providing more experimental results, including comparisons with other optimization methods and networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the initialization scheme used in the paper and how it relates to common initialization techniques?
* How do the results in the paper relate to other works on convergence of neural networks, such as those using stochastic gradient descent?
* Are there any plans to extend the results to more complex architectures, such as convolutional neural networks or recurrent neural networks?