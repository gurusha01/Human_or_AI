1. The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors empirically study its performance on several datasets, including CIFAR-10, CIFAR-100, and a downsampled version of the ImageNet dataset, and demonstrate new state-of-the-art results.
2. I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
3. The approach is well-motivated as it tackles the problem of improving the anytime performance of SGD, which is a crucial aspect of deep learning. The authors provide a clear and concise overview of the related work, including restart techniques in gradient-free and gradient-based optimization. The paper also provides a thorough analysis of the proposed warm restart technique, including its implementation and experimental evaluation. The results are correct and scientifically rigorous, as they are based on a thorough experimental evaluation on several datasets and are compared to state-of-the-art methods.
4. To further improve the paper, I suggest that the authors provide more insights into the hyperparameter tuning process, as the choice of hyperparameters seems to have a significant impact on the results. Additionally, it would be interesting to see more analysis on the effect of the warm restart technique on the convergence of SGD, and how it compares to other optimization methods. Some questions I would like the authors to answer include: (1) How did you choose the hyperparameters for the warm restart technique, and how sensitive are the results to these choices? (2) Can you provide more insights into the effect of the warm restart technique on the convergence of SGD, and how it compares to other optimization methods? (3) How do you plan to extend this work to other optimization methods, such as AdaDelta and Adam, and what are the potential challenges and benefits of doing so?