The paper "HYPERBAND: A Novel Bandit-Based Approach to Hyperparameter Optimization" presents a new algorithm for hyperparameter optimization, called HYPERBAND, which is based on a principled early-stopping strategy to allocate resources to randomly sampled configurations. The authors claim that HYPERBAND can provide more than an order of magnitude speedups over popular Bayesian optimization methods on various neural network and kernel-based learning problems.
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper provides strong empirical evidence to support its claims. The authors provide a clear and thorough explanation of the HYPERBAND algorithm, its theoretical underpinnings, and its relationship to existing methods. The empirical results demonstrate the effectiveness of HYPERBAND in various settings, including neural network and kernel-based classification tasks.
The supporting arguments for my decision include the fact that the authors have carefully evaluated HYPERBAND against several popular Bayesian optimization methods, including SMAC, TPE, and Spearmint, and have shown that it can outperform these methods in many cases. Additionally, the authors have provided a detailed analysis of the theoretical properties of HYPERBAND, including its budget requirements and its ability to adapt to different problem settings.
To further improve the paper, I would like to see more discussion on the following points: (1) the choice of the hyperparameter η, which controls the proportion of configurations discarded in each round of SUCCESSIVEHALVING, and (2) the potential applications of HYPERBAND in other domains, such as reinforcement learning or natural language processing. I would also like to see more detailed comparisons with other hyperparameter optimization methods, such as grid search or random search.
Some questions I would like the authors to answer include: (1) How does the choice of η affect the performance of HYPERBAND, and are there any guidelines for selecting a good value of η? (2) Can HYPERBAND be used in conjunction with other hyperparameter optimization methods, such as Bayesian optimization or gradient-based optimization? (3) Are there any plans to release an open-source implementation of HYPERBAND, and if so, what programming languages or frameworks will be supported?