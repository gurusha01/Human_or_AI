Summary
The paper proposes a modification to existing recurrent neural network (RNN) units, allowing them to learn to vary the amount of computation they perform at each step. This is achieved through a scheduler that decides how much of the hidden state needs to be updated, based on the current input and hidden state. The authors demonstrate the effectiveness of their approach on several sequence modeling tasks, including music modeling, bit-level and character-level language modeling, and show that their models require fewer operations while achieving better performance.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by experimental results. The authors provide a clear explanation of the problem they are trying to solve, and their solution is novel and interesting.
Supporting Arguments
The paper tackles a specific question of how to efficiently model processes happening at different time scales in sequential data. The approach is well-placed in the literature, building on existing work on RNNs and adaptive computation. The authors provide a thorough explanation of their model and learning procedure, and the experimental results demonstrate the effectiveness of their approach. The results show that the proposed models can learn time patterns of interest in sequential data, and achieve better performance with fewer operations.
Additional Feedback
To further improve the paper, I would suggest providing more analysis on the learned time patterns and how they relate to the underlying structure of the data. Additionally, it would be interesting to see more comparisons with other state-of-the-art models, and to explore the potential applications of this approach to other domains. The authors may also want to consider providing more details on the implementation and training of their models, to facilitate reproducibility.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the learned time patterns relate to the underlying structure of the data, and can they be interpreted in a meaningful way?
* Can the authors provide more details on the training procedure, including the hyperparameter settings and the optimization algorithm used?
* How do the authors plan to extend this work to other domains and applications, and what potential challenges do they foresee?