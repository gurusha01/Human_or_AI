Summary
The paper proposes two methods for "wild variational inference" that allow for the training of general inference networks with intractable density functions. The authors apply these methods to adaptively estimate the step sizes of stochastic gradient Langevin dynamics (SGLD), a Markov chain Monte Carlo (MCMC) algorithm. The paper claims to provide a significant improvement over traditional variational inference methods, which are limited by the requirement of using simple approximation families.
Decision
I decide to Accept this paper, with two key reasons for this choice:
1. The paper tackles a specific and important problem in the field of variational inference, which is the limitation of traditional methods in handling complex and structured models.
2. The approach is well-motivated and supported by theoretical and empirical evidence, demonstrating the effectiveness of the proposed methods in improving the performance of SGLD.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of variational inference and the limitations of traditional methods. The authors propose two methods, amortized SVGD and KSD variational inference, which are based on recent works that combine kernel techniques with Stein's method. The paper provides a detailed discussion of the theoretical background and the implementation of these methods. The empirical results demonstrate the effectiveness of the proposed methods in improving the performance of SGLD on several benchmark datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed comparisons with other related works, such as the recent work by Ranganath et al. (2016). Additionally, the authors could provide more insights into the choice of the kernel function and the hyperparameters of the proposed methods. It would also be helpful to include more visualizations of the results, such as plots of the learned step sizes and the performance of the proposed methods on different datasets.
Questions for the Authors
1. Can you provide more details on the choice of the kernel function and the hyperparameters of the proposed methods?
2. How do the proposed methods compare to other related works, such as the recent work by Ranganath et al. (2016)?
3. Can you provide more insights into the theoretical properties of the proposed methods, such as the convergence rates and the stability of the algorithms?