Summary of the Paper's Claims and Contributions
The paper tackles the specific question of whether the error surface of deep models is well-behaved, particularly in the context of finite-sized datasets and models. The authors claim to provide examples that demonstrate the existence of bad local minima in the error surface of rectified multi-layer perceptrons (MLPs) on finite datasets, contradicting the "no bad local minima" hypothesis. They contribute to the understanding of the error surface by providing theoretical examples and empirical evidence that highlight the importance of initialization, data structure, and model architecture in determining the learning dynamics.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The key reasons for this decision are:
1. The paper provides a clear and well-motivated approach to tackling the question of the error surface of deep models, placing itself well within the existing literature on the topic.
2. The authors provide theoretical examples and empirical evidence that support their claims, demonstrating a good understanding of the problem and its implications.
Supporting Arguments
The paper's approach is well-motivated, as it addresses a long-standing question in the field of deep learning. The authors provide a thorough review of the literature, highlighting the limitations of existing results and the need for a more nuanced understanding of the error surface. The theoretical examples and empirical evidence presented in the paper demonstrate a clear understanding of the problem and its implications. The authors' use of rectified MLPs and finite datasets allows them to construct counterexamples that challenge the "no bad local minima" hypothesis.
Additional Feedback and Questions
To further improve the paper, I would like the authors to:
* Provide more discussion on the implications of their results for the design of deep learning models and optimization algorithms.
* Consider exploring the relationship between the error surface and other factors, such as regularization techniques or batch normalization.
* Clarify the extent to which their results generalize to other types of models and datasets.
Some questions I would like the authors to address in their response are:
* How do the authors' results relate to existing work on the optimization of deep learning models, such as the use of gradient descent and stochastic gradient descent?
* Can the authors provide more insight into the role of initialization in determining the learning dynamics of deep models?
* How do the authors' results inform the design of more robust and efficient optimization algorithms for deep learning models?