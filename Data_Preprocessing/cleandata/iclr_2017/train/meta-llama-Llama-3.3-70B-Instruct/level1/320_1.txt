This paper proposes a novel approach to visual servoing, which combines learned visual features, learned predictive dynamics models, and reinforcement learning to learn visual servoing mechanisms. The authors focus on target following and demonstrate that standard deep features can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions.
I decide to accept this paper for several reasons. Firstly, the paper tackles a specific and well-defined problem in visual servoing, which is a classic problem in robotics. The approach is well-motivated and builds upon existing work in the field, including the use of learned visual features and predictive dynamics models. The authors provide a clear and detailed explanation of their method, including the use of a sample-efficient fitted Q-iteration algorithm to learn the weights for the visual features.
The paper supports its claims with extensive experiments on a complex synthetic car following benchmark, demonstrating substantial improvement over conventional approaches based on image pixels or hand-designed keypoints. The results show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. The authors also provide a detailed comparison with other methods, including classical image-based visual servoing and position-based visual servoing, and demonstrate the superiority of their approach.
To further improve the paper, I would like to see more discussion on the limitations of the approach and potential avenues for future work. For example, the authors could discuss the potential challenges of applying their method to real-world scenarios, such as dealing with varying lighting conditions or occlusions. Additionally, the authors could provide more details on the computational efficiency of their method and how it compares to other approaches.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend their approach to more complex scenarios, such as multi-target tracking or tracking in cluttered environments? How do the authors handle cases where the target object is partially occluded or has varying appearance? What are the potential applications of this approach in real-world robotics scenarios, and how do the authors plan to demonstrate its effectiveness in these scenarios?