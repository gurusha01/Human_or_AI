Summary of the Paper's Contributions
The paper presents a fascinating study on the eigenvalues of the Hessian of a loss function in deep learning, revealing a two-phased spectrum with a bulk concentrated around zero and isolated eigenvalues away from zero. The authors demonstrate that the bulk depends on the model's architecture, while the isolated eigenvalues depend on the input data. This discovery has significant implications for theoretical work and practical applications, including the development of new optimization methods and a deeper understanding of the loss function's landscape.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in deep learning, providing novel insights into the Hessian's structure and its implications for optimization; and (2) the approach is well-motivated, building upon existing literature and using rigorous empirical methods to support the claims.
Supporting Arguments
The paper's contributions are well-supported by empirical evidence, including experiments on various architectures and datasets. The authors demonstrate the robustness of their findings across different settings, such as varying network sizes, input data, and loss functions. The paper also provides a clear and concise discussion of the implications of their results, highlighting potential avenues for future research and practical applications.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors consider providing more detailed analysis of the isolated eigenvalues and their dependence on the input data. Additionally, it would be interesting to explore the relationship between the Hessian's spectrum and the loss function's landscape, potentially using visualization techniques or other methods to illustrate the connections between these concepts. Some questions I would like the authors to address include: (1) How do the isolated eigenvalues change as the input data distribution changes? (2) Can the authors provide more insight into the optimization methods that could be developed based on the understanding of the Hessian's spectrum? (3) How do the results generalize to other deep learning architectures and applications?