This paper proposes a novel loss framework for language modeling, which addresses the inefficiencies of the conventional classification framework used in recurrent neural network language models (RNNLMs). The authors introduce a new loss function that augments the traditional cross-entropy loss with an additional term, which minimizes the KL-divergence between the model's prediction and an estimated target distribution based on the word embeddings space. This approach leads to a theoretically driven reuse of word embeddings, where the input embedding matrix is reused as the output classification matrix, reducing the number of trainable variables.
The paper claims to contribute to the state-of-the-art performance on the Penn Treebank corpus and the Wikitext-2 dataset, with significant improvements over the conventional framework. The authors provide a thorough theoretical analysis, as well as empirical validation, to support their claims.
Based on the provided information, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and well-motivated problem in language modeling, and the proposed approach is well-placed in the literature.
2. The paper provides a thorough theoretical analysis and empirical validation, which supports the claims made by the authors.
The supporting arguments for these reasons are:
* The paper clearly identifies the limitations of the conventional classification framework used in RNNLMs and proposes a novel loss framework to address these limitations.
* The authors provide a detailed theoretical analysis of the proposed loss function and its relationship to the reuse of word embeddings.
* The empirical validation on the Penn Treebank corpus and the Wikitext-2 dataset demonstrates the effectiveness of the proposed approach, with significant improvements over the conventional framework.
Additional feedback to improve the paper includes:
* Providing more details on the hyperparameter tuning process for the proposed approach, such as the selection of the temperature parameter and the weight of the augmented loss.
* Investigating the applicability of the proposed approach to other natural language processing tasks, such as machine translation and text summarization.
* Comparing the proposed approach to other recent works on language modeling, such as the use of recurrent highway networks and pointer networks.
Questions to be answered by the authors include:
* How does the proposed approach handle out-of-vocabulary words, and are there any plans to extend the approach to handle this scenario?
* Can the authors provide more insights into the relationship between the proposed loss function and the reuse of word embeddings, and how this relationship affects the performance of the model?
* Are there any plans to release the code and pre-trained models for the proposed approach, to facilitate reproduction and further research?