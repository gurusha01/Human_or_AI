This paper proposes the Ensemble Policy Optimization (EPOpt) algorithm, which uses an ensemble of simulated source domains and adversarial training to learn policies that are robust and generalize to a broad range of possible target domains. The algorithm is designed to address the challenges of sample complexity and safety in reinforcement learning, particularly when learning policies with rich function approximators like deep neural networks.
The paper claims to contribute to the field of reinforcement learning by providing a novel approach to learning robust policies that can generalize to different target domains. The authors demonstrate the effectiveness of their approach through experiments on two simulated robotic tasks, the hopper and half-cheetah tasks, and show that their algorithm can learn policies that are robust to physical parameter discrepancies and unmodeled effects.
Based on the provided information, I decide to accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and important problem in reinforcement learning, namely the challenge of learning policies that can generalize to different target domains.
2. The approach proposed by the authors is well-motivated and supported by theoretical and empirical evidence, and the experiments demonstrate the effectiveness of the algorithm in learning robust policies.
The paper provides a clear and well-structured presentation of the problem, the proposed approach, and the experimental results. The authors also provide a thorough discussion of the related work and the limitations of their approach, which demonstrates a good understanding of the field and the challenges involved.
To improve the paper, I would suggest the following:
* Provide more details on the implementation of the algorithm, such as the specific hyperparameters used and the computational resources required.
* Consider adding more experiments to demonstrate the robustness of the algorithm to different types of disturbances and uncertainties.
* Provide a more detailed analysis of the trade-offs between the different components of the algorithm, such as the ensemble size and the adversarial training parameter.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors choose the ensemble size and the adversarial training parameter, and what is the sensitivity of the algorithm to these hyperparameters?
* Can the authors provide more details on the computational resources required to implement the algorithm, and how it scales to more complex tasks and larger ensemble sizes?
* How do the authors plan to extend the algorithm to more complex tasks and real-world applications, and what are the potential challenges and limitations of the approach?