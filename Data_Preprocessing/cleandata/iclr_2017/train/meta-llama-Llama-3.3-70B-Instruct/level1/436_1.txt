Summary
The paper presents a weakly supervised, end-to-end neural network model, called Neural Programmer, for inducing programs that map natural language queries to logical forms or programs that provide the desired response when executed on a database. The model is trained on a real-world dataset, WikiTableQuestions, with only 10,000 examples and achieves competitive performance with a state-of-the-art traditional semantic parser.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a challenging problem of learning a natural language interface for database tables with weak supervision, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear motivation for the problem and the approach, and the experimental results demonstrate the effectiveness of the model. The authors also provide a thorough analysis of the results, including an ablation study and error analysis, which helps to understand the strengths and weaknesses of the model. Additionally, the paper is well-written and easy to follow, making it a pleasure to read.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the effect of different hyperparameters on the performance of the model. Additionally, it would be interesting to see more examples of induced programs and a more detailed analysis of the types of programs that the model is able to learn. Finally, the authors may want to consider providing more context on the related work and how their approach differs from other neural program induction techniques.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
* Can you provide more details on the anonymization process of phrases in the question that match some table entry?
* How did you select the 15 operations used in the model, and are there any plans to extend the model to handle more complex operations?
* Can you provide more insights on the effect of strong regularization on the performance of the model, and how did you select the specific regularization techniques used in the paper?