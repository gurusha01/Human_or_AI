The paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks, which is motivated by the local geometry of the energy landscape. The authors leverage the observation that local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. They construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys.
The specific question tackled by the paper is how to modify the traditional SGD algorithm to actively seek flat regions in the energy landscape, which are believed to generalize better. The approach is well-motivated, as it is based on a thorough analysis of the local geometry of the energy landscape and its relation to generalization error. The paper provides a clear and concise explanation of the concept of local entropy and its connection to the energy landscape.
The paper supports its claims with theoretical analysis and empirical results. The authors show that the new objective has a smoother energy landscape and demonstrate improved generalization over SGD using uniform stability, under certain assumptions. The experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.
Based on the analysis, I decide to accept the paper. The two key reasons for this choice are: (1) the paper proposes a novel and well-motivated approach to optimization, which is supported by theoretical analysis and empirical results; and (2) the experiments demonstrate the effectiveness of the proposed algorithm in terms of generalization error and training time.
To improve the paper, I suggest the authors provide more details on the implementation of the Entropy-SGD algorithm, such as the choice of hyperparameters and the computational cost of the algorithm. Additionally, it would be interesting to see more experiments on different datasets and architectures to further demonstrate the effectiveness of the proposed algorithm.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How do the authors choose the hyperparameters, such as the learning rate and the scope parameter, in the Entropy-SGD algorithm? (2) Can the authors provide more details on the computational cost of the Entropy-SGD algorithm compared to traditional SGD? (3) How do the authors plan to extend the proposed algorithm to other optimization problems, such as reinforcement learning or generative models?