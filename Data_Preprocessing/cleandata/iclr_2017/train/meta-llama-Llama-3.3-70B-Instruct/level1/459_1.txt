This paper presents a novel framework for end-to-end multi-task representation learning in deep neural networks, which generalizes matrix factorization-based multi-task ideas to tensor factorization. The approach allows for flexible sharing of knowledge in fully connected and convolutional DNN layers, reducing the design choices and architectural search space in Deep MTL architecture design.
I decide to Accept this paper, with two key reasons for this choice: 
1. The paper tackles a specific and well-motivated problem in the field of multi-task learning, which is to learn a sharing structure across tasks in a deep neural network.
2. The approach is well-supported by experimental results, demonstrating the efficacy of the proposed method in both homogeneous and heterogeneous multi-task learning settings.
The paper provides a clear and well-structured presentation of the methodology, including a thorough review of related work and a detailed description of the proposed tensor factorization approach. The experimental results are convincing, showing that the proposed method outperforms single-task learning and comparable or better performance than the best results from exhaustive search of user-defined MTL architectures.
To further improve the paper, I suggest the authors provide more analysis on the learned sharing structure, such as visualizing the learned tensor factors and analyzing their interpretability. Additionally, it would be interesting to see more comparisons with other state-of-the-art multi-task learning methods, particularly those that also learn sharing structures in deep neural networks.
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
* How do the authors determine the rank of the tensor factors in the proposed method?
* Can the authors provide more insights into the learned sharing structure, such as which layers tend to share more or less across tasks?
* How does the proposed method handle negative transfer, where sharing knowledge across tasks can actually hurt performance?