Summary of the Paper
The paper investigates the cause of the generalization gap in large-batch methods for deep learning. The authors observe that large-batch methods tend to converge to sharp minimizers of the training function, which leads to poor generalization. In contrast, small-batch methods converge to flat minimizers, which generalize better. The paper presents numerical evidence to support this claim and explores several strategies to remedy the generalization problem of large-batch methods.
Decision
I decide to accept this paper with minor revisions. The paper tackles an important problem in deep learning and provides valuable insights into the behavior of large-batch methods. The authors' observations and conclusions are well-supported by numerical evidence, and the paper is well-written and easy to follow.
Supporting Arguments
The paper addresses a specific question/problem, namely the generalization gap in large-batch methods, and provides a clear and well-motivated approach to tackling this problem. The authors' use of numerical experiments and visualization techniques, such as parametric plots, helps to illustrate the differences between large-batch and small-batch methods. The paper also explores several strategies to remedy the generalization problem of large-batch methods, including data augmentation, conservative training, and robust training.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the experimental setup and the hyperparameters used in their experiments. Additionally, the authors could provide more discussion on the implications of their findings for the design of deep learning algorithms and the choice of batch size. It would also be helpful to see more comparisons with other related work in the field.
Questions for the Authors
I would like the authors to clarify the following points:
* How do the authors choose the batch size for the large-batch experiments, and what is the rationale behind using 10% of the training data as the batch size?
* Can the authors provide more details on the implementation of the conservative training approach and the choice of the regularization parameter Î»?
* How do the authors plan to extend their work to other deep learning architectures and tasks, and what are the potential limitations of their approach?