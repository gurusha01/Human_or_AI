Summary
The paper proposes a novel distributed transfer learning approach for deep convolutional networks, which tackles the optimization complexity and class-imbalance problem in transfer learning. The approach uses basic probability assignment to boost the performance of single-filter neural networks, and then combines them to improve the recognition performance on the target domain. The authors conduct experiments on several standard datasets, including MNIST, CIFAR, and SVHN, and demonstrate the consistent improvement of their distributed transfer learning strategy over conventional transfer learning methods.
Decision
I decide to Accept this paper, with two key reasons: (1) the approach is well-motivated and placed in the literature, addressing the important problem of optimization complexity and class-imbalance in transfer learning; and (2) the experimental results demonstrate the effectiveness of the proposed approach, showing consistent improvement over conventional transfer learning methods.
Supporting Arguments
The paper provides a clear and thorough introduction to the problem of transfer learning, and motivates the need for a novel approach to address the optimization complexity and class-imbalance problem. The proposed approach is well-explained, and the use of basic probability assignment to boost the performance of single-filter neural networks is a innovative and effective solution. The experimental results are thorough and well-presented, demonstrating the advantage of the proposed approach over conventional transfer learning methods.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the computational complexity of the proposed approach, and compare it with conventional transfer learning methods. Additionally, it would be interesting to see more experiments on different datasets and tasks, to further demonstrate the generalizability and effectiveness of the proposed approach. I also have some questions for the authors: (1) How do the authors select the number of single-filter neural networks to combine? (2) Can the authors provide more insights on the effect of using different types of classifiers (e.g. SVM, Softmax) on the performance of the proposed approach? (3) How does the proposed approach handle the case where the original and target domains have different numbers of classes?