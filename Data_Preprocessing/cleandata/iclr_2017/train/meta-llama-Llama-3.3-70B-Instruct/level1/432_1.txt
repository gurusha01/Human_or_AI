Summary
The paper proposes a new technique called PGQL, which combines policy gradient with off-policy Q-learning. The authors establish a connection between the fixed points of regularized policy gradient algorithms and the Q-values of the resulting policy, allowing them to estimate Q-values from the policy. They then use this estimate to perform Q-learning updates, which can be done off-policy using stored experience. The authors demonstrate the effectiveness of PGQL on a suite of Atari games, achieving better performance than both A3C and Q-learning.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in reinforcement learning, namely combining policy gradient and Q-learning, and (2) the approach is well-motivated and supported by theoretical analysis and empirical results.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of combining policy gradient and Q-learning, and motivates the need for such a combination. The authors then provide a thorough theoretical analysis of the connection between regularized policy gradient algorithms and Q-values, which forms the basis for their proposed PGQL technique. The empirical results on Atari games demonstrate the effectiveness of PGQL, with improved performance over both A3C and Q-learning.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of PGQL, such as the choice of hyperparameters and the architecture of the neural network used to parameterize the policy. Additionally, it would be interesting to see more analysis on the stability and data efficiency of PGQL, particularly in comparison to other reinforcement learning algorithms.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How did the authors choose the hyperparameters for PGQL, such as the learning rate and the weighting parameter Î·?
* Can the authors provide more details on the architecture of the neural network used to parameterize the policy, and how it was trained?
* How does the performance of PGQL compare to other reinforcement learning algorithms, such as DQN and TD-actor-critic?
* Are there any plans to extend PGQL to more complex domains, such as continuous control tasks or multi-agent environments?