Summary of the Paper
The paper proposes a novel approach to mitigate the problem of catastrophic mistakes in deep reinforcement learning (DRL) agents. The authors introduce "intrinsic fear," a method that uses a separate model to predict the probability of catastrophe within a short number of steps and penalizes the Q-learning objective to avoid dangerous states. The approach is evaluated on several environments, including Adventure Seeker, Cart-Pole, and Seaquest, and shows promising results in reducing the number of catastrophes.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the approach lacks a clear principle, relying on a separate model to predict catastrophe probabilities rather than using catastrophe as a direct signal to the learner. Secondly, the paper does not provide a thorough comparison to extensive baselines, including expected SARSA, to validate the effectiveness of the proposed method.
Supporting Arguments
The paper's approach to using a separate model to predict catastrophe probabilities is not well-motivated, and it is unclear why this approach is necessary. Additionally, the paper's evaluation is limited to a few environments, and it is unclear how the approach would perform in more complex domains. The use of reward replay buffers in deep RL agents is a relevant area of research that could help avoid revisiting catastrophic states, but the paper does not explore this direction.
Additional Feedback
To improve the paper, the authors could provide a more thorough comparison to existing baselines, including expected SARSA, and explore the use of reward replay buffers to avoid catastrophic states. Additionally, the authors could provide more insight into the choice of hyperparameters, such as the fear radius and fear factor, and how these parameters affect the performance of the approach.
Questions for the Authors
I would like the authors to clarify the following points:
1. Why is a separate model necessary to predict catastrophe probabilities, and how does this approach relate to existing methods for safe reinforcement learning?
2. How does the choice of fear radius and fear factor affect the performance of the approach, and are there any guidelines for selecting these hyperparameters in practice?
3. Can the authors provide more insight into the evaluation of the approach on more complex domains, such as robotics or dialogue systems, and how the approach would perform in these settings?