Summary
The paper proposes a global-local context attention framework for sentiment analysis, inspired by human reading behavior in reading comprehension tasks. The model uses a bidirectional LSTM (Bi-LSTM) network to extract a global context representation, which is then used as attention to incorporate local contexts in a second scan of the text. The authors also propose a simplified single-scan approach, which achieves similar performance. Experimental results demonstrate the effectiveness of the proposed models on several benchmark datasets.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the authors failed to address and fix pre-review comments, which is a significant concern. Secondly, the paper makes unscientific statements comparing the model's working procedure to human reading and comprehension without citing relevant neuroscience papers, which undermines the validity of the proposed approach.
Supporting Arguments
The experiments conducted in the paper are too simple, focusing on basic classification tasks and competing against straightforward models. This warrants a revision of the title and abstract to avoid exaggeration. Furthermore, the attention level approach bears similarity to dynamic memory networks, and the authors should provide a comparison and differentiation with existing work. The paper also lacks incorporation of missing related work, which is essential to contextualize the paper within current literature and enhance its validity.
Additional Feedback
To improve the paper, the authors should provide a more thorough review of related work, including recent advances in attention mechanisms and sentiment analysis. They should also conduct more comprehensive experiments, including comparisons with state-of-the-art models and evaluations on more challenging datasets. Additionally, the authors should provide more detailed analysis and visualization of the attention weights to support their claims.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. How do the authors plan to address the pre-review comments and revise the paper to address the concerns raised?
2. Can the authors provide more detailed explanations and citations to support their claims about the similarity between the proposed model and human reading behavior?
3. How do the authors plan to extend the experiments to more challenging datasets and compare their model with state-of-the-art approaches?