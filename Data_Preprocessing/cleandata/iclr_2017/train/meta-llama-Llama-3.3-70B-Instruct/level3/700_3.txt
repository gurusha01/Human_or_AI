Summary
The paper proposes a novel deep neural network architecture, ENet, designed for real-time semantic segmentation on low-power mobile devices. The authors claim that ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. The network is evaluated on several datasets, including CamVid, Cityscapes, and SUN, and is shown to outperform state-of-the-art models in terms of speed and accuracy.
Decision
I decide to accept this paper, with the main reason being that the authors have proposed a well-motivated and novel architecture that addresses a significant problem in the field of semantic segmentation. The paper provides convincing evidence that ENet outperforms existing models in terms of speed and accuracy, and the authors have conducted thorough experiments to evaluate the network's performance.
Supporting Arguments
The paper is well-written and easy to follow, with clear explanations of the network architecture and the design choices made by the authors. The authors have also provided a thorough review of related work, highlighting the limitations of existing models and the need for a more efficient and accurate architecture. The experimental results are impressive, with ENet showing significant improvements in speed and accuracy over existing models.
Additional Feedback
To further improve the paper, I would suggest that the authors provide more details on the computational cost of the network, including the number of floating-point operations and the memory requirements. Additionally, the authors could provide more analysis on the trade-offs between accuracy and speed, and explore the potential applications of ENet in other domains. I would also like to see more comparisons with other state-of-the-art models, including those that use different architectures or techniques.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the network's performance change when using different input resolutions or aspect ratios?
* Can the authors provide more details on the training procedure, including the learning rate schedule and the batch size used?
* How does the network's performance compare to other models that use different optimization techniques, such as stochastic depth or spatial dropout?
* Are there any plans to release the code or models for ENet, and if so, how will they be made available to the research community?