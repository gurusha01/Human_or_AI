Summary
The paper proposes a new model, called Neural Equivalence Networks (EQNETs), to learn continuous semantic representations of mathematical and logical expressions. The authors evaluate their model on a diverse class of symbolic algebraic and boolean expression types, showing that EQNETs significantly outperform existing architectures.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a fundamental problem in machine learning and artificial intelligence, and (2) the authors provide a reasonably extensive evaluation with similar approaches, demonstrating the effectiveness of their proposed model.
Supporting Arguments
The paper addresses the problem of learning continuous semantic representations of symbolic expressions, which is a crucial step towards combining abstract, symbolic reasoning with continuous neural reasoning. The authors propose a novel architecture, EQNETs, which learns to compose representations of equivalence classes into new equivalence classes. The evaluation results show that EQNETs perform dramatically better than state-of-the-art alternatives, including recursive neural networks (TREENNs) and recurrent neural networks (RNNs).
Additional Feedback
To further improve the paper, I suggest that the authors provide more motivation for the subexpforce loss, which is currently not well-justified. A direct comparison between a model with and without the subexpforce loss would help to better understand its impact. Additionally, the evaluation metric used, precision on a per query basis, may not be the most informative metric. Consider using more standard metrics like precision-recall or ROC curves to provide a more comprehensive assessment of the model's performance.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more intuition behind the subexpforce loss and how it helps to improve the model's performance?
2. How do you plan to address the issue of exploding and diminishing gradients in the EQNETs, which can become quite deep when using multiple layers?
3. Have you considered using other evaluation metrics, such as precision-recall or ROC curves, to provide a more comprehensive assessment of the model's performance?