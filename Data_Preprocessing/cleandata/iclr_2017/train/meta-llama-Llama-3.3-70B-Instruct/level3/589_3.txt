Summary
The paper proposes a novel approach to improve the performance of gradient descent when utilizing multiple compute resources. The authors design a baseline asynchronous gradient descent and a layer-wise gradient descent method, which overlaps weight updates of a layer with inter-node synchronization of other layers. They implement their approaches using the Machine Learning Toolkit for Extreme Scale (MaTEx) and evaluate them on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the experiment results section is brief and lacks interpretation, with limited comparisons to other models. Secondly, the writing is unclear, and the authors fail to provide sufficient comparisons, insights, or explanations for why the proposed model works better.
Supporting Arguments
The paper proposes a new approach to asynchronous gradient descent, but the technical novelty is limited. The authors do not provide a clear explanation of how their approach differs from previous work, and the experimental evaluation is not comprehensive. The results are presented in a way that makes it difficult to understand the significance of the improvements, and the authors do not provide enough context to appreciate the trade-offs between different approaches.
Additional Feedback
To improve the paper, the authors should provide more detailed explanations of their approach and its relationship to previous work. They should also conduct a more comprehensive experimental evaluation, including comparisons to other models and a more detailed analysis of the results. Additionally, the authors should clarify their writing and provide more insights into why their approach works better. Some specific questions I would like the authors to answer include: How does the proposed approach differ from previous work on asynchronous gradient descent? What are the key factors that contribute to the improved performance of the proposed approach? How do the authors plan to address the limitations of their approach, such as the need for careful tuning of hyperparameters?
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: Can you provide more details on the implementation of the layer-wise gradient descent method? How do you handle the communication latency in the proposed approach? Can you provide more insights into the trade-offs between different approaches, such as the delayed gradient approach and the layer-wise approach? How do you plan to extend the proposed approach to other deep learning models and applications?