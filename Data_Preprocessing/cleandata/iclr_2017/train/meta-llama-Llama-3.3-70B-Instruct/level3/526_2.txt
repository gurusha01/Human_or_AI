Summary
The paper proposes a novel neural language model with a key-value attention mechanism that separates output vectors into keys, values, and predict representations. This model outperforms existing memory-augmented neural language models on two corpora, Wikipedia and Children's Book Test (CBT). However, the authors found that their model mainly utilizes a memory of the five most recent output representations, leading to the conclusion that a simpler model, N-gram RNN, which concatenates previous output representations, can achieve comparable results.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated problem in neural language modeling, and (2) the approach is well-supported by theoretical and empirical results.
Supporting Arguments
The paper clearly addresses the problem of neural language models struggling to capture long-range dependencies. The proposed key-value attention mechanism is well-motivated and grounded in previous work. The empirical results demonstrate the effectiveness of the proposed model, and the comparison with state-of-the-art models provides a thorough evaluation. The authors' finding that the simpler N-gram RNN model can achieve comparable results is also an interesting contribution.
Additional Feedback
To further improve the paper, I suggest the authors provide more analysis on why the attentive models fail to exploit long-range dependencies. Additionally, it would be interesting to see more experiments on different datasets and tasks to verify the generalizability of the proposed model. The authors may also consider providing more details on the implementation of the N-gram RNN model and its comparison with other higher-order RNNs.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insights on why the key-value attention mechanism is effective in capturing short-range dependencies, but not long-range dependencies?
2. How do you plan to address the issue of attentive models failing to exploit long-range dependencies in future work?
3. Can you provide more details on the comparison between the N-gram RNN model and other higher-order RNNs, such as HORNNs?