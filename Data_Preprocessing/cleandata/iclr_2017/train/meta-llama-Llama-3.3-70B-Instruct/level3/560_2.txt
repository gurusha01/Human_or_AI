This paper presents a novel approach to policy search in stochastic dynamical systems using model-based reinforcement learning with Bayesian neural networks (BNNs) that include stochastic input variables. The authors propose an algorithm that uses random roll-outs and stochastic optimization for learning an optimal policy from the predictions of BNNs. The paper provides a valuable contribution by addressing a clear-cut question with a well-motivated approach.
I decide to accept this paper with two key reasons: (1) the paper tackles a specific and important problem in reinforcement learning, and (2) the approach is well-motivated and supported by experimental results.
The paper provides a thorough investigation of the use of BNNs with stochastic inputs for policy search, including a detailed description of the algorithm and experimental evaluations on several benchmark problems. The results show that the proposed approach outperforms other methods, including Gaussian processes and variational Bayes, in terms of test log-likelihood and policy performance.
However, I have some concerns that need to be addressed. Firstly, the experimental evaluation is limited to a few benchmark problems, and it would be beneficial to see more extensive evaluations on a wider range of problems. Secondly, the paper could benefit from a more detailed analysis of the computational complexity of the proposed algorithm, particularly in comparison to other methods.
To improve the paper, I suggest the following: (1) provide more extensive experimental evaluations on a wider range of problems, (2) include a more detailed analysis of the computational complexity of the proposed algorithm, and (3) consider additional applications of the proposed approach, such as safety and exploration.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How do the authors plan to extend the proposed approach to more complex problems, such as those with high-dimensional state and action spaces? (2) Can the authors provide more details on the computational complexity of the proposed algorithm, particularly in comparison to other methods? (3) How do the authors plan to address the issue of exploration in the proposed approach, particularly in situations where the agent needs to balance exploration and exploitation?