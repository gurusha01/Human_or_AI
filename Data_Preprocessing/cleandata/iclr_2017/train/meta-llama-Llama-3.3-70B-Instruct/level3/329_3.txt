Summary of the Paper's Contributions
The paper proposes a novel recurrent neural network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN architecture uses a multiplicative update rule, where the hidden state is updated by a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation. This approach ensures that the forward hidden state activation norm and the backwards gradient norm are exactly preserved, eliminating the vanishing and exploding gradients problem. The authors also propose a rotation plane parameterization to represent the orthogonal matrices, which allows for efficient optimization and flexible modeling of complex dependencies.
Decision and Reasons
Based on the provided guidelines, I decide to Accept this paper. The main reasons for this decision are:
1. The paper tackles a specific and well-defined problem in the field of RNNs, namely the vanishing and exploding gradients problem.
2. The proposed DORNN architecture is well-motivated and grounded in the literature, and the authors provide a clear and concise explanation of the underlying mathematics and intuition.
Supporting Arguments
The paper provides a thorough analysis of the DORNN architecture and its properties, including the proof of norm preservation and the discussion of the rotation plane parameterization. The authors also provide experimental results on a simplified memory copy task, demonstrating the ability of the DORNN architecture to learn long-term dependencies up to 5,000 time steps. While the experiments are limited, they provide a promising starting point for further exploration and validation of the proposed architecture.
Additional Feedback and Questions
To further improve the paper, I would like to see more extensive experimental results on more complex tasks and comparisons with other state-of-the-art RNN architectures. Additionally, the authors could provide more discussion on the potential limitations and challenges of the DORNN architecture, such as the sensitivity to initialization and the potential loss of expressivity due to the linear input-dependent transition.
Some specific questions I would like the authors to address are:
* How do the authors plan to extend the DORNN architecture to more complex tasks and datasets?
* Can the authors provide more insight into the optimization process and the sensitivity of the rotation plane parameterization to initialization?
* How do the authors envision combining the DORNN architecture with other RNN architectures, such as LSTMs, to leverage the strengths of both approaches?