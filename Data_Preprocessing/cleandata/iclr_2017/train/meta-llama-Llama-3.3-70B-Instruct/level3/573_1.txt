Summary
The paper presents a novel idea that challenges the common assumption that deep neural networks are not affected by bad local minima. The authors provide theoretical examples and empirical evidence to demonstrate that finite-sized neural networks applied to finite-sized datasets can exhibit bad learning dynamics, even when the dataset is linearly separable. They show that the initialization of the model and the structure of the data can significantly impact the learning process, leading to suboptimal solutions.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the evaluation of the paper's success is cursory and lacks comparison to other approaches or human performance, making it difficult to assess its impact. Secondly, the tasks used to test the approach are simple, making it unclear whether the approach is better than simpler alternatives.
Supporting Arguments
The paper's potential is hindered by the lack of thorough evaluation or application to a challenging, previously unsolved problem, leaving its advance unclear. While the authors provide some empirical evidence, it is limited to simple datasets such as MNIST and a synthetic "Jellyfish" dataset. The paper would benefit from more rigorous evaluation and comparison to existing methods to demonstrate its significance.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed evaluations and comparisons to other approaches. They should also consider applying their method to more challenging and real-world problems to demonstrate its effectiveness. Additionally, the authors could provide more insights into the implications of their findings for the design and training of deep neural networks.
Questions for the Authors
I would like the authors to clarify the following points:
1. How do the authors plan to extend their construction to high-dimensional problems, and what are the potential challenges and limitations of this approach?
2. Can the authors provide more detailed analysis of the relationship between the initialization of the model and the structure of the data, and how this impacts the learning process?
3. How do the authors propose to address the issue of bad local minima in deep neural networks, and what are the potential solutions or mitigations?