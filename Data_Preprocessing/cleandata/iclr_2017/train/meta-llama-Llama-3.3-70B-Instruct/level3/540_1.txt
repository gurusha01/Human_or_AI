This paper proposes a novel approach to network morphism, which enables the transformation of a convolutional layer into an arbitrary module of a neural network. The authors introduce a graph-based representation for modules and formulate the network morphism process as a graph transformation problem. They propose two atomic morphing operations and provide algorithms for simple morphable modules and complex modules.
The paper claims to contribute to the field of network morphism by providing a systematic study of the problem at a higher level, and by answering the central question of whether and how a convolutional layer can be morphed into an arbitrary module. The authors also provide extensive experiments on ResNet variants, demonstrating the effectiveness of their proposed morphing approach.
I decide to reject this paper, with two key reasons for this choice. Firstly, the "related work" section could be improved by changing "network morphism" to "knowledge transfer" to better connect with existing works, such as Net2Net and NetMorph. Secondly, the paper lacks experiments demonstrating the advantage of speeding up training and model exploration, which is a major drawback.
To support these reasons, I provide the following arguments. The paper builds upon previous works Net2Net and NetMorph, but the "related work" section does not clearly connect the proposed approach to these existing works. Additionally, the experiments on ResNet variants show promising results, but the source and comparison to training from scratch are unclear. The paper also lacks experiments demonstrating the advantage of speeding up training and model exploration, which is a major drawback.
To improve the paper, I provide the following feedback. The authors should consider changing the "related work" section to better connect with existing works. They should also provide more experiments demonstrating the advantage of speeding up training and model exploration. Furthermore, the authors should explore more complex transformations and provide a clearer comparison to training from scratch.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims. How do the authors plan to improve the "related work" section to better connect with existing works? Can they provide more experiments demonstrating the advantage of speeding up training and model exploration? How do the authors plan to explore more complex transformations and provide a clearer comparison to training from scratch?