This paper proposes a quantitative evaluation method for decoder-based generative models using Annealed Importance Sampling (AIS) to estimate log-likelihoods. The authors compare the performance of different generative models, including GANs, GMMNs, and VAEs, using AIS evaluation and report empirical results on the MNIST dataset. The evaluation framework proposed by the authors is a valuable contribution to the community, providing insights into the behavior of GANs from a log-likelihood perspective.
I decide to accept this paper with two key reasons: (1) the paper tackles a specific and important problem in the field of generative models, and (2) the approach is well-motivated and supported by empirical results. The authors provide a thorough analysis of the limitations of existing evaluation methods, such as Kernel Density Estimation (KDE), and demonstrate the effectiveness of AIS in estimating log-likelihoods.
The paper supports its claims through extensive experiments on the MNIST dataset, including validation of the AIS-based estimates using Bidirectional Monte Carlo (BDMC). The results show that AIS is more accurate than KDE and provides a more reliable measure of log-likelihood. The authors also analyze the degree of overfitting in VAEs, GANs, and GMMNs, and observe that GANs miss important modes of the data distribution.
To improve the paper, I suggest that the authors address some minor issues with the presentation, including missing references, unclear figure captions, and typos. Additionally, I would like the authors to clarify some aspects of the experimental setup, such as the choice of hyperparameters for AIS and the use of different numbers of examples and sources. Increasing the number of chains for AIS could also help tighten the confidence interval reported in the results.
Some questions I would like the authors to answer include: (1) How do the results change when using different observation models, such as a non-Gaussian noise model? (2) Can the authors provide more insights into why GANs miss important modes of the data distribution? (3) How do the results generalize to other datasets and generative models? Answering these questions would provide additional evidence to support the claims made in the paper and further demonstrate the effectiveness of the proposed evaluation framework.