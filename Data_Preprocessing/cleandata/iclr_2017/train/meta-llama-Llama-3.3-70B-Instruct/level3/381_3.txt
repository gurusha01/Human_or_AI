Summary of the Paper's Claims and Contributions
This paper proposes a novel approach to improve word vector representations by utilizing a lexicon and alleviating the adverse effects of polysemy. The authors introduce a fuzzy paraphrase method, where each paraphrase is annotated with a degree of reliability, and a control function is used to jointly learn a corpus with a lexicon. The approach achieves solid results, outperforming prior works, and is easier to use in practical terms as it keeps one vector per word, eliminating the need for word sense disambiguation or part-of-speech tagging.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The approach is well-motivated and placed in the literature, addressing a significant problem in natural language processing.
2. The paper provides a thorough evaluation of the proposed method, including experiments with different parameters, corpus sizes, and comparisons to prior works.
Supporting Arguments
The paper's contributions are substantial, and the authors have made a significant effort to evaluate their approach. The use of fuzzy paraphrases and a control function is innovative, and the results demonstrate the effectiveness of the method. The comparison to prior works is thorough, and the authors have made an effort to reproduce the results of other studies.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
* Providing more details on the implementation of the control function and the fuzzy paraphrase method.
* Including more analysis on the effects of the corpus size and the choice of parameters on the results.
* Considering additional experiments with other types of corpora or tasks to demonstrate the robustness of the approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the control function is implemented, and how the fuzzy paraphrases are annotated with degrees of reliability?
* How do you plan to extend this work to other models or tasks, and what are the potential limitations of the approach?
* Can you provide more insights on the trade-offs between the different parameters and the choice of corpus size, and how these affect the results?