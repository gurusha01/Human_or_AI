Summary
The paper proposes a novel approach to improve the performance of gradient descent when utilizing multiple compute resources. The authors design a baseline asynchronous gradient descent and a layer-wise gradient descent method, which overlaps weight updates of a layer with inter-node synchronization of other layers. They implement their approaches using the Machine Learning Toolkit for Extreme Scale (MaTEx) and evaluate them on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system.
Decision
I decide to reject this paper, with the main reason being that the proposed contribution is considered relatively incremental compared to previous works, and the results do not convincingly demonstrate the advantages of the proposed approach.
Supporting Arguments
While the paper presents a well-motivated approach to improving gradient descent, the experimental results are not directly comparable to previous works, with discrepancies in reported results and potential differences in experimental settings. Furthermore, the paper's evaluation of the proposed approach shows that it is not significantly better than existing methods, with a speedup of up to 1.7x compared to synchronous gradient descent. However, this improvement comes at the cost of delayed gradient updates, which may not be desirable in all scenarios.
Additional Feedback
To improve the paper, the authors should provide a more thorough comparison with existing works, including a detailed analysis of the trade-offs between different approaches. Additionally, the authors should consider evaluating their approach on a wider range of datasets and models to demonstrate its generalizability. It would also be helpful to provide more insight into the implementation details, such as the specific optimizations used to achieve the reported performance.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the potential drawbacks of delayed gradient updates, such as increased memory requirements and potential convergence issues?
2. Can the authors provide more details on the experimental setup, including the specific hardware configurations and dataset partitions used?
3. How do the authors plan to extend their approach to other optimization algorithms and deep learning models, such as those using attention mechanisms or graph neural networks?