Summary
The paper proposes a novel "density-diversity penalty" regularizer to encourage low diversity and high sparsity in the fully-connected layers of deep neural networks. This approach enables the compression of trained models, making them more suitable for deployment on lightweight architectures such as portable devices and Internet-of-Things devices. The authors demonstrate the effectiveness of their method on both MNIST and TIMIT datasets, achieving significant compression rates while maintaining comparable performance to the original models.
Decision
I decide to Accept this paper, with the primary reason being that it presents a well-motivated and novel approach to compressing deep neural networks. The paper is well-structured, and the authors provide a clear explanation of their methodology and experimental results.
Supporting Arguments
The paper tackles a specific and relevant problem in the field of deep learning, namely the compression of fully-connected layers in neural networks. The authors' approach is well-motivated, and they provide a thorough review of related work in the area. The experimental results demonstrate the effectiveness of the proposed method, and the authors provide a detailed analysis of the compression rates achieved on both MNIST and TIMIT datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the choice of hyperparameters, such as the value of λj, and how they affect the compression rate and performance of the model. Additionally, it would be interesting to see a comparison with other compression methods, such as quantization and pruning, to better understand the strengths and weaknesses of the proposed approach.
Questions for the Authors
1. Can you provide more details on how the density-diversity penalty is optimized during training, and how the sorting trick is implemented in practice?
2. How do you choose the value of λj, and what is the effect of varying this hyperparameter on the compression rate and performance of the model?
3. Have you considered applying the density-diversity penalty to other types of neural network layers, such as convolutional or recurrent layers, and if so, what were the results?