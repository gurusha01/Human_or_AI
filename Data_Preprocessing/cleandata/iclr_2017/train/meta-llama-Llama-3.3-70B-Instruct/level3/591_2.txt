Summary of the Paper's Contributions
This paper explores the concept of "sample importance" in deep neural networks, which refers to the contribution of each sample to the change in model parameters during training. The authors provide empirical results on the effects of sample importance on the learning process, revealing that different training cases induce larger gradients at various stages of learning and layers. They also demonstrate that ordering samples by either negative log-likelihood (NLL) or sample importance (SI) can be worse than mixed or random batch construction.
Decision and Key Reasons
Based on the evaluation guidelines, I decide to Accept this paper. The two key reasons for this decision are:
1. The paper tackles a specific and well-motivated question, namely, understanding the concept of sample importance and its impact on the learning process of deep neural networks.
2. The approach is well-placed in the literature, and the empirical results provide valuable insights into the effects of sample importance on the learning process, challenging common curriculum learning ideas.
Supporting Arguments
The paper provides a clear and concise introduction to the concept of sample importance, which is well-motivated by the need to understand how different samples contribute to the learning process. The empirical analysis is thorough, and the results are well-presented, providing insights into the effects of sample importance on the learning process. The paper also explores the relationship between sample importance and negative log-likelihood, which is an interesting and relevant aspect of the research.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors consider factoring out gradient magnitudes to better understand sample importance and exploring improvements to batch selection algorithms based on sample importance. Additionally, the authors may want to investigate the application of sample importance to other deep learning structures, such as convolutional neural networks and recurrent neural networks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the sample importance is calculated, and how it relates to the concept of leverage in statistics?
2. How do you think the results of this paper can be applied to improve the efficiency and effectiveness of deep learning models in practice?
3. Are there any potential limitations or biases in the empirical analysis, and how can they be addressed in future research?