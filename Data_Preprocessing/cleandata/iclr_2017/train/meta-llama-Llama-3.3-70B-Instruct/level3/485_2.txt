Summary
The paper explores the optimization of real-valued matrices within a configurable margin about the Stiefel manifold, which is a set of orthogonal matrices. The authors propose a weight matrix factorization and parameterization strategy that allows them to bound matrix norms and control the degree of expansivity induced during backpropagation. They investigate the effect of loosening hard orthogonality constraints on convergence and performance in various tasks, including synthetic memory tasks and real-world datasets such as MNIST and Penn Treebank.
Decision
I decide to accept this paper with minor revisions. The paper provides a well-motivated approach to addressing the vanishing and exploding gradient problems in deep neural networks, and the experiments demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper tackles a specific question of optimizing real-valued matrices within a configurable margin about the Stiefel manifold, which is a well-defined problem. The approach is well-motivated, and the authors provide a clear explanation of the background and related work. The experiments are thorough and demonstrate the effectiveness of the proposed method in various tasks. The paper also provides a detailed analysis of the results and discusses the implications of the findings.
Additional Feedback
To improve the paper, I suggest that the authors provide more clarity on the theory sections, including formal statements of expectations for an embedding that accurately and efficiently preserves a monotonic chain. Additionally, the authors could explore deeper networks and classification loss in their experiments to further demonstrate the effectiveness of the proposed method. It would also be helpful to provide more details on the implementation of the method, including the running time and computational resources required.
Questions for the Authors
1. Can you provide more details on the choice of the sigmoidal parameterization for the singular values, and how it affects the optimization process?
2. How do the authors plan to extend the proposed method to more complex tasks and larger datasets?
3. Can you provide more insights on the relationship between the spectral margin and the convergence rate of the optimization process?