Summary of the Paper's Contributions
The paper introduces Attentive Recurrent Comparators (ARCs), a novel class of neural networks that learn to estimate the similarity of a set of objects by cycling through them and making observations. The authors demonstrate that ARCs outperform traditional Siamese neural networks and achieve state-of-the-art performance on the Omniglot dataset for one-shot classification, surpassing human performance.
Decision and Key Reasons
I decide to accept this paper with the following key reasons: (1) the paper presents a well-motivated and novel approach to similarity learning, and (2) the empirical results demonstrate the effectiveness of ARCs in achieving state-of-the-art performance on a challenging task.
Supporting Arguments
The paper is well-executed, with a clear and concise presentation of the ARC model and its components. The authors provide a thorough analysis of the model's performance, including qualitative and quantitative studies, and demonstrate its superiority over traditional Siamese neural networks. The use of attention and recurrence in ARCs allows for early fusion of information, which is shown to be beneficial for similarity learning.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more details on the computational cost of ARCs and compare it to traditional Siamese neural networks. Additionally, it would be interesting to see an analysis of the attention mechanism's behavior and how it contributes to the model's performance. Some questions I would like the authors to answer include: (1) How do the authors plan to address the potential computational expense of ARCs, and (2) Can the ARC model be applied to other modalities beyond images? 
I would also like the authors to elaborate on the following points: 
- How does the model's performance change with the number of glimpses and the size of the attention window?
- Can the authors provide more insights into the learned attention patterns and how they relate to human attention?
- How does the model's performance compare to other state-of-the-art methods on other datasets and tasks? 
Overall, the paper presents a significant contribution to the field of similarity learning and one-shot classification, and with some additional clarification and analysis, it has the potential to be a strong accept.