Summary of the Paper's Contributions
The paper proposes a novel approach to learning bilingual word vectors by leveraging the concept of orthogonal transformations. The authors demonstrate that a self-consistent linear transformation between vector spaces should be orthogonal and can be obtained using the singular value decomposition (SVD) on a dictionary of translation pairs. They introduce a new "inverted softmax" method to improve the accuracy of predicted translations and show that combining the SVD with the inverted softmax and dimensionality reduction achieves state-of-the-art results in translating English words into Italian.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's empirical evaluation is questionable, particularly the use of the MNIST dataset, which may not be suitable for demonstrating the method's effectiveness. Secondly, the authors' comparison of their approach with regular CNNs on the MNIST dataset is flawed due to the removal of important structural information, leading to unfair conclusions.
Supporting Arguments
The paper's current form is not ready for publication, but it may be resubmitted after addressing concerns and modifying the method to incorporate additional graph structure. The authors' approach to learning bilingual word vectors is novel and shows promise, but the evaluation methodology is flawed. The use of the MNIST dataset is not suitable for evaluating the effectiveness of the method, and the comparison with regular CNNs is unfair.
Additional Feedback and Questions
To improve the paper, I suggest that the authors revisit their evaluation methodology and consider using more suitable datasets and comparison methods. Additionally, I would like to see more analysis on the robustness of the orthogonal transformations to noise and the impact of dimensionality reduction on the translation performance. Some questions I would like the authors to answer include: How do the authors plan to address the issue of overfitting to the training dictionary? Can the authors provide more insights into the choice of hyperparameters for the inverted softmax and dimensionality reduction? How do the authors plan to extend their method to other languages and domains?