Summary
The paper proposes a novel LSTM parametrization, called Normalized LSTM, which preserves the means and variances of the hidden states and memory cells across time. This approach is inspired by Normalization Propagation (Norm Prop) and is designed to avoid the extra computations required by Batch Normalization (BN) and Layer Normalization (LN). The authors derive the gradients of the Normalized LSTM and investigate its impact on the gradient flow. They also propose a scheme to initialize the weight matrices and validate the performances of the Normalized LSTM on two tasks: character-level language modelling and image generative modelling.
Decision
I decide to Accept this paper, with the main reason being that the proposed approach is well-motivated and provides a significant improvement in computation time compared to existing normalization techniques. The paper is also well-structured and easy to follow, with a clear explanation of the proposed method and its advantages.
Supporting Arguments
The paper tackles a specific problem in the field of recurrent neural networks, namely the need for efficient and effective normalization techniques. The proposed approach is well-placed in the literature, building upon existing work on Norm Prop and BN. The authors provide a clear and concise explanation of the proposed method, including its theoretical foundations and empirical evaluations. The experimental results demonstrate the effectiveness of the Normalized LSTM in terms of computation time and performance, making it a valuable contribution to the field.
Additional Feedback
To further improve the paper, I would like to see more discussion on the implications of the proposed approach on the gradient flow and its potential impact on the training process. Additionally, it would be interesting to see more experiments on the sensitivity of the proposed method to hyperparameters, such as the initialization of the weight matrices and the choice of the rescaling parameters. I would also like to ask the authors to clarify the relationship between the proposed approach and other existing normalization techniques, such as Weight Normalization and Layer Normalization.
Questions for the Authors
1. Can you provide more intuition on why the proposed approach is able to preserve the normalization of the hidden states and memory cells across time, and how this relates to the theoretical results on the gradient flow?
2. How do you think the proposed approach will perform on more challenging tasks, such as speech recognition or machine translation, and what potential limitations or challenges do you foresee?
3. Can you provide more details on the implementation of the proposed approach, including the specific hyperparameters used and the computational resources required for the experiments?