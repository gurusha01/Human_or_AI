Summary of the Paper's Contributions
The paper explores the concept of preimages in deep neural networks, specifically in the context of fully connected multi-layer rectifier networks. The authors demonstrate how to compute the preimages of activities at arbitrary levels in a deep network, which are the sets of inputs that result in the same node activity. They show that these preimages are piecewise linear manifolds in the input space and can be used as building blocks for describing the input manifolds of specific classes. The paper also discusses the implications of this concept for understanding the efficiency of deep learning networks and potentially designing more efficient training algorithms.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Reject it. The two key reasons for this decision are:
1. Lack of Novel Theoretical Results or Practical Conclusions: While the paper provides a preliminary analysis of the geometry of preimages, it lacks novel theoretical results or practical conclusions that significantly advance our understanding of deep neural networks.
2. Preliminary Nature of the Analysis: The paper's analysis seems preliminary, and further research is needed to fully develop this direction, including investigating how network depth, width, and training affect the division of space.
Supporting Arguments
The paper's contribution to the understanding of preimages in deep neural networks is interesting, but it requires more development to be considered a significant advancement in the field. The authors' approach to formalizing the geometry of preimages is well-motivated, but the current analysis is not sufficient to support the claims made in the paper. Additionally, the paper could benefit from alternative approaches, such as a direct inductive approach, to provide a more comprehensive understanding of the preimage concept.
Additional Feedback and Questions
To improve the paper, I suggest that the authors:
* Provide more concrete examples and empirical evidence to support their claims about the preimage concept.
* Investigate the relationship between preimages and other concepts in deep learning, such as adversarial examples and robustness.
* Consider alternative approaches to formalizing the geometry of preimages, such as using differential geometry or topological methods.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to extend their analysis to convolutional neural networks, which are commonly used in practice?
* Can the authors provide more insight into how the preimage concept relates to the efficiency of deep learning networks and the design of more efficient training algorithms?
* How do the authors plan to address the issue of pooling in deep learning networks, which is not considered in the current analysis?