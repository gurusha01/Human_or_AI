This paper presents a novel approach to learning reward functions for robotic tasks using visual demonstrations. The authors propose a method that leverages pre-trained deep models to infer task goals and sub-goals from a few video demonstrations, without requiring explicit sub-goal supervision. The approach is based on a simple approximate inverse reinforcement learning method, which assumes that the distribution over each activation at each time step is independent of all other activations and time steps.
The paper claims to contribute a method for perceptual reward learning from only a few demonstrations of real-world tasks, with automated unsupervised discovery of intermediate steps. The authors also demonstrate the first vision-based reward learning method that can learn a complex robotic manipulation task from a few human demonstrations in real-world robotic experiments.
I decide to accept this paper with minor revisions. The key reasons for this choice are that the paper presents a well-motivated approach to learning reward functions for robotic tasks, and the experimental results demonstrate the effectiveness of the method in learning complex tasks such as door opening and liquid pouring.
The approach is well-motivated, as it addresses the challenges of designing suitable reward functions for real-world tasks, which often require considerable manual engineering and additional sensors. The use of pre-trained deep models to infer task goals and sub-goals is a clever idea, and the experimental results show that the method can learn effective and generalizable reward functions for real-world manipulation skills.
The paper supports its claims with empirical results on two real-world tasks, door opening and liquid pouring, and demonstrates that the learned reward functions can be used to learn a real-world robotic motion skill. The results show that the method can learn a door opening skill with a real physical robot, and that the learned reward function is robust to variations in appearance.
To improve the paper, I suggest providing additional details on the implementation of the PI2 reinforcement learning algorithm, and clarifying the relationship between the learned reward function and the policy learned by the robot. Additionally, it would be helpful to provide more quantitative results on the performance of the method, such as the success rate of the robot in learning the task, and the number of demonstrations required to learn the reward function.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How does the method handle cases where the demonstrations are noisy or incomplete? (2) Can the method be extended to learn reward functions for tasks with multiple objectives or constraints? (3) How does the method compare to other approaches to learning reward functions, such as inverse reinforcement learning or apprenticeship learning?