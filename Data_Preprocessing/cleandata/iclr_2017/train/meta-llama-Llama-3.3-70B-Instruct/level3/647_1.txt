Summary
The paper proposes a novel learning framework called Recurrent Inference Machines (RIMs) for solving inverse problems. RIMs abandon the traditional separation between model and inference, instead learning both components jointly without defining their explicit functional form. The authors demonstrate the effectiveness of RIMs on various image reconstruction tasks, showing competitive results with state-of-the-art methods.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the novelty of the paper is limited, as the idea of learning inference procedures is not new and has been explored in previous works such as LISTA and Andrychowicz et al. (2016). Secondly, the choice of RNN block architecture and non-linearity restricts flexibility and implicitly forms a family of variational energy and inference algorithms, similar to existing works.
Supporting Arguments
The paper's claim of novelty is debated, as the idea of separating the model and inference procedure is not entirely new. The authors' argument regarding references [1] and [2] is also disagreed upon, as these works do not require a separate step for MAP inference and can be explained through a learnable neural network. Furthermore, the choice of RNN block architecture and non-linearity limits the flexibility of the framework and makes it similar to existing works.
Additional Feedback
To improve the paper, I suggest adding discussions on the chosen architecture and non-linearity, and exploring more flexible and generalizable frameworks. The authors could also provide more comparisons with existing works and demonstrate the advantages of RIMs over other methods. Additionally, the authors could consider addressing the potential limitations and challenges of the proposed framework, such as the need for large amounts of training data and the potential for overfitting.
Questions for the Authors
I would like the authors to clarify the following points:
* How do the authors respond to the criticism that the novelty of the paper is limited, and how do they differentiate their work from existing approaches?
* Can the authors provide more insights into the choice of RNN block architecture and non-linearity, and how these choices affect the performance of the framework?
* How do the authors plan to address the potential limitations and challenges of the proposed framework, such as the need for large amounts of training data and the potential for overfitting?