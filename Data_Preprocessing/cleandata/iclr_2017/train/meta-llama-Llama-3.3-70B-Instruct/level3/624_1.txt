Summary of the Paper's Contributions
The paper highlights the importance of initialization in deep networks and counters the notion that modern architectures and techniques have made optimization issues obsolete. It presents counter-examples of bad initialization leading to poor solutions, specifically in the context of finite-sized datasets and rectified linear unit (ReLU) networks. The authors argue that the error surface of deep models is not always well-behaved, and that initialization plays a crucial role in determining the quality of the solution.
Decision: Reject
The main reasons for this decision are:
1. Lack of systematic analysis: The paper fails to provide a systematic analysis of current heuristic techniques for initialization and their limitations.
2. Artificial counter-examples: The counter-examples presented in the paper seem artificial and do not consider popular heuristics like non-saturating activation functions and batch normalization.
Supporting Arguments
The paper's focus on ReLU networks and finite-sized datasets is too narrow, and the counter-examples presented are not convincing. The authors do not provide a thorough analysis of the conditions under which bad initialization can lead to poor solutions, and the paper lacks a clear conclusion or recommendation for practitioners.
Additional Feedback
To improve the paper, the authors should:
1. Conduct a more thorough analysis: Provide a systematic analysis of current heuristic techniques for initialization and their limitations.
2. Consider more realistic scenarios: Use more realistic datasets and models to demonstrate the importance of initialization.
3. Provide concrete recommendations: Offer concrete recommendations for practitioners on how to initialize deep networks to avoid poor solutions.
Questions for the Authors
1. Can you provide more insight into why you chose to focus on ReLU networks and finite-sized datasets?
2. How do you plan to extend your analysis to more realistic scenarios and models?
3. Can you provide more concrete recommendations for practitioners on how to initialize deep networks to avoid poor solutions?