This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning (RL) to refine a pre-trained Recurrent Neural Network (RNN) for music generation tasks. The approach uses a pre-trained RNN to supply part of the reward value in an RL model, allowing the model to learn from data while incorporating music theory constraints.
The paper claims to contribute to the sequence training and RL literature by proposing a new method for combining maximum likelihood (ML) and RL training, and by exploring the connection between this approach and stochastic optimal control (SOC)/KL-control. The authors also provide empirical comparisons between their approach and other methods, such as Q-learning and G-learning.
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks concrete numbers on training and testing time, as well as justification for not making high-resolution comparisons. This makes it difficult to assess the practicality and effectiveness of the proposed approach. Secondly, the results are somewhat tantalizing, especially in terms of diversity, but the low resolution and lack of thorough comparisons to other methods are major drawbacks.
To support my decision, I provide the following arguments. The paper's approach is well-motivated, and the use of RL to refine a pre-trained RNN is a promising direction. However, the lack of concrete numbers and justification for not making high-resolution comparisons raises concerns about the paper's rigor and thoroughness. Additionally, the results, while promising, are not sufficient to demonstrate the effectiveness of the proposed approach, especially given the low resolution and limited comparisons.
To improve the paper, I suggest that the authors provide more detailed information about the training and testing time, as well as justification for not making high-resolution comparisons. Additionally, the authors should consider providing more thorough comparisons to other methods, such as Q-learning and G-learning, to demonstrate the effectiveness of their approach. Finally, the authors should consider increasing the resolution of their results to better demonstrate the quality of the generated melodies.
I would like to ask the authors the following questions to clarify my understanding of the paper: (1) Can you provide more detailed information about the training and testing time for the proposed approach? (2) Can you justify why you did not make high-resolution comparisons in your experiments? (3) Can you provide more thorough comparisons to other methods, such as Q-learning and G-learning, to demonstrate the effectiveness of your approach? (4) Can you increase the resolution of your results to better demonstrate the quality of the generated melodies? 
The specific question or problem tackled by the paper is how to refine a pre-trained RNN for music generation tasks using RL. The approach is well-motivated, and the use of RL to refine a pre-trained RNN is a promising direction. However, the paper's lack of concrete numbers and justification for not making high-resolution comparisons raises concerns about its rigor and thoroughness. 
The paper does not fully support its claims, as the results are not sufficient to demonstrate the effectiveness of the proposed approach, especially given the low resolution and limited comparisons. To better support its claims, the paper should provide more detailed information about the training and testing time, as well as justification for not making high-resolution comparisons. Additionally, the paper should provide more thorough comparisons to other methods, such as Q-learning and G-learning, to demonstrate the effectiveness of the proposed approach. 
Overall, while the paper proposes a promising approach, its lack of concrete numbers and justification for not making high-resolution comparisons, as well as its limited results, make it difficult to assess its effectiveness and practicality. With additional information and more thorough comparisons, the paper could be significantly improved.