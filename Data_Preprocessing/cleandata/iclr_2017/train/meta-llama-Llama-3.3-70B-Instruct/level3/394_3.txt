Summary
The paper proposes a novel attention framework for sentiment analysis, which incorporates local contexts with a rough global context attention. The model is inspired by human reading behavior and consists of two parts: a bidirectional LSTM (Bi-LSTM) network to extract a global context representation, and another Bi-LSTM with attention to learn local contexts and composite them using the global context attention. The authors evaluate their model on several benchmark datasets and demonstrate its effectiveness in achieving state-of-the-art results.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in sentiment analysis, and (2) the approach is well-motivated and supported by a variety of experiments that validate the claims made about the model's effectiveness.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of sentiment analysis and the limitations of existing methods. The authors propose a novel attention framework that is inspired by human reading behavior, which is a unique and interesting approach. The model is evaluated on several benchmark datasets, and the results demonstrate its effectiveness in achieving state-of-the-art results. The authors also provide a detailed analysis of the attention weights, which provides insight into how the model works.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the sensitivity of the model to different hyperparameters. Additionally, it would be interesting to see a comparison with other attention-based models, such as those used in machine translation or question answering tasks. The authors may also want to consider providing more visualizations of the attention weights, such as heatmaps or plots, to help illustrate how the model is working.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How did they select the hyperparameters for the model, and what was the sensitivity of the model to different hyperparameters? (2) Can they provide more details on the attention mechanism, such as how the attention weights are calculated and what is the role of the global context representation in the attention mechanism? (3) How do they plan to extend this work to other NLP tasks, such as machine translation or question answering?