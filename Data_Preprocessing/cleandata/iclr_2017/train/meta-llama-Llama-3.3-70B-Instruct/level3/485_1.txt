Summary
The paper presents a novel approach to controlling the expansivity of gradients during backpropagation in Recurrent Neural Networks (RNNs) by manipulating orthogonality constraints and regularization on matrices. The authors propose a factorization technique that allows for bounding the spectral norms of weight matrices, which can help improve optimization convergence rate and model performance. The paper also explores the effect of loosening hard orthogonality constraints and introducing soft constraints on the spectral norms of weight matrices.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks reference to relevant prior work on learning data representations from local tangent planes, which could provide insight into the inductive biases of deep networks. Secondly, the analysis may not generalize to more complex data or deep network architectures that are not pure ReLU networks, such as those using batch normalization.
Supporting Arguments
The paper presents a well-motivated approach to addressing the vanishing and exploding gradient problem in RNNs. However, the results are not surprising given the existing knowledge about deep networks and the selected architecture and data structure being highly compatible. The paper also lacks a clear comparison to non-parametric techniques, which could provide a more comprehensive understanding of the proposed approach. Furthermore, the error bound presented is vacuous for practical settings, with an upper bound exponential in total curvature, making it uninformative about the representational power of deep nets.
Additional Feedback
To improve the paper, the authors should address the lack of reference to prior work and the limitations of the analysis. They should also consider comparing the deep network approach to non-parametric techniques and exploring the inductive biases introduced by the deep net. Additionally, the loss function used in the experiments is unclear and may encourage the network to produce infinitely large distances, which could be resolved by squaring the difference. The authors should also provide more details on the experimental setup and the hyperparameter tuning process.
Questions for the Authors
1. How do the authors plan to address the lack of reference to prior work on learning data representations from local tangent planes?
2. Can the authors provide more details on the experimental setup and the hyperparameter tuning process?
3. How do the authors plan to extend the analysis to more complex data or deep network architectures that are not pure ReLU networks?