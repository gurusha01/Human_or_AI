The paper proposes a novel approach to adapt neural network language models to their recent history by introducing a neural cache model. This model stores past hidden activations as memory and accesses them through a dot product with the current hidden activation, allowing for efficient and scalable adaptation to dynamic environments. The authors demonstrate the effectiveness of their approach on several language modeling datasets and the LAMBADA dataset, showing significant performance gains over traditional language models.
I decide to reject this paper, primarily due to the lack of convincing quantitative evidence and the overly long and verbose presentation. While the proposed method is interesting and shows promise, the results are not thoroughly evaluated, and the paper relies heavily on qualitative assessments. Additionally, the explanation for the phenomenon of scaling an image by a small factor placing more importance on pixels related to the correct class prediction is unconvincing and lacks deeper insight.
The paper's main contribution is the proposal of a continuous version of the cache model, which can be adapted to any neural network language model. However, the approach is not well-motivated, and the authors fail to provide a clear understanding of how the neural cache model works and why it is effective. The paper also assumes pixel independence, which may not be a realistic assumption, and the proposed scheme for feature importance ranking is noisy.
To improve the paper, I suggest that the authors provide more rigorous quantitative evaluations, including comparisons to other state-of-the-art methods, and clarify the explanation for the phenomenon of scaling an image. Additionally, the authors should consider providing more insight into the neural cache model and its properties, as well as addressing the assumption of pixel independence.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) Can you provide more detailed quantitative results, including comparisons to other state-of-the-art methods? (2) Can you clarify the explanation for the phenomenon of scaling an image by a small factor placing more importance on pixels related to the correct class prediction? (3) How do you address the assumption of pixel independence in the proposed scheme for feature importance ranking?