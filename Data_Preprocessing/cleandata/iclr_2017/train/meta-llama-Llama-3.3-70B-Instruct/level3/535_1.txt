Summary
The paper proposes a novel approach to distributed stochastic optimization, called synchronous stochastic optimization with backup workers (Sync-SGD). This method aims to mitigate the straggler effect in synchronous stochastic optimization while avoiding the staleness issue in asynchronous stochastic optimization. The authors demonstrate the effectiveness of their approach through empirical experiments on various models, including Inception and PixelCNN, and show that it outperforms asynchronous stochastic optimization in terms of test accuracy and convergence speed.
Decision
I decide to reject this paper, with the main reason being that the contribution is considered too incremental for a conference paper at ICLR. While the paper presents a well-motivated approach and demonstrates its effectiveness through experiments, the novelty of the contribution is not sufficient to warrant acceptance at a top-tier conference like ICLR.
Supporting Arguments
The paper builds upon previous work on distributed stochastic optimization and proposes a specific architecture to mitigate the straggler effect. However, the idea of using backup workers to alleviate stragglers is not entirely new, and the paper does not provide a significant breakthrough in terms of theory or methodology. The experimental results are impressive, but they are mostly limited to two models (Inception and PixelCNN), and it is unclear whether the approach will generalize to other models and datasets.
Additional Feedback
To improve the paper, I suggest that the authors conduct more extensive experiments on various models and datasets to demonstrate the robustness and generality of their approach. Additionally, they could provide more theoretical analysis to support their claims and provide a more detailed comparison with existing methods. The authors may also consider resubmitting the paper to a more suitable venue, such as a workshop or a conference with a focus on distributed optimization.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the backup workers are selected and how their gradients are aggregated with the primary workers?
2. How do you handle the case where the backup workers are slower than the primary workers, and what is the impact on the overall convergence speed?
3. Can you provide more experimental results on other models and datasets to demonstrate the generality of your approach?
4. How does your approach compare to other methods that aim to mitigate the straggler effect, such as softsync or asynchronous stochastic optimization with gradient clipping?