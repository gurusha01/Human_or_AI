Summary
The paper introduces a novel framework for multitask deep reinforcement learning guided by policy sketches. The approach associates each subtask with a modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating significant improvements over standard baselines.
Decision
I decide to Accept this paper, with the primary reason being the novelty and effectiveness of the proposed approach. The paper presents a well-motivated and well-placed contribution in the literature, addressing the challenging problem of multitask reinforcement learning with sparse rewards.
Supporting Arguments
The paper provides a clear and concise introduction to the problem, followed by a detailed description of the proposed approach. The authors demonstrate the effectiveness of their method through extensive experiments on two complex environments, showcasing significant improvements over baseline methods. The paper also provides a thorough analysis of the importance of various components of the training procedure, including the decoupled critic and curriculum learning.
Additional Feedback
To further improve the paper, I suggest providing more details on the implementation of the subpolicies and critics, as well as additional visualizations of the learned policies and sketches. Additionally, it would be beneficial to explore the applicability of the proposed approach to more complex and realistic environments.
Questions for the Authors
1. Can you provide more insight into the choice of hyperparameters, such as the step size and gradient clipping, and how they were tuned?
2. How do you plan to extend the proposed approach to more complex environments, such as those with continuous action spaces or high-dimensional state spaces?
3. Can you provide more details on the computational resources required to train the models, and how the approach scales to larger environments and task sets?