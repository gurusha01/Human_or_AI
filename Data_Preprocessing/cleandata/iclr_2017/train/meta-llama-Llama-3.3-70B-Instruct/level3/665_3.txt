Summary of the Paper's Contributions
The paper proposes a novel approach to detecting errors and out-of-distribution examples in machine learning models. The authors introduce a simple baseline that utilizes probabilities from softmax distributions to detect whether an example is misclassified or from a different distribution than the training data. They demonstrate the effectiveness of this baseline across various computer vision, natural language processing, and automatic speech recognition tasks. Additionally, they propose an abnormality module that can be used to improve detection performance by exploiting the learned internal representations of neural networks.
Decision and Key Reasons
Based on the review, I decide to Accept the paper with minor revisions. The key reasons for this decision are:
1. The paper tackles a specific and important problem in machine learning, namely detecting errors and out-of-distribution examples.
2. The approach is well-motivated and grounded in the literature, and the authors provide a clear and concise explanation of their methodology.
Supporting Arguments
The paper provides a thorough analysis of the problem and the proposed solution, including a detailed description of the experimental setup and results. The authors also provide a clear discussion of the limitations of their approach and potential avenues for future research. The use of various evaluation metrics, including AUROC and AUPR, provides a comprehensive understanding of the performance of the proposed baseline and abnormality module.
Additional Feedback
To improve the paper, I suggest that the authors:
1. Provide more details on the implementation of the abnormality module, including the architecture and training procedure.
2. Consider adding more experiments to evaluate the robustness of the proposed approach to different types of noise and distortions.
3. Provide a more detailed discussion of the potential applications and implications of the proposed approach in real-world scenarios.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the choice of evaluation metrics, particularly AUROC and AUPR, and how they relate to the problem of error and out-of-distribution detection?
2. How do you plan to address the potential limitations of the proposed approach, such as the reliance on softmax probabilities and the potential for overfitting to the training data?
3. Can you provide more insights into the potential applications and implications of the proposed approach in real-world scenarios, such as medical diagnosis or autonomous driving?