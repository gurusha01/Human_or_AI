Summary of the Paper's Claims and Contributions
The paper explores the possibility of learning chess game concepts under weak supervision with convolutional neural networks (CNNs). Specifically, it trains a CNN to predict 18 categories of tic-tac-toe boards with 100% accuracy and uses Class Activation Mapping (CAM) to visualize where the CNN focuses when making decisions. The paper claims to have discovered game rules and demonstrates the applicability of cross-modal supervision for representation learning in the context of higher-level semantics. Additionally, it shows that CAM can activate at non-salient regions, highlighting the CNN's ability to collect information from context.
Decision and Key Reasons
I decide to Reject this paper, with two key reasons for this choice. Firstly, I am unsure about the paper's claims of discovering game rules and its connection to weakly supervised learning or multi-modal learning. The paper's scenario seems contrived, using a complex CNN for a simple game domain and a specific visualization method, which may limit its significance. Secondly, I lack expertise in reinforcement learning, which may be relevant to appreciating the paper's contributions in the context of related works on CNN game playing.
Supporting Arguments
The paper's use of a simple game like tic-tac-toe and a specific visualization method like CAM may not be representative of more complex game domains or real-world scenarios. Furthermore, the paper's claims of discovering game rules may be overstated, as the CNN may be simply learning patterns in the visual inputs rather than truly understanding the game concepts. The paper's lack of comparison to other methods or baselines also makes it difficult to evaluate the significance of its contributions.
Additional Feedback and Questions
To improve the paper, I would suggest that the authors provide more context and comparison to other related works, particularly in the area of reinforcement learning. Additionally, the authors could consider using more complex game domains or real-world scenarios to demonstrate the applicability of their approach. I would also like to ask the authors to clarify their claims of discovering game rules and how they define "game rules" in the context of their work. Specifically, I would like to know:
* How do the authors define "game rules" and what specific rules do they claim to have discovered?
* How do the authors plan to extend their approach to more complex game domains or real-world scenarios?
* What are the implications of their work for the field of reinforcement learning and game playing?