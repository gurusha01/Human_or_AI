Summary of the Paper
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in RNNs. The DORNN architecture uses a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation to update the hidden state, ensuring that the forward hidden state activation norm and backwards gradient norm are preserved. The authors demonstrate the effectiveness of their approach on a simplified memory copy task, achieving long-term dependencies of up to 5,000 timesteps.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's hypothesis that having gradients flow is enough to solve long-term dependency problems lacks empirical evidence to support this claim. Secondly, the restriction to orthogonal matrices limits the representational power of the model, making it unable to capture complex attractors or deal with noise effectively.
Supporting Arguments
The paper's approach, while theoretically sound, neglects potential side-effects of preserving gradients, such as the inability to learn what to preserve and what to discard in limited capacity scenarios. Furthermore, the focus on preserving gradients has led to a lack of consideration for other important aspects of RNNs, such as the ability to capture complex patterns and relationships in the data. The experimental results, while promising, are limited to a simplified memory copy task and do not demonstrate the effectiveness of the approach in more realistic scenarios with complex datasets.
Additional Feedback
To improve the paper, I suggest that the authors provide more empirical evidence to support their claims, including experiments on more complex tasks and datasets. Additionally, the authors should consider the limitations of their approach and explore ways to address these limitations, such as incorporating non-linearities or other techniques to improve the representational power of the model. The authors should also provide more discussion on the potential applications and implications of their approach, including its potential to be used in combination with other RNN architectures.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. Can you provide more empirical evidence to support the claim that preserving gradients is sufficient to solve long-term dependency problems?
2. How do you plan to address the limitations of your approach, including the restriction to orthogonal matrices and the lack of consideration for other important aspects of RNNs?
3. Can you provide more discussion on the potential applications and implications of your approach, including its potential to be used in combination with other RNN architectures?