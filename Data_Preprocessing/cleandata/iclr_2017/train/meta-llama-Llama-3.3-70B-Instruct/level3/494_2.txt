Summary
The paper presents a novel approach to understanding the Hessian of the loss function in deep learning, revealing a two-phased spectrum with a bulk of eigenvalues concentrated around zero and a discrete part that depends on the data. The authors provide empirical evidence for this phenomenon, demonstrating its implications on the theoretical foundations of deep learning and potential applications in optimizing training procedures.
Decision
I decide to Accept this paper, with the primary reason being the novelty and significance of the findings, which have the potential to impact our understanding of deep learning landscapes. The second reason is the thoroughness of the experimental evaluation, which provides convincing evidence for the authors' claims.
Supporting Arguments
The paper is well-written, and the authors effectively motivate their approach, placing it within the context of existing research in deep learning. The experimental results are impressive, demonstrating a clear understanding of the Hessian's behavior and its dependence on the data and model architecture. The authors also provide a clear discussion of the implications of their findings, highlighting potential avenues for future research.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more context on the computational complexity of calculating the Hessian, as well as discussing potential limitations of their approach. Additionally, it would be helpful to see more visualizations of the Hessian matrices, such as heatmaps or scatter plots, to facilitate a deeper understanding of the results.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to address the following questions:
1. Can you provide more insight into the choice of initialization methods and their impact on the Hessian's spectrum?
2. How do you envision the proposed approach being applied to more complex models, such as convolutional neural networks or recurrent neural networks?
3. Can you elaborate on the potential connections between the Hessian's spectrum and the notion of "flatness" in deep learning landscapes, and how this might be explored in future work?