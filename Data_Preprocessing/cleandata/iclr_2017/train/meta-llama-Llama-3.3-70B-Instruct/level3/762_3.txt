Summary of the Paper's Contributions
The paper proposes a novel approach to adjusting for the variance introduced by dropout in neural networks. The authors derive a new weight initialization technique that corrects for the influence of dropout rates and an arbitrary nonlinearity's effect on neuron output variance. Additionally, they propose a simple method to re-estimate the Batch Normalization variance parameters after training, which leads to improved accuracy. The paper demonstrates the effectiveness of these methods on various architectures, including fully connected and convolutional neural networks, and achieves state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The paper tackles a specific and important problem in deep learning, namely the variance introduced by dropout, and proposes a well-motivated and effective solution.
2. The approach is well-placed in the literature, building upon existing weight initialization techniques and Batch Normalization, and provides a clear and concise derivation of the new initialization method.
Supporting Arguments
The paper provides a thorough analysis of the problem and proposes a simple yet effective solution. The experiments demonstrate the effectiveness of the new initialization technique and the Batch Normalization variance re-estimation method on various architectures and datasets. The paper also provides a clear and concise derivation of the new initialization method, making it easy to understand and implement.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more detailed comparisons with other state-of-the-art methods and techniques for addressing the variance introduced by dropout. Additionally, it would be interesting to see more experiments on larger datasets and more complex architectures.
I would like the authors to answer the following questions:
* How do the proposed methods perform on datasets with larger batch sizes or online learning tasks?
* Can the authors provide more insights into the relationship between the dropout rate and the corrective factor in the new initialization technique?
* How do the proposed methods compare to other techniques for addressing the variance introduced by dropout, such as dropout regularization or adaptive dropout rates?