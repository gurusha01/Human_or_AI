Summary of the Paper's Claims and Contributions
The paper proposes a technique to reduce the parameters of Recurrent Neural Networks (RNNs) by pruning weights during the initial training of the network. The authors claim that their method can achieve a sparsity of 90% with a small loss in accuracy, and that the pruned models can be deployed on mobile devices and server farms due to their small size and increased computational efficiency. The paper also presents experimental results showing that the pruned models can achieve better performance than dense models with fewer parameters.
Decision and Key Reasons
Based on the provided guidelines, I decide to Reject the paper. The two key reasons for this decision are:
1. Lack of Clarity and Missing Details: The paper lacks clarity and skips many details, such as the experiments that generated the results in Table 5, which limits its contribution. The authors do not provide sufficient information about the hyperparameters used in the experiments, the training procedures, and the evaluation metrics.
2. Unimpressive Experimental Results: The improved version of the AEC algorithm proposed in the paper has unimpressive experimental results, which is a drawback. The authors do not provide a thorough comparison with other state-of-the-art methods, and the results presented in the paper are not convincing enough to demonstrate the effectiveness of the proposed technique.
Supporting Arguments
The paper evaluates the robustness of defense methods, including RAD and AEC, and finds that RAD and distillation have the best performances, but are still vulnerable to certain attacks. However, the paper does not provide a clear explanation of how the proposed technique addresses these vulnerabilities. The authors also present interesting and inspiring experimental results, particularly in Section 3.3, but these results are not sufficient to overcome the limitations of the paper.
Additional Feedback and Questions
To improve the paper, the authors should provide more details about the experiments, including the hyperparameters used, the training procedures, and the evaluation metrics. The authors should also provide a more thorough comparison with other state-of-the-art methods and address the vulnerabilities of the proposed technique. Some questions that I would like the authors to answer include:
* How did the authors choose the hyperparameters for the experiments, and what is the sensitivity of the results to these hyperparameters?
* How do the authors plan to address the vulnerabilities of the proposed technique to certain attacks?
* Can the authors provide more details about the implementation of the proposed technique, including the code and the computational resources used?
* How do the authors plan to extend the proposed technique to other types of neural networks, such as convolutional neural networks?