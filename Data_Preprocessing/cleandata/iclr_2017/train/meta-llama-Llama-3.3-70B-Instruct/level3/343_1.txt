Summary
The paper proposes a framework for multitask deep reinforcement learning guided by policy sketches, which annotate each task with a sequence of named subtasks. The approach associates every subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating improved performance compared to standard baselines.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity and sufficient explanation, making it difficult for readers to understand and re-implement the approach. Secondly, the paper fails to adequately address key issues, including the validity of the proposed approach as a probabilistic model and the training process used to fit the model to data.
Supporting Arguments
The paper's lack of clarity and sufficient explanation is evident in the model description, which is incomplete and unclear in terms of probability computations and training methods. Additionally, the experiments are incomplete without samples drawn from the generative model or analysis of the learned model. The training process is not adequately explained, with insufficient discussion of how optimality is achieved, and the experiments do not demonstrate the model's ability to sample structured data, such as long-range syntax constraints.
Additional Feedback
To improve the paper, the authors should provide a more detailed and clear explanation of the approach, including the model description and training process. Additionally, the authors should provide more comprehensive experiments, including samples drawn from the generative model and analysis of the learned model. The authors should also address the key issues mentioned above, including the validity of the proposed approach as a probabilistic model and the training process used to fit the model to data.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence:
1. Can you provide a more detailed explanation of the model description, including the probability computations and training methods?
2. How do you ensure the validity of the proposed approach as a probabilistic model?
3. Can you provide more comprehensive experiments, including samples drawn from the generative model and analysis of the learned model?
4. How do you address the issue of overfitting in the training process, and what methods do you use to prevent it?