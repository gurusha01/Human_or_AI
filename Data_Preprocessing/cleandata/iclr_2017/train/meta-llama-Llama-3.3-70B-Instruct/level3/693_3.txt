Summary
The paper proposes a novel approach to predict learning curves of iterative machine learning methods using Bayesian neural networks. The authors develop a specialized neural network architecture with a learning curve layer that improves learning curve predictions. They evaluate their approach on several datasets and demonstrate its effectiveness in predicting asymptotic values of partially observed curves and unobserved learning curves. The authors also extend the Hyperband optimization algorithm using their model, which leads to faster convergence to good hyperparameter configurations.
Decision
I decide to accept this paper with minor revisions. The main reasons for this decision are: (1) the paper tackles a specific and important problem in machine learning, namely predicting learning curves, and (2) the proposed approach is well-motivated and demonstrates promising results on several datasets.
Supporting Arguments
The paper provides a clear and well-structured presentation of the problem, the proposed approach, and the experimental results. The authors demonstrate the effectiveness of their approach in predicting asymptotic values of partially observed curves and unobserved learning curves, which is a crucial aspect of hyperparameter optimization. The extension of the Hyperband algorithm using their model is also a significant contribution, as it leads to faster convergence to good hyperparameter configurations.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the implementation of the learning curve layer and the Bayesian neural network architecture. Additionally, it would be helpful to include more comparisons with other state-of-the-art methods for predicting learning curves. The authors may also consider providing more insights into the interpretability of the predicted learning curves and how they can be used in practice.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the choice of basis functions used in the learning curve layer?
2. How do you handle the case where the learning curve does not converge to a fixed asymptotic value?
3. Can you provide more insights into the computational cost of training the Bayesian neural network architecture and how it compares to other methods?