Summary
The paper proposes a novel approach to adjust for the variance introduced by dropout in neural networks. The authors suggest correcting for this variance in weight initialization and test-time batch normalization statistics, which leads to improved performance. They derive a new weight initialization technique that accounts for the influence of dropout rates and an arbitrary nonlinearity's effect on variance. Additionally, they propose re-estimating batch normalization variance parameters after training by feeding forward the training data with dropout deactivated.
Decision
I decide to reject this paper, with the main reason being the lack of experimental validation on whether correcting for dropout variance at test time is beneficial in networks without batch normalization. Another key reason is that the observation, although important, is not extremely novel, as it has been previously noted in the literature that simple dropout approximations at test time do not achieve the same accuracy as full Monte Carlo dropout.
Supporting Arguments
The paper's approach is well-motivated, and the authors provide a clear derivation of their weight initialization technique. However, the experimental validation is limited to networks with batch normalization, and it is unclear whether the proposed approach would be beneficial in other settings. Furthermore, the paper does not provide a comprehensive comparison to other methods, such as Monte Carlo dropout, which could provide a more robust evaluation of the proposed approach.
Additional Feedback
To improve the paper, the authors could provide additional experimental results on networks without batch normalization, as well as a more comprehensive comparison to other methods. Additionally, the authors could consider providing more insight into the theoretical implications of their approach and how it relates to existing work on dropout and batch normalization. It would also be helpful to see more detailed analysis of the results, including error bars and statistical significance tests.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the experimental setup and hyperparameter tuning process used in the paper?
2. How do the authors plan to address the lack of novelty in their observation, and what do they believe are the key contributions of their work?
3. Can you provide more insight into the theoretical implications of the proposed approach and how it relates to existing work on dropout and batch normalization?