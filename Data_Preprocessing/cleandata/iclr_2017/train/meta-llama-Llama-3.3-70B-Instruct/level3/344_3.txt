This paper presents a novel algorithm, ZO, that addresses the challenge of high-dimensional and variable action spaces in StarCraft micro-management tasks, a problem that has been difficult for recent DeepRL methods. The algorithm combines policy gradient, deep networks, and gradient-free optimization to provide better structured exploration and outperforms existing baselines. The paper is well-written and applies to a relevant problem, with the introduced tasks being interesting environments for structured state and action spaces in RL.
I decide to accept this paper with two key reasons: (1) the algorithm ZO is well-motivated and provides a significant improvement over existing baselines, and (2) the paper supports its claims with thorough experiments and analysis.
The approach is well-motivated, as it combines the strengths of policy gradient, deep networks, and gradient-free optimization to tackle the challenges of high-dimensional and variable action spaces. The use of hashing to discretize the state space is a clever idea, and the experiments demonstrate its effectiveness. The paper also provides a thorough analysis of the factors that contribute to good performance, such as the granularity of the hash function and the importance of encoding relevant information.
However, I do have some suggestions for improvement. Firstly, it would be helpful if the authors could share the source code or specifications for the tasks and algorithm, as this would facilitate comparisons and future work. Secondly, the paper lacks a baseline comparison with value-based approaches that model uncertainty for better exploration, such as Bootstrapped DQN. Finally, I found section 5 of the paper difficult to understand and suggest that the authors clarify and provide source code for the encoder to allow careful comparisons.
Additionally, I would like to see more discussion on action embedding models, such as energy-based approaches, and how they compare to the proposed algorithm. I also have some questions for the authors: (1) How does the algorithm perform in environments with very large action spaces? (2) Can the authors provide more insights into the trade-offs between exploration and exploitation in the algorithm? (3) How does the algorithm handle partial observability, and are there any plans to extend it to partially observable environments? 
Overall, this is a strong paper that presents a significant contribution to the field of RL, and with some revisions, it has the potential to be even more impactful.