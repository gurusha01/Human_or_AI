This paper proposes a novel attention mechanism for neural language models, which separates the output vectors into key, value, and predict parts. The authors demonstrate that this approach outperforms existing memory-augmented neural language models on two corpora, and that a much simpler model, the N-gram RNN, can achieve comparable results by using a concatenation of previous output representations. 
I decide to accept this paper, with the main reason being that it presents a well-motivated and well-placed approach in the literature, and provides thorough experimental analysis to support its claims. The paper clearly addresses the problem of neural language models struggling to capture long-range dependencies, and proposes a novel solution that is both effective and efficient.
The approach is well-motivated, as it builds upon existing work on attention mechanisms and memory-augmented neural networks. The authors provide a clear explanation of the limitations of existing approaches and how their proposed method addresses these limitations. The paper is also well-placed in the literature, as it references and compares to relevant existing work in the field.
The experimental analysis is thorough and well-conducted, with results presented on two different corpora. The authors demonstrate that their proposed approach outperforms existing models, and that the simpler N-gram RNN model can achieve comparable results. The results are also rigorously evaluated, with statistical significance tests and comparisons to state-of-the-art models.
To further improve the paper, I suggest that the authors add references to Ba et al., Reed & de Freitas, and Gulcehre et al. in the related work section, as their work is relevant to the proposed approach. Additionally, I would like the authors to clarify how they plan to address the issue of training neural language models to leverage long-range dependencies, as this is an important area of future work.
Some questions I would like the authors to answer include: How do the authors plan to encourage the model to attend over a longer history, and what specific techniques do they propose to use? How do the authors think their approach could be extended to other natural language processing tasks, such as machine translation or question answering? What are the potential limitations of the proposed approach, and how do the authors plan to address these limitations in future work?