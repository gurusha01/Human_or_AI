Summary of the Paper's Contributions
The paper proposes a novel approach to learning bilingual word vectors offline, leveraging the concept of orthogonal transformations to align vector spaces. The authors introduce a simple yet effective method using the Singular Value Decomposition (SVD) to obtain an orthogonal transformation, which outperforms existing methods. Additionally, they propose an "inverted softmax" to mitigate the hubness problem, further improving translation accuracy. The paper demonstrates the robustness of orthogonal transformations by achieving high precision using a pseudo-dictionary compiled without expert bilingual knowledge.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons: (1) the paper proposes a well-motivated and novel approach to offline bilingual word vector learning, and (2) the experimental results demonstrate significant improvements over existing methods, particularly in terms of robustness to low-quality dictionaries.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of offline bilingual word vector learning, highlighting the limitations of existing approaches. The proposed method, based on SVD and orthogonal transformations, is well-motivated and easy to understand. The experimental results are thorough and demonstrate the effectiveness of the proposed approach, including the use of pseudo-dictionaries and sentence vectors. The paper also provides a detailed analysis of the results, discussing the implications and potential applications of the proposed method.
Additional Feedback and Questions
To further improve the paper, I suggest the authors consider the following:
* Provide more insight into the choice of hyperparameters, particularly the inverse temperature Î², and how it affects the results.
* Discuss potential applications of the proposed method beyond bilingual word vector learning, such as multilingual or cross-lingual tasks.
* Consider comparing the proposed method to other state-of-the-art approaches, such as those using deep learning or attention mechanisms.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more details on the computational complexity of the proposed method, particularly in comparison to existing approaches like CCA?
* How do you plan to address the issue of word order and sentence length in the sentence vector approach, and what potential improvements can be made to the current implementation?