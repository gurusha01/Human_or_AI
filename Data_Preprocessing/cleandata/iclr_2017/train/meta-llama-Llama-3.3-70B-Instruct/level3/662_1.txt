This paper proposes a dynamic neural Turing machine (D-NTM) model that extends the original neural Turing machine (NTM) model by introducing learnable addressing and curriculum learning. The D-NTM model is evaluated on the Facebook bAbI task and shows improvement over the original NTM model. The paper also provides comprehensive comparisons of feed-forward controllers vs. recurrent controllers and demonstrates the effectiveness of discrete attention mechanisms.
I decide to Reject this paper for the following reasons:
1. The paper lacks clarity, particularly in Section 3, which is hard to follow. The writing needs improvement to effectively convey the authors' ideas.
2. The NTM baseline used in the paper is weak, with a reported 31% error rate compared to the 20% error rate reported in a previous study. This raises concerns about the validity of the comparisons made in the paper.
To improve the paper, I suggest the following:
* Clarify the writing, especially in Section 3, to make it easier to understand the authors' ideas.
* Strengthen the NTM baseline by using a more robust implementation or hyper-parameter tuning.
* Provide more detailed analysis of the results, including ablation studies to understand the contribution of each component of the D-NTM model.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more details on the implementation of the NTM baseline and how it was tuned?
* How did you choose the hyper-parameters for the D-NTM model, and what was the effect of different hyper-parameters on the results?
* Can you provide more analysis on the performance of the D-NTM model on different tasks and how it compares to other state-of-the-art models?