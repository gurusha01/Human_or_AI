Summary
The paper proposes a technique to convert dense to sparse networks for Recurrent Neural Networks (RNNs), reducing storage requirements and increasing inference rates. The authors achieve 90% sparsity without re-training and affecting the training phase, resulting in fewer parameters. The approach involves maintaining a set of masks, a monotonically increasing threshold, and a set of hyperparameters to determine the threshold. The technique is computationally efficient, easy to implement, and agnostic to the optimization algorithm.
Decision
I decide to accept this paper, with the primary reason being the significant reduction in model size and increase in computational efficiency achieved by the proposed pruning technique. The secondary reason is the thorough experimentation and evaluation of the approach on various RNN models, including bidirectional RNNs and Gated Recurrent Units (GRUs).
Supporting Arguments
The paper provides a clear motivation for reducing the size of RNN models, citing the challenges of deploying large models on mobile devices and embedded systems. The authors also provide a comprehensive review of related work, highlighting the limitations of existing approaches and the advantages of their proposed technique. The experimental results demonstrate the effectiveness of the approach in reducing model size and improving computational efficiency, with significant speedups achieved on various hardware platforms.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the choice of hyperparameters and their impact on the pruning process. A formula to predict sparsity, parameters, and accuracy given a set of hyperparameters would be helpful. Additionally, more results on GRU sparse big models would be beneficial to demonstrate the scalability of the approach. The authors should also clarify the inconsistencies in sparsity values between tables and text and provide more information on the models used in different tables. Finally, the claim of a 3-4x increase in training time compared to Han et al. (2015) should be revisited, as both approaches use weight pruning techniques and should have similar training times.
Questions for the Authors
1. Can you provide more details on the choice of hyperparameters and their impact on the pruning process?
2. How do you plan to generalize this technique to language modeling tasks and reduce the size of embedding layers?
3. Can you compare the sparsity generated by your pruning technique to that obtained by L1 regularization?
4. How do you plan to implement optimal small batch sparse matrix-dense vector routines for GPUs and ARM processors to facilitate deployment?