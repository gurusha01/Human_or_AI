This paper introduces the concept of "sample importance" to measure the influence of a training example on a deep neural network. The authors propose a metric to quantify this importance and conduct extensive experiments on two standard datasets to analyze its behavior. The paper claims to provide insights into how easy and hard samples impact the training of different layers in a neural network.
I decide to reject this paper, primarily due to two key reasons. Firstly, the validity of the "sample importance" metric as a measure of a sample's influence on the network is questionable. The metric's dependence on the learning rate and the use of the L2 norm may not accurately capture the sensitivity of the classifier to the input. Secondly, the experimental analysis is flawed, with results that are not entirely convincing and may be specific to certain model architectures.
The paper's approach is not well-motivated, and the experimental design has several limitations. The use of a simple 3-layer ReLU network and two standard datasets may not be representative of more complex deep learning models and real-world scenarios. The results may not generalize to other architectures, such as convolutional neural networks or recurrent neural networks. Furthermore, the paper's conclusion that constructing batches with mixed "easiness" is beneficial for training contradicts some existing results on curriculum learning, which suggests that ordering training examples by difficulty can lead to better performance.
To improve the paper, I suggest that the authors revisit the definition of sample importance and explore alternative metrics that are more robust and less dependent on hyperparameters. Additionally, the authors should conduct more extensive experiments on various model architectures and datasets to demonstrate the generality of their findings. It would also be helpful to provide a more detailed analysis of the results, including a discussion of the limitations and potential biases of the experimental design.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How do the authors plan to address the limitations of the sample importance metric, and what alternative metrics have they considered? (2) Can the authors provide more insight into the experimental design and the choice of model architectures and datasets? (3) How do the authors reconcile their findings with existing results on curriculum learning, and what implications do their results have for the design of training protocols?