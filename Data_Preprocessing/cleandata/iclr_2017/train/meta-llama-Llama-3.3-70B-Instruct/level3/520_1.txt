This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning (RL) to refine a pre-trained Recurrent Neural Network (RNN) for sequence generation tasks. The approach uses the pre-trained RNN to supply part of the reward value in an RL model, allowing the model to learn from both data and heuristic rewards. The authors demonstrate the effectiveness of RL Tuner in music generation tasks, showing that it can produce more pleasing and structured melodies compared to a baseline RNN model.
I decide to accept this paper with two key reasons: (1) the paper tackles a specific and interesting problem in sequence generation, and (2) the approach is well-motivated and supported by experiments. The authors provide a clear and detailed explanation of the RL Tuner framework, and the experimental results demonstrate its effectiveness in improving the quality of generated melodies.
The paper supports its claims through a combination of theoretical analysis and empirical experiments. The authors provide a detailed derivation of the RL Tuner framework, and the experimental results show that the approach can correct unwanted behaviors of the RNN model, such as excessive repetition, while maintaining information learned from data. The user study results also demonstrate that the melodies generated by the RL Tuner models are preferred by human listeners over those generated by the baseline RNN model.
To improve the paper, I suggest that the authors provide more visualizations and quantitative comparisons to fully evaluate the results. Additionally, the authors could discuss the pros and cons of using RL Tuner compared to other approaches, such as conditional GANs, to provide a clearer comparison between the two models. Analysis of training and generation time would also be helpful to understand the limitations of the RL Tuner method, particularly in terms of resolution.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the RL Tuner framework to more complex sequence generation tasks, such as polyphonic music generation or text generation? (2) Can the authors provide more details on the music-theory-based reward function used in the experiments, and how it was designed to encourage more pleasing and structured melodies? (3) How do the authors plan to address the potential issue of mode collapse in the RL Tuner framework, where the model may converge to a limited set of solutions?