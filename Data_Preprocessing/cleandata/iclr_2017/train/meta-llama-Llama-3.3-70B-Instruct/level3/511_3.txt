Summary
The paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses to learn the embeddings. The approach is evaluated on three tasks: acoustic word discrimination, cross-view word discrimination, and word similarity. The results show that the proposed approach outperforms previous methods on acoustic word discrimination and achieves promising results on cross-view word discrimination and word similarity.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-defined problem in the area of speech processing, and the approach is well-motivated and grounded in the literature. Secondly, the paper provides a thorough evaluation of the proposed approach, including a comparison with previous methods and an analysis of the results.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of learning acoustic word embeddings, and the proposed approach is well-motivated by the need to capture the similarity between words in both acoustic and orthographic spaces. The use of multi-view contrastive losses is a novel and interesting approach, and the authors provide a thorough evaluation of the proposed approach, including a comparison with previous methods. The results show that the proposed approach outperforms previous methods on acoustic word discrimination and achieves promising results on cross-view word discrimination and word similarity.
Additional Feedback
To improve the paper, I would suggest providing more details on the experimental setup, including the specific hyperparameters used and the computational resources required. Additionally, it would be interesting to see a more detailed analysis of the results, including a discussion of the limitations of the proposed approach and potential avenues for future work. Some questions I would like the authors to answer include: How do the proposed embeddings perform on out-of-vocabulary words? Can the proposed approach be extended to other languages or domains? How do the proposed embeddings compare to other types of word embeddings, such as semantic word embeddings?
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: Can you provide more details on the computational resources required to train the proposed models? How do you plan to address the issue of out-of-vocabulary words in the proposed approach? Can you provide more insights into the choice of hyperparameters and the sensitivity of the proposed approach to these hyperparameters?