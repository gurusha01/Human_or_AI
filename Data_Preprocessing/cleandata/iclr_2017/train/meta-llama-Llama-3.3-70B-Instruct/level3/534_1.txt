Summary of the Paper
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors introduce a new learning rate schedule, called SGDR, which simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster. The paper also demonstrates the effectiveness of SGDR on a dataset of EEG recordings and a downsampled version of the ImageNet dataset.
Decision
I decide to accept this paper with minor revisions. The paper provides a clear and well-motivated approach to improving the performance of SGD, and the experimental results demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper tackles a specific question of improving the performance of SGD, which is a widely used optimization algorithm in deep learning. The approach is well-motivated, and the authors provide a clear explanation of the proposed method and its advantages. The experimental results are thorough and demonstrate the effectiveness of SGDR on several datasets. The paper also provides a good discussion of the related work and the potential applications of the proposed method.
Additional Feedback
To improve the paper, I suggest the authors provide more details on the hyperparameter tuning process and the selection of the initial learning rate and the total number of epochs. Additionally, the authors could provide more insights into the effect of the restart mechanism on the convergence of the algorithm. It would also be helpful to include more comparisons with other optimization algorithms, such as AdaDelta and Adam.
Questions for the Authors
I would like the authors to clarify the following points:
1. How did the authors select the initial learning rate and the total number of epochs for the SGDR algorithm?
2. Can the authors provide more insights into the effect of the restart mechanism on the convergence of the algorithm?
3. How does the SGDR algorithm compare to other optimization algorithms, such as AdaDelta and Adam, in terms of performance and computational efficiency?