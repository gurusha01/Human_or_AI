This paper presents Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs) that exploits the variable per-layer precision requirements of DNNs to deliver execution time proportional to the precision used. The authors claim that TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90× without any loss in accuracy and is 1.17× more energy efficient.
The paper tackles the specific question of how to design a hardware accelerator that can efficiently execute DNNs with variable precision requirements. The approach is well-motivated, as it builds upon the observation that DNNs exhibit varying precision requirements across and within layers. The authors provide a thorough review of related work and position their contribution within the context of existing research.
However, I have some concerns regarding the methodology used to estimate energy numbers for TRT. The claimed 17% improvement in energy efficiency may be within the margin of error, and I would like to see more details on the methodology used to arrive at this estimate. Additionally, the novel bit-serial compute unit in TRT incurs a severe area overhead of nearly 3x over DaDianNao's compute units, which may be a significant drawback.
While the idea of bit-serial computation is interesting, I wonder if it may be better suited for a circuit design or architecture-focused venue rather than the current conference. The paper provides a thorough evaluation of TRT's performance, energy, and area, but I would like to see more discussion on the trade-offs involved in using bit-serial computation.
To improve the paper, I would suggest providing more details on the methodology used to estimate energy numbers and addressing the area overhead of the bit-serial compute unit. Additionally, the authors could explore ways to reduce the area overhead and provide more discussion on the potential applications and limitations of bit-serial computation in DNN accelerators.
Based on these concerns, I would reject the paper in its current form. However, with revisions to address these concerns, I believe the paper could be a strong contribution to the field.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on the methodology used to estimate energy numbers for TRT?
* How do you plan to address the area overhead of the bit-serial compute unit?
* What are the potential applications and limitations of bit-serial computation in DNN accelerators?
* How does TRT compare to other DNN accelerators in terms of performance, energy efficiency, and area overhead?