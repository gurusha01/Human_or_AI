Summary
The paper proposes a novel approach to reduce the cost of communication in parallel training of neural networks using a Linear Pipeline (LP) based collective design. The authors claim that their approach achieves significant speedups over existing methods, such as Minimum Spanning Tree (MST) and Bidirectional Exchange (BE), while preserving the convergence properties of Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD). The paper presents a theoretical analysis and experimental results to support their claims.
Decision
I decide to reject this paper, primarily due to two reasons. Firstly, the approach lacks an ablation study to validate the design choices, making it difficult to understand the contribution of each component. Secondly, the performance comparison is weak, only conducted against SegNet, and the proposed model does not achieve state-of-the-art results on public benchmarks.
Supporting Arguments
The paper's approach uses an encoder-decoder architecture with existing techniques, but the components used are not new to the community, and the design choices seem ad-hoc. The speed-up improvement is reasonable but comes with a significant sacrifice in performance on some benchmarks, making the approach less promising. Additionally, the model size of 0.7MB is impressive, but there is no analysis on the trade-off between model size and performance, and no report on memory consumption for the inference stage.
Additional Feedback
To improve the paper, the authors should consider conducting a thorough ablation study to validate their design choices and provide a more comprehensive performance comparison against state-of-the-art methods. They should also analyze the trade-off between model size and performance and report on memory consumption for the inference stage. Furthermore, the authors should provide more details on the implementation of their approach and the experimental setup to facilitate reproducibility.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on the ablation study you conducted to validate your design choices?
2. How do you plan to address the performance sacrifice on some benchmarks?
3. Can you provide more information on the implementation of your approach and the experimental setup?
4. How do you think your approach can be improved to achieve state-of-the-art results on public benchmarks?