Summary of the Paper's Contributions
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) for regularizing deep neural networks and achieving better optimization performance. The DSD training flow consists of three steps: dense, sparse, and re-dense. The method demonstrates consistent improvements in accuracy across multiple architectures, tasks, and datasets, including image classification, caption generation, and speech recognition. The algorithm's simplicity and the fact that it only incurs one extra hyper-parameter make it a practical solution.
Decision and Reasons
Based on the evaluation, I decide to Accept this paper with minor revisions. The two key reasons for this decision are:
1. The paper tackles a significant problem in deep learning, namely, the difficulty of training non-convex deep neural networks, and provides a well-motivated and effective solution.
2. The empirical evaluations demonstrate the efficacy of the DSD training strategy, showing consistent improvements in accuracy across various tasks and datasets.
Supporting Arguments
The paper provides a clear and well-structured presentation of the DSD training strategy, including a detailed description of the algorithm and its components. The empirical evaluations are thorough and convincing, demonstrating the effectiveness of the method in improving the accuracy of various deep neural networks. The paper also provides a good discussion of the related work and the potential benefits and limitations of the proposed approach.
Additional Feedback and Questions
To further improve the paper, I suggest the authors provide additional experiments and discussions, particularly accuracy comparisons with the same number of epochs. Some questions I would like the authors to address include:
* How does the choice of sparsity ratio affect the performance of the DSD training strategy?
* Can the authors provide more insights into the optimization process and how the DSD training strategy helps to escape saddle points and achieve better local minima?
* How does the DSD training strategy compare to other regularization techniques, such as dropout and weight decay, in terms of performance and computational efficiency?
Overall, the paper presents a significant contribution to the field of deep learning, and with minor revisions, it has the potential to be a strong and impactful publication.