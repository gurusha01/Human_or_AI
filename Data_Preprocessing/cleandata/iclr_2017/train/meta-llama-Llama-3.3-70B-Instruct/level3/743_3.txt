Summary
The paper proposes a variational approximation to the information bottleneck (IB) method, called Deep Variational Information Bottleneck (Deep VIB), which allows for efficient training of deep neural networks using the IB objective. The authors demonstrate the effectiveness of Deep VIB in improving the generalization performance and robustness to adversarial attacks of neural networks on several datasets, including MNIST and ImageNet.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's empirical study on universality is unconvincing due to the presentation of only one algorithm for the deep learning example, lacking comparisons with other algorithms. Secondly, the concept of universality and its definition seem to have been previously covered in Deift (2014), and the paper lacks sufficient comparisons of different algorithms to make the concept more plausible.
Supporting Arguments
The paper's focus on the distribution of running times may not be directly useful for algorithm tuning, as practitioners may care more about the mean and variance of running times, which are not clearly highlighted. Additionally, the paper's lack of comparison with other algorithms and its reliance on a single example make it difficult to assess the generality and effectiveness of the proposed method.
Additional Feedback
To improve the paper, the authors could provide more comparisons with other algorithms and techniques, such as variational autoencoders and confidence penalty methods. They could also explore the connections to differential privacy and open universe classification problems, as mentioned in the paper. Furthermore, the authors could provide more details on the hyperparameter settings and training procedures used in the experiments, to facilitate reproducibility and comparison with other methods.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How do the authors plan to address the lack of comparison with other algorithms and techniques in the field?
2. Can the authors provide more details on the hyperparameter settings and training procedures used in the experiments?
3. How do the authors think the proposed method can be applied to other domains and problems, such as sequence prediction and natural language processing?
4. Can the authors provide more insights into the connections between the proposed method and other techniques, such as variational autoencoders and confidence penalty methods?