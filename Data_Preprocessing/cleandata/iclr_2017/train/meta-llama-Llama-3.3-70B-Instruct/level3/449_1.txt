This paper proposes a novel neural network architecture called FractalNet, which is based on self-similarity and fractal geometry. The authors claim that FractalNet can match the performance of state-of-the-art residual networks on various image classification tasks, without relying on residual connections. The paper also introduces a new regularization technique called drop-path, which is designed to prevent co-adaptation of parallel paths in the fractal network.
The paper attempts to address the question of whether residual connections are fundamental to the success of deep neural networks. The authors argue that the key to training ultra-deep networks is not the residual formulation itself, but rather the ability to transition from effectively shallow to deep during training. They demonstrate that FractalNet can achieve competitive results on CIFAR and ImageNet datasets, and that the fractal structure can be used as a learning framework to extract high-performance subnetworks.
However, I have several concerns with the paper. Firstly, the authors' claims about the fractal network architecture are open to debate and require strong experimental evidence to support their intuitive claims. The comparison to state-of-the-art residual networks is lacking, and the paper does not provide comprehensive results to demonstrate the superiority of FractalNet. Additionally, the scalability of FractalNet to "ultra" deep networks is not demonstrated, with no results reported for dozens of layers.
Furthermore, the number of parameters in the proposed architecture is significantly greater than state-of-the-art ResNet variants, which raises concerns about the practicality of the model. The authors' rebuttal lacks satisfactory answers and uses manipulative arguments about narrow reviewing and baseline comparison. The paper's claims about the simplifying power of fractal networks are not convincing, given the harder training procedure and large number of parameters required.
My decision is to reject this paper, primarily due to the lack of empirical support for its claims and the restrictive nature of the model in practice. The main bottleneck of the paper is the high number of parameters required for FractalNet, making it hard to scale to "ultra-deep" networks. The comparison to DenseNet is also lacking, despite its state-of-the-art results and significant outperformance of FractalNet.
To improve the paper, I would suggest that the authors provide more comprehensive experimental results, including comparisons to other state-of-the-art architectures and ablation studies to demonstrate the effectiveness of the fractal structure and drop-path regularization. Additionally, the authors should address the concerns about the number of parameters and the scalability of the model.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to address the issue of high parameter count in FractalNet, and what techniques can be used to reduce the number of parameters while maintaining performance? (2) Can the authors provide more detailed analysis of the training dynamics of FractalNet, including the evolution of the loss landscape and the behavior of the drop-path regularization technique? (3) How do the authors plan to extend the fractal network architecture to other domains and tasks, and what potential applications do they envision for this architecture?