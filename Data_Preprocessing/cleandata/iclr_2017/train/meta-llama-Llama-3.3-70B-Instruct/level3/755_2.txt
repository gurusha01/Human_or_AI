Summary
The paper presents a comprehensive analysis of vocabulary selection techniques for neural machine translation, exploring the trade-off between speed and accuracy for different vocabulary sizes. The authors extend previous work by considering a wide range of simple and complex selection techniques, including bilingual word co-occurrence counts, bilingual embeddings, word alignments, phrase pairs, and discriminative SVM classifiers. The experiments demonstrate that decoding speed can be reduced by up to 90% without compromising accuracy, and training speed can be increased by up to 25% with a bi-directional LSTM encoder.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated question, namely, the optimization issue of linear ResNet and its implications for training deep linear networks. Secondly, the approach is well-placed in the literature, building upon previous work on vocabulary selection techniques for neural machine translation.
Supporting Arguments
The paper provides a thorough analysis of the trade-off between speed and accuracy for different vocabulary sizes, and the results are well-supported by experiments on two language pairs and several test sets. The authors also provide a detailed comparison of different selection techniques, including word alignments, phrase pairs, and SVMs, and demonstrate that word alignments can achieve high accuracy even with fewer than 1,000 word types per sentence.
Additional Feedback
To improve the paper, I suggest that the authors provide more discussion on the implications of their results for the design of neural machine translation systems. Additionally, it would be helpful to include more analysis on the computational complexity of the different selection techniques and their impact on training and decoding times. Furthermore, the authors could consider exploring the use of other selection techniques, such as those based on attention mechanisms or graph-based methods.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the results of this paper relate to the use of ReLU units in neural networks, and what implications do the findings have for the design of deep linear networks?
* Can the authors provide more insight into the computational complexity of the different selection techniques and their impact on training and decoding times?
* How do the authors plan to extend this work to other areas of natural language processing, such as language modeling or text classification?