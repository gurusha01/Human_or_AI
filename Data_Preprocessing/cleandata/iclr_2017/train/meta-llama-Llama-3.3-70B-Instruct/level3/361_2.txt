Summary of the Paper's Contributions
The paper proposes using Bayesian neural networks to model learning curves for hyper-parameter optimization, allowing for early termination of bad runs to save time. The authors develop a specialized neural network architecture with a learning curve layer that improves learning curve predictions and evaluate its performance on several datasets. The results show that the proposed approach outperforms other methods, including random forests and Gaussian processes, in predicting asymptotic values of partially observed learning curves and unobserved learning curves.
Decision and Key Reasons
I recommend accepting the paper due to its solid effort on an interesting problem and potential practical contribution, particularly if the code is released. The paper addresses a real need for early termination in hyperparameter optimization, and the experiments seem thorough. However, the results are underwhelming, and I would like to see bigger speedups and more practical evaluations, such as the impact on hyperparameter optimization.
Supporting Arguments
The paper's main strength is its addressing of a real need for early termination in hyperparameter optimization, with high demand for a working system. The authors provide a thorough evaluation of their approach on several datasets, including CNN, FCNet, LR, and VAE. The results show that the proposed approach outperforms other methods, including random forests and Gaussian processes, in predicting asymptotic values of partially observed learning curves and unobserved learning curves. However, the speedups achieved by the proposed approach are not significant enough to demonstrate its practical impact.
Additional Feedback
To improve the paper, I suggest adding more figures to provide more intuition about the method's effectiveness. For example, a histogram of termination times and a plot of iterations needed to reach a fixed objective function value would be helpful. Additionally, the authors could provide more details on the computational resources required to run the experiments and the optimization overhead of the proposed approach. It would also be interesting to see a comparison with other Bayesian optimization methods, such as Bayesian optimization with Bayesian neural networks (BOHAMIANN).
Questions for the Authors
1. Can you provide more details on the computational resources required to run the experiments and the optimization overhead of the proposed approach?
2. How do you plan to release the code and make it accessible to the community?
3. Can you provide more insights into the hyperparameter tuning process and how the proposed approach can be used in practice?
4. How do you think the proposed approach can be extended to other optimization problems, such as reinforcement learning or multi-objective optimization?