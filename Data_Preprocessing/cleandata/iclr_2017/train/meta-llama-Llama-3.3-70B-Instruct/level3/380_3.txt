Summary
The paper proposes a new regularization method, Central Moment Discrepancy (CMD), for domain-invariant representation learning in the context of unsupervised domain adaptation. The authors demonstrate that CMD is a metric on the set of probability distributions on a compact interval and that convergence in CMD implies convergence in distribution. The experimental results show that CMD outperforms state-of-the-art methods, including Maximum Mean Discrepancy (MMD) and Domain Adversarial Neural Networks (DANN), on benchmark datasets such as Office and Amazon reviews.
Decision
I decide to Accept this paper with minor revisions. The main reason for this decision is that the paper presents a well-motivated and well-executed approach to domain-invariant representation learning, with a clear theoretical foundation and convincing experimental results. The authors provide a thorough analysis of the proposed method and demonstrate its effectiveness in various settings.
Supporting Arguments
The paper tackles a specific and important problem in the field of domain adaptation, and the proposed approach is well-placed in the literature. The authors provide a clear and concise explanation of the method, including the theoretical foundations and the experimental setup. The results are convincing, with CMD outperforming state-of-the-art methods on several benchmark datasets. Additionally, the authors provide a thorough analysis of the sensitivity of the method to parameter changes, which demonstrates the robustness of the approach.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the relationship between CMD and other distribution matching methods, such as MMD and KL-divergence. Additionally, it would be interesting to see more experiments on the application of CMD to other domains, such as computer vision and natural language processing. Finally, the authors may want to consider providing more details on the computational complexity of the proposed method and its scalability to large datasets.
Questions for the Authors
1. Can you provide more insights into the choice of the parameter K, which determines the number of central moments to match? How does this parameter affect the performance of the method?
2. How does CMD compare to other distribution matching methods, such as MMD and KL-divergence, in terms of computational complexity and scalability?
3. Can you provide more experiments on the application of CMD to other domains, such as computer vision and natural language processing? How does the method perform in these settings?