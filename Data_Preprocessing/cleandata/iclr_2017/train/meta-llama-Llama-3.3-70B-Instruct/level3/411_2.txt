Summary of the Paper's Contributions
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) training, which regularizes deep neural networks by pruning and then restoring connections. The approach consists of three steps: initial dense training, sparse training with pruning, and final dense training with re-densification. The authors demonstrate the effectiveness of DSD training on various neural network architectures, including CNNs, RNNs, and LSTMs, and achieve significant performance gains on tasks such as image classification, caption generation, and speech recognition.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons:
1. The paper presents a well-motivated and novel approach to regularizing deep neural networks, which addresses the common problem of overfitting and improves optimization performance.
2. The authors provide extensive experimental results on various datasets and architectures, demonstrating the effectiveness and consistency of the DSD training approach.
Supporting Arguments
The paper is well-written, and the authors provide a clear explanation of the DSD training approach and its underlying motivations. The experimental results are thorough and convincing, showing significant performance gains on various tasks and datasets. The authors also provide a detailed analysis of the results, discussing the potential reasons for the effectiveness of DSD training, such as escaping saddle points and achieving better minima.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors consider the following:
* Providing more insights into the hyperparameter tuning process, particularly for the sparsity ratio and learning rate.
* Investigating the applicability of DSD training to other domains and tasks, such as natural language processing and reinforcement learning.
* Comparing DSD training with other regularization techniques, such as dropout and weight decay, to better understand its strengths and weaknesses.
I would like the authors to answer the following questions:
* Can you provide more details on the computational cost and training time of DSD training compared to conventional training methods?
* How do you choose the sparsity ratio and learning rate for each dataset and architecture, and are there any general guidelines for selecting these hyperparameters?
* Have you explored the use of DSD training in conjunction with other regularization techniques, such as dropout and weight decay, and if so, what are the results?