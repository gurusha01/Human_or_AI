Summary
The paper proposes a novel approach to Canonical Correlation Analysis (CCA) by formulating it as a fully differentiable neural network layer, enabling backpropagation through the computation of CCA. This allows CCA to be used as a building block within multi-modality neural networks, producing maximally-correlated projections of its inputs. The authors demonstrate the effectiveness of this approach in cross-modality retrieval experiments on two public image-to-text datasets, surpassing both Deep CCA and a multi-view network with freely-learned projections.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks thorough experiments on various neural network architectures beyond CNNs, which limits the generalizability of the proposed approach. Secondly, the experimental sections for MNIST and CIFAR lack architectural details and omit mini-batch size information, making it difficult to draw conclusive insights about the proposed method.
Supporting Arguments
The paper's organization is clear, and the authors provide a thorough review of classic and deep CCA. However, the lack of experimentation on different neural network architectures and the omission of important experimental details, such as mini-batch size, limit the paper's contribution. Additionally, the paper would benefit from comparisons with baseline optimizers like Adam and clearer explanations of hyper-parameter choices for the baseline SGD method.
Additional Feedback
To improve the paper, I suggest that the authors conduct more extensive experiments on various neural network architectures and provide detailed information about the experimental setup, including mini-batch size and hyper-parameter choices. Additionally, the authors should consider comparing their approach with other state-of-the-art methods and providing more insights into the learned representations.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details about the neural network architectures used in the experiments, including the number of layers, units, and activation functions?
2. How did you choose the hyper-parameters for the baseline SGD method, and what was the effect of different hyper-parameter choices on the results?
3. Can you provide more insights into the learned representations, such as visualizations or analyses of the correlation coefficients, to better understand the benefits of the proposed approach?