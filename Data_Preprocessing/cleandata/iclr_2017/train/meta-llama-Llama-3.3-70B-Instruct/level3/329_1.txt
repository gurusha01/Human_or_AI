Summary of the Paper's Contributions
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN architecture uses a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation to update the hidden state, ensuring that the forward hidden state activation norm and backwards gradient norm are preserved. The authors also introduce a rotation plane parameterization to represent the orthogonal matrices, which allows for efficient computation and optimization. The paper demonstrates the effectiveness of the DORNN architecture on a simplified memory copy task, achieving long-term dependencies of up to 5,000 time steps.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Accept the paper. The two key reasons for this decision are:
1. Effective solution to the vanishing and exploding gradients problem: The DORNN architecture provides a well-motivated and effective solution to the vanishing and exploding gradients problem, which is a fundamental challenge in training RNNs.
2. Strong empirical results: The paper presents strong empirical results on a simplified memory copy task, demonstrating the ability of the DORNN architecture to learn long-term dependencies of up to 5,000 time steps.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of vanishing and exploding gradients in RNNs and motivates the need for a new architecture. The DORNN architecture is well-designed, and the use of orthogonal transformations ensures that the forward and backward passes are norm-preserving, which is essential for stable training. The rotation plane parameterization is also a key contribution, as it allows for efficient computation and optimization of the orthogonal matrices.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more discussion on the relationship between the DORNN architecture and other orthogonal RNN architectures, such as the Unitary Evolution Recurrent Neural Network (uRNN). Additionally, it would be interesting to see more experimental results on more complex tasks, such as language modeling or image captioning.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How does the DORNN architecture compare to other orthogonal RNN architectures in terms of computational efficiency and optimization difficulty?
* Can the DORNN architecture be extended to other types of neural networks, such as feedforward networks or convolutional neural networks?
* How does the choice of rotation planes affect the performance of the DORNN architecture, and are there any methods to optimize the rotation planes jointly with the angle parameters?