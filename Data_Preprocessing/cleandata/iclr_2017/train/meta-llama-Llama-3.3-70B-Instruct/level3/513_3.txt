Summary of the Paper
The paper proposes a novel approach to understanding the Hessian of the loss function in deep learning, which has significant implications for the theoretical and practical aspects of training neural networks. The authors provide empirical evidence that the Hessian is degenerate, with a bulk of eigenvalues concentrated around zero and a discrete set of eigenvalues that depend on the input data. This two-phased non-degeneracy is a desirable property that can be leveraged to improve training methods.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a clear comparison with alternative approaches to understanding the Hessian of the loss function, which makes it difficult to assess the significance of the proposed method. Secondly, the experimental section is too preliminary and does not provide sufficient evidence to support the claims made in the paper.
Supporting Arguments
The paper proposes a predictive model to improve a supervised classifier, but the approach seems preliminary and lacks comparison with alternative semi-supervised strategies. The discussion on related work is insufficient, missing previous work on learning unsupervised features from video for classification tasks. The chosen end task may not be the best to advocate for unsupervised training, as the label can be inferred trivially from the unsupervised data.
Additional Feedback
To improve the paper, I suggest that the authors provide a more comprehensive discussion of related work, including previous studies on learning unsupervised features from video for classification tasks. The experimental section should be expanded to include more tasks and datasets, and the results should be compared with alternative approaches to understanding the Hessian of the loss function. Additionally, the authors should consider reporting test results on the dataset from Lerrer et al and exploring tasks where the label cannot be easily inferred from the unsupervised data.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions: (1) How does the proposed approach compare with alternative methods for understanding the Hessian of the loss function? (2) Can the authors provide more evidence to support the claim that the two-phased non-degeneracy of the Hessian is a desirable property? (3) How do the authors plan to address the issue of the Hessian being degenerate, and what implications does this have for the theoretical and practical aspects of training neural networks?