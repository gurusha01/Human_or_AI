Summary
The paper proposes a novel attention-based framework for sentiment analysis, inspired by human reading behavior. The model uses a bidirectional LSTM (Bi-LSTM) network to extract a global context representation, which is then used as attention to selectively focus on important local contexts. The authors also propose a simplified single-scan approach, which achieves comparable results. The model is evaluated on several benchmark datasets, demonstrating its effectiveness in sentiment analysis.
Decision
I decide to reject this paper, with two key reasons: (1) the paper lacks a thorough comparison with existing attention-based models, and (2) the experimental results are not rigorously evaluated, with potential issues in hyperparameter tuning and regularization techniques.
Supporting Arguments
The paper's idea of using a global context representation as attention is intriguing, but it is not clear how it differs from existing attention-based models. The authors should provide a more detailed comparison with other models, such as the Neural Attention Model (NAM), to demonstrate the superiority of their approach. Additionally, the experimental results are not convincing, as the authors use fixed hyperparameters and regularization techniques that are not present in the theoretical arguments. The results on the CIFAR dataset show small improvements over the baseline, but the standard deviations are not reported, making it difficult to determine the significance of the results.
Additional Feedback
To improve the paper, the authors should provide a more thorough analysis of the attention mechanism, including visualizations of the attention weights and a more detailed discussion of the results. The authors should also consider using more rigorous evaluation metrics, such as cross-validation, to demonstrate the robustness of their model. Furthermore, the authors should provide more details on the hyperparameter tuning process and the regularization techniques used, to ensure that the results are reproducible.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How does the global context representation differ from existing attention-based models, and what are the advantages of using a Bi-LSTM network to extract this representation? (2) Can the authors provide more details on the hyperparameter tuning process and the regularization techniques used, and how they affect the results? (3) How do the authors plan to address the potential issues with the attention mechanism, such as the attention weights becoming too focused on a single word or phrase?