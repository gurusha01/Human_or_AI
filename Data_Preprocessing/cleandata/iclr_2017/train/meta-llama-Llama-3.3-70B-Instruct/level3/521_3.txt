The paper proposes a dynamic neural Turing machine (D-NTM) that extends the traditional neural Turing machine (NTM) by introducing a learnable addressing scheme. This allows the D-NTM to perform highly nonlinear location-based addressing, making it more flexible and powerful than the original NTM. The authors demonstrate the effectiveness of the D-NTM on various tasks, including episodic question-answering, sequential MNIST, and algorithmic tasks such as copy and associative recall.
The specific question tackled by the paper is how to improve the addressing mechanism of the NTM to make it more efficient and effective in various tasks. The approach is well-motivated, as the authors identify the limitations of the original NTM and propose a solution that addresses these limitations. The paper is also well-placed in the literature, as it builds upon the existing work on NTMs and memory-augmented neural networks.
However, I decide to reject this paper for two main reasons. Firstly, the paper uses undefined terms such as AM-FM and MAP, which can make it difficult to follow in some places. Secondly, the empirical evidence provided is limited to a narrow domain of tasks, with relatively small accuracy gains. While the results are promising, they do not provide sufficient evidence to support the claims made by the authors.
To improve the paper, I would suggest that the authors provide more detailed explanations of the terminology used and clarify any ambiguities. Additionally, the authors should provide more extensive empirical evidence to demonstrate the effectiveness of the D-NTM on a wider range of tasks, including speech tasks. This would help to strengthen the claims made by the authors and provide more convincing evidence for the benefits of the proposed approach.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the learnable addressing scheme of the D-NTM differ from other addressing mechanisms, such as content-based addressing? What are the advantages and disadvantages of using a discrete attention mechanism compared to a continuous attention mechanism? How do the authors plan to extend the D-NTM to more complex tasks, such as text summarization and visual question-answering? 
Overall, while the paper proposes an interesting and promising approach, it requires more clarification, explanation, and empirical evidence to support its claims. With additional work to address these limitations, the paper has the potential to make a significant contribution to the field of neural networks and artificial intelligence.