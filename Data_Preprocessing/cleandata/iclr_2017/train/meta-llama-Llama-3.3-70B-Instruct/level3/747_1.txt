Summary
The paper proposes a novel approach to analyzing feature importance in deep neural networks, called interior gradients. This method calculates the gradient of a scaled version of the input to determine feature importance. The authors compare their approach to standard gradients using visualizations on real images, demonstrating that integrated gradients correspond to an intuitive notion of feature importance.
Decision
I decide to reject this paper, with the primary reason being the lack of qualitative and quantitative comparison to prior work, such as DeepLift and layer-wise relevance propagation. The absence of comparison to other methods that attack the same problem of feature importance makes the paper unfit for publication without such comparisons.
Supporting Arguments
The paper's approach, while novel, is not sufficiently motivated by a thorough review of existing literature on feature importance. The authors fail to provide a clear understanding of how their method improves upon or differs from existing approaches, making it difficult to assess the significance of their contribution. Furthermore, the paper's evaluation is limited to visualizations on real images, which, while intuitive, do not provide a comprehensive understanding of the method's performance.
Additional Feedback
To improve the paper, I suggest that the authors conduct a thorough review of existing literature on feature importance and provide a clear comparison of their approach to prior work. This should include both qualitative and quantitative evaluations to demonstrate the effectiveness of their method. Additionally, the authors should consider providing more comprehensive experiments, including evaluations on a variety of datasets and tasks, to demonstrate the generalizability of their approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How does the interior gradients method differ from existing approaches to feature importance, such as DeepLift and layer-wise relevance propagation?
2. Can the authors provide a more comprehensive evaluation of their method, including quantitative comparisons to prior work and evaluations on a variety of datasets and tasks?
3. How do the authors plan to address the lack of comparison to other methods that attack the same problem of feature importance, and what additional experiments or analyses would they propose to demonstrate the significance of their contribution?