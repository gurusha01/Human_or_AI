Summary
The paper proposes a novel approach to designing neural network architectures based on self-similarity, resulting in fractal networks that can match the performance of standard residual networks on image classification tasks. The authors introduce FractalNet, a simple alternative to ResNet, and demonstrate its effectiveness through experiments on CIFAR, SVHN, and ImageNet datasets. They also develop drop-path, a regularization protocol that prevents co-adaptation of parallel paths in fractal networks.
Decision
I decide to Accept this paper, with the key reason being that the approach is well-motivated and the results are scientifically rigorous. The paper provides a clear and interesting solution to the problem of training ultra-deep neural networks, and the experiments demonstrate the effectiveness of FractalNet and drop-path.
Supporting Arguments
The paper tackles the specific question of how to design neural network architectures that can effectively train ultra-deep networks. The approach is well-motivated, as it is based on the idea of self-similarity, which is a fundamental concept in many areas of science. The authors provide a clear and detailed explanation of the FractalNet architecture and the drop-path regularization protocol, and the experiments demonstrate the effectiveness of these methods. The results are scientifically rigorous, as they are based on a thorough evaluation of the proposed methods on several datasets.
Additional Feedback
To improve the paper, I suggest that the authors provide more analysis of the internal behavior of fractal networks, such as the evolution of the columns during training. Additionally, it would be interesting to see more experiments on the effectiveness of drop-path as a regularization protocol, such as comparing it to other regularization methods. Finally, the authors could provide more discussion on the potential applications of fractal networks and drop-path in other areas of deep learning.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* Can you provide more intuition on why the fractal structure is effective in training ultra-deep networks?
* How do you think the drop-path regularization protocol could be applied to other types of neural networks, such as recurrent neural networks?
* Can you provide more details on the computational resources required to train the fractal networks, and how they compare to the resources required to train residual networks?