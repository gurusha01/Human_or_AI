Summary
The paper proposes a novel variational autoencoder (VAE) model, called PixelVAE, which combines the strengths of VAEs and PixelCNNs. The model uses an autoregressive decoder based on PixelCNN, allowing it to capture fine details in images while maintaining a latent representation. The authors demonstrate the effectiveness of PixelVAE on several datasets, including MNIST, LSUN bedrooms, and 64x64 ImageNet, achieving state-of-the-art performance on MNIST and competitive results on the other datasets.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the choice of prior in the VAE model is unconvincing, and some conceptual claims made in the paper are incorrect. Secondly, the experimental results are unconvincing, with suspected inconsistencies in the units used to compare log likelihoods between the proposed method and competing algorithms.
Supporting Arguments
The paper's proposal of a group sparse prior in VAEs is not well-motivated, and the authors fail to provide a clear explanation for this choice. Furthermore, the experimental results are questionable, with potential inconsistencies in the units used to compare log likelihoods. The reported log likelihoods may be in bits rather than nats, which could lead to unrealistic comparisons with other models, such as the 10K k-means mixture model. Additionally, the MNIST sample quality is not visually competitive, and the images appear to show probabilities of activation rather than actual samples from the model.
Additional Feedback
To improve the paper, I suggest that the authors report and compare the variational lower bound on the log likelihood, or use alternative methods like Annealed Importance Sampling (AIS) to estimate the log likelihood. This would provide a more accurate and reliable evaluation of the model's performance. Furthermore, the authors should conduct experiments on non-toy datasets to demonstrate the scalability and effectiveness of their model in more realistic settings. Finally, the authors should address the concerns raised in the initial questions, including the construction of minibatches, the trade-off between data reconstruction and factorization, and the evaluation of model quality using Parzen window estimation.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:
1. Can you provide a clear explanation for the choice of prior in the VAE model, and how it relates to the autoregressive decoder?
2. How do you ensure that the units used to compare log likelihoods are consistent across different models and datasets?
3. Can you provide more detailed information about the experimental setup, including the construction of minibatches and the evaluation of model quality using Parzen window estimation?