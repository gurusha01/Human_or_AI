Summary
The paper introduces Charged Point Normalization (CPN), a dynamic normalization technique that helps gradient-based optimization algorithms escape saddle points in high-dimensional non-convex optimization problems. The authors provide a thorough analysis of the problem of saddle points and the behavior of first-order gradient descent algorithms around them. They also present empirical results on various neural network architectures, demonstrating the effectiveness of CPN in escaping saddle points.
Decision
I decide to reject this paper, primarily due to two reasons. Firstly, the empirical results, although promising, are not strong enough to outweigh the weaknesses of the proposed approach, as discussed in Section 6. Secondly, the selection of hyper-parameters in the paper is overly simplistic, which may impact the validity of the results.
Supporting Arguments
The paper's approach to tackling the problem of saddle points is interesting and well-motivated, and the authors provide a good analysis of the behavior of first-order gradient descent algorithms around saddle points. However, the empirical results, while promising, are not comprehensive enough to fully demonstrate the effectiveness of CPN. Additionally, the selection of hyper-parameters is not rigorous, which may lead to biased results. The authors themselves acknowledge the limitations of their hyper-parameter selection process in Section 5.
Additional Feedback
To improve the paper, the authors should consider conducting a more comprehensive hyper-parameter search to ensure that the results are not sensitive to the specific choice of hyper-parameters. Additionally, they should provide more detailed analysis of the behavior of CPN in different scenarios, such as when the optimization algorithm reaches a saddle point late in the optimization process. The authors should also consider comparing CPN with other state-of-the-art optimization algorithms to demonstrate its superiority.
Questions for the Authors
I would like the authors to clarify the following points:
1. How did they choose the specific values of the hyper-parameters, such as β and λ, and what was the rationale behind these choices?
2. Can they provide more detailed analysis of the behavior of CPN in different scenarios, such as when the optimization algorithm reaches a saddle point late in the optimization process?
3. How do they plan to address the issue of numerical instability caused by the fraction term in CPN?