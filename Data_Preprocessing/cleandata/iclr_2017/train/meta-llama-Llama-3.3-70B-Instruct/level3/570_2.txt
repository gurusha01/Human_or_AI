Summary of the Paper's Contributions
The paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by introducing an intermediate model called Simplified-SFNN. This model approximates certain SFNNs by simplifying the upper latent units above stochastic ones, making it easier to train than SFNNs. The authors establish a connection between three models: DNN → Simplified-SFNN → SFNN, which leads to an efficient training procedure for stochastic models using pre-trained parameters of deterministic deep neural networks (DNNs).
Decision and Reasons
Based on the review guidelines, I decide to Accept this paper with some minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and relevant problem in the field of neural networks, namely, the difficulty of training SFNNs.
2. The approach proposed by the authors is well-motivated and grounded in the literature, with a clear connection to previous work on SFNNs and DNNs.
3. The paper provides empirical evidence to support the claims made by the authors, demonstrating the effectiveness of the proposed approach on various tasks and datasets.
Supporting Arguments
The paper's contributions are significant, as they address a long-standing challenge in training SFNNs. The proposed Simplified-SFNN model is a useful intermediate step between DNNs and SFNNs, allowing for the transfer of knowledge from pre-trained DNNs to SFNNs. The authors provide a thorough analysis of the model's properties and behavior, including the proof of Theorem 1, which establishes the connection between DNNs and Simplified-SFNNs.
The experimental results presented in the paper are convincing, demonstrating the effectiveness of the proposed approach on various tasks, including classification and multi-modal learning. The authors also provide a detailed comparison with baseline models, including DNNs and other SFNNs, which helps to establish the superiority of their approach.
Additional Feedback and Questions
To further improve the paper, I suggest the following:
1. Provide more intuition about the Simplified-SFNN model and its relationship to SFNNs and DNNs.
2. Consider adding more experimental results to demonstrate the robustness of the proposed approach to different hyperparameters and datasets.
3. Clarify the computational complexity of the proposed approach and its scalability to larger models and datasets.
Some questions I would like the authors to address in their response are:
1. How do the authors plan to extend their approach to more complex SFNN architectures, such as those with multiple stochastic layers?
2. Can the authors provide more insight into the choice of hyperparameters, such as the number of samples used in the Monte Carlo approximation?
3. How do the authors envision their approach being used in practice, and what are the potential applications of their work?