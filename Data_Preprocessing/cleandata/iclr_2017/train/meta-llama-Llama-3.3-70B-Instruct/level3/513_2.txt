Summary of the Paper's Contributions
The paper proposes a novel approach to understanding the Hessian of the loss function in deep learning, revealing a two-phased spectrum with a bulk of eigenvalues concentrated around zero and a discrete part that depends on the data. The authors demonstrate that the Hessian is degenerate, with implications for theoretical work and practical applications. They also show that the spectrum responds to changes in data complexity and network size.
Decision and Key Reasons
I decide to Reject this paper, with two key reasons: (1) the paper lacks detailed analysis and exploration of the limitations of the block tower rendering and generalization to different block heights, and (2) the writing, while clear, does not provide sufficient scientific rigor to support the claims.
Supporting Arguments
The paper's approach is well-motivated, and the results are interesting, but the lack of detailed analysis and exploration of limitations is a significant concern. The authors do not provide sufficient evidence to support their claims, and the paper would benefit from additional experiments and analysis to demonstrate the robustness of their findings.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed analysis and exploration of the limitations of their approach. This could include investigating the effect of different block heights, shapes, and materials on the Hessian spectrum. Additionally, the authors could provide more context on the implications of their findings for practical applications, such as optimizing neural network architectures or improving training procedures.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the authors plan to address the limitations of their approach, particularly with regards to generalization to different block heights and shapes? (2) Can the authors provide more context on the implications of their findings for practical applications, such as optimizing neural network architectures or improving training procedures? (3) How do the authors plan to release the dataset or code to generate it, and what baselines will they use for further evaluation?