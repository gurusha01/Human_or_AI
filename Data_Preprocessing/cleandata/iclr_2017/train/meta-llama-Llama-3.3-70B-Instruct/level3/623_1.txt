Summary of the Paper's Contributions
The paper presents an intriguing investigation into the Hessian of small deep networks, revealing that many eigenvalues are approximately zero, making the Hessian highly singular. This finding has significant implications for theoretical work in deep learning, as it renders much of the existing theory inapplicable. The authors provide empirical evidence for the emergence of a clear bulk/outlier behavior in small networks, where the bulk of the eigenvalues depend on the architecture and the outlier eigenvalues depend on the input data.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the idea that deep learning algorithms are singular and undercut many theoretical results is not new, and the paper could benefit from a more thorough discussion of prior work, such as Watanabe's 2007 paper and others. Secondly, the experimental results, although valuable, may not be generalizable to larger networks, and the paper's results feel preliminary, requiring further development to reveal practical relevance.
Supporting Arguments
The paper's investigation into the Hessian of small deep networks is a valuable contribution, but it is essential to situate this work within the broader context of existing research. The authors acknowledge some prior work, but a more comprehensive discussion of the literature would strengthen the paper. Additionally, the experimental results, although intriguing, are limited to small networks, and it is unclear whether these findings will generalize to larger, more complex networks.
Additional Feedback and Suggestions
To improve the paper, I suggest that the authors provide a more thorough review of prior work on the Hessian in deep learning, including Dauphin et al. and Amari's work. Furthermore, conducting larger-scale experiments to demonstrate the practical relevance of their findings would significantly strengthen the paper. The authors may also consider exploring the implications of their results for optimization algorithms and the design of neural networks.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to address the following questions:
1. How do the authors plan to extend their results to larger networks, and what implications do they expect these findings to have for optimization algorithms and neural network design?
2. Can the authors provide more insight into the relationship between the singular values of the Hessian and the convergence properties of gradient-based algorithms?
3. How do the authors propose to leverage the degenerate Hessian to devise separate methods for the directions corresponding to the top eigenvalues, and what potential benefits do they expect from this approach?