Summary
The paper proposes a differentiable version of Canonical Correlation Analysis (CCA), which enables the computation of CCA projection matrices and their gradients with respect to the input data. This allows CCA to be used as a building block within multi-modality neural networks, and the authors demonstrate its effectiveness in cross-modality retrieval experiments on two public image-to-text datasets.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's methodological novelty is smaller than expected, with the key advance being the introduction of a causal regularizer, but it lacks clear terminology and standard methodological assumptions. Secondly, the experiments fail to address the central question of causality, only showing regularization behaving as expected, but lacking meaningful quantitative evidence that causality has been learned.
Supporting Arguments
The paper's presentation is disorganized, with key data and results relegated to the appendices, and an assumption of familiarity with the Cholupka preprint that makes it hard to stand alone. The work is considered premature due to the lack of a dataset to validate the learning of causality, making it impossible to really validate the results. The authors extend their method of causal discovery to include assumptions about sparsity via regularization, but the presentation is unclear and confusing.
Additional Feedback
To improve the paper, the authors should provide a clearer and more organized presentation of their method and results. They should also provide more meaningful quantitative evidence that causality has been learned, and address the central question of causality in their experiments. Additionally, the authors should consider providing a dataset to validate the learning of causality, and clarify their terminology and methodological assumptions.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence:
1. Can you provide more details on how the causal regularizer is introduced and how it affects the learning of causality?
2. Can you provide more quantitative evidence that causality has been learned, such as metrics that measure the strength of the causal relationships?
3. Can you clarify how the authors' method addresses the central question of causality, and how it differs from existing methods?
4. Can you provide more information on the dataset used to validate the learning of causality, and how it was collected and preprocessed?
5. Can you provide more details on the experimental setup, including the hyperparameters used and the training procedure?