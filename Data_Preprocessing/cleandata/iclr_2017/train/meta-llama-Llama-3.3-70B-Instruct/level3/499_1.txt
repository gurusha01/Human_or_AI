This paper proposes a novel approach to network quantization, which is a crucial technique for reducing the memory footprint of deep neural networks. The authors introduce Hessian-weighted k-means clustering, a method that takes into account the importance of each network parameter when quantizing, and demonstrate its effectiveness in minimizing performance loss due to quantization. 
I marginally accept this paper due to its poor writing style, which hinders its overall impact. The key reasons for this decision are that the paper's approach is well-motivated and the results show significant gain, particularly in language modeling, when using an increased number of parameters. However, the reduction of trainable parameters does not necessarily lead to a decrease in training and recognition speech time, and the authors fail to achieve better results with fewer parameters.
To improve the paper, I suggest reorganizing and shortening it to enhance clarity and consistency. Some sections are difficult to follow, and presenting fewer results could improve the paper's readability and overall quality. Additionally, the authors should provide the number of trainable parameters for Table 6 to enhance transparency. The use of weights in feedforward and recurrent networks is inconsistent and should be addressed.
To clarify my understanding of the paper, I would like the authors to answer the following questions: How do the authors plan to address the inconsistency in using weights in feedforward and recurrent networks? Can the authors provide more details on how they computed the Hessian matrix and its diagonal elements? How do the authors think their approach can be extended to other types of neural networks, such as recurrent neural networks or transformers? 
Overall, while the paper has some flaws, its contributions to the field of network quantization are significant, and with some revisions, it has the potential to be a strong paper.