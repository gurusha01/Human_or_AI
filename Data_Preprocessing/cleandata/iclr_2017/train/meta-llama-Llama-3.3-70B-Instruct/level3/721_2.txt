Summary
The paper proposes a novel "density-diversity penalty" regularizer that encourages low diversity and high sparsity in the weight matrices of fully-connected layers in deep neural networks. The approach is shown to be effective in compressing models on both MNIST and TIMIT datasets, achieving 20X to 200X compression rates on fully-connected layers while maintaining comparable performance to the original models.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity on the data used for training, its generation, and the process of averaging over many data points to smooth the error function. Secondly, the connection between the density-diversity penalty and the compression of weight matrices is not well-defined, making it difficult to understand how the learning rule is used to learn the weight matrices.
Supporting Arguments
The paper's central idea of using a density-diversity penalty to encourage low diversity and high sparsity in weight matrices is promising, but the implementation and explanation of the approach are unclear. The authors propose a "sorting trick" to efficiently optimize the density-diversity penalty, but the computational cost of this trick is still high, and the authors have to apply it with a small probability to reduce the computational cost. Additionally, the paper lacks a compelling conclusion, leaving many concepts unconnected and unclear.
Additional Feedback
To improve the paper, the authors should provide more clarity on the data used for training and the process of averaging over many data points to smooth the error function. They should also provide more details on the connection between the density-diversity penalty and the compression of weight matrices, including how the learning rule is used to learn the weight matrices. Furthermore, the authors should consider providing more experimental results to demonstrate the effectiveness of their approach on different datasets and models.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more details on the data used for training and the process of averaging over many data points to smooth the error function?
2. How does the density-diversity penalty encourage low diversity and high sparsity in weight matrices, and how is the learning rule used to learn the weight matrices?
3. Can you provide more experimental results to demonstrate the effectiveness of your approach on different datasets and models?