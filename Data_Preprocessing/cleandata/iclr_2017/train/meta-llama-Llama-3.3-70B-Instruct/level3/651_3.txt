Summary
The paper proposes a multi-view Bayesian non-parametric algorithm for learning multi-sense word embeddings. The approach leverages multilingual distributional information from parallel corpora in multiple languages to improve sense embeddings. The algorithm uses a principled approach to learn a variable number of senses per word in a data-driven manner. The authors demonstrate the effectiveness of their approach through experiments on word sense induction and contextual word similarity tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity on how the proposed approach differs from existing methods, such as adaptive dropout, in terms of functionality. Secondly, the experimental validation is insufficient, as the paper does not compare the proposed approach with state-of-the-art (SOTA) methods.
Supporting Arguments
The paper proposes a novel approach to learning multi-sense word embeddings, but it is unclear how this approach differs from existing methods. The authors mention that their approach is the first to leverage multilingual distributional information, but they do not provide a clear comparison with other methods that use crosslingual signals. Additionally, the experimental validation is limited, as the paper only compares the proposed approach with a monolingually trained model and does not evaluate its performance on a wider range of tasks or datasets.
Additional Feedback
To improve the paper, the authors should provide a clearer comparison with existing methods and evaluate their approach on a wider range of tasks and datasets. They should also consider adding more details on the implementation of their algorithm and the hyperparameter tuning process. Furthermore, the authors could provide more qualitative analysis of the learned sense embeddings and explore the potential applications of their approach in other NLP tasks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How does the proposed approach differ from existing methods, such as adaptive dropout, in terms of functionality?
2. Can the authors provide more details on the implementation of their algorithm and the hyperparameter tuning process?
3. How do the authors plan to evaluate the performance of their approach on a wider range of tasks and datasets?
4. Can the authors provide more qualitative analysis of the learned sense embeddings and explore the potential applications of their approach in other NLP tasks?