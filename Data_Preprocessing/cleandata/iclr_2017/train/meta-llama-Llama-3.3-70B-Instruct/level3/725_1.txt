This paper proposes a novel approach to training stochastic feedforward neural networks (SFNN) by leveraging the connection between deterministic deep neural networks (DNN) and SFNN. The authors introduce an intermediate model, Simplified-SFNN, which approximates SFNN by simplifying its upper latent units above stochastic ones. This connection enables an efficient training procedure for SFNN using pre-trained parameters of DNN.
The paper claims to contribute to the development of efficient training methods for large-scale SFNN, which is a significant problem in the field. The proposed approach is well-motivated, and the authors provide a clear explanation of the limitations of existing methods and the benefits of their approach.
Based on the provided information, I decide to reject this paper. The main reason for this decision is the lack of results on real tasks with large training sets, which raises questions about the method's scalability. While the authors demonstrate the effectiveness of their approach on small tasks like MNIST, it is unclear whether their method can be applied to more complex tasks with larger datasets.
The proposed multi-stage training methods are simple to implement, but they lack theoretical rigor. The authors rely on empirical evaluations to demonstrate the effectiveness of their approach, which may not be sufficient to establish the validity of their method. Furthermore, the paper does not provide a thorough exploration of the scalability of the learning methods when the training data becomes larger, which is a crucial aspect for real-world applications.
To improve the paper, I suggest that the authors provide more theoretical analysis of their approach and demonstrate its effectiveness on more complex tasks with larger datasets. Additionally, the authors should explore the connections with other models, such as variational autoencoder models, which also involve stochastic hidden layers. This could potentially enhance the proposed method and provide a more comprehensive understanding of its strengths and limitations.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors plan to address the scalability issue of their method, and what experiments can be conducted to demonstrate its effectiveness on larger datasets?
* Can the authors provide more theoretical analysis of their approach, including proofs of convergence and bounds on the error?
* How does the proposed method compare to other approaches, such as variational autoencoder models, and what are the advantages and disadvantages of each approach?