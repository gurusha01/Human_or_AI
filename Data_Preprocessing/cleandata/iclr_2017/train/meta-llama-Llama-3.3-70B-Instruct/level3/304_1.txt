Summary
The paper presents Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs) that exploits the variable precision requirements of DNNs to deliver execution time proportional to the precision used per layer. TRT is built upon the DaDianNao accelerator and processes activations bit-serially, allowing it to outperform a state-of-the-art bit-parallel accelerator by 1.90× without any loss in accuracy, while being 1.17× more energy efficient.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in the field of DNN acceleration, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear and detailed explanation of the TRT architecture and its advantages over existing accelerators. The experimental results demonstrate the effectiveness of TRT in improving execution time and energy efficiency, while maintaining accuracy. The paper also discusses the limitations of the current work and provides directions for future research, such as applying TRT to other network architectures and exploring its potential for training.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of TRT, such as the specific hardware components used and the programming model. Additionally, it would be interesting to see a comparison of TRT with other accelerators, such as those using quantization or pruning techniques. The authors may also want to consider exploring the potential of TRT for other applications, such as natural language processing or computer vision.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does TRT handle layers with varying precision requirements, and what are the implications for the overall system performance?
* Can TRT be used for training DNNs, and if so, what modifications would be necessary to support the additional operations required during training?
* How does TRT compare to other accelerators in terms of area overhead and power consumption, and what are the trade-offs between these factors and performance?