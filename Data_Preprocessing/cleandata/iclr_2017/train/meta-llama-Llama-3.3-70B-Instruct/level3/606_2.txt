The paper presents a novel framework for formulating data-structures in a learnable way, with potential to generalize to various data-structures and algorithms. Specifically, it introduces NewsQA, a challenging machine comprehension dataset of over 100,000 question-answer pairs, designed to encourage exploratory, curiosity-based questions that reflect human information seeking. The dataset is collected through a four-stage process, including article curation, question sourcing, answer sourcing, and validation, and is characterized by its diversity in answer types and the significant proportion of questions that require reasoning ability to solve.
Based on the provided guidelines, I decide to reject this paper with two key reasons: inadequate analysis of related work and insufficient experimental evidence to support the claims made. The paper lacks comparison to related works, such as DeepMind's neural touring machine, and fails to conceptually compare its results to these existing studies. Furthermore, the experimental section is mostly qualitative and requires improvements, such as evaluating the accuracy of stack and queue structures for increasing numbers of elements.
To support this decision, I argue that the paper's claims about the dataset's challenge and usefulness as a machine comprehension benchmark are not sufficiently supported by empirical evidence. While the paper provides some statistics and analysis of the dataset, it does not provide a thorough comparison to existing datasets and models, making it difficult to assess the significance of the results. Additionally, the paper's experimental evaluation is limited to a few baseline models, and the results are not thoroughly analyzed or discussed.
To improve the paper, I suggest that the authors provide a more comprehensive analysis of related work, including a comparison to existing datasets and models. Additionally, the authors should provide more extensive experimental evidence, including quantitative evaluations of the dataset's challenge and usefulness, and a more thorough analysis of the results. Specifically, I recommend experimenting with arbitrary push-pop operations and increasing numbers of stack/queue elements to test the model's robustness and efficiency. I also suggest that the authors remove or provide additional evidence to support the claims about "mental representations", as they are currently unsupported.
I would like to ask the authors to clarify the following points: (1) How does the NewsQA dataset compare to existing datasets in terms of its challenge and usefulness as a machine comprehension benchmark? (2) What are the key differences between the NewsQA dataset and other datasets, such as SQuAD, and how do these differences impact the results? (3) How do the authors plan to address the limitations of the current experimental evaluation, and what additional experiments or analyses do they propose to provide more comprehensive evidence for the claims made in the paper?