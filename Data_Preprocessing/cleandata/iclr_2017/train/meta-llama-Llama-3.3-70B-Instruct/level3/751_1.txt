Summary
The paper introduces a novel approach to avoiding large negative rewards in deep reinforcement learning by creating a "danger model" that distills previously experienced negative events. This approach, called intrinsic fear, is evaluated in two toy domains and shows promising results in reducing the occurrence of catastrophic mistakes. The paper is well-written, and the authors provide a clear motivation for their work, placing it within the context of existing research on safe reinforcement learning and intrinsically motivated reinforcement learning.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to addressing a significant problem in deep reinforcement learning. The authors provide a clear and concise explanation of their method, and the experimental results demonstrate its effectiveness in reducing catastrophic mistakes.
Supporting Arguments
The paper tackles a specific and important question in the field of deep reinforcement learning, namely, how to avoid large negative rewards and prevent catastrophic mistakes. The authors provide a thorough analysis of the problem, discussing the limitations of existing approaches and the need for a new solution. The intrinsic fear approach is well-motivated, and the authors provide a clear explanation of how it works and why it is effective. The experimental results are convincing, demonstrating a significant reduction in catastrophic mistakes in the two toy domains.
Additional Feedback
To further improve the paper, I suggest that the authors consider evaluating their approach on more complex domains, such as the Atari games, to demonstrate its scalability and effectiveness in more realistic settings. Additionally, the authors could provide more analysis on the trade-offs between the fear factor and the fear radius, and how these hyperparameters affect the performance of the intrinsic fear approach. It would also be interesting to see a comparison with other approaches to safe reinforcement learning, such as those that use external knowledge to improve safety.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. How do the authors plan to extend their approach to more complex domains, and what challenges do they anticipate in doing so?
2. Can the authors provide more insight into the choice of hyperparameters, such as the fear factor and the fear radius, and how these affect the performance of the intrinsic fear approach?
3. How does the intrinsic fear approach compare to other methods for safe reinforcement learning, such as those that use external knowledge to improve safety?
4. What are the potential limitations and drawbacks of the intrinsic fear approach, and how might these be addressed in future work?