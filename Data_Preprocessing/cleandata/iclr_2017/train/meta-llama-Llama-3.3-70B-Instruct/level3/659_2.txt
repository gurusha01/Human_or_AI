This paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by leveraging the knowledge of pre-trained deterministic deep neural networks (DNNs). The authors introduce an intermediate model, called Simplified-SFNN, which approximates SFNN by simplifying its upper latent units above stochastic ones. This connection enables an efficient training procedure for SFNNs using the pre-trained parameters of DNNs.
The paper is well-written, and the experiments demonstrate the effectiveness of the proposed approach. The authors show that Simplified-SFNN consistently outperforms its baseline DNN due to the stochastic regularizing effect, even when trained using dropout and batch normalization. The results on various datasets, including MNIST, TFD, CIFAR-10, CIFAR-100, and SVHN, demonstrate the potential of the proposed approach.
However, I decide to reject this paper for the following reasons:
1. The contribution of the paper is limited due to similarities with previous approaches, such as the use of stochastic neural networks and knowledge distillation. While the authors propose a new intermediate model, the overall idea of leveraging pre-trained DNNs to train SFNNs is not entirely new.
2. The paper lacks a thorough comparison to other relevant works, which makes it difficult to assess the significance of the proposed approach. The authors should provide a more comprehensive review of existing methods and compare their approach to state-of-the-art techniques.
To improve the paper, I suggest the following:
* Provide a more detailed comparison to other relevant works, including a discussion of the strengths and weaknesses of each approach.
* Consider adding more experiments to demonstrate the robustness of the proposed approach, such as evaluating the performance on more datasets or using different architectures.
* Clarify the novelty of the proposed approach and how it differs from existing methods.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How does the proposed approach differ from existing methods, such as knowledge distillation and stochastic neural networks?
* Can the authors provide more insights into the choice of hyperparameters, such as the number of samples used for estimating the expectations in Simplified-SFNN?
* How does the proposed approach scale to larger datasets and more complex architectures?