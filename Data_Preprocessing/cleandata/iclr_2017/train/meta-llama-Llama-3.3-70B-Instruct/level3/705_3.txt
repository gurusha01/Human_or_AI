Summary
The paper proposes a method for learning semantic units from audio and visual data using a multimodal neural network architecture. The model allows for joint training of image and spoken language captions into a shared representation space, enabling audio-visual groundings and relating specific visual regions to audio segments. The authors demonstrate the effectiveness of their approach in bidirectional image/audio retrieval and acoustic pattern discovery.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the method for associating image regions with audio subsequences is relatively straightforward, combining standard techniques with limited novelty. Secondly, the paper has many hyperparameters with no discussion on how they were set or the sensitivity of the method to these choices, limiting the paper's novelty and significance.
Supporting Arguments
The paper builds upon prior work by the authors, but the incremental advancements in multimodal learning are primarily through a novel combination of input modalities. While the authors demonstrate improved performance in bidirectional image/audio retrieval and acoustic pattern discovery, the clustering method used has limited novelty and feels heuristic. Furthermore, the lack of discussion on hyperparameter selection and sensitivity analysis raises concerns about the robustness and generalizability of the proposed method.
Additional Feedback
To improve the paper, I suggest that the authors provide a more detailed discussion on the selection of hyperparameters and their impact on the performance of the proposed method. Additionally, the authors could consider comparing their approach to other state-of-the-art methods in multimodal learning and providing more insights into the learned representations and their interpretability. It would also be helpful to see more qualitative results and visualizations to illustrate the effectiveness of the proposed method.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How did you select the hyperparameters for your model, and what was the sensitivity of the method to these choices?
2. Can you provide more insights into the learned representations and their interpretability, and how they relate to human perception and cognition?
3. How does your approach compare to other state-of-the-art methods in multimodal learning, and what are the key advantages and limitations of your method?