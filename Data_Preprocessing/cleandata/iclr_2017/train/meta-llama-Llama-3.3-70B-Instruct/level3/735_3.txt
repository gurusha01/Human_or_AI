Summary of the Paper
The paper proposes a novel approach to understanding the importance of individual samples in training deep neural networks. The authors define sample importance as the change in parameters induced by a sample and provide a quantitative measurement of this importance. They conduct empirical experiments on two standard datasets, MNIST and CIFAR-10, and demonstrate that easy samples shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also show that mixing hard samples with easy samples in each batch improves training performance.
Decision
I decide to reject this paper. The main reasons for this decision are that the experimental evaluation is not convincing, and the authors' claims of state-of-the-art results are questionable. The paper only compares to old techniques on toy datasets and lacks robustness to complex noise and outliers. Additionally, the paper fails to address the out-of-sample problem, a significant issue for kernel-based methods.
Supporting Arguments
The experimental evaluation is limited to two standard datasets, MNIST and CIFAR-10, which are not representative of real-world scenarios. The authors do not provide any comparisons to more recent and advanced techniques, making it difficult to assess the significance of their contributions. Furthermore, the paper contains several errors, including incorrect claims about the proposed approach being a closed-form solution and its ability to be generalized to incorporate other cost functions.
Additional Feedback
To improve the paper, the authors should provide more comprehensive experimental evaluations on a variety of datasets, including those with complex noise and outliers. They should also address the out-of-sample problem and provide a more detailed analysis of the sample importance in different layers and training stages. Additionally, the authors should clarify the terminology and provide more precise definitions of key concepts, such as sample importance and parameter affectibility.
Questions for the Authors
1. How do the authors plan to address the out-of-sample problem in their approach?
2. Can the authors provide more detailed comparisons to recent and advanced techniques in the field?
3. How do the authors define and measure the "easiness" of a sample, and what are the implications of this definition for the sample importance calculation?
4. Can the authors provide more insights into the relationship between sample importance and negative log-likelihood, and how they plan to use this relationship to improve training performance?