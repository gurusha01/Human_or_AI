Summary
The paper proposes a novel variational autoencoder (VAE) model, called PixelVAE, which combines the strengths of VAEs and PixelCNNs. The model uses a conditional PixelCNN in the decoder to capture fine details in images, while the latent variables model global structure. The authors demonstrate the effectiveness of PixelVAE on several datasets, including MNIST, LSUN bedrooms, and 64x64 ImageNet.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the proposed solution to latent variable over-pruning does not seem to offer significant improvements over existing methods, such as a mixture of VAEs. Secondly, the experimental results are misleading, with the log-likelihood of the proposed models being evaluated using a Parzen window estimator instead of a more accurate lower bound.
Supporting Arguments
The paper's claim that the VAE "overfits" to the training data is not justified, as no evidence is presented to support this claim. Additionally, the use of dropout in the dropout VAE is not clearly specified, which could significantly impact the model's behavior. The samples and reconstructions from the eVAE model are blurry and do not accurately encode the position of strokes, consistent with an interpretation as a mixture of smaller VAEs rather than a higher-dimensional VAE.
Additional Feedback
To improve the paper, the authors should provide a stronger baseline VAE and reliable evaluation methods. The paper would also benefit from a clearer explanation of the model's architecture and the role of the conditional PixelCNN in the decoder. Furthermore, the authors should address the issue of over-pruning and provide evidence to support their claims.
Questions for the Authors
1. Can you provide a more detailed comparison of PixelVAE with existing VAE models, including a mixture of VAEs?
2. How do you plan to address the issue of over-pruning in the latent variables?
3. Can you provide more evidence to support the claim that the VAE "overfits" to the training data?
4. How does the use of dropout in the dropout VAE impact the model's behavior, and can you provide more details on the implementation?
5. Can you provide more samples and reconstructions from the eVAE model to demonstrate its effectiveness in capturing fine details in images?