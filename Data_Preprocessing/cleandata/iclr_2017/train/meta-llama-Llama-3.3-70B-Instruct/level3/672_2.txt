Summary of the Paper
The paper presents a novel approach to understanding the Hessian of the loss function in deep learning, which has significant implications for the theoretical and practical aspects of training neural networks. The authors empirically demonstrate that the Hessian is degenerate, with a bulk of eigenvalues concentrated around zero and a discrete set of eigenvalues that depend on the data. This two-phased non-degeneracy is a desirable property, as it implies locally flat regions in the loss landscape.
Decision and Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper does not provide a clear and concise motivation for why the Hessian of the loss function is an important object to study, and how it relates to the broader literature on deep learning. Secondly, the experimental results, while intriguing, are not thoroughly analyzed, and the conclusions drawn from them are not fully supported by the data.
Supporting Arguments
The paper's lack of motivation and context makes it difficult to understand the significance of the results. For example, the authors do not discuss how the degeneracy of the Hessian affects the convergence properties of gradient-based optimization algorithms, or how it relates to other work on the loss landscape of deep neural networks. Additionally, the experimental results are largely qualitative, and the authors do not provide a clear and quantitative analysis of the data. For instance, the plots of the eigenvalue distribution are not accompanied by any statistical analysis or error bars, making it difficult to assess the significance of the results.
Additional Feedback
To improve the paper, I suggest that the authors provide a clearer motivation for studying the Hessian of the loss function, and situate their work within the broader literature on deep learning. Additionally, the authors should provide a more thorough and quantitative analysis of the experimental results, including statistical analysis and error bars. Finally, the authors should consider providing more context and discussion of the implications of their results, including how they relate to other work on the loss landscape of deep neural networks.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How does the degeneracy of the Hessian affect the convergence properties of gradient-based optimization algorithms? (2) How do the results relate to other work on the loss landscape of deep neural networks, such as the work on saddle points and wide basins? (3) Can the authors provide a more quantitative analysis of the experimental results, including statistical analysis and error bars?