Summary of the Paper's Contributions
The paper proposes a novel deep learning model, GRU-D, which effectively handles missing values in multivariate time series data by incorporating masking and time interval directly into the GRU architecture. The model captures informative missingness and achieves state-of-the-art performance on real-world healthcare datasets, including MIMIC-III and PhysioNet. The authors demonstrate the model's ability to exploit missing patterns, make online predictions, and scale with growing data size.
Decision to Accept or Reject
Based on the review guidelines, I decide to accept the paper. The reasons for this decision are:
1. The paper tackles a specific question/problem, namely handling missing values in multivariate time series data, and proposes a novel solution.
2. The approach is well-motivated, and the authors provide a clear explanation of the problem and the proposed solution.
3. The paper supports its claims with empirical results, demonstrating the effectiveness of the proposed model on real-world datasets.
Supporting Arguments
The paper provides a thorough analysis of the problem, including an investigation of the relationship between missingness and labels. The authors propose a novel model, GRU-D, which incorporates trainable decays to capture temporal missing patterns. The model is evaluated on synthetic and real-world datasets, demonstrating its ability to exploit informative missingness and achieve state-of-the-art performance. The authors also provide a detailed comparison with alternative methods, including non-RNN baselines and other RNN models.
Additional Feedback
To further improve the paper, I suggest the authors:
1. Provide more insight into the interpretability of the proposed model, including the decay rates and their relationship to the missing patterns.
2. Investigate the applicability of the proposed model to other domains, such as finance or environmental monitoring.
3. Consider providing more detailed analysis of the model's performance on different types of missing patterns, such as missing completely at random (MCAR) or missing not at random (MNAR).
Questions for the Authors
1. Can you provide more insight into the choice of the exponentiated negative rectifier for the decay term, and how it affects the model's performance?
2. How do you plan to extend the proposed model to handle more complex missing patterns, such as non-stationary or non-linear relationships between missingness and labels?
3. Can you provide more details on the computational resources required to train the proposed model, and how it scales with larger datasets?