Summary of the Paper's Contributions
The paper proposes a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the expressive power of RNN language models. The NKLM predicts whether a word is based on a fact or not and generates words either from the vocabulary or by copying from the fact description. The model is trained on a new dataset, WikiFacts, and outperforms the traditional RNNLM in terms of perplexity and generates named entities that are not observed during training.
Decision and Key Reasons
I decide to reject this paper, with the key reasons being that the results are not state-of-the-art and lack convincing evidence to justify the new model architecture. Additionally, the small size of the datasets used, including the Yelp 2013 dataset, raises concerns about the model's ability to scale to larger datasets.
Supporting Arguments
The paper presents two architectures, TS-ATT and SS-ATT, but the results are not convincing, and the authors do not provide a clear comparison with previous studies, such as Tang et al., 2015. The paper also lacks minor comments, such as correcting typos and downplaying the significance of not needing pre-trained embeddings. Furthermore, the authors' claim that the model can immediately adapt to adding or modifying knowledge is not fully supported by the experimental results.
Additional Feedback
To improve the paper, I suggest that the authors include results from previous studies for comparison and provide more convincing evidence to justify the new model architecture. Additionally, the authors should consider using larger datasets to demonstrate the model's ability to scale. The authors should also provide more details about the experimental setup, such as the hyperparameter tuning process, and consider using more advanced evaluation metrics, such as the Unknown-Penalized Perplexity (UPP) introduced in the paper.
Questions for the Authors
I would like the authors to clarify the following points:
1. How do the authors plan to address the issue of scaling the model to larger datasets?
2. Can the authors provide more details about the experimental setup, such as the hyperparameter tuning process?
3. How do the authors plan to evaluate the model's performance on more challenging tasks, such as question answering and dialogue modeling?