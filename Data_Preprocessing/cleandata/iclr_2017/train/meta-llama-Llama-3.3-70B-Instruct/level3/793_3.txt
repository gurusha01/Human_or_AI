The paper proposes a novel approach to sequence-to-sequence prediction tasks, specifically in the context of text summarization, by introducing a "Read-Again" model that reads the input sequence twice to better capture the meaning of each word. Additionally, the paper proposes a copy mechanism that allows the model to handle out-of-vocabulary words in a principled manner, enabling the use of smaller vocabulary sizes and faster inference times.
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity and analysis in certain sections, particularly in the description of the feedback RNN and LSTM equations, which makes it difficult to fully understand the proposed approach. Secondly, the experimental evaluation is limited to only one dataset, enwik8, and fails to demonstrate consistent improvements across different datasets, which raises concerns about the generalizability of the proposed approach.
To support these reasons, I argue that the paper's lack of clarity and analysis in certain sections may indicate a lack of rigor in the development of the proposed approach. Furthermore, the limited experimental evaluation may not be sufficient to demonstrate the effectiveness of the proposed approach in a wide range of scenarios. While the paper claims state-of-the-art results on enwik8, it is actually outperformed by hypernetworks, which achieves better results.
To improve the paper, I suggest that the authors provide more detailed analysis and clarity in the description of the proposed approach, particularly in the equations and algorithms presented. Additionally, I recommend that the authors conduct more extensive experimental evaluations on multiple datasets to demonstrate the generalizability and effectiveness of the proposed approach. Specifically, I would like the authors to answer the following questions: (1) Can you provide more detailed explanations of the feedback RNN and LSTM equations, and how they are used in the proposed approach? (2) How do you plan to address the limited experimental evaluation, and what additional datasets or scenarios do you plan to explore in future work? (3) Can you provide more insights into the copy mechanism, and how it handles out-of-vocabulary words in different contexts? By addressing these questions and concerns, the authors can strengthen the paper and provide more convincing evidence for the effectiveness of the proposed approach.