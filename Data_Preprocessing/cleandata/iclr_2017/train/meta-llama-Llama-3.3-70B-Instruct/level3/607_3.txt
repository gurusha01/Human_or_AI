This paper proposes a method for learning perceptual reward functions from a few demonstrations of real-world tasks, leveraging the abstraction power of intermediate visual representations learned by deep models. The approach is able to identify key intermediate steps of a task and automatically discover the most discriminative features for identifying these steps. The resulting reward functions are dense and smooth, making them suitable for use in reinforcement learning algorithms.
The paper tackles the specific question of how to design reward functions for complex robotic manipulation tasks, which is a crucial problem in the field of reinforcement learning. The approach is well-motivated, building on the idea of inverse reinforcement learning and leveraging the power of deep neural networks to learn generalizable visual features.
However, I decide to reject this paper due to two key reasons. Firstly, the writing quality of the paper is poor, making it difficult to understand the methodology and results. There are notational discrepancies and unclear descriptions of novel modules, such as HAM. Secondly, the experimental results show mixed improvements with the addition of TEM and HAM, with a small increase in METEOR score that may not be significant due to the small test set.
To improve the paper, I suggest that the authors clarify their methodology and provide more detailed descriptions of their approach. Additionally, they should provide more comprehensive experimental results, including a larger test set and more detailed analysis of the results. The authors should also consider providing more visualizations and examples to illustrate their approach and results.
Some questions I would like the authors to answer include: How do the authors plan to address the issue of notational discrepancies and unclear descriptions of novel modules? How do the authors plan to provide more comprehensive experimental results, including a larger test set and more detailed analysis of the results? How do the authors plan to demonstrate the significance of their approach, given the small increase in METEOR score?
Overall, while the paper proposes an interesting approach to learning perceptual reward functions, the poor writing quality and mixed experimental results make it difficult to accept in its current form. With revisions to address these issues, the paper has the potential to make a significant contribution to the field of reinforcement learning.