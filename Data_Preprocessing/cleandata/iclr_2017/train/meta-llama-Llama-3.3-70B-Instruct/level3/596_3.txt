Summary
The paper proposes a novel approach to knowledge base completion using an implicit shared memory and a multi-step search process. The authors introduce NoiseOut, a pruning algorithm that removes redundant neurons in neural networks based on the correlation between activations of neurons. The approach is motivated by the need to reduce computation and memory usage in neural networks while maintaining accuracy. The paper presents promising experimental results on various networks and datasets, demonstrating significant compression rates without loss of accuracy.
Decision
I decide to Accept this paper, with the main reason being the innovative approach to pruning neural networks and the promising experimental results. However, I have some concerns regarding the lack of analysis on the sensitivity of the model to hyper-parameters, particularly the size of the shared memory.
Supporting Arguments
The paper presents a well-motivated approach to pruning neural networks, and the experimental results demonstrate the effectiveness of the proposed method. The authors provide a clear explanation of the NoiseOut algorithm and its application to various networks and datasets. The results show significant compression rates without loss of accuracy, which is a notable achievement.
Additional Feedback
To improve the paper, I suggest that the authors provide a more detailed analysis of the sensitivity of the model to hyper-parameters, particularly the size of the shared memory. Additionally, it would be beneficial to explore the possibility of a dynamic memory structure and demonstrate the effectiveness of the RL setting by comparing it to a model without a termination gate and a fixed number of inference steps. Furthermore, applying the shortest path analysis to a real KB setting, such as Freebase, would further demonstrate the model's capabilities.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the issue of hyper-parameter sensitivity, particularly with regards to the size of the shared memory?
2. Can the authors provide more details on the dynamic memory structure and how it would be implemented?
3. How do the authors plan to demonstrate the effectiveness of the RL setting in comparison to other models?
4. Are there any plans to apply the shortest path analysis to real-world KB settings, such as Freebase?