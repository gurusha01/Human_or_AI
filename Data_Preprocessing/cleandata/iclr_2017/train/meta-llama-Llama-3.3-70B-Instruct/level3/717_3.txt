Summary
The paper proposes an online structure learning technique for Gaussian Sum-Product Networks (SPNs), which is a type of deep neural network with clear semantics and tractable inference. The algorithm, called oSLRAU, starts with a fully factorized joint probability distribution and updates the structure and parameters of the network as new data points are processed. The technique is evaluated on several benchmark datasets and is shown to outperform other algorithms, including online Bayesian moment matching, online expectation maximization, and RealNVP.
Decision
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and well-motivated problem in the field of deep learning, namely online structure learning for Gaussian SPNs.
2. The approach is well-placed in the literature, building on existing work on SPNs and online learning techniques.
3. The paper provides a clear and concise description of the algorithm and its components, including the parameter update and structure update procedures.
Supporting Arguments
The paper provides a thorough evaluation of the algorithm on several benchmark datasets, including a toy dataset and larger datasets. The results show that oSLRAU outperforms other algorithms in terms of log-likelihood and is able to learn a suitable network structure with streaming data. The paper also provides a detailed analysis of the effects of varying the correlation threshold and the maximum number of variables per leaf node on the resulting SPN.
Additional Feedback
To improve the paper, I suggest the following:
* Provide more details on the computational complexity of the algorithm, including the time and space complexity of the parameter update and structure update procedures.
* Consider adding more experiments to evaluate the robustness of the algorithm to different types of noise and missing data.
* Provide more discussion on the potential applications of the algorithm, including its use in real-world domains such as natural language processing and computer vision.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the initialization of the network structure and the choice of the correlation threshold?
* How do you handle the case where the number of variables in the dataset is very large, and the algorithm needs to branch off into a new subtree?
* Can you provide more discussion on the potential limitations of the algorithm, including its sensitivity to the choice of hyperparameters and its ability to handle complex correlations between variables?