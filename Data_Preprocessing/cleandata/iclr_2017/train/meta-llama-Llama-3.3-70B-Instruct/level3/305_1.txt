Summary of the Paper's Contributions
The paper presents a novel approach to image compression using deep neural networks, incorporating rate-distortion theory and object persistence constraints. The authors propose a re-trained deep convolutional neural network (DCNN) called Object Persistence Net (OPnet), which learns to associate different views of each 3D object to capture the notion of object persistence and continuity in visual experience. The OPnet is evaluated on various test sets, including novel objects, categories, and artificial synthetic objects, demonstrating significant improvements in similarity judgment and transferability of learning.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons:
1. The paper presents a well-motivated and well-placed approach in the literature, effectively incorporating object persistence constraints into the training of a DCNN.
2. The experimental results demonstrate significant improvements in similarity judgment and transferability of learning, with a thorough evaluation on various test sets.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of perceptual similarity judgment and the importance of object persistence constraints. The authors propose a novel approach to re-train a DCNN using a Siamese triplet architecture, which effectively modifies the view-manifold of object representations. The experimental results demonstrate the effectiveness of the OPnet in similarity judgment and transferability of learning, with significant improvements over baseline methods.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
* Providing more detailed analysis of the view-manifold structure and its relation to human perceptual similarity judgment.
* Exploring the application of the OPnet to other domains, such as image compression and retrieval.
* Investigating the use of other architectures, such as generative models, for similarity judgment and transferability of learning.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the rendering process of the multi-view datasets and the selection of the 3D object models?
* How do you plan to extend the OPnet to handle more complex variations, such as lighting and scale, in real-world environments?
* Can you provide more insights into the relation between the view-manifold structure and human perceptual similarity judgment, and how the OPnet can be used to model human perception?