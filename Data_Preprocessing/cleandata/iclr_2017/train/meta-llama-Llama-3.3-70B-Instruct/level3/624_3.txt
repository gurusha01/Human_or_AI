Summary
The paper introduces Attentive Recurrent Comparators (ARCs), a novel class of neural networks that learn to estimate the similarity of a set of objects by cycling through them and making observations. The model uses attention and recurrence to focus on salient aspects of the objects and has achieved state-of-the-art performance on the Omniglot dataset for one-shot classification, surpassing human performance.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated question of similarity learning, and (2) the approach is well-placed in the literature, building upon existing work on attention and recurrence.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of similarity learning and the limitations of existing approaches. The authors propose a novel architecture that addresses these limitations and demonstrate its effectiveness on a challenging dataset. The experimental results are thorough and well-analyzed, providing insights into the strengths and weaknesses of the proposed approach. Additionally, the paper provides a clear and concise overview of related work, demonstrating a good understanding of the literature.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of the proposed approach and its potential applications to other domains. Additionally, it would be interesting to see more analysis on the attention mechanism and its role in the overall performance of the model. Some questions I would like the authors to answer include: (1) How does the attention mechanism adapt to different types of data and tasks? (2) Can the proposed approach be extended to more complex tasks, such as multi-modal learning? (3) How does the model's performance compare to other state-of-the-art approaches on other datasets? 
Overall, the paper presents a well-motivated and well-executed approach to similarity learning, and I believe it makes a significant contribution to the field. With some additional analysis and experimentation, the paper has the potential to be even stronger.