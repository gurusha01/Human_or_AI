This paper introduces a novel approach to comparing images using an attention-based recurrent network, called Attentive Recurrent Comparators (ARCs). The authors claim that their model achieves state-of-the-art results on the Omniglot dataset, surpassing human performance and outperforming other deep learning methods.
Based on the provided guidelines, I will evaluate the paper and make a decision to accept or reject it.
The specific question/problem tackled by the paper is the task of estimating the similarity of a set of objects, particularly images, using a bottom-up approach with attention and recurrence. The approach is well-motivated, drawing inspiration from human behavior and leveraging modern developments in attention mechanisms and recurrent neural networks.
However, I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks strong qualitative results and analysis, with the attention mechanism in Fig. 2 always attending to the full character, which seems like a trivial solution. Secondly, the paper still requires more details in some aspects, such as the specifics of the convolutional feature extractor used, which is responsible for the large performance gain.
In support of these reasons, the paper's qualitative analysis is limited, and the attention mechanism's behavior is not thoroughly explored. Additionally, the authors acknowledge that the performance gain is significantly influenced by the use of extracted convolutional features as input, but they do not provide sufficient details about the convolutional feature extractor used.
To improve the paper, I suggest that the authors provide more detailed qualitative analysis and exploration of the attention mechanism's behavior. They should also provide more information about the convolutional feature extractor used and its hyperparameters. Furthermore, the authors could benefit from comparing their model to other state-of-the-art methods in more detail and providing more insights into the model's strengths and weaknesses.
I would like to ask the authors to clarify the following points: (1) Can you provide more detailed visualizations of the attention mechanism's behavior, such as heatmaps or attention maps, to better understand how the model is making comparisons? (2) How did you select the hyperparameters for the convolutional feature extractor, and what is the impact of different hyperparameters on the model's performance? (3) Can you provide more insights into the model's performance on other datasets and tasks, to demonstrate its generalizability and robustness?