This paper proposes a simple count-based exploration technique for high-dimensional reinforcement learning (RL) applications, such as Atari Games, using state hash to group similar states together. The technique is solid with extensive experiments, but its generalizability to more Atari games, particularly those with continuous state spaces, is questionable.
I decide to Reject this paper for the following reasons:
1. The paper tackles the specific question of exploration in high-dimensional RL applications, but its approach may not be well-suited for all types of games, particularly those with continuous state spaces.
2. The approach is well-motivated and placed in the literature, but its generalizability and robustness to different environments and hyperparameter settings are not thoroughly demonstrated.
To support my decision, I provide the following arguments:
* The paper's experiments are mostly focused on Atari games with discrete state spaces, and it is unclear how the approach would perform on games with continuous state spaces.
* The use of hashing to group similar states together may not be effective in games where the state space is highly complex or has many local optima.
* The paper's section on learned embedding (Sec. 2.3) is confusing and requires clarification, which makes it difficult to understand the approach and its limitations.
To improve the paper, I suggest the following:
* Provide more experiments on games with continuous state spaces to demonstrate the approach's generalizability.
* Clarify the section on learned embedding and provide more details on how it works and its limitations.
* Discuss the potential limitations of the approach and provide more analysis on its robustness to different environments and hyperparameter settings.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
* How do the authors plan to address the issue of generalizability to games with continuous state spaces?
* Can the authors provide more details on the learned embedding approach and how it works in practice?
* How do the authors plan to demonstrate the robustness of their approach to different environments and hyperparameter settings?