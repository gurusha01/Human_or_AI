Summary
The paper presents a novel approach to addressing the safety concerns in deep reinforcement learning (DRL) by introducing a concept called "intrinsic fear." This approach aims to prevent DRL agents from making catastrophic mistakes by learning a reward shaping that accelerates learning and guards against repeated catastrophes. The authors demonstrate the effectiveness of their approach on several toy problems, including Adventure Seeker and Cart-Pole, and provide preliminary results on the Atari game Seaquest.
Decision
I decide to accept this paper with minor revisions. The paper is well-written, understandable, and conclusive, and the approach presented is well-motivated and supported by empirical results. However, I have some minor criticisms and suggestions for improvement, which I outline below.
Supporting Arguments
The paper tackles a specific and important problem in DRL, namely the tendency of DRL agents to make catastrophic mistakes due to the use of function approximation. The authors provide a clear and concise overview of the problem and its significance, and their approach to addressing it is well-motivated and supported by empirical results. The experiments presented in the paper demonstrate the effectiveness of the intrinsic fear approach in preventing catastrophic mistakes and improving the performance of DRL agents.
Additional Feedback
To improve the paper, I suggest that the authors provide more information on the hyperparameters used in their experiments, such as the learning rate, batch size, and number of episodes. Additionally, it would be helpful to include more details on the architecture of the danger model and how it is trained. Furthermore, the authors could provide more discussion on the potential limitations and challenges of their approach, such as the need for a catastrophe detector and the potential for overfitting to the danger model.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How did the authors choose the hyperparameters for their experiments, and what was the effect of varying these hyperparameters on the results?
2. Can the authors provide more details on the architecture of the danger model and how it is trained, including the loss function and optimization algorithm used?
3. How do the authors plan to address the potential limitations and challenges of their approach, such as the need for a catastrophe detector and the potential for overfitting to the danger model?
Overall, I believe that the paper presents a significant contribution to the field of DRL and has the potential to improve the safety and performance of DRL agents in a wide range of applications. With some minor revisions to address the suggestions and questions outlined above, I believe that the paper will be even stronger and more effective in communicating its contributions to the reader.