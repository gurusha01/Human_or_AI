This paper proposes a novel method for finding dependent subspaces across multiple views of data, with a focus on preserving neighborhood relationships between data points. The approach is well-motivated, as it addresses a common problem in data analysis where the goal is to identify relationships between different views of the data. The method is also well-placed in the literature, as it builds upon existing work on Canonical Correlation Analysis (CCA) and other multiview learning techniques.
The paper claims to contribute a new method that directly maximizes the between-view similarity of neighborhoods of data samples, which is a natural measure for similarity of data relationships among views. The method is shown to detect nonlinear and local dependencies, has strong invariance properties, and is related to an information retrieval task of the analyst.
However, I have some concerns regarding the paper. Firstly, I am unclear about the time complexity reported by the authors, and I would like clarification on the computational cost of optimizing the objective function, including the number of iterations of the L-BFGS method. Secondly, the authors' approach is compared to CCA-based multiview learning, but I note that CCA can be solved exactly with a computational cost that is linear in data size and quadratic in dimensionality, making the proposed method seem less tractable.
Furthermore, the empirical results with synthetic data are confusing due to a convoluted data generation procedure, and I suggest using a simpler benchmark to evaluate the proposed method's performance. I also question why CCA does not recover the true subspace in the experiments, and whether this is due to the specific experimental setup or a fundamental limitation of the CCA approach.
Based on these concerns, I would like to reject the paper, but with the possibility of resubmission after addressing these issues. Specifically, I would like the authors to provide more details on the computational complexity of their method, simplify the experimental setup, and provide a more thorough comparison with existing CCA-based approaches.
To improve the paper, I suggest the authors provide more intuition on why their method is able to detect nonlinear and local dependencies, and how it relates to existing work on multiview learning. Additionally, the authors could provide more details on the optimization procedure, including the choice of hyperparameters and the number of iterations required to converge.
Some questions I would like the authors to answer include: (1) Can you provide more details on the computational complexity of your method, including the number of iterations required to converge? (2) How does your method compare to existing CCA-based approaches in terms of computational cost and accuracy? (3) Can you simplify the experimental setup and provide a more thorough comparison with existing methods? (4) Why does CCA not recover the true subspace in the experiments, and is this due to a fundamental limitation of the CCA approach?