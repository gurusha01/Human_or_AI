The paper proposes a novel framework for sequence-to-sequence prediction tasks, specifically text summarization, by introducing a 'Read-Again' model and a copy mechanism. The 'Read-Again' model allows the encoder to read the input sequence twice, with the first read biasing the second read, enabling the encoder to capture the meaning of each word in the context of the entire sentence. The copy mechanism enables the decoder to generate out-of-vocabulary words by copying them from the input sequence, reducing the need for a large vocabulary size.
I decide to reject this paper, with the primary reason being that the clarity of the presentation is compromised due to excessive content, making it difficult to follow. The paper could be split into two separate papers, one focusing on the 'Read-Again' model and the other on the copy mechanism.
The experimental setup is not clearly explained, relying heavily on references to other works, which may limit the audience and make it hard for non-experts to understand. Additionally, the significance of the work to the representation learning community is questionable, given the increasing number of submissions to the conference.
To improve the paper, I suggest the authors focus on clarifying the presentation, providing more details on the experimental setup, and highlighting the significance of their work to the representation learning community. Specifically, I would like the authors to answer the following questions: (1) How does the 'Read-Again' model compare to other attention mechanisms in terms of capturing long-range dependencies? (2) Can the copy mechanism be applied to other sequence-to-sequence tasks, such as machine translation? (3) How does the reduction in vocabulary size affect the overall performance of the model, and are there any trade-offs between vocabulary size and performance? 
In terms of the three key questions, (1) the specific question/problem tackled by the paper is the limitation of current encoder-decoder models in capturing the meaning of each word in the context of the entire sentence, (2) the approach is well-motivated, drawing inspiration from human reading habits, but could be better placed in the literature with more comparisons to other attention mechanisms, and (3) the paper supports its claims with empirical results, but could benefit from more analysis and discussion of the results.