This paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by leveraging the knowledge of pre-trained deterministic deep neural networks (DNNs). The authors introduce an intermediate model, called Simplified-SFNN, which approximates SFNN by simplifying its upper latent units above stochastic ones. This connection enables an efficient training procedure for SFNNs using the pre-trained parameters of DNNs.
The paper claims to contribute to the development of efficient training methods for large-scale SFNNs, which have several advantages over DNNs, including more expressive power and better regularization. The authors demonstrate the effectiveness of their approach on various tasks, including multi-modal learning and classification, using popular DNN architectures such as Lenet-5, NIN, and WRN.
I decide to accept this paper with minor revisions. The main reasons for this decision are:
1. The paper tackles a specific and relevant problem in the field of neural networks, namely, the efficient training of SFNNs.
2. The approach proposed by the authors is well-motivated and grounded in the literature, and they provide a clear and concise explanation of their methodology.
3. The experimental results demonstrate the effectiveness of the proposed approach, showing that SFNNs trained using the Simplified-SFNN model outperform their DNN counterparts in various tasks.
However, I have some minor concerns and suggestions for improvement:
* The paper could benefit from a more detailed analysis of the results, including a discussion of the limitations and potential drawbacks of the proposed approach.
* The authors could provide more insight into the choice of hyperparameters, such as the number of samples used for estimating the expectations in the SFNN inference.
* The paper could be improved by including more visualizations and illustrations to help readers understand the proposed methodology and results.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the computational complexity of the proposed approach compared to traditional SFNN training methods?
* How do you choose the hyperparameters, such as the number of samples used for estimating the expectations in the SFNN inference, and what is the sensitivity of the results to these hyperparameters?
* Can you provide more insight into the potential applications of the proposed approach, beyond the tasks and datasets considered in the paper?