Summary of the Paper's Contributions
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors introduce a cosine annealing schedule for the learning rate, which is reset to its initial value at each restart. They demonstrate the effectiveness of their approach, called SGDR, on several datasets, including CIFAR-10, CIFAR-100, and a downsampled version of ImageNet. The results show that SGDR achieves competitive or even better results than the current state-of-the-art methods, while requiring fewer epochs.
Decision and Reasons
I decide to reject this paper, with the main reason being the lack of novelty in the proposed approach. While the paper provides useful guidance for practitioners on selecting objective functions to train implicit generative models, the idea of warm restarts is not new and has been explored in previous works, such as those by Sugiyama et al. Additionally, the paper lacks experiments and mathematical theorems to support its claims, which makes it difficult to evaluate the significance of the contributions.
Supporting Arguments
The paper provides some empirical evidence for the effectiveness of SGDR, but the results are not comprehensive, and the comparison to other methods is limited. The authors also do not provide a clear explanation of why their approach works better than others, which makes it difficult to understand the underlying mechanisms. Furthermore, the paper does not address the potential limitations and drawbacks of SGDR, such as the choice of hyperparameters and the impact of restarts on the convergence of the algorithm.
Additional Feedback
To improve the paper, the authors should provide more comprehensive experiments, including comparisons to other state-of-the-art methods and an analysis of the sensitivity of SGDR to hyperparameters. They should also provide a more detailed explanation of the theoretical foundations of their approach and address potential limitations and drawbacks. Additionally, the authors could consider exploring the application of SGDR to other domains and tasks, such as natural language processing or reinforcement learning.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors choose the hyperparameters for SGDR, such as the initial learning rate and the restart period?
2. Can the authors provide more details on the theoretical foundations of SGDR and why it works better than other methods?
3. How do the authors plan to address the potential limitations and drawbacks of SGDR, such as the choice of hyperparameters and the impact of restarts on convergence?