Summary
The paper explores the optimization of real-valued matrices within a configurable margin about the Stiefel manifold, which is a set of orthogonal matrices. The authors propose a weight matrix factorization and parameterization strategy to bound matrix norms and control the degree of expansivity induced during backpropagation. They investigate the effect of loosening hard orthogonality constraints on convergence and performance in various tasks, including synthetic memory tasks and real-world datasets such as MNIST and Penn Treebank.
Decision
I decide to accept this paper with minor revisions. The paper is well-written, and the authors provide a clear and thorough explanation of their approach. The experiments are well-designed, and the results are interesting and insightful. However, there are some minor issues that need to be addressed, such as improving figure referencing and color coding, clarifying technical details, and providing more context for the results.
Supporting Arguments
The paper tackles a specific question/problem, which is the optimization of real-valued matrices within a configurable margin about the Stiefel manifold. The approach is well-motivated, and the authors provide a clear explanation of the background and related work. The experiments are well-designed, and the results are interesting and insightful. The paper supports its claims with empirical evidence, and the results are scientifically rigorous.
Additional Feedback
To improve the paper, I suggest the following:
* Improve figure referencing and color coding to make the figures easier to read and understand.
* Clarify technical details, such as the orthogonal projection and error measurement, to make the paper more accessible to a broader audience.
* Provide more context for the results, such as comparing the performance of the proposed approach with other state-of-the-art methods.
* Consider adding more discussion on the implications of the results and the potential applications of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
* Can you provide more details on the implementation of the geodesic gradient descent and the Cayley transformation?
* How did you choose the hyperparameters, such as the learning rate and the spectral margin, for the experiments?
* Can you provide more insight into the results on the Penn Treebank dataset, such as the performance of the proposed approach on different sentence lengths and the effect of the orthogonality constraint on the model's ability to capture long-range dependencies?