Summary
The paper proposes a novel architecture, called neural equivalence networks (EQNETs), for learning continuous semantic representations of mathematical and logical expressions. The goal is to assign continuous vectors to symbolic expressions such that semantically equivalent but syntactically diverse expressions are mapped to identical or highly similar vectors. The authors introduce a new training method, subexpression forcing, to guide the network to cluster its output to one location per equivalence class. The paper evaluates EQNETs on a diverse class of symbolic algebraic and boolean expression types, showing that they significantly outperform existing architectures.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a fundamental problem in machine learning and artificial intelligence, namely representing and inferring procedural knowledge. Secondly, the proposed architecture, EQNETs, demonstrates state-of-the-art performance on a range of datasets, outperforming existing baselines.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of learning continuous semantic representations of symbolic expressions. The authors provide a thorough analysis of the limitations of existing approaches, such as recursive neural networks (TREENNs), and propose a novel architecture that addresses these limitations. The experimental evaluation is extensive and well-designed, demonstrating the effectiveness of EQNETs on a range of datasets. The paper also provides a detailed analysis of the impact of EQNET components, such as subexpression forcing, on performance.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insight into the interpretability of the learned representations. For example, how do the learned vectors relate to the underlying syntax and semantics of the expressions? Additionally, the authors may want to consider evaluating EQNETs on more complex datasets, such as those involving multiple variables or more complex operators. Finally, the authors may want to provide more details on the computational resources required to train and evaluate EQNETs, as this may be an important consideration for practitioners.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How do the learned representations relate to the underlying syntax and semantics of the expressions?
2. Can the authors provide more insight into the impact of subexpression forcing on the learned representations?
3. How do the authors plan to address the scalability of EQNETs to more complex datasets and larger expression sizes?