This paper proposes a new regularization method for learning domain-invariant representations with neural networks by matching higher-order central moments of hidden activations. The approach, called Central Moment Discrepancy (CMD), is compared to other domain adaptation algorithms, including Maximum Mean Discrepancy (MMD), and shows comparable performance on Amazon review and Office datasets.
The paper tackles the specific question of domain-invariant representation learning in the context of unsupervised domain adaptation. The approach is well-motivated, building on the idea of matching statistical moments of hidden activations, and is well-placed in the literature, drawing on existing work on MMD and other distribution matching methods.
Based on the results presented, I would accept this paper. The key reasons for this decision are: (1) the paper proposes a novel and well-motivated approach to domain-invariant representation learning, and (2) the approach shows competitive performance with state-of-the-art methods on benchmark datasets.
However, there are some limitations and potential areas for improvement. One limitation is the assumption that hidden activations are independently distributed, which may not hold for convolutional layers with dependent neighboring activations. Additionally, the paper raises questions about the validity of Figure 3, suggesting that the performance improvement may be limited to a single class, the "mouse" class.
To improve the paper, the authors could consider addressing these limitations and providing additional evidence to support their claims. Some potential questions to answer include: (1) How does the proposed method perform on other datasets and tasks? (2) Can the method be extended to handle dependent activations in convolutional layers? (3) How does the choice of hyperparameters, such as the order of moments to match, affect the performance of the method?
Overall, this paper presents a promising new approach to domain-invariant representation learning, and with some additional work to address the limitations and provide more evidence, it has the potential to make a significant contribution to the field. 
Some additional feedback to improve the paper includes: 
- Providing more details on the experimental setup and hyperparameter tuning process.
- Including more visualizations and plots to help illustrate the results and intuition behind the method.
- Discussing potential applications and extensions of the proposed method to other areas, such as generative models and transfer learning.
- Providing more comparisons to other state-of-the-art methods and baselines to further demonstrate the effectiveness of the proposed approach. 
By addressing these points, the authors can strengthen the paper and provide a more comprehensive and convincing presentation of their work.