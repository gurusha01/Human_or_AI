Summary
The paper proposes a novel generative model, PixelVAE, which combines the strengths of Variational Autoencoders (VAEs) and PixelCNNs. PixelVAE uses a conditional PixelCNN as the decoder in a VAE, allowing it to model complex dependencies between pixels while still learning a useful latent representation. The authors demonstrate the effectiveness of PixelVAE on several datasets, including MNIST, LSUN bedrooms, and 64x64 ImageNet, achieving state-of-the-art performance on MNIST and competitive results on the other datasets.
Decision
I decide to Accept this paper, with the main reason being the novelty and effectiveness of the proposed PixelVAE model. The paper presents a clear and well-motivated approach to addressing the limitations of existing generative models, and the experimental results demonstrate the potential of PixelVAE to achieve high-quality image generation and representation learning.
Supporting Arguments
The paper provides a thorough analysis of the strengths and weaknesses of existing generative models, including VAEs and PixelCNNs. The authors clearly motivate the need for a model that combines the benefits of both approaches, and the proposed PixelVAE model is well-designed to address this need. The experimental results are comprehensive and demonstrate the effectiveness of PixelVAE on several datasets. Additionally, the paper provides a detailed analysis of the learned representations and the role of the autoregressive decoder in capturing local dependencies.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the training procedure and the hyperparameter settings used for each experiment. Additionally, it would be helpful to include more visualizations of the learned representations and the generated images, to provide a better understanding of the model's capabilities. Furthermore, the authors may want to consider exploring the application of PixelVAE to other domains, such as video generation or image-to-image translation.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the computational cost of training PixelVAE compared to PixelCNN and VAE?
2. How do you select the number of autoregressive layers and the receptive field size for each dataset?
3. Can you provide more insights into the learned representations and how they differ from those learned by VAEs and PixelCNNs?
4. Have you explored the use of PixelVAE for other tasks, such as semi-supervised learning or image segmentation?