Summary
The paper proposes a novel neural network architecture called FractalNet, which is based on a self-similar design principle. The authors demonstrate that FractalNet can match the performance of residual networks (ResNets) on various image classification tasks, including CIFAR and ImageNet, without relying on residual connections. The paper also introduces a regularization technique called drop-path, which prevents co-adaptation of parallel paths in the network and allows for the extraction of high-performance subnetworks.
Decision
I decide to accept this paper, with the primary reason being that the approach is well-motivated and the results are scientifically rigorous. The authors provide a clear and concise explanation of their design principle and experimental methodology, and the results demonstrate the effectiveness of FractalNet and drop-path.
Supporting Arguments
The paper tackles a specific question of whether residual connections are necessary for training ultra-deep neural networks. The authors provide a well-motivated approach by introducing a novel architecture that does not rely on residuals, and they demonstrate its effectiveness through extensive experiments. The results show that FractalNet can match the performance of ResNets on various tasks, and the drop-path regularization technique can prevent overfitting and allow for the extraction of high-performance subnetworks.
Additional Feedback
To improve the paper, I suggest that the authors provide more analysis on the internal behavior of FractalNet, such as visualizing the feature representations learned by the network and analyzing the effect of drop-path on the network's performance. Additionally, the authors may want to consider comparing FractalNet with other state-of-the-art architectures, such as DenseNets, to further demonstrate its effectiveness.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* Can you provide more insight into the design principle behind FractalNet, and how it relates to other architectures such as ResNets and DenseNets?
* How does the drop-path regularization technique affect the network's performance on different tasks, and are there any limitations to its applicability?
* Can you provide more analysis on the internal behavior of FractalNet, such as visualizing the feature representations learned by the network, to gain a deeper understanding of its strengths and weaknesses?