This paper introduces a novel approach to incorporating prior procedural knowledge into neural networks through the use of a differentiable interpreter for the Forth programming language. The authors propose a neural implementation of the Forth abstract machine, which allows for the integration of code and neural networks, enabling the training of neural networks specified by slots via backpropagation through code interpretation.
The paper claims to contribute to the field of neural programming by introducing a differentiable abstract machine for the Forth programming language, which can be used to complement a programmer's prior knowledge through the learning of unspecified behavior in Forth sketches. The authors demonstrate the effectiveness of their approach on two tasks: sorting and addition.
I decide to accept this paper with two key reasons: (1) the paper introduces a novel and well-motivated approach to incorporating prior procedural knowledge into neural networks, and (2) the experimental results demonstrate the effectiveness of the proposed approach on two tasks.
The approach is well-motivated, as it addresses the problem of incorporating prior knowledge into neural networks, which is a long-standing challenge in the field. The use of a differentiable interpreter for the Forth programming language is a clever idea, as it allows for the integration of code and neural networks. The experimental results are also strong, as they demonstrate the ability of the proposed approach to learn complex tasks with minimal training data.
However, I have some concerns regarding the paper. Firstly, the paper could benefit from more qualitative analysis of what the prior modes represent, and how they relate to the tasks being performed. Secondly, the paper could benefit from more experiments on standard datasets, such as MNIST, to demonstrate the broad applicability of the proposed variational family.
To improve the paper, I suggest that the authors provide more details on the implementation of the differentiable Forth abstract machine, and how it is used to train the neural networks. Additionally, the authors could provide more analysis on the types of prior knowledge that are most effective in improving the performance of the neural networks.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the proposed approach to more complex tasks and domains? (2) How do the authors plan to address the issue of incongruencies between traditional language properties and the desire for neural networks to learn behaviors that generalize to unseen data? (3) How do the authors plan to investigate the relationship between backpropagation and programming language semantics, and what implications does this have for the design of neural networks?