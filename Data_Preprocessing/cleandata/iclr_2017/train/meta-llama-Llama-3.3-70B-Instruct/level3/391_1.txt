The paper proposes a method for pruning weights in neural networks to obtain sparse solutions, which can lead to large savings in test-time computations without affecting task performance. The approach is applied to an RNN-based system and evaluated on a speech recognition dataset, showing substantial computational gains and improved evaluation performance in some cases. The authors demonstrate that their pruning technique can reduce the number of parameters in RNNs by up to 90% while maintaining or even improving accuracy.
Based on the provided guidelines, I will evaluate the paper and make a decision to accept or reject it. 
The specific question tackled by the paper is how to reduce the number of parameters in RNNs while maintaining or improving accuracy. The approach is well-motivated, as the authors discuss the limitations of current RNN models and the need for more efficient deployment on mobile devices and server farms. However, the novelty of the approach is limited, as it is similar to earlier work, particularly Han et al.
I decide to reject the paper, with the main reason being the lack of novelty and comparison with other pruning methods. The paper does not provide a thorough comparison with other state-of-the-art pruning techniques, which makes it difficult to assess the effectiveness of the proposed method. Additionally, the use of a private dataset and the lack of detailed explanations of the method and its motivations are notable limitations.
To improve the paper, I suggest that the authors provide a more detailed comparison with other pruning methods and include experiments on public datasets to allow for replication of the results. Furthermore, the authors should provide more detailed explanations of the method and its motivations, as well as a more thorough discussion of the potential future speed-ups and how they relate to the proposed pruning algorithm. 
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the proposed pruning technique compare to other state-of-the-art pruning methods in terms of accuracy and computational efficiency? Can the authors provide more detailed explanations of the hyperparameters used in the pruning algorithm and how they were chosen? How do the authors plan to address the limitations of the current sparse matrix-vector libraries and implement optimal small batch sparse matrix-dense vector routines for GPUs and ARM processors?