The paper proposes a novel extension to the Variational Autoencoder (VAE) framework, called the Stick-Breaking Variational Autoencoder (SB-VAE), which utilizes stochastic latent dimensionality through an infinite prior, the stick-breaking process, and inference tailored to this model. The SB-VAE model also has a semi-supervised extension and uses the Kumaraswamy distribution as an approximate variational posterior, drawing attention to its potential use in machine learning.
I decide to accept this paper with two key reasons for this choice. Firstly, the paper tackles a specific and interesting problem in the field of deep generative models, which is the extension of Variational Autoencoders to Bayesian nonparametric processes. Secondly, the approach is well-motivated and the paper provides a clear and well-organized structure, making it easy to follow and understand.
The paper supports its claims through a series of experiments on various image datasets, demonstrating the effectiveness of the SB-VAE model in learning highly discriminative latent representations that often outperform the Gaussian VAE's. The results show that the SB-VAE model is able to capture class structure and preserve class boundaries, and that it provides beneficial regularization for semi-supervised learning.
To improve the paper, I would suggest providing more clarification on the methodology, particularly on the conditioning of the model directly on the stick-breaking weights, which seems unusual and requires further explanation. Additionally, the motivation behind reformulating the VAE as a SB-VAE could be more thoroughly demonstrated and compared to other methods. It would also be helpful to provide more details on the implementation and optimization of the SB-VAE model, such as the choice of hyperparameters and the computational cost of assembling the stick segments.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How does the stick-breaking process affect the capacity of the model, and how does it compare to other methods for controlling model capacity? How does the choice of the Kumaraswamy distribution as an approximate variational posterior affect the results, and are there other distributions that could be used instead? What are the potential applications of the SB-VAE model beyond image datasets, and how could it be extended to other types of data?