Summary of the Paper's Contributions
The paper proposes a novel recurrent neural network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which provably preserves forward hidden state activation norm and backpropagated gradient norm, thus alleviating the vanishing and exploding gradients problem. The authors also introduce a rotation plane parameterization to represent orthogonal matrices, allowing for efficient computation and optimization. The paper demonstrates the effectiveness of the DORNN architecture on a simplified memory copy task, achieving long-term dependencies of up to 5,000 time steps.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The paper tackles a significant problem in RNNs, namely the vanishing and exploding gradients issue, and proposes a novel and theoretically sound solution.
2. The authors provide a thorough analysis of the DORNN architecture, including its ability to preserve norm and gradient norm, and demonstrate its effectiveness on a challenging task.
Supporting Arguments
The paper's contributions are well-motivated, and the approach is well-placed in the literature. The authors provide a clear and concise explanation of the DORNN architecture and its properties, making it easy to follow and understand. The experimental results are impressive, demonstrating the ability of the DORNN to learn long-term dependencies that are significantly longer than those achieved by previous approaches.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
1. Providing more detailed comparisons with other state-of-the-art RNN architectures, such as LSTMs and GRUs, to better understand the advantages and limitations of the DORNN.
2. Exploring the application of the DORNN to more complex tasks, such as natural language processing or speech recognition, to demonstrate its practical utility.
3. Investigating the use of other parameterization methods for orthogonal matrices, such as the Cayley transform, to potentially improve the efficiency and effectiveness of the DORNN.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the potential limitation of the DORNN architecture, namely its linear input-dependent transition, which may not be sufficient for modeling complex dependencies?
2. Can the authors provide more insight into the optimization process of the DORNN, particularly with regards to the rotation plane parameterization, and how it affects the training time and stability of the model?
3. Are there any plans to explore the use of the DORNN in combination with other RNN architectures, such as LSTMs or GRUs, to potentially leverage the strengths of each approach?