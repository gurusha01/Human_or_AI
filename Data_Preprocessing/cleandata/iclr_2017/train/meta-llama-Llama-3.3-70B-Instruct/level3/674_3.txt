Summary
The paper proposes a novel technique for collective operations, referred to as Linear Pipelining (LP), to reduce the cost of communication in parallel training of neural networks. The authors demonstrate that LP provides an O(log p) speedup over Minimal Spanning Tree (MST) collectives and up to a 2x speedup over Bidirectional Exchange (BE) collectives. The paper also presents a theoretical analysis of the cost model and evaluates the performance of LP collectives in training large-scale neural networks.
Decision
I decide to Accept this paper with some minor revisions. The key reasons for this choice are:
1. The paper tackles a specific and important problem in parallel training of neural networks, which is the reduction of communication overhead.
2. The approach is well-motivated and placed in the literature, with a clear explanation of the limitations of existing methods and the benefits of the proposed LP technique.
Supporting Arguments
The paper provides a thorough analysis of the cost model and evaluates the performance of LP collectives in various scenarios. The experimental results demonstrate significant speedups over existing methods, which supports the claims made in the paper. Additionally, the paper provides a clear explanation of the LP technique and its implementation, which makes it easy to understand and replicate.
Additional Feedback
To improve the paper, I suggest the following:
* Provide more details on the implementation of the LP technique, such as the block size and the number of GPUs used in the experiments.
* Consider adding more related work on parallel training of neural networks, such as the use of asynchronous SGD and decentralized optimization methods.
* Provide more insights on the trade-offs between the different collective algorithms and the impact of model size and batch size on the communication time.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the block size and the number of GPUs used in the experiments?
* How do you handle the case where the model size is larger than the available memory on a single GPU?
* Can you provide more insights on the impact of the LP technique on the convergence properties of the SGD algorithm?