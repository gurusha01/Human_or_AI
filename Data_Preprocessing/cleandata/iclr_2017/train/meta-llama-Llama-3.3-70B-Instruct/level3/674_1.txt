Summary
The paper proposes a new technique for collective operations, referred to as Linear Pipelining (LP), to reduce the cost of communication in parallel training of neural networks. The authors demonstrate that LP provides an O(log p) speedup over Minimal Spanning Tree (MST) collectives and up to a 2 times speedup over Bidirectional Exchange (BE) collectives as the message size increases. The paper also presents a theoretical analysis of the cost of LP, MST, and BE collectives and evaluates the performance of LP collectives in training large-scale neural networks.
Decision
I decide to accept this paper with minor revisions. The paper tackles a specific question of reducing communication overhead in parallel training of neural networks and proposes a well-motivated approach. The theoretical analysis and experimental results demonstrate the effectiveness of the proposed LP collectives.
Supporting Arguments
The paper provides a clear and concise explanation of the problem and the proposed solution. The theoretical analysis is thorough and well-supported by experimental results. The authors demonstrate the scalability of the LP collectives and their impact on the neural network training time. The paper is well-organized, and the writing is clear and concise.
Additional Feedback
To further improve the paper, I suggest the authors provide more details on the implementation of the LP collectives and the experimental setup. Additionally, it would be helpful to include more results on the convergence properties of the proposed BSP SGD algorithms. The authors may also consider discussing the potential applications of the proposed LP collectives in other domains.
Questions for the Authors
1. Can you provide more details on the implementation of the LP collectives, including the block size and the number of GPUs used in the experiments?
2. How do the authors plan to address the potential issue of precision loss in the gradient update due to the use of float multiplications in the allreduce operation?
3. Can you provide more results on the convergence properties of the proposed BSP SGD algorithms, including the iteration profiles and the loss curves?