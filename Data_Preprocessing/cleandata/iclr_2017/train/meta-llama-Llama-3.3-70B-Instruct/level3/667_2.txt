Summary of the Paper's Contributions
The paper proposes a novel approach to integrating domain knowledge into word/code embeddings using a direct acyclic graph (DAG) structure, which is then used to train a Recurrent Neural Network (RNN) with limited data. The model embeds non-leaf nodes in the DAG and combines code and ancestor embeddings using a convex sum, acting as an attention mechanism over the representation. The approach is evaluated on two medical datasets, demonstrating the effectiveness of both the DAG and pretraining embeddings in achieving the best performance.
Decision and Key Reasons
Based on the evaluation, I decide to Accept this paper. The two key reasons for this choice are: (1) the paper introduces a well-motivated approach to leveraging domain knowledge in word/code embeddings, which is a significant contribution to the field; and (2) the experimental results demonstrate the effectiveness of the proposed approach in achieving state-of-the-art performance on two medical datasets.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of integrating domain knowledge into word/code embeddings, highlighting the limitations of existing approaches. The proposed DAG-based approach is well-motivated and grounded in the literature. The experimental evaluation is thorough, with a clear description of the datasets, experimental setup, and results. The paper also provides a detailed analysis of the results, discussing the contribution of the DAG and pretraining embeddings to the overall performance.
Additional Feedback and Questions
To further improve the paper, I would like to see more discussion on the choice of the convex sum as an attention mechanism and its impact on the performance. Additionally, it would be interesting to see an analysis of the learned embeddings and how they capture the domain knowledge. Some questions I would like the authors to address are: (1) How does the choice of the DAG structure affect the performance of the model? (2) Can the proposed approach be applied to other domains beyond medical codes? (3) How does the model handle out-of-vocabulary words or codes that are not present in the training data? 
These points are not necessarily part of my decision assessment but are rather intended to help the authors improve their work. Overall, the paper presents a significant contribution to the field, and with some additional clarification and analysis, it has the potential to be a strong publication.