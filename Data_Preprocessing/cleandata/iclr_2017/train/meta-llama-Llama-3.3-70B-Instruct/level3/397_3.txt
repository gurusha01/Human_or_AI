Summary of the Paper's Contributions
The paper presents a novel technique called PGQL, which combines policy gradient with Q-learning in a reinforcement learning setting. The authors establish a connection between the fixed points of regularized policy gradient algorithms and the Q-values of the resulting policy, showing that for small regularization, the Bellman residual of the induced Q-values is small. This leads to the development of PGQL, which adds an auxiliary update to the policy gradient that reduces the Bellman residual. The authors demonstrate the effectiveness of PGQL on a suite of Atari games, achieving performance exceeding that of both A3C and Q-learning.
Decision and Reasons
Based on the review, I decide to accept the paper. The main reasons for this decision are:
1. The paper presents a novel and well-motivated technique that combines policy gradient with Q-learning, addressing a significant problem in reinforcement learning.
2. The authors provide a clear and well-written explanation of the technique, including a thorough analysis of the connection between regularized policy gradient algorithms and Q-values.
3. The experimental results demonstrate the effectiveness of PGQL on a range of Atari games, showing improved data efficiency and stability compared to existing methods.
Supporting Arguments
The paper provides a thorough analysis of the connection between regularized policy gradient algorithms and Q-values, which is a key contribution of the work. The authors also demonstrate the effectiveness of PGQL on a range of Atari games, using a neural network to parameterize the policy. The results show that PGQL outperforms both A3C and Q-learning in terms of data efficiency and stability, making it a promising technique for reinforcement learning.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of PGQL, including the choice of hyperparameters and the architecture of the neural network used to parameterize the policy. Additionally, it would be helpful to include more analysis on the trade-offs between the policy gradient and Q-learning updates, and how these trade-offs affect the performance of PGQL.
Questions for the Authors
1. Can you provide more details on the choice of hyperparameters for PGQL, including the learning rate, entropy penalty, and batch size?
2. How do you select the architecture of the neural network used to parameterize the policy, and what are the key factors that affect the performance of PGQL?
3. Can you provide more analysis on the trade-offs between the policy gradient and Q-learning updates, and how these trade-offs affect the performance of PGQL?