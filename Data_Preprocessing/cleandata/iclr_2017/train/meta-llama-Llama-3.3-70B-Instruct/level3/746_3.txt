Summary
The paper proposes a novel approach to training models for combinatorial search procedures using dynamic programming and scale invariance property. The authors introduce a neural cache model that adds a cache-like memory to neural network language models, allowing them to adapt to their recent history and capture long-range dependencies in documents. The model is shown to be efficient, scalable, and achieves state-of-the-art performance on several language modeling datasets.
Decision
I decide to reject this paper, with the main reason being the lack of clear motivation for the approach and the poor readability of the paper. The authors fail to provide a clear problem formulation, and the paper's organization is chaotic, making it hard to understand the overall methodology.
Supporting Arguments
The paper lacks a clear motivation for using dynamic programming and scale invariance property, and the authors do not provide a thorough explanation of how these concepts are applied to the neural cache model. Additionally, the paper's organization is confusing, with the authors switching between low-level and high-level details without providing a clear overview of the methodology. The authors also fail to explain how discrete choices made during the split and merge phases are backpropagated through the network in an unbiased manner.
Additional Feedback
To improve the paper, I suggest that the authors provide a clear problem formulation and a more readable structure, starting with an introduction to the neural cache model and its components. The authors should also provide more details on the training procedure and the hyperparameter tuning process. Furthermore, the authors should consider adding more visualizations and examples to illustrate the methodology and the results.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a clear motivation for using dynamic programming and scale invariance property in the neural cache model?
2. How do you backpropagate discrete choices made during the split and merge phases through the network in an unbiased manner?
3. Can you provide more details on the training procedure and the hyperparameter tuning process?
4. How do you plan to address the issue of poor readability and provide a more clear and concise explanation of the methodology?