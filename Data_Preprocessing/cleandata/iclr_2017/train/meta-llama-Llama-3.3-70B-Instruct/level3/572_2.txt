Summary
The paper proposes a method for synchronous stochastic optimization with backup workers to mitigate stragglers in distributed deep learning. The authors argue that asynchronous stochastic optimization, which is commonly used in distributed deep learning, suffers from gradient staleness, leading to poorer test accuracy. They propose a synchronous approach with backup workers, which avoids gradient staleness and reduces the impact of stragglers. The authors demonstrate the effectiveness of their approach through experiments on several deep learning models, including Inception and PixelCNN.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's novelty and originality are limited, as it applies well-known methods to a new problem without developing novel algorithms specifically for generative models. Secondly, the clarity and presentation of the paper are unsatisfying, with a disjointed structure and unrelated experiments due to significant changes between revisions.
Supporting Arguments
The paper tackles an important problem in distributed deep learning, but the approach proposed is not particularly novel or original. The use of backup workers to mitigate stragglers is a well-known technique, and the authors do not provide significant new insights or improvements. Additionally, the paper's structure and presentation are confusing, with multiple experiments and results that are not well-connected. The authors also fail to provide a clear and concise summary of their contributions and results.
Additional Feedback
To improve the paper, the authors should focus on providing a clear and concise summary of their contributions and results. They should also reorganize the paper to have a more logical structure, with related experiments and results presented together. Additionally, the authors should provide more detailed analysis and discussion of their results, including the implications of their findings and the potential limitations of their approach.
Questions for the Authors
I would like the authors to clarify the following points:
* How do the authors plan to address the issue of gradient staleness in asynchronous stochastic optimization, and what are the potential limitations of their approach?
* Can the authors provide more detailed analysis and discussion of their results, including the implications of their findings and the potential limitations of their approach?
* How do the authors plan to improve the clarity and presentation of the paper, and what changes do they propose to make to address the issues mentioned in this review?