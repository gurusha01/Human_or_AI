Summary
The paper explores the concept of sample importance in deep neural networks, which refers to the contribution of each sample to the change in model parameters during training. The authors propose a quantitative measurement of sample importance and conduct empirical analysis on two standard datasets, MNIST and CIFAR-10. The results show that easy samples shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also investigate the effect of batch construction on training and find that mixing hard and easy samples in each batch leads to better performance.
Decision
I decide to accept this paper, with the main reason being that it tackles an interesting and important problem in deep learning, and the approach is well-motivated and supported by empirical evidence.
Supporting Arguments
The paper provides a clear and concise introduction to the concept of sample importance and its relevance to deep learning. The proposed measurement of sample importance is well-defined and easy to understand. The empirical analysis is thorough and provides valuable insights into the behavior of sample importance in different layers and training stages. The results on batch construction are also interesting and have implications for training deep neural networks.
Additional Feedback
To improve the paper, I suggest that the authors provide more discussion on the implications of their findings for deep learning practice. For example, how can the sample importance be used to improve training efficiency or robustness? Additionally, the authors may want to consider exploring the sample importance in other deep learning architectures, such as convolutional neural networks or recurrent neural networks.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to extend their analysis to other deep learning architectures?
* Can the sample importance be used to identify and remove redundant or noisy samples in the training dataset?
* How does the sample importance relate to other concepts in deep learning, such as attention or feature importance?