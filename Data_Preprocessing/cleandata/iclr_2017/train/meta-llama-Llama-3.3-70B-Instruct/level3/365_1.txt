This paper proposes a novel approach to compressing deep neural networks by introducing a density-diversity penalty regularizer that encourages high sparsity and low diversity in the trained weight matrices. The method is well-motivated, as it addresses the issue of reducing the memory and computational costs of deep neural networks, particularly for fully-connected layers.
The approach is well-placed in the literature, as it builds upon existing work on compressing neural networks, such as the "deep compression" method. The authors provide a clear and detailed explanation of their method, including the density-diversity penalty regularizer, the sorting trick for efficient optimization, and the weight-tying strategy.
The paper supports its claims with empirical results on the MNIST and TIMIT datasets, demonstrating that the proposed method achieves significant compression rates without performance loss. The results are impressive, with compression rates of up to 226X on fully-connected layers.
Based on the provided information, I decide to Accept this paper. The key reasons for this choice are:
1. The paper proposes a novel and well-motivated approach to compressing deep neural networks.
2. The method is well-supported by empirical results on benchmark datasets.
Supporting arguments for these reasons include:
* The density-diversity penalty regularizer is a clever and effective way to encourage high sparsity and low diversity in the trained weight matrices.
* The sorting trick for efficient optimization is a significant contribution, as it makes the density-diversity penalty computationally feasible.
* The weight-tying strategy is a useful technique for reducing the number of distinct values in the weight matrices.
Additional feedback to improve the paper includes:
* Clarifying the result tables and figures to make them easier to understand.
* Correcting the English mistake in the paper.
* Providing more details on the hyperparameter tuning process and the sensitivity of the results to the choice of hyperparameters.
Questions I would like answered by the authors to clarify my understanding of the paper and provide additional evidence include:
* Can the authors provide more details on the computational cost of the sorting trick and how it compares to other optimization methods?
* How does the density-diversity penalty regularizer affect the generalization performance of the network, and are there any trade-offs between compression and generalization?
* Can the authors provide more results on other datasets and tasks to demonstrate the broader applicability of their method?