The paper proposes a novel approach to neural network language models by introducing a neural cache model that adapts the prediction to the recent history. The authors claim that their method outperforms existing memory-augmented neural networks and can be applied to pre-trained neural networks without fine-tuning. The paper is well-written, particularly in section 2, which provides a clear overview of language modeling and the motivation behind the proposed approach.
However, I decide to reject this paper due to two key reasons. Firstly, the method has a significant drawback in that it may not be feasible for larger networks, which limits its applicability in production environments. Secondly, the results presented in the paper are inconsistent, and the intuition behind some of the findings is unclear. For example, the large number of points with improved accuracy in Figure 2 is not fully explained, and the difference in performance between the linear interpolation and global normalization methods is not adequately discussed.
To support my decision, I would like to highlight that the paper lacks relevant details about the used models, such as the architecture and hyperparameters. Additionally, the paper has several minor issues, including formatting errors, missing TeX commands, and typos, which need to be corrected. The authors should also provide more information about the computational cost of their method and how it compares to existing approaches.
To improve the paper, I suggest that the authors provide more detailed explanations of their results, including the intuition behind the findings and the limitations of their approach. They should also consider evaluating their method on larger networks and providing more information about the computational cost and scalability of their approach. Furthermore, the authors should address the inconsistencies in their results and provide more detailed comparisons with existing methods.
I would like to ask the authors to clarify the following points: (1) How do they plan to address the limitation of their method for larger networks? (2) Can they provide more detailed explanations of the results in Figure 2 and the difference in performance between the linear interpolation and global normalization methods? (3) How does the computational cost of their method compare to existing approaches, and what are the implications for scalability?