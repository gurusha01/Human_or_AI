Summary of the Paper's Contributions
The paper proposes a novel Neural Knowledge Language Model (NKLM) that incorporates symbolic knowledge from a knowledge graph into a recurrent neural network (RNN) language model. The model predicts whether a word is based on a fact or not and generates words either from the vocabulary or by copying from the fact description. The paper also introduces a new dataset, WikiFacts, which aligns words with Freebase facts and corresponding Wikipedia descriptions. The experiments show that the NKLM significantly improves perplexity and generates named entities that are not observed during training.
Decision and Key Reasons
I decide to accept this paper with minor revisions. The key reasons for this decision are: (1) the paper proposes a novel and well-motivated approach to incorporating knowledge into language models, and (2) the experiments demonstrate significant improvements in perplexity and named entity generation.
Supporting Arguments
The paper provides a clear and well-structured presentation of the NKLM model, including its architecture, training objective, and inference procedure. The experiments are well-designed and demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities. The introduction of the WikiFacts dataset and the Unknown-Penalized Perplexity (UPP) metric are also significant contributions.
Additional Feedback
To improve the paper, I suggest the following: (1) clarify the notation in Section 3, particularly with regards to the entity and fact embeddings; (2) consider adding a latent variable to decide whether to generate a fact or not, as this may improve the model's efficiency; (3) discuss the limitations of the model, particularly with regards to its requirement for annotated data and its potential performance on out-of-vocabulary words; and (4) compare the NKLM to other language models, such as character-level models, which may provide an alternative approach to handling unknown words.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do you plan to extend the NKLM to handle more complex knowledge graphs and larger datasets? (2) Can you provide more details on the WikiFacts dataset, including its size, coverage, and annotation process? (3) How do you evaluate the quality of the generated named entities, and what metrics do you use to measure their accuracy? (4) Have you considered applying the NKLM to other natural language processing tasks, such as question answering or text summarization?