Summary
The paper introduces a novel recurrent neural network architecture, multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM's ability to have different recurrent transition functions for each possible input makes it more expressive for autoregressive density estimation. The paper demonstrates the effectiveness of mLSTM on various character-level language modeling tasks, showing improvements over standard LSTM and its deep variants.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated and the results are promising. The authors provide a clear explanation of the limitations of existing RNN architectures and demonstrate how mLSTM addresses these limitations.
Supporting Arguments
The paper provides a thorough analysis of the strengths and weaknesses of existing RNN architectures, including LSTM and mRNN. The authors clearly explain how mLSTM combines the benefits of these architectures, allowing for flexible input-dependent transitions while maintaining the ability to control information flow. The experimental results demonstrate the effectiveness of mLSTM on various character-level language modeling tasks, including the Hutter Prize dataset.
Additional Feedback
To further improve the paper, I suggest that the authors provide more theoretical analysis of the mLSTM architecture, including a more detailed explanation of how the factorized hidden weights and gating units interact. Additionally, it would be helpful to see more comparisons with other state-of-the-art RNN architectures, such as gated recurrent units (GRUs) and recurrent highway networks (RHNs). The authors may also consider exploring the application of mLSTM to other sequence modeling tasks, such as word-level language modeling and speech recognition.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the design choices behind the mLSTM architecture, particularly the decision to combine the factorized hidden weights of mRNN with the gating units of LSTM?
2. How do you plan to address the potential limitations of mLSTM, such as the increased computational cost and the need for careful hyperparameter tuning?
3. Can you provide more details on the dynamic evaluation procedure used in the experiments, including the specific hyperparameters and training protocols employed?