Summary
This paper proposes various optimization formulations to enforce orthogonality in RNNs' recurrent weight matrix, with results showing that soft orthogonality or orthogonal initialization can improve learning. The authors explore the issues of optimization convergence, speed, and gradient stability using a variety of different methods for encouraging or enforcing orthogonality. The experiments demonstrate that enforcing exact orthogonality does not aid learning, while soft orthogonality constraints and orthogonal initialization can be beneficial and relatively inexpensive for practical use.
Decision
I decide to Accept this paper with two key reasons: (1) the paper provides a thorough investigation of the effects of orthogonality constraints on RNNs, and (2) the experimental results are well-done, yielding useful insights into the impact of soft orthogonality on learning stability.
Supporting Arguments
The paper's findings suggest that orthogonality is useful as an initialization technique, providing an optimization speed advantage, rather than as a regularizer that fundamentally changes the optimization problem. The authors also highlight the importance of separating the optimization speed advantage conferred by orthogonality from its regularization advantage, with orthogonal weights performing better among models that attain similar training loss. Additionally, the paper explores the effect of loosening hard orthogonality constraints and demonstrates that this can increase the rate of convergence, while also showing that orthogonal initialization appears to significantly outperform other initialization methods.
Additional Feedback
To further improve the paper, I suggest that the authors consider optimizing the learning rate for each regularization strength separately, as this could provide further insights into the impact of soft orthogonality on learning stability. Additionally, initializing to orthogonal matrices multiplied by a small factor may confer advantages over standard orthogonal matrices, particularly on certain tasks like the T=10K copy task. The authors may also want to explore the use of different activation functions, such as the orthogonal permutation linear unit (OPLU), which preserves norm and makes a fully norm-preserving RNN possible.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) Can you provide more details on the implementation of the geodesic gradient descent algorithm and how it is used to update the orthogonal bases? (2) How do the results change when using different optimization algorithms, such as Adam or SGD with momentum? (3) Can you provide more insights into the effect of the spectral margin on the convergence rate and model performance, particularly for longer sequence lengths?