Summary
The paper explores the concept of sample importance in deep neural networks, which refers to the contribution of each sample to the change in model parameters during training. The authors propose a method to quantify sample importance and demonstrate its application in understanding how different samples impact the training process. They show that easy samples tend to shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also investigate the effect of batch construction on training and find that mixing hard and easy samples in each batch leads to better performance.
Decision
I decide to accept this paper, with the main reason being that it provides a novel and well-motivated approach to understanding the role of individual samples in deep neural network training. The paper is well-written, and the experiments are thorough and well-designed.
Supporting Arguments
The paper tackles a specific and interesting question, namely, how different samples contribute to the training process in deep neural networks. The approach is well-motivated, and the authors provide a clear and concise explanation of their method. The experiments are comprehensive, and the results are convincing, demonstrating the effectiveness of the proposed approach. The paper also provides a good discussion of the implications of the results and potential future directions.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of calculating sample importance and how it can be applied to larger datasets. Additionally, it would be interesting to see more analysis on the relationship between sample importance and other concepts, such as attention mechanisms or feature importance. The authors may also want to consider providing more visualizations of the sample importance values to help illustrate the results.
Questions for the Authors
I would like the authors to clarify the following points:
1. How do the authors plan to extend their approach to more complex deep learning structures, such as convolutional neural networks or recurrent neural networks?
2. Can the authors provide more insights into the relationship between sample importance and the learned representations in the network?
3. How do the authors think their approach can be used in practice to improve the training of deep neural networks, for example, by selecting a subset of the most important samples for training?