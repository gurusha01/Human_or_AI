Summary of the Paper's Contributions
The paper proposes a novel approach to training sparse Recurrent Neural Networks (RNNs) by masking weights below a threshold to zero and using a schedule for the threshold to improve model performance. The authors provide experimental results on a Baidu-internal task, demonstrating that applying sparsification to a large model can lead to better performance and fewer non-zero parameters than a dense baseline model. The paper also highlights the potential for faster model evaluation with sufficiently sparse models using the cuSPARSE library.
Decision and Key Reasons
Based on the reviewer guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper tackles an important problem in deep learning, namely deploying models at low computational and memory cost, and proposes a well-motivated approach to address this challenge.
2. The paper provides empirical results that support the claims made by the authors, demonstrating the effectiveness of their approach in improving model performance and reducing the number of non-zero parameters.
Supporting Arguments
The paper is well-structured and easy to follow, with clear explanations of the proposed approach and the experimental results. The authors provide a thorough comparison to previous work, including the sparsity method of Yu et al. (2012), which makes the results more plausible. The use of a schedule for the threshold is a novel contribution, and the authors demonstrate its effectiveness in improving model performance.
Additional Feedback
To further improve the paper, I suggest that the authors compare their approach to "distillation" approaches as a second baseline to strengthen the results. Additionally, the authors may want to consider providing more details on the implementation of the cuSPARSE library and the potential limitations of their approach.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence to support my assessment, I would like the authors to answer the following questions:
1. Can you provide more details on the schedule used for the threshold, and how it was determined?
2. How do you plan to extend your approach to other types of neural networks, such as convolutional neural networks?
3. Can you provide more information on the computational resources required to train the sparse RNNs, and how they compare to the resources required for training dense RNNs?