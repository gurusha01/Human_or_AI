Summary of the Paper's Contributions
The paper proposes a novel Neural Knowledge Language Model (NKLM) that combines the strengths of Recurrent Neural Network Language Models (RNNLM) with the symbolic knowledge from a knowledge graph. The NKLM is designed to overcome the limitations of traditional language models in handling unknown words, particularly named entities, by incorporating a knowledge-copy mechanism that allows the model to generate words not in the predefined vocabulary. The paper introduces a new dataset, WikiFacts, and a new evaluation metric, Unknown-Penalized Perplexity (UPP), to assess the performance of the NKLM.
Decision and Key Reasons
Based on the review, I decide to accept the paper with minor revisions. The key reasons for this decision are: (1) the paper proposes a novel and well-motivated approach to addressing the limitations of traditional language models, and (2) the experimental results demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities.
Supporting Arguments
The paper provides a clear and well-structured presentation of the NKLM, including its architecture, training, and evaluation. The experimental results show that the NKLM outperforms the RNNLM in terms of perplexity and generates a much smaller number of unknown words. The introduction of the WikiFacts dataset and the UPP metric is also a significant contribution to the field. However, the writing quality, particularly in Section 3, could be improved for better clarity and readability.
Additional Feedback and Questions
To further improve the paper, I suggest the authors address the following questions and concerns:
* How does the NKLM perform on standard benchmarks, such as the Penn Treebank dataset?
* What is the training time for the NKLM, and how does it compare to the RNNLM?
* How important is the knowledge context in the NKLM, and can the model still perform well without it?
* How are the fact embeddings initialized, and what is the impact of using different initialization methods?
* How does the NKLM encode copied unknown words, and what are the implications for downstream tasks?
Overall, the paper presents a significant contribution to the field of language modeling, and with minor revisions, it has the potential to be a strong publication.