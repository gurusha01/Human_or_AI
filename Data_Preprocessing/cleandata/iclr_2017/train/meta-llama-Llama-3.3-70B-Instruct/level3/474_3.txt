Summary
The paper proposes a novel framework for neural language models, introducing a key-value-predict attention mechanism that separates output vectors into distinct components for querying a memory of previous token representations. This approach outperforms existing memory-augmented neural language models on two corpora, demonstrating the effectiveness of this new architecture. Additionally, the authors find that a simpler N-gram RNN model, which uses a concatenation of previous output representations, achieves comparable results to more complex models.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper lacks a thorough comparison with previous versions and other baseline models, such as LSTM generative models, which raises concerns about the effectiveness of the proposed approach. Secondly, the paper is difficult to read due to the use of technical music terms and overly complicated math symbols and concepts, hindering understanding of the system's performance.
Supporting Arguments
The proposed framework is distinct from GANs in that both its components are interpretable, allowing for sensible rule learning and application. However, the lack of comparison with other baselines and previous versions makes it challenging to assess the significance of the results. Furthermore, the use of technical terms and complex math concepts makes it difficult to understand the system's performance, which is a crucial aspect of evaluating the paper's contribution.
Additional Feedback
To improve the paper, I suggest that the authors provide a more detailed comparison with other baseline models and previous versions of their approach. Additionally, they should consider simplifying the technical terms and math concepts used in the paper to make it more accessible to a broader audience. It would also be helpful to include more visualizations or examples to illustrate the performance of the proposed framework.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How does the proposed key-value-predict attention mechanism differ from other attention mechanisms used in neural language models?
2. Can you provide more details on the experimental setup and the datasets used to evaluate the proposed framework?
3. How do you plan to address the issue of the model mainly utilizing a memory of only the most recent history, and what strategies do you propose to encourage attending over a longer history?