Summary of the Paper's Contributions
The paper presents a novel technique, called PGQL, which combines policy gradient and Q-learning methods in reinforcement learning. The authors establish a connection between the fixed points of regularized policy gradient algorithms and the Q-values of the resulting policy, showing that the Bellman residual of the induced Q-values is small for small regularization. This leads to the development of PGQL, which adds an auxiliary update to the policy gradient that reduces the Bellman residual. The paper provides theoretical results and empirical evidence demonstrating the effectiveness of PGQL, including improved data efficiency and stability compared to actor-critic and Q-learning alone.
Decision to Accept
Based on the review, I decide to accept the paper. The main reasons for this decision are:
1. The paper presents a novel and well-motivated approach to combining policy gradient and Q-learning methods, which addresses a significant problem in reinforcement learning.
2. The authors provide a clear and well-structured presentation of their ideas, including theoretical results and empirical evidence supporting the effectiveness of PGQL.
Supporting Arguments
The paper's contributions are significant, and the authors provide a thorough analysis of the relationship between policy gradient and Q-learning methods. The empirical results demonstrate the effectiveness of PGQL in various environments, including the Atari suite, and show improved performance compared to state-of-the-art algorithms. The paper is well-written, and the authors provide a clear and concise presentation of their ideas.
Additional Feedback
To further improve the paper, the authors may consider addressing the minor comment regarding the stationary distribution used for a policy, specifically the distinction between discounted and non-discounted distributions. Additionally, the authors may provide more detailed analysis of the hyperparameters used in the experiments and their impact on the performance of PGQL.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the choice of hyperparameters used in the experiments, and how they were tuned?
2. How does the performance of PGQL compare to other state-of-the-art algorithms in reinforcement learning, such as deep Q-networks and policy gradient methods with entropy regularization?
3. Can you provide more insight into the theoretical guarantees of PGQL, and how they relate to the performance of the algorithm in practice?