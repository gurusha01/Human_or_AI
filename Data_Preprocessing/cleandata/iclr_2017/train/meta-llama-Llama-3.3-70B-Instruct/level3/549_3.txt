Summary of the Paper's Contributions
The paper presents a novel approach to understanding the behavior of deep neural networks by introducing the concept of preimages, which are the sets of inputs that result in the same output activity at a certain level of the network. The authors demonstrate how to compute these preimages for fully connected multi-layer rectifier networks and show that they form piecewise linear manifolds in the input space. This work has the potential to provide valuable insights into the efficiency of deep learning networks and could be used to design more efficient training algorithms.
Decision: Reject
The main reasons for this decision are the limited experimental evaluation and the lack of convincing results. The paper only uses a single database and baseline for comparison, which is not sufficient to demonstrate the effectiveness of the proposed approach. Additionally, the evaluation cost of the approach can be high in multi-class settings, making it less attractive compared to other methods.
Supporting Arguments
The paper's use of cosine loss for sparse coding is an interesting approach, and the energy-based formulation enables bi-directional coding, which incorporates both top-down and bottom-up information in feature extraction. However, the sparse coding scheme's motivation is to enable feed-forward inference, but this property does not hold in the multi-stage setting, requiring optimization. The bi-directional coding scheme is also interesting, but its efficiency is uncertain due to the need for multiple evaluations for classification.
Additional Feedback
To improve the paper, the authors should provide more extensive experimental evaluations using multiple databases and baselines. They should also discuss the trade-offs between computational costs and performance when using the proposed model. Furthermore, the authors should clarify whether the baseline or model uses batch normalization and test its relevance if not already done. The paper could also benefit from a more detailed analysis of the preimage problem, including the effect of pooling on the preimages.
Questions for the Authors
1. How do the authors plan to address the limited experimental evaluation and provide more convincing results?
2. Can the authors provide more insights into the computational costs of the proposed approach and how it compares to other methods?
3. How do the authors plan to extend the work to convolutional layers and what implications does this have for the preimage concept?
4. Can the authors provide more details on the potential applications of the preimage concept in designing more efficient training algorithms?