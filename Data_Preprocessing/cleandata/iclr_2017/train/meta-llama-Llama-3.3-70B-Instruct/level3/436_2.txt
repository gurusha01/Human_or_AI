Summary
The paper proposes a novel Linear Pipeline (LP) based collective design for multi-GPU training of neural networks, which aims to reduce the communication overhead in Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD). The authors demonstrate that LP collectives can achieve significant speedups over traditional Minimal Spanning Tree (MST) and Bidirectional Exchange (BE) based collectives, with up to 2x higher bandwidth and O(logP) speedups. The paper also presents a theoretical analysis of the cost model and experimental results on various neural network training tasks, showing that LP collectives can effectively reduce the total training time without affecting SGD's convergence properties.
Decision
I decide to Accept this paper, with the main reason being that the proposed LP collective design shows significant promise in reducing communication overhead in multi-GPU training of neural networks. The paper provides a thorough theoretical analysis and experimental evaluation, demonstrating the effectiveness of the proposed approach.
Supporting Arguments
The paper tackles a specific and important problem in the field of parallel computing and deep learning, namely reducing communication overhead in multi-GPU training. The approach is well-motivated, and the authors provide a clear and detailed explanation of the proposed LP collective design. The experimental results demonstrate significant speedups over traditional collectives, and the paper provides a thorough analysis of the cost model and scalability experiments.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the LP collective design, including any optimizations or techniques used to achieve the reported speedups. Additionally, it would be helpful to include more experimental results on different neural network architectures and datasets to demonstrate the generality of the proposed approach. Finally, the authors may want to consider discussing potential limitations or challenges of the proposed approach, such as scalability to larger numbers of GPUs or applicability to other parallel computing frameworks.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the implementation of the LP collective design, including any optimizations or techniques used to achieve the reported speedups?
2. How do you plan to address potential scalability limitations of the proposed approach to larger numbers of GPUs or more complex neural network architectures?
3. Have you considered applying the proposed LP collective design to other parallel computing frameworks or domains, such as distributed computing or high-performance computing?