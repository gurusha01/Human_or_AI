This paper proposes a new sparse coding model that learns features jointly with their transformations using a tree of transformations to generate features. The authors claim that their model achieves similar reconstruction error as traditional sparse coding while using fewer parameters. 
I decide to reject this paper, primarily due to two key reasons. Firstly, the usefulness and ability of the model to enable new applications are questionable. The authors' claim that the model extracts pose information is unclear, as inference is only performed over sparse coefficients associated with each feature-transformation combination. Secondly, the model's loss surface looks problematic, and it's unclear if the transformation parameters actually change significantly from their initialization values.
The paper's approach is not well-motivated, and the authors fail to convincingly demonstrate the advantage of using a tree of transformations, particularly deeper trees. The method's ability to be extended to multiple layers is also unclear, as the transformation operators cannot be defined in the same way in the learned feature space. The paper's novelty is moderate, but the benefits of the new aspects are not convincingly shown.
To improve the paper, the authors should provide more convincing evidence of the model's usefulness and ability to enable new applications. They should also clarify how the model extracts pose information and provide more insight into the model's loss surface. Additionally, the authors should demonstrate the advantage of using a tree of transformations and explore the possibility of extending the method to multiple layers.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: 
1. Can you provide more details on how the model extracts pose information, and how the inference is performed over sparse coefficients associated with each feature-transformation combination?
2. How do you ensure that the transformation parameters actually change significantly from their initialization values, and what is the impact of this on the model's performance?
3. Can you provide more convincing evidence of the advantage of using a tree of transformations, particularly deeper trees, and explore the possibility of extending the method to multiple layers?