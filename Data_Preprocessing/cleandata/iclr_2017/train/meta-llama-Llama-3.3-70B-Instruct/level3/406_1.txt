This paper proposes a novel optimization algorithm called Entropy-SGD for training deep neural networks. The algorithm is motivated by the local geometry of the energy landscape and aims to exploit the phenomenon of wide valleys in the energy landscape, which are known to generalize well. The authors introduce the concept of local entropy, which measures the flatness of the energy landscape, and use it to modify the traditional SGD algorithm. The resulting Entropy-SGD algorithm has a smoother energy landscape and obtains better generalization error than traditional SGD.
The paper is well-written and accessible, and the authors provide a clear and concise explanation of the proposed algorithm and its theoretical properties. The experimental results demonstrate the effectiveness of Entropy-SGD on various deep learning tasks, including image classification and text prediction.
I decide to accept this paper because it tackles an important problem in deep learning, namely the optimization of deep neural networks, and proposes a novel and well-motivated solution. The paper is well-organized, and the authors provide a clear and concise explanation of the proposed algorithm and its theoretical properties.
The approach is well-motivated, and the authors provide a clear explanation of how the local entropy objective is related to the energy landscape of deep neural networks. The paper also provides a thorough analysis of the theoretical properties of Entropy-SGD, including its smoothness and generalization error.
The paper supports its claims with extensive experimental results on various deep learning tasks, including image classification and text prediction. The results demonstrate the effectiveness of Entropy-SGD in obtaining better generalization error than traditional SGD.
To further improve the paper, I would like the authors to provide more insights into the relationship between local entropy and other optimization algorithms, such as stochastic variational inference. Additionally, it would be interesting to see more experiments on larger-scale datasets and more complex deep learning tasks.
Some questions I would like the authors to answer include: How does the local entropy objective relate to other optimization algorithms, such as stochastic variational inference? Can the authors provide more insights into the theoretical properties of Entropy-SGD, such as its convergence rate and stability? How does Entropy-SGD perform on larger-scale datasets and more complex deep learning tasks?