This paper proposes a novel methodology for morphing a trained neural network to different architectures without requiring retraining from scratch, offering a potential solution for network adaptation. The manuscript is well-written and easy to follow, making it simple for readers to understand the proposed methodology. The authors introduce a graph-based representation for a module, which allows the network morphing process to be formulated as a graph transformation problem. They also propose two atomic morphing operations and provide a unified morphing solution for both simple morphable modules and complex modules.
However, I decide to reject this paper for two key reasons. Firstly, the results are unconvincing due to the selection of outdated baselines that are far from the current state of the art in the field. The authors only compare their approach with ResNet, which is an old architecture, and do not provide comparisons with more recent and powerful architectures such as wide residual networks. Secondly, the paper lacks a thorough evaluation of the proposed approach, as it does not provide parameter numbers for each architecture in the tables, making it difficult to assess the efficiency of the proposed methodology.
To improve the paper, I suggest that the authors provide a more comprehensive evaluation of their approach by comparing it with state-of-the-art architectures and including parameter numbers in the tables. Additionally, they should consider providing more detailed analysis of the results and discussing the potential limitations and future directions of their work. Some questions that I would like the authors to answer include: How does the proposed approach perform on more complex datasets and tasks? Can the authors provide more insights into the graph transformation process and how it relates to the network morphism equation? How does the proposed approach handle non-linear activation functions and batch normalization layers in the network morphism process?