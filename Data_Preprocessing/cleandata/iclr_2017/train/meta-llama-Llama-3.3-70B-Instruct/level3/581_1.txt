Summary
The paper proposes a system for generating handwriting sequences using a combination of a physiologically plausible model of handwriting, known as the Sigma Lognormal model, and Recurrent Mixture Density Networks (RMDNs). The system first preprocesses the input data to extract a concise representation of the handwriting trajectory, which is then used to train the RMDN models. The authors demonstrate the effectiveness of their approach in generating handwriting sequences that capture the statistical and dynamic qualities of handwriting.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a significant machine learning algorithmic contribution, as it uses the same combination of LSTM and bivariate mixture density network as existing work. Secondly, the paper misses a key point in explaining how Gaussian parameters are obtained from the LSTM output, which is a crucial aspect of the proposed approach.
Supporting Arguments
The paper builds on existing work by Graves (2013) and uses a similar network architecture, which raises concerns about the novelty and originality of the proposed approach. Furthermore, the paper does not provide a clear explanation of how the Gaussian parameters are obtained from the LSTM output, which makes it difficult to understand the underlying mechanics of the proposed system. Additionally, the paper lacks numerical evaluation to suggest that the algorithm is an improvement over the state-of-the-art, which is a critical aspect of any machine learning paper.
Additional Feedback
To improve the paper, I would suggest that the authors provide a clear explanation of how the Gaussian parameters are obtained from the LSTM output and demonstrate the effectiveness of their approach through numerical evaluation. Additionally, the authors may want to consider submitting their paper to a conference focused on handwriting recognition, rather than a general AI conference like ICLR, as the paper's focus is quite specific.
Questions for the Authors
I would like to ask the authors to clarify how they obtain the Gaussian parameters from the LSTM output and provide more details on the numerical evaluation of their approach. Specifically, I would like to know:
* How do you derive the Gaussian parameters from the LSTM output, and what are the underlying assumptions made in this process?
* Can you provide more details on the numerical evaluation of your approach, including the metrics used and the results obtained?
* How do you plan to address the lack of novelty and originality in your approach, given that it builds on existing work by Graves (2013)?