Summary
The paper proposes an online structure learning technique for Gaussian Sum-Product Networks (SPNs) that does a single pass through the data. The algorithm starts with a fully factorized joint probability distribution and updates the structure and parameters as new data points are processed. The technique is evaluated on several benchmark datasets and compared to other algorithms, showing promising results.
Decision
I decide to accept this paper with minor revisions. The main reason for this decision is that the paper presents a novel and efficient online structure learning technique for Gaussian SPNs, which is a significant contribution to the field. The technique is well-motivated, and the paper provides a clear and thorough explanation of the algorithm and its evaluation.
Supporting Arguments
The paper tackles a specific question/problem in the field of SPNs, which is the need for an online structure learning technique that can scale to large datasets. The approach is well-motivated, and the paper provides a clear and thorough explanation of the algorithm and its evaluation. The results show that the proposed technique outperforms other algorithms on several benchmark datasets, which demonstrates its effectiveness.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the computational complexity of the algorithm and its scalability to very large datasets. Additionally, it would be helpful to include more comparisons with other state-of-the-art algorithms and to discuss the potential applications of the proposed technique in real-world scenarios.
Questions for the Authors
I would like the authors to clarify the following points:
1. How does the algorithm handle missing data or outliers in the dataset?
2. Can the authors provide more insights into the choice of the correlation threshold and its impact on the performance of the algorithm?
3. How does the algorithm compare to other online learning techniques, such as online Bayesian moment matching or online expectation maximization?
By addressing these questions and providing more details on the algorithm and its evaluation, the authors can further improve the paper and make it more convincing to the readers.