Summary
The paper proposes a novel LSTM parametrization, called Normalized LSTM, which preserves the means and variances of the hidden states and memory cells across time. This approach is motivated by the need to address the vanishing and exploding gradient problems in recurrent neural networks. The authors claim that their method is faster and more efficient than existing approaches, such as Batch Normalization and Layer Normalization, while achieving similar or better performance on language modeling and image generative modeling tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the proposed time-series model is considered too simplistic due to its matched state space, linear displacements, and one-dimensional observations. Secondly, the authors lack comparison to other methods, except for an out-of-the-box LSTM model, which is a significant limitation.
Supporting Arguments
The paper's approach, although well-motivated, seems to be behind the current state of the art, such as the "Embed to Control" method by Watter et al 2015, which learns state representation directly from pixels. Furthermore, the authors' evaluation is limited to only two tasks, and the comparison to other methods is incomplete. The paper also fails to relate their work to prior literature on combining Hidden Markov Models (HMMs) and Neural Networks (NNs) to establish its relevance and contribution.
Additional Feedback
To improve the paper, the authors should consider the following suggestions: (1) provide a more comprehensive comparison to other state-of-the-art methods, including those that learn state representation directly from pixels; (2) evaluate their approach on more tasks and datasets to demonstrate its generalizability; (3) relate their work to prior literature on combining HMMs and NNs to establish its relevance and contribution; and (4) consider more complex time-series models that can capture non-linear relationships and multi-dimensional observations.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions: (1) How do the authors plan to address the simplicity of their time-series model, and what are the potential limitations of their approach? (2) Can the authors provide more details on their evaluation methodology and why they chose to compare their approach only to an out-of-the-box LSTM model? (3) How do the authors plan to relate their work to prior literature on combining HMMs and NNs, and what are the potential contributions of their approach to this field?