Summary of the Paper
The paper presents a study on posing architecture search as a meta-learning problem. The authors propose a joint many-task model that can handle multiple NLP tasks, including POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, in a single end-to-end deep model. The model is trained jointly over all datasets, and the authors use a successive regularization strategy to avoid catastrophic interference between tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's approach to fixing important hyper-parameters, such as the learning rate, may not be optimal, and optimizing them instead could potentially achieve better results. Secondly, the protocol for generating architectures may not be creating a diverse set of architectures, and implementing a pruning mechanism to filter out similar networks could improve the results.
Supporting Arguments
The paper's approach to fixing hyper-parameters, such as the learning rate, may not be optimal. The authors use a fixed learning rate throughout the experiments, which may not be suitable for all tasks. Optimizing the hyper-parameters instead could potentially achieve better results. Additionally, the protocol for generating architectures may not be creating a diverse set of architectures. The authors use a simple strategy to generate architectures, which may not be sufficient to create a diverse set of architectures. Implementing a pruning mechanism to filter out similar networks could improve the results.
Additional Feedback
To improve the paper, I suggest that the authors provide more insights into the top-performing architectures, such as visualizations or trends, to better understand the results. Additionally, the authors could consider implementing a pruning mechanism to filter out similar networks and optimize the hyper-parameters instead of fixing them. The authors could also provide more details on the batch normalization experiments, which are currently under-explained.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How did the authors decide on the fixed learning rate, and did they try optimizing it instead?
* Can the authors provide more details on the protocol for generating architectures, and how they ensured that the generated architectures are diverse?
* How do the authors plan to address the issue of catastrophic interference between tasks, and do they think that their successive regularization strategy is sufficient?
* Can the authors provide more insights into the top-performing architectures, such as visualizations or trends, to better understand the results?