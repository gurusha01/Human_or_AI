Summary
The paper proposes a novel approach to learning deep feature representations by incorporating object persistence constraints into a Siamese triplet architecture. The resulting network, called Object Persistence Net (OPnet), is trained on a dataset of rendered 3D objects and demonstrates improved performance in similarity judgment tasks, including instance and categorical retrieval of novel objects. The authors claim that the OPnet's feature representations better match human perceptual similarity judgment than AlexNet, a standard deep convolutional neural network.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper's methodology in section 3.1 is flawed, as it relies on tree-to-tree distance comparisons rather than instance-to-instance similarity assessments. Secondly, the paper overemphasizes its connection to human vision, using terms like "object persistence" without fully justifying their relevance to the problem at hand.
Supporting Arguments
The paper's approach to learning deep feature representations is interesting, but the evaluation methodology is limited. The use of a synthetic dataset, while useful for controlling variables, may not generalize well to real-world scenarios. Furthermore, the comparison to human perceptual similarity judgment is based on a limited study, and the claim that the OPnet's performance is "remarkably" similar to human judgment is not fully justified. The paper could benefit from more extensive experiments with real data and a more nuanced discussion of the relationship between object persistence and human vision.
Additional Feedback
To improve the paper, I suggest the following: (1) conduct more experiments with real data to demonstrate the generalizability of the OPnet's performance; (2) revise the methodology in section 3.1 to focus on instance-to-instance similarity assessments; (3) provide a more detailed discussion of the relationship between object persistence and human vision, including potential limitations and biases of the approach; and (4) consider alternative evaluation metrics that can provide a more comprehensive understanding of the OPnet's performance.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to address the following questions: (1) How do you plan to extend the OPnet's performance to more complex real-world scenarios, such as scenes with multiple objects and varying lighting conditions? (2) Can you provide more details on the rendering process used to generate the synthetic dataset, and how it affects the OPnet's performance? (3) How do you respond to potential criticisms that the OPnet's performance is due to the specific architecture or training procedure, rather than the object persistence constraint itself?