This paper proposes a novel approach to integrate efficient inference within the Generative Adversarial Network (GAN) framework, called Adversarially Learned Inference (ALI). The authors introduce a new method for learning an inference network in the GAN framework, where the objective is to match the joint distribution of hidden and visible units imposed by an encoder and decoder network. I believe that ALI stabilizes the GAN training and learns a reasonable inference network, as demonstrated in Figure 8, which led to an increase in their score.
I decide to accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The paper provides a clear and concise explanation of the proposed method, and the experimental results demonstrate the effectiveness of ALI in learning mutually coherent inference and generation networks.
To further improve the paper, I suggest that the authors provide an extensive comparison with other methods for learning an inference network for GANs, such as the infoGAN approach. This would help to discuss why ALI's inference network is superior and provide a more comprehensive understanding of the proposed method. Additionally, the stochastic nature of ALI's inference network could be highlighted by including different reconstructions of the same image and comparing it to deterministic inference networks like the one in the BiGAN paper.
The quality of samples is very good, but the paper lacks quantitative experiments to compare ALI's samples with other GAN variants. Including an inception score could help to provide a more objective evaluation of the generated samples. Furthermore, the semi-supervised results could be improved by training the semi-supervised path at the same time with the generative path and using the inference network on a categorical latent variable directly for classification.
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How does the choice of the prior distribution p(z) affect the performance of ALI? (2) Can the authors provide more insights into why ALI is able to learn a more disentangled representation compared to other GAN variants? (3) How does the proposed method relate to other approaches that combine VAEs and GANs, such as VAE-GAN hybrids? 
Overall, I believe that this paper provides a significant contribution to the field of generative models and has the potential to inspire further research in this area. With some additional experiments and comparisons, the paper could be even stronger and more convincing.