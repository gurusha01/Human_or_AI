Summary
The paper proposes a joint approach to learning word vector representations for character sequences and acoustic spans. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses to learn acoustic word embeddings that improve over previous approaches for the task of word discrimination. They also present results on other tasks enabled by the multi-view approach, including cross-view word discrimination and word similarity.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the motivation and tasks seem synthetic, relying on acoustics spans for pre-segmented words from continuous speech, and the evaluation tasks feel artificial. Secondly, the paper would benefit from comparing character embeddings with phone string embeddings and exploring homophones to see how the network handles them.
Supporting Arguments
The paper's approach to learning acoustic word embeddings is well-motivated, and the use of multi-view contrastive losses is a good idea. However, the evaluation tasks and datasets used are limited, and the paper could benefit from more realistic and challenging evaluation scenarios. Additionally, the paper lacks a thorough comparison with other related work, particularly in the area of phone string embeddings and homophone handling.
Additional Feedback
To improve the paper, I suggest that the authors consider the following: (1) use more realistic and challenging evaluation datasets, such as those with continuous speech and varying acoustic conditions; (2) compare their approach with other related work, particularly in the area of phone string embeddings and homophone handling; (3) explore the use of phoneme string edit distance in addition to character edit distance; and (4) provide more detailed analysis and visualization of the learned embeddings, such as t-SNE plots and precision-recall curves.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence: (1) How do the authors plan to address the issue of keyword spotting in longer spoken utterances, where the approach may not be effective? (2) Can the authors provide more details on the phonetic pronunciations of the words in the dataset and how they relate to the learned embeddings? (3) How do the authors plan to extend their approach to directly train on both word and non-word segments?