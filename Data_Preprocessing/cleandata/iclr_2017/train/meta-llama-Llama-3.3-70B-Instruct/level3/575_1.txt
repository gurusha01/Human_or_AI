Summary of the Paper's Claims and Contributions
The paper proposes a differentiable version of Canonical Correlation Analysis (CCA), which enables the computation of CCA to be cast as a layer within a multi-view neural network. The authors claim that this formulation allows for gradient flow through the computation of the CCA projection matrices, enabling direct back-propagation and free choice of the final optimization target. The paper demonstrates the effectiveness of this approach in cross-modality retrieval experiments on two public image-to-text datasets, surpassing both Deep CCA and a multi-view network with freely-learned projections.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's contribution is unclear, as Deep CCA already provides the gradient derivation of the correlation objective. Secondly, the paper's claim of enabling direct back-propagation through the computation of CCA is confusing, as it simply gives the gradient of the correlation w.r.t. the network outputs, similar to Deep CCA.
Supporting Arguments
The paper's formulation of differentiable CCA is not significantly different from Deep CCA, which already allows for gradient-based optimization of the nonlinear transformations. The authors' claim of enabling direct back-propagation through the computation of CCA is misleading, as it does not provide a new or significant improvement over existing methods. The experimental results, while showing some improvement over Deep CCA, do not demonstrate a clear advantage of the proposed method.
Additional Feedback and Questions
To improve the paper, the authors should clarify the contribution of their work and provide a more detailed comparison with existing methods, such as Deep CCA. They should also provide more insight into the experimental results, including a more detailed analysis of the learned representations and the effect of the differentiable CCA layer on the optimization process. Some questions that I would like the authors to answer include: How does the differentiable CCA layer improve the optimization process, and what are the benefits of using this layer compared to existing methods? How do the learned representations differ from those obtained using Deep CCA, and what are the implications of these differences for cross-modality retrieval tasks?