Summary of the Paper's Contributions
The authors propose a novel approach to constructing stochastic volatility models for financial time series using a recurrent neural network (RNN) and an inference network to compute the posterior distribution for latent variables. The method is validated through experiments on synthetic and real-world time series, demonstrating superior performance to parametric GARCH models and a Gaussian process volatility model.
Decision and Key Reasons
I decide to accept this paper with minor revisions. The key reasons for this decision are: (1) the paper tackles a specific and important problem in financial time series analysis, and (2) the approach is well-motivated and technically correct, with a notable exception in equation (19) where the inference model performs filtering instead of smoothing.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of stochastic volatility modeling and the limitations of existing approaches. The authors propose a novel solution using an RNN and an inference network, which is well-motivated and technically sound. The experimental results demonstrate the effectiveness of the proposed method in capturing complex patterns in financial time series.
Additional Feedback and Questions
To improve the paper, I suggest that the authors: (1) clarify the notation and terminology used in the paper, particularly in the introduction and methodology sections; (2) provide more details on the implementation of the RNN and inference network, including the architecture and hyperparameter settings; and (3) discuss the potential limitations and extensions of the proposed method, including its applicability to other types of time series data.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) How do the authors select the number of latent variables and the truncation level in the stick-breaking process? (2) Can the authors provide more insight into the choice of the Kumaraswamy distribution as an approximate posterior, and how it compares to other distributions? (3) How do the authors plan to address the issue of component collapsing in the variational autoencoder, and what are the implications for the stick-breaking process?