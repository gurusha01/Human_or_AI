The paper proposes a novel sequence learning approach, RL Tuner, which combines reinforcement learning (RL) with a pre-trained recurrent neural network (RNN) to refine the generated sequences. The method uses the pre-trained RNN to supply part of the reward value in the RL model, allowing the model to maintain good predictive properties learned from data while optimizing for imposed reward functions. The approach is tested on music generation tasks, where the goal is to generate melodies that adhere to music theory rules while maintaining creativity.
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks comparison to existing literature results, making it difficult to evaluate the effectiveness of the proposed optimization strategy. Secondly, the experiments show limited accuracy, around 92%, which is not impressive and does not demonstrate the learning of an interesting problem representation.
To support these reasons, the paper proposes a simple optimization technique of adding gradient noise with a specific schedule to improve optimization properties in neural networks. However, the results show that adding noise in the training procedure consistently leads to better optimization properties, but it is unclear if this leads to actually good models or just better than those without noise. The paper also lacks a clear comparison to existing literature results, which makes it challenging to assess the significance of the proposed approach.
To improve the paper, I suggest that the authors provide a more comprehensive comparison to existing literature results, including a discussion of the strengths and weaknesses of the proposed approach. Additionally, the authors should consider providing more detailed analysis of the results, including visualizations and examples of the generated melodies, to demonstrate the effectiveness of the proposed approach.
I would like to ask the authors to clarify the following points: (1) How does the proposed approach compare to existing methods for sequence generation, such as SeqGAN and MIXER? (2) Can the authors provide more detailed analysis of the results, including visualizations and examples of the generated melodies? (3) How does the choice of hyperparameters, such as the weight placed on the music-theory rewards, affect the performance of the model? 
These questions will help to clarify the understanding of the paper and provide additional evidence to make a confident assessment.