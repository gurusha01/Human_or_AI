This paper introduces a novel optimization algorithm called Entropy-SGD, which is designed to exploit the local geometric properties of the objective function in deep learning. The key idea is to modify the traditional SGD algorithm to search for a flat local minimum of reasonably low loss, motivated by empirical observations that such minima tend to have good generalization performance. The authors propose a regularization term based on the free local energy, which is estimated using Monte Carlo methods with a stochastic gradient Langevin dynamics (SGLD) sampler.
The paper presents a clear and well-written introduction to the concept of local entropy and its connection to the optimization framework. The authors provide a thorough analysis of the energy landscape of deep neural networks, including the spectrum of the Hessian at local minima, and demonstrate that the proposed Entropy-SGD algorithm can effectively exploit the geometric properties of the landscape to improve generalization performance.
The experimental results show that Entropy-SGD compares favorably to state-of-the-art techniques, including Adam and SGD, on various benchmark datasets, including MNIST, CIFAR-10, and Penn Tree Bank. The authors also demonstrate that Entropy-SGD can be applied to large convolutional and recurrent neural networks, and that it can achieve significant speedups over traditional SGD.
However, there are some potential issues with the paper that need to be addressed. Firstly, the correctness of a term in equation (8) is questionable, and the authors should provide a clear explanation or correction. Additionally, the mixed usage of terminology, such as "free energy" and "free entropy", can be confusing and needs to be clarified.
To further improve the paper, I would suggest that the authors provide additional experiments on various models, such as RNNs and seq2seq models, to demonstrate the robustness and applicability of Entropy-SGD. Furthermore, a more detailed analysis of the computational complexity and memory requirements of the algorithm would be helpful, as well as a comparison with other optimization algorithms that also exploit the geometric properties of the energy landscape.
Overall, I would recommend accepting this paper, as it presents a novel and well-motivated approach to optimization in deep learning, with promising experimental results and a clear potential for impact on the field.
Decision: Accept
Reasons:
1. The paper introduces a novel and well-motivated approach to optimization in deep learning, with a clear potential for impact on the field.
2. The experimental results demonstrate the effectiveness of Entropy-SGD on various benchmark datasets, including MNIST, CIFAR-10, and Penn Tree Bank.
Questions for the authors:
1. Can you provide a clear explanation or correction for the term in equation (8) that was questioned by the reviewer?
2. How do you plan to address the mixed usage of terminology, such as "free energy" and "free entropy", to avoid confusion?
3. Can you provide additional experiments on various models, such as RNNs and seq2seq models, to demonstrate the robustness and applicability of Entropy-SGD?
4. How does the computational complexity and memory requirements of Entropy-SGD compare to other optimization algorithms that also exploit the geometric properties of the energy landscape?