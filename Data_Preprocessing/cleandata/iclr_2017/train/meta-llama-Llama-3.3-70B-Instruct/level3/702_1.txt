Summary of the Paper's Claims and Contributions
The paper presents a novel approach to adjusting for the variance introduced by dropout in neural networks, focusing on weight initialization and Batch Normalization. The authors propose a new weight initialization technique that incorporates corrective factors for dropout rates and arbitrary nonlinearities, allowing for faster and more accurate convergence. Additionally, they demonstrate that re-estimating Batch Normalization variance parameters after training can significantly improve test performance. The paper claims to achieve state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision and Key Reasons
Based on the review, I decide to Accept the paper. The key reasons for this decision are:
1. The paper tackles a specific and important problem in deep learning, namely the variance introduced by dropout, and provides a well-motivated and well-placed solution in the literature.
2. The approach is supported by thorough empirical evaluations, including experiments on fully connected and convolutional neural networks, and demonstrates significant improvements over existing methods.
Supporting Arguments
The paper provides a clear and detailed derivation of the new weight initialization technique, which is supported by empirical evidence. The experiments demonstrate the effectiveness of the approach in various settings, including different dropout rates and optimizers. The re-estimation of Batch Normalization variance parameters is also shown to be a simple yet effective technique for improving test performance.
Additional Feedback and Suggestions
To further improve the paper, I suggest:
* Providing more insights into the choice of the 0.5 default adjustment factor for nonlinearities and exploring its impact on the results.
* Investigating the applicability of the proposed approach to other types of neural networks, such as recurrent neural networks.
* Considering the potential computational costs and trade-offs of the proposed approach, particularly in comparison to other variance stabilization techniques.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more details on the sensitivity of the results to the choice of the 0.5 default adjustment factor and its impact on the performance of the proposed approach?
* How do the proposed weight initialization and Batch Normalization variance re-estimation techniques interact with other regularization techniques, such as dropout and weight decay?
* Are there any plans to release the code and models used in the experiments to facilitate reproducibility and further research?