Summary of the Paper's Contributions
The paper proposes a novel approach to learning multi-sense word embeddings using aligned text across languages. The authors introduce a multi-view Bayesian non-parametric algorithm that leverages multilingual distributional information to improve word sense disambiguation. The model is capable of using multiple languages to disambiguate words in English, and the authors demonstrate its effectiveness in experiments.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the model section is convoluted and could be described more concisely, making it harder to understand the idea. Secondly, the paper lacks comparison with other work, making it impossible to evaluate the proposed model's merits objectively.
Supporting Arguments
The paper's model description is lengthy and complex, which may hinder the reader's ability to fully comprehend the approach. Additionally, the lack of comparison with other state-of-the-art models makes it challenging to assess the proposed model's performance and contributions. The authors only compare their model to a monolingually trained version of another model, which is not sufficient to demonstrate the superiority of their approach.
Additional Feedback
To improve the paper, I suggest that the authors simplify the model description and provide a clearer explanation of the algorithm. Additionally, they should include a more comprehensive comparison with other relevant works in the field, including bilingual and multilingual models. This would help to demonstrate the strengths and weaknesses of their approach and provide a more nuanced understanding of its contributions.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a more detailed explanation of the Î¨(ye, xf) term and its role in the model?
2. How do you plan to extend the model to handle polysemy in foreign languages?
3. Can you provide more information on the parameter tuning strategy and how it was chosen?
4. How do you think the model's performance would be affected by using different language families or domains?