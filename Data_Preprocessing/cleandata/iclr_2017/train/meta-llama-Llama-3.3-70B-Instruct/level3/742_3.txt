This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning to refine a pre-trained Recurrent Neural Network (RNN) for sequence generation tasks. The approach uses the pre-trained RNN to supply part of the reward value in a reinforcement learning model, allowing the model to learn from both data and heuristic rewards. The paper applies this approach to music generation and demonstrates its effectiveness in producing more pleasing and structured melodies.
I decide to reject this paper, primarily due to the lack of clarity in the introduction and definitions throughout the paper, as well as the unclear relationship between the proposed approach and existing methods. The paper's key contributions, such as the combination of maximum likelihood and reinforcement learning, are not clearly motivated or explained, making it difficult to understand the significance of the proposed approach.
The paper's approach to studying the expressivity of neural networks by analyzing the trajectory length of a one-dimensional trajectory is interesting, but the connection to the main contribution of the paper is unclear. The experiments presented on MNIST and CIFAR10 are not well-motivated, and the results are not clearly analyzed or discussed. The paper's use of unclear notation and definitions, such as the "trajectory length" and "growth factor," further adds to the confusion.
To improve the paper, the authors should provide a clearer introduction and motivation for the proposed approach, as well as more detailed explanations of the key concepts and definitions. The authors should also provide more analysis and discussion of the experimental results, and clarify the relationship between the proposed approach and existing methods. Additionally, the authors should address the unclear notation and definitions, and provide more context and background information on the sequence generation tasks and the reinforcement learning framework used.
Some questions I would like the authors to answer to clarify my understanding of the paper include: What is the specific problem or challenge that the proposed approach is trying to address? How does the proposed approach relate to existing methods in sequence generation and reinforcement learning? What are the key insights or contributions of the paper, and how do they advance the state-of-the-art in the field? How do the authors plan to address the unclear notation and definitions, and provide more context and background information on the sequence generation tasks and the reinforcement learning framework used?