Summary
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN uses a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation to update the hidden state, ensuring that the forward hidden state activation norm and backward gradient norm are preserved. The authors demonstrate the effectiveness of their approach on a simplified memory copy task, achieving dependencies of up to 5,000 timesteps.
Decision
I decide to reject this paper, primarily due to the lack of sufficient experimental evidence to support its claims. While the proposed architecture is promising, the experiments are limited to a single task with a small number of distinct input/output sequences, and the training process is unstable for longer sequence lengths.
Supporting Arguments
The paper raises several important questions, such as whether the resulting model remains a universal approximator with large enough hidden dimensions and number of layers, and how it compares to equivalent models without orthogonal matrices. However, the experiments do not provide sufficient evidence to answer these questions. Additionally, the paper lacks comparisons with other state-of-the-art RNN architectures, making it difficult to assess the significance of the proposed approach.
Additional Feedback
To improve the paper, I suggest expanding the experiment section to include more tasks, larger datasets, and comparisons with other RNN architectures. The authors should also investigate the stability of the training process and explore ways to improve it. Furthermore, the paper could benefit from a more detailed analysis of the trade-offs between the proposed architecture and other approaches, such as LSTMs and GRUs.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the instability of the training process for longer sequence lengths?
2. Can the authors provide more insights into the choice of parameterization for the orthogonal matrices, and how it affects the performance of the model?
3. How do the authors envision combining the proposed architecture with other RNN architectures, such as LSTMs, to leverage the strengths of both approaches?