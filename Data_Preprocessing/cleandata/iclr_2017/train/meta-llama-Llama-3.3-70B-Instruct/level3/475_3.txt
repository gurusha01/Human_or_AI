Summary of the Paper's Contributions
The paper introduces a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with a recurrent neural network (RNN) language model. The NKLM predicts whether a word is based on a fact or not and generates words either from the vocabulary or by copying from the fact description. The model is trained on a new dataset, WikiFacts, and outperforms the traditional RNN language model in terms of perplexity and generates named entities that are not observed during training.
Decision and Reasons
I decide to Accept this paper with two key reasons: (1) the paper introduces a novel and well-motivated approach to incorporating symbolic knowledge into a language model, and (2) the experimental results demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities.
Supporting Arguments
The paper is well-motivated, and the approach is well-placed in the literature. The authors provide a clear explanation of the limitations of traditional language models in encoding and decoding factual knowledge and propose a solution that addresses these limitations. The experimental results are thorough and demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities. The introduction of the Unknown-Penalized Perplexity (UPP) metric is also a valuable contribution to the field.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the performance of the NKLM on different types of facts and entities. Additionally, it would be interesting to see how the NKLM performs on other knowledge-related language tasks, such as question answering and dialogue modeling. The authors may also consider providing more details on the implementation of the NKLM, such as the hyperparameter settings and the training procedure.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the NKLM to handle more complex knowledge graphs and entities? (2) Can the authors provide more insights into the performance of the NKLM on different types of facts and entities? (3) How do the authors plan to address the limitation of the NKLM in assuming that the true topic of a given description is known?