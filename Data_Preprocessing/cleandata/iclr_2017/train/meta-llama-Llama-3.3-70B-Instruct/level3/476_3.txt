Summary
The paper proposes a novel method for finding dependent subspaces across multiple views of data, with a focus on preserving neighborhood relationships between data points. The approach directly maximizes the similarity of neighborhoods between subspaces of each view, using a well-defined objective function that favors sparse and informative neighborhoods. The method is shown to outperform existing approaches, such as Canonical Correlation Analysis (CCA) and Locality Preserving CCA (LPCCA), in experiments on artificial and real-world datasets.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and well-motivated problem in multi-view learning, and (2) the approach is well-supported by theoretical analysis and empirical results.
Supporting Arguments
The paper provides a clear and thorough introduction to the problem of finding dependent subspaces across multiple views, and motivates the need for a method that preserves neighborhood relationships. The proposed approach is well-founded in information retrieval theory, and the use of a symmetrized divergence measure and a sparse neighborhood similarity measure is well-justified. The experimental results demonstrate the effectiveness of the method in finding dependent subspaces and outperforming existing approaches.
Additional Feedback
To further improve the paper, I suggest the following:
* Provide more details on the optimization technique used to maximize the objective function, and discuss the potential challenges and limitations of the approach.
* Consider adding more experiments to demonstrate the robustness of the method to different types of noise and distortions in the data.
* Provide more insights into the interpretation of the results, and discuss the potential applications of the method in real-world domains.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on the choice of the parameter Îµ in the smoothed neighbor distribution (Equation 8), and how it affects the results?
* How do you plan to extend the method to handle nonlinear transformations and more complex dependencies between views?
* Can you discuss the potential connections between the proposed method and other approaches in multi-view learning, such as deep learning-based methods?