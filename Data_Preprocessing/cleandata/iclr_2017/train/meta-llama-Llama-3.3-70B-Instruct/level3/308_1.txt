Summary
The paper proposes a novel Group Sparse Autoencoders (GSA) framework, which functions as dictionary learning and sparse coding models with inter- and intra- group sparse constraints. The authors also introduce a Group Sparse Convolutional Neural Networks (GSCNNs) by embedding GSA into CNNs, demonstrating that CNNs can benefit from GSA by learning more meaningful representations from dictionaries. The paper claims to improve question classification performance by utilizing answer information and encoding group sparse constraints.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper provides a novel and well-motivated approach to question classification, addressing important stability issues in GANs and utilizing answer information to improve question representation; (2) the experimental results demonstrate significant improvements over strong baselines on four datasets.
Supporting Arguments
The paper tackles the specific question of how to effectively utilize answer information to improve question classification performance. The approach is well-motivated, as it addresses the unique characteristics of question classification, such as hierarchical and overlapping structures in question categories, and the availability of well-prepared answer sets. The paper supports its claims with theoretical insights, novel theorems, and empirical results, demonstrating the effectiveness of the proposed GSA and GSCNNs frameworks.
Additional Feedback
To further improve the paper, I suggest the authors provide more detailed explanations of the proof of Theorem 2.5, as I have some minor technical questions regarding this proof. Additionally, it would be helpful to include more visualizations of the internal parameters of GSA and GSCNNs to facilitate a better understanding of how these models behave. I would also like the authors to clarify how the group size is determined in the experiments and whether this hyperparameter is tuned.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) Can you provide more details on how the group size is determined in the experiments? (2) How do you tune the hyperparameters, such as the intra-group sparsity ρ and inter-group sparsity η, in the GSA and GSCNNs frameworks? (3) Can you provide more insights into the visualization of the projection matrix and activations in GSA, and how these visualizations relate to the performance of the model?