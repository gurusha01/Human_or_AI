Summary of the Paper's Contributions
The paper presents a comprehensive analysis of vocabulary selection techniques for neural machine translation, exploring the trade-off between speed and accuracy for different vocabulary sizes. The authors extend previous work by considering a wide range of simple and complex selection techniques, including bilingual word co-occurrence counts, bilingual embeddings, word alignments, phrase pairs, and discriminative SVM classifiers. The experiments demonstrate that decoding speed can be reduced by up to 90% without compromising accuracy, and word alignments, bilingual phrases, and SVMs can achieve high accuracy even with fewer than 1,000 word types per sentence.
Decision and Key Reasons
I decide to accept this paper, with the key reasons being the thorough analysis of vocabulary selection techniques and the significant speed-ups achieved in decoding time. The paper provides a comprehensive evaluation of different selection methods, and the results show that word alignments, phrase pairs, and SVMs can achieve high accuracy while reducing decoding time.
Supporting Arguments
The paper's contributions are well-motivated, and the approach is well-placed in the literature. The authors provide a clear and detailed explanation of the selection techniques and their implementation. The experiments are well-designed, and the results are thoroughly analyzed. The paper also provides a good discussion of the trade-offs between speed and accuracy and the potential applications of the proposed techniques.
Additional Feedback and Questions
To further improve the paper, I would like the authors to provide more discussion on the potential limitations of the proposed techniques and the potential applications in other areas of natural language processing. I would also like to see more analysis on the impact of vocabulary selection on training time and the potential benefits of using faster encoder models. Some questions I would like the authors to answer include: How do the proposed techniques compare to other methods for reducing decoding time, such as pruning or quantization? Can the proposed techniques be applied to other sequence-to-sequence tasks, such as text summarization or chatbots? How do the authors plan to make the code and data available to the research community?