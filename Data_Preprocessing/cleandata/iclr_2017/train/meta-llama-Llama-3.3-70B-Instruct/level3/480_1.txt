Summary of the Paper's Contributions
The paper proposes a novel approach to improve vector-space word representations by alleviating the adverse effects of polysemy when using a lexicon. The authors introduce a fuzzy paraphrase set, where each paraphrase is annotated with a degree of reliability, and use this set to learn word vectors. The approach is shown to be effective in improving word vector quality, outperforming prior works, and is easier to use in practical terms as it does not require word sense disambiguation or part-of-speech tagging.
Decision and Reasons
I decide to accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The paper tackles a specific problem in natural language processing, namely the issue of polysemy in word vector representations, and proposes a novel solution that is supported by experimental results.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem, and the proposed approach is easy to follow. The experimental results are thorough and demonstrate the effectiveness of the approach. The comparison with prior works is also well-done, and the paper provides a clear conclusion and future work directions.
Additional Feedback
To improve the paper, I suggest providing more explanations for some of the results, such as the effects of different paraphrase sets and parameters. Additionally, it would be helpful to briefly mention the relationship between "diagonal" connections and other similar ideas, such as skip connections. I also request clarification on certain terms, such as "per-bout" and "bout", to ensure accurate understanding. Furthermore, I am curious about how classification accuracy relates to the time-scale of behavior and suggest investigating this further if feasible.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
1. Can you provide more details on how the fuzzy paraphrase set is constructed and how the degrees of reliability are estimated?
2. How do you plan to extend this approach to other models, and what changes would be required?
3. Can you provide more insights into the effects of corpus size on the effectiveness of the approach, and how this relates to the results in Figure 6?
4. How do you think the control function could be improved to consider all context, and what potential benefits or challenges do you see in this approach?