Summary
The paper proposes a novel approach to improve the performance of gradient descent in deep learning by utilizing multiple compute resources. The authors design and implement several asynchronous gradient descent methods, including a layer-wise approach that overlaps weight updates with inter-node synchronization, and an asynchronous approach that delays gradient updates by a fixed number of iterations. The methods are evaluated on a large-scale CPU cluster and an NVIDIA DGX-1 multi-GPU system using well-known datasets and neural network architectures.
Decision
I decide to Accept this paper with minor revisions. The main reasons for this decision are: (1) the paper tackles a specific and relevant problem in deep learning, and (2) the proposed approaches show promising results in terms of speedup and convergence.
Supporting Arguments
The paper is well-motivated, and the authors provide a clear overview of the related work and the background of deep learning and gradient descent. The proposed approaches are well-explained, and the implementation details are provided. The evaluation results show that the asynchronous methods can achieve a significant speedup while maintaining the accuracy of the model. The paper also provides a thorough analysis of the tradeoff between maintaining equivalence to sequential methods and leveraging computational resources.
Additional Feedback
To improve the paper, I suggest the authors provide more details on the experimental setup, such as the specific hardware configurations and the hyperparameters used for the evaluations. Additionally, it would be helpful to include more results on the convergence of the proposed methods, such as the loss curves and the accuracy plots. The authors may also consider providing more insights on the applicability of the proposed approaches to other deep learning tasks and architectures.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the layer-wise approach and how it is implemented in practice?
2. How do the proposed methods handle the case where the mini-batch size is very small, and the communication latency is significant?
3. Can you provide more results on the evaluation of the proposed methods on other datasets and neural network architectures?