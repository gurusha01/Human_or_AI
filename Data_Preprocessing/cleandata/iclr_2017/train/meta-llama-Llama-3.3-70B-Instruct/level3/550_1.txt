Summary
The paper proposes a framework for multitask deep reinforcement learning guided by policy sketches. The approach associates every subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating that it outperforms standard baselines and induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper appears to be incremental and lacks comparison to contractive auto-encoders, which is a crucial aspect of the proposed approach. Secondly, the experimental support for the approach is weak, with few experiments on synthetic and small-scale data, and the results on MNIST show limited improvement over the original DAE.
Supporting Arguments
The paper proposes a modified DAE objective that combines denoising and contractive auto-encoders objectives, but it is unclear how this modification improves upon existing approaches. The authors claim that their approach induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks, but this claim is not sufficiently supported by the experimental results. The experiments are limited to two environments, and the results are not statistically significant. Furthermore, the paper lacks a formal treatment of the representation collapse issue, which is a critical aspect of the proposed approach.
Additional Feedback
To improve the paper, the authors should provide a more thorough comparison to contractive auto-encoders and other related approaches. They should also conduct more extensive experiments on larger-scale datasets and provide a more detailed analysis of the results. Additionally, the authors should provide a formal treatment of the representation collapse issue and demonstrate how their approach addresses this issue. The authors should also consider providing more visualizations and illustrations of the learned policies and subpolicies to help clarify the results.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. How does the modified DAE objective improve upon existing approaches, and what are the key advantages of this modification?
2. Can the authors provide more details on the experimental setup and the hyperparameters used in the experiments?
3. How do the authors plan to address the representation collapse issue in future work, and what are the potential limitations of their approach?