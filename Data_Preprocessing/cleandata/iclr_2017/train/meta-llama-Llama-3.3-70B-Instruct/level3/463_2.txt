Summary of the Paper's Contributions
The paper proposes a novel approach to learning multi-scale spatial contextual information in deep neural networks using a Layer-Recurrent Neural Network (L-RNN) module. The L-RNN module is designed to capture long-range dependencies within layers, and can be seamlessly inserted into pre-trained convolutional neural networks (CNNs) to boost their performance. The authors demonstrate the effectiveness of the L-RNN module on two tasks: image classification on CIFAR-10 and semantic segmentation on PASCAL VOC 2012.
Decision and Key Reasons
Based on the review, I decide to Reject the paper. The two key reasons for this decision are:
1. Limited Experimental Justification: The experimental evaluation of the proposed method is limited, making it difficult to gauge its effectiveness and scalability. The authors only evaluate the L-RNN module on two tasks, and the results, while promising, are not comprehensive enough to demonstrate the method's robustness.
2. Lack of Comparison to Alternative Methods: The authors do not compare the proposed L-RNN module to alternative methods, such as training with high-quality but limited data, which could provide a more comprehensive understanding of the method's performance.
Supporting Arguments
The paper's contributions are well-motivated, and the L-RNN module is a novel and interesting approach to learning multi-scale spatial contextual information. However, the experimental evaluation is limited, and the authors do not provide a thorough comparison to alternative methods. Additionally, the method's behavior, as shown in Figure 2, degrades dramatically when the noise fraction exceeds a certain threshold, which requires further analysis and justification.
Additional Feedback
To improve the paper, the authors should provide more comprehensive experimental results, including comparisons to alternative methods and evaluations on more tasks. Additionally, the authors should analyze and justify the method's behavior in the presence of noise and provide more details on the implementation and training of the L-RNN module.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on the implementation and training of the L-RNN module, including the choice of hyperparameters and the optimization algorithm used?
2. How do you plan to address the limited experimental evaluation and provide more comprehensive results to demonstrate the method's robustness and scalability?
3. Can you provide a more thorough comparison to alternative methods, such as training with high-quality but limited data, to provide a more comprehensive understanding of the method's performance?