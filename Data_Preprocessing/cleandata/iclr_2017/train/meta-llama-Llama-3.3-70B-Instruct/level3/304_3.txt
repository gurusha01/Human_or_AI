Summary
The paper presents Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs) that exploits the variable precision requirements of DNNs to deliver execution time proportional to the precision used per layer. TRT improves upon the state-of-the-art DaDianNao accelerator by processing activations bit-serially, allowing for better performance and energy efficiency. The authors demonstrate improved sample complexity and generalization results for addition and bubblesort programs and introduce two new tasks: topological sort and quicksort.
Decision
I decide to Accept this paper with two key reasons: (1) the paper presents a novel and well-motivated approach to improving the performance and energy efficiency of DNN accelerators, and (2) the authors provide thorough evaluations and comparisons with state-of-the-art accelerators, demonstrating the effectiveness of their approach.
Supporting Arguments
The paper is well-organized and clearly explains the key concepts and ideas behind TRT. The authors provide a detailed analysis of the precision requirements of DNNs and demonstrate how TRT can exploit these requirements to achieve better performance and energy efficiency. The evaluation results show that TRT outperforms the state-of-the-art DaDianNao accelerator in terms of execution time and energy efficiency, while maintaining accuracy.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of TRT, including the hardware architecture and the software framework used to evaluate the accelerator. Additionally, it would be interesting to see more results on the trade-off between accuracy and performance, as well as an analysis of the limitations and potential challenges of the TRT approach.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does TRT handle the case where the precision requirements of different layers vary significantly?
* Can TRT be applied to other types of neural networks, such as recurrent neural networks or long short-term memory networks?
* How does TRT compare to other state-of-the-art accelerators, such as Google's Tensor Processing Unit, in terms of performance and energy efficiency?