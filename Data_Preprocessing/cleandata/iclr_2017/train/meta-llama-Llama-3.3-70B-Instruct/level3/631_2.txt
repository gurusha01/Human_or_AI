This paper proposes a novel architecture, called Layer-RNN (L-RNN), which integrates recurrent layers within larger convolutional networks to combine feature extraction and global context information. The authors validate their idea on image classification and semantic segmentation tasks, achieving positive results. However, I find the novelty of the work to be relatively limited, as the idea of combining recurrent layers with CNNs has been proposed before with similar technical differences.
The evaluation is systematic, but I find it lacking in terms of comparing results to other state-of-the-art methods, such as Wide Residual Networks. The authors claim that their approach can learn contextual information at multiple levels, but I question the necessity of using recurrent layers, especially when other methods, such as increasing network depth, can achieve better results without recurrence.
The evaluation on semantic segmentation is also questionable, as the boost in performance may be due to the addition of extra parameters rather than the recurrent layer itself. I would like to see a more thorough analysis of the contribution of the recurrent layer to the overall performance.
Some aspects of the paper, such as Figure 4 and Appendix A, are unclear or disorganized, and I suggest improvements, such as using tables instead of figures and providing more information on learning rate schedules.
Based on these concerns, I decide to reject this paper. The main reasons for this decision are the limited novelty of the work and the lack of thorough evaluation and comparison to other state-of-the-art methods.
To improve the paper, I suggest the authors provide more detailed comparisons to other methods, such as Wide Residual Networks, and conduct a more thorough analysis of the contribution of the recurrent layer to the overall performance. Additionally, the authors should clarify and reorganize some aspects of the paper, such as Figure 4 and Appendix A, to make it easier to follow.
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. How do the authors justify the use of recurrent layers over other methods, such as increasing network depth, to learn contextual information?
2. Can the authors provide more detailed comparisons to other state-of-the-art methods, such as Wide Residual Networks, to demonstrate the effectiveness of their approach?
3. How do the authors plan to address the issue of increased parameters due to the addition of recurrent layers, and what measures can be taken to prevent overfitting?