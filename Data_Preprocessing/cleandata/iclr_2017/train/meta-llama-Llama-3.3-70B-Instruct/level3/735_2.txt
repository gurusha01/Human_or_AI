Summary of the Paper's Claims and Contributions
The paper explores the concept of sample importance in deep neural networks, which refers to the contribution of each sample to the change in model parameters during training. The authors propose a quantitative measurement of sample importance and conduct empirical analysis on two standard datasets, MNIST and CIFAR-10. They find that easy samples tend to shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also investigate the effect of batch construction on training and find that mixing hard and easy samples in each batch leads to better performance.
Decision and Key Reasons
I decide to reject this paper, with two key reasons:
1. Lack of clear motivation and placement in the literature: Although the paper proposes an interesting concept, it lacks a clear motivation and placement in the literature. The authors do not provide a thorough discussion of the relevance and significance of sample importance in deep learning, and the paper does not engage with existing work on related topics, such as curriculum learning and self-paced learning.
2. Insufficient theoretical analysis and empirical comparison: The paper lacks a rigorous theoretical analysis of the proposed sample importance measurement, and the empirical comparison with existing methods is limited. The authors do not provide a clear explanation of why their approach is superior to existing methods, and the experimental results are not comprehensive enough to support the claims made in the paper.
Supporting Arguments
The paper's writing is unclear and lacks focus on a specific topic, making it difficult to follow and understand the authors' arguments. The notation used in the paper is also unclear, with undefined terms and symbols. Additionally, the paper lacks experimental comparisons with standard algorithms for matrix completion and alternate approaches for non-rigid structure from motion, which makes it difficult to evaluate the effectiveness of the proposed approach.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide a clearer motivation and placement in the literature, as well as a more rigorous theoretical analysis of the proposed sample importance measurement. The authors should also conduct more comprehensive empirical comparisons with existing methods and provide a clearer explanation of the results. Some questions I would like the authors to answer include:
* How does the proposed sample importance measurement relate to existing work on curriculum learning and self-paced learning?
* Why is the proposed approach superior to existing methods, and what are the theoretical guarantees of the approach?
* How do the authors plan to extend the sample importance analysis to different deep learning structures, such as convolutional neural networks and recurrent neural networks?