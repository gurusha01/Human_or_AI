Summary of the Paper's Contributions
The paper proposes a novel approach to address the issue of catastrophic mistakes in deep reinforcement learning (DRL) agents. The authors introduce the concept of "intrinsic fear," which involves training a separate danger model to predict the likelihood of a catastrophe within a short number of steps. This model is used to penalize the Q-learning objective, shaping the reward function away from catastrophic states. The approach is evaluated on several environments, including Adventure Seeker, Cart-Pole, and Seaquest, and is shown to outperform traditional DQN methods in terms of avoiding catastrophes.
Decision and Reasons
I decide to accept this paper, with the main reason being that it tackles a crucial and timely problem in the field of DRL, namely the issue of catastrophic mistakes. The paper provides a well-motivated and clearly explained approach to addressing this problem, and the experimental results demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper provides a thorough analysis of the limitations of traditional DQN methods in avoiding catastrophes, and the proposed approach of intrinsic fear is well-supported by theoretical and empirical evidence. The authors also provide a clear and concise explanation of the danger model and its integration with the Q-learning objective. The experimental results are convincing, and the comparison to traditional DQN methods is fair and well-executed.
Additional Feedback and Questions
To further improve the paper, I would suggest providing more details on the implementation of the danger model, such as the choice of architecture and hyperparameters. Additionally, it would be interesting to see more extensive experiments on more complex environments, such as those with high-dimensional state and action spaces. I would also like to ask the authors to clarify the relationship between the proposed approach and other methods for addressing safety in DRL, such as those based on modifying the objective function or using external knowledge to improve exploration. Specifically, I would like to know:
* How does the proposed approach relate to other methods for addressing safety in DRL, such as those based on modifying the objective function or using external knowledge to improve exploration?
* Can the authors provide more details on the implementation of the danger model, such as the choice of architecture and hyperparameters?
* How do the authors plan to extend the proposed approach to more complex environments, such as those with high-dimensional state and action spaces?