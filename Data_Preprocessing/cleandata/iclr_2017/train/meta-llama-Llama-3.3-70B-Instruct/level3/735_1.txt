Summary
The paper proposes a novel approach to understanding the importance of samples in training deep neural networks. The authors define sample importance as the change in parameters induced by a sample and provide a quantitative measurement of this importance. They conduct empirical experiments on two standard datasets, MNIST and CIFAR-10, and show that easy samples shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also demonstrate that mixing hard samples with easy samples in each batch improves training performance.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper's clarity and organization could be improved, particularly in the introduction, where the flow between dimensionality reduction techniques and inverse problems is confusing at times. Secondly, the empirical evaluations are not extensive, using old and non-standard datasets, limited comparisons, and lacking a study on the convergence of the alternating procedure.
Supporting Arguments
The paper's proposal to use sample importance to guide the training process is interesting, but the motivation behind relaxing the rank to a nuclear norm is unclear and increases computations. The authors should discuss the pros and cons of this approach compared to fixing the rank of C. Additionally, the proposed alternating optimization approach is computationally intensive and hard to scale to moderate-sized data due to the need to compute the kernel matrix and perform full SVD in every iteration.
Additional Feedback
To improve the paper, the authors should provide more extensive empirical evaluations using modern datasets and compare their approach to state-of-the-art methods. They should also consider providing more theoretical analysis of the sample importance measurement and its relationship to the final trained model. Furthermore, the authors should clarify the connection between sample importance and the concept of "easiness" of a sample, as used in curriculum learning and self-paced learning.
Questions for the Authors
1. Can you provide more insight into the motivation behind using a nuclear norm penalty on the Cholesky factor of the kernel matrix?
2. How do you plan to address the computational intensity of the proposed alternating optimization approach?
3. Can you provide more extensive empirical evaluations using modern datasets and compare your approach to state-of-the-art methods?
4. How do you think the sample importance measurement can be used to guide the training process in practice?
5. Can you clarify the connection between sample importance and the concept of "easiness" of a sample, as used in curriculum learning and self-paced learning?