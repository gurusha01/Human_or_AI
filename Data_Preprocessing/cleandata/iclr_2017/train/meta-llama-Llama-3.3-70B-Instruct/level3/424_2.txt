Summary
The paper proposes a novel framework for improving optimization of complicated functions using a series of approximations. The authors connect this framework to a neural network formulation, where the network behaves as a simpler network at high noise levels but regains full capacity as training proceeds and noise lowers. The paper presents a new architecture, called neural equivalence networks (EQNETs), for learning continuous semantic representations of mathematical and logical expressions.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the presentation of the method for neural networks lacks clarity, particularly in Algorithm 1 and the explanation of Equation 25, which needs to be improved for better understanding. Secondly, the experimental evaluations have several concerns, including the lack of discussion on why the method doesn't work for more challenging network training problems, such as thin and deep networks.
Supporting Arguments
The paper's contribution to the field of machine learning and artificial intelligence is significant, as it tackles the fundamental problem of representing and inferring procedural knowledge. However, the paper's technical presentation and experimental evaluation need improvement. The authors' use of a series of approximations to improve optimization is innovative, but the connection to the neural network formulation is not clearly explained. The experimental evaluation is thorough, but it lacks comparison to other weight noise regularization methods, highway networks, and experimentation with systematically increasing network depth to test the method's effectiveness.
Additional Feedback
To improve the paper, I suggest that the authors provide a clearer explanation of the method for neural networks, including Algorithm 1 and Equation 25. Additionally, the authors should address the concerns in the experimental evaluation, such as discussing why the method doesn't work for more challenging network training problems and comparing their method to other relevant approaches. The authors may also consider providing more visualizations and examples to illustrate the effectiveness of their approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a more detailed explanation of Algorithm 1 and Equation 25, and how they relate to the neural network formulation?
2. Why does the method not work for more challenging network training problems, such as thin and deep networks?
3. How does the proposed method compare to other weight noise regularization methods, highway networks, and other relevant approaches?
4. Can you provide more visualizations and examples to illustrate the effectiveness of the proposed approach?