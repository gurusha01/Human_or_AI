Summary
The paper proposes a novel pruning method, NoiseOut, which reduces the number of parameters in neural networks by removing neurons with correlated activations during training. The approach is based on the idea of adding noise outputs to the network, which increases the correlation between neurons in the hidden layer, making pruning more efficient. The authors demonstrate the effectiveness of NoiseOut on various networks and datasets, achieving significant compression rates without loss of accuracy.
Decision
I recommend a weak accept with low confidence. The main reason for this decision is that while the proposed approach is interesting and shows promising results, the experiments section is not convincing, and more empirical studies are needed to justify the proposed method.
Supporting Arguments
The paper proposes a novel and interesting approach to pruning neural networks, which complements existing methods. The idea of adding noise outputs to increase correlation between neurons is well-motivated and supported by theoretical analysis. The experimental results show significant compression rates without loss of accuracy, which is a desirable outcome. However, the experiments section lacks comparison with ensemble baselines and late fusion methods, which would provide additional insights into the effectiveness of NoiseOut.
Additional Feedback
To improve the paper, I suggest the following:
* Compare NoiseOut with ensemble baselines, such as combining multiple source domain predictors, to evaluate its performance.
* Investigate the effect of NoiseOut on test accuracy and explore the relationship between NoiseOut and techniques designed to avoid overfitting, such as Dropout and Regularization.
* Provide more empirical studies to justify the proposed method, including experiments on more datasets and networks.
* Clarify the effect of different random distributions on the performance of NoiseOut and provide more insights into the choice of hyperparameters.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more details on the choice of hyperparameters, such as the number of noise outputs and the random distribution used?
* How does NoiseOut perform on more complex networks and datasets, such as ImageNet and CIFAR-10?
* Can you provide more insights into the relationship between NoiseOut and Dropout, and how they can be combined to achieve better results?