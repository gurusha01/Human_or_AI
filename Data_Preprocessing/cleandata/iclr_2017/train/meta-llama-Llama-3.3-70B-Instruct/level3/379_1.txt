This paper proposes a novel approach to visual servoing, which combines learned visual features, learned predictive dynamics models, and reinforcement learning to learn visual servoing mechanisms. The authors demonstrate the effectiveness of their approach on a complex synthetic car following benchmark, achieving substantial improvement over conventional approaches based on image pixels or hand-designed keypoints, and showing an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms.
The specific question tackled by the paper is how to learn a visual servoing mechanism that can adapt to new targets with minimal data. The approach is well-motivated, building on the idea that learned visual features and predictive dynamics models can be used to improve the robustness and efficiency of visual servoing. The paper is well-placed in the literature, drawing on recent advances in deep learning and reinforcement learning.
The paper supports its claims with extensive experiments, demonstrating the effectiveness of the proposed approach on a range of tasks and comparing it to several baseline methods. The results show that the proposed approach can learn an effective visual servoing mechanism with minimal data, and that it outperforms several alternative methods.
Based on these strengths, I decide to accept this paper. The key reasons for this decision are the paper's novel approach to visual servoing, its extensive experimental evaluation, and its demonstration of significant improvements over existing methods.
To further improve the paper, I suggest that the authors provide more details on the implementation of their approach, including the specific architectures used for the visual features and dynamics models, and the hyperparameters used for training. Additionally, it would be helpful to see more analysis of the results, including visualizations of the learned visual servoing mechanisms and more detailed comparisons to the baseline methods.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the learned visual features and dynamics models generalize to new targets and environments? What is the computational cost of the proposed approach, and how does it compare to existing methods? How do the authors plan to extend their approach to more complex visual servoing tasks, such as those involving multiple targets or dynamic environments?