Summary
The paper proposes a novel approach to adapt neural network language models to their recent history by introducing a neural cache model. This model stores past hidden activations as memory and accesses them through a dot product with the current hidden activation, allowing for efficient and scalable adaptation to dynamic environments. The authors demonstrate the effectiveness of their approach on several language modeling datasets and the LAMBADA dataset, showing significant performance gains over traditional language models.
Decision
I decide to reject this paper, primarily due to two reasons. Firstly, the approach, although interesting, seems ad hoc, and the authors do not provide a clear motivation for why regular gradients are not correlated with feature importance. Secondly, the impact of the scaling parameter α on feature importance and its relation to attention in the interior gradient approach is unclear, which raises concerns about the scientific rigor of the results.
Supporting Arguments
The paper lacks a thorough explanation of why the proposed approach is necessary and how it relates to existing methods. The authors do not provide a clear comparison with other memory-augmented neural networks, making it difficult to understand the significance of their contribution. Additionally, the experimental results, although promising, are not sufficient to support the claims made by the authors, and more rigorous evaluation is needed to demonstrate the effectiveness of the neural cache model.
Additional Feedback
To improve the paper, the authors should provide a more detailed explanation of the motivation behind their approach and how it differs from existing methods. They should also conduct more thorough experiments to evaluate the effectiveness of their approach and provide a clear comparison with other memory-augmented neural networks. Furthermore, the authors should clarify the impact of the scaling parameter α on feature importance and its relation to attention in the interior gradient approach. Some questions that I would like the authors to answer include: What is the intuition behind using the dot product to access the memory? How does the neural cache model handle out-of-vocabulary words? Can the authors provide more details on the training procedure and the hyperparameter tuning process? Does the model utilize batch normalization, and if so, how does it affect the results?