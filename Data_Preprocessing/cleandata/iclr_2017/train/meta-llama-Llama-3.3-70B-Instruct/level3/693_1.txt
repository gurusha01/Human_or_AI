Summary of the Paper's Claims and Contributions
The paper proposes a meta-learning algorithm for learning generative models from a small set of examples, leveraging Bayesian neural networks to predict learning curves for iterative machine learning methods. The authors develop a specialized neural network architecture with a learning curve layer, which improves learning curve predictions. They evaluate their approach on various datasets, demonstrating its effectiveness in predicting asymptotic values of partially observed curves and unobserved learning curves. The paper also extends the Hyperband optimization method using the proposed model, showing improved performance in hyperparameter optimization.
Decision and Key Reasons
I decide to reject this paper, with two key reasons:
1. Lack of clarity and differing terminology: The exposition is hard to follow due to unclear terminology and lack of clarity on novel aspects of the paper, particularly in relation to matching networks.
2. Insufficient experimental validation: The paper lacks comprehensive experimental validation through comparisons against prior work, which is necessary given the similarity in spirit to other proposed methods.
Supporting Arguments
The paper's meta-learning setup is confusing, with unclear representation of the query q and differing weighting of data points compared to a natural formulation. Additionally, the paper's claims are not sufficiently supported by empirical evidence, making it difficult to assess the effectiveness of the proposed approach.
Additional Feedback and Suggestions
To improve the paper, I suggest the authors:
* Clarify the terminology and notation used throughout the paper
* Provide a more detailed comparison with prior work, including matching networks
* Conduct more comprehensive experiments to validate the effectiveness of the proposed approach
* Consider using more robust evaluation metrics to assess the performance of the proposed method
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide a more detailed explanation of the meta-learning setup and how it differs from existing approaches?
* How do you plan to address the lack of clarity and differing terminology in the paper?
* Can you provide more comprehensive experimental results, including comparisons with prior work and more robust evaluation metrics?