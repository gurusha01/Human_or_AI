Summary
The paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses to learn the embeddings. The approach is evaluated on three tasks: acoustic word discrimination, cross-view word discrimination, and word similarity. The results show that the proposed approach improves over previous methods on acoustic word discrimination and achieves encouraging results on cross-view discrimination and word similarity.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the absence of proper Automatic Speech Recognition (ASR) technique-based baselines is a significant drawback, as it makes it difficult to compare the performance of the proposed approach with existing state-of-the-art methods. Secondly, the dataset used is quite small, which may impact the reliability of the results and limit the generalizability of the proposed approach to larger datasets.
Supporting Arguments
The paper presents an interesting approach to learning acoustic word embeddings, and the use of multi-view contrastive losses is a novel contribution. However, the lack of ASR technique-based baselines makes it challenging to evaluate the effectiveness of the proposed approach in a real-world setting. Additionally, the small dataset used in the experiments may not be representative of the complexity and variability of real-world speech data. The paper also lacks a detailed analysis of the results, including comparisons of precision for in-vocabulary and out-of-vocabulary words, which would provide a more comprehensive understanding of the proposed approach's strengths and weaknesses.
Additional Feedback
To improve the paper, I suggest including ROC curves and measurements like EER, in addition to AP, for a more comprehensive evaluation. The authors could also provide a more detailed analysis of the results, including comparisons of precision for in-vocabulary and out-of-vocabulary words. Furthermore, the addition of scatter plots to visualize the relationship between embedding and orthographic distances would be a useful visualization tool. The authors may also consider using a larger dataset or exploring data augmentation techniques to increase the size and diversity of the training data.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence to support my assessment, I would like the authors to answer the following questions:
1. How do the authors plan to address the lack of ASR technique-based baselines in future work?
2. Can the authors provide more details on the dataset used in the experiments, including the number of speakers, recording conditions, and vocabulary size?
3. How do the authors plan to extend the proposed approach to larger datasets and more complex speech recognition tasks?