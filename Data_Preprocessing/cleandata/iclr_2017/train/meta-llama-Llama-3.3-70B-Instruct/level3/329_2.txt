Summary
The paper proposes a novel recurrent neural network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which utilizes orthogonal matrices to update the hidden state. The authors claim that this architecture can learn long-term dependencies without suffering from vanishing or exploding gradients. The paper provides a detailed explanation of the DORNN architecture, its properties, and its application to a simplified memory copy task.
Decision
I decide to Accept this paper, with the main reason being that the authors have proposed a well-motivated and novel approach to addressing the vanishing and exploding gradient problems in RNNs. The paper is well-written, and the authors have provided a clear explanation of their architecture and its properties.
Supporting Arguments
The paper tackles a specific question/problem in the field of RNNs, which is the vanishing and exploding gradient problem. The authors have provided a well-motivated approach to addressing this problem, and their architecture is well-placed in the literature. The paper supports its claims with theoretical proofs and experimental results, which demonstrate the effectiveness of the DORNN architecture in learning long-term dependencies.
Additional Feedback
To improve the paper, I would like to see more experimental results on more complicated tasks, such as language modeling or image captioning. Additionally, the authors could provide more analysis on the trade-offs between the DORNN architecture and other RNN architectures, such as LSTMs or GRUs. It would also be helpful to see a comparison of the DORNN architecture with other orthogonal RNN architectures, such as the Unitary Evolution Recurrent Neural Network (uRNN).
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to extend the DORNN architecture to more complicated tasks, such as language modeling or image captioning?
* Can the authors provide more analysis on the trade-offs between the DORNN architecture and other RNN architectures, such as LSTMs or GRUs?
* How does the DORNN architecture compare to other orthogonal RNN architectures, such as the uRNN?
* Can the authors provide more details on the optimization process for the DORNN architecture, and how it compares to other optimization methods for RNNs?