Summary
The paper introduces a novel recurrent neural network architecture, multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM's ability to have different recurrent transition functions for each possible input makes it more expressive for autoregressive density estimation. They demonstrate the effectiveness of mLSTM on various character-level language modeling tasks, showing improvements over standard LSTM and its deep variants.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the presentation of the proposed algorithm is unclear and could be improved with pseudo-code or diagrams to describe the compute flow and updates. Secondly, the evaluation is flawed due to the absence of comparison with baseline async methods, such as using a parameter server.
Supporting Arguments
The paper's lack of clarity in presenting the algorithm makes it difficult to understand the specifics of the proposed architecture. The use of mathematical notation alone is not sufficient to convey the intricacies of the model. Additionally, the evaluation of the model is limited, as it does not compare the performance of mLSTM with other state-of-the-art models, such as those using parameter servers. This makes it challenging to assess the true effectiveness of the proposed architecture.
Additional Feedback
To improve the paper, I suggest that the authors provide a clearer presentation of the algorithm, including pseudo-code or diagrams to illustrate the compute flow and updates. Additionally, they should consider comparing the performance of mLSTM with other baseline models, including those using parameter servers. This would provide a more comprehensive evaluation of the proposed architecture. I would also like the authors to clarify the meaning of "SGD task-wise, 1 comm" in Figure 1 and whether it refers to layer-wise computation.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a step-by-step explanation of the mLSTM algorithm, including any necessary mathematical derivations?
2. How does the proposed architecture handle vanishing gradients, and what measures are taken to prevent this issue?
3. Can you provide more details on the experimental setup, including the specific hyperparameters used and the computational resources required to train the models?