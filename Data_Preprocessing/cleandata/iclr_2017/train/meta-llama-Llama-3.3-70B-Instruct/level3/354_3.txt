The paper proposes a novel approach to integrate efficient inference within the Generative Adversarial Network (GAN) framework, called Adversarially Learned Inference (ALI). The method jointly learns a generation network and an inference network using an adversarial process, allowing for the learning of mutually coherent inference and generation networks. The paper is well-written, and the experiments are thorough, demonstrating the effectiveness of ALI in learning useful representations for semi-supervised learning tasks.
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to addressing the limitation of GANs in learning an efficient inference mechanism. The paper provides a clear and concise description of the proposed method, and the experimental results demonstrate its effectiveness in learning useful representations.
The approach is well-motivated, as it addresses a significant limitation of GANs, which is the lack of an efficient inference mechanism. The authors provide a clear explanation of the problem and how ALI addresses it. The paper is also well-placed in the literature, as it builds upon existing work on GANs and Variational Autoencoders (VAEs).
However, I do have some concerns regarding the comparison with "true ensembles" of networks trained independently. The authors claim that ALI almost matches the results of true ensembles, but the results in Table 4 suggest that the snapshot ensemble achieves approximately 66% of the improvement of the true ensemble over the single baseline model. I would like to see a more thorough comparison with true ensembles to better understand the effectiveness of ALI.
To improve the paper, I suggest that the authors provide more analysis on the diversity of the snapshot ensembles and compare it with other ensembling techniques, such as true ensembles and ensembles from dropout. This would help to better understand the strengths and limitations of ALI.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) Can you provide more details on the experimental setup and hyperparameter tuning for the semi-supervised learning tasks? (2) How do you plan to address the issue of mode collapse in ALI, which is a common problem in GANs? (3) Can you provide more insights on the relationship between ALI and other generative models, such as VAEs and autoregressive models?