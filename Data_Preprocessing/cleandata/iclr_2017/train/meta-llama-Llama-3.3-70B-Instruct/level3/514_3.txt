Summary of the Paper
The paper proposes an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The model is designed to learn a beneficial common initialization that serves as a good starting point for fine-tuning, allowing the classifier to quickly converge to a good solution after a few updates. The authors demonstrate that their meta-learning model is competitive with deep metric-learning techniques for few-shot learning.
Decision
I decide to reject this paper, with the main reason being that the paper is difficult to understand due to its overly abstract presentation. Despite being clearly written, the lack of concrete examples and unclear notation and terminology make it hard for readers to connect the concepts and understand the topic.
Supporting Arguments
The paper lacks concrete examples to ground its abstract formalism, making it challenging for readers to understand how the abstract concepts relate to concrete cases. The use of specific notation and terminology, such as "(x, x')" pairs, Cardinals, and Cartesian convolutions, is unclear and requires further explanation. Additionally, the relationships between different variables and symbols, such as x, xI, and xS, are not well-explained, making it difficult to understand how they relate to specific examples like convolving a signal with a filter.
Additional Feedback
To improve the paper, I suggest that the authors provide more explanations and examples to help readers understand the concepts. This could include adding more intuitive explanations of the meta-learning formulation, the LSTM-based meta-learner model, and the training algorithm. Additionally, the authors could provide more details on the relationships between different variables and symbols, and how they relate to specific examples. The paper could also benefit from more hand-holding and explanations to help readers understand the concepts, rather than leaving it to the reader to figure out the connections between ideas.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more intuitive explanations of the meta-learning formulation and the LSTM-based meta-learner model?
2. How do the relationships between different variables and symbols, such as x, xI, and xS, relate to specific examples like convolving a signal with a filter?
3. Can you provide more concrete examples to illustrate the abstract concepts and notation used in the paper?