Summary of the Paper
The paper proposes a novel recurrent neural network architecture, called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM is better suited for sequence modeling tasks, particularly those with discrete and mutually exclusive elements, such as language modeling. The paper presents a series of experiments on character-level language modeling tasks, demonstrating the effectiveness of mLSTM in improving performance over standard LSTM and its deep variants.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's performance is underwhelming, and the authors' decision not to use batch normalization, layer normalization, or zoneout is questionable. Secondly, the connection between dynamic evaluation and fast weights in the paper is misleading, as dynamic evaluation uses test error signals, which may not always be available.
Supporting Arguments
The paper proposes an extension of the multiplicative RNN by applying a reparametrization trick to the weight matrices of the LSTM, introducing some interesting but non-crucial tricks. However, the authors' proposed tricks, such as multiplying the output gate and sharing m_t across inference, bring only a slight increase in model parameters, but their impact is not thoroughly evaluated. Furthermore, the paper's performance is often worse than other methods, only improving when dynamic evaluation is used, which could also benefit other models.
Additional Feedback
To improve the paper, I suggest that the authors provide a more thorough evaluation of their proposed tricks and their impact on the model's performance. Additionally, the authors should consider using batch normalization, layer normalization, or zoneout to improve the model's performance. The authors should also clarify the connection between dynamic evaluation and fast weights, and provide more evidence to support their claims.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How do the authors plan to address the issue of overfitting, particularly in the case of large-scale language modeling tasks?
2. Can the authors provide more evidence to support their claim that mLSTM is more robust to mistakes when encountering surprising inputs?
3. How do the authors plan to extend their work to word-level language modeling and other discrete multinomial generative modeling tasks?