This paper presents a novel approach to neural language modeling by introducing a key-value-predict attention mechanism, which separates the output vectors into distinct representations for querying a memory of previous token representations. The authors demonstrate that this new architecture outperforms existing memory-augmented neural language models on two corpora, including the Children's Book Test and a Wikipedia corpus. The paper's strengths lie in its exploration of hierarchies in two dimensions and effective memory selection, which enables the model to capture mid- and long-range dependencies.
I decide to accept this paper, with the primary reason being its originality and potential to advance the field of neural language modeling. The paper's contributions, including the proposed key-value-predict attention mechanism and the N-gram RNN model, offer a fresh perspective on how to improve language modeling performance. Additionally, the authors' finding that simple models can be on par with more sophisticated ones highlights the importance of careful evaluation and comparison of different architectures.
To further improve the paper, I suggest that the authors provide clearer definitions and optimization descriptions, as well as a survey of music students' opinions to evaluate system performance. It would also be beneficial to investigate ways to encourage attending over a longer history, as the authors mention that training neural language models to leverage long-range dependencies is notoriously hard. Some questions I would like the authors to answer include: How do the authors plan to extend their model to capture longer-range dependencies? What are the potential applications of the key-value-predict attention mechanism beyond language modeling? How do the authors envision the N-gram RNN model being used in practice, and what are its potential limitations? 
Overall, this paper presents a significant contribution to the field of neural language modeling, and with some revisions to address the mentioned limitations, it has the potential to make a substantial impact on the development of more effective language models.