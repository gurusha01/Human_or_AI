Summary
The paper proposes a technique to reduce the parameters of a Recurrent Neural Network (RNN) by pruning weights during the initial training of the network. The authors demonstrate that their approach can achieve sparsity of 90% with a small loss in accuracy, and in some cases, even improve the accuracy over a dense baseline. The technique is computationally efficient, easy to implement, and can be applied to various RNN architectures, including vanilla RNNs and Gated Recurrent Units (GRUs).
Decision
I decide to Accept this paper, with the primary reason being the novelty and effectiveness of the proposed pruning technique. The authors provide a thorough evaluation of their approach, demonstrating its ability to reduce model size while maintaining or even improving accuracy.
Supporting Arguments
The paper is well-motivated, and the authors provide a clear overview of the challenges associated with deploying large RNN models on mobile devices and embedded systems. The proposed technique is well-placed in the literature, and the authors provide a thorough discussion of related work in the area of model pruning and quantization. The experimental results are convincing, demonstrating the effectiveness of the proposed approach on various RNN architectures and datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details about the construction of the pruning algorithm, including the choice of hyperparameters and the threshold function. Additionally, the authors could provide more insights into the trade-offs between model size, accuracy, and computational efficiency. It would also be helpful to include more visualizations, such as diagrams or plots, to illustrate the pruning process and the resulting sparse models.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details about the choice of hyperparameters, such as the start and end iteration counts, and the threshold function?
2. How do you determine the optimal sparsity level for a given model and dataset?
3. Have you explored the application of your pruning technique to other deep learning models, such as convolutional neural networks (CNNs)?