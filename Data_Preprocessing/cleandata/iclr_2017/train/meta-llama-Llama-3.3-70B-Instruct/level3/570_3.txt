This paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by leveraging the knowledge of pre-trained deterministic deep neural networks (DNNs). The authors introduce an intermediate model, called Simplified-SFNN, which approximates SFNN by simplifying its upper latent units above stochastic ones. This connection enables an efficient training procedure for SFNNs using the pre-trained parameters of DNNs.
The paper tackles the specific question of developing efficient training methods for large-scale SFNNs, which is a significant problem in the field of neural networks. The approach is well-motivated, as it builds upon the existing literature on SFNNs and DNNs, and provides a rigorous connection between the two models.
The paper supports its claims through extensive experiments on various datasets, including MNIST, TFD, CIFAR-10, CIFAR-100, and SVHN. The results demonstrate that the proposed approach outperforms the baseline DNNs and SFNNs trained using simple transformations. The authors also provide a detailed analysis of the regularization effect of Simplified-SFNN, which is an essential aspect of the paper.
Based on the provided information, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper proposes a novel and well-motivated approach to training SFNNs, which addresses a significant problem in the field of neural networks.
2. The paper provides extensive experimental results that demonstrate the effectiveness of the proposed approach, including a detailed analysis of the regularization effect of Simplified-SFNN.
To further improve the paper, I provide the following feedback:
* The authors could provide more insights into the choice of hyper-parameters, such as the number of samples used for estimating the expectations in Simplified-SFNN.
* The authors could discuss the potential applications of the proposed approach in other domains, such as natural language processing or computer vision.
* The authors could provide more details on the computational complexity of the proposed approach, including the training time and memory requirements.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can the authors provide more details on the implementation of Simplified-SFNN, including the specific architectures and hyper-parameters used in the experiments?
* How do the authors plan to extend the proposed approach to more complex models, such as recurrent neural networks or generative adversarial networks?
* Can the authors provide more insights into the theoretical aspects of the proposed approach, including the convergence properties of the training algorithm and the regularization effect of Simplified-SFNN?