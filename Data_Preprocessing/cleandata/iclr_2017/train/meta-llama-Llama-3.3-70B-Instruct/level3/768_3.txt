This paper proposes a simple yet effective approach to exploration in deep reinforcement learning (RL) by extending classic count-based methods to high-dimensional state spaces through hashing. The authors demonstrate that their method, which uses a hash function to discretize the state space and assigns a bonus reward based on the state-visitation count, can achieve near state-of-the-art performance on various benchmarks, including continuous control tasks and Atari 2600 games.
The key insight of this paper is that decorrelated neurons can provide independent pieces of information to subsequent decisions, giving "complementary viewpoints" of the input to the subsequent layers. The authors propose a method to decorrelate neuron activations by splitting them into "foreground" and "background" subsets and using side-losses to force them to be zero on respective pixels. This approach is demonstrated to improve classification on a mid-scale example, outperforming a "vanilla" baseline that does not use these losses.
To evaluate this paper, I will answer the three key questions outlined in the conference guidelines:
1. What is the specific question/problem tackled by the paper?
The paper tackles the problem of exploration in deep RL, specifically how to balance exploration and exploitation in high-dimensional state spaces.
2. Is the approach well motivated, including being well-placed in the literature?
The approach is well-motivated, as it builds on classic count-based methods and addresses the limitations of these methods in high-dimensional state spaces. The paper provides a clear overview of the related work and demonstrates how their approach improves upon existing methods.
3. Does the paper support the claims?
The paper provides extensive experimental results to support its claims, demonstrating the effectiveness of their approach on various benchmarks. The results show that their method can achieve near state-of-the-art performance on continuous control tasks and Atari 2600 games, and that it is robust to hyperparameter changes.
Based on these questions, I decide to Accept this paper. The paper presents a simple yet effective approach to exploration in deep RL, and the experimental results demonstrate its effectiveness on various benchmarks. The approach is well-motivated, and the paper provides a clear overview of the related work.
To improve the paper, I suggest that the authors provide more analysis on the choice of hash function and its impact on the performance of the algorithm. Additionally, it would be interesting to see more experiments on the robustness of the algorithm to different hyperparameter settings and environments.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors choose the hash function, and what are the trade-offs between different hash functions?
* How does the algorithm perform in environments with very high-dimensional state spaces, such as those with large images or complex game states?
* Can the authors provide more insight into the learned binary representation, and how it relates to the state space and the task at hand?