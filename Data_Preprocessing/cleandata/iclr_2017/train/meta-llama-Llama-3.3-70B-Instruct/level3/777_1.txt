Summary
The paper proposes a novel recurrent neural network architecture called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The mLSTM architecture is designed to have flexible input-dependent transitions, allowing it to recover quickly from surprising inputs and maintain long-term information. The authors demonstrate the effectiveness of mLSTM on various character-level language modeling tasks, showing improvements over standard LSTM and its deep variants.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the derivation of Equation (3) from total correlation is questionable and lacks careful justification, particularly in estimating H(X|Z) and H(Z|Y). Secondly, the proposal lacks clarity on whether σ in Equation 8 is a trainable parameter or hyperparameter, and how it is trained or set, with unclear notation for j.
Supporting Arguments
The paper's approach to combining LSTM and mRNN architectures is well-motivated, and the authors provide a clear explanation of the benefits of flexible input-dependent transitions. However, the mathematical derivations and notation used in the paper are not always clear, which makes it difficult to fully understand the proposed architecture. Additionally, the experiments are limited, and the authors should provide more comprehensive results, including mean and standard error of results from multiple runs with different random subsets.
Additional Feedback
To improve the paper, the authors should provide more detailed explanations of the mathematical derivations, particularly for Equation (3). They should also clarify the role of σ in Equation 8 and provide more information on how it is trained or set. Furthermore, the authors should consider adding more experiments to demonstrate the effectiveness of mLSTM on a wider range of tasks and datasets. It would also be helpful to include a more detailed comparison with existing methods, such as Gaussian dropout and DisturbLabel.
Questions for the Authors
I would like the authors to clarify the following points:
* Can you provide a more detailed explanation of the derivation of Equation (3) from total correlation, particularly in estimating H(X|Z) and H(Z|Y)?
* Is σ in Equation 8 a trainable parameter or hyperparameter, and how is it trained or set?
* Can you provide more information on the notation used for j in Equation 8?
* How do you plan to extend the mLSTM architecture to tasks with continuous or non-sparse input units?