Summary
The paper introduces a novel approach to safe reinforcement learning, called intrinsic fear, which aims to prevent deep Q-networks (DQNs) from perpetually revisiting catastrophic failure modes. The authors propose a danger model that predicts the likelihood of entering a catastrophic state within a short number of steps and uses this prediction to penalize the Q-learning objective. The approach is evaluated on several environments, including Adventure Seeker, Cart-Pole, and Seaquest, and shows promising results in reducing the number of catastrophes and improving the overall performance.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and important problem in deep reinforcement learning, namely the brittleness of DQNs in the face of catastrophic failures, and (2) the proposed approach, intrinsic fear, shows promising results in addressing this problem.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of safe reinforcement learning and the limitations of current approaches. The authors provide a thorough analysis of the failures of DQNs in several environments and propose a novel solution that addresses the root cause of these failures. The experimental results demonstrate the effectiveness of the intrinsic fear approach in reducing the number of catastrophes and improving the overall performance.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the danger model and the hyperparameter tuning process. Additionally, it would be interesting to see more experiments on more complex environments and a comparison with other approaches to safe reinforcement learning. I also have some questions for the authors: (1) How do the authors plan to formalize the notion of danger and catastrophe in future work? (2) What are the potential limitations and challenges of applying the intrinsic fear approach to more complex domains? (3) How does the intrinsic fear approach relate to other approaches to safe reinforcement learning, such as reward shaping and constrained reinforcement learning? 
Questions for the Authors
1. Can you provide more details on the implementation of the danger model, including the architecture and training procedure?
2. How did you tune the hyperparameters of the intrinsic fear approach, such as the fear factor and fear radius?
3. What are the potential applications and limitations of the intrinsic fear approach in real-world domains, such as robotics and autonomous driving?
4. How does the intrinsic fear approach relate to other approaches to safe reinforcement learning, such as reward shaping and constrained reinforcement learning?
5. What are the plans for future work on formalizing the notion of danger and catastrophe, and exploring the effectiveness of the intrinsic fear approach on more complex domains?