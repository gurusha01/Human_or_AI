This paper introduces a novel design strategy for neural network macro-architecture based on self-similarity, resulting in fractal networks that can match the performance of standard residual networks on various classification tasks. The authors propose a simple expansion rule to generate deep networks with a truncated fractal structure, which contains interacting subpaths of different lengths without any pass-through or residual connections. The paper also develops drop-path, a regularization protocol that prevents co-adaptation of parallel paths by randomly dropping operands of the join layers.
The paper claims to contribute to the field by introducing FractalNet, a simple alternative to ResNet, and by elucidating connections between FractalNet and various phenomena engineered into previous deep network designs. The authors demonstrate the effectiveness of their approach through experiments on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets, showing that fractal networks can achieve competitive results with residual networks.
Based on the provided guidelines, I will evaluate the paper as follows:
1. The specific question/problem tackled by the paper is the design of ultra-deep neural networks that can be trained effectively without relying on residual connections. The paper proposes a novel fractal network architecture and a regularization protocol called drop-path to address this problem.
2. The approach is well-motivated, and the paper provides a clear and concise overview of the related work in the field. The authors demonstrate a good understanding of the existing literature and position their work within the context of recent advances in deep learning.
3. The paper supports its claims through extensive experiments on various datasets, demonstrating the effectiveness of fractal networks and drop-path regularization. The results are scientifically rigorous, and the authors provide a detailed analysis of the internal behavior of fractal networks, connecting it to phenomena such as deep supervision and student-teacher learning.
Based on these evaluations, I decide to Accept this paper. The key reasons for this choice are:
* The paper introduces a novel and well-motivated approach to designing ultra-deep neural networks, which has the potential to advance the field.
* The experiments are extensive and demonstrate the effectiveness of the proposed approach, providing strong evidence for the claims made in the paper.
Additional feedback to improve the paper:
* The authors could provide more insights into the computational cost and memory requirements of fractal networks compared to residual networks.
* It would be interesting to see more experiments on larger datasets, such as ImageNet, to further demonstrate the scalability of fractal networks.
* The authors could provide more details on the implementation of drop-path regularization, including the choice of hyperparameters and the effect of different sampling strategies.
Questions to the authors:
* Can you provide more insights into the choice of the expansion rule and the design of the fractal network architecture?
* How do you plan to extend the fractal network architecture to other domains, such as natural language processing or speech recognition?
* Can you provide more details on the connection between fractal networks and other phenomena, such as deep supervision and student-teacher learning, and how these connections can be leveraged to improve the performance of fractal networks?