Summary
The paper proposes a neural network architecture and statistical framework for modeling frames in videos, inspired by computer graphics pipelines. The approach, called Perception Updating Networks (PUN), explicitly represents "sprites" or percepts inferred from the scene and infers their movement independently of their content. The authors demonstrate the effectiveness of PUN on synthetic datasets, including bouncing shapes and moving MNIST, and show that it can generate videos that are more interpretable and better suited for long-term generation than baseline RNNs.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper does not present anything particularly novel on top of existing work in the field, such as the use of variational auto-encoding Bayes and spatial transformer networks. Secondly, the experimental study lacks important baseline numbers, such as direct + LM + bias and direct + bias, which would provide a more complete comparison.
Supporting Arguments
The paper's approach is based on the segment-to-segment neural transduction (SSNT) model, but it does not provide significant new insights or improvements over this existing work. The use of a language model (LM) may be key to explaining why the noisy channel model outperforms the direct model, but this is not fully explored in the paper. Additionally, the direct model used in the experiments is unclear, and it is not specified whether it is based on SSNT or a sequence-to-sequence model. The training complexity of O(|x|^2*|y|) is still relatively expensive for long input sequences, and further reduction in computational cost would be beneficial for applications such as paragraph or document level modeling.
Additional Feedback
To improve the paper, the authors could provide more detailed comparisons with existing work, including baseline numbers and analysis of the strengths and weaknesses of their approach. They could also explore the use of language models and other techniques to improve the performance of their model. Additionally, the authors could consider providing more visualizations and examples of the generated videos to help illustrate the effectiveness of their approach.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How does the proposed approach differ from existing work in the field, such as the use of variational auto-encoding Bayes and spatial transformer networks?
* Can the authors provide more detailed comparisons with existing work, including baseline numbers and analysis of the strengths and weaknesses of their approach?
* How do the authors plan to address the high training complexity of their model, and what techniques do they propose to reduce the computational cost for long input sequences?