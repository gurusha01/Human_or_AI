Summary
The paper proposes a novel approach to neural sequence modeling, introducing quasi-recurrent neural networks (QRNNs) that combine the strengths of convolutional and recurrent neural networks. QRNNs allow for parallel computation across both timestep and minibatch dimensions, enabling high throughput and good scaling to long sequences. The authors demonstrate the effectiveness of QRNNs on three natural language tasks: document-level sentiment classification, language modeling, and character-level neural machine translation, outperforming LSTM-based models of equal hidden size while reducing computation time.
Decision
I decide to Accept this paper, with the primary reason being the innovative approach to neural sequence modeling that QRNNs represent, offering a promising solution to the limitations of traditional RNNs. The second key reason is the thorough evaluation of QRNNs on various tasks, demonstrating their competitive performance and efficiency.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the QRNN architecture, highlighting its advantages over traditional RNNs. The authors also provide a comprehensive evaluation of QRNNs on multiple tasks, including sentiment classification, language modeling, and machine translation, demonstrating their effectiveness and efficiency. The use of densely-connected layers, zoneout regularization, and attention mechanisms further enhances the QRNN architecture, making it a robust and versatile model for sequence tasks.
Additional Feedback
To further improve the paper, I suggest the authors provide more intuition on the choice of hyperparameters, such as the filter width and the number of layers. Additionally, the authors could explore the application of QRNNs to other sequence tasks, such as speech recognition or time-series forecasting. The visualization in Figure 3 could be improved by removing some classes, and Figure 4 is missing a legend. The authors should also consider discussing the potential limitations of QRNNs, such as their reliance on convolutional layers, which may not be suitable for all sequence tasks.
Questions for the Authors
1. Can you provide more insight into the choice of filter width and the number of layers in the QRNN architecture?
2. How do you plan to address the potential limitations of QRNNs, such as their reliance on convolutional layers?
3. Have you considered applying QRNNs to other sequence tasks, such as speech recognition or time-series forecasting? If so, what were the results?