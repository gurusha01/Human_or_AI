This paper explores the performance and trainability characteristics of various neural network architectures, particularly RNN motifs, in a clear and systematic way. The authors address an important question about the comparison of different neural network architectures, providing a useful answer that LSTMs are reliable but GRUs are the better choice in typical training environments. The work highlights the importance of distinguishing between capacity and trainability in neural networks, showing that gated architectures are more easily trainable but have lower capacity than vanilla RNNs.
The paper emphasizes the significance of hyperparameter tuning and quantifying the fraction of infeasible parameters in neural networks. The findings on the near-equivalence of capacity at equal numbers of parameters and the importance of hyperparameter tuning are particularly useful. However, the work on UGRNNs and +RNNs seems preliminary and requires more statistics to support the recommendation of +RNN as a general architecture.
The paper lacks details on the hyperparameter tuning algorithm used, making it difficult to replicate the results. Some figures in the paper are hard to read due to small panels, excessive details, and poor visual design choices. The reference to neuroscience in the paper seems gratuitous and lacks a clear connection to the computational architectures discussed.
Based on the conference guidelines, I will answer the three key questions:
1. What is the specific question/problem tackled by the paper?
The paper tackles the problem of comparing different neural network architectures and identifying the most suitable one for a given task.
2. Is the approach well motivated, including being well-placed in the literature?
The approach is well-motivated, and the paper provides a clear and systematic comparison of different neural network architectures.
3. Does the paper support the claims?
The paper provides some evidence to support its claims, but the work on UGRNNs and +RNNs seems preliminary, and more statistics are needed to fully support the recommendations.
Based on these questions, I decide to Reject the paper, with the main reason being the lack of sufficient evidence to support the claims made about UGRNNs and +RNNs.
To improve the paper, I suggest the following:
* Provide more details on the hyperparameter tuning algorithm used.
* Improve the visual design of the figures to make them easier to read.
* Clarify the connection between the computational architectures discussed and the reference to neuroscience.
* Provide more statistics to support the recommendation of +RNN as a general architecture.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence:
* Can you provide more details on the hyperparameter tuning algorithm used?
* How do you plan to address the issue of poor visual design in the figures?
* Can you clarify the connection between the computational architectures discussed and the reference to neuroscience?
* Can you provide more statistics to support the recommendation of +RNN as a general architecture?