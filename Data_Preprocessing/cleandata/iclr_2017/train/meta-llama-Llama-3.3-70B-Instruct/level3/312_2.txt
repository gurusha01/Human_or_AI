Summary of the Paper's Contributions
The paper presents a novel method for searching optimal neural network architectures using an actor-critic framework, treating deep neural networks (DNNs) as variable-length sequences and utilizing reinforcement learning (RL) to find the target architecture. The approach is evaluated on two different problems, CIFAR-10 and Penn Treebank, and compared with human-created architectures, demonstrating its capability and differences between generated and human-generated architectures. The paper is well-written, clear, and accessible, with good coverage and contrast with related literature.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Accept it. The two key reasons for this choice are: (1) the paper tackles a specific and important problem in the field of deep learning, namely, the automated design of neural network architectures, and (2) the approach is well-motivated, and the results demonstrate the effectiveness of the method in finding good architectures that rival human-designed ones.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of neural architecture search and motivates the use of RL to solve it. The method is well-described, and the experiments are thorough and well-designed. The results on CIFAR-10 and Penn Treebank demonstrate the effectiveness of the approach in finding good architectures that are competitive with human-designed ones. The paper also provides a good discussion of related work and highlights the advantages and limitations of the proposed method.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors consider analyzing the time and resources needed for training and their correlation with model quality. Additionally, exploring human bootstrapped models could provide valuable insights into the design of neural network architectures. It would also be interesting to see the application of the proposed method to other domains and tasks.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the computational resources required for training the controller and the child networks?
2. How do you plan to extend the proposed method to other domains and tasks, such as natural language processing and computer vision?
3. Can you provide more insights into the design of the reward function and its impact on the search process?
Overall, the paper presents a significant contribution to the field of deep learning, and I believe that it has the potential to inspire further research in the area of neural architecture search.