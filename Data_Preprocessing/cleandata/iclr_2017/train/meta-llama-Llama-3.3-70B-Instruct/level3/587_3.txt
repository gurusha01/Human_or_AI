Summary
The paper proposes a novel dynamic normalization technique, called Charged Point Normalization (CPN), which enables gradient-based optimization algorithms to escape saddle points in high-dimensional non-convex optimization problems. The authors introduce a dynamic normalization function that rewards the optimization algorithm for taking steps that maximize the distance between the new point and the trailing point, allowing the algorithm to escape saddle points and converge to a better solution.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a clear comparison with existing methods for escaping saddle points, such as operator fusion methods that do not require retraining. Secondly, the experimental setup and results are not thoroughly explained, making it difficult to reproduce the results and understand the effectiveness of CPN.
Supporting Arguments
The paper proposes an interesting idea, but the lack of comparison with existing methods makes it difficult to assess the novelty and significance of CPN. Additionally, the experimental setup is not well-explained, and the results are not comprehensive, which raises concerns about the validity and reliability of the findings. For example, the paper does not provide a clear explanation of how the hyperparameters were chosen, and the results are not compared to other optimization algorithms.
Additional Feedback
To improve the paper, I suggest that the authors provide a more comprehensive comparison with existing methods for escaping saddle points, including operator fusion methods that do not require retraining. Additionally, the authors should provide more details on the experimental setup, including the relationship between the depth of fused layers and original layers, and clarify how the hyperparameters were chosen. The authors should also consider open-sourcing their implementation to enhance the usefulness of their work.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. How do the authors choose the hyperparameters for CPN, and what is the effect of different hyperparameter settings on the performance of CPN?
2. Can the authors provide a more comprehensive comparison with existing methods for escaping saddle points, including operator fusion methods that do not require retraining?
3. How do the authors ensure that the exponential decay term in CPN does not nullify the effects of CPN when saddle points are reached at a late time in the optimization algorithm?