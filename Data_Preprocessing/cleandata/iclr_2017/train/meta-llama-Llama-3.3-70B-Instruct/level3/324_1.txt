Summary
The paper proposes a novel neural network architecture, called Perception Updating Networks (PUN), for modeling and generating videos of 2D scenes. The approach is inspired by computer graphics pipelines and variational auto-encoding Bayes, and it decouples the representation of objects (sprites) and their dynamics in a video. The authors demonstrate the effectiveness of their approach on synthetic datasets, including bouncing shapes and moving MNIST digits.
Decision
I decide to Accept this paper, with the main reason being the novelty and potential of the proposed approach. The paper presents a well-motivated and well-placed idea in the literature, and the experiments demonstrate encouraging results.
Supporting Arguments
The paper is well-written, and the description of the approach is clear and easy to follow. The use of figures and equations helps to illustrate the key concepts, and the specification of layer mappings is well-defined. The experiments are well-done, and the results are promising, especially in terms of generating interpretable and long videos. The comparison to baseline RNNs is also helpful in demonstrating the advantages of the proposed approach.
Additional Feedback
To further improve the paper, I suggest that the authors consider additional experiments to evaluate the effectiveness of their approach on more complex datasets, such as real-world videos. It would also be helpful to compare the proposed approach to other state-of-the-art methods in video generation, such as Video Pixel Networks. Additionally, the authors may want to consider exploring ways to improve the internal dynamics of the RNN, such as using more advanced RNN architectures or incorporating additional memory units.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the sprite memory is learned and updated during training?
2. How do you handle cases where the sprites change or occlude each other in the video?
3. Have you considered using other types of neural network architectures, such as convolutional neural networks (CNNs) or graph neural networks (GNNs), to model the sprites and their dynamics?
Overall, I believe that the paper presents a promising approach to video generation, and with some additional experiments and comparisons, it has the potential to make a significant contribution to the field.