The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The approach, called SGDR, simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster. The paper also demonstrates new state-of-the-art results with SGDR, mainly by using even wider WRNs and ensembles of snapshots from SGDR's trajectory.
I decide to reject this paper, with two key reasons for this choice. Firstly, the idea of combining warm restarts with SGD is not entirely new, and the paper builds upon previously published work by the authors. Secondly, the paper lacks a thorough comparison with other state-of-the-art optimization methods, such as AdaDelta and Adam, which makes it difficult to assess the significance of the proposed approach.
The paper's approach is well-motivated, and the use of warm restarts to improve the convergence rate of SGD is a valid idea. However, the paper's contribution is limited to applying this idea to SGD, and the results, although impressive, do not provide significant new insights into the optimization of deep neural networks. The paper's evaluation is also limited to a few datasets, and the results may not generalize to other domains.
To improve the paper, I suggest that the authors provide a more thorough comparison with other optimization methods and evaluate the approach on a wider range of datasets. Additionally, the authors could provide more insights into the theoretical foundations of the proposed approach and its relationship to other optimization methods. Some questions that I would like the authors to answer include: How does the proposed approach relate to other optimization methods, such as AdaDelta and Adam? Can the authors provide a more detailed analysis of the convergence rate of SGDR and its comparison to other methods? How do the authors plan to extend the approach to other optimization algorithms and domains?