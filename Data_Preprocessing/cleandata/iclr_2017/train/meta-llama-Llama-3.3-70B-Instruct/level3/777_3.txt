Summary
The paper introduces a novel recurrent neural network architecture, multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors argue that mLSTM's ability to have different recurrent transition functions for each possible input makes it more expressive for autoregressive density estimation. The paper demonstrates the effectiveness of mLSTM on various character-level language modeling tasks, showing improvements over standard LSTM and its deep variants.
Decision
I decide to reject this paper, primarily due to technical issues with its formulation and the lack of explicit probabilistic or information theoretic motivation for the chosen objective.
Supporting Arguments
The paper's formulation of the maximum total correlation procedure with target and noise perturbations has technical issues, particularly in the move from equation (1) to (2), which ignores H(Z) and encourages low entropy Z, potentially leading to entropy collapse. Additionally, the introduction of arbitrary balancing parameters lambda1 and lambda2 in equation (3) lacks theoretical motivation. The addition of L_{NLL} to the objective in equation (5) is also not justified and disconnects the conditional entropy of Z from the data it is to be conditioned on.
Additional Feedback
To improve the paper, the authors should provide a more rigorous theoretical motivation for the chosen objective and address the technical issues in the formulation. They should also consider providing more explicit connections to the literature on semantic noise modeling and information theory. Furthermore, the authors may want to explore the effectiveness of their approach on other tasks, such as word-level language modeling, to demonstrate its broader applicability.
Questions for the Authors
1. Can you provide a more detailed explanation of the theoretical motivation behind the introduction of lambda1 and lambda2 in equation (3)?
2. How do you address the potential issue of entropy collapse in the move from equation (1) to (2)?
3. Can you provide more insight into the relationship between the conditional entropy of Z and the data it is to be conditioned on, and how the addition of L_{NLL} affects this relationship?