Summary
The paper proposes a novel approach to accelerate gradient descent using layer-wise asynchronicity, achieving a speedup of up to 1.7x compared to synchronous training. The authors design and implement multiple approaches, including a baseline asynchronous gradient descent and a layer-wise gradient descent method, which overlaps weight updates of a layer with inter-node synchronization of other layers. The proposed method is evaluated on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system using well-known datasets and neural network topologies.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the baseline used for comparison is weak, which may not accurately reflect the effectiveness of the proposed method. Secondly, the paper fails to compare with parameter-server based methods, which are becoming the standard approach in the field.
Supporting Arguments
The proposed method's performance is compared to a synchronous approach, which may not be the most efficient or widely used method in practice. A more comprehensive evaluation would involve comparing the proposed method to other state-of-the-art approaches, including parameter-server based methods. Additionally, the paper lacks wall-time measurements, which are crucial for evaluating the performance of the proposed method in real-world scenarios.
Additional Feedback
To improve the paper, I suggest that the authors provide a more comprehensive evaluation of their proposed method, including comparisons to other state-of-the-art approaches and wall-time measurements. The authors should also consider providing more details on the implementation of their method, including any optimizations or techniques used to improve performance. Furthermore, the authors may want to explore the tradeoff between maintaining equivalence to sequential methods and leveraging computational resources, as this is an important consideration in the design of parallel algorithms.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the baseline approach used for comparison, and why it was chosen?
2. How do you plan to address the lack of comparison to parameter-server based methods, and what are the implications of this omission on the validity of your results?
3. Can you provide wall-time measurements for your proposed method, and how do these measurements impact the overall performance evaluation?