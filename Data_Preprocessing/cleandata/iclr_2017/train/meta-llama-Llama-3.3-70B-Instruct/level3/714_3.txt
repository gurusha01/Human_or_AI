Summary
The paper proposes a novel approach to learning state representations in a reinforcement learning setting using unsupervised learning with robotic priors. The authors claim that their method can learn a simple, low-dimensional representation of the world from images acquired by a robot, which can be used for transfer learning. The paper presents a deep neural network architecture that uses siamese networks to impose constraints on the learned representation, and demonstrates the effectiveness of the approach on a simulated robot head movement task.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the use of reward signals for training contradicts the definition of unsupervised learning, which is a fundamental aspect of the paper's approach. Secondly, the experiments are premature and overly simplistic, involving only basic robot head movements with no comparison to baseline methods.
Supporting Arguments
The paper's claim of using unsupervised learning is undermined by the use of reward signals, which is a characteristic of supervised learning. This inconsistency raises questions about the validity of the approach. Furthermore, the experiments are limited to a simple task and do not demonstrate the scalability or generalizability of the method. The lack of comparison to baseline methods makes it difficult to evaluate the effectiveness of the approach.
Additional Feedback
To improve the paper, the authors should clarify the role of reward signals in their approach and provide a more rigorous evaluation of the method's effectiveness. This could involve comparing the approach to baseline methods, such as supervised learning or other unsupervised learning techniques, and demonstrating the scalability of the method to more complex tasks. Additionally, the authors should provide more details on the implementation of the robotic priors and the siamese network architecture.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors reconcile the use of reward signals with the claim of using unsupervised learning?
2. Can the authors provide more details on the implementation of the robotic priors and the siamese network architecture?
3. How do the authors plan to evaluate the effectiveness of the approach on more complex tasks, and what baseline methods will they use for comparison?