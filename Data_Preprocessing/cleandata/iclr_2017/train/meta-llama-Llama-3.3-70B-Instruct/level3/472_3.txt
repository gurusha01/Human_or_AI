Summary of the Paper's Contributions
The paper proposes a novel approach to dialogue agents, where the agent learns to interact with users by asking questions. The authors design a simulator and a set of synthetic tasks in the movie question answering domain, allowing the agent to ask questions in both offline supervised and online reinforcement learning settings. The paper demonstrates that the agent improves its performance when asking questions, and this improvement is observed in both simulator and real-world experiments using Amazon Mechanical Turk.
Decision
I decide to Accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and well-motivated problem in the field of dialogue agents, and the approach is well-placed in the literature. Secondly, the paper provides a thorough experimental evaluation, including both simulator and real-world experiments, which demonstrates the effectiveness of the proposed approach.
Supporting Arguments
The paper's key contribution is the design of a simulator and a set of tasks that allow the agent to ask questions, which is a crucial aspect of human-like dialogue. The authors provide a thorough analysis of the results, including the benefits of asking questions in different settings, and the comparison between different models and training regimes. The use of Mechanical Turk to collect real-world data adds credibility to the results and demonstrates the potential of the approach in real-world applications.
Additional Feedback
To further improve the paper, I suggest that the authors consider evaluating the importance of having an architecture that matches the optimization algorithm, compared to a generic network. This analysis has been partially performed in the past, and it would be interesting to see how the authors' approach compares to other methods. Additionally, the authors may want to consider exploring more complex dialogue scenarios, such as multi-turn dialogues or dialogues with multiple participants.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the simulator was designed, and how the tasks were selected?
2. How do you plan to extend the approach to more complex dialogue scenarios, such as multi-turn dialogues or dialogues with multiple participants?
3. Can you provide more analysis on the differences between the simulator and real-world experiments, and how the results can be generalized to other domains?