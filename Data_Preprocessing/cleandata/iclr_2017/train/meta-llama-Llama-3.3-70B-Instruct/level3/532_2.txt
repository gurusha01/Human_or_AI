Summary of the Paper's Claims and Contributions
The paper proposes a novel method for pruning the weights of Recurrent Neural Networks (RNNs) during training, resulting in sparse models that are more accurate than dense models while significantly reducing model size. The authors demonstrate the effectiveness of their approach on several RNN architectures, including vanilla RNNs and Gated Recurrent Units (GRUs), and show that their sparse models achieve state-of-the-art performance on a speech recognition task while reducing the model size by up to 90%. The paper also highlights the potential benefits of sparse models for deployment on mobile devices and server farms, including reduced memory footprint and increased computational efficiency.
Decision and Key Reasons
Based on the provided guidelines, I decide to Reject this paper. The two key reasons for this decision are:
1. Lack of Rigorous Experiments: The paper lacks rigorous experiments to prove the effectiveness of the proposed method. The authors only test their approach on one dataset and fail to compare its performance with other baselines, such as Adam, making the comparison with plain SGD unfair.
2. Methodological Drawbacks: The paper has several methodological drawbacks, including the use of validation accuracy as a feature vector, which invalidates the experiments. Additionally, the authors only prune the weights of the recurrent and linear layers, but not the biases or batch norm parameters, which may not be the optimal choice.
Supporting Arguments
The paper's claims are not supported by sufficient evidence, and the experiments are not comprehensive enough to demonstrate the effectiveness of the proposed method. The authors' decision to prune only the weights of the recurrent and linear layers, but not the biases or batch norm parameters, may not be the optimal choice, and the use of validation accuracy as a feature vector is problematic. Furthermore, the paper only tests the proposed method on one dataset, which is not sufficient to demonstrate its consistency and effectiveness.
Additional Feedback and Suggestions
To improve the paper, I suggest that the authors:
* Conduct more comprehensive experiments on larger datasets, such as Imagenet, to demonstrate the consistency and effectiveness of the proposed method.
* Compare the performance of their approach with other baselines, such as Adam, and other state-of-the-art methods for pruning RNNs.
* Investigate the optimal choice of layers to prune and the optimal pruning schedule.
* Provide more detailed analysis of the results, including the effect of pruning on the model's performance and the computational efficiency of the sparse models.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more details on the experimental setup and the hyperparameters used for the pruning algorithm?
* How did you choose the layers to prune and the pruning schedule, and what is the rationale behind these choices?
* Can you provide more comprehensive results on the performance of the sparse models, including the effect of pruning on the model's accuracy and computational efficiency?
* How do you plan to address the methodological drawbacks mentioned in the review, and what additional experiments or analysis do you propose to conduct to support the claims made in the paper?