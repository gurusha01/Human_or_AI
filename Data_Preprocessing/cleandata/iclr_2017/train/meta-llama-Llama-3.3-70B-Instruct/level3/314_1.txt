Summary
The paper proposes a new Deep RL algorithm that performs well in novel 3D environments and allows for better generalization across goals and environments. The algorithm uses additional low-dimensional observations as a supervised target for prediction, conditioned on a goal vector and the current action, to choose the optimal action. The key contributions of this work include the focus on Monte Carlo estimation, use of low-dimensional measurements, parametrized goals, and empirical comparison to prior work. The algorithm learns generalizable policies that can respond to limited changes in the goal without further training, and the paper presents well-communicated and compelling empirical results.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated problem in Deep RL, and (2) the approach is well-supported by empirical results and comparisons to prior work.
Supporting Arguments
The paper clearly motivates the problem of learning generalizable policies in Deep RL and provides a well-structured approach to addressing this challenge. The use of additional low-dimensional observations as a supervised target for prediction is a novel and interesting contribution. The empirical results demonstrate the effectiveness of the proposed algorithm in novel 3D environments and across different goals. The paper also provides a thorough comparison to prior work, which helps to establish the significance of the contributions.
Additional Feedback
To further improve the paper, I suggest that the authors discuss the potential limitations of using additional metadata, such as the requirement for specific measurements in the sensory environment. Additionally, it would be helpful to provide more details on the implementation of the algorithm and the experimental setup. Some questions that I would like the authors to answer include: (1) How do the authors plan to address the potential issue of overfitting to the specific environment or goal? (2) Can the authors provide more insights into the choice of low-dimensional observations and how they are selected? (3) How do the authors plan to extend the algorithm to more complex environments or tasks? 
Questions for Authors
1. How do you plan to address the potential issue of overfitting to the specific environment or goal?
2. Can you provide more insights into the choice of low-dimensional observations and how they are selected?
3. How do you plan to extend the algorithm to more complex environments or tasks?