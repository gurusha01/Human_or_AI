This paper proposes the use of Annealed Importance Sampling (AIS) to estimate log-likelihoods for decoder-based generative models, and validates its accuracy using Bidirectional Monte Carlo (BDMC). The authors analyze the performance of various decoder-based models, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Generative Moment Matching Networks (GMMNs), and find that VAEs achieve substantially higher log-likelihoods than GANs and GMMNs.
The paper tackles the specific question of how to evaluate the performance of decoder-based generative models, which is a crucial problem in the field of generative modeling. The approach is well-motivated, as the authors highlight the limitations of existing evaluation methods, such as Kernel Density Estimation (KDE) and importance weighting, and demonstrate the effectiveness of AIS in estimating log-likelihoods.
However, I decide to reject this paper for two main reasons. Firstly, the paper lacks control experiments and mathematical analysis to support its claims. For example, the authors do not provide a thorough analysis of the factors that contribute to the difference in log-likelihoods between VAEs and GANs/GMMNs. Secondly, the paper has formatting issues, including unreadable plot legends and labels, inconsistent x-axis ranges, and improperly rendered images in printed format, which makes it difficult to follow and understand the results.
To improve the paper, I suggest that the authors provide more detailed mathematical analysis and control experiments to support their claims. Additionally, they should address the formatting issues and provide clearer and more concise visualizations of their results. Some specific questions that I would like the authors to answer include: (1) Can you provide a more detailed analysis of the factors that contribute to the difference in log-likelihoods between VAEs and GANs/GMMNs? (2) How do the results change when using different observation models, such as a non-Gaussian noise model? (3) Can you provide more visualizations of the posterior samples for each model, to help illustrate the differences in their performance?