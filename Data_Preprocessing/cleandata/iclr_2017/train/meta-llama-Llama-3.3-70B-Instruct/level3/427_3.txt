The paper proposes a novel approach to learning algorithmic tasks by leveraging the principle of divide and conquer, which is a powerful inductive bias that can break the curse of dimensionality. The authors introduce a recursive split and merge architecture, where the split module partitions the input into two disjoint sets, and the merge module combines the outputs of the split module to produce the final output. The approach is well-motivated, and the manuscript is well-written, making it easy to follow.
I decide to reject this paper, primarily due to two key reasons. Firstly, the approach has a significant limitation in assuming translation invariance and neglecting the impact of global context on pixel semantics. This limitation may hinder the model's ability to generalize to more complex tasks. Secondly, the algorithm's computational expense is a concern, as it takes 30 minutes to analyze a single image, which raises questions about its practicality for large datasets.
To support my decision, I provide the following arguments. The paper's contribution is notable, as it introduces a new approach to learning algorithmic tasks. However, the assumption of translation invariance may not hold in many real-world scenarios, which could limit the model's applicability. Furthermore, the computational expense of the algorithm is a significant concern, as it may not be feasible to apply this approach to large datasets.
To improve the paper, I suggest that the authors consider the following feedback. Firstly, they should investigate ways to relax the assumption of translation invariance and incorporate global context into the model. This could involve using more advanced architectures, such as attention mechanisms or graph neural networks. Secondly, the authors should explore ways to reduce the computational expense of the algorithm, such as using more efficient optimization methods or parallelizing the computation.
I would like the authors to answer the following questions to clarify my understanding of the paper. How do the authors plan to address the limitation of assuming translation invariance, and what alternative architectures or techniques could be used to incorporate global context? What steps can be taken to reduce the computational expense of the algorithm, and are there any potential applications where the current computational expense is acceptable?