Summary
The paper proposes a novel neural network architecture called FractalNet, which is based on a self-similar design principle. The authors claim that FractalNet can match the performance of standard residual networks on image classification tasks without relying on residual connections. The paper also introduces a regularization technique called drop-path, which prevents co-adaptation of parallel paths in the network.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, I am unconvinced by the paper's results, as the new architecture does not significantly outperform existing methods, and the claims made by the authors are not sufficiently supported by the experiments. Secondly, the idea of modifying existing architectures to create new ones is not novel, and the paper does not provide enough evidence to demonstrate the superiority of FractalNet over other approaches.
Supporting Arguments
The paper's results on image classification tasks are not impressive, and the authors' claims about the effectiveness of FractalNet are not supported by the experiments. The paper also lacks clarity on what problem FractalNet is trying to solve, and how it addresses the limitations of existing architectures. Furthermore, the idea of using a self-similar design principle to create neural networks is not new, and the paper does not provide enough evidence to demonstrate the novelty and significance of FractalNet.
Additional Feedback
To improve the paper, the authors should provide more convincing results and experiments to demonstrate the effectiveness of FractalNet. They should also clarify the problem that FractalNet is trying to solve and provide more evidence to support their claims. Additionally, the authors should discuss the limitations of FractalNet and provide more insights into its internal behavior and training dynamics.
Questions for the Authors
I would like to ask the authors to provide more information on the following:
* How does FractalNet address the limitations of existing architectures, and what are the key advantages of using a self-similar design principle?
* Can the authors provide more convincing results and experiments to demonstrate the effectiveness of FractalNet on image classification tasks?
* How does the drop-path regularization technique work, and what are the benefits of using it in FractalNet?
* Can the authors provide more insights into the internal behavior and training dynamics of FractalNet, and how it relates to other neural network architectures?