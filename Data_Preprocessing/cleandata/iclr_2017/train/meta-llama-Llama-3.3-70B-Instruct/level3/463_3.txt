This paper proposes a novel architecture, Layer-RNN (L-RNN), which combines traditional convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to learn multi-scale contextual information. The authors demonstrate the effectiveness of L-RNNs in image classification and semantic segmentation tasks.
The specific question tackled by the paper is how to effectively incorporate contextual information into deep neural networks. The approach is well-motivated, building upon existing work on CNNs and RNNs. The authors provide a clear and detailed explanation of the L-RNN architecture and its components.
However, I decide to reject this paper for two key reasons. Firstly, the evaluation of the approach is limited to the CIFAR-10 and PASCAL VOC 2012 datasets, which may not be representative of more complex and realistic scenarios. Secondly, the simplicity of the proposed architecture, with only two hidden layers, may not be sufficient to capture the complexities of real-world data.
To support my decision, I provide the following arguments. The authors' decision to use consecutive softmaxes is questionable, and alternative approaches such as compound objectives or parallel losses may be more effective. Additionally, the lack of experimentation on more complex datasets and deeper architectures limits the generalizability of the results.
To improve the paper, I suggest that the authors consider the following feedback. Firstly, they should evaluate their approach on more complex and realistic datasets, such as ImageNet or COCO. Secondly, they should experiment with deeper architectures and more advanced RNN variants, such as LSTM or GRU. Finally, they should provide more detailed analysis and visualization of the learned features and contextual information.
I would like the authors to answer the following questions to clarify my understanding of the paper. How do the L-RNNs handle long-range dependencies and contextual information in images? Can the authors provide more detailed comparisons with other state-of-the-art architectures, such as ResNets or DenseNets? How do the authors plan to extend their approach to more complex and realistic scenarios, such as image generation or video analysis?