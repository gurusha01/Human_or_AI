This paper presents a novel LSTM-based meta-learning framework that learns the optimization algorithm for a neural network, specifically designed for few-shot learning tasks. The approach draws a parallel between the Robbins-Monroe update rule and the LSTM update rule, allowing the meta-learner to learn an update rule for training a neural network. The paper is well-written, with clear presentation of the main material and intriguing ideas, including the use of parameter sharing and normalization.
The experiments demonstrate the effectiveness of the proposed framework, showing competitive results with state-of-the-art deep metric-learning techniques for few-shot learning. However, I have some concerns about the redundancy of using loss, gradient, and parameters as input to the meta-learner. It is not entirely clear why all three are necessary, and whether the meta-learner could learn an effective update rule with fewer inputs.
Based on the conference guidelines, I will answer the three key questions:
1. The specific question/problem tackled by the paper is how to learn an optimization algorithm for few-shot learning tasks, where a classifier must quickly generalize after seeing very few examples from each class.
2. The approach is well-motivated, drawing on previous work in meta-learning and optimization. However, the related work section could be more comprehensive, including older works on meta-learning by Samy Bengio and Schmidhuber.
3. The paper supports its claims with convincing experiments, demonstrating the effectiveness of the proposed framework. However, I would like to see more analysis on the learned update rules and the behavior of the meta-learner.
My decision is to accept the paper, with the following reasons:
* The paper presents a novel and well-motivated approach to few-shot learning, with clear potential for impact in the field.
* The experiments demonstrate the effectiveness of the proposed framework, showing competitive results with state-of-the-art techniques.
However, I would like to see some revisions to address the concerns mentioned above, including:
* More analysis on the learned update rules and the behavior of the meta-learner.
* A more comprehensive related work section, including older works on meta-learning.
* Consideration of alternative inputs to the meta-learner, to reduce redundancy and improve efficiency.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more insight into why all three inputs (loss, gradient, and parameters) are necessary for the meta-learner, and whether the meta-learner could learn an effective update rule with fewer inputs?
* How do the learned update rules compare to traditional optimization algorithms, such as gradient descent or Adam?
* Can you provide more details on the hyperparameter selection process for the meta-learner, and how the hyperparameters were chosen?