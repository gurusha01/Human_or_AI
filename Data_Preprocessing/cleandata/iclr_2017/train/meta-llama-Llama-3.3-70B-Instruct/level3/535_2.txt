Summary of the Paper
The authors propose a novel approach to distributed stochastic optimization, called synchronous stochastic optimization with backup workers, which mitigates the straggler effect while avoiding gradient staleness. The paper demonstrates the effectiveness of this approach through empirical evaluations on various models, including Inception and PixelCNN, and shows that it outperforms asynchronous stochastic optimization in terms of test accuracy and convergence speed.
Decision
I decide to accept this paper, with the main reason being that the approach is well-motivated and the experiments demonstrate its effectiveness. The paper provides a clear and thorough analysis of the trade-offs between synchronous and asynchronous stochastic optimization and presents a viable solution to mitigate the weaknesses of both approaches.
Supporting Arguments
The paper provides a thorough analysis of the impact of staleness on test accuracy and demonstrates the effectiveness of the proposed approach through empirical evaluations on various models. The authors also provide a detailed examination of the trade-offs between dropping more stragglers to reduce iteration time and waiting for more gradients to improve gradient quality. The results show that the proposed approach outperforms asynchronous stochastic optimization in terms of test accuracy and convergence speed.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the choice of hyperparameters, such as the number of backup workers and the learning rate schedule. Additionally, it would be interesting to see more experiments on different models and datasets to further demonstrate the effectiveness of the proposed approach. The authors may also want to consider providing more details on the implementation of the proposed approach, such as how the backup workers are implemented and how the gradients are aggregated.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How did the authors choose the number of backup workers and the learning rate schedule for the experiments?
* Can the authors provide more insights into the trade-offs between dropping more stragglers and waiting for more gradients?
* How does the proposed approach perform on other models and datasets, such as language models and reinforcement learning tasks?
* Can the authors provide more details on the implementation of the proposed approach, such as how the backup workers are implemented and how the gradients are aggregated?