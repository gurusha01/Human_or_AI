Summary
The paper proposes a novel method for finding dependent subspaces across multiple views, preserving neighborhood relationships of data. The method directly maximizes the between-view similarity of neighborhoods of data samples, a natural measure for similarity of data relationships among the views. The authors demonstrate the effectiveness of their method on various datasets, including artificial and real-world data, and show that it outperforms existing methods such as CCA and LPCCA.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's analysis is limited to simple models and does not convincingly extend to deep non-linear computation structures, requiring further analysis and experimentation. Secondly, the paper's impact on low-shot learning in high-dimensional spaces is not convincingly shown, despite its potential for numerical stability.
Supporting Arguments
The paper's method is well-motivated and has strong invariance properties, detecting nonlinear dependencies and relating to an information retrieval task of the analyst. However, the authors' analysis is limited to simple models, and it is unclear how their method would perform in more complex scenarios. Additionally, the paper's experiments do not convincingly demonstrate the method's impact on low-shot learning in high-dimensional spaces, which is a key application area for this type of research.
Additional Feedback
To improve the paper, the authors should consider extending their analysis to more complex models and scenarios, and providing more convincing experiments to demonstrate the method's impact on low-shot learning in high-dimensional spaces. Additionally, the authors should improve the formatting, citation, and analysis of the meaningfulness of the representations learned by the model. The authors should also consider providing more details on the optimization technique used and the time complexity of the method.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. How do the authors plan to extend their method to deep non-linear computation structures?
2. Can the authors provide more convincing experiments to demonstrate the method's impact on low-shot learning in high-dimensional spaces?
3. How do the authors ensure that the learned representations are meaningful and informative, rather than simply optimizing a mathematical objective?