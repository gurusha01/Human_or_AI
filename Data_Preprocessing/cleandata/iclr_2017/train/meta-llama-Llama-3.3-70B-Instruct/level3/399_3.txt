This paper proposes a novel approach to policy search in stochastic dynamical systems using model-based reinforcement learning with Bayesian neural networks (BNNs) that include stochastic input variables. The authors demonstrate the effectiveness of their approach in solving a challenging benchmark problem, the Wet-Chicken problem, and in achieving promising results on real-world applications, including the control of a gas turbine and an industrial benchmark.
The specific question tackled by the paper is how to learn a policy in a stochastic dynamical system using a model-based approach, where the system dynamics are described by a BNN with stochastic inputs. The approach is well-motivated, as it addresses the limitations of traditional model-based reinforcement learning methods, which often assume deterministic transition functions and fail to capture complex stochastic patterns.
The paper supports its claims with extensive experiments on various benchmark problems, demonstrating the superiority of the proposed approach over other methods, including Gaussian processes and variational Bayes. The results show that the proposed approach can learn effective policies in complex stochastic systems, outperforming other methods in terms of average reward and test log-likelihood.
I decide to accept this paper, with the main reason being the novelty and effectiveness of the proposed approach in solving complex stochastic dynamical systems. The paper provides a clear and well-motivated presentation of the approach, and the experimental results demonstrate its superiority over other methods.
To improve the paper, I suggest that the authors provide more details on the computational complexity of the proposed approach, including the training time and the number of parameters required. Additionally, the authors could provide more insights into the choice of hyperparameters, such as the value of α and the number of hidden units in the BNNs.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors choose the value of α, and what is the effect of different values of α on the performance of the proposed approach?
* How do the authors handle the case where the system dynamics are not well-modeled by a BNN with stochastic inputs, and what are the limitations of the proposed approach in such cases?
* Can the authors provide more details on the implementation of the proposed approach, including the specific architectures used for the BNNs and the policy networks, and the optimization algorithms used for training?