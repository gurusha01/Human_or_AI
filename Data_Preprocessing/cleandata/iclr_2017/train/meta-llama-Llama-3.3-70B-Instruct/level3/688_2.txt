This paper proposes a modification to the PoWER algorithm with variational bounds on the value function, which is a strong and well-written contribution to reinforcement learning with limited policy updates. The authors provide numerical results on cartpole and online advertising problems, demonstrating the effectiveness of their approach. However, the lack of baselines for comparison in the experimental section is a notable weakness.
The use of control variates as constant scalars is unclear and seems to be treated as hyperparameters, raising questions about why they are not learned or estimated. Furthermore, the section on constrained optimization, although interesting, feels disconnected from the rest of the paper and lacks citation to relevant literature on constrained MDPs.
To answer the three key questions for evaluation: 
1. The specific question/problem tackled by the paper is the modification of the PoWER algorithm with variational bounds on the value function for reinforcement learning with limited policy updates.
2. The approach is well-motivated, including being well-placed in the literature, as it addresses a significant challenge in reinforcement learning.
3. The paper supports its claims with numerical results, but the lack of baselines and unclear use of control variates raise some concerns about the scientific rigor.
Based on these points, I would reject the paper, primarily due to the lack of baselines for comparison and unclear use of control variates. However, with additional experiments and clarification on the control variates, this paper has the potential to be a strong contribution to the field.
To improve the paper, I would suggest adding baselines for comparison in the experimental section and providing more details on the use of control variates. Additionally, the authors could consider citing relevant literature on constrained MDPs to strengthen the section on constrained optimization. 
Some questions I would like the authors to answer to clarify my understanding of the paper include: 
- Can you provide more details on why control variates are used as constant scalars and not learned or estimated?
- How do you plan to address the lack of baselines for comparison in the experimental section?
- Can you provide more context on how the section on constrained optimization relates to the rest of the paper and cite relevant literature on constrained MDPs?