The paper explores the concept of semi-supervised reinforcement learning (SSRL), where an agent learns to perform a task in a setting with limited reward supervision. The authors propose a method called semi-supervised skill generalization (S3G), which leverages both labeled and unlabeled experiences to improve the generalization of a learned policy. S3G uses an inverse reinforcement learning (IRL) algorithm to infer the reward function in the unlabeled settings, based on the agent's prior experience in the labeled settings.
The paper tackles the specific question of how to enable an agent to continue learning in the real world without access to a reward function. The approach is well-motivated, as it addresses a significant challenge in real-world reinforcement learning applications, where reward functions are often difficult to measure. The paper is also well-placed in the literature, as it builds upon existing work in semi-supervised learning, inverse reinforcement learning, and reinforcement learning.
However, I decide to reject this paper, with two key reasons for this choice. Firstly, the analogy between supervised learning and reinforcement learning is somewhat incoherent due to differences between labels and reward functions. The authors introduce the terms "labeled MDP" and "unlabeled MDP", but the latter refers to a situation where the reward is unknown, which is technically not an MDP. This lack of clarity may limit the paper's impact on the RL community.
Secondly, while the paper presents an interesting approach to SSRL, it fails to provide a simple and general notion of semi-supervised RL that will be of wide interest to the RL community. The method is specific to the authors' formulation of SSRL and may not be easily applicable to other settings. Additionally, the experimental evaluation, although thorough, is limited to a few tasks and may not demonstrate the full potential of the approach.
To improve the paper, I suggest that the authors provide a clearer and more concise definition of SSRL and its relationship to existing reinforcement learning frameworks. They should also consider providing more theoretical analysis of the S3G algorithm and its convergence properties. Furthermore, the authors could explore more tasks and domains to demonstrate the applicability and effectiveness of their approach.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend their approach to more complex and high-dimensional tasks? Can they provide more insight into the choice of the reward function architecture and its impact on the performance of S3G? How do the authors envision the S3G algorithm being used in real-world applications, such as robotics or autonomous driving?