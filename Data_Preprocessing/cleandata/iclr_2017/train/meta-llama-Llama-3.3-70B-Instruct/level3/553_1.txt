Summary
The paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses to learn the embeddings. The approach is evaluated on three tasks: acoustic word discrimination, cross-view word discrimination, and word similarity. The results show that the proposed approach outperforms previous methods on acoustic word discrimination and achieves promising results on cross-view word discrimination and word similarity.
Decision
I decide to reject this paper. The main reason for this decision is that the paper is not well-suited for the ICLR conference, as it focuses on linear training algorithms and does not address the broader interests of the ICLR audience. Additionally, the choice of MNIST for experiments is questionable, and the authors do not report the results.
Supporting Arguments
The paper's focus on linear training algorithms makes it more suitable for a conference like ICML, which targets a large-scale linear ML audience. Furthermore, the paper would benefit from a proper benchmark on a large-scale linear task to demonstrate its effectiveness. The use of MNIST as an experimental dataset is also problematic, as it is a small-scale dataset that may not accurately reflect the challenges of real-world applications.
Additional Feedback
To improve the paper, the authors could consider the following suggestions:
* Evaluate the approach on a larger-scale dataset to demonstrate its scalability and effectiveness.
* Compare the proposed approach with other state-of-the-art methods on acoustic word embeddings.
* Provide more detailed analysis of the results, including visualizations and discussions of the strengths and weaknesses of the approach.
* Consider using a more diverse set of experimental datasets to demonstrate the robustness of the approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on why you chose to use MNIST as an experimental dataset, and how you think it relates to the task of acoustic word embeddings?
* How do you plan to address the scalability of the approach to larger-scale datasets and more complex tasks?
* Can you provide more insights into the choice of contrastive losses and how they contribute to the effectiveness of the approach?