This paper proposes a dynamic neural Turing machine (D-NTM) that extends the traditional neural Turing machine (NTM) by introducing a learnable addressing scheme. The D-NTM allows for more flexible and nonlinear location-based addressing, which is shown to be beneficial in various tasks, including episodic question-answering and sequential MNIST.
The paper claims to contribute to the field of neural networks by introducing a new type of memory-based model that can learn to read and write dynamically. The authors demonstrate the effectiveness of the D-NTM on several tasks, including Facebook bAbI tasks, sequential MNIST, and toy tasks such as copy and associative recall.
I decide to accept this paper with two key reasons: (1) the introduction of a learnable addressing scheme is a novel and interesting contribution to the field of neural networks, and (2) the experimental results demonstrate the effectiveness of the D-NTM on various tasks.
The paper provides a clear and well-motivated approach to addressing the limitations of traditional NTMs. The authors provide a thorough analysis of the D-NTM's performance on various tasks and compare it to other state-of-the-art models. The results show that the D-NTM outperforms other models on several tasks, including episodic question-answering and sequential MNIST.
However, there are some areas that need improvement. The paper could benefit from a clearer explanation of the connection between the D-NTM and the scattering transform. Additionally, the writing style could be improved in certain areas to enhance the overall quality of the paper.
To improve the paper, I suggest that the authors provide more details on the implementation of the D-NTM, including the architecture of the controller and the memory. Additionally, the authors could provide more analysis on the performance of the D-NTM on different tasks and compare it to other models.
I would like to ask the authors to clarify the following points: (1) How does the learnable addressing scheme in the D-NTM differ from other addressing schemes in the literature? (2) Can the authors provide more details on the experimental setup and hyperparameter tuning for the bAbI tasks? (3) How does the D-NTM perform on tasks that require longer-term memory, such as text summarization or visual question-answering?