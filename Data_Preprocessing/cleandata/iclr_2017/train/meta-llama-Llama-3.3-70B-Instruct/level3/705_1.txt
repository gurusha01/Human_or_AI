Summary
The paper proposes a model, Object Persistence Net (OPnet), which fine-tunes a deep convolutional neural network (DCNN) with object persistence constraints to learn a representation that captures the notion of object persistence and continuity in visual experience. The authors demonstrate that this approach improves the network's ability to recognize similarity between novel objects, including those from untrained categories and artificial synthetic objects. The paper also shows that the learned representation matches human perceptual similarity judgment better than the original AlexNet.
Decision
I decide to reject this paper, with the main reason being the lack of novelty in the work. The paper builds upon a previous NIPS 2016 paper and performs acoustic segmentation and clustering to learn a lexicon of word-like units using learned embeddings, but the analysis is not satisfying.
Supporting Arguments
While the paper is well-written and explores an important research direction, it may not be novel or deep enough to stand out at the ICLR conference. The approach is well-motivated, and the authors provide a clear explanation of the object persistence constraint and its implementation in the Siamese triplet architecture. However, the results, although promising, do not provide a significant breakthrough in the field. The comparison with other approaches, such as the joint embedding model, is limited, and the authors do not provide a thorough analysis of the strengths and weaknesses of their approach.
Additional Feedback
To improve the paper, I suggest that the authors provide a more detailed analysis of the learned representation and its properties. For example, they could investigate how the object persistence constraint affects the feature representations at different layers of the network. Additionally, the authors could explore the application of their approach to other tasks, such as object recognition or image retrieval, to demonstrate its versatility. It would also be helpful to include more visualizations of the learned representation, such as t-SNE plots or feature visualizations, to provide a better understanding of the results.
Questions
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. How does the object persistence constraint affect the feature representations at different layers of the network?
2. Can the authors provide more visualizations of the learned representation to demonstrate its properties?
3. How does the approach compare to other methods that learn representations with similar properties, such as slow-feature analysis or memory trace models?
4. Can the authors provide more details on the implementation of the Siamese triplet architecture and the object persistence constraint?
5. How do the authors plan to extend their approach to more complex scenarios, such as real-world images with varying lighting conditions and backgrounds?