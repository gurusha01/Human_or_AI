The paper proposes a novel pruning strategy for convolutional neural networks (CNNs) based on the Taylor expansion of the neural network function. This approach removes feature maps with small activation and gradient, which are deemed less important for the network's performance. The authors demonstrate the effectiveness of their strategy through various experiments on transfer learning tasks, including fine-grained image classification and hand gesture recognition.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and relevant problem in the field of deep learning, namely reducing the computational cost of CNNs while maintaining their accuracy.
2. The approach is well-motivated and grounded in theoretical foundations, leveraging the Taylor expansion to approximate the change in the cost function induced by pruning network parameters.
The paper supports its claims through extensive experiments on various datasets, including Birds-200, Flowers-102, and ImageNet. The results demonstrate the superiority of the proposed Taylor criterion over other pruning criteria, such as the norm of kernel weights or feature map activation. Additionally, the authors provide a detailed analysis of the oracle ranking, which serves as a benchmark for evaluating the effectiveness of different pruning criteria.
To further improve the paper, I suggest the following:
* Provide more comprehensive results on fine-tuning, including testing both networks on both datasets.
* Compare the accuracy between pruned and original networks with the same number of training iterations to ensure a fair evaluation.
* Consider additional comparisons with other pruning methods, such as the optimal damage framework, to analyze the differences in weights removed.
* Evaluate the trade-offs between pruning and precision, such as lower precision computation, to assess the effectiveness of the proposed approach in reducing GFLOPs.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How do the authors plan to extend their approach to other types of neural networks, such as recurrent neural networks or fully connected networks?
* Can the authors provide more insights into the computational cost of their pruning strategy, particularly in terms of the number of floating-point operations required?
* How do the authors envision their approach being used in practice, particularly in resource-constrained environments such as embedded devices or mobile devices?