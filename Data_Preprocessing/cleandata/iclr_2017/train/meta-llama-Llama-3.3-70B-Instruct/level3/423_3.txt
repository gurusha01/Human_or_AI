This paper proposes a novel approach to exploration in reinforcement learning by extending classic count-based methods to high-dimensional state spaces through hashing. The authors demonstrate that this approach can achieve near state-of-the-art performance on various deep RL benchmarks, including continuous control tasks and Atari 2600 games.
The specific question tackled by the paper is how to balance exploration and exploitation in high-dimensional state spaces, where traditional count-based methods are not applicable. The authors address this question by proposing a simple yet effective method that uses hashing to discretize the state space and assign a bonus reward to encourage exploration.
The approach is well-motivated, building on the idea of count-based exploration, which has been shown to be effective in tabular reinforcement learning settings. The authors provide a clear and concise overview of the related work, highlighting the limitations of existing methods and the need for a more general and robust approach to exploration.
The paper supports its claims through extensive experiments on various benchmarks, demonstrating the effectiveness of the proposed method in achieving near state-of-the-art performance. The results are scientifically rigorous, with careful consideration of hyperparameter settings, robustness analysis, and comparison to existing methods.
Based on the evaluation, I decide to Accept this paper. The key reasons for this choice are:
1. The paper proposes a novel and effective approach to exploration in high-dimensional state spaces, which addresses a significant challenge in reinforcement learning.
2. The approach is well-motivated, building on existing work and providing a clear and concise overview of the related literature.
To improve the paper, I suggest the following additional feedback:
* Provide more insight into the choice of hash function and its impact on the performance of the algorithm.
* Consider providing more detailed analysis of the robustness of the algorithm to hyperparameter changes and its sensitivity to different environments.
* It would be helpful to include more visualizations or examples to illustrate the effectiveness of the proposed method in different scenarios.
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more details on the implementation of the hashing function and how it is used to assign a bonus reward?
* How do you choose the hyperparameters for the algorithm, and what is the sensitivity of the performance to these hyperparameters?
* Can you provide more insight into the comparison with existing methods, such as pseudo-counts and other exploration strategies?