Summary
The paper proposes Tartan (TRT), a hardware accelerator for inference with Deep Neural Networks (DNNs) that exploits variable per-layer precision requirements to deliver execution time proportional to the precision used. TRT uses hybrid bit-serial/bit-parallel functional units and achieves significant performance and energy efficiency improvements over state-of-the-art accelerators.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a specific and worthwhile problem in the field of DNN acceleration, and (2) the approach is well-motivated and demonstrates significant performance and energy efficiency improvements.
Supporting Arguments
The paper presents a clear and detailed description of the TRT architecture and its components, including the Serial Inner-Product Units (SIPs) and the Dispatcher and Reducer units. The evaluation methodology is sound, and the results demonstrate an average speedup of 1.90× and energy efficiency improvement of 1.17× over a state-of-the-art accelerator. The paper also discusses related work and limitations, providing a comprehensive overview of the field.
Additional Feedback
To further improve the paper, I suggest providing more details on the learning and inference processes, as well as more thorough experiments to demonstrate the robustness of the approach. Additionally, the paper could benefit from a clearer presentation of the results, with more tables and figures to illustrate the performance and energy efficiency improvements.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the precision requirements analysis methodology used to determine the per-layer precision profiles?
2. How do you plan to address the limitations of the current design, such as the assumption that each layer fits on-chip, and the potential need for multiple TRT nodes?
3. Have you considered applying the TRT design to other types of neural networks, such as recurrent neural networks or long short-term memory networks?