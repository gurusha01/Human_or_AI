This paper introduces a novel approach to automatic dialogue evaluation, which is a crucial component in the development of conversational AI systems. The proposed model, ADEM, learns to predict human-like scores for input responses using a hierarchical recurrent neural network (RNN) encoder and a dot-product based scoring function. The authors demonstrate that ADEM correlates significantly with human judgements at both the utterance and system level, outperforming traditional word-overlap metrics such as BLEU.
The paper is well-written and easy to follow, with a clear motivation and a thorough explanation of the technical details. The experimental results are impressive, showing that ADEM can generalize to evaluating new models and responses that were unseen during training. The authors also provide a detailed analysis of the results, including a failure analysis and a discussion of the limitations of the model.
However, I have some concerns regarding the impact and scalability of the proposed approach. While the results are promising, it is unclear how well ADEM will perform in more complex and nuanced dialogue scenarios. Additionally, the model requires a significant amount of labeled data to train, which may be a limitation in certain applications.
To improve the paper, I would suggest providing more details on the experimental setup, including the hyperparameter tuning process and the evaluation metrics used. Additionally, it would be helpful to include more examples of the model's outputs and errors, to provide a better understanding of its strengths and weaknesses.
Overall, I believe that this paper makes a significant contribution to the field of conversational AI and dialogue evaluation. While there are some limitations and areas for improvement, the proposed approach shows promise and has the potential to be widely adopted in the field.
Decision: Accept
Reasons:
1. The paper introduces a novel and well-motivated approach to automatic dialogue evaluation, which is a crucial component in the development of conversational AI systems.
2. The experimental results are impressive, showing that ADEM correlates significantly with human judgements at both the utterance and system level, outperforming traditional word-overlap metrics such as BLEU.
Additional feedback:
* Provide more details on the experimental setup, including the hyperparameter tuning process and the evaluation metrics used.
* Include more examples of the model's outputs and errors, to provide a better understanding of its strengths and weaknesses.
* Discuss the limitations of the model and potential areas for improvement, such as handling more complex and nuanced dialogue scenarios.
* Consider providing more analysis on the data efficiency of the model, including the amount of labeled data required to train ADEM.
Questions for the authors:
* How do you plan to address the limitation of requiring a significant amount of labeled data to train ADEM?
* Can you provide more examples of the model's outputs and errors, to provide a better understanding of its strengths and weaknesses?
* How do you think ADEM can be improved to handle more complex and nuanced dialogue scenarios?
* Can you discuss the potential applications of ADEM in real-world conversational AI systems?