Summary of the Paper's Contributions
The paper proposes a novel approach to understanding the behavior of deep neural networks by introducing the concept of preimages, which are the sets of inputs that result in the same output activity at a certain level of the network. The authors demonstrate how to compute these preimages for fully connected multi-layer rectifier networks and show that they can be used to describe the network's ability to model input manifolds and achieve efficient classification. The paper also explores the implications of this concept for convolutional networks and discusses the potential for using preimages to enhance the efficiency of network training.
Decision
Based on the provided guidelines, I decide to Accept this paper. The main reasons for this decision are:
1. The paper tackles a specific and well-defined problem in the field of deep learning, namely understanding the behavior of neural networks in terms of preimages.
2. The approach is well-motivated and placed in the literature, drawing on existing work on neural networks and manifold learning.
Supporting Arguments
The paper provides a clear and well-structured introduction to the concept of preimages and their relevance to deep learning. The authors demonstrate a good understanding of the underlying mathematics and provide a rigorous derivation of the preimage computation procedure. The paper also includes a thorough discussion of the implications of this concept for convolutional networks and the potential for using preimages to improve network training.
Additional Feedback
To further improve the paper, I suggest that the authors provide more concrete examples and visualizations to illustrate the concept of preimages and their computation. Additionally, it would be helpful to include more experimental results to demonstrate the effectiveness of the proposed approach in practice. Finally, the authors may want to consider discussing the potential limitations and challenges of using preimages in deep learning, such as the computational cost of computing preimages and the potential for overfitting.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the preimage computation procedure can be specialized to convolutional layers?
2. How do you plan to address the issue of pooling in deep learning networks, which is not considered in the current paper?
3. Can you discuss the potential relationship between preimages and adversarial examples in deep learning, and how this might be explored in future work?