Summary
The paper proposes a novel approach to automatic dialogue evaluation, formulating it as a learning problem. The authors introduce ADEM, a model that learns to predict human-like scores for input responses using a new dataset of human response scores. ADEM correlates significantly with human judgements at both the utterance and system-level, outperforming traditional word-overlap metrics such as BLEU. The model also demonstrates the ability to generalize to evaluating new models unseen during training.
Decision
I decide to reject this paper, with the main reason being the lack of experiments on non-synthetic datasets, which makes it unclear if the work is useful for modeling general videos. Additionally, the paper is more suitable as a workshop contribution in its current form due to its limited experimental evaluation.
Supporting Arguments
The paper's approach to automatic dialogue evaluation is well-motivated, and the use of a hierarchical RNN encoder to learn distributed representations of the context, model response, and reference response is a sensible choice. However, the evaluation is limited to a single dataset, and the results may not generalize to other datasets or domains. Furthermore, the paper lacks a thorough analysis of the model's performance on different types of dialogue responses, such as those with varying levels of complexity or nuance.
Additional Feedback
To improve the paper, the authors should consider conducting experiments on multiple datasets, including non-synthetic ones, to demonstrate the model's ability to generalize. Additionally, the authors should provide a more detailed analysis of the model's performance on different types of dialogue responses, and explore ways to address the model's tendency to be too conservative when predicting response scores. The authors should also consider incorporating additional features or metrics into the model, such as those related to response length or semantic similarity, to improve its performance.
Questions for the Authors
1. How do the authors plan to address the issue of the model's tendency to be too conservative when predicting response scores?
2. Can the authors provide more details on the dataset used for training and evaluating ADEM, including the distribution of response lengths and the level of noise in the human evaluations?
3. How do the authors plan to extend ADEM to evaluate dialogue systems in other domains, such as task-oriented dialogue systems?
4. Can the authors provide a more detailed comparison of ADEM's performance with other evaluation metrics, such as those used in machine translation or summarization?
5. How do the authors plan to address the issue of human biases in the evaluation data, such as the tendency to give higher ratings to shorter responses?