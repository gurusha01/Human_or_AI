Summary
The paper introduces NewsQA, a large-scale machine comprehension dataset consisting of over 100,000 question-answer pairs based on news articles from CNN. The dataset is designed to encourage exploratory, curiosity-based questions that require reasoning, making it a challenging benchmark for machine comprehension models. The authors provide a thorough analysis of the dataset, including answer types and reasoning types, and evaluate the performance of human data analysts and two neural models on the dataset.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks empirical support for its claims about the effectiveness of the proposed dataset in teaching models to learn reasoning mechanisms. Secondly, the paper does not demonstrate the usefulness of the dataset in a realistic application or compare its performance to standard RNN approaches on algorithmic learning tasks.
Supporting Arguments
The paper presents a well-motivated approach to creating a challenging machine comprehension dataset, and the collection methodology is thorough and well-designed. However, the evaluation of the dataset's effectiveness is limited to a comparison with human performance and two neural models, which may not be sufficient to demonstrate its usefulness in a broader range of applications. Additionally, the paper's claims about the mental representations of humans and machines are not well-supported and may be misleading.
Additional Feedback
To improve the paper, I suggest that the authors provide more empirical evidence for the effectiveness of the dataset in teaching models to learn reasoning mechanisms. This could involve evaluating the performance of models trained on NewsQA on a range of downstream tasks that require reasoning, such as question answering, text summarization, or natural language inference. Additionally, the authors could compare the performance of models trained on NewsQA to models trained on other machine comprehension datasets, such as SQuAD or CNN/Daily Mail, to demonstrate its relative effectiveness.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to address the lack of empirical support for the effectiveness of the dataset in teaching models to learn reasoning mechanisms?
* Can the authors provide more details on the evaluation methodology used to compare the performance of human data analysts and neural models on the dataset?
* How do the authors plan to demonstrate the usefulness of the dataset in a realistic application or compare its performance to standard RNN approaches on algorithmic learning tasks?