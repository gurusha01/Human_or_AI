This paper proposes a new learning framework called "Deep Variational Information Bottleneck" (VIB) that combines variational inference and the information bottleneck method to learn a stochastic representation of the input data. The authors claim that VIB can learn a representation that is maximally informative about the target variable while being maximally compressive about the input data.
The paper is well-motivated, and the idea of using variational inference to approximate the information bottleneck objective is interesting. However, the paper is not without its limitations. The experimental results are limited to a few datasets, and the comparison to other methods is not comprehensive. Additionally, the paper could benefit from a more detailed analysis of the learned representations and the trade-off between compression and informativeness.
Based on the provided guidelines, I would reject this paper for the following reasons:
1. The paper lacks a comprehensive comparison to other methods, particularly in the context of adversarial robustness.
2. The experimental results are limited to a few datasets, and the paper could benefit from more extensive experiments to demonstrate the effectiveness of VIB.
However, I would like to provide additional feedback to help improve the paper:
* The paper could benefit from a more detailed analysis of the learned representations and the trade-off between compression and informativeness.
* The authors could provide more insight into the choice of hyperparameters, particularly the value of β, and how it affects the performance of VIB.
* The paper could benefit from a more comprehensive comparison to other methods, particularly in the context of adversarial robustness.
* The authors could provide more details on the computational cost of VIB and how it compares to other methods.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors choose the value of β, and how does it affect the performance of VIB?
* Can the authors provide more insight into the learned representations and the trade-off between compression and informativeness?
* How does VIB compare to other methods in terms of computational cost and scalability?
* Can the authors provide more details on the experimental setup and the datasets used in the experiments?