This paper proposes a novel approach to compressing fully-connected neural networks by introducing a density-diversity penalty regularizer. The approach achieves impressive compression rates with minimal loss of accuracy on both MNIST and TIMIT datasets. The density-diversity penalty encourages low diversity and high sparsity in the weight matrices, making them highly compressible.
I decide to reject this paper, primarily due to two key reasons. Firstly, the evaluation of the algorithm is insufficient, with only compression rate shown as a quantitative metric. This makes it difficult to compare the effects of sparsity and diversity separately, and to understand the trade-offs between compression and accuracy. Secondly, the paper lacks clarity on the application of the density-diversity penalty, with inconsistent statements in sections 3.1 and 3.4, which requires further verification.
The approach proposed in the paper is well-motivated, and the use of a density-diversity penalty regularizer is a novel and interesting idea. The paper also provides a thorough review of previous related work, and the experimental results are promising. However, the lack of clarity and insufficient evaluation of the algorithm are significant concerns that need to be addressed.
To improve the paper, I suggest that the authors provide a more detailed evaluation of the algorithm, including metrics such as accuracy, F1-score, and mean average precision. Additionally, the authors should clarify the application of the density-diversity penalty and provide more consistent statements throughout the paper. It would also be helpful to provide more insights into the relative importance of the different components of the approach, such as sparse initialization, weight tying, and probabilistic application of density-diversity penalty.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the density-diversity penalty to convolutional layers, and what are the potential challenges and benefits of doing so? (2) Can the authors provide more insights into the trade-offs between compression and accuracy, and how the density-diversity penalty affects the performance of the model? (3) How do the authors plan to apply the density-diversity penalty to recurrent models, and what are the potential applications and challenges of doing so?