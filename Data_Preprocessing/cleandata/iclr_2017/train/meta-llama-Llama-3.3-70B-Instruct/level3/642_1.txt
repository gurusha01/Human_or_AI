This paper investigates the effect of customized number representations on neural network inference, specifically focusing on the impact of quantization on the performance of deep neural networks. The authors propose a novel approach to network quantization, utilizing Hessian-weighted k-means clustering to minimize the performance loss due to quantization. They also establish a connection between the network quantization problem and the entropy-constrained scalar quantization (ECSQ) problem in information theory, leading to the development of two efficient heuristic solutions for ECSQ.
The paper claims to contribute to the field by providing a new perspective on network quantization, highlighting the importance of considering the impact of quantization errors on the neural network loss function. The authors demonstrate the effectiveness of their approach through experiments on various neural network architectures, including LeNet, ResNet, and AlexNet.
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper lacks clarity on the computation of running time and power consumption, which is a crucial aspect of evaluating the efficiency of the proposed approach. The authors need to elaborate on the simulation details and accuracy to provide a comprehensive understanding of their method. Secondly, the discussion on "efficient customized precision search" is deemed unimportant and could be replaced with an exhaustive search process that can be parallelized for better results.
To improve the paper, I suggest that the authors provide more detailed information on the simulation setup, including the specific hardware and software used, as well as the metrics employed to evaluate the performance of their approach. Additionally, they should consider discussing the potential applications of their method in real-world scenarios, such as deploying neural networks on resource-constrained devices. The authors may also want to explore the impact of number representations on other aspects of neural networks, such as batch normalization and recurrent neural networks, to further demonstrate the significance of their research.
Some questions I would like the authors to answer to clarify my understanding of the paper include: How do the authors plan to extend their approach to other types of neural networks, such as recurrent neural networks or transformers? Can they provide more insight into the trade-offs between the compression ratio and the performance loss due to quantization? How do the authors envision their method being used in practice, and what are the potential challenges and limitations of deploying their approach in real-world applications?