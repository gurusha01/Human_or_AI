Summary
The paper proposes a hardware accelerator, Tartan (TRT), for inference with Deep Neural Networks (DNNs) that exploits the variable precision requirements of DNNs to deliver execution time proportional to the precision used per layer. TRT uses hybrid bit-serial/bit-parallel functional units and achieves significant performance and energy efficiency improvements over state-of-the-art bit-parallel accelerators.
Decision
I decide to Accept this paper with two key reasons: (1) the paper presents a novel and well-motivated approach to improving the performance and energy efficiency of DNN inference, and (2) the experimental results demonstrate significant improvements over existing accelerators.
Supporting Arguments
The paper provides a clear and detailed explanation of the TRT architecture and its advantages over existing bit-parallel accelerators. The experimental results show that TRT outperforms a state-of-the-art bit-parallel accelerator by 1.90Ã— on average across all networks studied, with no loss in accuracy. Additionally, TRT enables trading off accuracy for additional improvements in execution performance and energy efficiency.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights on how TRT can be applied to other types of neural networks and machine learning algorithms. Additionally, it would be interesting to see a more detailed analysis of the area and energy overheads of TRT and how they can be optimized.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to extend TRT to support other types of neural networks, such as recurrent neural networks (RNNs) and long short-term memory (LSTM) networks?
2. Can the authors provide more details on the implementation of the bit-serial loading of weights into processing units and how it facilitates the processing of fully-connected layers?
3. How do the authors plan to optimize the area and energy overheads of TRT, and what are the potential trade-offs between performance, energy efficiency, and area?