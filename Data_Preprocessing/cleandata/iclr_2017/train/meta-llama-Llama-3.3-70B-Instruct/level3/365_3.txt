Summary
The paper proposes a novel "density-diversity penalty" regularizer that encourages low diversity and high sparsity in the weight matrices of fully-connected layers in deep neural networks. The authors demonstrate the effectiveness of this approach on two tasks, computer vision and speech recognition, achieving significant compression rates while maintaining comparable performance to the original models.
Decision
I decide to Accept this paper, with the primary reason being the promising results and potential of the proposed density-diversity penalty regularizer. However, I also note that the paper's readability and followability are compromised due to poor presentation, which makes it challenging to assess each concept's performance individually.
Supporting Arguments
The paper tackles a specific question of reducing the memory and computational cost of deep neural networks by compressing fully-connected layers. The approach is well-motivated, building upon previous work on regularization and compression techniques. The authors provide empirical results on two datasets, demonstrating the effectiveness of their approach in achieving high compression rates while maintaining performance. The proposed "sorting trick" for efficiently optimizing the density-diversity penalty is also a notable contribution.
Additional Feedback
To improve the paper, I suggest the authors reorganize and clarify their presentation, focusing on one concept at a time. Additionally, they could provide more detailed analysis and comparison of their approach to existing methods, such as "deep compression." It would also be helpful to include more visualizations and examples to illustrate the effectiveness of the density-diversity penalty.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the choice of the pairwise L1 loss for the diversity portion of the density-diversity penalty? How did you determine this was the most effective form of regularization?
2. How do you plan to extend the density-diversity penalty to convolutional layers, and what challenges do you anticipate in doing so?
3. Can you provide more details on the computational cost of the sorting trick and how it scales with the size of the weight matrices?