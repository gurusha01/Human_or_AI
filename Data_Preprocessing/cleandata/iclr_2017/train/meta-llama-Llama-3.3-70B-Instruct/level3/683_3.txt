Summary
The paper proposes a novel approach to neural language modeling by introducing a key-value-predict attention mechanism, which separates the output vectors into three distinct parts: key, value, and predict. This allows the model to effectively utilize a dynamically populated memory of previous token representations. The authors also experiment with a simpler N-gram RNN model, which achieves comparable results to more complex attention-based models.
Decision
I decide to reject this paper, primarily due to the lack of strong empirical validation and the failure to compare with existing literature on CIFAR-10 and CIFAR-100 benchmarks. Although the paper presents an interesting approach to neural language modeling, the results are not sufficiently convincing to demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper's results on CIFAR-10 do not improve upon simple, single-network published baselines, and the CIFAR-100 results, although somewhat interesting, do not employ common data augmentation schemes. Furthermore, the method may be more suitable for scarce labeled datasets where data augmentation is non-trivial, but this would require different benchmarks and comparison with simpler methods like data augmentation and dropout.
Additional Feedback
To improve the paper, I suggest that the authors provide more comprehensive empirical validation, including comparisons with state-of-the-art models on CIFAR-10 and CIFAR-100 benchmarks. Additionally, the authors should consider exploring the application of their method to scarce labeled datasets and comparing it with simpler methods. It would also be helpful to provide more detailed analysis of the attention mechanism and its effectiveness in capturing long-range dependencies.
Questions for the Authors
1. Can you provide more detailed comparisons with state-of-the-art models on CIFAR-10 and CIFAR-100 benchmarks?
2. How do you plan to address the issue of scarce labeled datasets, and what benchmarks would you use to evaluate the effectiveness of your method?
3. Can you provide more insight into the attention mechanism and its ability to capture long-range dependencies, and how it compares to other attention-based models?