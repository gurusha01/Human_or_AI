Summary of the Paper
The paper proposes a novel approach to understanding the Hessian of the loss function in deep learning, which has significant implications for the theoretical and practical aspects of optimization in neural networks. The authors provide empirical evidence for the existence of two phases in the spectrum of the Hessian: a bulk phase concentrated around zero, which depends on the model architecture, and a discrete phase, which depends on the input data. This work has the potential to shed new light on the convergence properties of gradient-based algorithms and the nature of the loss landscape in deep learning.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity in explaining technical terms and concepts, making it difficult for non-experts to understand. Secondly, the paper does not provide a clear analysis of the algorithm's success and failure cases, which could inform the design of future models.
Supporting Arguments
The paper's opacity and lack of clarity make it challenging to follow, and revising the text to make it clearer would be beneficial. Additionally, the authors do not provide sufficient insight into what abilities the models are lacking, and providing examples of mistakes could make the message clearer. The application of deep learning models is also unclear, including how the global max-pooling layer processes the input.
Additional Feedback
To improve the paper, I suggest that the authors provide a more detailed explanation of the technical terms and concepts used, such as LCF, OCaml-top level, and deBruijn indices. Additionally, the authors should consider comparing their deep learning methods with standard NLP techniques to provide a better understanding of the problem's difficulty. The authors should also provide a clear analysis of the algorithm's success and failure cases, which could inform the design of future models.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide a more detailed explanation of the technical terms and concepts used in the paper?
2. How do the deep learning models process the input, and what is the role of the global max-pooling layer?
3. Can you provide examples of mistakes made by the models, and what insights can be gained from these mistakes?