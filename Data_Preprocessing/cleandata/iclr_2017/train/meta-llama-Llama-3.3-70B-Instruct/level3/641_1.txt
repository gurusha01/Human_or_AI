Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to automatic dialogue evaluation, formulating it as a learning problem. The authors introduce an evaluation model, ADEM, which learns to predict human-like scores for input responses using a new dataset of human response scores. ADEM is trained in a semi-supervised manner using a hierarchical recurrent neural network (RNN) to predict human scores. The model is shown to correlate significantly with human judgements at both the utterance and system levels, outperforming traditional word-overlap metrics such as BLEU.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's writing is unclear, making it difficult to understand the exact contributions compared to previous works, and the main message is overshadowed by lengthy explanations of previous research. Secondly, the experiments have limitations, such as comparing methods for the same number of update steps and using simple datasets, including a 54-dimensional Bayesian logistic regression with a nearly Gaussian posterior.
Supporting Arguments
The paper's unclear writing makes it challenging to evaluate the significance of the contributions. The authors could improve the paper by providing a clearer explanation of the proposed method and its advantages over existing approaches. Additionally, the experiments could be strengthened by using more complex datasets and evaluating the model's performance under different conditions.
Additional Feedback and Suggestions
To improve the paper, I suggest that the authors provide more significant high-dimension, large-scale experiments to demonstrate the effectiveness of their approach. Furthermore, clearer writing and a more concise presentation of the main contributions would make the paper more accessible to readers. The authors could also consider addressing the potential limitations of their approach, such as the reliance on human evaluations and the potential biases in the dataset.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on the dataset used to train and evaluate ADEM, including the size of the dataset and the distribution of human scores?
2. How do you plan to address the potential limitations of ADEM, such as its reliance on human evaluations and the potential biases in the dataset?
3. Can you provide more information on the hyperparameter tuning process for ADEM, including the range of values considered for each hyperparameter and the criteria used to select the best combination?