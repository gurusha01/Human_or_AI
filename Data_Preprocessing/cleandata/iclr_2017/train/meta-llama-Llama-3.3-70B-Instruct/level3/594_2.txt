This paper proposes a novel optimization algorithm called Entropy-SGD for training deep neural networks. The algorithm is motivated by the local geometry of the energy landscape and aims to exploit the phenomenon of wide valleys in the energy landscape, which are known to generalize better. The authors introduce a local entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in sharp valleys.
The paper provides a clear and well-motivated introduction to the problem, and the proposed approach is well-placed in the literature. The authors provide a thorough analysis of the energy landscape of deep neural networks and demonstrate that the local geometry of the energy landscape is almost flat at local minima discovered by gradient descent.
The experimental results demonstrate that Entropy-SGD is applicable to large convolutional and recurrent deep networks used in practice and achieves comparable generalization error to state-of-the-art techniques. The authors also provide a theoretical analysis of the algorithm, showing that it results in a smoother loss function and obtains better generalization error than the original objective.
However, I have some concerns regarding the paper. Firstly, the introduction of an additional hyper-parameter to tune, namely the scope γ, adds complexity to the model. Secondly, the results of the proposed method are not very convincing overall, failing to surpass state-of-the-art performance on sequential MNIST or the memory task.
To improve the paper, I suggest that the authors include competing approaches, such as uRNNs, in tables and figures for a more comprehensive comparison. Additionally, the authors could provide more insight into the choice of the hyper-parameter γ and its effect on the performance of the algorithm.
Overall, I would reject this paper, but with the suggestion that the authors revise and resubmit it, addressing the concerns mentioned above.
To answer the three key questions:
1. The specific question/problem tackled by the paper is the optimization of deep neural networks using a novel algorithm called Entropy-SGD, which exploits the local geometry of the energy landscape.
2. The approach is well-motivated, including being well-placed in the literature, as it builds upon existing work on the energy landscape of deep neural networks and provides a clear and thorough analysis of the problem.
3. The paper supports the claims, including determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous. However, the results are not very convincing overall, and the introduction of an additional hyper-parameter adds complexity to the model.
Additional feedback:
* The paper could benefit from a more detailed analysis of the effect of the hyper-parameter γ on the performance of the algorithm.
* The authors could provide more insight into the choice of the hyper-parameter γ and its relationship to the energy landscape of the deep neural network.
* The paper could include more comparisons with other optimization algorithms, such as SGD and Adam, to demonstrate the effectiveness of Entropy-SGD.
Questions to the authors:
* Can you provide more insight into the choice of the hyper-parameter γ and its effect on the performance of the algorithm?
* How does the scope γ relate to the energy landscape of the deep neural network, and what are the implications of this relationship for the optimization process?
* Can you provide more comparisons with other optimization algorithms, such as SGD and Adam, to demonstrate the effectiveness of Entropy-SGD?