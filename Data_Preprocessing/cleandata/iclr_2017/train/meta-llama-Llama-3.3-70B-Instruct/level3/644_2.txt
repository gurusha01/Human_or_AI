Summary of the Paper's Contributions
The paper presents a novel approach to multitask deep reinforcement learning guided by policy sketches. The authors introduce a framework that associates each subtask with a modular subpolicy, which are jointly optimized to learn shared abstractions across tasks. The approach is evaluated on two environments, a maze navigation game and a 2-D Minecraft-inspired crafting game, and demonstrates significant improvements over baseline methods.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Accept the paper. The key reasons for this decision are:
1. The paper tackles a specific and important problem in reinforcement learning, namely, learning hierarchical policies from minimal supervision.
2. The approach is well-motivated and placed in the literature, drawing on existing work on hierarchical reinforcement learning and policy sketches.
3. The paper provides empirical evidence of the effectiveness of the approach, demonstrating significant improvements over baseline methods on two challenging environments.
Supporting Arguments
The paper provides a clear and well-structured presentation of the approach, including a detailed description of the model, policy optimization, and curriculum learning. The experiments are well-designed and provide a thorough evaluation of the approach, including ablation studies and zero-shot generalization experiments. The results demonstrate the effectiveness of the approach in learning hierarchical policies and adapting to new tasks.
Additional Feedback and Questions
To further improve the paper, I suggest providing more details on the implementation of the subpolicies and critics, as well as the hyperparameter tuning process. Additionally, it would be interesting to see more analysis on the learned subpolicies and their interpretability.
Some questions I would like the authors to answer are:
* How do the learned subpolicies generalize to new tasks and environments?
* Can the approach be applied to more complex environments, such as 3-D games or real-world robotics?
* How does the approach compare to other methods for learning hierarchical policies, such as options or hierarchical abstract machines?