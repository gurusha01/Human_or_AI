Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and aims to reduce the computational cost associated with Batch Normalization (BN) and Layer Normalization (LN) in recurrent neural networks. The authors derive the gradients of the Normalized LSTM and investigate its impact on the gradient flow. They also propose a scheme to initialize the weight matrices and demonstrate the effectiveness of their approach on character-level language modeling and image generative modeling tasks.
Decision
I decide to reject this paper, with the main reason being that the experiments are not convincing in showing significant improvement over Layer Norm or other methods. While the paper presents some promising results, the speedup is not substantially faster, and the theoretical analysis does not provide new insights, making the work seem like good incremental progress, but possibly not significant enough for ICLR.
Supporting Arguments
The paper builds upon previous studies, attempting to create a batch norm equivalent for RNNs, which is a well-motivated approach. However, the experimental results are not convincing, and the comparison to other methods is not thorough enough. For example, the paper only compares the proposed method to LN and BN, but not to other recent approaches, such as Weight Normalization or other variants of normalization. Additionally, the paper could benefit from more detailed analysis of the gradient flow and the impact of the proposed reparametrization on the training process.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed experiments, including comparisons to other recent approaches, and a more thorough analysis of the gradient flow and the impact of the proposed reparametrization on the training process. Additionally, the authors could investigate the effect of not keeping the variance estimates of the cell and hidden states fixed during the learning process, as mentioned in the conclusion. It would also be helpful to provide more insights into the trade-offs between the proposed method and other normalization techniques, such as computational cost, memory usage, and training time.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the computational cost and memory usage of the proposed method compared to other normalization techniques?
2. How do you plan to address the issue of saturation or explosion regimes in the gradient flow, and what are the implications of the proposed reparametrization on the training process?
3. Can you provide more insights into the effect of the proposed method on the training process, such as the convergence rate, and the impact on the model's performance on different tasks?