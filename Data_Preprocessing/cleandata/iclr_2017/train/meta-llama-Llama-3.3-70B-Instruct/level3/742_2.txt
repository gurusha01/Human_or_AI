This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning to refine a pre-trained Recurrent Neural Network (RNN) for sequence generation tasks. The approach uses the pre-trained RNN to supply part of the reward value in a reinforcement learning model, allowing the model to learn from data while incorporating domain-specific constraints. The paper demonstrates the effectiveness of RL Tuner in music generation tasks, showing that it can produce more coherent and pleasing melodies than a standard RNN.
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper does not contribute significantly new aspects to the discussion, despite advancing and elaborating on interesting topics. The idea of combining supervised learning and reinforcement learning is not novel, and the paper's contribution lies mainly in its application to music generation tasks. Secondly, the paper's length and rushed writing make the main points clear, but the results and conclusions could be clearer about assumptions and experimental vs theoretical nature.
To support this decision, I provide the following arguments. The paper's discussion on measures of expressivity, such as the number of Dichotomies, relates to statistical learning theory and VC-dimension, implying high statistical complexity. However, the statement on the underlying reason for the proportionality of three measures to trajectory length is questioned, suggesting that assumptions on trajectory types also play a crucial role. Additionally, Theorem 1 could be improved with more specific definitions of "random neural network" and "one-dimensional trajectory", and clearer notation for expressing asymptotic lower bounds.
To improve the paper, I suggest that the authors provide more detailed explanations of their methods and results, particularly in the experimental section. The authors should also consider providing more context and background information on the music generation task and the relevance of the proposed approach to this task. Furthermore, the authors should address the potential limitations and biases of their approach, such as the reliance on a pre-trained RNN and the choice of reward function.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to address the issue of overfitting in the RL Tuner model, particularly when the reward function is complex and difficult to optimize? (2) Can the authors provide more details on the music theory rules used to define the reward function, and how these rules are selected and validated? (3) How do the authors plan to extend the RL Tuner approach to more complex sequence generation tasks, such as text or image generation?