Summary
The paper proposes a novel approach to sequence learning, called Incremental Sequence Learning, which involves training a network on the first few steps of each sequence and gradually increasing the length of the sequences used for training. The authors demonstrate the effectiveness of this approach on a new sequence learning task, predicting and classifying MNIST pen stroke sequences, and show that it significantly improves sequence learning performance, reducing the test error by 74% and achieving a 20-fold speedup in computation time.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper proposes a novel and well-motivated approach to sequence learning, and (2) the experimental results demonstrate the effectiveness of the proposed approach on a new and challenging sequence learning task.
Supporting Arguments
The paper provides a clear and well-structured presentation of the proposed approach, including a detailed description of the methodology and experimental setup. The results are impressive, showing significant improvements in sequence learning performance and computation time. The authors also provide a thorough analysis of the origins of the performance improvement, including experiments that rule out alternative explanations.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following points: (1) provide more details on the implementation of the Incremental Sequence Learning approach, including the specific hyperparameters used and the criteria for increasing the sequence length; (2) compare the proposed approach to other state-of-the-art sequence learning methods, such as scheduled sampling and generative adversarial networks; and (3) provide more insights into the learned representations and how they relate to the sequence learning task.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How did you choose the specific hyperparameters for the Incremental Sequence Learning approach, and what is the sensitivity of the results to these hyperparameters? (2) Can you provide more details on the learned representations and how they relate to the sequence learning task? (3) Have you considered applying the proposed approach to other sequence learning tasks, such as language modeling or speech recognition?