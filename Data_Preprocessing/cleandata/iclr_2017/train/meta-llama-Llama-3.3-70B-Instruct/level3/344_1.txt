This paper presents a novel approach to exploration in deep reinforcement learning (RL) by generalizing classic count-based methods to high-dimensional state spaces through hashing. The authors propose a simple yet effective algorithm that achieves near state-of-the-art performance on various benchmarks, including continuous control tasks and Atari 2600 games.
The paper tackles the specific question of how to balance exploration and exploitation in RL, which is a fundamental problem in the field. The approach is well-motivated, building on the idea of count-based exploration, which has been shown to be effective in tabular settings. The authors provide a clear and concise explanation of their methodology, including the use of locality-sensitive hashing (LSH) to convert continuous state spaces into discrete hash codes.
The results presented in the paper are impressive, demonstrating significant improvements over baseline methods on several challenging tasks. The authors also provide a thorough analysis of the factors that contribute to the success of their approach, including the choice of hash function, granularity, and bonus coefficient.
Based on the results and analysis presented, I decide to Accept this paper. The two key reasons for this choice are: (1) the paper presents a novel and effective approach to exploration in deep RL, which has the potential to impact the field; and (2) the results are thoroughly evaluated and analyzed, providing a clear understanding of the strengths and limitations of the approach.
To improve the paper, I suggest that the authors provide more context and explanations for certain ideas and notation, such as the role of the w vector, to avoid confusion. Additionally, the authors may want to consider evaluating their approach on other domains and tasks to further demonstrate its general applicability.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) How do the authors choose the hyperparameters for their algorithm, such as the bonus coefficient and granularity? (2) Can the authors provide more insight into the trade-offs between different hash functions and their impact on performance? (3) How does the authors' approach compare to other exploration strategies, such as entropy regularization and curiosity-driven exploration?