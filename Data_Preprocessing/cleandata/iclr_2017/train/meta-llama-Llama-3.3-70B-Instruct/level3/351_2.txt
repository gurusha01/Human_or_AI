1. The paper proposes a novel framework called Generative Multi-Adversarial Network (GMAN), which extends the traditional Generative Adversarial Network (GAN) to multiple discriminators. This approach enables the generator to receive more diverse and informative feedback, leading to improved performance and faster convergence. The authors demonstrate the effectiveness of GMAN on various image generation tasks, including MNIST, CIFAR-10, and CelebA.
2. Based on the provided information, I decide to Accept this paper. The two key reasons for this choice are: (1) the paper tackles a significant issue in deep learning, namely the difficulty of training GANs, and provides a well-motivated and theoretically grounded solution; (2) the experimental results demonstrate the effectiveness of GMAN in improving the quality and convergence speed of generated images, compared to traditional GANs and other variants.
3. The paper is well-written, and the authors provide a clear and concise explanation of the GMAN framework, its theoretical foundations, and the experimental setup. The results are impressive, showing that GMAN can achieve higher-quality images and faster convergence than traditional GANs. The use of multiple discriminators allows the generator to receive more diverse feedback, which helps to avoid mode collapse and improve the overall performance. The introduction of a softmax function to control the leniency of the discriminators is also a nice touch, as it enables the generator to adapt to the changing dynamics of the game.
4. To further improve the paper, I would suggest the following: (1) provide more visualizations of the generated images, especially for the CIFAR-10 and CelebA datasets, to give a better sense of the quality and diversity of the results; (2) consider adding more ablation studies to investigate the effect of different components of the GMAN framework, such as the number of discriminators, the softmax function, and the regularization term; (3) provide more discussion on the potential applications of GMAN beyond image generation, such as data augmentation, style transfer, or domain adaptation.
5. I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How do the authors ensure that the multiple discriminators are diverse and do not collapse to a single mode? (2) Can the authors provide more insight into the choice of the softmax function and its hyperparameters, and how they affect the performance of GMAN? (3) How does GMAN compare to other multi-discriminator approaches, such as the one proposed in Yoo et al. (2016), and what are the key differences and advantages of GMAN?