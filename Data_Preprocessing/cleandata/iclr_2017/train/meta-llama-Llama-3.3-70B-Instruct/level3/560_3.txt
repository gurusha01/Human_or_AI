This paper presents a novel approach to policy search in stochastic dynamical systems using model-based reinforcement learning with Bayesian neural networks (BNNs) that include stochastic input variables. The authors propose an algorithm that uses random roll-outs and stochastic optimization for learning an optimal policy from the predictions of BNNs. The paper claims to achieve state-of-the-art results on several benchmark problems, including the Wet-Chicken problem, which has not been solved by model-based approaches before.
Based on the provided guidelines, I will evaluate the paper as follows:
1. The specific question/problem tackled by the paper is policy search in stochastic dynamical systems using model-based reinforcement learning with BNNs.
2. The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing methods and how their approach addresses these limitations.
3. The paper supports its claims with experimental results on several benchmark problems, including the Wet-Chicken problem, gas turbine data, and an industrial benchmark.
However, I have some concerns regarding the experimental evaluation. The paper presents results on only a few benchmark problems, and the experimental setup is not thoroughly described. Additionally, the authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to assess the significance of their results.
Therefore, my decision is to reject the paper, with the main reason being the lack of comprehensive experimental evaluation and comparison with other state-of-the-art methods.
To improve the paper, I suggest the following:
* Provide a more detailed description of the experimental setup, including the hyperparameters used and the computational resources required.
* Include a more comprehensive comparison with other state-of-the-art methods, including model-free reinforcement learning approaches.
* Consider adding more benchmark problems to demonstrate the generality of the proposed approach.
* Provide a more detailed analysis of the results, including an discussion of the limitations of the proposed approach and potential avenues for future research.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on the hyperparameters used for the BNNs and the policy optimization algorithm?
* How do you select the number of hidden units and layers for the BNNs?
* Can you provide more information on the computational resources required for training the BNNs and the policy optimization algorithm?
* How do you handle exploration-exploitation trade-off in the proposed approach?