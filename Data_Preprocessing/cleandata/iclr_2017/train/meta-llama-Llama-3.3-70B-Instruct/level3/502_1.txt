Summary
The paper proposes a novel approach to evaluating the quality of dialogue responses, called ADEM, which learns to predict human-like scores to input responses using a hierarchical recurrent neural network (RNN) encoder. The model is trained on a dataset of human scores to various dialogue responses and achieves a significant correlation with human judgements at both the utterance and system-level. ADEM also generalizes to evaluating new models, whose responses were unseen during training, without a drop in performance.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the effectiveness of the proposed metric is questionable as it may be meaningless if the initial dialogue model is not of good quality. Secondly, the paper relies heavily on the quality of the initial dialogue model, which may not always be available or reliable.
Supporting Arguments
The paper tackles the issue of evaluating automatic dialogue responses, highlighting the limitations of current methods like BLEU that don't correlate well with human annotation quality. However, the proposed approach relies on having a reasonably good initial dialogue model, which may not always be the case. The paper shows that ADEM correlates significantly with human judgements, but this correlation may not hold if the initial model is of poor quality. Furthermore, the paper does not provide a clear explanation of how to select or evaluate the initial dialogue model, which is a crucial component of the proposed approach.
Additional Feedback
To improve the paper, the authors could provide more details on how to select or evaluate the initial dialogue model, and explore ways to make the proposed approach more robust to the quality of the initial model. Additionally, the authors could provide more analysis on the failure cases of the ADEM model, and explore ways to improve its performance in these cases. The authors could also consider providing more details on the dataset used to train and evaluate the model, and explore ways to make the model more generalizable to different domains and tasks.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do you select or evaluate the initial dialogue model, and what are the implications of using a poor-quality initial model on the performance of ADEM?
* Can you provide more analysis on the failure cases of the ADEM model, and explore ways to improve its performance in these cases?
* How do you plan to make the proposed approach more robust to the quality of the initial model, and what are the potential limitations of the approach?
* Can you provide more details on the dataset used to train and evaluate the model, and explore ways to make the model more generalizable to different domains and tasks?