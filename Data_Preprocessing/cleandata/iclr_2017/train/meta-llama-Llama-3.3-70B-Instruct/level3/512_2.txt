This paper introduces a novel approach to learning activation functions in deep neural networks, allowing for nonparametric estimation of these functions through a Fourier basis expansion. The authors propose a two-stage optimization process to learn both the weights and non-linearity weights, demonstrating improved performance on several benchmark datasets. The significance of this work lies in its innovative idea and experimental results, which show relative improvements in test error rates of up to 15% on the MNIST and CIFAR-10 datasets.
However, I have some concerns regarding the theoretical analysis presented in the paper. While the authors provide a generalization bound for their model using the algorithmic stability approach, I find the analysis to be somewhat uninformative and distracting from the main contributions of the paper. Furthermore, I would like to see more thorough experiments using different basis and comparing them to wider networks to support the paper's results.
One specific question I have is whether the learned non-linearity weights are shared across all units and layers, or if each unit has its own non-linearity. If the weights are shared, it would be interesting to study the existence of an optimal non-linearity if all weights are tied across units and layers. Additionally, I would like to know how the learned non-linearity changes when using batch normalization or not, and if normalization affects the conclusion about polynomial basis failure.
Based on these considerations, I decide to reject the paper, primarily due to the lack of thorough experimentation and the uninformative theoretical analysis. However, I believe that the paper has potential and with additional experiments and clarifications, it could be a strong contribution to the field.
To improve the paper, I suggest the following:
* Conduct more thorough experiments using different basis and comparing them to wider networks.
* Clarify whether the learned non-linearity weights are shared across all units and layers, or if each unit has its own non-linearity.
* Study the existence of an optimal non-linearity if all weights are tied across units and layers.
* Investigate how the learned non-linearity changes when using batch normalization or not, and if normalization affects the conclusion about polynomial basis failure.
* Provide more detailed analysis of the theoretical results, and consider alternative approaches to demonstrating the generalization bound.
By addressing these concerns and suggestions, the authors can strengthen their paper and provide a more convincing contribution to the field of deep learning.