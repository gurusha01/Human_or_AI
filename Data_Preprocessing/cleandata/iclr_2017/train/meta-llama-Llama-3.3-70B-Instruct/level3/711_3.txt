Summary
The paper proposes a novel approach to question classification by utilizing answer data to improve question representation. The authors introduce Group Sparse Autoencoders (GSA) and Group Sparse Convolutional Neural Networks (GSCNNs) to encode answer information into question representation. The GSA is a neural network-based model that learns a dictionary with group sparse constraints, while the GSCNNs incorporate GSA into CNNs to learn sentence representations with group sparse constraints. The proposed model shows significant improvements over strong baselines on four datasets.
Decision
I decide to Accept this paper with some minor revisions. The main reason for this decision is that the paper proposes a novel and well-motivated approach to question classification, and the experimental results demonstrate significant improvements over strong baselines.
Supporting Arguments
The paper tackles a specific question/problem in question classification, which is to utilize answer data to improve question representation. The approach is well-motivated, as it exploits the unique properties of question classification, such as the hierarchical structure of categories and the availability of answer data. The paper also provides a clear and concise explanation of the proposed models, GSA and GSCNNs, and demonstrates their effectiveness through experiments on four datasets.
Additional Feedback
To improve the paper, I suggest that the authors provide more analysis on the importance of question-independent representation and the superiority of the proposed model over Match-LSTM. Additionally, the authors could provide more details on the initialization methods for the projection matrix in GSA and the hyperparameter tuning process for GSCNNs. It would also be helpful to include more visualizations of the learned representations and dictionaries to provide a better understanding of the proposed models.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the dictionary learning process in GSA and how it relates to sparse coding approaches?
2. How do you select the hyperparameters for GSCNNs, such as the number of groups and the sparsity parameters?
3. Can you provide more analysis on the importance of question-independent representation in question classification and how the proposed model captures this property?