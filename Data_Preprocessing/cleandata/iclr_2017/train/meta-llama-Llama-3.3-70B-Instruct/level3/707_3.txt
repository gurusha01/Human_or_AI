This paper introduces a novel architecture, called neural equivalence networks (EQNETs), for learning continuous semantic representations of mathematical and logical expressions. The authors propose a new approach to represent procedural knowledge, which is a fundamental problem in machine learning and artificial intelligence. The paper claims to outperform existing architectures, such as recursive neural networks (TREENNs) and recurrent neural networks (RNNs), on a task of checking equivalence on a diverse class of symbolic algebraic and boolean expression types.
Based on the provided guidelines, I will evaluate this paper and provide a review. 
The specific question/problem tackled by the paper is how to represent procedural knowledge using continuous semantic representations of mathematical and logical expressions. The approach is well-motivated, as it combines abstract, symbolic reasoning with continuous neural reasoning, which is a grand challenge of representation learning. The paper is well-placed in the literature, as it builds upon existing work on program induction and neural architectures for learning representations of algorithms.
However, I have some concerns regarding the paper's claims and results. The evaluation is limited to a specific task and dataset, and it is unclear how the proposed architecture would perform on other tasks and datasets. Additionally, the paper contains some errors and typos, which need to be corrected for clarity and accuracy.
My decision is to reject the paper, with the main reason being the limited evaluation and the need for more thorough testing and validation of the proposed architecture. 
To improve the paper, I suggest that the authors provide more detailed evaluations and comparisons with existing architectures on a variety of tasks and datasets. Additionally, the authors should clarify some of the implementation details, such as the attention-based decoder and the entity state update rule, and provide more insights into the strengths and limitations of the proposed architecture.
Some questions I would like the authors to answer are: 
* How does the proposed architecture handle more complex expressions and datasets?
* Can the authors provide more insights into the learned representations and how they capture the semantics of the expressions?
* How does the proposed architecture compare to other existing architectures, such as graph neural networks and transformer-based models?
Overall, while the paper presents an interesting and novel approach to representing procedural knowledge, it requires more thorough evaluation and testing to demonstrate its effectiveness and robustness.