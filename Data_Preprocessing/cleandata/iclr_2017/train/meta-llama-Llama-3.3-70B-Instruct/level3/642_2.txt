This paper explores the tradeoff between performance, area, energy, and model accuracy in custom number representations for deep learning inference. The authors find that using fewer than 16 bits can lead to improved runtime performance and energy efficiency with minimal loss in accuracy. 
I decide to reject this paper for the following two key reasons: 
Firstly, the paper lacks a thorough comparison with existing work, particularly in the context of custom floating-point representations. For instance, the authors do not discuss whether their custom floating-point representation supports denormal numbers or if the custom floating-point unit is clocked at the same frequency as the baseline 32-bit unit. 
Secondly, the claim of energy savings from custom floating-point representation is misleading, as it does not account for the energy spent on data movement to and from memories. Furthermore, the claimed speedup only applies to the floating-point unit and may be limited by memory bandwidth.
To improve the paper, I suggest that the authors provide a more detailed comparison with existing work, including the use of IEEE half-precision floating point as a baseline for comparison. Additionally, the authors should consider the complexities and overheads introduced by non-byte aligned memory accesses when using custom number representations. 
Some questions I would like the authors to answer include: How does the custom floating-point representation handle denormal numbers? Is the custom floating-point unit clocked at the same frequency as the baseline 32-bit unit? How do the authors plan to address the energy spent on data movement to and from memories? 
Overall, while the paper presents an interesting exploration of custom number representations for deep learning inference, it requires more thorough comparison with existing work and consideration of additional factors to support its claims.