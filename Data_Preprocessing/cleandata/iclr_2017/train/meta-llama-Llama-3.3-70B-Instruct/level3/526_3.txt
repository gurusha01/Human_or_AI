Summary
The paper proposes a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. The authors demonstrate that this new architecture outperforms previous memory-augmented neural language models on two corpora, but mainly utilizes a memory of the previous five representations. They also experiment with a simpler model that uses the concatenation of previous output representations to predict the next word, which achieves comparable results.
Decision
I decide to Accept this paper, with the main reason being that the authors propose a novel and well-motivated approach to neural language modeling, and provide empirical evidence to support their claims. The paper is well-written, and the experiments are thorough and well-designed.
Supporting Arguments
The paper tackles a specific question of how to improve neural language modeling by incorporating attention mechanisms and memory-augmented architectures. The approach is well-motivated, and the authors provide a clear and concise overview of the related work. The empirical results are impressive, and the authors provide a thorough analysis of the performance of their models on two corpora. The paper also raises interesting questions about the limitations of current neural language models in capturing long-range dependencies, and provides a clear direction for future work.
Additional Feedback
To improve the paper, I suggest that the authors consider using a cross-validation set to make statements about the generalization of the resulting network. Additionally, I would like to see more analysis on the performance of the models on different types of dependencies, such as syntactic and semantic dependencies. The authors may also want to consider comparing their models to other state-of-the-art models in the field.
Questions for the Authors
I would like to ask the authors to clarify the notation used in the paper, particularly with regards to the subscript on the Jacobian. I would also like to know more about the hyperparameter tuning process, and how the authors selected the optimal hyperparameters for their models. Finally, I would like to ask the authors to provide more insight into the limitations of their models in capturing long-range dependencies, and what potential solutions they envision for addressing this challenge.