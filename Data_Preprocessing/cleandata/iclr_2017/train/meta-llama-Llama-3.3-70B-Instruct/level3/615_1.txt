Summary
The paper proposes a novel loss framework for language modeling, which addresses the limitations of the conventional classification framework. The framework introduces an additional loss term that minimizes the KL-divergence between the model's prediction and an estimated target distribution based on word embeddings. The authors also theoretically motivate and introduce a second modification, which involves reusing the input embedding matrix in the output projection layer. The framework is empirically validated on the Penn Treebank and Wikitext-2 datasets, showing significant improvements over the conventional framework.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the experimental section is unconvincing due to the lack of wall-clock time performance and limited advantage over competitor methods. Secondly, the work is considered not ready for publication due to unconvincing experiments and lack of a compelling reason to implement the proposed framework for deep model training.
Supporting Arguments
The paper proposes a new loss framework, but the experimental results do not demonstrate a significant advantage over existing methods. The authors claim that the proposed framework outperforms the conventional one, but the improvements are not substantial enough to justify the additional complexity. Furthermore, the paper lacks a formal theoretical argument and intuition for the proposed framework, which makes it difficult to understand the underlying mechanisms.
Additional Feedback
To improve the paper, the authors should provide more convincing experimental results, including wall-clock time performance and comparisons with state-of-the-art methods. Additionally, the authors should provide a more detailed theoretical analysis of the proposed framework, including a formal proof of the benefits of reusing the input embedding matrix in the output projection layer. The authors should also consider providing more qualitative results, such as visualizations or examples, to illustrate the benefits of the proposed framework.
Questions for the Authors
1. Can you provide more details on the experimental setup, including the hyperparameters used and the computational resources required?
2. How do you plan to address the lack of a formal theoretical argument and intuition for the proposed framework?
3. Can you provide more comparisons with state-of-the-art methods, including those that use different loss functions or optimization algorithms?
4. How do you plan to extend the proposed framework to other tasks, such as neural machine translation or speech recognition?
5. Can you provide more qualitative results, such as visualizations or examples, to illustrate the benefits of the proposed framework?