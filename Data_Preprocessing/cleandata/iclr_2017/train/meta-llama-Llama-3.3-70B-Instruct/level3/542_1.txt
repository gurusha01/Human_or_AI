Summary
The paper presents a novel approach to learning shared neural representations of temporal abstractions in hierarchical reinforcement learning (RL) using actor-critic methods. The approach, guided by policy sketches, associates each subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating improved performance over standard baselines.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the approach lacks clear novelty and contribution beyond existing ideas in hierarchical RL, and relies heavily on curriculum learning, which can be complicated to design for larger problems. Secondly, the experiments are not described in enough detail, making them non-reproducible, and the authors should provide more information on task setup choices and optimization.
Supporting Arguments
The paper's approach, while promising, builds upon existing ideas in hierarchical RL, such as the options framework and hierarchical abstract machines. The use of curriculum learning, while effective in the presented experiments, may not be easily scalable to more complex tasks. Furthermore, the lack of detailed experimental descriptions and reproducibility raises concerns about the validity of the results. The authors should provide more information on the task setup, optimization procedures, and hyperparameter choices to facilitate reproducibility.
Additional Feedback
To improve the paper, the authors should step back to think more generally about the approach and provide a clearer motivation for the use of policy sketches. They should also provide more detailed descriptions of the experiments, including task setup, optimization procedures, and hyperparameter choices. Additionally, the authors should consider providing more theoretical analysis or comparisons to existing hierarchical RL methods to demonstrate the novelty and effectiveness of their approach.
Questions for the Authors
1. Can you provide more details on the task setup and optimization procedures used in the experiments?
2. How do you plan to address the scalability issues with curriculum learning for more complex tasks?
3. Can you provide more theoretical analysis or comparisons to existing hierarchical RL methods to demonstrate the novelty and effectiveness of your approach?