Summary
The paper proposes a novel neural network architecture, called Perception Updating Networks (PUN), for modeling and generating videos of 2D scenes. The approach is inspired by computer graphics pipelines and variational auto-encoding Bayes, and it decouples the representation of objects (sprites) and their dynamics in a video. The authors demonstrate the effectiveness of their approach on synthetic datasets, including bouncing shapes and moving MNIST, and show that it can generate more realistic and longer videos compared to baseline RNNs.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and novel approach to video modeling and generation. The paper is well-structured, and the authors provide a clear explanation of their methodology and experiments.
Supporting Arguments
The paper tackles a specific question of modeling and generating videos of 2D scenes, which is a relevant problem in the field of computer vision. The approach is well-motivated, as it draws inspiration from computer graphics pipelines and variational auto-encoding Bayes. The authors provide a clear explanation of their methodology, including the statistical framework and the Perception Updating Networks architecture. The experiments are well-designed, and the results demonstrate the effectiveness of the approach.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the computational resources required to train the models. Additionally, it would be interesting to see more comparisons with other state-of-the-art video generation models. The authors may also consider providing more visualizations of the generated videos to better illustrate the quality and diversity of the results.
Questions for the Authors
1. Can you provide more details on the hyperparameter tuning process and the computational resources required to train the models?
2. How do you plan to extend the approach to more complex scenes and videos, such as those with multiple objects and dynamic backgrounds?
3. Can you provide more comparisons with other state-of-the-art video generation models, such as Video Pixel Networks and Generative Adversarial Networks?