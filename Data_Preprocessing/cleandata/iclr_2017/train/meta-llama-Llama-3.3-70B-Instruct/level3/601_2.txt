The paper introduces a new comprehension dataset called NewsQA, which contains 100,000 question-answer pairs from over 10,000 news articles. This dataset has the potential to benefit models with its diverse set of datasets, and its size is sufficient for deep learning models to train. The proposed BARB model performs as well as a published state-of-the-art model while being much faster, showcasing its potential.
Based on the provided guidelines, I will evaluate the paper and provide a review. 
The specific question/problem tackled by the paper is the creation of a challenging machine comprehension dataset that requires reasoning mechanisms, such as synthesis of information across different parts of an article. The approach is well-motivated, including being well-placed in the literature, as it addresses the limitations of existing datasets and provides a new resource for the research community.
However, I decide to reject this paper with two key reasons: the human evaluation is weak, and the paper lacks a clear comparison of human performance on NewsQA and SQuAD. The human evaluation is limited to only two near-native English speakers' performance on 100 examples each, which is not representative of the complete dataset. Additionally, the paper does not provide a clear comparison of human performance on NewsQA and SQuAD, making it difficult to demonstrate that NewsQA is harder than SQuAD.
Supporting arguments for these reasons include the inconsistencies in the data collection process, such as the percentage of questions with answers agreed upon by at least two workers, and the small test set. The hyperparameters are not tuned using a validation set from NewsQA, which may impact the model's performance. Furthermore, the dataset release could be improved by providing two versions, one with all answers collected and one with the validation step, to cater to different needs.
To improve the paper, I suggest that the authors conduct a more comprehensive human evaluation, including a larger and more diverse group of participants, and provide a clear comparison of human performance on NewsQA and SQuAD. Additionally, the authors should address the inconsistencies in the data collection process and provide more details on the hyperparameter tuning.
I would like to ask the authors to clarify the following points: How do the authors plan to address the limitations of the human evaluation, and what steps will they take to provide a more comprehensive comparison of human performance on NewsQA and SQuAD? How do the authors plan to improve the data collection process, and what measures will they take to ensure the consistency of the dataset? What are the plans for future work, and how do the authors envision the NewsQA dataset being used in the research community?