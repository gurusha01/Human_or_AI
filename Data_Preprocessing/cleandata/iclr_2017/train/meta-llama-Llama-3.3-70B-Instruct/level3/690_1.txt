Summary
The paper presents a comprehensive investigation of different context types and representations for learning word embeddings, a crucial aspect of natural language processing. The authors conduct extensive experiments to evaluate the effectiveness of various context definitions under four tasks and 21 datasets, providing valuable insights into context selection. This work has the potential to serve as a guideline for the community in choosing the appropriate context for word embedding models.
Decision
I decide to Reject this paper, with the primary reason being that the paper does not adequately address the practical value of its findings in realistic scenarios, such as fine-tuning networks and production settings. Additionally, the evaluation lacks consideration of standard steps like compression and network topology analysis, which are essential for assessing the true effectiveness of word embedding models.
Supporting Arguments
The paper's focus on evaluating recent ILSVRC CNN architectures from a resource utilization perspective is well-organized, but the results may not be surprising for regular CNN users. The lack of discussion on realistic circumstances, such as fine-tuning networks, limits the paper's practical impact. Furthermore, the comparison of model compression is crucial, particularly against well-known compressed networks, to assess accuracy/parameter density. The paper's failure to account for these factors reduces its scientific rigor and usefulness.
Additional Feedback
To improve the paper, I suggest that the authors consider analyzing network topology and bottlenecks to provide more insightful results. Additionally, they should discuss the use of batch normalization in NiN and AlexNet, as it raises minor concerns about design choices. The authors should also provide more context about the experimental setup and hyperparameter tuning to increase the reproducibility of their results.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. How do the authors plan to address the lack of discussion on realistic scenarios, such as fine-tuning networks, in their evaluation?
2. Can the authors provide more details about their experimental setup and hyperparameter tuning to increase the reproducibility of their results?
3. How do the authors envision their work being applied in production settings, and what steps can be taken to make their findings more practical and useful for the community?