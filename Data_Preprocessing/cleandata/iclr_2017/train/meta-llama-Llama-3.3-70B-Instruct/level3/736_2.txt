Summary
The paper proposes a novel method called zoneout for regularizing Recurrent Neural Networks (RNNs). Zoneout stochastically preserves hidden units' activations, which helps to improve the robustness of RNNs to perturbations in the hidden state. The authors demonstrate the effectiveness of zoneout on several tasks, including character-level and word-level language modeling, and classification on permuted sequential MNIST.
Decision
I decide to reject this paper, with the main reason being that the method needs more experimental evaluation, particularly in scenarios where training from scratch is required, and a fair comparison with state-of-the-art methods on toy datasets. Additionally, I believe the paper is more suitable for a workshop track due to concerns that the main idea is obscured and the reported improvement may be due to the strong base CNN used.
Supporting Arguments
The paper proposes a simple trick to turn batch normalization into a domain adaptation technique, but the experimental evaluation is limited, and it is unclear whether the method would work in more challenging scenarios. The authors claim that zoneout improves performance across tasks, but the comparison with other regularizers is not thorough, and it is unclear whether zoneout is truly better than other methods. Furthermore, the paper lacks a clear explanation of why zoneout works, and the authors rely heavily on empirical results to support their claims.
Additional Feedback
To improve the paper, I suggest that the authors conduct more extensive experimental evaluations, including comparisons with other state-of-the-art methods on a variety of datasets. Additionally, the authors should provide a clearer explanation of why zoneout works and how it relates to other regularization techniques. It would also be helpful to include more detailed analysis of the results, including ablation studies and visualizations of the learned representations.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more details on the experimental setup and hyperparameter tuning process?
2. How does zoneout compare to other regularization techniques, such as dropout and weight decay, in terms of performance and computational cost?
3. Can you provide more insight into why zoneout works, and how it relates to other techniques, such as stochastic depth and recurrent dropout?