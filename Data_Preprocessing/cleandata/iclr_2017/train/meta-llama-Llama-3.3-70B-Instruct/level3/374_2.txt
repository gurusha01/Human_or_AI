Summary
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN architecture utilizes a time-invariant orthogonal transformation followed by an input-modulated orthogonal transformation to update the hidden state, ensuring that the forward hidden state activation norm and backwards gradient norm are preserved. The authors demonstrate the effectiveness of their approach on a simplified memory copy task, achieving dependencies of up to 5,000 timesteps.
Decision
I decide to Accept this paper, with the primary reason being that the approach is well-motivated and effectively addresses a significant problem in RNNs. The use of orthogonal transformations to preserve norm and prevent vanishing or exploding gradients is a clever and insightful solution.
Supporting Arguments
The paper provides a clear and concise explanation of the problem and the proposed solution. The authors demonstrate a thorough understanding of the literature and provide a comprehensive overview of existing approaches to addressing the vanishing and exploding gradients problem. The experimental results are convincing, showing that the DORNN architecture can learn dependencies of up to 5,000 timesteps on a simplified memory copy task.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed analysis of the computational complexity and memory requirements of the DORNN architecture, as well as its potential applications to more complex tasks. Additionally, it would be interesting to see a comparison with other state-of-the-art RNN architectures, such as LSTMs and GRUs, to better understand the strengths and weaknesses of the DORNN approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to address the following questions:
1. How do the authors plan to extend the DORNN architecture to more complex tasks, such as natural language processing or image captioning?
2. Can the authors provide more insight into the choice of parameterization for the orthogonal matrices, and how it affects the performance of the DORNN architecture?
3. How does the DORNN architecture handle cases where the input data is not orthogonal, and what are the implications for the preservation of norm and prevention of vanishing or exploding gradients?