Summary
The paper proposes an extension of weight normalization to recurrent neural networks, specifically LSTMs, which preserves the means and variances of the hidden states and memory cells across time. This approach, called Normalized LSTM, is shown to have similar training benefits to Recurrent Batch Normalization and Layer Normalization, but with fewer computations required. The authors also investigate the impact of this parametrization on gradient flows and propose a way of initializing the weights accordingly.
Decision
I decide to reject this paper, with the main reasons being the lack of extreme novelty in the contribution and the unconvincing experiments. While the idea of extending weight normalization to LSTMs is interesting, the changes made to the original weight normalization technique are minor, and the experiments do not provide strong evidence to support the claims made by the authors.
Supporting Arguments
The paper's contribution, although potentially useful, lacks significant novelty. The authors build upon existing work on weight normalization and batch normalization, and the modifications made to the original techniques are not substantial. Furthermore, the experiments presented in the paper are not convincing, as the results show that layer normalization performs better in some cases, and the authors' approach does not significantly outperform other state-of-the-art methods.
Additional Feedback
To improve the paper, the authors could provide more detailed comparisons with other normalization techniques, such as a thorough analysis of the computational costs and training times. Additionally, the authors could investigate the impact of their approach on more challenging tasks and datasets, to demonstrate its effectiveness in a wider range of scenarios. It would also be helpful to include more visualizations, such as plots of the gradient flows and variance estimates, to provide a clearer understanding of the proposed method.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on the initialization scheme used for the weight matrices, and how it affects the learning process?
2. How do you plan to address the issue of overfitting, which is mentioned in the paper as a potential problem with layer normalization?
3. Can you provide more insights into the computational costs and training times of the proposed approach, compared to other normalization techniques?