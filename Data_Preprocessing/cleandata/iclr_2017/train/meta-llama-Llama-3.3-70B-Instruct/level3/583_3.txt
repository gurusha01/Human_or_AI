The paper proposes a method to measure out-of-distribution novelty by using another model trained on a different dataset to judge a model's ability to generate novel samples. The authors also introduce a Markov chain Monte Carlo (MCMC) sampling process for generative autoencoders, which allows sampling from the learned latent distribution. 
I decide to reject this paper for two main reasons. Firstly, the proposed metric for measuring out-of-distribution novelty is problematic as it uses a naive combination of MNIST and letters dataset, which does not represent the natural distribution of handwritten digits and letters. Secondly, the "novel" samples generated by the model are not truly novel, but rather distorted digits, and quantizing the samples to a higher bit depth would make them look even more like digits.
The paper's approach to using MCMC sampling to improve the quality of samples drawn from generative autoencoders is well-motivated and has a clear contribution to the field. However, the evaluation of the method is limited, and the results are not convincing. The paper would benefit from more rigorous evaluation and comparison to existing methods.
To improve the paper, I would suggest that the authors revisit their evaluation methodology and provide more convincing results. Additionally, they should consider using more realistic datasets and evaluating their method on a wider range of tasks. I would also like the authors to answer the following questions: How do the authors plan to address the issue of the proposed metric being problematic? Can they provide more examples of truly novel samples generated by their model? How do they plan to improve the evaluation of their method to make it more convincing?
Overall, while the paper has some interesting ideas, it falls short in terms of evaluation and results, and therefore, I decide to reject it.