This paper proposes an online learning method for sum-product networks (SPNs) that learns both parameters and structure online, assuming Gaussian coordinate-wise marginal distributions. The authors introduce a new online structure learning technique, called oSLRAU, which starts with a fully factorized joint probability distribution and updates the structure as new data points are processed. The algorithm detects correlations between variables and modifies the structure to represent these correlations.
The paper claims to contribute to the state of the art by proposing the first online structure learning technique for Gaussian SPNs, which can scale to large datasets efficiently. The authors evaluate their algorithm on several benchmark datasets and compare it to other algorithms, including online Bayesian moment matching, online expectation maximization, stacked restricted Boltzmann machines, and generative moment matching networks.
Based on the results, I decide to reject this paper, with two key reasons for this choice. Firstly, the algorithm's scalability is promising, but limited by the small number of variables used in the dataset, and its ability to scale "horizontally" to large numbers of observations and instances is unclear. Secondly, the use of SPN-specific datasets and GenMMN as a baseline may not be effective in bridging the gap to the neural network community, and alternative baselines like VAE or Real-NVP may be more suitable.
To support these reasons, I provide the following arguments. The paper shows that oSLRAU outperforms other algorithms on several datasets, but the number of variables in these datasets is relatively small. It is unclear whether the algorithm can handle larger datasets with many more variables. Additionally, the paper uses GenMMN as a baseline, which may not be a fair comparison since GenMMN is a different type of model. Using alternative baselines like VAE or Real-NVP may provide a more accurate comparison.
To improve the paper, I suggest that the authors evaluate the feasibility of modeling large datasets like MNIST and use alternative baselines like VAE or Real-NVP. This would help to bridge the gap with other literature in deep generative modeling and provide a more comprehensive evaluation of the algorithm's performance. I would also like to see more analysis on the effect of varying the correlation threshold and the maximum number of variables per leaf node on the resulting SPN.
I have several questions that I would like the authors to answer to clarify my understanding of the paper. How does the algorithm handle missing data or outliers in the dataset? Can the authors provide more details on the computational complexity of the algorithm and how it scales to large datasets? How does the algorithm compare to other online learning methods for SPNs, such as online Bayesian moment matching or online expectation maximization?