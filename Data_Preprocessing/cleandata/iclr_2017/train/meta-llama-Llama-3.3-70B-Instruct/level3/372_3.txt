This paper proposes a novel extension to the Neural Turing Machine (NTM) model, called the Dynamic Neural Turing Machine (D-NTM), which introduces a learnable addressing scheme to improve the model's ability to store and retrieve information from its external memory. The D-NTM is evaluated on several tasks, including episodic question-answering, sequential MNIST, and algorithmic tasks, and demonstrates significant improvements over the original NTM and other baseline models.
The paper tackles the specific question of how to improve the addressing mechanism of NTMs to enable more efficient and effective storage and retrieval of information. The approach is well-motivated, as it addresses a key limitation of the original NTM model, and is well-placed in the literature, building on previous work on NTMs and other neural network models with external memory.
The paper supports its claims with extensive experimental results, demonstrating the effectiveness of the D-NTM on a range of tasks. The results are scientifically rigorous, with careful consideration of hyperparameters, training procedures, and evaluation metrics.
Based on the results, I decide to accept this paper, with two key reasons for this choice: (1) the paper proposes a novel and well-motivated extension to the NTM model, and (2) the experimental results demonstrate significant improvements over baseline models on several tasks.
To further improve the paper, I provide the following feedback: (1) consider adding more comparisons to other existing memory modules, such as associative LSTMs, to provide a more comprehensive evaluation of the D-NTM; (2) provide more detailed analysis of the learned addressing schemes, to gain a better understanding of how the D-NTM is using its external memory; and (3) consider applying the D-NTM to more real-world tasks, to demonstrate its practical applicability.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How does the learnable addressing scheme of the D-NTM differ from other addressing mechanisms, such as content-based addressing or location-based addressing? (2) Can you provide more details on the training procedure for the D-NTM, including the optimization algorithm and hyperparameter settings used? (3) How do you plan to extend the D-NTM to more complex tasks, such as text summarization or visual question-answering, and what challenges do you anticipate in doing so?