This paper proposes a novel approach to learning activation functions in deep neural networks using nonparametric estimation. The authors introduce a class of nonparametric models for activation functions, which can be incorporated into the back-propagation framework, allowing for easy implementation. The paper provides a theoretical justification for the choice of nonparametric activation functions and demonstrates their effectiveness on several benchmark datasets, including MNIST and CIFAR-10.
The paper tackles the specific question of how to learn activation functions in deep neural networks, which is a crucial aspect of neural network design. The approach is well-motivated, as it allows for the expansion of the function class that each node in the network can learn, potentially leading to improved performance. The paper is also well-placed in the literature, as it builds upon existing work on learning activation functions and provides a new perspective on this topic.
However, I have some concerns regarding the paper's claims and the experimental results. The paper claims that the proposed approach achieves state-of-the-art performance on several datasets, but the experimental results are not comprehensive, and the comparison to other methods is limited. Additionally, the paper lacks a thorough analysis of the computational cost and the potential limitations of the proposed approach.
Based on these concerns, I decide to reject the paper. The main reasons for this decision are the lack of comprehensive experimental results and the limited comparison to other methods. While the paper proposes an interesting approach to learning activation functions, it falls short in providing a thorough evaluation of its effectiveness and potential limitations.
To improve the paper, I suggest that the authors provide more comprehensive experimental results, including a comparison to other state-of-the-art methods, and a thorough analysis of the computational cost and potential limitations of the proposed approach. Additionally, the authors should consider providing more details on the implementation of the proposed approach and the hyperparameter tuning process.
Some questions that I would like the authors to answer to clarify my understanding of the paper include:
* How do the proposed nonparametric activation functions compare to other state-of-the-art activation functions, such as ReLU and sigmoid?
* What is the computational cost of the proposed approach, and how does it compare to other methods?
* How do the authors tune the hyperparameters of the proposed approach, and what is the sensitivity of the results to these hyperparameters?
* What are the potential limitations of the proposed approach, and how do the authors plan to address them in future work?