Summary
The paper presents a comprehensive study on improving the performance of gradient descent when utilizing multiple compute resources. The authors propose several approaches, including asynchronous layer-wise gradient descent, which maximizes overlap of layer-wise backpropagation with gradient synchronization. They implement these approaches using Caffe and evaluate them on both an Intel Sandy Bridge cluster and an NVIDIA DGX-1 system. The results show that delaying gradient updates by one or more iterations is the most effective means of hiding communication latency, achieving a speedup of up to 1.7x compared to synchronous gradient descent.
Decision
I decide to Accept this paper, with the primary reason being that it presents a well-motivated and well-executed study on improving the performance of gradient descent. The paper is well-written, and the authors provide a clear and detailed explanation of their approaches and results.
Supporting Arguments
The paper tackles a specific and relevant problem in the field of deep learning, and the authors provide a thorough analysis of the tradeoffs between maintaining equivalence to sequential methods and leveraging computational resources. The experimental evaluation is comprehensive, covering multiple hardware platforms and network architectures. The results are significant, demonstrating a substantial speedup without sacrificing accuracy.
Additional Feedback
To further improve the paper, I suggest that the authors provide more theoretical explanations for their empirical observations. For example, they could analyze the convergence properties of their proposed approaches and provide a more detailed discussion of the tradeoffs between speedup and accuracy. Additionally, it would be helpful to include more comparisons with other related work in the field.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how you implemented the asynchronous layer-wise gradient descent approach, and how you handled the communication latency?
2. How do you plan to extend your work to other deep learning frameworks and applications?
3. Can you provide more insights into the tradeoffs between speedup and accuracy, and how they relate to the specific problem being solved?