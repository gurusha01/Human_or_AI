The paper proposes a novel approach to learning symmetric and asymmetric encryption using neural networks, where the networks are trained to protect communications from an adversary. The authors demonstrate that neural networks can learn to use secret keys to protect information from other neural networks, and that they can discover forms of encryption and decryption without being taught specific algorithms.
I decide to reject this paper, with the main reason being that the evaluation of the word embeddings is lacking, and the paper could benefit from a more in-depth exploration of each contribution. The paper presents two main contributions: an improved inference technique for density estimation of sparse data using deep generative Gaussian models, and a method for deriving word embeddings from the model's generative parameters. However, the evaluation of the word embeddings is limited, and the paper could benefit from a more detailed analysis of the results.
The paper's approach to learning symmetric and asymmetric encryption is well-motivated, and the authors provide a clear explanation of the objectives and training procedures. The use of a "mix & transform" architecture for the neural networks is interesting, and the results show that the networks can learn to communicate securely. However, the paper could benefit from more guidance for readers on how to interpret the results, particularly in Figures 2c and 2d.
To improve the paper, I suggest that the authors provide a more detailed evaluation of the word embeddings, including an evaluation on a predictive task and more information on the construction of Table 2b. Additionally, the authors could provide more guidance for readers on how to interpret the results, and consider presenting the inference technique and the embedding technique separately to allow for a more in-depth exploration of each contribution.
I would like the authors to answer the following questions to clarify my understanding of the paper: Can you provide more information on the optimization procedure, including the functional form of the gradient used and the update of global variational parameters? How do you evaluate the word embeddings, and what metrics do you use to measure their quality? Can you provide more guidance for readers on how to interpret the results in Figures 2c and 2d?