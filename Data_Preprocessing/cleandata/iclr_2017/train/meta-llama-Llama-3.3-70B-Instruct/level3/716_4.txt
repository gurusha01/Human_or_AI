Summary
The paper proposes a novel collective communication technique, Linear Pipelining (LP), designed to reduce the communication overhead in parallel training of neural networks on multi-GPU systems. The authors claim that LP provides an O(log p) speedup over Minimal Spanning Tree (MST) collectives and up to a 2x speedup over Bidirectional Exchange (BE) collectives. The paper presents a theoretical analysis and experimental results demonstrating the effectiveness of LP in reducing communication bottlenecks and improving the convergence speed of Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD).
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper fails to provide a clear and concise explanation of the novelty of the proposed technique, making it difficult to understand how it differs from existing methods. Secondly, the comparison to existing work is limited, with only a few baseline methods (SegNet, MST, and BE) being evaluated, and no justification for why other relevant works (e.g., Deeplab v2) are not considered.
Supporting Arguments
The paper's contribution is not clearly motivated, and the authors do not provide a thorough analysis of the design choices and their impact on the performance of the proposed technique. The experimental results, while promising, are limited to a few specific scenarios and do not provide a comprehensive evaluation of the technique's effectiveness. Furthermore, the paper's discussion of related work is incomplete, and the authors do not provide a clear understanding of how their work fits into the broader context of parallel training of neural networks.
Additional Feedback
To improve the paper, the authors should provide a more detailed explanation of the novelty of the proposed technique and its differences from existing methods. They should also conduct a more comprehensive evaluation of the technique, including comparisons to other relevant works and a more thorough analysis of the design choices and their impact on performance. Additionally, the authors should provide more context and background information on the problem of parallel training of neural networks and the current state of the art in this area.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence for my assessment, I would like the authors to answer the following questions:
1. Can you provide a more detailed explanation of how the proposed Linear Pipelining technique differs from existing collective communication methods, such as Minimal Spanning Tree and Bidirectional Exchange?
2. How do you plan to address the limited comparison to existing work, and what other baseline methods do you think are relevant to include in the evaluation?
3. Can you provide more context and background information on the problem of parallel training of neural networks and the current state of the art in this area, and how your work fits into this broader context?