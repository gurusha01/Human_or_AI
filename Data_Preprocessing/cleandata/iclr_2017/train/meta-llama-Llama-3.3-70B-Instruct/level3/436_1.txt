Summary
The paper proposes a novel approach to reduce the cost of communication in parallel training of neural networks using a technique called Linear Pipelining (LP). The authors design a new collective operation that can efficiently exchange messages between GPUs, achieving significant speedups over traditional methods such as Minimum Spanning Tree (MST) and Bidirectional Exchange (BE). The proposed approach is validated on the WikiTableQuestions dataset, demonstrating comparable performance to state-of-the-art methods.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the experimental section is weak, with only a single small dataset used, and lacking ablation studies and comparisons to other models like memory networks. Secondly, the model details are convoluted and poorly explained, making replication difficult, particularly with regards to discrete operations, row selector, scalar answer, and lookup answer.
Supporting Arguments
The paper's contribution is significant, as it addresses a crucial problem in parallelizing neural network training. However, the experimental evaluation is limited, and the authors fail to provide a thorough analysis of the proposed approach's strengths and weaknesses. Additionally, the model's complexity and lack of clear explanations make it challenging to understand and replicate the results. The paper's claims are not fully supported by the empirical results, and the authors need to provide more comprehensive experiments and analysis to demonstrate the effectiveness of their approach.
Additional Feedback
To improve the paper, the authors should provide more detailed explanations of the model's components and their interactions. They should also conduct more extensive experiments on larger datasets, including ablation studies and comparisons to other state-of-the-art models. Furthermore, the authors should consider providing more insights into the theoretical analysis of the proposed approach and its limitations. Some questions that I would like the authors to answer include: How do the authors plan to address the scalability issues of their approach for huge databases? Can they provide more details on the implementation of the discrete operations and row selector? How do the authors intend to improve the interpretability of their model, given its complexity?
Questions for the Authors
1. Can you provide more details on the experimental setup and the datasets used in the evaluation?
2. How do you plan to address the scalability issues of your approach for huge databases?
3. Can you provide more insights into the theoretical analysis of the proposed approach and its limitations?
4. How do you intend to improve the interpretability of your model, given its complexity?
5. Can you provide more details on the implementation of the discrete operations and row selector?