Summary
The paper proposes a novel Linear Pipeline (LP) based collective design for multi-GPU training of neural networks, which reduces the communication overhead in Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD). The authors demonstrate that LP collectives achieve significant speedups over traditional Minimal Spanning Tree (MST) and Bidirectional Exchange (BE) based collectives, with up to 2x higher bandwidth and O(logP) speedups. The proposed approach is also shown to be invariant to the number of GPUs, making it scalable for large-scale neural network training.
Decision
I decide to Accept this paper, with the main reason being the significant contributions to the field of parallel computing and deep learning. The paper presents a well-motivated and well-placed approach in the literature, with a clear and thorough evaluation of the proposed LP collectives.
Supporting Arguments
The paper provides a thorough analysis of the communication overhead in BSP-SGD and identifies the limitations of traditional collective algorithms. The proposed LP collectives are shown to be effective in reducing the communication overhead, with significant speedups over MST and BE based collectives. The authors also provide a detailed theoretical analysis of the LP collectives, demonstrating their scalability and efficiency.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed experiments on the impact of LP collectives on the convergence properties of BSP-SGD. Additionally, it would be interesting to see a comparison of the proposed approach with other parallelization techniques, such as asynchronous SGD or decentralized SGD. The authors may also consider providing more insights into the implementation details of the LP collectives and the overlap of communications with computations.
Questions for the Authors
1. Can you provide more details on the implementation of the LP collectives, including the block size and the overlap of communications with computations?
2. How do the authors plan to address the potential issue of precision loss due to the use of float multiplications in the GradientUpdate step?
3. Can you provide more experiments on the impact of LP collectives on the convergence properties of BSP-SGD, including the effect of different batch sizes and model sizes?