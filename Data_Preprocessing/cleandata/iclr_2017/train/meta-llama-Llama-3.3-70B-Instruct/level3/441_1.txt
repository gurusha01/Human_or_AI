Summary of the Paper's Contributions
The paper proposes a novel approach to sequence generation with recurrent mixture density networks, using a physiologically plausible model of handwriting as a feature representation. The authors build on recent results in handwriting prediction and focus on generating sequences that possess the statistical and dynamic qualities of handwriting and calligraphic art forms. They introduce a modular workflow that can be exploited in various ways, such as mixing styles and learning from small datasets.
Decision: Reject
The main reasons for this decision are the lack of comparison to other recent and advanced architectures, such as stacked LSTM, and the unimpressive results on the PTB BPC task. Additionally, the paper could benefit from more citations and equation numbers to support the claims made.
Supporting Arguments
The authors propose a variable computation approach in recurrent neural networks, similar to Adaptive Computation Time, which updates only a subset of the state at each timestep. However, the gating mechanism used in this approach feels clunky and awkward, and lacks comparison to other gating mechanisms. Furthermore, the variable computation may not actually save computation in practice, despite theoretical savings, and the authors fail to report wallclock numbers to support their claim.
The evaluation of the proposed approach is also lacking, with limited comparison to other work and a weak baseline. The results are unimpressive, particularly on the PTB BPC task, which raises concerns about the effectiveness of the proposed approach.
Additional Feedback
To improve the paper, the authors should consider comparing their work to more recent and advanced architectures, such as stacked LSTM, and provide more citations and equation numbers to support their claims. Additionally, the authors should report wallclock numbers to support their claim of computational savings and provide a more thorough evaluation of their approach, including a stronger baseline and more comprehensive comparison to other work.
Questions for the Authors
1. How does the proposed approach compare to other recent and advanced architectures, such as stacked LSTM, in terms of performance and computational efficiency?
2. Can the authors provide more details on the gating mechanism used in the variable computation approach and how it compares to other gating mechanisms?
3. How do the authors plan to address the lack of computational savings in practice, despite theoretical savings, and what additional experiments or evaluations can be done to support their claim?