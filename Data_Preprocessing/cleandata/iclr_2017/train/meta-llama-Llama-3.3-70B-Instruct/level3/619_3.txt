This paper proposes a novel sequence learning approach, RL Tuner, which combines reinforcement learning (RL) with a pre-trained recurrent neural network (RNN) to refine the generated sequences. The authors demonstrate the effectiveness of this approach in music generation tasks, where the goal is to produce melodies that adhere to certain music theory rules while maintaining the information learned from the training data.
The paper claims to contribute to the sequence training and RL literature by proposing a new method for combining maximum likelihood (ML) and RL training, showing the connection between this approach and stochastic optimal control (SOC)/KL-control, and exploring the usefulness of this approach in music generation. The authors also provide a thorough evaluation of their method, including quantitative results and a user study, which demonstrates the effectiveness of RL Tuner in producing more pleasing melodies.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific question/problem of refining RNN models using RL to correct undesirable behaviors, which is a well-defined and relevant problem in the field of sequence generation.
2. The approach is well-motivated, and the authors provide a clear connection to prior work in SOC and KL-control, demonstrating a good understanding of the literature.
3. The paper supports its claims with thorough evaluations, including quantitative results and a user study, which provides strong evidence for the effectiveness of the proposed method.
However, I also have some concerns and suggestions for improvement:
* The paper could benefit from more theoretical analysis and justification for the proposed method, particularly in terms of its connection to SOC and KL-control.
* The authors could provide more details on the hyperparameter tuning process and the sensitivity of the results to different hyperparameter settings.
* The user study could be improved by collecting more ratings and using a more diverse set of participants to increase the reliability of the results.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the music theory rules were designed and selected for the reward function?
* How did you determine the optimal value of the hyperparameter c, which controls the emphasis on the music theory reward?
* Can you provide more examples of the generated melodies and discuss the trade-offs between the different methods (Q-learning, generalized Î¨-learning, and G-learning) in terms of their performance on the music theory metrics and the subjective quality of the generated melodies?