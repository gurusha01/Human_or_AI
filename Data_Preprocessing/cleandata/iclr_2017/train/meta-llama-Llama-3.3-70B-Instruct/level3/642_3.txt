The paper explores the use of customized precision hardware for large convolutional networks, achieving speed-ups of up to 7x with floating-point precision. The study predicts custom floating-point precision parameters from neural network activations, but the methodology is unclear and may have limitations. 
I decide to reject this paper for the following two key reasons: 
1. The paper lacks novelty, focusing on a useful but not particularly original study of numerical precision trade-offs at neural network test time. 
2. The results may be useful for hardware manufacturers, but their applicability is uncertain due to the absence of batch normalization in the evaluated networks.
To support my decision, I argue that the paper's approach, although well-motivated, does not provide significant new insights into the field. The use of Hessian-weighted distortion measures and entropy-constrained scalar quantization is interesting, but the paper does not demonstrate a substantial improvement over existing methods. Furthermore, the evaluation of the proposed methods is limited to a few specific networks and datasets, which raises concerns about the generalizability of the results.
To improve the paper, I suggest that the authors provide more context and comparison to existing work on network quantization and pruning. Additionally, they should consider evaluating their methods on a wider range of networks and datasets to demonstrate their applicability. It would also be helpful to clarify the methodology used to predict custom floating-point precision parameters and to discuss the potential limitations of their approach.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: 
1. How do the authors plan to address the potential limitations of their methodology, particularly with regards to the prediction of custom floating-point precision parameters?
2. Can the authors provide more details on the evaluation of their methods, including the specific networks and datasets used, and the metrics used to measure performance?
3. How do the authors think their work can be extended to other areas of deep learning, such as natural language processing or reinforcement learning?