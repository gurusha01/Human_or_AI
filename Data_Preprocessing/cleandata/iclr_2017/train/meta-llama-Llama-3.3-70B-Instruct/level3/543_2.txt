Summary
The paper proposes a novel approach to mitigate the problem of catastrophic mistakes in deep reinforcement learning (DRL) by introducing a concept called "intrinsic fear". This approach involves training a separate danger model alongside the DQN to predict the probability of catastrophe within a short number of steps, and using this prediction to penalize the Q-learning objective. The authors demonstrate the effectiveness of this approach on several toy problems, including Adventure Seeker and Cart-Pole, and provide preliminary results on the Atari game Seaquest.
Decision
I decide to reject this paper, with the main reason being that the paper's target audience may not be the mainstream ICLR audience, as they may prefer using conventional ML toolkits. Additionally, the paper does not introduce any new algorithms or improvements to make neural network design and training easier for non-experts, which may limit its appeal to the broader ICLR community.
Supporting Arguments
While the paper presents an interesting approach to addressing the problem of catastrophic mistakes in DRL, it may not be of significant interest to the ICLR audience. The paper's focus on a specific problem in DRL, rather than a more general contribution to the field of machine learning, may limit its appeal. Furthermore, the paper's methodology and presentation are satisfactory, but the lack of contribution to advancing the state of the art in machine learning may make it less competitive for acceptance at ICLR.
Additional Feedback
To improve the paper, the authors could consider providing more context on the relevance of their approach to the broader field of machine learning, and highlighting any potential connections to other areas of research. Additionally, the authors could provide more detailed analysis of the results, including a more thorough comparison to other approaches and a discussion of the limitations of their method. Finally, the authors could consider providing more concrete examples of how their approach could be applied in practice, to make the paper more accessible to a wider audience.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more context on how your approach relates to other areas of research in machine learning, such as robustness and uncertainty estimation?
* How do you plan to extend your approach to more complex domains, and what challenges do you anticipate facing in doing so?
* Can you provide more detailed analysis of the results, including a comparison to other approaches and a discussion of the limitations of your method?