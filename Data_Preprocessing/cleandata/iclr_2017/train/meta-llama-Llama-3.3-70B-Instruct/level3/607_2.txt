Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to learning reward functions for robotic manipulation tasks from visual demonstrations. The authors leverage the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from a small number of demonstrations. The method is able to identify key intermediate steps of a task and automatically discover the most discriminative features for identifying these steps. The resulting reward functions are dense and smooth, enabling a reinforcement learning agent to learn to perform the task in real-world settings.
Decision and Key Reasons
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks clarity, with inconsistent notation, unclear figures, and missing details, making it difficult to understand the model and attention mechanisms. Secondly, the evaluation is flawed, with incorrect claims of state-of-the-art results, missing qualitative results of attention, and insignificant performance improvements over model ablations.
Supporting Arguments
The paper proposes a novel approach to learning reward functions, but the claims of novelty and contribution seem exaggerated. The experiments are evaluated on two datasets, but the results are not convincing, and the paper lacks human evaluation and qualitative results of attention. The hyperparameters are inconsistent between the ablation analysis and the performance comparison, and the performance of all ablations with 8 frames is not reported. Additionally, several comments raised by reviewers have not been incorporated, which further weakens the paper.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide more details on the model and attention mechanisms, clarify the notation and figures, and report more comprehensive results, including human evaluation and qualitative results of attention. I would like the authors to answer the following questions: (1) How do the authors plan to address the issue of inconsistent hyperparameters and missing results? (2) Can the authors provide more insights into the feature selection algorithm and its impact on the results? (3) How do the authors plan to incorporate the comments raised by reviewers and improve the paper accordingly?