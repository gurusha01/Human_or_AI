Summary
The paper introduces a novel training strategy called Dense-Sparse-Dense (DSD) training, which regularizes deep neural networks by pruning and then restoring connections. The authors propose a three-step process: initial dense training, sparse training with pruning, and final dense training with re-densification. The paper claims that DSD training achieves superior optimization performance and improves the accuracy of various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, speech recognition, and caption generation.
Decision
I decide to Accept this paper with two key reasons: (1) the paper introduces a novel and well-motivated training strategy that addresses the issue of over-fitting in deep neural networks, and (2) the experimental results demonstrate significant improvements in accuracy on various tasks and datasets.
Supporting Arguments
The paper provides a clear and well-structured introduction to the DSD training strategy, including a detailed explanation of the three-step process and the motivation behind it. The authors also provide a thorough analysis of the related work, highlighting the differences between DSD and other regularization techniques such as dropout and model compression. The experimental results are extensive and demonstrate the effectiveness of DSD training on various tasks and datasets, including ImageNet, WSJ, and Flickr-8K.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the theoretical foundations of DSD training and its relationship to other optimization techniques. Additionally, it would be helpful to include more visualizations and plots to illustrate the progression of the weight distribution during the DSD training process. Finally, the authors may want to consider exploring the application of DSD training to other tasks and datasets to further demonstrate its generalizability.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How did they choose the sparsity ratio for each layer, and what is the effect of different sparsity ratios on the performance of DSD training? (2) Can they provide more details on the computational cost and memory requirements of DSD training compared to conventional training methods? (3) How do they plan to extend DSD training to other types of neural networks, such as transformers and graph neural networks?