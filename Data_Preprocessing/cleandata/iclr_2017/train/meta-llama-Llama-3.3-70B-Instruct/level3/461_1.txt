Summary of the Paper's Contributions
The paper presents a semi-supervised learning model that encourages feature invariance to stochastic perturbations of the network and/or inputs. The authors propose two models, which are evaluated using the CIFAR-10 and SVHN datasets. The models show decent gains in performance, with an additional application demonstrating some tolerance to corrupted labels. The authors also discuss similar recent work that corroborates their findings.
Decision and Reasons
Based on the review, I decide to Reject the paper. The main reasons for this decision are:
1. The paper only uses small datasets, such as CIFAR and SVHN, which may not be representative of real-world scenarios. The authors should test their models on larger datasets with 1M+ data samples and 1K-10K labeled samples.
2. The paper's use of data augmentations is restricted to translations and horizontal flips, which may not be sufficient to capture the complexity of real-world data. The authors should explore more augmentations to see how the model's performance behaves with a larger array of augmentations and fewer labels.
Supporting Arguments
The paper's approach seems simple yet effective, but the lack of experimentation on larger datasets and more comprehensive data augmentations raises concerns about the model's generalizability. The authors should provide more extensive experiments to demonstrate the robustness of their approach.
Additional Feedback
To improve the paper, the authors should:
1. Test their models on larger datasets, such as ImageNet, to demonstrate the scalability of their approach.
2. Explore more comprehensive data augmentations, including rotations, scaling, and color jittering, to capture the complexity of real-world data.
3. Provide more detailed analysis of the model's performance, including visualizations of the learned features and attention mechanisms.
Questions for the Authors
1. How do the authors plan to address the issue of scalability, given the limited size of the datasets used in the paper?
2. Can the authors provide more insights into the choice of data augmentations used in the paper, and how they plan to extend this to more comprehensive augmentations?
3. How do the authors plan to evaluate the model's performance on more complex datasets, such as ImageNet, and what metrics will they use to measure success?