Summary
The paper proposes a novel Linear Pipeline (LP) based collective design for multi-GPU training of neural networks, which achieves significant speedups over existing collective algorithms such as Minimum Spanning Tree (MST) and Bidirectional Exchange (BE). The authors demonstrate the effectiveness of their approach through theoretical analysis and experimental results, showing up to 2x higher bandwidth and 2.3x to 360.55x speedups over Open MPI alternatives.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks a thorough comparison with existing systems, including additional datasets such as Pascal or COCO, which makes it difficult to evaluate the proposed system's performance comprehensively. Secondly, the paper needs to demonstrate the effects of each component and sizing/tuning to provide a more robust picture of the system's performance as a practical implementation.
Supporting Arguments
While the paper presents an interesting approach to reducing communication overhead in parallel training of neural networks, it falls short in providing a comprehensive evaluation of the proposed system. The authors only compare their approach with MST and BE, which may not be the most competitive baselines. Additionally, the paper lacks an in-depth analysis of the trade-offs between communication overhead, computation time, and model size, which is crucial for understanding the practical implications of the proposed approach.
Additional Feedback
To improve the paper, I suggest the authors provide a more thorough comparison with existing systems, including additional datasets and baselines. They should also conduct a more detailed analysis of the trade-offs between communication overhead, computation time, and model size. Furthermore, the authors should demonstrate the effects of each component and sizing/tuning to provide a more robust picture of the system's performance as a practical implementation.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. How do the authors plan to extend their approach to support more complex neural network architectures and larger models?
2. Can the authors provide more details on the implementation of their LP-based collective design, including any optimizations or techniques used to improve performance?
3. How do the authors plan to address the potential scalability limitations of their approach, particularly when dealing with a large number of GPUs?