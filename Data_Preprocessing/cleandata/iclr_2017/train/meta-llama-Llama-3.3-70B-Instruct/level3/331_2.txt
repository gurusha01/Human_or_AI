Summary
The paper proposes a novel approach to similarity learning using Attentive Recurrent Comparators (ARCs), which learn to estimate the similarity of a set of objects by cycling through them and making observations. The authors demonstrate the effectiveness of ARCs in various visual tasks, including one-shot learning on the Omniglot dataset, where they achieve state-of-the-art performance surpassing human performance.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles an important problem in artificial intelligence, namely similarity learning, and proposes a novel and well-motivated approach; (2) the experimental results demonstrate the effectiveness of ARCs in various tasks, including one-shot learning, and show significant improvements over existing methods.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem of similarity learning and the limitations of existing approaches. The authors propose a novel approach using ARCs, which is well-motivated and grounded in the literature. The experimental results are thorough and demonstrate the effectiveness of ARCs in various tasks, including one-shot learning. The paper also provides a detailed analysis of the results and discusses the implications of the findings.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of ARCs compared to existing methods. Additionally, it would be interesting to see more experiments on other datasets and tasks to demonstrate the generalizability of ARCs. Furthermore, the authors could provide more insights into the interpretability of ARCs and how they can be used to gain a better understanding of the underlying data.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do ARCs handle cases where the objects being compared have different sizes or resolutions? (2) Can ARCs be used for multi-modal learning, where the objects being compared are from different modalities (e.g., images and text)? (3) How do the authors plan to address the potential computational cost of ARCs in large-scale applications?