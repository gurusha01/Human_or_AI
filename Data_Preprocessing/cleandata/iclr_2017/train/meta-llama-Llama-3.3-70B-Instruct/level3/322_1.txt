Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and is designed to reduce the computational complexity of Batch Normalization (BN) in LSTMs. The authors derive the gradients of the Normalized LSTM and investigate its impact on the gradient flow. They also propose a scheme to initialize the weight matrices and demonstrate the effectiveness of the approach on character-level language modeling and image generative modeling tasks.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper proposes a novel and well-motivated approach to reduce the computational complexity of BN in LSTMs, and (2) the experimental results demonstrate the effectiveness of the approach on two different tasks.
Supporting Arguments
The paper is well-written, and the authors provide a clear and concise explanation of the proposed approach. The derivation of the gradients of the Normalized LSTM is thorough, and the analysis of the gradient flow is insightful. The experimental results are convincing, and the comparison with other state-of-the-art approaches is fair. The paper also provides a good discussion of the limitations of the approach and potential future directions.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational complexity of the proposed approach and compare it with other normalization techniques. Additionally, it would be interesting to see more experiments on other tasks and datasets to further demonstrate the effectiveness of the approach. The authors may also want to consider discussing the potential applications of the proposed approach in other areas, such as speech recognition or natural language processing.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the initialization scheme for the weight matrices and how it affects the learning process?
2. How do you plan to extend the proposed approach to other types of recurrent neural networks, such as Gated Recurrent Units (GRUs) or Bidirectional LSTMs?
3. Can you provide more insights on the relationship between the proposed approach and other normalization techniques, such as Layer Normalization or Weight Normalization?