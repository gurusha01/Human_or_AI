Summary
The paper proposes a multi-view Bayesian non-parametric algorithm for learning multi-sense word embeddings. The approach leverages multilingual distributional information from parallel corpora in multiple languages to improve sense embeddings. The algorithm uses a principled approach to learn a variable number of senses per word in a data-driven manner. The authors demonstrate the effectiveness of their approach through experiments on word sense induction and contextual word similarity tasks.
Decision
I decide to accept this paper with the following key reasons:
1. The paper tackles a specific and interesting question in the field of natural language processing, namely, learning multi-sense word embeddings using multilingual distributional information.
2. The approach is well-motivated and grounded in the literature, with a clear explanation of the limitations of existing approaches and how the proposed algorithm addresses these limitations.
Supporting Arguments
The paper provides a thorough review of related work, highlighting the limitations of existing approaches and the benefits of using multilingual distributional information. The authors also provide a clear and detailed explanation of their algorithm, including the modeling assumptions and the optimization procedure. The experimental results demonstrate the effectiveness of the approach, with significant improvements over monolingual and bilingual training. The paper also provides a qualitative illustration of the benefits of multilingual training, showing how the sense vectors are more clearly clustered and separated.
Additional Feedback
To further improve the paper, I suggest the authors consider the following:
* Provide more analysis on the effect of language family distance on the performance of the algorithm.
* Investigate the use of other optimization techniques, such as stochastic gradient descent, to improve the efficiency of the algorithm.
* Consider applying the algorithm to other languages, such as Chinese, to demonstrate its applicability beyond English.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the hyper-prior concentration Î± is chosen and how it affects the performance of the algorithm?
* How do you handle out-of-vocabulary words in the multilingual setting, where the word may not be present in all languages?
* Can you provide more insights on the qualitative illustration of the sense vectors, such as how the senses are separated and clustered, and how this relates to the quantitative results?