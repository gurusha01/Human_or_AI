Summary of the Paper
The paper presents a novel approach to sampling from generative autoencoders, which are trained to softly enforce a prior on the latent distribution learned by the inference model. The authors propose a Markov chain Monte Carlo (MCMC) sampling process that allows sampling from the learned latent distribution, rather than the prior. This approach is shown to improve the quality of generated samples, especially when the learned latent distribution is far from the prior.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The paper tackles a specific problem in generative autoencoders and provides a clear and concise solution.
Supporting Arguments
The paper is well-structured and easy to follow, with a clear introduction to the problem and the proposed solution. The authors provide a thorough background on generative autoencoders and the motivation behind their approach. The experimental results are convincing, showing that the MCMC sampling process improves the quality of generated samples.
Additional Feedback
To further improve the paper, I suggest that the authors consider the following minor improvements:
* Bold the best-in-category results in tables to make them more visible.
* Discuss the ramp-up of w(t) in the main paper, rather than relegating it to the supplementary material.
* Include state-of-the-art results for the fully-supervised case in tables to provide a more comprehensive comparison.
Questions for the Authors
I would like the authors to clarify the following points:
* Can you provide more insight into why the MCMC sampling process is able to improve the quality of generated samples, especially when the learned latent distribution is far from the prior?
* How does the choice of prior distribution affect the performance of the MCMC sampling process?
* Are there any plans to apply this approach to other types of generative models, such as GANs or flow-based models?