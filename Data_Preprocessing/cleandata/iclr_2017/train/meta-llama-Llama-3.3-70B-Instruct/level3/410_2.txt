Summary
The paper proposes a simple baseline for detecting misclassified or out-of-distribution examples by utilizing statistics of softmax outputs. The authors demonstrate the effectiveness of this baseline across various tasks in computer vision, natural language processing, and automatic speech recognition. They also introduce an abnormality module that can outperform the baseline in some cases, highlighting the potential for future research in this area.
Decision
I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and important problem in machine learning, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper addresses a crucial issue in machine learning, where classifiers often fail silently by providing high-confidence predictions while being incorrect. The authors provide a simple yet effective baseline for detecting such errors, which is a significant contribution to the field. The experimental results demonstrate the effectiveness of the baseline across various tasks and datasets, making a strong case for its adoption. Additionally, the introduction of the abnormality module provides a promising direction for future research, highlighting the potential for improving error detection and out-of-distribution detection.
Additional Feedback
To improve the paper, I suggest that the authors elaborate on ignoring the logit of the blank symbol in the softmax output, as this seems to be a crucial step in their approach. Furthermore, the description of the auxiliary decoder setup in Section 4 could be more detailed to facilitate better understanding. It would also be beneficial to test the performance of the proposed method in more confusable settings, such as speech recognition with different languages. Recent work on performance monitoring and accuracy prediction in speech recognition, such as studies by Ogawa, Hermansky, and Variani, could be relevant to this research and worth exploring.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into why ignoring the logit of the blank symbol is necessary for effective detection?
2. How do you plan to extend the abnormality module to more complex tasks and datasets?
3. Have you considered exploring other metrics for evaluating the confidence model, such as those used in recent studies on uncertainty estimation?