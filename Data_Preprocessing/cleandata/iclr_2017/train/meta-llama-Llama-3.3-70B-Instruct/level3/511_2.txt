Summary
The paper proposes a multi-view approach to learning acoustic word embeddings, where both acoustic sequences and their corresponding character sequences are jointly embedded into a common space. The authors use deep bidirectional LSTM embedding models and multi-view contrastive losses to learn the embeddings. The approach is evaluated on several tasks, including acoustic word discrimination, cross-view word discrimination, and word similarity.
Decision
I decide to reject this paper, with the main reason being that it does not align well with the conference focus on learned representations. Although the paper presents an interesting approach to learning acoustic word embeddings, it does not involve learned representations in the context of the conference.
Supporting Arguments
The paper's approach to learning acoustic word embeddings is well-motivated, and the authors provide a clear explanation of their methodology. However, the paper lacks a direct correlation between the PDE residual and the well-performing policy, which is a crucial aspect of the conference. Additionally, the 2D toy examples used in the paper are inadequate and do not demonstrate scalability to more complex problems.
Additional Feedback
To improve the paper, the authors could consider providing more detailed explanations of their methodology and results. Additionally, they could explore more complex examples and demonstrate the scalability of their approach. The authors could also benefit from proper citation of existing work on this topic and addressing the typos and errors in the paper, such as "Range-Kutta" instead of "Runge-Kutta".
Questions for the Authors
1. Can you provide more details on how the multi-view approach improves the performance of acoustic word embeddings?
2. How do you plan to address the lack of scalability of your approach to more complex problems?
3. Can you provide more information on how the cost-sensitive loss improves the correlation between embedding distances and orthographic edit distances?