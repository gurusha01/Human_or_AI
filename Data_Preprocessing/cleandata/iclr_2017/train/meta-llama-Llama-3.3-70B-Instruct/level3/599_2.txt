Summary
The paper proposes a method to accelerate gradient descent in deep learning by utilizing multiple compute resources. The authors design and implement several approaches, including synchronous and asynchronous gradient descent, and evaluate their performance on a large-scale CPU cluster and an NVIDIA DGX-1 multi-GPU system. The results show that delaying gradient updates by one or more iterations can effectively hide communication latency and achieve a speedup of up to 1.7x compared to synchronous gradient descent.
Decision
I decide to reject this paper, with the main reasons being the limited novelty of the proposed approaches and the lack of generalizability of the results to other domains.
Supporting Arguments
While the paper presents a thorough evaluation of the proposed approaches, the ideas of synchronous and asynchronous gradient descent are not new and have been explored in previous works. The authors' contribution lies in the implementation and evaluation of these approaches on specific hardware architectures, but the results may not be generalizable to other domains or hardware setups. Additionally, the paper focuses on a specific problem (accelerating gradient descent) and may not have a broader impact on the field of deep learning.
Additional Feedback
To improve the paper, the authors could consider exploring more novel approaches to accelerating gradient descent, such as using new optimization algorithms or leveraging emerging hardware architectures like TPUs or FPGAs. They could also provide more detailed analysis of the tradeoffs between different approaches and their impact on the accuracy and convergence of the models. Furthermore, the authors could consider evaluating their approaches on more diverse datasets and models to demonstrate the generalizability of their results.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the limited novelty of their approaches and demonstrate the significance of their contributions to the field of deep learning?
2. Can the authors provide more details on the implementation of their approaches and the specific optimizations used to achieve the reported speedups?
3. How do the authors plan to extend their work to other domains and hardware architectures, and what are the potential challenges and limitations they anticipate?