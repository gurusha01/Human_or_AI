Summary
The paper proposes a novel approach to pruning neural networks, called NoiseOut, which identifies and removes highly correlated neuron pairs. Additionally, the authors introduce a method to increase neuron correlation by adding auxiliary noise target outputs. The approach is tested on various networks and datasets, demonstrating significant compression rates without loss of accuracy.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the value of the second idea, NoiseOut, is unclear and may simply add a regularizing effect to the network, similar to Dropout or L2 regularization. Secondly, the experiments lack comparisons to other methods of reducing network capacity, making it impossible to conclude that NoiseOut does anything but provide similar regularization.
Supporting Arguments
The paper proposes two ideas, but the first idea of pruning networks by identifying highly correlated neuron pairs seems to work well, although it is unclear if it has been tried before. The second idea, NoiseOut, is not well motivated, and its effectiveness is not clearly demonstrated. The experiments are also lacking, as they do not compare the proposed approach to other pruning methods, making it difficult to evaluate the effectiveness of the proposed approach.
Additional Feedback
To improve the paper, the authors should provide more clarity on the stop criteria used in the experiments, specifically whether the accuracy threshold is based on train or test accuracy. Additionally, the notation used in the paper, such as lowercase rho, should be clearly defined to avoid confusion. The authors should also provide comparisons to other pruning methods to demonstrate the effectiveness of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on how the NoiseOut method is different from existing regularization techniques, such as Dropout and L2 regularization?
2. How do you ensure that the correlation between neurons is not simply a result of the network's architecture or the dataset used?
3. Can you provide more experiments comparing the proposed approach to other pruning methods, such as Optimal Brain Damage and Optimal Brain Surgeon?