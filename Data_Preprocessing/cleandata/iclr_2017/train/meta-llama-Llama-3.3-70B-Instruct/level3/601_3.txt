Summary
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) training, which regularizes deep neural networks by pruning and then restoring connections. The authors claim that DSD training can improve the performance of various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, speech recognition, and caption generation.
Decision
I decide to accept this paper, with two key reasons: (1) the proposed DSD training strategy is well-motivated and grounded in the literature, and (2) the experimental results demonstrate significant improvements in performance over baseline models.
Supporting Arguments
The paper provides a clear and concise explanation of the DSD training strategy, which involves three steps: dense, sparse, and re-dense. The authors also provide a thorough analysis of the related work, highlighting the differences between DSD and other regularization techniques such as dropout and distillation. The experimental results are extensive and demonstrate the effectiveness of DSD training on various tasks and datasets.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational efficiency of the DSD training strategy, including the time and memory requirements. Additionally, the authors could provide more analysis on the robustness of the DSD training strategy to different hyperparameters and initialization methods.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the pruning strategy used in the sparse step, including the criteria for selecting the weights to be pruned?
2. How do you determine the optimal sparsity ratio for each layer, and what is the sensitivity of the results to this hyperparameter?
3. Can you provide more analysis on the effect of DSD training on the interpretability of the neural networks, including the feature importance and attention mechanisms?