Summary of the Paper's Contributions
The paper presents a significant contribution to the field of reinforcement learning by extending recent work on video frame prediction to enable joint prediction of future states and rewards using a single latent representation. The authors propose a network architecture and training procedure for joint state and reward prediction, and evaluate their approach in the Arcade Learning Environment (ALE) on five Atari games. The results demonstrate accurate cumulative reward prediction up to roughly 200 frames, with best results achieved in Freeway and Q*bert.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Reject the paper with the option for resubmission. The two key reasons for this decision are: (1) the paper lacks justification for the specific dataset used, which may limit the generalizability of the results, and (2) the paper bears a strong resemblance to previous work by Zaremba et al. (2016), which is not adequately addressed.
Supporting Arguments
The paper provides a thorough analysis of different methods for curriculum learning, but the dataset used is specific and lacks justification. The authors should provide more context on why they chose the ALE environment and the specific games used in the evaluation. Additionally, the paper does not provide a clear comparison to previous work, particularly the work by Zaremba et al. (2016), which presents a similar approach to learning system dynamics and reward functions.
The results presented in the paper are impressive, but the evaluation is limited to a specific environment and games. The authors should consider evaluating their approach in other environments and tasks to demonstrate its generalizability. Furthermore, the paper is overly long and could be condensed to focus on the key contributions and results.
Additional Feedback and Questions
To improve the paper, the authors should provide more context on the dataset used and justify their choice of environment and games. They should also provide a clear comparison to previous work, particularly the work by Zaremba et al. (2016), and discuss the similarities and differences between their approach and previous work.
Some questions that I would like the authors to address in a revised version of the paper are:
* What is the rationale behind choosing the ALE environment and the specific games used in the evaluation?
* How does the proposed approach differ from previous work, particularly the work by Zaremba et al. (2016)?
* Can the authors provide more results on the generalizability of their approach to other environments and tasks?
* How can the authors condense the paper to focus on the key contributions and results, while maintaining the necessary details and context?