Summary of the Paper's Contributions
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors empirically study its performance on the CIFAR-10 and CIFAR-100 datasets and demonstrate new state-of-the-art results. They also show its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.
Decision and Key Reasons
I decide to accept this paper with some reservations. The key reasons for this choice are that the proposed heuristic is simple and effective, and the authors have revised the paper, adding new experiments and improving the contents. However, I find the demonstration of the heuristic limited and lacking theoretical support for generalization.
Supporting Arguments
The paper's contributions are significant, and the authors have made a substantial effort to improve the paper based on the reviewer's feedback. The experimental results are impressive, and the authors have demonstrated the effectiveness of their approach on multiple datasets. However, I still have some concerns about the limited demonstration of the heuristic and the lack of theoretical support for generalization. The authors' experience with DNNs for NLP contradicts some observations in the paper, particularly regarding the performance of ADAM-type techniques versus simple SGD with momentum.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide more theoretical support for their approach and demonstrate its effectiveness on a wider range of datasets. I would like the authors to answer the following questions: (1) Can you provide more insights into why the proposed heuristic works well on some datasets but not others? (2) How do you plan to address the issue of hyperparameter tuning, which is crucial for the performance of the proposed approach? (3) Can you provide more comparisons with other state-of-the-art optimization techniques, such as ADAM and AdaDelta? 
Assessment of the Paper's Quality
The paper tackles the specific question of improving the anytime performance of SGD for training deep neural networks. The approach is well-motivated, and the authors have placed it well in the literature. However, the paper's claims are not fully supported by theoretical analysis, and the results are mainly empirical. The paper's scientific rigor is good, but it could be improved with more theoretical support and a wider range of experiments. Overall, I believe that the paper makes a significant contribution to the field and deserves to be accepted.