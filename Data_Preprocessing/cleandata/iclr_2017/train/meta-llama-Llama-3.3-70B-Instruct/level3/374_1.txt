Summary of the Paper's Contributions
The paper proposes a novel Recurrent Neural Network (RNN) architecture, called the Doubly Orthogonal Recurrent Neural Network (DORNN), which addresses the vanishing and exploding gradients problem in traditional RNNs. The DORNN uses a multiplicative update rule, combining a time-invariant orthogonal transformation with an input-modulated orthogonal transformation, to preserve the norm of the hidden state and gradients. The authors also introduce a rotation plane parameterization to represent the orthogonal matrices, allowing for efficient computation and optimization.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons: (1) the proposed DORNN architecture is well-motivated and grounded in theoretical foundations, providing a novel solution to the vanishing and exploding gradients problem; and (2) the experimental results demonstrate the effectiveness of the DORNN in learning long-term dependencies, with state-of-the-art performance on a simplified memory copy task.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of vanishing and exploding gradients in RNNs, and motivates the need for a new architecture. The proposed DORNN architecture is well-designed, with a multiplicative update rule that preserves the norm of the hidden state and gradients. The rotation plane parameterization is also a key contribution, allowing for efficient computation and optimization of the orthogonal matrices. The experimental results are convincing, demonstrating the ability of the DORNN to learn long-term dependencies and outperforming baseline models.
Additional Feedback and Questions
To further improve the paper, I suggest the authors provide more detailed analysis of the computational complexity and memory requirements of the DORNN architecture, as well as its potential applications to more complex tasks and datasets. I also have some questions regarding the experimental setup: (1) How did the authors choose the hyperparameters for the DORNN, and what is the sensitivity of the results to these hyperparameters? (2) Can the authors provide more insight into the training dynamics of the DORNN, and how it compares to other RNN architectures? (3) Are there any plans to extend the DORNN to more complex tasks, such as natural language processing or image classification?