The paper proposes a novel approach to learning binary autoencoders by formulating it as a biconvex optimization problem. The authors derive an optimal decoding function, which is a single layer of logistic sigmoid artificial neurons, and an efficient algorithm to learn the autoencoder. The approach is based on minimizing the worst-case reconstruction loss, and it does not rely on any explicit model assumptions. The paper also provides a theoretical analysis of the approach and demonstrates its competitiveness with standard autoencoders trained with backpropagation through experiments.
I decide to reject this paper, with the main reason being that the significance of the results compared to other methods, such as standard autoencoders, is not clear, particularly on large datasets. While the paper presents some promising experimental results, the comparison to other methods is limited, and it is not clear whether the proposed approach offers any significant advantages over existing methods.
The paper tackles the specific question of learning binary autoencoders, and the approach is well-motivated by the desire to avoid explicit model assumptions and non-convex optimization. However, the paper could benefit from a more thorough comparison to other methods and a clearer discussion of the advantages and limitations of the proposed approach.
To improve the paper, I would suggest the following:
* Provide a more detailed comparison to other methods, including standard autoencoders and other approaches to learning binary autoencoders.
* Discuss the advantages and limitations of the proposed approach more clearly, including any potential drawbacks or limitations.
* Consider providing more theoretical analysis or guarantees for the proposed approach, such as bounds on the reconstruction loss or convergence rates for the algorithm.
* Clarify the relationship between the proposed approach and other methods, such as PCA and other dimensionality reduction techniques.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* How does the proposed approach compare to other methods in terms of computational complexity and scalability?
* Can the authors provide more insight into the nature of the optimal decoding function and why it takes the form of a single layer of logistic sigmoid artificial neurons?
* How does the proposed approach handle missing or noisy data, and are there any plans to extend the approach to handle such cases?