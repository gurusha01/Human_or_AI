Summary
The paper proposes a framework for multitask deep reinforcement learning guided by policy sketches. The approach associates each subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating that it outperforms standard baselines and naturally induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.
Decision
I decide to reject this paper, with the main reason being that the approach, although well-motivated and well-placed in the literature, is considered empirically unproven. The authors evaluate their approach on only two environments, and the results, while promising, are not sufficient to fully support the claims made in the paper.
Supporting Arguments
The paper proposes a novel approach to multitask reinforcement learning, and the authors provide a clear and well-structured presentation of their method. However, the evaluation of the approach is limited to two environments, and the results, while promising, do not provide sufficient evidence to fully support the claims made in the paper. Additionally, the authors do not provide a thorough comparison with existing methods, which makes it difficult to assess the significance of their contributions.
Additional Feedback
To improve the paper, the authors should provide a more comprehensive evaluation of their approach, including experiments on a wider range of environments and a more thorough comparison with existing methods. Additionally, the authors should provide more details on the implementation of their approach, including the architecture of the subpolicies and the critic, and the hyperparameters used in the experiments.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the architecture of the subpolicies and the critic, and the hyperparameters used in the experiments?
2. How do you plan to extend your approach to more complex environments, and what are the potential challenges and limitations of your method?
3. Can you provide a more thorough comparison with existing methods, including a discussion of the advantages and disadvantages of your approach?