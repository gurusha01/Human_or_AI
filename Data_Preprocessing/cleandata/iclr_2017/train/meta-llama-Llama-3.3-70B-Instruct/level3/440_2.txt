This paper proposes a novel approach to policy search in stochastic dynamical systems using Bayesian neural networks (BNNs) and minimizing alpha-divergence with alpha=0.5. The authors claim that their method is the first model-based system to solve a 20-year-old benchmark problem, and they demonstrate promising results in real-world scenarios, including the control of a gas turbine and an industrial benchmark.
The specific question tackled by the paper is how to learn a policy in a stochastic dynamical system using a model-based approach. The approach is well-motivated, as it addresses the limitations of traditional methods in capturing complex stochastic patterns in the transition dynamics. The paper is technically sound, and the authors provide a clear explanation of their methodology and experimental results.
However, I have some concerns that lead me to reject this paper. Firstly, the writing needs improvement, particularly in sections 2-3, where the notation and terminology are dense and hard to follow. Additionally, the authors do not provide a clear justification for the use of simulated data instead of original data in section 4.2.1, which raises questions about the validity of their results.
Furthermore, the authors do not comment on the applicability of stochastic gradient MCMC for their setup, which is a similar approach that has shown promising results in previous papers. The computational complexity of the different approaches is also not clearly discussed, which makes it difficult to understand the practical implications of the proposed method.
To improve the paper, I suggest that the authors clarify their notation and terminology, provide a clear justification for the use of simulated data, and discuss the applicability of stochastic gradient MCMC and the computational complexity of their approach. Additionally, I would like the authors to evaluate their method on the problem using PSO-P, which would provide a more comprehensive understanding of their approach.
Some questions I would like the authors to answer are: (1) Can you provide a clear explanation of why you chose to use simulated data instead of original data in section 4.2.1? (2) How does your approach compare to stochastic gradient MCMC in terms of computational complexity and performance? (3) Can you provide more details on the hyperparameters used in your experiments and how they were tuned? (4) How do you plan to address the issue of exploration in your future work, and what potential approaches do you think could be effective?