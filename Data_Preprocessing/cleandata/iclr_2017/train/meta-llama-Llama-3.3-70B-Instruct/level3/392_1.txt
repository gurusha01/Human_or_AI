This paper proposes a novel loss framework for language modeling, which leverages the metric space of word embeddings to generate a more informed data distribution. The authors demonstrate that training against this distribution improves learning, and that reusing the input embedding matrix in the output projection layer is an optimal choice in this framework. The paper is well-written, and the results are impressive, achieving state-of-the-art performance on the Penn Treebank corpus.
I decide to accept this paper, with the primary reason being the significant improvement in performance achieved by the proposed framework. The authors provide a thorough analysis of the mechanism behind their approach, and the empirical results support their claims.
The key strengths of the paper are the novelty of the proposed loss framework, the thorough theoretical analysis, and the impressive empirical results. The authors also provide a clear and well-organized presentation of their work, making it easy to follow and understand.
One potential weakness of the paper is the lack of comparison with other recent works in the field. While the authors mention some related works, a more comprehensive comparison would strengthen the paper.
To improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process, as well as more analysis on the effect of the temperature parameter on the results. Additionally, it would be interesting to see more qualitative results, such as examples of generated text, to better understand the improvements achieved by the proposed framework.
I would like to ask the authors to clarify the following points:
* How did the authors choose the value of the temperature parameter, and what is the effect of varying this parameter on the results?
* Can the authors provide more details on the computational resources required to train the models, and how this compares to other state-of-the-art models?
* How do the authors plan to extend this work to other tasks, such as neural machine translation or text summarization, and what challenges do they anticipate in doing so?