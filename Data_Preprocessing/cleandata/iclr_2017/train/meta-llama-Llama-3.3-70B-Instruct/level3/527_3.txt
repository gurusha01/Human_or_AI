Summary of the Paper's Contributions
The paper introduces a novel recurrent neural network architecture called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The mLSTM model is designed to have flexible input-dependent transitions, allowing it to recover quickly from surprising inputs and making it more expressive for autoregressive density estimation. The authors demonstrate the effectiveness of mLSTM on various character-level language modeling tasks, showing improvements over standard LSTM and its deep variants, especially on more complex tasks.
Decision and Key Reasons
Based on the review, I decide to Reject the paper, with two key reasons:
1. Lack of parameter numbers in comparisons: The paper does not provide the number of parameters in the compared models, making it difficult to assess the efficiency and scalability of the proposed mLSTM architecture.
2. mLSTM's performance is not consistently better than other models: Although mLSTM shows improvements on some tasks, it is outperformed by other architectures on others, which raises questions about its overall effectiveness and robustness.
Supporting Arguments
The paper presents interesting experiments on big data, exploring the applications and potential of the mLSTM model. However, the absence of parameter numbers in comparisons between different models makes it challenging to evaluate the proposed architecture's efficiency and scalability. Additionally, the fact that mLSTM is less performant than some other models in most tasks suggests that its advantages may be task-specific and not universally applicable.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide more detailed information about the parameter numbers and computational resources used in the experiments. Additionally, it would be helpful to investigate the reasons behind mLSTM's inconsistent performance across different tasks and to explore ways to improve its robustness.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details about the hyperparameter tuning process and the sensitivity of the results to different hyperparameter settings?
* How do you plan to extend the mLSTM architecture to word-level language modeling and other discrete multinomial generative modeling tasks?
* Can you discuss the potential applications and limitations of the dynamic evaluation approach used in the paper?