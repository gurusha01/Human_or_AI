Summary of the Paper's Contributions
The paper proposes a novel framework for multitask deep reinforcement learning guided by policy sketches. The approach associates each subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. This optimization is accomplished via a simple decoupled actor-critic training objective that facilitates learning common behaviors from dissimilar reward functions. The paper demonstrates the effectiveness of this approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, both featuring extremely sparse rewards.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Accept the paper. The two key reasons for this choice are:
1. The paper tackles a specific and important question in the field of reinforcement learning, namely, how to learn hierarchical policies from minimal supervision.
2. The approach proposed in the paper is well-motivated, and the experimental results demonstrate its effectiveness in learning multitask policies and generalizing to new tasks.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of multitask reinforcement learning and the concept of policy sketches. The approach proposed in the paper is novel and builds upon existing work in hierarchical reinforcement learning. The experimental results demonstrate the effectiveness of the approach in learning multitask policies and generalizing to new tasks. The paper also provides a thorough analysis of the importance of various components of the training procedure, including the decoupled critic and the curriculum.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the subpolicies and the critic, including the specific architectures used and the hyperparameters chosen. Additionally, it would be helpful to include more visualizations of the learned policies and the environments used in the experiments.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the subpolicies are initialized and updated during training?
2. How do you handle the case where a subpolicy is not applicable to a particular task or environment?
3. Can you provide more information on the curriculum learning scheme used in the paper, including the specific criteria used to determine when to increment the maximum sketch length?