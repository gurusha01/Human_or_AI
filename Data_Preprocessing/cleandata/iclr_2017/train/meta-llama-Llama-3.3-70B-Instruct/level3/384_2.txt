This paper proposes a novel approach to reinforcement learning by jointly predicting video frames and cumulative rewards in high-dimensional visual state spaces. The authors extend previous work on video frame prediction to enable reward prediction, achieving significant performance improvements over a baseline model. The paper's strengths include its innovative approach, thorough evaluation, and insightful analysis of results.
The model's performance is impressive, with accurate cumulative reward prediction up to 200 frames in five different Atari games. The authors provide a detailed error analysis, identifying stochasticity in state transitions as a likely cause of relatively low performance in long-term cumulative reward prediction. The paper also discusses potential applications of the model, including integration with model-free approaches for effective interactive learning and planning in complex environments.
However, there are some areas that require further improvement. The paper lacks a quantitative analysis of the impact of attention in the match-LSTM and answer pointer layer, which could provide valuable insights into the model's performance. Additionally, the authors could provide more insights into the performance gap between boundary and sequence models.
To further improve the paper, I suggest the following:
1. Conduct additional analyses to compare the model's performance with and without attention, and evaluate its performance on questions requiring different types of reasoning.
2. Provide more details on the design choices, such as the repetition of activations across dimensions and the exclusion of Bi-Ans-Ptr from the ensemble model.
3. Include a more detailed discussion of the comparison to the DCR model, highlighting the strengths and weaknesses of each approach.
Overall, I believe that this paper has the potential to make a significant contribution to the field of reinforcement learning, and with some revisions, it could be even stronger.
Decision: Accept
Reasons:
1. The paper proposes a novel and innovative approach to reinforcement learning, which has the potential to make a significant impact in the field.
2. The evaluation is thorough, and the results are impressive, demonstrating the model's ability to accurately predict cumulative rewards in high-dimensional visual state spaces.
Additional feedback:
* Consider adding more visualizations to illustrate the model's performance and error analysis.
* Provide more details on the training procedure, including the optimization algorithm and hyperparameter tuning.
* Discuss potential limitations of the approach and avenues for future research. 
Questions for the authors:
* Can you provide more insights into the performance gap between boundary and sequence models?
* How do you plan to address the limitations of the approach, such as the need for a large amount of training data?
* Can you discuss potential applications of the model in real-world scenarios, such as robotics or autonomous driving?