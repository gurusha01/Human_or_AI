The paper proposes a method for automatic dialogue evaluation, which is a crucial problem in dialogue research. The authors introduce an evaluation model, ADEM, that learns to predict human-like scores to input responses using a new dataset of human response scores. The model is trained in a semi-supervised manner using a hierarchical recurrent neural network (RNN) to predict human scores. The results show that ADEM scores correlate significantly with human judgements at both the utterance and system-level, and can generalize to evaluating new models unseen during training.
I decide to reject this paper, with the main reason being that the paper lacks clarity and is hard to follow due to the introduction of multiple methods and concepts. The authors propose a new evaluation model, but the originality of the proposed contributions is unclear, with similarities to existing work. Additionally, the significance of the proposed methods is hard to evaluate due to the simplicity and low dimensionality of the tested distributions, and the lack of comparison with simpler variational approaches.
The paper attempts to address the problem of automatic dialogue evaluation, which is a well-motivated and important research question. However, the approach is not well-placed in the literature, and the authors do not provide a clear and concise explanation of the proposed method. The results are promising, but the lack of clarity and the similarity to existing work make it difficult to assess the significance of the contributions.
To improve the paper, I would suggest that the authors provide a clearer and more concise explanation of the proposed method, and clarify the originality of the contributions. Additionally, the authors should provide more detailed comparisons with existing work and simpler variational approaches, and evaluate the proposed method on more complex and high-dimensional distributions.
Some questions I would like the authors to answer to clarify my understanding of the paper are: (1) How does the proposed method differ from existing work on automatic dialogue evaluation? (2) Can the authors provide more detailed explanations of the hierarchical RNN encoder and the VHRED model? (3) How do the authors plan to address the issue of human biases in the evaluation model, and what are the implications of this bias on the results? (4) Can the authors provide more detailed comparisons with simpler variational approaches, and evaluate the proposed method on more complex and high-dimensional distributions?