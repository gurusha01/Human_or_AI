Summary of the Paper's Contributions
The paper proposes a novel approach to generating image-like containers for steganography applications using Deep Convolutional Generative Adversarial Networks (DCGAN). The authors introduce a Steganographic Generative Adversarial Networks (SGAN) model, which consists of a generator network, a discriminator network, and a steganalyzer network. The SGAN model is trained to produce realistic images that can deceive a steganalyzer, and the authors demonstrate that their approach can decrease the detection accuracy of a steganalysis method almost to that of a random classifier.
Decision
I decide to reject this paper, with the main reasons being the lack of empirical studies and baseline results. The paper uses tiny training datasets of 1-6 images, which may lead to overtraining and mask the true value of the cooperative training approach. Additionally, the paper lacks baseline results, making it hard to assess the value of the cooperative training approach, with comparisons missing for most experiments and only partial comparisons provided for face completion experiments.
Supporting Arguments
The paper's ideas are intuitively appealing, and the authors demonstrate the potential of their approach in decreasing the detection accuracy of a steganalysis method. However, the lack of empirical studies and baseline results makes it difficult to evaluate the effectiveness of the SGAN model. The authors also fail to provide a comprehensive related work section, missing a discussion on the relation with variational autoencoders, a relevant topic in the field.
Additional Feedback
To improve the paper, the authors should provide more comprehensive empirical studies, including larger training datasets and baseline results. They should also provide a more detailed related work section, discussing the relation with other relevant topics in the field, such as variational autoencoders. Additionally, the authors should consider providing more experimental results, including comparisons with other state-of-the-art methods, to demonstrate the effectiveness of their approach.
Questions for the Authors
1. How do the authors plan to address the issue of overtraining with tiny training datasets?
2. Can the authors provide more baseline results, including comparisons with other state-of-the-art methods, to demonstrate the effectiveness of their approach?
3. How do the authors plan to extend their approach to more advanced steganographic algorithms, such as WOW, HUGO, and S-UNIWARD?