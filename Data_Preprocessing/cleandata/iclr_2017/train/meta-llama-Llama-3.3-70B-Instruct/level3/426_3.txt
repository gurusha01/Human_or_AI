Summary
The paper proposes a novel approach to learning word embeddings by systematically investigating different context types and representations. The authors conduct comprehensive experiments to evaluate the effectiveness of these context types under various tasks and datasets, aiming to provide a guideline for choosing the best context for word embedding models. This contribution has the potential to advance the field of natural language processing by shedding light on the optimal definition of context.
Decision
I decide to Reject this paper, with the primary reason being that it does not directly address the problem stated in the abstract. The abstract mentions an investigation of context types for learning word embeddings, but the paper actually focuses on mapping between word embedding spaces of two languages using bilingual information.
Supporting Arguments
The paper's approach, although well-motivated, seems to be misplaced in the literature. Previous works, such as Faruqui & Dyer 2014 and Xing et al 2015, have already explored similar methods, which raises questions about the paper's novelty. Furthermore, the use of cognates instead of a bilingual dictionary is limited to languages with the same alphabet, and the term "cognate" is misused, referring to words with similar written forms rather than a common etymological origin.
Additional Feedback
To improve the paper, I suggest the authors clarify the connection between their proposed approach and the problem stated in the abstract. Additionally, they should provide a more thorough discussion of the limitations of their method, particularly with regards to the use of cognates and the assumption of similar alphabets. It would also be beneficial to include a more detailed comparison with previous works, highlighting the unique contributions of their approach.
Questions for the Authors
To better understand the paper and make a more informed decision, I would like the authors to answer the following questions: (1) Can you provide a clear explanation of how your approach addresses the problem of defining context for word embedding models, as stated in the abstract? (2) How do you plan to extend your method to languages with different alphabets, and what are the potential challenges and limitations of such an extension? (3) Can you provide more details on the experimental setup and the datasets used to evaluate the effectiveness of your approach, and how do you ensure the scientific rigor of your results?