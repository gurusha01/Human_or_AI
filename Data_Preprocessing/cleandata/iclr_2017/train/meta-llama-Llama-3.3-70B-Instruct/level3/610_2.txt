Summary
The paper proposes a technique to reduce the parameters of Recurrent Neural Networks (RNNs) by pruning weights during the initial training of the network. The authors demonstrate that their approach can achieve sparsity of 90% with a small loss in accuracy, and even improve the accuracy over a dense baseline by starting with a larger dense matrix and then pruning it down. The technique is computationally efficient, easy to implement, and can be used to reduce the size of RNN models, making them more suitable for deployment on mobile devices and server farms.
Decision
I decide to accept this paper, with the main reason being that it provides a well-motivated and well-executed approach to reducing the parameters of RNNs, which is a significant challenge in deploying these models on resource-constrained devices. The paper provides convincing experimental evidence that the proposed technique can achieve significant reductions in model size while maintaining or even improving accuracy.
Supporting Arguments
The paper is well-written, and the authors provide a clear and concise explanation of their approach, including the pruning algorithm and the hyperparameters used to control the pruning process. The experimental results are thorough and demonstrate the effectiveness of the proposed technique on several benchmark datasets. The authors also provide a detailed analysis of the computational efficiency of their approach and its potential for deployment on mobile devices and server farms.
Additional Feedback
To further improve the paper, I suggest that the authors provide more insights into the pruning process, such as the effect of different pruning schedules and the relationship between sparsity and accuracy. Additionally, it would be interesting to see a comparison of the proposed technique with other methods for reducing the size of RNN models, such as quantization and knowledge distillation.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the hyperparameters for the pruning process are chosen, and how sensitive the results are to these hyperparameters?
2. How do you plan to extend this work to other types of neural networks, such as convolutional neural networks (CNNs) and transformers?
3. Can you provide more insights into the computational efficiency of the proposed technique, including the memory and computational requirements for deploying the pruned models on mobile devices and server farms?