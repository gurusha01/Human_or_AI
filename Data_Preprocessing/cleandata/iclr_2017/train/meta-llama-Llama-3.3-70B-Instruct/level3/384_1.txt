This paper proposes a novel approach to reinforcement learning by jointly predicting video frames and cumulative rewards in high-dimensional visual state spaces. The authors extend recent work on video frame prediction to enable reward prediction, demonstrating accurate cumulative reward prediction up to 200 frames in five Atari games. The approach is well-motivated, building on the success of model-based reinforcement learning and addressing the challenge of unknown environment dynamics and reward functions.
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-defined problem in reinforcement learning, and the approach is well-motivated and clearly explained.
2. The experimental evaluation is thorough, with a detailed analysis of the results and a discussion of the limitations of the approach.
The paper supports its claims with empirical results, demonstrating the effectiveness of the joint prediction model in cumulative reward prediction. The results are promising, with the model achieving accurate predictions in several Atari games. However, the paper also acknowledges the limitations of the approach, including the challenge of handling stochastic transitions and the potential for overestimation of cumulative rewards.
To improve the paper, I suggest the following:
- Provide more details on the network architecture and training procedure, including the specific hyperparameters used and the optimization algorithm employed.
- Consider adding more visualizations of the predicted video frames and rewards to help illustrate the performance of the model.
- Discuss potential applications of the approach beyond Atari games, such as in real-world environments with high-dimensional visual state spaces.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
- How does the joint prediction model handle cases where the reward function is not well-defined or is changing over time?
- Can the authors provide more insight into the trade-off between video frame reconstruction loss and reward prediction loss, and how this trade-off affects the overall performance of the model?
- How does the approach compare to other model-based reinforcement learning methods, such as those using Monte Carlo tree search or planning-based approaches?