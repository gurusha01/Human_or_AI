Summary
The paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by introducing an intermediate model, called Simplified-SFNN. This model approximates SFNNs by simplifying the upper latent units above stochastic ones, making it easier to train than SFNNs. The authors establish a connection between three models: DNN → Simplified-SFNN → SFNN, which leads to an efficient training procedure for SFNNs using pre-trained parameters of DNNs.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the approach is well-motivated, and the authors provide a clear explanation of the connection between the three models. Secondly, the experimental results demonstrate the effectiveness of the proposed approach, showing that Simplified-SFNNs outperform their baseline DNNs due to the stochastic regularizing effect.
Supporting Arguments
The paper tackles the specific problem of training large-scale SFNNs, which is a challenging task due to the computational issues involved in computing expectations and gradients. The authors provide a rigorous analysis of the connection between the three models, including theoretical proofs and empirical results. The experimental results demonstrate the effectiveness of the proposed approach on various tasks, including multi-modal learning and classification tasks on popular datasets such as MNIST, CIFAR-10, and CIFAR-100.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed explanations of the experimental setups, including the hyperparameter tuning process and the specific architectures used for each dataset. Additionally, it would be helpful to include more visualizations of the results, such as plots of the training and test errors, to provide a clearer understanding of the performance of the proposed approach.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How did the authors choose the hyperparameters for the Simplified-SFNNs, and what was the effect of different hyperparameter settings on the performance of the model?
2. Can the authors provide more details on the computational cost of training Simplified-SFNNs compared to SFNNs and DNNs?
3. How do the authors plan to extend the proposed approach to more complex models, such as recurrent neural networks or generative adversarial networks?