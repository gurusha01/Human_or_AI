Summary
This paper proposes a novel approach to learning state representations for robots using deep neural networks and robotic priors. The authors demonstrate the effectiveness of their approach in learning a one-dimensional representation of a Baxter robot's head position from raw images. The paper claims to contribute to the field of robotics by providing a method for unsupervised learning of state representations that can be used for task-specific control.
Decision
I decide to reject this paper, with the main reason being that the approach, although promising, lacks motivation and comparison to other techniques, particularly in cold-start scenarios. Additionally, the paper has several sub-optimal aspects, such as using pre-trained modules instead of training the system end-to-end, and the use of pairwise residual units is confusing and not well-motivated.
Supporting Arguments
The paper's approach to combining multiple modalities to learn a joint embedding for recommender systems is promising, but the architecture has several sub-optimal aspects. The use of pre-trained modules instead of training the system end-to-end may limit the model's ability to learn task-specific representations. Furthermore, the choice of TextCNN for text embedding vectors is reasonable, but the details of its usage are unclear, and using only the first 10 words of the title and product description may be insufficient to capture relevant information.
Additional Feedback
To improve the paper, the authors should provide more motivation for the choices made in the architecture and compare their approach to other techniques, particularly in cold-start scenarios. Additionally, the authors should consider using an alternative approach, such as using an additional fully-connected layer to mix modality-specific embeddings, instead of pairwise residual units. The authors should also provide more details on the usage of TextCNN and consider using more words from the title and product description to capture relevant information.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. Can you provide more motivation for the choice of pre-trained modules instead of training the system end-to-end?
2. How do you plan to address the limitation of using only the first 10 words of the title and product description to capture relevant information?
3. Can you provide more details on the comparison of your approach to other techniques, particularly in cold-start scenarios?