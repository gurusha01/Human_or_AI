Summary of the Paper's Contributions
The paper introduces a new large-scale machine reading comprehension dataset called MS MARCO, which features real user queries, real web documents, and human-generated answers. The dataset's strengths include its use of real queries, context passages from web documents, human-generated answers, and its large scale with 100,000 queries. The authors also propose a simple baseline method for detecting errors and out-of-distribution examples using softmax prediction probabilities.
Decision: Reject
I decide to reject this paper due to two key reasons: (1) the paper lacks a comprehensive analysis and evaluation of the MS MARCO dataset, and (2) the writing and analysis are incomplete and hurried, with mistakes and a lack of clear presentation of what QA models can learn from MS MARCO.
Supporting Arguments
The paper's weaknesses include a lack of study to support its claims, unclear presentation of what QA models can learn from MS MARCO, and incomplete analysis and evaluation. The paper does not provide enough information on the challenges of obtaining good performance on the dataset, human performance compared to models, and train/test splits. Additionally, the paper's writing and analysis are deemed incomplete and hurried, with mistakes and a lack of comprehensive evaluation of the dataset's worth.
Additional Feedback
To improve the paper, I suggest that the authors provide a more comprehensive analysis and evaluation of the MS MARCO dataset, including a clear presentation of what QA models can learn from it. The authors should also provide more information on the challenges of obtaining good performance on the dataset, human performance compared to models, and train/test splits. Furthermore, the authors should revise the paper to address the mistakes and incomplete analysis, and provide a more thorough evaluation of the dataset's worth.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more information on the challenges of obtaining good performance on the MS MARCO dataset?
* How do human performance and model performance compare on the dataset?
* Can you provide more details on the train/test splits and how they were determined?
* How do you plan to address the mistakes and incomplete analysis in the paper?
* Can you provide a more comprehensive evaluation of the dataset's worth and its potential impact on the field of machine reading comprehension?