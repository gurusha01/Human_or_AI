This paper introduces the dynamic neural Turing machine (D-NTM), an extension of the neural Turing machine (NTM) that incorporates a learnable addressing scheme, allowing for more flexible and nonlinear location-based addressing. The D-NTM is evaluated on various tasks, including episodic question-answering, sequential MNIST, and algorithmic tasks, demonstrating its ability to outperform baseline models such as LSTM-RNN and NTM.
The approach is well-motivated, building upon the existing NTM architecture and addressing its limitations. The introduction of a learnable addressing scheme enables the D-NTM to learn sophisticated location-based addressing strategies, which is essential for tasks that require complex memory access patterns. The use of discrete attention and curriculum learning strategies also contributes to the model's performance.
However, I decide to reject this paper due to two key reasons. Firstly, the experiment description in Section VI lacks sufficient details for reproducibility, such as state representation, initial state distribution, and noise magnitude. This omission makes it challenging to replicate the results and verify the claims made in the paper. Secondly, the footnote on page 8 regarding the use of Newton's method and the argmax_theta operator seems inconsistent with the understanding of the EM-style step in Eq (8) of the Kober paper, which requires further clarification.
To improve the paper, I suggest providing more detailed descriptions of the experimental setup, including the specific hyperparameters used, the initialization methods, and the evaluation metrics. Additionally, the authors should address the inconsistency in the footnote and provide a clearer explanation of the argmax_theta operator and its relationship to the EM-style step. Furthermore, I would like the authors to answer the following questions: (1) Can you provide more insights into the learnable addressing scheme and how it contributes to the model's performance? (2) How does the discrete attention mechanism affect the model's ability to generalize to new tasks and datasets? (3) Can you provide more details on the curriculum learning strategy and its impact on the model's training and performance?