This paper proposes a novel optimization algorithm called Entropy-SGD for training deep neural networks. The algorithm is motivated by the local geometry of the energy landscape and aims to exploit the phenomenon of wide valleys in the energy landscape, which are known to generalize better. The authors introduce the concept of local entropy, which measures the flatness of the energy landscape, and use it to modify the traditional SGD algorithm. The resulting Entropy-SGD algorithm has a smoother energy landscape and obtains better generalization error than the original SGD algorithm.
The paper is well-written, and the idea presented is novel and clearly described. The authors provide a thorough analysis of the algorithm, including its theoretical properties and experimental results on various datasets. The experimental results show that Entropy-SGD performs comparably to state-of-the-art techniques in terms of generalization error and training time.
However, I have some concerns regarding the experimental evaluation. The paper lacks a fair comparison between models, as it does not mention the number of parameters for all models. Additionally, the effect of drop-path seems to lose significance when data augmentation is applied, which raises questions about the robustness of the algorithm.
Based on the conference guidelines, I will answer the three key questions:
1. What is the specific question/problem tackled by the paper?
The paper tackles the problem of optimizing deep neural networks by exploiting the local geometry of the energy landscape.
2. Is the approach well-motivated, including being well-placed in the literature?
The approach is well-motivated, and the authors provide a thorough analysis of the algorithm and its relation to existing literature.
3. Does the paper support the claims?
The paper provides experimental results that support the claims, but I have some concerns regarding the fairness of the comparison and the robustness of the algorithm.
Based on these questions, I decide to reject the paper due to the concerns regarding the experimental evaluation. However, I believe that the idea presented is novel and has potential, and I provide additional feedback to help improve the paper.
Additional feedback:
To improve the paper, I suggest that the authors provide a more detailed comparison between models, including the number of parameters, and investigate the effect of drop-path on the algorithm's performance. Additionally, the authors could explore the application of Entropy-SGD to other optimization problems and provide more theoretical analysis of the algorithm's properties.
Questions to the authors:
1. Can you provide more details on the experimental setup, including the number of parameters for each model?
2. How do you plan to address the issue of drop-path losing significance when data augmentation is applied?
3. Can you provide more theoretical analysis of the algorithm's properties, such as its convergence rate and stability?