Summary of the Paper's Contributions
The paper introduces a novel reward-predicting head to an existing neural network (NN) architecture for video frame prediction, demonstrating success in Atari game playing scenarios. The authors extend recent work on video frame prediction to enable joint prediction of future states and rewards using a single latent representation. The proposed network architecture and training procedure are evaluated on five Atari games, showing accurate cumulative reward prediction up to roughly 200 frames.
Decision and Reasons
I decide to reject this paper, with the main reasons being that the model is only incrementally different from the baseline and the paper is thin on new contributions. While the paper is well-written and easy to follow, the introduced changes to the existing architecture are relatively minor, and the results, although positive, do not significantly advance the state-of-the-art in reinforcement learning.
Supporting Arguments
The paper's contributions are limited to the addition of a reward-predicting head to an existing architecture, which, although showing promising results, does not substantially deviate from previous work. The evaluation is conducted on a limited set of Atari games, and the results, while encouraging, do not demonstrate a significant improvement over existing methods. Furthermore, the paper lacks a thorough analysis of the introduced changes and their impact on the overall performance of the model.
Additional Feedback and Suggestions
To improve the paper, I suggest the authors conduct a more comprehensive evaluation of their model on a wider range of tasks and environments, including more complex scenarios and real-world applications. Additionally, a more detailed analysis of the introduced changes and their effects on the model's performance would be beneficial. The authors may also consider exploring other architectures and techniques to further improve the model's performance and advance the state-of-the-art in reinforcement learning.
Questions for the Authors
To clarify my understanding of the paper and provide more informed feedback, I would like the authors to address the following questions:
1. Can you provide more details on the design choices behind the introduced reward-predicting head and how it differs from existing approaches?
2. How do you plan to extend the proposed model to more complex environments and real-world applications?
3. Can you provide a more thorough analysis of the model's performance on the evaluated Atari games, including a comparison to existing methods and an examination of the introduced changes' impact on the results?