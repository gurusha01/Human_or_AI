Summary
The paper proposes an automatic dialogue evaluation model (ADEM) that learns to predict human-like scores for input responses. ADEM uses a hierarchical recurrent neural network (RNN) encoder to learn distributed representations of the context, model response, and reference response. The model is trained on a dataset of human scores for various dialogue responses and achieves significant correlations with human judgements at both the utterance and system levels.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the experiments are limited to toy examples and do not address potential issues with scalability, comparison to existing work, or runtime and training difficulty. Secondly, the authors' understanding of previous work on expressive variational families and inference networks is unclear, and their motivation and terminology could be improved for better clarity.
Supporting Arguments
The paper proposes a novel approach to automatic dialogue evaluation, but the methodology is incremental, building upon existing work, and the authors could clarify the difference between their approach and previous work. The solution to the maximum problem in Section 3.2 requires a kernel that may not scale in high dimensions, limiting its practical applicability. Additionally, the introduction of a Langevin inference network in Section 4 is unclear, and the authors' definition of an "inference network" differs from the usual sense of amortized inference.
Additional Feedback
To improve the paper, the authors could provide more detailed comparisons to existing work, including a thorough analysis of the strengths and weaknesses of their approach. They could also consider evaluating their model on more challenging datasets and providing more detailed ablation studies to understand the contributions of each component. Furthermore, the authors could clarify their terminology and provide more precise language to describe their approach.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the scalability issues of their approach, particularly in high-dimensional spaces?
2. Can the authors provide more detailed comparisons to existing work on automatic dialogue evaluation, including a thorough analysis of the strengths and weaknesses of their approach?
3. How do the authors define an "inference network," and how does their definition differ from the usual sense of amortized inference?