Summary of the Paper's Claims and Contributions
The paper proposes a new technique called PGQL, which combines policy gradient with Q-learning in a reinforcement learning setting. The authors establish a connection between the fixed points of regularized policy gradient algorithms and the Q-values of the resulting policy, showing that for small regularization, the Bellman residual of the induced Q-values is small. This leads to the development of PGQL, which adds an auxiliary update to the policy gradient that reduces the Bellman residual. The authors demonstrate the effectiveness of PGQL through numerical experiments on a grid world and the Atari games suite, achieving better data efficiency and stability compared to actor-critic and Q-learning alone.
Decision and Key Reasons
Based on the evaluation, I decide to Reject the paper. The key reasons for this decision are:
1. The paper's contribution, while interesting, is not significant enough for publication at ICLR. The results are merely okay, and the writing needs improvement, giving the impression of simply applying existing techniques to a new dataset.
2. The connection between policy gradient and Q-learning, while novel, is not thoroughly explored, and the discussion of related work requires reworking for accuracy and clarity.
Supporting Arguments
The paper's main contribution is the development of PGQL, which combines policy gradient with Q-learning. However, the authors do not provide a clear motivation for why this combination is necessary or how it improves upon existing methods. The experimental results, while promising, are not comprehensive enough to demonstrate the superiority of PGQL over other techniques. Furthermore, the paper's writing and organization could be improved, with some sections feeling disjointed or lacking in clarity.
Additional Feedback and Questions
To improve the paper, I suggest the authors:
1. Provide a clearer motivation for the development of PGQL and its potential advantages over existing methods.
2. Conduct more comprehensive experiments to demonstrate the effectiveness of PGQL, including comparisons to other state-of-the-art techniques.
3. Rework the discussion of related work to ensure accuracy and clarity.
4. Improve the writing and organization of the paper to make it more cohesive and easier to follow.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
1. Can you provide more insight into the choice of regularization parameter Î± and its effect on the performance of PGQL?
2. How do you plan to address the potential issue of overfitting or local optima in the PGQL algorithm?
3. Can you provide more details on the implementation of PGQL, including the choice of network architecture and hyperparameters?