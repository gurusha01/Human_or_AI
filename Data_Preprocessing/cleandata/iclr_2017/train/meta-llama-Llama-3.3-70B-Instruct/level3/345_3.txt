Summary
The paper proposes a Neural Cache Model, an extension to neural network language models that adapts their prediction to the recent history by storing past hidden activations as memory and accessing them through a dot product with the current hidden activation. This approach is a simplified version of memory-augmented networks and is shown to be efficient and scalable to large memory sizes. The authors demonstrate the effectiveness of their approach on several language model datasets, including the LAMBADA dataset, and show that it outperforms recent memory-augmented networks.
Decision
I decide to Accept this paper with two key reasons: (1) the approach is well-motivated and clearly placed in the literature, and (2) the paper supports its claims with thorough experiments and results.
Supporting Arguments
The paper effectively applies a simple idea to an important problem, presenting it in a clear and well-referenced manner. The addition of filter visualizations enhances the contribution of the paper. The authors provide a thorough review of related work, including cache models and memory-augmented neural networks, and demonstrate how their approach improves upon existing methods. The experiments are well-designed and comprehensive, covering various language model datasets and evaluating the performance of the Neural Cache Model against state-of-the-art baselines.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more analysis on the computational cost of their approach compared to memory-augmented neural networks. Additionally, it would be interesting to see more experiments on the effect of different cache sizes and interpolation parameters on the performance of the Neural Cache Model.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to adapt the interpolation parameter based on the current vector representation of the history ht, as mentioned in the conclusion? (2) Can the authors provide more details on the computational cost of their approach compared to memory-augmented neural networks? (3) How do the authors think their approach can be extended to other sequence prediction tasks beyond language modeling?