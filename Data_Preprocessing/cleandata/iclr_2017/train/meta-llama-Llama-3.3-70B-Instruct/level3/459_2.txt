Summary of the Paper's Contributions
The paper proposes a tensor factorization approach for Multi-Task Learning (MTL) to learn cross-task structures for better generalization. The authors present a clean and clear presentation, and their experimental justification is convincing. The paper tackles the specific question of how to effectively learn cross-task structures in MTL, and the approach is well-motivated and well-placed in the literature.
Decision
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are: (1) the paper proposes a novel and well-motivated approach to MTL, and (2) the experimental results demonstrate the effectiveness of the proposed approach.
Supporting Arguments
The paper provides a clear and concise presentation of the proposed approach, and the experimental results are convincing. The authors demonstrate the effectiveness of their approach on several tasks, including abstractive sentence summarization, machine translation, and morphological inflection generation. The results show that the proposed approach outperforms existing methods, and the authors provide a thorough analysis of the results.
Additional Feedback
To further improve the paper, I suggest that the authors discuss the effect of model size vs. performance in the final version. Additionally, exploring applications in other related fields could provide more insights into the effectiveness of the proposed approach. I also question the importance of pretraining in building the DMTRL, wondering if random initialization would also work. Furthermore, I would like to see an analysis of the model's performance on unbalanced data with few examples in some classes.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on the pretraining process and its importance in building the DMTRL?
2. How does the model perform on unbalanced data with few examples in some classes?
3. Have you explored applications in other related fields, and if so, what were the results?