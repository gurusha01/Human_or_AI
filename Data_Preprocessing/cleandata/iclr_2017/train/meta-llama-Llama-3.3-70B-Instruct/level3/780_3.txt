This paper proposes a technique to reduce the parameters of a Recurrent Neural Network (RNN) by pruning weights during the initial training of the network. The approach involves maintaining a set of masks and a monotonically increasing threshold to determine which weights to prune. The authors demonstrate that their pruning approach can achieve sparsity of 90% with a small loss in accuracy, and that pruning a larger dense network can lead to better performance than a smaller dense network.
I decide to accept this paper with some revisions. The main reason for this decision is that the paper tackles a specific and relevant problem in the field of deep learning, and the approach proposed is well-motivated and supported by experimental results. The authors provide a thorough analysis of the performance of their pruning technique and demonstrate its effectiveness in reducing the size of RNN models while maintaining their accuracy.
One of the key strengths of the paper is the thorough evaluation of the pruning technique on several RNN models, including bidirectional RNNs and Gated Recurrent Units (GRUs). The authors also provide a detailed analysis of the computational efficiency of their approach, including the speed-ups achieved by using sparse matrix-vector multiplication.
However, there are some areas that need improvement. For example, the paper could benefit from a more detailed comparison with other pruning techniques, such as L1 regularization. Additionally, the authors could provide more insight into the choice of hyperparameters for the pruning algorithm and how they affect the performance of the model.
To improve the paper, I suggest that the authors clarify some of the sentences and correct some of the claims. For example, the authors mention that their approach is "computationally efficient" but do not provide a clear definition of what this means. Additionally, the authors claim that their approach is "orthogonal" to quantization techniques, but do not provide a clear explanation of what this means.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more details on how the hyperparameters for the pruning algorithm are chosen, and how they affect the performance of the model?
* How does the pruning technique affect the interpretability of the model, and are there any plans to investigate this further?
* Can you provide more insight into the computational efficiency of the pruning technique, and how it compares to other pruning techniques?
Overall, this is a well-written paper that proposes a novel and effective technique for pruning RNNs. With some revisions to address the areas mentioned above, I believe that this paper has the potential to make a significant contribution to the field of deep learning.