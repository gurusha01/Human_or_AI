This paper proposes a novel approach to handling combinatorial perception and action-spaces in reinforcement learning, making significant contributions to the field. The authors introduce a new algorithm that combines black-box optimization with REINFORCE, facilitating consistent exploration and improving overall performance. The paper also establishes deep RL baseline results on Starcraft subdomains, demonstrating the effectiveness of the proposed approach.
I decide to accept this paper, with the primary reason being the significant contributions it makes to the field of reinforcement learning. The paper proposes a well-motivated approach that is grounded in the literature, and the results demonstrate the effectiveness of the proposed algorithm.
One of the key strengths of the paper is its ability to handle high-dimensional state spaces, which is a significant challenge in reinforcement learning. The authors demonstrate that their approach can achieve near state-of-the-art performance on various benchmarks, including continuous control tasks and Atari 2600 games. The paper also provides a detailed analysis of the factors that contribute to good performance, including the importance of granularity and encoding relevant information in the hash function.
However, I do have some concerns regarding the characterization of DPQ and gradient-free optimization. The authors incorrectly characterize DPQ as following a deterministic behavior policy, when in fact it follows a stochastic behavior policy and learns off-policy about the deterministic policy. Additionally, the authors claim that gradient-free optimization is not scalable, when in fact recent work has shown that it can scale to many parameters.
To improve the paper, I suggest that the authors provide more clarification on the DQN transfer results, where the model achieves a 96% win rate in transfer despite only reaching 13% on the training domain. This result seems puzzling, and the authors should provide more explanation or justification for this outcome. Additionally, the authors should consider providing more detailed analysis of the robustness of their approach to hyperparameter changes and the sensitivity of the results to different hyperparameter settings.
Some questions I would like the authors to answer include: Can you provide more insight into the choice of "gradient of the average cumulative reward" as the objective, and how it relates to the measured objective? How do you plan to address the incorrect characterization of DPQ and gradient-free optimization in future work? Can you provide more details on the preliminary experiments that were conducted to evaluate the effectiveness of the proposed approach? How do you think the proposed approach can be extended to more complex domains, such as those with multiple agents or partial observability?