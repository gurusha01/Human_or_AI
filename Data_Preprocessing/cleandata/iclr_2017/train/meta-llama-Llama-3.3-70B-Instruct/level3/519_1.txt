This paper proposes a novel LSTM parametrization, called Normalized LSTM, which preserves the means and variances of the hidden states and memory cells across time. The approach is well-motivated, building upon the recent success of normalization techniques in feed-forward networks and addressing the limitations of existing recurrent normalization methods.
The paper claims to contribute a faster and more efficient alternative to existing methods, such as Batch Normalization and Layer Normalization, while achieving similar or better performance on language modeling and image generative modeling tasks. The experimental results show promising performance, with the proposed method outperforming Weight Normalization and comparing favorably to other state-of-the-art approaches.
However, I decide to reject this paper for the following reasons:
1. The writing quality is poor in some areas, with numerous typos and grammatical mistakes that make it difficult to follow. A list of errors is provided at the end of this review.
2. The experimental results show only marginal improvements over existing methods, and the statistical significance of these results is unclear. Furthermore, the method's value is uncertain compared to alternatives like weight normalization.
The analysis in Section 4 is commendable, as it provides a useful examination of the method's gradient behavior. However, this analysis is not sufficient to overcome the limitations of the experimental results.
To improve the paper, I suggest the following:
* Carefully proofread the manuscript to eliminate typos and grammatical mistakes.
* Provide more detailed and rigorous experimental results, including statistical analysis and comparisons to existing methods.
* Consider additional experiments to demonstrate the method's effectiveness on more challenging tasks and to study the impact of not keeping the variance estimates fixed during learning.
Questions for the authors:
* Can you provide more detailed information about the experimental setup and hyperparameter tuning process?
* How do you plan to address the limitations of the current experimental results and demonstrate the method's effectiveness on more challenging tasks?
* Can you provide more insight into the importance of the initialization scheme and the impact of the rescaling parameters on the method's performance? 
List of errors:
* Typos: "recurent" instead of "recurrent" (multiple instances), "normalisation" instead of "normalization" (multiple instances)
* Grammatical mistakes: missing articles, incorrect verb tenses, and punctuation errors throughout the manuscript.