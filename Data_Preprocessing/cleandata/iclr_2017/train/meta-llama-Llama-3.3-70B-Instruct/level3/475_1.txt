Summary of the Paper's Contributions
The paper proposes a novel Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the expressive power of RNN language models. The NKLM significantly outperforms the traditional RNNLM in terms of perplexity and generates named entities that are not observed during training. The model also demonstrates its ability to adapt immediately to changes in knowledge.
Decision and Key Reasons
I decide to accept this paper with the key reason being that the proposed NKLM model achieves state-of-the-art performance on the WikiFacts dataset and demonstrates its ability to generate named entities and adapt to changes in knowledge. Another key reason is that the paper introduces a new evaluation metric, Unknown-Penalized Perplexity (UPP), which provides a more accurate assessment of language models for knowledge-related tasks.
Supporting Arguments
The paper provides a clear and well-motivated approach to addressing the limitations of traditional language models in encoding and decoding factual knowledge. The NKLM model is well-designed, and the experiments demonstrate its effectiveness in improving perplexity and generating named entities. The introduction of the UPP metric is also a significant contribution, as it provides a more nuanced evaluation of language models for knowledge-related tasks.
Additional Feedback and Questions
To further improve the paper, I would like the authors to provide more details on the training procedure and the comparison to other related models, such as Factored conditional restricted Boltzmann machines. I would also like to see more analysis on the performance of the NKLM model on different types of knowledge graphs and datasets. Some questions I would like the authors to answer include: How does the NKLM model handle cases where the knowledge graph is incomplete or noisy? Can the NKLM model be applied to other natural language processing tasks, such as question answering or dialogue modeling? How does the UPP metric compare to other evaluation metrics for language models, such as BLEU or ROUGE?