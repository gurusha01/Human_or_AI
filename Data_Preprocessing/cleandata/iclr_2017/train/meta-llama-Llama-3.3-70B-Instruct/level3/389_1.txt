Summary
The paper proposes a novel approach to safe reinforcement learning, introducing the concept of "intrinsic fear" to mitigate the problem of catastrophic mistakes in deep Q-networks (DQNs). The authors demonstrate that even in simple environments, DQNs can be prone to periodic reliving of catastrophic mistakes, which they term the "Sisyphean curse." To address this issue, they propose a supervised danger model that predicts the probability of catastrophe within a short number of steps, and use this model to penalize the Q-learning objective. The approach is evaluated on several environments, including Adventure Seeker, Cart-Pole, and Seaquest, and is shown to outperform traditional DQN methods.
Decision
I decide to accept this paper, with the primary reason being that it tackles a crucial problem in deep reinforcement learning, namely the safety of agents in real-world environments. The paper provides a well-motivated approach to addressing this problem, and the experimental results demonstrate the effectiveness of the proposed method.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of safe reinforcement learning, and motivates the need for a new approach. The proposed method of intrinsic fear is well-explained, and the experimental results demonstrate its effectiveness in several environments. The paper also provides a thorough discussion of related work, and highlights the novelty of the proposed approach.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the implementation of the danger model, and explore the sensitivity of the approach to different hyperparameters. Additionally, it would be useful to see more extensive experiments on more complex environments, to demonstrate the scalability of the approach. I also have some questions for the authors: (1) How do the authors plan to extend the approach to more complex environments, where the notion of "danger" may be more nuanced? (2) Can the authors provide more insight into the choice of hyperparameters, such as the fear radius and fear factor, and how these parameters affect the performance of the approach? (3) How does the approach handle cases where the catastrophe detector is imperfect, or where the agent is unable to recognize a catastrophe once it has happened?