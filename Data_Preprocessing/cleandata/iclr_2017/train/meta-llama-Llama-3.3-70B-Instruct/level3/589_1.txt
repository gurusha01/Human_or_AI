Summary of the Paper's Contributions
The paper proposes a novel approach to improve the performance of gradient descent when utilizing multiple compute resources. The authors design a baseline asynchronous gradient descent method, which delays the gradient updates of the entire model by one or more iterations adaptively. They also propose a layer-wise gradient descent method, which overlaps weight updates of a layer with inter-node synchronization of other layers. The paper evaluates these approaches on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system, achieving a speedup of up to 1.7x compared to synchronous gradient descent.
Decision and Key Reasons
I decide to Reject this paper, with two key reasons for this choice. Firstly, the ideas presented in the paper, although well-motivated and well-placed in the literature, are considered trivial and in line with current trends of combining different architectures. Secondly, the experiment on Penn Treebank is questionable due to the improper use of a predefined configuration rather than tuning the model with the given development set.
Supporting Arguments
The paper's approach to combining graph convolution with RNNs is not particularly novel, as it builds upon existing work by Shi et al. The authors' claim of achieving good experimental results, including lower perplexity on Penn Treebank, is also questionable due to the experimental setup. Furthermore, the paper's focus on asynchronous gradient descent, while relevant to the field of deep learning, does not provide significant new insights or contributions to the state-of-the-art.
Additional Feedback and Questions
To improve the paper, I suggest that the authors provide more rigorous experimental evaluations, including a thorough hyperparameter search and comparison to state-of-the-art methods. I would also like to see more detailed analysis of the trade-offs between maintaining equivalence to sequential methods and leveraging computational resources. Some questions I would like the authors to answer include: How do the authors plan to address the issue of communication latency in their proposed approaches? Can they provide more insights into the scalability of their methods on larger-scale datasets and compute resources? How do the authors plan to extend their work to other deep learning architectures and applications?