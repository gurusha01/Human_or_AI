Summary of the Paper
The paper proposes a joint many-task model (JMT) that can handle multiple natural language processing (NLP) tasks in a single end-to-end deep model. The model is designed to predict increasingly complex NLP tasks at successively deeper layers, with shortcut connections to both word representations and lower-level task predictions. The authors claim that their model achieves state-of-the-art results on several NLP tasks, including chunking, dependency parsing, semantic relatedness, and textual entailment.
Decision
I decide to reject this paper. The main reasons for this decision are:
1. Lack of convincing experiments: The paper only explores simple networks and fixed hyperparameters, which may not be representative of the best model architecture for different tasks and domains.
2. Inaccurate conclusions: The paper's findings on the insignificance of training data difference are unlikely to be true, and the lack of experiments on large datasets may have contributed to this conclusion.
Supporting Arguments
The paper's experiments are limited to feed-forward DNNs, which may not be the best model architecture for different tasks and domains. The fixed hyperparameters, such as learning rate schedule, may have impacted the results. Additionally, the paper's title does not accurately reflect its content, and the citation of Sainath et al. 2015 as a breakthrough in speech recognition is incorrect.
Additional Feedback
To improve the paper, the authors should:
1. Conduct more comprehensive experiments: Explore different model architectures, hyperparameters, and datasets to ensure the results are robust and generalizable.
2. Provide more accurate conclusions: Avoid making claims that are not supported by the experiments, and be more careful with citations and references.
3. Clarify the paper's title and content: Ensure that the title accurately reflects the content of the paper, and provide a clear and concise summary of the contributions.
Questions for the Authors
1. How do the authors plan to address the limitations of their experiments, and what additional experiments would they conduct to strengthen their claims?
2. Can the authors provide more details on their training procedure, including the optimization algorithm, batch size, and number of epochs?
3. How do the authors plan to improve the accuracy of their conclusions, and what additional analysis would they conduct to support their claims?