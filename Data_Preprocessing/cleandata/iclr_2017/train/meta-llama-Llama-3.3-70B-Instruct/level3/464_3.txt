Summary
The paper proposes a new method to learn hierarchical representations of sentences using reinforcement learning, specifically the policy gradient method REINFORCE, to induce tree structures for good performance on a downstream task. The authors evaluate their approach on four tasks: sentiment analysis, semantic relatedness, textual entailment, and sentence generation. Although the paper has interesting ideas, such as learning structures of sentences adapted for a downstream task, and is well-written, the experimental results show that the model's performance is far from state-of-the-art models in most tasks.
Decision
I decide to reject this paper, with the main reason being the weak experimental results that do not support the authors' claims. The results show that the model's performance is significantly worse than state-of-the-art models in most tasks, which raises questions about the effectiveness of the proposed approach.
Supporting Arguments
The paper's experimental results are not convincing, and the authors do not provide a clear explanation for the poor performance of their model. The results suggest that either the model or the reinforcement learning algorithm is not suitable for the tasks, which undermines the authors' claims. Furthermore, the paper does not provide a thorough comparison with other state-of-the-art models, which makes it difficult to evaluate the strengths and weaknesses of the proposed approach.
Additional Feedback
To improve the paper, the authors should provide more convincing experimental results, including a thorough comparison with other state-of-the-art models. They should also provide a clear explanation for the poor performance of their model and discuss the limitations of their approach. Additionally, the authors should consider using more advanced reinforcement learning algorithms or techniques to improve the performance of their model.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. Can you provide more details about the reinforcement learning algorithm used in the paper and why you chose this specific algorithm?
2. How do you explain the poor performance of your model on most tasks, and what steps can be taken to improve the results?
3. Can you provide a more thorough comparison with other state-of-the-art models, including their strengths and weaknesses?
4. Have you considered using more advanced techniques, such as transfer learning or multi-task learning, to improve the performance of your model?