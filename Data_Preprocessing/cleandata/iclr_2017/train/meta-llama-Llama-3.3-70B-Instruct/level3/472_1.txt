Summary of the Paper's Contributions
This paper proposes a novel approach to developing end-to-end learned interactive dialogue agents that can learn from both responding to questions and asking questions. The authors design a simulator and a set of synthetic tasks in the movie question answering domain, allowing a bot to interact with a teacher to address issues such as question clarification, knowledge operation, and knowledge acquisition. The paper explores both offline supervised learning and online reinforcement learning settings, demonstrating that the learner improves when asking questions. The authors also validate their approach on real data using Amazon Mechanical Turk, showing similar results.
Decision and Reasons
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and well-defined problem in the field of dialogue agents, which is the ability to learn from both responding to questions and asking questions.
2. The approach is well-motivated, and the authors provide a thorough analysis of the results, demonstrating the effectiveness of their method in both simulated and real-world settings.
Supporting Arguments
The paper is well-written and complete, providing a clear overview of the problem, the proposed approach, and the experimental results. The authors also provide a thorough analysis of the results, discussing the implications of their findings and the potential limitations of their approach. The use of both simulated and real-world data adds strength to the paper, demonstrating the robustness of the proposed method.
Additional Feedback and Questions
To further improve the paper, I would suggest providing more details on the implementation of the MemN2N model and the Forward Prediction model. Additionally, it would be interesting to see more analysis on the differences between the simulated and real-world data, and how the proposed method can be applied to other domains.
Some questions I would like the authors to answer are:
* How do the authors plan to extend their approach to more complex dialogue scenarios, such as multi-turn conversations or conversations with multiple participants?
* How do the authors evaluate the quality of the questions asked by the bot, and what metrics do they use to measure the effectiveness of the question-asking strategy?
* Can the authors provide more insights on the trade-off between the cost of asking questions and the benefits of improved question answering ability, and how this trade-off can be optimized in different scenarios?