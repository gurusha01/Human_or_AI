This paper proposes a novel approach to training stochastic feedforward neural networks (SFNNs) by leveraging the knowledge of pre-trained deterministic deep neural networks (DNNs). The authors introduce an intermediate model, called Simplified-SFNN, which approximates SFNN by simplifying its upper latent units above stochastic ones. The connection between DNN, Simplified-SFNN, and SFNN enables an efficient training procedure for SFNN using pre-trained parameters of DNN.
The paper is well-written and easy to follow, making it a pleasure to read and understand the content. The authors provide a clear motivation for their work, discussing the advantages of SFNNs over DNNs, such as their ability to model complex stochastic relationships and regularize better. They also highlight the challenges of training SFNNs, including the computational issues of computing expectations and gradients.
The proposed Simplified-SFNN model is novel and shows promising results on various tasks, including multi-modal learning and classification. The authors demonstrate that Simplified-SFNN can outperform its baseline DNN due to the stochastic regularizing effect, even when trained using dropout and batch normalization.
However, I have some concerns regarding the significance of the paper. While the proposed approach is interesting, it is not entirely clear whether it reveals anything surprising or unexpected about recurrent networks. The results, although promising, are not significantly better than existing architectures on real-world tasks.
To improve the paper, I would like to see more detailed comparisons with existing methods, particularly those that also utilize pre-trained parameters of DNNs. Additionally, it would be helpful to provide more insights into the choice of hyperparameters, such as the number of samples used for estimating expectations and the learning rate schedule.
In terms of the conference guidelines, I would answer the three key questions as follows:
1. What is the specific question/problem tackled by the paper? The paper addresses the challenge of training stochastic feedforward neural networks (SFNNs) by leveraging the knowledge of pre-trained deterministic deep neural networks (DNNs).
2. Is the approach well-motivated, including being well-placed in the literature? The approach is well-motivated, and the authors provide a clear discussion of the advantages and challenges of SFNNs. However, the paper could benefit from a more detailed review of existing methods that utilize pre-trained parameters of DNNs.
3. Does the paper support the claims? The paper provides empirical evidence to support the claims, but more detailed comparisons with existing methods would strengthen the results.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more insights into the choice of hyperparameters, such as the number of samples used for estimating expectations and the learning rate schedule?
* How do you plan to extend the proposed approach to more complex tasks, such as those involving multiple stochastic layers or larger datasets?
* Can you provide more detailed comparisons with existing methods that utilize pre-trained parameters of DNNs, particularly those that also address the challenge of training SFNNs?