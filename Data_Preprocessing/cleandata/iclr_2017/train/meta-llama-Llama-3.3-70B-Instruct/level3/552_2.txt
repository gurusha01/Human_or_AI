Summary of the Paper
The paper proposes a novel recurrent neural network (RNN) architecture called the Doubly Orthogonal Recurrent Neural Network (DORNN), which uses orthogonal or unitary matrices to update the hidden state. The authors claim that this architecture can learn long-term dependencies without suffering from vanishing or exploding gradients. They validate their model on a simplified memory copy task and demonstrate its ability to learn dependencies up to 5,000 timesteps.
Decision
I decide to reject this paper, primarily due to two key reasons. Firstly, the proposed model is not new and has been explored in other literature, which diminishes its novelty and contribution to the field. Secondly, the experiments presented in the paper are weak and do not provide sufficient evidence to support the claims made by the authors. The simplified memory copy task used in the experiments is not a challenging task, and the results do not demonstrate the model's ability to learn complex dependencies.
Supporting Arguments
The paper's contribution is limited by the fact that the proposed model is not new and has been explored in other literature. The authors do not provide a clear motivation for why their specific implementation of the orthogonal RNN is necessary or useful. Furthermore, the experiments presented in the paper are not rigorous and do not provide a thorough evaluation of the model's performance. The simplified memory copy task used in the experiments is not a challenging task, and the results do not demonstrate the model's ability to learn complex dependencies. The authors also do not provide a comparison with other state-of-the-art models, which makes it difficult to assess the model's performance.
Additional Feedback
To improve the paper, the authors should provide a more thorough evaluation of the model's performance on more challenging tasks, such as language modeling or machine translation. They should also provide a comparison with other state-of-the-art models to demonstrate the model's strengths and weaknesses. Additionally, the authors should consider exploring ways to utilize the model's linearity in h or R_x, as suggested by the reviewer. The authors should also address the model's inability to forget, which is a significant limitation, and demonstrate its usefulness on a non-trivial task to justify acceptance.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. How does the proposed model differ from other orthogonal RNN models that have been proposed in the literature?
2. Can the authors provide more rigorous experiments to demonstrate the model's ability to learn complex dependencies?
3. How do the authors plan to address the model's inability to forget, which is a significant limitation?
4. Can the authors provide a comparison with other state-of-the-art models to demonstrate the model's strengths and weaknesses?