Summary of the Paper's Contributions
The paper proposes a Joint Many-Task (JMT) model, a single end-to-end trainable model that handles multiple Natural Language Processing (NLP) tasks, including POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. The model uses a successive training strategy, where each task is trained on top of the previous one, and employs shortcut connections to allow word representations to be fed into all layers. The authors also introduce a successive regularization term to prevent catastrophic interference between tasks.
Decision: Accept
I decide to accept this paper because it presents a novel and well-motivated approach to multi-task learning in NLP. The JMT model achieves state-of-the-art results on several tasks, and the authors provide a thorough analysis of the model's components and their contributions to its performance.
Supporting Arguments
The paper is well-organized, and the authors provide a clear explanation of the JMT model and its components. The experimental results are impressive, with the JMT model outperforming single-task models and other multi-task learning approaches on several tasks. The authors also provide a detailed analysis of the model's performance, including ablation studies and visualizations of the shared embeddings.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the hyperparameter tuning process and the computational resources required to train the JMT model. Additionally, it would be interesting to see the results of the JMT model on other NLP tasks and datasets.
Questions for the Authors
1. How did you select the specific tasks and datasets used in the paper, and are there plans to extend the JMT model to other NLP tasks?
2. Can you provide more details on the successive regularization term and how it prevents catastrophic interference between tasks?
3. How do you plan to address the issue of task imbalance, where some tasks have much larger datasets than others?