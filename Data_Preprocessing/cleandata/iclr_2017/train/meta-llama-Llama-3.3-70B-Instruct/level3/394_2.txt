Summary
The paper proposes a novel approach to sentiment analysis, utilizing a global-local context attention framework. This framework is inspired by human reading behavior, where a rough global context is first obtained, and then used as attention to selectively focus on important local contexts. The model consists of two parts: a bidirectional LSTM (Bi-LSTM) network to extract a global context representation, and another Bi-LSTM network with attention to incorporate local contexts. The authors also propose a simplified version of the model, which only requires a single scan of the text.
Decision
I decide to accept this paper, with two key reasons: (1) the proposed approach is novel and well-motivated, and (2) the experimental results demonstrate the effectiveness of the model in sentiment analysis tasks.
Supporting Arguments
The paper is well-written, and the authors provide a clear and detailed explanation of the proposed approach. The experimental results show that the model outperforms existing related models on several benchmark datasets, including IMDB and Yelp2013. The attention visualization and case studies also provide valuable insights into how the model works, and demonstrate its ability to effectively focus on important local contexts.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the contribution of the noise and the ability to pass gradients back without decay in the zoneout method. Additionally, it would be interesting to see more comparisons with other state-of-the-art models, such as variational dropout. The authors may also consider providing more details on the hyperparameter tuning process, and the sensitivity of the model to different hyperparameter settings.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on how the global context representation is used as attention in the second Bi-LSTM network?
* How do you handle out-of-vocabulary words in the model, and what is the impact on the performance?
* Have you considered applying the proposed approach to other NLP tasks, such as machine translation or question answering?