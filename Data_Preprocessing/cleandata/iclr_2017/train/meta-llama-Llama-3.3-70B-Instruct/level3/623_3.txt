This paper proposes the use of Annealed Importance Sampling (AIS) for evaluating log-likelihoods of decoder-based generative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Generative Moment Matching Networks (GMMNs). The authors validate the accuracy of AIS using Bidirectional Monte Carlo (BDMC) and demonstrate its effectiveness in estimating log-likelihoods, outperforming existing methods such as Kernel Density Estimation (KDE) and Importance Weighted Autoencoder (IWAE) bounds.
The paper tackles the specific question of evaluating the performance of decoder-based generative models, which is a crucial problem in the field of generative modeling. The approach is well-motivated, as the authors highlight the limitations of existing evaluation methods and provide a thorough analysis of the strengths and weaknesses of AIS.
The paper supports its claims through extensive experiments on the MNIST dataset, demonstrating the accuracy and effectiveness of AIS in estimating log-likelihoods and comparing the performance of different models. The results show that AIS is able to identify significant differences in log-likelihoods between models, which are not captured by existing evaluation methods.
Based on the provided guidelines, I would accept this paper. The key reasons for this decision are:
1. The paper tackles a specific and important problem in the field of generative modeling.
2. The approach is well-motivated and thoroughly analyzed, with a clear explanation of the strengths and weaknesses of AIS.
3. The paper provides extensive experimental evidence to support its claims, demonstrating the accuracy and effectiveness of AIS in estimating log-likelihoods and comparing the performance of different models.
To improve the paper, I would suggest the following:
* Provide more visualizations and plots to illustrate the results, such as plots of the Hessian during optimization.
* Consider adding more comprehensive experiments, such as comparing AIS with other machine learning methods and normalizing weights before calculating the Hessian.
* Improve the clarity of the paper by adding references to figures in the main text and making the text in the figures larger and easier to read.
* Consider adding more analysis on the degree of overfitting and the effectiveness of existing log-likelihood estimators.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on the implementation of AIS and BDMC, such as the choice of hyperparameters and the number of intermediate distributions?
* How do you handle the case where the posterior distribution is highly concentrated, and the prior is very broad?
* Can you provide more analysis on the relationship between the log-likelihood estimates and the quality of the generated samples?
* How do you plan to extend this work to more complex datasets and models, such as image and video generation?