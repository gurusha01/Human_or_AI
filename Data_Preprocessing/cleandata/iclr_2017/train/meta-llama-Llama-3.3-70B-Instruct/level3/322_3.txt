Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and aims to reduce the computational cost of Batch Normalization (BN) while maintaining its benefits. The authors derive the gradients of the Normalized LSTM and investigate its impact on gradient propagation. They also propose a scheme to initialize the weight matrices and demonstrate the effectiveness of their approach on character-level language modeling and image generative modeling tasks.
Decision
I decide to accept this paper, with the main reason being that it presents a well-motivated and clearly written idea that addresses a significant problem in training recurrent neural networks. The approach is well-placed in the literature, and the authors provide a thorough analysis of the gradient propagation in the Normalized LSTM.
Supporting Arguments
The paper tackles a specific question/problem, which is the mitigation of the vanishing and exploding gradient problems in recurrent neural networks. The approach is well-motivated, as it builds upon existing work on normalization techniques, such as Batch Normalization and Layer Normalization. The authors provide a clear and concise explanation of their method, including the derivation of the gradients and the initialization scheme. The experimental results demonstrate the effectiveness of the Normalized LSTM on two different tasks, showing similar performances to existing state-of-the-art approaches while being computationally faster.
Additional Feedback
To further improve the paper, I suggest that the authors provide more comprehensive experiments on larger problems, such as machine translation or question answering tasks. Additionally, it would be helpful to compare the Normalized LSTM to other existing approaches, such as Weight Normalization and Recurrent Batch Normalization, in terms of computational cost and performance. The authors may also want to consider investigating the impact of not keeping the variance estimates of the cell and hidden states fixed during the learning process, as mentioned in the conclusion.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the initialization scheme for the weight matrices, and how it affects the learning process?
2. How do you plan to address the potential issue of bad initialization of the rescaling parameters, which could lead to saturation or explosion regimes?
3. Can you provide more insights on why the Normalized LSTM works well with recurrent dropout, despite not combining well with dropout in feed-forward networks?