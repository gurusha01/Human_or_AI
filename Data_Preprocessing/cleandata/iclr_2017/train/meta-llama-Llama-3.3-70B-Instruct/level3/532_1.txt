Summary of the Paper
The paper proposes a technique to reduce the parameters of Recurrent Neural Networks (RNNs) by pruning weights during the initial training of the network. The approach achieves a significant reduction in model size, with a 90% reduction in parameters, while maintaining a small loss in accuracy. The authors demonstrate the effectiveness of their approach on several datasets, including speech recognition and language modeling tasks.
Decision
I decide to reject the paper, with the main reasons being the difficulty in evaluating the experimental results and the lack of comparison to other state-of-the-art methods.
Supporting Arguments
The paper presents an interesting approach to reducing the parameters of RNNs, but the experimental results are difficult to evaluate due to the lack of additional baselines for comparison. The authors only compare their approach to a dense baseline model, which makes it challenging to assess the effectiveness of their method. Furthermore, the training convergence could be improved with a well-tuned SGD and a learning rate schedule, which is not mentioned in the paper.
Additional Feedback
To improve the paper, the authors should consider adding more baselines for comparison, such as other pruning techniques or quantization methods. Additionally, the authors should provide more details on the hyperparameter tuning process and the learning rate schedule used in their experiments. It would also be beneficial to test the approach on other datasets to ensure its generalizability and not just task-dependent success.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How did the authors choose the hyperparameters for the pruning algorithm, and what was the effect of different hyperparameter settings on the results?
2. Can the authors provide more details on the computational resources used for the experiments, and how the pruning technique affects the inference time on different hardware platforms?
3. How does the authors' approach compare to other state-of-the-art pruning techniques, such as L1 regularization or other sparse learning methods?