This paper proposes a principled framework for nonparametrically learning activation functions in deep neural networks. The authors introduce a class of nonparametric models for activation functions, which can be incorporated into the back-propagation framework, allowing for easy addition to current practice. The paper also provides a theoretical justification for the choice of nonparametric activation functions and demonstrates that networks with these activation functions generalize well.
The paper tackles the specific question of learning activation functions in deep neural networks, which is a crucial aspect of improving the performance of these models. The approach is well-motivated, as it addresses the limitation of current deep learning literature, which largely focuses on improving architectures and adding regularization to the training process. The paper also provides a clear and well-structured presentation of the methodology, including the introduction of a two-stage training process and the use of Fourier basis expansion for activation functions.
However, I have some concerns regarding the clarity and concision of the paper. The paper is too long and needs to be condensed to make it easier for readers to follow. Some of the notation, such as in equation (2.11), could be improved for better readability. Additionally, the choice of K_1 instead of N in equation (2.11) needs clarification. Furthermore, equation (2.12) may contain an error with H(X) that should be removed.
To support the claims made in the paper, I would like to see more detailed explanations and justifications for the theoretical results, particularly in Section 4. The paper could also benefit from more experimental results and comparisons with other state-of-the-art methods. Additionally, the authors could provide more insights into why Fourier basis expansion is successful in practice, while other methods, such as polynomial basis expansion, are not.
To improve the paper, I suggest the following:
* Condense the paper to focus on the most important contributions and results.
* Improve the notation and clarity of the presentation, particularly in the methodology and theoretical sections.
* Provide more detailed explanations and justifications for the theoretical results.
* Include more experimental results and comparisons with other state-of-the-art methods.
* Offer more insights into the success of Fourier basis expansion in practice.
Based on these suggestions, I would like to ask the authors to clarify the following:
* Can you provide more intuition behind the choice of Fourier basis expansion for activation functions?
* How do you plan to address the potential issue of overfitting with the nonparametric activation functions?
* Can you provide more details on the computational complexity of the proposed method and its scalability to larger datasets?
Overall, the paper presents an interesting and promising approach to learning activation functions in deep neural networks. With some revisions to address the concerns mentioned above, the paper has the potential to make a significant contribution to the field. 
Decision: Reject, but encourage resubmission after addressing the concerns mentioned above.