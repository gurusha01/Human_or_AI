Summary
The paper explores the concept of sample importance in deep neural networks, which refers to the contribution of each sample to the change in model parameters during training. The authors propose a quantitative measurement of sample importance and conduct empirical experiments on two standard datasets, MNIST and CIFAR-10. The results show that easy samples tend to shape parameters in the top layers at early training stages, while hard samples shape parameters in the bottom layers at late training stages. The authors also find that mixing hard samples with easy samples in each batch improves training performance.
Decision
I decide to reject this paper, primarily due to two reasons. Firstly, the proposed baselines are weak, and the authors' comparison between binary and floating-point precision in the CIFAR-10 experiments is counterintuitive. Secondly, the comparison between float and binary precision is unfair, as the authors do not apply all possible techniques to float that are not applicable to binary.
Supporting Arguments
The authors' baselines are weak, and they can achieve better results with more robust baselines. For example, they can achieve a 0.8% misclassification rate for a specific topology. The comparison between binary and floating-point precision is also questionable, as binary precision outperforms floating-point precision, which is counterintuitive. Furthermore, the authors do not apply techniques like Gaussian noise or other regularizations to float, which makes the comparison unfair.
Additional Feedback
To improve the paper, the authors should consider using more robust baselines and applying all possible techniques to float that are not applicable to binary. They should also provide more convincing evaluations, such as applying their techniques to more challenging scenarios. Additionally, the authors should clarify their results and provide more intuitive explanations for their findings.
Questions for the Authors
I would like the authors to answer the following questions to clarify their paper:
1. Can you provide more details on how you calculated the sample importance and how you defined easy and hard samples?
2. How do you explain the counterintuitive result that binary precision outperforms floating-point precision in the CIFAR-10 experiments?
3. Can you provide more information on how you constructed the batches and how you determined the order of batch training?
4. How do you plan to extend your work to more challenging scenarios and more complex deep learning structures?