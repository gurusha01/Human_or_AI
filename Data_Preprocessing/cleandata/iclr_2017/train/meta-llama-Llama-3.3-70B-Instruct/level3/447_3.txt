This paper introduces a simulator and synthetic tasks to evaluate a dialogue agent's ability to learn from user feedback, utilizing memory networks and supervised and reinforcement learning methods. The motivation behind the paper is excellent, as it addresses a crucial aspect of human-computer interaction: the ability of a conversational agent to learn through interactions with users.
However, the execution falls short due to the use of a highly artificial and limited synthetic dialogue simulator. The simulator's assumptions, such as always correct user feedback and limited forms of feedback, simplify the task and make it difficult to draw conclusions about learning from unstructured user feedback. The paper's results, while promising, are largely based on simulated data, which may not generalize well to real-world scenarios.
I recommend conducting experiments with real human users to provide stronger evidence for the hypothesis that a dialogue agent can learn from user feedback. The paper has several limitations, including the agent's inability to learn what questions to ask, only when to ask them, and the specification of questions a priori. The presentation of results is overwhelming and confusing, with some results not clearly contributing to the validation of the original hypothesis.
To improve the paper, I suggest moving some results, such as the vanilla-MemN2N or Cont-MemN2N results, to the appendix to improve clarity. Additionally, the authors should consider conducting more extensive experiments with real human users to validate their approach.
My decision is to reject the paper, primarily due to the limitations of the simulator and the lack of real-world experiments. However, I believe that the paper has potential and that the authors can address these limitations to produce a stronger contribution to the field.
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the limitations of the simulator and incorporate more realistic user feedback?
2. Can the authors provide more details on the Mechanical Turk experiments, including the instructions given to the Turkers and the analysis of the differences between simulated and real training data?
3. How do the authors plan to extend their approach to more complex and open-ended dialogue tasks, where the agent needs to learn what questions to ask and not just when to ask them?