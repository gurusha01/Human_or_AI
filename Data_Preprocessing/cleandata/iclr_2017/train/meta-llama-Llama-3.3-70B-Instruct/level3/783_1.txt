This paper presents a well-written and easy-to-read contribution to the field of distributed training of deep learning models. The authors revisit the conventional beliefs about synchronous and asynchronous stochastic optimization and propose a third approach, synchronous optimization with backup workers, which avoids asynchronous noise while mitigating the worst stragglers.
The specific question tackled by the paper is how to efficiently train deep learning models on large-scale training data using distributed stochastic optimization. The approach is well-motivated, and the authors provide a clear presentation of the main idea, making it easy to understand their points. The paper is also well-placed in the literature, with a thorough discussion of related work.
However, I have some concerns about the experimental methodology. The authors propose taking gradients from the first "N" workers out of "N+b" workers available, but they do not consider the potential bottleneck of the parameter server and its impact on performance. Additionally, the tuning of learning rates and other parameters seems to be done in an ad-hoc manner, and it would be more representative to run experiments multiple times and report average, best, and worst-case behavior.
Based on these concerns, I would like to reject this paper, with the main reason being the lack of rigorous experimental methodology. The authors' approach is promising, but it requires more thorough evaluation to support their claims.
To improve the paper, I would suggest the following:
* Provide a more detailed analysis of the parameter server's impact on performance and how it affects the proposed approach.
* Run experiments multiple times and report average, best, and worst-case behavior to provide a more comprehensive evaluation of the approach.
* Consider using dedicated nodes to mitigate the issue of slow nodes in synchronous algorithms.
* Provide more insight into the trade-off between dropping more stragglers to reduce iteration time and waiting for more gradients to improve gradient quality.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* How do the authors plan to address the potential bottleneck of the parameter server in their proposed approach?
* Can the authors provide more details on the experimental setup and implementation of communication between workers and parameter servers?
* How do the authors choose the value of "N" and "b" in their proposed approach, and what is the impact of these values on performance?