Summary of the Paper
The paper proposes a novel approach to distributed training of deep learning models, called synchronous stochastic optimization with backup workers. This approach aims to mitigate the straggler effect in synchronous stochastic optimization, which occurs when the slowest worker in a distributed setup delays the entire training process. The authors demonstrate that their approach can avoid asynchronous noise while reducing the impact of stragglers, leading to faster convergence and better test accuracies.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's novelty is limited, and the presentation is suboptimal, with the title and content not corresponding to each other. Secondly, the paper's focus is not clearly defined, and the authors' attack on deterministic encoder-decoder models is not the same as attacking generative models.
Supporting Arguments
The paper's results are interesting, but the approach is not well-motivated, and the experiments are limited to MNIST and SVHN datasets. The paper's motivation and attack scenario are not convincing, and experiments on natural images are necessary to judge the proposed attack's success. Additionally, the paper is too long, exceeding the recommended page limit, and should be shortened and made more concise.
Additional Feedback
To improve the paper, the authors should clarify the focus of their approach and provide more convincing motivation and attack scenarios. They should also extend their experiments to other domains, such as faces, and provide more detailed analysis of the results. Furthermore, the authors should address the smaller issues with the paper, including incorrect usage of terms and grammatical errors.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on how your approach differs from existing synchronous stochastic optimization methods?
2. How do you plan to extend your approach to other domains, such as natural images and faces?
3. Can you provide more analysis on the trade-off between dropping stragglers and waiting for more gradients to improve the gradient quality?
By addressing these questions and concerns, the authors can improve the paper and provide a more convincing argument for their approach.