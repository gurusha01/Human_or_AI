Summary
The paper proposes a novel method for domain-invariant representation learning, called Central Moment Discrepancy (CMD), which measures the difference between the distributions of the source and target domains. The authors prove that CMD is a metric and that convergence in CMD implies convergence in distribution for probability distributions on compact intervals. The method is evaluated on two benchmark datasets, Amazon reviews and Office, and achieves state-of-the-art performance on most tasks.
Decision
I decide to accept this paper with minor revisions.
Reasons
The paper is well-written, and the theoretical derivation of CMD is clear and concise. The experiments are well-designed, and the results demonstrate the effectiveness of the proposed method. The authors provide a thorough analysis of the related work and clearly motivate the need for a new approach.
Supporting Arguments
The paper addresses a specific question/problem in the field of domain adaptation, which is a well-studied area of research. The approach is well-motivated, and the authors provide a clear explanation of the limitations of existing methods. The theoretical analysis of CMD is rigorous, and the experimental results demonstrate its effectiveness.
Additional Feedback
To improve the paper, I suggest that the authors provide more details about the generator and discriminator architectures used in the experiments. Additionally, the authors could provide more quantitative results to evaluate the performance of the proposed method, such as separating real from fake examples and robustness to injected noise.
Questions
I would like the authors to clarify the following points:
1. How does the choice of the parameter K affect the performance of the proposed method?
2. Can the authors provide more insights into the relationship between CMD and other distribution matching methods, such as MMD and KL-divergence?
3. How does the proposed method handle cases where the source and target domains have different dimensions or distributions?