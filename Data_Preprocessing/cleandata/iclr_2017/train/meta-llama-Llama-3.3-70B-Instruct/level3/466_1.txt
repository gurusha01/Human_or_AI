This paper presents a novel approach to policy search in stochastic dynamical systems using model-based reinforcement learning with Bayesian neural networks (BNNs) that include stochastic input variables. The authors demonstrate the effectiveness of their approach in solving a challenging benchmark problem and achieving promising results on real-world industrial applications.
The specific question tackled by the paper is how to learn a policy in a stochastic dynamical system using a model-based approach with BNNs. The approach is well-motivated, building on recent advances in Bayesian neural networks and reinforcement learning. The paper provides a clear and detailed explanation of the methodology, including the use of α-divergence minimization for training the BNNs.
The paper supports its claims with extensive experimental results, demonstrating the effectiveness of the approach in solving the Wet-Chicken benchmark problem and achieving good performance on real-world industrial applications, including gas turbine control and an industrial benchmark. The results show that the proposed approach outperforms other methods, including Gaussian processes and variational Bayes.
I decide to accept this paper because it presents a novel and well-motivated approach to policy search in stochastic dynamical systems, and the experimental results demonstrate the effectiveness of the approach. The paper is well-written, and the methodology is clearly explained.
To improve the paper, I suggest that the authors provide more discussion on the computational complexity of the approach and its potential scalability to larger systems. Additionally, it would be interesting to see more analysis on the robustness of the approach to different types of noise and uncertainties in the system.
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more details on the computational complexity of the approach and how it scales to larger systems?
2. How does the approach handle different types of noise and uncertainties in the system, and what are the limitations of the approach in this regard?
3. Can you provide more discussion on the choice of α = 0.5 for the α-divergence minimization, and how this value was selected? 
Overall, the paper presents a significant contribution to the field of reinforcement learning and stochastic dynamical systems, and I believe it deserves to be accepted.