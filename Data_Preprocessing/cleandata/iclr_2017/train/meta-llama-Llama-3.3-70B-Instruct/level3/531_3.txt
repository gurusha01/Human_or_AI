Summary
The paper proposes a novel approach to automatic dialogue evaluation, formulating it as a learning problem. The authors introduce ADEM, a model that learns to predict human-like scores for input responses using a new dataset of human response scores. ADEM correlates significantly with human judgements at both the utterance and system levels, outperforming traditional word-overlap metrics such as BLEU. The model also demonstrates the ability to generalize to evaluating new models unseen during training.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper lacks novelty in its approach, as the idea of using a hierarchical RNN encoder to evaluate dialogue responses is not new. Secondly, the paper's results, although promising, are not competitive with state-of-the-art numbers, particularly on the Moving MNIST data.
Supporting Arguments
The paper's approach, although well-motivated, is not novel, and the authors fail to provide relevant citations to previous work, such as the DRAW paper and Efficient Inference in Occlusion-Aware Generative Models of Images. Additionally, the paper lacks important details, including the form of the recognition model and low-level details like initialization strategy, making it non-reproducible from the text alone. The results, although promising, are not competitive with state-of-the-art numbers, and the paper's evaluation is limited to a single dataset.
Additional Feedback
To improve the paper, the authors should provide more details on the recognition model and initialization strategy, and conduct a more thorough evaluation on multiple datasets. Additionally, the authors should provide more analysis on the failure cases of the ADEM model, and explore ways to improve its performance. The authors should also consider using more advanced techniques, such as attention mechanisms or graph-based models, to improve the performance of the ADEM model.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. Can the authors provide more details on the recognition model and initialization strategy used in the ADEM model?
2. How do the authors plan to address the lack of novelty in their approach, and what additional contributions can they make to the field of dialogue evaluation?
3. Can the authors provide more analysis on the failure cases of the ADEM model, and explore ways to improve its performance?
4. How do the authors plan to evaluate the ADEM model on multiple datasets, and what datasets do they plan to use for evaluation?