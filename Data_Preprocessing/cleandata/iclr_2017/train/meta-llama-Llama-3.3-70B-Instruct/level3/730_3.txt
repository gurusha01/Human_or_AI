Summary
The paper proposes an online structure learning technique for Gaussian Sum-Product Networks (SPNs) that can learn the structure of the network in a single pass through the data. The algorithm starts with a fully factorized distribution and updates the structure and parameters as new data points are processed. The technique is evaluated on several benchmark datasets and is shown to outperform other algorithms, including online Bayesian moment matching, online expectation maximization, and stacked restricted Boltzmann machines.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the application areas of the paper are limited and need to be expanded to generalize the results. Secondly, the paper's methods for improving recurrent baselines lack generalizability to other tasks and need to be tested across a wide variety of tasks.
Supporting Arguments
The paper's contribution is significant, as it proposes the first online structure learning technique for Gaussian SPNs. However, the evaluation of the technique is limited to a few benchmark datasets, and it is not clear how well the technique will perform on other tasks or in other domains. Additionally, the paper's methods for improving recurrent baselines are specific to SPNs and may not be applicable to other types of neural networks.
Additional Feedback
To improve the paper, the authors could consider evaluating the technique on a wider range of tasks and datasets, including those with discrete variables. They could also explore the combination of their structure learning technique with other parameter learning methods, such as stochastic gradient descent or expectation maximization. Furthermore, the authors could investigate ways to automatically control the complexity of the networks, such as adding a regularization mechanism to avoid overfitting.
Questions for the Authors
1. How do the authors plan to extend the technique to discrete variables, and what challenges do they anticipate in doing so?
2. Can the authors provide more details on the computational complexity of the algorithm, and how it scales to large datasets?
3. How do the authors plan to address the issue of overfitting, and what regularization mechanisms do they propose to use?