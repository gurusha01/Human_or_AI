This paper presents a novel approach to neural network architecture morphing, which allows for the modification and improvement of existing architectures. The authors propose a graph-based representation of neural networks, where modules are abstracted as graphs with blobs as vertices and convolutional layers as edges. This formulation enables the study of network morphism at a higher level, focusing on the changes of basic modules of networks.
The paper claims to contribute to the field by providing a systematic study on the problem of network morphism, introducing atomic morphing operations, and proposing algorithms for simple morphable modules and complex modules. The authors also conduct extensive experiments on benchmark datasets, including CIFAR10, CIFAR100, and ImageNet, to demonstrate the effectiveness of their approach.
I decide to reject this paper for two key reasons. Firstly, the lack of results on larger datasets and newer network architectures, such as Xception and DenseNet, raises concerns about the usability of the proposed approach in production systems. Secondly, the computational time and effort required for the proposed techniques are not adequately discussed, making it difficult to quantify the extra effort needed to implement them.
To support my decision, I argue that the paper's focus on ResNet architectures, although state-of-the-art, may not be representative of the broader range of neural network architectures used in practice. Furthermore, the absence of a thorough analysis of the computational requirements of the proposed approach limits its applicability to real-world scenarios.
To improve the paper, I suggest that the authors provide additional results on larger datasets and newer network architectures to demonstrate the scalability and versatility of their approach. Additionally, a more detailed analysis of the computational requirements and potential optimizations would strengthen the paper's contributions.
I would like to ask the authors to clarify the following points: (1) How do the proposed atomic morphing operations compare to existing network pruning and architecture search methods? (2) Can the authors provide more insights into the computational requirements of their approach and potential ways to optimize it? (3) How do the authors plan to extend their approach to other types of neural network architectures, such as recurrent neural networks or graph neural networks?