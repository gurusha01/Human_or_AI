This paper presents an algorithm for policy search in stochastic dynamical systems using model-based reinforcement learning. The authors propose using Bayesian neural networks (BNNs) with stochastic inputs to model the system dynamics, which allows for capturing complex stochastic patterns. The BNNs are trained using α-divergence minimization with α = 0.5, which is shown to produce better results than variational Bayes. The authors also present an algorithm for policy optimization using random roll-outs and stochastic optimization.
The paper claims to contribute to the field of model-based reinforcement learning by providing a powerful black-box tool for policy search. The authors demonstrate the effectiveness of their approach on several benchmark problems, including the Wet-Chicken problem, which is a challenging problem for model-based approaches. They also show promising results on industrial benchmarks, including real-world data from a gas turbine.
However, I have some concerns regarding the paper. Firstly, the reduction of Batch Normalization to identity transformation is not properly discussed, and I disagree with this reduction. Secondly, the assumption 3.1, where x^(i)=1 implies ||x^(i)||_2=1, may require further clarification. Additionally, the experiments in Section 4 are well-designed, but the combination of random projection and all-convolutional residual networks makes it hard to separate their individual effects.
To answer the three key questions for myself:
1. The specific question/problem tackled by the paper is policy search in stochastic dynamical systems using model-based reinforcement learning.
2. The approach is well-motivated, and the authors provide a clear explanation of the background and related work.
3. The paper supports its claims with experimental results, but some of the results may require further clarification or additional evidence.
Based on these questions, I would reject the paper due to the lack of clarity and additional evidence required to support some of the claims. However, I believe that the paper has the potential to be accepted with revisions.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on the reduction of Batch Normalization to identity transformation?
* Can you clarify the assumption 3.1 and provide more context on why this assumption is necessary?
* Can you separate the effects of random projection and all-convolutional residual networks in the experiments?
* Can you provide more evidence to support the claims made in the paper, such as additional experimental results or theoretical analysis?
Additional feedback to improve the paper:
* Provide more context on the background and related work to help readers understand the significance of the contributions.
* Clarify the notation and terminology used in the paper to make it easier to follow.
* Consider adding more experimental results or theoretical analysis to support the claims made in the paper.
* Provide more details on the implementation of the algorithm and the experiments, such as the hyperparameters used and the computational resources required.