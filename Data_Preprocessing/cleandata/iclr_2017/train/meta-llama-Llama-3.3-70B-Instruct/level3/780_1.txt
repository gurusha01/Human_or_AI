Summary
The paper presents a novel Linear Pipeline (LP) approach for parallel neural networks on multiple GPUs, focusing on reducing communication costs in Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD). The authors provide both theoretical analysis and experimental results, demonstrating significant speedups over existing approaches, such as Minimum Spanning Tree (MST) and Bidirectional Exchange (BE). The proposed LP collectives are designed to exploit the network bandwidth of multi-GPU systems, achieving up to 2x higher bandwidth than BE techniques.
Decision
I decide to Reject this paper, with two key reasons: (1) the writing needs significant improvement to provide more intuition about the proposed approach, particularly in the introduction and Section 3, and (2) the analysis in Section 3.2 contains errors, specifically in comparing the communication cost of linear pipeline with BE and MST.
Supporting Arguments
The paper's writing is dense and lacks clear explanations, making it difficult to follow. For example, the introduction could benefit from a more detailed motivation of the research, including the differences between designing parallel algorithms on CPU versus GPU. Additionally, the analysis in Section 3.2 requires correction to accurately represent the relationship between the proposed LP approach and existing methods.
Additional Feedback
To improve the paper, I suggest the authors: (1) provide more intuitive explanations of the proposed LP approach, including diagrams or illustrations to help readers understand the concept, (2) correct the errors in the analysis and provide a more detailed comparison with existing methods, and (3) emphasize the differences between designing parallel algorithms on CPU versus GPU to better motivate the research and provide context.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions: (1) Can you provide more details on how the LP approach is implemented in practice, including any optimizations or techniques used to achieve the reported speedups? (2) How do you plan to address the errors in the analysis and provide a more accurate comparison with existing methods? (3) Can you provide more insight into the trade-offs between the proposed LP approach and existing methods, including any potential limitations or drawbacks of the LP approach?