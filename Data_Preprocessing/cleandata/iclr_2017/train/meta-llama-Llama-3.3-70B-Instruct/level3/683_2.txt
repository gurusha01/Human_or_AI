Summary
This paper proposes a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. The authors demonstrate that this model outperforms existing memory-augmented neural language models on two corpora, but surprisingly, it mainly utilizes a memory of the previous five representations. They also experiment with a simpler model that uses the concatenation of previous output representations to predict the next word, which achieves comparable results.
Decision
I decide to reject this paper, mainly due to two reasons: (1) the literature review is incomplete, failing to mention relevant works such as wide resnets, and (2) the ensemble of resnets has lower performance than some single network results, highlighting the need for further experimentation on larger datasets.
Supporting Arguments
The paper's contribution to the field of neural language models is notable, but the incomplete literature review raises concerns about the authors' understanding of the current state of the art. Additionally, the fact that the simpler N-gram RNN model achieves comparable results to more sophisticated memory-augmented neural language models suggests that the proposed model may not be as effective as claimed.
Additional Feedback
To improve the paper, I suggest that the authors conduct a more thorough literature review, including relevant works on wide resnets and other attention mechanisms. Additionally, they should experiment with larger datasets to demonstrate the effectiveness of their proposed model. It would also be helpful to provide more analysis on why the simpler N-gram RNN model achieves comparable results, and how this relates to the limitations of the proposed model.
Questions for the Authors
I would like the authors to clarify the following points: (1) How do they plan to address the issue of the model mainly utilizing a memory of the previous five representations, and what implications does this have for the model's ability to capture long-range dependencies? (2) Can they provide more details on the experimental setup and hyperparameter tuning process, particularly with regards to the ensemble of resnets? (3) How do they think their proposed model can be improved to achieve state-of-the-art results, and what future directions do they envision for this research?