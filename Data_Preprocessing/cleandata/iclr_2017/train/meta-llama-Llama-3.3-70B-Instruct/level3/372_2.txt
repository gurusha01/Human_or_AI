This paper presents a novel extension of the Neural Turing Machine (NTM) called the Dynamic Neural Turing Machine (D-NTM), which introduces a learnable addressing scheme allowing for nonlinear location-based addressing. The D-NTM is evaluated on various tasks, including episodic question-answering, sequential MNIST, and algorithmic tasks, demonstrating its effectiveness.
The paper claims to contribute to the development of general-purpose learning algorithms by proposing a new memory module based on k-NN, which is well-written and provides convincing results. The model performs well on the Omniglot test, demonstrating its effectiveness as a sanity check. The artificial task validates the authors' claims and highlights the need for better benchmarks in the domain. The translation task showcases the practical usefulness of the proposed model.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and well-placed in the literature, and (2) the paper supports its claims with correct and scientifically rigorous results.
The approach is well-motivated as it addresses the limitation of the original NTM, which uses a linear addressing scheme. The introduction of a learnable addressing scheme allows the D-NTM to learn sophisticated location-based addressing strategies, making it more expressive and flexible. The paper is also well-placed in the literature, as it builds upon the existing work on NTMs and memory-based neural networks.
The paper supports its claims with correct and scientifically rigorous results. The experiments on episodic question-answering, sequential MNIST, and algorithmic tasks demonstrate the effectiveness of the D-NTM. The results are also compared to other models, such as LSTM-RNN and memory networks, showing that the D-NTM performs better in certain tasks.
To improve the paper, I suggest including all relevant references, such as associative LSTM, to ensure comprehensiveness. Additionally, publishing the code would further enhance the paper's usefulness.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How does the learnable addressing scheme affect the performance of the D-NTM on tasks that require precise retrieval of memory content? (2) Can the authors provide more details on the curriculum strategy used for training the discrete attention model? (3) How does the D-NTM perform on tasks that require long-term dependencies, and how does it compare to other models in this regard?