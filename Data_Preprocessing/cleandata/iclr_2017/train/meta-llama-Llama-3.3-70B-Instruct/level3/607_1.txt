Summary of the Paper's Claims and Contributions
The paper proposes a method for learning perceptual reward functions from a few demonstrations of real-world tasks, leveraging the abstraction power of intermediate visual representations learned by deep models. The authors claim that their approach can quickly infer task goals and sub-goals using a simple approximate inverse reinforcement learning method, and that the resulting reward functions are dense, incremental, and can be used to learn complex robotic manipulation skills. The paper also presents empirical results on two real-world tasks, door opening and liquid pouring, demonstrating the effectiveness of the proposed method.
Decision and Key Reasons
I decide to reject this paper, with two key reasons:
1. Lack of Clear Contribution: While the paper presents an interesting application of deep learning to reinforcement learning, it is not clear what the specific contribution of the paper is, beyond demonstrating the effectiveness of a particular approach on two tasks. The paper does not provide a clear comparison to existing methods or a thorough analysis of the limitations of the proposed approach.
2. Limited Evaluation: The evaluation of the proposed method is limited to two tasks, and the results are mostly qualitative. The paper could benefit from a more thorough quantitative evaluation, including a comparison to other methods and an analysis of the robustness of the proposed approach to different types of demonstrations and tasks.
Supporting Arguments
The paper's approach is based on a simple approximate inverse reinforcement learning method, which may not be sufficient to capture the complexity of real-world tasks. The use of pre-trained visual features may also limit the generality of the proposed method to tasks that are similar to those used in the pre-training dataset. Additionally, the paper's evaluation is limited to a small number of tasks, and it is not clear how the proposed method would perform on more complex tasks or in more challenging environments.
Additional Feedback and Questions
To improve the paper, I would suggest the following:
* Provide a clear comparison to existing methods for learning reward functions from demonstrations.
* Conduct a more thorough quantitative evaluation of the proposed method, including a comparison to other methods and an analysis of the robustness of the proposed approach to different types of demonstrations and tasks.
* Discuss the limitations of the proposed method and potential avenues for future work.
Some questions I would like the authors to answer include:
* How does the proposed method compare to other approaches for learning reward functions from demonstrations, such as inverse reinforcement learning or apprenticeship learning?
* How robust is the proposed method to different types of demonstrations, such as demonstrations with varying levels of noise or demonstrations that are not perfectly executed?
* How can the proposed method be extended to more complex tasks or more challenging environments, such as tasks that require multiple robots or tasks that are performed in a dynamic environment?