This paper presents a novel approach to understanding the behavior of deep neural networks by analyzing the preimages of outputs at various layers. The authors propose a method to compute these preimages, which are the sets of inputs that result in the same output activity, and demonstrate how they can be used to describe the input manifolds of specific classes. The paper also explores the implications of this concept for modeling image manifolds and achieving efficient classification.
I decide to reject this paper, primarily due to its lack of polish and insufficient contributions for a finished paper. The paper contains numerous typos, technical errors, and unclear explanations, which hinder its overall readability and understanding. Furthermore, the paper lacks a thorough discussion of related concepts, such as the work by Montufar et al. on local linear maps of ReLU networks.
The paper's ideas are intriguing, but they do not contribute sufficiently many results for a conference paper, making it more suitable for a workshop track. The authors' use of advanced mathematical concepts, such as Grassmann-Cayley algebra, may not be necessary and could be replaced with elementary linear algebra. Additionally, the paper's figures, particularly Figure 1, lack clarity and specificity, with unclear arrows and undefined terms.
To improve the paper, I suggest that the authors address the technical errors, clarify their explanations, and provide more context and discussion of related work. They should also consider simplifying their mathematical approach and improving the quality of their figures. Furthermore, the authors should investigate the practical considerations of computing preimages, which could be troublesome despite their straightforward definition.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. How do the authors plan to address the issue of pooling in deep learning networks, which is currently disregarded in their approach?
2. Can the authors provide more empirical evidence to support their hypothesis that the preimages of activities at various levels are piecewise linear manifolds?
3. How do the authors intend to use the concept of preimages to enhance the efficiency of training deep neural networks, and what specific applications do they envision for this approach?