Summary
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) for regularizing deep neural networks and achieving better optimization performance. The DSD method involves training a dense network, pruning it to make it sparse, training the sparse network, and then adding connections back to train the model as dense again. The authors demonstrate that DSD can improve the performance of various neural network architectures, including CNNs, RNNs, and LSTMs, on tasks such as image classification, caption generation, and speech recognition.
Decision
I decide to accept this paper with minor revisions. The main reasons for this decision are that the paper proposes a well-motivated and novel approach to training neural networks, and the experimental results demonstrate significant improvements in performance on various tasks.
Supporting Arguments
The DSD method is well-motivated by the need to regularize deep neural networks and avoid overfitting. The authors provide a clear explanation of the method and its components, including the pruning and re-densifying steps. The experimental results are thorough and demonstrate the effectiveness of DSD on various tasks and architectures. The paper also provides a good discussion of the related work and the potential benefits and limitations of the DSD approach.
Additional Feedback
To improve the paper, I suggest that the authors provide more analysis of the computational cost of the DSD method and its potential applications to other tasks and domains. Additionally, it would be helpful to include more visualizations of the weight distributions and the pruning process to provide a better understanding of how the DSD method works. Finally, the authors may want to consider providing more details on the hyperparameter tuning process and the sensitivity of the DSD method to different hyperparameter settings.
Questions for the Authors
I have a few questions for the authors to clarify some aspects of the paper:
1. Can you provide more details on the computational cost of the DSD method and how it compares to other regularization techniques?
2. How do you choose the sparsity ratio and the number of iterations for the DSD method?
3. Can you provide more examples of the captions generated by the NeuralTalk model with and without DSD training to demonstrate the improvement in performance?
4. Have you considered applying the DSD method to other tasks and domains, such as natural language processing or reinforcement learning?