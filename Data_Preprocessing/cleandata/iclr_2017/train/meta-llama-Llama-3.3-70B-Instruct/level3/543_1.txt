Summary
The paper proposes a novel approach to address the problem of catastrophic mistakes in deep reinforcement learning (DRL) agents. The authors introduce "intrinsic fear," a method that incorporates a supervised danger model to predict the probability of catastrophe within a short number of steps. This model is used to penalize the Q-learning objective, shaping the reward function away from catastrophic states. The approach is evaluated on several environments, including Adventure Seeker, Cart-Pole, and Seaquest, and shows promising results in reducing the number of catastrophic mistakes.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the approach is well-motivated and addresses a significant problem in DRL, and (2) the experimental results demonstrate the effectiveness of the proposed method in reducing catastrophic mistakes.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of catastrophic mistakes in DRL, and the proposed approach is well-motivated by the need to avoid such mistakes. The authors provide a thorough analysis of the limitations of existing methods and demonstrate the effectiveness of their approach through experiments on several environments. The use of a supervised danger model to predict the probability of catastrophe is a novel and interesting idea, and the results show that it can be an effective way to shape the reward function and avoid catastrophic states.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of the danger model and the choice of hyperparameters. Additionally, it would be interesting to see more experiments on more complex environments and to explore the limitations of the approach. The authors may also want to consider providing more theoretical analysis of the approach and its relationship to existing methods in DRL.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the implementation of the danger model, including the choice of architecture and hyperparameters?
* How do you choose the fear radius and fear factor, and what is the sensitivity of the results to these parameters?
* Can you provide more experiments on more complex environments, such as those with high-dimensional state spaces or multiple catastrophic states?
* How does the approach relate to existing methods in DRL, such as safe exploration and robust reinforcement learning?