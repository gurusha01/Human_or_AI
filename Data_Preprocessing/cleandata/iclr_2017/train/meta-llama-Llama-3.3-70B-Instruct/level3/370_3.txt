Summary of the Paper's Contributions
The paper proposes a novel training strategy for deep neural networks, called Dense-Sparse-Dense (DSD) training, which achieves high-quality results on various datasets. The method involves three steps: dense training, sparse training with weight pruning, and re-dense training with weight restoration. The authors claim that DSD training can improve the performance of various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, caption generation, and speech recognition.
Decision and Reasons
I decide to accept this paper with minor revisions. The main reason for this decision is that the paper proposes a simple yet effective method for improving the performance of deep neural networks, which is supported by extensive experimental results on various datasets. The method is also easy to implement and requires only a few hyper-parameters to tune.
Supporting Arguments
The paper provides a clear and well-motivated introduction to the problem of optimizing deep neural networks, and the proposed DSD training method is well-placed in the literature. The experimental results are extensive and demonstrate the effectiveness of the method on various datasets and tasks. The authors also provide a detailed analysis of the results and discuss the potential reasons for the improved performance of DSD training.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more analysis on the effect of the sparsity hyper-parameter on the performance of DSD training. Additionally, it would be interesting to see more results on the application of DSD training to other tasks and datasets. Some questions that I would like the authors to answer include: How does the choice of sparsity hyper-parameter affect the performance of DSD training? Can DSD training be applied to other types of neural networks, such as graph neural networks or transformers? How does DSD training compare to other methods for improving the performance of deep neural networks, such as knowledge distillation or quantization?
Questions for the Authors
To clarify some aspects of the paper, I would like to ask the authors the following questions: Can you provide more details on the implementation of DSD training, including the specific hyper-parameters used and the computational resources required? How do you choose the sparsity hyper-parameter, and what is the effect of different sparsity values on the performance of DSD training? Can you provide more results on the application of DSD training to other tasks and datasets, such as natural language processing or computer vision tasks?