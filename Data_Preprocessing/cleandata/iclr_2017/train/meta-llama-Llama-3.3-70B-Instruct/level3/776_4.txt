The paper proposes a noisy channel model for sequence-to-sequence transduction tasks, which uses a recurrent neural network to parameterize the source and channel models. The approach is well-motivated, as it allows for the use of unpaired output data to estimate the parameters of the source model, and input-output pairs to train the channel model. The paper also provides a clear and detailed description of the model and its components, including the use of a latent alignment variable to enable the model to compute probabilities with incomplete conditioning contexts.
However, I have some concerns regarding the experimental evaluation of the model. The results show that the noisy channel model outperforms the direct model on several tasks, including abstractive sentence summarization, machine translation, and morphological inflection generation. However, the improvements are not always significant, and the model's performance is often comparable to that of other state-of-the-art models.
My decision is to reject the paper, with two key reasons for this choice. Firstly, the paper lacks a thorough comparison to a baseline model that is a neural system, which makes it difficult to assess the true benefits of the proposed approach. Secondly, the use of a limited window of 2k words for making decisions about word changes seems restrictive and counterintuitive, as it doesn't consider the full generated sentence.
To improve the paper, I would suggest adding more experimental results, including a comparison to a baseline neural model, and exploring the use of global sentence-level scores instead of local word-level scores. Additionally, the authors could consider alternative approaches, such as learning an encoder-decoder that takes in the source sentence and generated sentence to produce a reference sentence.
Some questions I would like the authors to answer include: How does the proposed approach compare to other sequence-to-sequence models that use attention mechanisms or other techniques to handle long-range dependencies? Can the authors provide more insight into the benefits and limitations of using a latent alignment variable, and how it affects the model's performance? How does the model's performance change when using different sizes of unpaired output data, and what are the implications for real-world applications?