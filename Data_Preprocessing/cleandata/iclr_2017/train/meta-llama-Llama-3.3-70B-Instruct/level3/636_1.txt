Summary of the Paper
The paper proposes a new initialization technique and a correction trick to batch normalization to adjust for the variance introduced by dropout in neural networks. The authors claim that their method yields higher accuracy and faster convergence, and they provide experimental results on MNIST, CIFAR-10, and CIFAR-100 datasets.
Decision
I decide to reject this paper, with the main reasons being the limited and incomplete experimental evaluation, and the lack of a comprehensive and fair comparison to state-of-the-art methods.
Supporting Arguments
The paper's experimental setup is unclear and lacks generality, with results only shown for MNIST and limited dropout values, and without data augmentation or comparison to state-of-the-art methods. The convergence results are limited to three dropout values and may not be representative of the full range of possible values. The comparison to other methods is unfair due to the lack of tuning of hyperparameters. The results for CIFAR10 and CIFAR100 are not significant, and the comparison to batch normalization is incomplete, with some results missing and no clear conclusion about the effectiveness of the proposed method.
Additional Feedback
To improve the paper, the authors should provide a more comprehensive and fair comparison to state-of-the-art methods, including ResNet variants and DenseNet, and provide results for ImageNet or other large-scale datasets. The authors should also fine-tune the parameters reasonably, using random search or Bayesian optimization to obtain meaningful comparisons. Additionally, the authors should clarify the experimental setup and provide more detailed results, including the effect of different dropout rates and learning rates on the convergence and accuracy of the proposed method.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more detailed results on the effect of different dropout rates and learning rates on the convergence and accuracy of the proposed method?
2. How do you plan to extend the proposed method to other types of neural networks, such as recurrent neural networks or graph neural networks?
3. Can you provide a more comprehensive comparison to state-of-the-art methods, including ResNet variants and DenseNet, and provide results for ImageNet or other large-scale datasets?