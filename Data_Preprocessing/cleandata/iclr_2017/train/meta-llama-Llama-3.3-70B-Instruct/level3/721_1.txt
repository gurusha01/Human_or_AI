The paper proposes a novel approach to compressing deep neural networks by introducing a "density-diversity penalty" regularizer that encourages high sparsity and low diversity in the trained weight matrices. This approach is motivated by the characteristics of convolutional layers, which have low diversity and high sparsity due to the sharing of weights and the presence of zeros. The authors demonstrate the effectiveness of their approach on both computer vision and speech recognition tasks, achieving significant compression rates while maintaining comparable performance to the original models.
I decide to accept this paper, with the key reason being that the approach is well-motivated and supported by thorough experiments. The authors provide a clear and detailed explanation of their method, including the formulation of the density-diversity penalty and the efficient optimization technique using a sorting trick. The experimental results on both MNIST and TIMIT datasets demonstrate the effectiveness of the approach in compressing fully-connected layers while maintaining performance.
One of the strengths of the paper is the thorough comparison with existing compression methods, such as "deep compression". The authors provide a detailed analysis of the diversity and sparsity of the trained matrices using their approach and the "deep compression" method, showing that their approach achieves higher sparsity but more diversity. This suggests that the density-diversity penalty is a more balanced approach to compression, as it enforces both high sparsity and low diversity simultaneously.
To further improve the paper, I suggest that the authors provide more insight into the choice of hyperparameters, such as the value of p in the density-diversity penalty. Additionally, it would be interesting to see the application of the density-diversity penalty to other types of neural networks, such as recurrent models or convolutional layers. The authors may also want to consider exploring other forms of regularizations to reduce the diversity of the trained weight matrices.
Some questions I would like the authors to answer include: How do the authors choose the value of p in the density-diversity penalty, and what is the effect of different values of p on the compression rate and performance? Can the authors provide more insight into the trade-off between sparsity and diversity in the trained weight matrices, and how this trade-off affects the performance of the compressed models? How do the authors plan to extend the density-diversity penalty to other types of neural networks, and what are the potential challenges and opportunities in doing so?