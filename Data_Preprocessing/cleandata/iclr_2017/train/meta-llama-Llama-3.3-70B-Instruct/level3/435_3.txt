Summary of the Paper's Contributions
The paper proposes a simple yet effective method to speed up the convergence of stochastic gradient descent (SGD) by introducing warm restarts, which involve sudden increases in learning rates. The authors evaluate their approach, called SGDR, on the CIFAR-10 and CIFAR-100 datasets, achieving new state-of-the-art results. They also demonstrate the advantages of SGDR on a dataset of EEG recordings and a downsampled version of the ImageNet dataset.
Decision and Key Reasons
Based on the evaluation of the paper, I decide to Accept it. The two key reasons for this decision are: (1) the paper proposes a well-motivated and simple approach to improve the convergence of SGD, and (2) the authors provide thorough empirical evaluations on several datasets, demonstrating the effectiveness of their method.
Supporting Arguments
The paper is well-structured, and the authors provide a clear introduction to the problem of optimizing deep neural networks. They also provide a thorough review of related work on restart techniques in gradient-free and gradient-based optimization. The proposed SGDR method is easy to understand, and the authors provide a detailed description of their approach, including the cosine annealing schedule and the warm restart mechanism. The empirical evaluations are comprehensive, and the authors demonstrate the advantages of SGDR over the default learning rate schedule used in previous works.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide additional analysis on the relationship between the network depth and performance, as well as the loss surface for T0 against Tmult. I would also like to see more discussion on the potential applications of SGDR beyond image classification. Some questions I would like the authors to answer include: (1) How do the authors plan to extend their approach to other optimization algorithms, such as AdaDelta and Adam? (2) Can the authors provide more insights into the effect of warm restarts on the convergence of SGD, and how it relates to the concept of multi-modality in the optimization landscape? (3) How do the authors plan to address the potential issue of overfitting when using SGDR with wider networks or larger datasets?