Summary of the Paper's Contributions
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors demonstrate that their approach, called SGDR, can achieve state-of-the-art results on CIFAR-10 and CIFAR-100 datasets, and also show its advantages on a dataset of EEG recordings and a downsampled version of the ImageNet dataset.
Decision and Reasons
I decide to reject this paper, with the main reason being that the paper may not be well-suited for the ICLR conference due to its focus on hardware and circuit design, which is not the primary focus of the conference. Additionally, the main takeaway of the paper, using low-precision to make inference cheaper, is not novel enough as it has been explored in previous papers and last year's ICLR.
Supporting Arguments
The paper's contribution, while significant, is more relevant to the hardware or circuit design community. The authors' approach to improving the performance of SGD is interesting, but it may not be the best fit for the ICLR conference, which focuses on machine learning and artificial intelligence. Furthermore, the idea of using low-precision to make inference cheaper is not new and has been explored in previous works, which reduces the novelty of the paper.
Additional Feedback
To improve the paper, the authors could consider providing more detailed comparisons with existing works on warm restarts and low-precision inference. Additionally, they could explore the application of their approach to other optimization algorithms and datasets. It would also be helpful to provide more insights into the theoretical aspects of their approach and its potential limitations.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how your approach differs from existing works on warm restarts and low-precision inference?
2. How do you plan to address the potential limitations of your approach, such as the need for careful tuning of hyperparameters?
3. Can you provide more insights into the theoretical aspects of your approach and its potential applications to other optimization algorithms and datasets?