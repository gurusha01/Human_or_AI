Summary of the Paper's Contributions
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) training, which regularizes deep neural networks by pruning and then restoring connections. The authors demonstrate the effectiveness of DSD training on various neural network architectures, including CNNs, RNNs, and LSTMs, and achieve significant performance gains on tasks such as image classification, caption generation, and speech recognition.
Decision and Key Reasons
I decide to Accept this paper, with two key reasons:
1. The paper presents a well-motivated and novel approach to regularizing deep neural networks, which addresses the issue of overfitting and improves optimization performance.
2. The authors provide extensive experimental results on various datasets and architectures, demonstrating the effectiveness and consistency of the DSD training method.
Supporting Arguments
The paper is well-organized, and the authors provide a clear explanation of the DSD training flow, including the dense, sparse, and re-dense phases. The experimental results are thorough and convincing, showing significant improvements over baseline models on various tasks. The authors also provide a detailed analysis of the benefits of DSD training, including escaping saddle points, achieving better minima, and regularizing the network.
Additional Feedback and Questions
To further improve the paper, I suggest the authors provide more insights into the choice of sparsity ratio and its impact on the performance of DSD training. Additionally, it would be interesting to see more comparisons with other regularization techniques, such as dropout and DropConnect. I also have the following questions:
* How does the DSD training method handle very deep networks, and are there any limitations to its applicability?
* Can the authors provide more details on the computational cost and memory requirements of DSD training compared to conventional training methods?
* Are there any plans to explore the application of DSD training to other domains, such as natural language processing or reinforcement learning?