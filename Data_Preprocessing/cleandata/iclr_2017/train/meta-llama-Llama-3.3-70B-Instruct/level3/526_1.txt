Summary
The paper proposes a novel key-value attention mechanism for neural language models, which separates output vectors into keys, values, and predict representations. This model outperforms existing memory-augmented neural language models on two corpora, but surprisingly, it mainly utilizes a memory of the previous five representations. The authors also experiment with a simpler N-gram RNN model, which achieves comparable results by concatenating previous output representations.
Decision
I decide to reject this paper, mainly due to two key reasons: (1) the unclear convergence of the Taylor optimum, with a buggy and insufficient proof in Appendix 3, and (2) the mismatch between the theoretical requirements and experimental settings, specifically the learning rate.
Supporting Arguments
The paper's novelty lies in using Taylor approximation to simplify the analysis of the model's behavior, but the convergence of the Taylor optimum is unclear, which raises concerns about the validity of the theoretical guarantees. Furthermore, the experiments do not follow the required learning rate, which could lead to significantly worse regret. These issues undermine the paper's claims and require further development to address these concerns.
Additional Feedback
To improve the paper, the authors should provide a clear and rigorous proof of the convergence of the Taylor optimum and ensure that the experimental settings align with the theoretical requirements. Additionally, the authors may want to consider exploring ways to encourage attending over a longer history, as they mention in the conclusion. Some questions I would like the authors to answer include: (1) Can you provide a corrected proof of the convergence of the Taylor optimum? (2) How do you plan to address the mismatch between the theoretical requirements and experimental settings? (3) What are the potential implications of the model's inability to exploit long-range dependencies, and how do you plan to investigate this further?