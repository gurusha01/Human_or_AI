Summary
The paper proposes a novel approach to compressing fully-connected layers of deep neural networks by introducing a density-diversity penalty regularization. This regularization encourages high sparsity and low diversity in the trained weight matrices, allowing for significant compression of the models without appreciable loss in performance. The authors demonstrate the effectiveness of their approach on the MNIST and TIMIT datasets, achieving compression rates of up to 200X on fully-connected layers while maintaining comparable performance to the original models.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's approach, although novel, may not be as generally applicable as claimed, as the experiments are limited to relatively simple problems and datasets. Secondly, the choice of architecture and hyperparameters used in the paper is unclear and may be limiting, which raises concerns about the robustness of the results.
Supporting Arguments
The paper's approach to compressing fully-connected layers is well-motivated, and the use of a density-diversity penalty regularization is an interesting idea. However, the experiments are limited to the MNIST and TIMIT datasets, which are relatively simple and may not be representative of more complex real-world problems. Additionally, the choice of architecture, specifically the use of one layer with 50 hidden units and softplus activations, is unclear and may be limiting. The paper could benefit from more extensive experiments on more complex problems and datasets to demonstrate the general applicability of the approach.
Additional Feedback
To improve the paper, I suggest that the authors conduct more extensive experiments on more complex problems and datasets to demonstrate the general applicability of their approach. Additionally, the authors should provide more details on the choice of architecture and hyperparameters used in the paper, and explore the use of different architectures and hyperparameters to demonstrate the robustness of the results. The authors should also consider applying their approach to other types of neural networks, such as recurrent neural networks, and exploring the use of different forms of regularization to reduce the diversity of the trained weight matrices.
Questions
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
* Can the authors provide more details on the choice of architecture and hyperparameters used in the paper, and justify why these choices were made?
* How do the authors plan to extend their approach to more complex problems and datasets, and what additional challenges do they anticipate facing in these domains?
* Can the authors provide more information on the computational cost of their approach, and how it compares to other compression methods?
* How do the authors plan to apply their approach to other types of neural networks, such as recurrent neural networks, and what benefits do they expect to achieve in these domains?