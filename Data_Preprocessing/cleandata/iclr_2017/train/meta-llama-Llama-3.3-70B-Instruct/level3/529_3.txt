Summary
The paper proposes a novel extension of Stochastic Gradient Variational Bayes (SGVB) to perform posterior inference for the weights of Stick-Breaking processes, enabling the definition of a Stick-Breaking Variational Autoencoder (SB-VAE). The SB-VAE is a Bayesian nonparametric version of the variational autoencoder, with a latent representation of stochastic dimensionality. The authors demonstrate the effectiveness of the SB-VAE in learning highly discriminative latent representations, outperforming the Gaussian VAE in various experiments.
Decision
I decide to Accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and interesting problem in the field of deep generative models, and (2) the approach is well-motivated and supported by thorough experiments.
Supporting Arguments
The paper combines likelihood and reward-based learning to learn sequence models for music, building on existing literature that unifies inference and learning. The authors provide a clear and well-structured presentation of their work, including a thorough review of the relevant background material and a detailed explanation of their proposed method. The experiments demonstrate the effectiveness of the SB-VAE in learning discriminative latent representations, and the authors provide a thorough analysis of the results.
Additional Feedback
To further improve the paper, I suggest that the authors provide more discussion on the history and reward augmentation in SOC, as well as a more detailed comparison with other methods. Additionally, the authors may want to consider exploring the use of differentiable models to approximate the music theory reward, potentially mitigating the need for an RL approach. I would also like to see more experiments on the robustness of the SB-VAE to different hyperparameters and datasets.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
* Can you provide more details on the choice of the Kumaraswamy distribution as an approximate posterior, and how it compares to other possible choices?
* How do you plan to extend the SB-VAE to more complex datasets and tasks, such as image and speech recognition?
* Can you provide more insight into the computational cost of the SB-VAE, and how it compares to other deep generative models?