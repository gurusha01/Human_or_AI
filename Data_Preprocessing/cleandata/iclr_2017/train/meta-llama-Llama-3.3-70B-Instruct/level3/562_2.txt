Summary
The paper proposes a novel approach to sequence learning, called Incremental Sequence Learning, which involves training a network on the first few steps of each sequence and gradually increasing the length of the sequences used for training. The authors demonstrate the effectiveness of this approach on a new dataset, MNIST pen stroke sequences, and show that it significantly improves sequence learning performance, reducing the test error by 74% and achieving a 20-fold speedup in training time.
Decision
I decide to accept this paper, with two key reasons for this choice: (1) the paper tackles a specific and well-motivated problem in sequence learning, and (2) the approach is well-supported by experimental results, which demonstrate its effectiveness and provide insights into the underlying mechanisms.
Supporting Arguments
The paper is well-motivated, and the authors provide a clear explanation of the problem and the proposed solution. The experimental results are thorough and well-presented, and the authors provide a detailed analysis of the results, including comparisons with other methods and ablation studies. The paper also provides a clear and concise introduction to the background and related work, which helps to situate the proposed approach in the context of existing research.
Additional Feedback
To further improve the paper, I suggest that the authors provide more discussion on the potential applications of Incremental Sequence Learning beyond the MNIST pen stroke sequences dataset. Additionally, it would be interesting to see more analysis on the robustness of the approach to different hyperparameters and network architectures. Finally, the authors may want to consider providing more visualizations of the generated sequences to help illustrate the quality of the results.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the approach to more complex sequence learning tasks, such as language modeling or machine translation? (2) Can the authors provide more insights into the trade-offs between the speedup in training time and the improvement in generalization performance? (3) How do the authors think the approach could be combined with other techniques, such as transfer learning or multi-task learning, to further improve sequence learning performance?