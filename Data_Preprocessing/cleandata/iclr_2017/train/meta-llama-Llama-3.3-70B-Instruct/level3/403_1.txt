Summary of the Paper's Contributions
The paper proposes a novel adversarial learning formulation that enables the discriminator to retain density information, allowing it to be used as a scoring function for sample evaluation. The authors introduce a flexible adversarial training framework, which they prove ensures the generator converges to the true data distribution while enabling the discriminator to retain density information at the global optimum. They derive the analytic form of the induced solution and analyze its properties, providing a rigorous characterization of the learned discriminator in the non-parametric setting. The authors also propose two methods for instantiating the formulation in the parametric setting and verify their results empirically on several synthetic and real datasets.
Decision and Key Reasons
Based on the provided guidelines, I decide to Accept this paper. The key reasons for this decision are:
1. The paper tackles a specific question/problem (enabling the discriminator to retain density information) and provides a well-motivated approach to addressing it.
2. The approach is well-placed in the literature, building upon existing work on Generative Adversarial Networks (GANs) and Inverse Reinforcement Learning (IRL).
3. The paper provides a rigorous theoretical analysis of the proposed formulation, including a characterization of the optimal discriminator and its properties.
Supporting Arguments
The paper's contributions are significant, as they address a fundamental limitation of GANs, which is their inability to provide sensible energy estimates for samples. The proposed formulation is novel and interesting, and the paper is well-written and easy to follow. The experimental results verify the theoretical analysis, demonstrating that the discriminator obtained from the entropy-regularized adversarial training can capture density information while ensuring the generator distribution matches the data distribution.
Additional Feedback and Questions
To further improve the paper, I would like to see more discussion on the limitations of the approach, particularly with regards to the choice of the convex function K(pgen) and its impact on the optimal discriminator. Additionally, it would be interesting to explore the connection between the proposed formulation and other existing approaches, such as energy-based GANs.
Some questions I would like the authors to address are:
* How does the choice of K(pgen) affect the optimal discriminator, and are there any specific guidelines for selecting this function?
* Can the proposed formulation be extended to other types of generative models, such as Variational Autoencoders (VAEs)?
* How does the performance of the proposed approach compare to other methods for evaluating sample quality, such as Inception Score or Fr√©chet Inception Distance?