Summary
The paper proposes a differentiable version of Canonical Correlation Analysis (CCA), which enables backpropagation through the computation of CCA. This allows CCA to be used as a building block within multi-modality neural networks, producing maximally-correlated projections of its inputs. The authors demonstrate the effectiveness of this approach in cross-modality retrieval experiments on two public image-to-text datasets, surpassing both Deep CCA and a multi-view network with freely-learned projections.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the technical quality of the paper is lacking, and the authors should provide more rigorous analysis of their approach. Secondly, the paper lacks baselines, such as comparing the neural network to simple linear or quadratic models, or heuristic rules, to understand the complexity of the weight evolution learned by the neural network.
Supporting Arguments
The paper's main idea is interesting, but the technical quality is not sufficient to support the claims. The authors should provide more details on the training process, including training, validation, and test losses, and clarify the statement that RNNs did not work as the introspection network. Additionally, the use of default TensorFlow example hyperparameters is not justified, and the authors should select hyperparameters that produce good results in a reasonable time as the baseline.
Additional Feedback
To improve the paper, the authors should provide more rigorous analysis of their approach, including a clear presentation of the findings in Section 3.0, and more details on the training process. The authors should also provide baselines to compare the neural network to simple linear or quadratic models, or heuristic rules, to understand the complexity of the weight evolution learned by the neural network. Furthermore, the authors should clarify the statement that RNNs did not work as the introspection network and provide more details on what "didn't work" means in this context.
Questions
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence:
1. Can you provide more details on the training process, including training, validation, and test losses?
2. Can you clarify the statement that RNNs did not work as the introspection network and provide more details on what "didn't work" means in this context?
3. How do you justify the use of default TensorFlow example hyperparameters, and can you select hyperparameters that produce good results in a reasonable time as the baseline?