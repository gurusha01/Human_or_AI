Summary
The paper proposes a novel Group Sparse Autoencoders (GSA) framework, which functions as dictionary learning and sparse coding models with inter- and intra-group sparse constraints. The authors also introduce a Group Sparse Convolutional Neural Networks (GSCNNs) by embedding GSA into CNNs, demonstrating that CNNs can benefit from GSA by learning more meaningful representations from dictionaries. The proposed model shows significant improvements over strong baselines on four datasets.
Decision
I decide to Accept this paper with one key reason being the novelty and effectiveness of the proposed GSA framework and its application to CNNs. The paper provides a clear understanding of the problem, a well-motivated approach, and empirical evidence to support the claims.
Supporting Arguments
The paper tackles a specific question of how to effectively utilize answer data to improve question representation in question classification tasks. The approach is well-motivated, drawing inspiration from dictionary learning and sparse coding models. The authors provide a clear explanation of the GSA framework and its application to CNNs, along with empirical evidence to support the claims. The results show significant improvements over strong baselines on four datasets, demonstrating the effectiveness of the proposed model.
Additional Feedback
To further improve the paper, I suggest the authors provide more discussion on the relationship between the proposed GSA framework and other sparse coding approaches, such as Sparse Group Lasso (SGL). Additionally, it would be helpful to include more analysis on the sensitivity of the hyperparameters, such as the sparsity parameters and the number of groups, to provide a better understanding of the model's behavior. Furthermore, the authors may consider exploring the application of the proposed model to other tasks, such as text classification or information retrieval, to demonstrate its generalizability.
Questions for the Authors
1. Can you provide more insight into the choice of hyperparameters, such as the sparsity parameters and the number of groups, and how they affect the performance of the model?
2. How does the proposed GSA framework compare to other sparse coding approaches, such as SGL, in terms of computational complexity and optimization?
3. Have you considered exploring the application of the proposed model to other tasks, such as text classification or information retrieval, and if so, what are the preliminary results?