Summary
The paper proposes a novel method for finding dependent subspaces across multiple views of data, with a focus on preserving neighborhood relationships. The approach directly maximizes the similarity of neighborhoods between subspaces of each view, using a well-defined objective function that favors sparse and informative neighborhoods. The method is shown to outperform traditional Canonical Correlation Analysis (CCA) and other state-of-the-art methods in experiments on artificial and real-world datasets.
Decision
I decide to Accept this paper, with two key reasons: (1) the paper tackles a well-motivated and important problem in data analysis, and (2) the approach is well-supported by theoretical and empirical results, demonstrating its effectiveness in finding dependent subspaces.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of finding dependent subspaces, and motivates the need for a new approach that focuses on neighborhood relationships. The proposed method is well-defined and easy to understand, with a clear objective function that captures the desired properties of dependent subspaces. The experiments demonstrate the effectiveness of the method in various scenarios, including artificial and real-world datasets with multiple views.
Additional Feedback
To further improve the paper, I suggest the authors consider the following points: (1) provide more insights into the choice of hyperparameters, such as the value of Îµ in the smoothed neighbor distribution, and (2) explore the application of the method to more complex datasets, such as those with non-linear relationships between views. Additionally, it would be interesting to see a comparison with other methods that focus on neighborhood relationships, such as those used in graph-based learning.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) Can you provide more details on the optimization technique used to maximize the objective function, and how it handles local optima? (2) How do you choose the dimensionality of the subspaces in the experiments, and are there any guidelines for selecting the optimal dimensionality in practice? (3) Have you considered extending the method to handle non-linear transformations between views, and if so, what are the potential challenges and benefits of such an extension?