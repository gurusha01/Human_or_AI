Summary of the Paper
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors introduce a new learning rate schedule, called SGDR, which simulates warm restarts by scheduling the learning rate to achieve competitive results on CIFAR-10 and CIFAR-100 roughly two to four times faster. The paper also demonstrates the advantages of SGDR on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset.
Decision
I decide to reject this paper, not because of its lack of merit, but because I feel unqualified to review it due to my limited background in digital logic. The paper tackles a specific question in the field of deep learning, and its approach is well-motivated and well-placed in the literature. However, I believe that a more sophisticated reviewer with expertise in the area would be better equipped to evaluate the paper's claims and provide more insightful feedback.
Supporting Arguments
The paper's approach is well-motivated, and the authors provide a clear explanation of the problem they are trying to solve. The experimental results are also impressive, demonstrating the effectiveness of SGDR on several datasets. However, I lack the expertise to fully evaluate the paper's technical contributions and to provide detailed feedback on its strengths and weaknesses.
Additional Feedback
To improve the paper, I suggest that the authors consider submitting it to another venue where it can receive a more suitable review. Additionally, the authors may want to consider providing more background information on the related work in the field, as well as more details on the experimental setup and the hyperparameter tuning process. The authors may also want to explore the application of SGDR to other domains and datasets to further demonstrate its effectiveness.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the hyperparameter tuning process and how the values of T0 and Tmult were chosen?
2. How do you plan to extend the application of SGDR to other domains and datasets?
3. Can you provide more insights into the theoretical foundations of SGDR and its relationship to other optimization algorithms?