Summary
The paper presents a comprehensive analysis of vocabulary selection techniques for neural machine translation, exploring the trade-off between speed and accuracy for different vocabulary sizes. The authors extend previous work by considering a wide range of simple and complex selection techniques, including bilingual word co-occurrence counts, bilingual embeddings, word alignments, phrase pairs, and discriminative SVM classifiers. The experiments demonstrate that decoding speed can be reduced by up to 90% without compromising accuracy, and training speed can be increased by up to 25% with a bi-directional LSTM encoder.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in neural machine translation, namely the efficiency of decoding and training. Secondly, the approach is well-motivated and well-placed in the literature, with a clear and comprehensive analysis of various selection techniques.
Supporting Arguments
The paper provides a thorough evaluation of different selection techniques, including their strengths and weaknesses. The experiments are well-designed and provide valuable insights into the trade-off between speed and accuracy. The authors also provide a detailed analysis of the results, highlighting the importance of efficient encoder and decoder architectures. Additionally, the paper is well-written and easy to follow, with clear explanations of the techniques and results.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed analysis of the computational costs of each selection technique, as well as their impact on the overall efficiency of the neural machine translation system. Additionally, it would be interesting to see more experiments on other language pairs and datasets, to further demonstrate the effectiveness of the proposed techniques.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the selection techniques perform on out-of-vocabulary words, and what strategies can be used to handle such cases?
* Can the authors provide more details on the computational costs of each selection technique, and how they compare to the costs of the full vocabulary model?
* How do the results change when using different encoder and decoder architectures, such as transformer-based models?