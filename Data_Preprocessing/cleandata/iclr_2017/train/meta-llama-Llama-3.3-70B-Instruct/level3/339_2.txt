Summary
The paper proposes a novel extension to neural network language models, called the Neural Cache Model, which adds a cache component to improve the model's functionality. This cache stores recent hidden activations and uses them to define a probability distribution over words, allowing the model to adapt to its recent history. The authors demonstrate the effectiveness of their approach on several language modeling datasets and the LAMBADA dataset, showing significant performance gains over existing models.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and supported by empirical results. The authors provide a clear explanation of the Neural Cache Model and its relationship to existing memory-augmented neural networks and cache models. The experimental results demonstrate the effectiveness of the approach, particularly on the LAMBADA dataset, which is a challenging task for language models.
Supporting Arguments
The paper tackles a specific question of how to improve the functionality of neural network language models by adapting to their recent history. The approach is well-motivated, drawing on existing research in memory-augmented neural networks and cache models. The authors provide a clear explanation of the Neural Cache Model and its advantages over existing approaches, including its ability to use larger cache sizes and avoid learning the memory lookup component. The experimental results are thorough and demonstrate the effectiveness of the approach on several datasets.
Additional Feedback
To further improve the paper, I would suggest providing more details on the specifics of what is stored in the cache component and how it is used to define the probability distribution over words. Additionally, it would be helpful to provide more analysis on the computational cost of the Neural Cache Model and how it compares to existing memory-augmented neural networks. Finally, the authors may want to consider exploring other applications of the Neural Cache Model beyond language modeling, such as question answering or machine translation.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* Can you provide more details on the specifics of what is stored in the cache component and how it is used to define the probability distribution over words?
* How does the Neural Cache Model handle out-of-vocabulary words, and are there any plans to explore this further in future work?
* Can you provide more analysis on the computational cost of the Neural Cache Model and how it compares to existing memory-augmented neural networks?