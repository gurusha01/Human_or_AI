This paper proposes a novel approach to adjusting for the variance introduced by dropout in neural networks, through corrections to weight initialization and Batch Normalization. The authors demonstrate that their method yields higher accuracy and faster convergence on various datasets, including CIFAR-10 and CIFAR-100, without data augmentation. The paper is well-motivated, and the approach is well-placed in the literature, building upon existing weight initialization techniques and Batch Normalization.
I decide to accept this paper, with the primary reason being the significant improvement in accuracy achieved by the proposed method on several benchmark datasets. The authors provide a thorough analysis of the effects of dropout on variance and demonstrate the effectiveness of their approach through extensive experiments.
To support this decision, I note that the paper provides a clear and concise explanation of the proposed method, and the experiments are well-designed and thoroughly executed. The results are impressive, with the proposed method achieving state-of-the-art performance on CIFAR-10 and CIFAR-100 without data augmentation.
However, I do have some suggestions for improvement. Firstly, I recommend including state-of-the-art comparisons to better understand the quality of the results. For example, using denoising autoencoders on pixel patches could provide a more comprehensive evaluation of the proposed method. Additionally, the difference in implementation between SCT-R and FOV-R conditions, where part of the image is set to zero in SCT-R and removed in FOV-R, may affect the results and should be considered.
To further improve the paper, I suggest recalculating the comparison between FOV-R and DS-D performance to only consider the error in the periphery, to accurately assess the additional information extracted from the fovea. Moreover, the images in Figure 2 appear blurry due to interpolation, and using "nearest" interpolation could provide a more accurate representation. The caption for Figure 3 is too vague and should be clarified, possibly with an added footnote. Finally, the placement of figures in the paper is often too early, resulting in a large distance between the text and the corresponding figures.
I would like the authors to answer the following questions to clarify my understanding of the paper: (1) How do the authors plan to extend their method to other types of neural networks, such as recurrent neural networks or graph neural networks? (2) Can the authors provide more insight into the computational cost of their method, particularly in comparison to Batch Normalization? (3) How do the authors think their method could be applied to real-world problems, such as image classification or object detection?