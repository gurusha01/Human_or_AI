This paper proposes a novel sequence learning approach, RL Tuner, which combines supervised learning and reinforcement learning to refine a pre-trained Recurrent Neural Network (RNN) for sequence generation tasks. The approach uses a reward function that incorporates both the probability of a given action learned from data and music theory-based rules to impose structure on the generated sequences. The authors demonstrate the effectiveness of RL Tuner in generating more pleasing and structured melodies compared to a baseline RNN model.
However, I decide to reject this paper for publication at ICLR 2017 for two main reasons. Firstly, the paper lacks comparison to other recent innovations in the field, such as SeqGAN and MIXER, which have also explored the use of reinforcement learning for sequence generation tasks. This makes it difficult to assess the novelty and significance of the proposed approach. Secondly, the paper's claim that batch normalization is ineffective for RNNs is incorrect and contradicts existing research, which undermines the credibility of the authors' claims.
The proposed method is also extremely simple and similar to previous training strategies, making its contribution incremental at best without strong empirical results. Furthermore, the paper is now outdated and lacks empirical comparison to other training strategies and algorithms, making it unsuitable for publication at ICLR 2017.
To improve the paper, I suggest that the authors provide a more comprehensive review of related work, including recent approaches that have explored the use of reinforcement learning for sequence generation tasks. Additionally, the authors should provide more rigorous empirical evaluations, including comparisons to other state-of-the-art methods, to demonstrate the effectiveness of their approach. I would also like the authors to clarify how their approach differs from existing methods, such as SeqGAN and MIXER, and what specific advantages their approach offers.
Some questions I would like the authors to answer include: How does the proposed approach differ from existing methods, such as SeqGAN and MIXER? What specific advantages does the proposed approach offer over existing methods? How do the authors plan to address the issue of outdatedness and lack of empirical comparison to other training strategies and algorithms? What additional experiments or evaluations would the authors propose to demonstrate the effectiveness of their approach?