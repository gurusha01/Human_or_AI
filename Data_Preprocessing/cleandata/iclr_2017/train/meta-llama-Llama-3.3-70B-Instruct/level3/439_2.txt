Summary
The paper proposes a novel recurrent neural network architecture, called multiplicative LSTM (mLSTM), which combines the strengths of long short-term memory (LSTM) and multiplicative recurrent neural network (mRNN) architectures. The authors demonstrate the effectiveness of mLSTM in character-level language modeling tasks, showing significant improvements over standard LSTM and its deep variants. The paper is well-written, and the main story is well-described, making it easy to follow.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The authors provide a clear explanation of the limitations of traditional RNN architectures and how mLSTM addresses these limitations. The experimental results demonstrate the effectiveness of mLSTM in various character-level language modeling tasks.
Supporting Arguments
The paper tackles a specific question of how to improve the performance of RNNs in sequence modeling tasks. The approach is well-motivated, and the authors provide a clear explanation of the limitations of traditional RNN architectures and how mLSTM addresses these limitations. The experimental results demonstrate the effectiveness of mLSTM in various character-level language modeling tasks, including the Hutter Prize dataset, where mLSTM achieves a state-of-the-art result with dynamic evaluation.
Additional Feedback
To further improve the paper, I would like to see more analysis on the computational expense of the search-based procedure and how it affects the overall performance of mLSTM. Additionally, it would be helpful to see more experiments on the robustness of mLSTM to different types of noise and perturbations. The authors may also want to consider exploring the application of mLSTM to other sequence modeling tasks, such as word-level language modeling or speech recognition.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* Can you provide more details on the computational expense of the search-based procedure and how it affects the overall performance of mLSTM?
* How does the performance of mLSTM change when the input data is noisy or perturbed?
* Have you considered exploring the application of mLSTM to other sequence modeling tasks, such as word-level language modeling or speech recognition?
* Can you provide more insights into the interaction effect between the number of languages and the performance of mLSTM in multilingual learning tasks?