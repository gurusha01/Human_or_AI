This paper proposes a novel neural language model with a key-value attention mechanism, which outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. The authors demonstrate that this model outperforms existing memory-augmented neural language models on two corpora, and that a much simpler model based on the concatenation of recent output representations is on par with more sophisticated models.
I found the paper hard to follow due to the extensive notation, but the ideas and approaches presented are good and worth publishing. However, I think that the paper's current form is too dense for a short conference paper, and I would consider raising my score if the paper were revised substantially to provide clearer explanations and examples.
To answer the conference guidelines, I would say that:
1. The specific question/problem tackled by the paper is how to effectively utilize attention mechanisms in neural language models to capture long-range dependencies.
2. The approach is well-motivated, and the authors provide a clear overview of the related work in the field.
3. The paper supports its claims with experimental results, but I have some doubts about the scalability of the approach, particularly when dealing with larger and more complex datasets.
My decision is to reject the paper, mainly due to its density and lack of clarity. However, I think that the paper has potential and could be improved with revisions.
Some additional feedback I would like to provide is that the authors could benefit from adding more intuitive explanations of their models and algorithms, as well as more visualizations to help illustrate the key concepts. Additionally, I would like to see more analysis of the results, particularly in terms of how the models perform on different types of dependencies (e.g. short-range vs. long-range).
Some questions I would like the authors to answer are:
* How do the authors plan to address the issue of scalability, particularly when dealing with larger datasets?
* Can the authors provide more insight into why the simpler N-gram RNN model performs so well, and whether this is due to the specific characteristics of the datasets used?
* How do the authors think their approach could be extended to other NLP tasks, such as machine translation or question answering?