Summary
The paper proposes a novel approach to combining algorithms and machine learning for state estimation in robotics. The authors introduce end-to-end learnable histogram filters (E2E-HFs), which encode problem-specific information from algorithms and learn task-specific details from data. The E2E-HFs are demonstrated to be more data-efficient and perform better than traditional machine learning approaches, such as long-short-term memory networks (LSTMs). The paper also shows that E2E-HFs can learn state estimation without state labels, making them a promising approach for unsupervised learning.
Decision
I decide to Accept this paper, with the main reason being that it presents a well-motivated and novel approach to combining algorithms and machine learning, which is supported by thorough experiments and results.
Supporting Arguments
The paper is well-structured and easy to follow, with a clear introduction to the problem and the proposed approach. The authors provide a thorough review of related work and demonstrate a good understanding of the strengths and weaknesses of different approaches. The experiments are well-designed and provide strong evidence for the effectiveness of E2E-HFs. The results show that E2E-HFs outperform traditional machine learning approaches, such as LSTMs, and can learn state estimation without state labels.
Additional Feedback
To further improve the paper, I suggest that the authors consider exploring the loss incurred when moving from restrictive to less restrictive transfer learning approaches. This could provide additional insight into the trade-offs between data efficiency and generality. Additionally, the authors may want to consider providing more details on the computational complexity of the E2E-HF approach and its scalability to higher-dimensional problems.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on how the E2E-HF approach handles high-dimensional state spaces, and what are the potential limitations of the approach in terms of scalability?
2. How do the authors plan to address the issue of computational complexity, which is mentioned as a potential bottleneck for scaling the approach to higher-dimensional problems?
3. Can you provide more insight into the choice of the histogram filter as the algorithmic prior, and how it compares to other potential choices, such as Kalman filters or particle filters?