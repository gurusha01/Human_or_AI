Summary of the Paper's Contributions
The paper presents a novel Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the expressive power of RNN language models. The NKLM significantly outperforms the RNNLM in terms of perplexity and generates named entities that are not observed during training. The model also adapts immediately to changes in knowledge. The paper introduces a new dataset, WikiFacts, and a new evaluation metric, Unknown-Penalized Perplexity (UPP), to address the limitations of standard perplexity in evaluating language models for knowledge-related tasks.
Decision and Key Reasons
I decide to accept this paper with minor revisions. The key reasons for this decision are: (1) the paper presents a well-motivated and novel approach to incorporating symbolic knowledge into RNN language models, and (2) the experimental results demonstrate significant improvements in perplexity and named entity generation.
Supporting Arguments
The paper is well-written, and the authors provide a clear and thorough explanation of the NKLM model and its components. The experimental results are convincing, and the introduction of the WikiFacts dataset and the UPP evaluation metric addresses an important limitation of standard perplexity. The paper also provides a thorough analysis of the results and discusses the implications of the NKLM model for knowledge-related language tasks.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors provide more details on the training process, including the hyperparameter tuning and the convergence of the model. Additionally, it would be interesting to see more examples of the generated text and named entities to better understand the capabilities of the NKLM model. I also have the following questions: (1) How does the NKLM model handle out-of-vocabulary words that are not present in the knowledge graph? (2) Can the NKLM model be applied to other knowledge-related tasks, such as question answering or dialogue modeling? (3) How does the NKLM model compare to other state-of-the-art language models, such as transformer-based models?