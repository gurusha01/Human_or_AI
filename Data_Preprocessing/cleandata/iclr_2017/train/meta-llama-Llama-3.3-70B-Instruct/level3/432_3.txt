Summary of the Paper
The paper proposes a new technique called Policy Gradient and Q-Learning (PGQL) that combines policy gradient and Q-learning to improve learning in the Atari Learning Environment. The authors establish a connection between the fixed points of the regularized policy gradient algorithm and the Q-values, allowing them to estimate the Q-values from the action preferences of the policy. They then apply Q-learning updates to refine the Q-values using off-policy data from a replay buffer. The authors demonstrate the effectiveness of PGQL through numerical experiments on a grid world and the Atari games suite, achieving improved data efficiency and stability compared to actor-critic and Q-learning alone.
Decision
Based on the review, I decide to accept the paper with minor revisions. The paper presents an interesting and well-motivated approach to combining policy gradient and Q-learning, and the experimental results demonstrate its effectiveness.
Supporting Arguments
The paper provides a clear and well-structured introduction to the problem and the proposed approach. The authors establish a connection between policy gradient and Q-learning, which is a significant contribution to the field. The experimental results are thorough and demonstrate the effectiveness of PGQL in improving data efficiency and stability.
Additional Feedback
To improve the paper, I suggest the following:
1. Clarify the notation and introduction of certain concepts, such as \tilde{Q}^pi, to avoid confusion for readers.
2. Consider removing or expanding Section 3.2 to improve clarity and provide more insight into the general case.
3. Provide more explicit and detailed links to dueling networks and other related work.
4. Clarify the relationship between Section 3.2 and Section 3.3 to improve the flow of the paper.
5. Address minor remarks, such as undefined functions, confusing definitions, and typos throughout the paper.
6. Provide more clarification on several points, including the use of alpha, critic estimate of Q(s, a), and Q-learning step implementation.
Questions for the Authors
1. Can you provide more insight into the choice of alpha and its effect on the performance of PGQL?
2. How does the critic estimate of Q(s, a) affect the performance of PGQL, and what are the implications of using a different critic estimate?
3. Can you provide more details on the implementation of the Q-learning step in PGQL, including the choice of hyperparameters and the effect of different hyperparameters on performance?