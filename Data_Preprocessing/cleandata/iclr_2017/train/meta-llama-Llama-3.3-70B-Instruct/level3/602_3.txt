The paper proposes a novel loss framework for language modeling, which augments the conventional cross-entropy loss with an additional term that minimizes the KL-divergence between the model's prediction and an estimated target distribution based on the word embeddings space. This framework leads to state-of-the-art performance on the Penn Treebank corpus and Wikitext-2 dataset. The main contribution of the paper is the theoretically driven reuse of word embeddings, which reduces the number of trainable variables in the model and improves learning.
Based on the provided guidelines, I decide to Accept this paper with two key reasons: (1) the paper tackles a specific and well-motivated problem in language modeling, and (2) the approach is well-supported by theoretical analysis and empirical results.
The paper provides a clear and well-structured presentation of the proposed framework, including a thorough analysis of the augmented loss and its effect on the output projection matrix. The experimental results demonstrate the effectiveness of the proposed approach, with significant improvements over the baseline model on both datasets. The paper also provides a detailed comparison with previous work and discusses the implications of the proposed framework for other tasks that utilize language models.
To further improve the paper, I suggest that the authors provide more discussion on the limitations of the proposed framework and potential future directions. For example, how does the proposed framework perform on other datasets or tasks, and what are the potential applications of the theoretically driven reuse of word embeddings? Additionally, the authors may want to consider providing more visualizations or examples to illustrate the effectiveness of the proposed approach.
I would like to ask the authors to clarify the following points: (1) How does the temperature parameter τ affect the performance of the proposed framework, and what is the optimal value for τ? (2) Can the authors provide more insights into the theoretical analysis of the augmented loss and its effect on the output projection matrix? (3) How does the proposed framework compare to other recent advances in language modeling, such as recurrent highway networks or pointer networks?