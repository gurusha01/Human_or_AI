This paper proposes a novel approach to generating transferable adversarial examples for deep neural networks, with a focus on both non-targeted and targeted attacks. The authors introduce an ensemble-based method that generates adversarial examples for multiple models, which are then used to attack a black-box model. The results show that the proposed approach can generate transferable targeted adversarial examples with a high success rate, outperforming existing methods.
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper tackles a specific and important problem in the field of deep learning, namely the transferability of adversarial examples. Secondly, the approach proposed by the authors is well-motivated and supported by extensive experimental results, demonstrating the effectiveness of the ensemble-based method in generating transferable adversarial examples.
The paper provides a thorough analysis of the transferability of non-targeted and targeted adversarial examples, and the results are numerous and convincing. The authors also provide a geometric interpretation of the results, which helps to understand the underlying mechanisms of transferable adversarial examples. The evaluation of the proposed approach on a black-box image classification system, Clarifai.com, demonstrates the practical significance of the results.
To improve the paper, I would suggest providing more details on the implementation of the ensemble-based approach, such as the choice of hyperparameters and the optimization algorithm used. Additionally, it would be interesting to see more analysis on the geometric properties of the models, and how they relate to the transferability of adversarial examples.
I would like to ask the authors to clarify the following points: (1) How did you choose the hyperparameters for the ensemble-based approach, and what is the sensitivity of the results to these hyperparameters? (2) Can you provide more details on the optimization algorithm used to generate the adversarial examples, and how it affects the transferability of the results? (3) How do you think the geometric properties of the models, such as the orthogonality of the gradient directions, affect the transferability of adversarial examples?