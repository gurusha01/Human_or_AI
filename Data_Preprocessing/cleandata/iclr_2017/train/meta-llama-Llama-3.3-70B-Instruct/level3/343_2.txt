Summary
The paper proposes a novel framework for multitask deep reinforcement learning guided by policy sketches. The approach associates each subtask with its own modular subpolicy and jointly optimizes over full task-specific policies by tying parameters across shared subpolicies. The authors evaluate the effectiveness of their approach on a maze navigation game and a 2-D Minecraft-inspired crafting game, demonstrating that it outperforms standard baselines and induces a library of primitive behaviors that can be recombined to rapidly acquire policies for new tasks.
Decision
I decide to accept this paper with minor revisions. The key reasons for this choice are the innovative approach to multitask reinforcement learning and the strong empirical results on modeling code. However, I have some concerns regarding the vagueness of the synthesis procedure using MCMC and the lack of basic information in the training time evaluation.
Supporting Arguments
The paper presents a well-motivated approach to multitask reinforcement learning, and the authors provide a clear explanation of the related work in the field. The experimental results demonstrate the effectiveness of the proposed approach, and the authors provide a thorough analysis of the importance of various components of the training procedure. However, I would like to see more compact and convincing examples of human interpretability to support the method's effectiveness.
Additional Feedback
To improve the paper, I suggest that the authors provide more details on the synthesis procedure using MCMC and include basic information such as hardware specifications in the training time evaluation. Additionally, the related work section could be expanded to better put the current work in context, particularly in relation to the PL literature. I would also like to see more analysis on the adaptability of the proposed approach to different environments and tasks.
Questions for the Authors
1. Can you provide more details on the synthesis procedure using MCMC and how it is used to generate the policy sketches?
2. How do you plan to address the issue of overfitting in the proposed approach, particularly when dealing with complex tasks?
3. Can you provide more examples of human interpretability and how the proposed approach can be used to generate interpretable policies?