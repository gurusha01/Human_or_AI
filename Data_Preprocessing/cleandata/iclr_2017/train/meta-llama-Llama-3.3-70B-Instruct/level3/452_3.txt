This paper proposes a novel approach to learning activation functions in deep neural networks using nonparametric estimation. The authors introduce a class of nonparametric models for activation functions, which can be incorporated into the back-propagation framework, allowing for easy integration into existing deep learning architectures. The paper provides a theoretical justification for the choice of nonparametric activation functions and demonstrates their effectiveness on several benchmark datasets, including MNIST and CIFAR-10.
The paper tackles the specific question of how to learn activation functions in deep neural networks, which is a crucial aspect of deep learning. The approach is well-motivated, as it addresses the limitation of current deep learning methods, which treat the choice of activation function as a hyperparameter. The paper provides a clear and concise introduction to the problem, related work, and the proposed approach.
The paper supports its claims with empirical results, demonstrating the effectiveness of the proposed approach on several datasets. The results show that the nonparametric activation functions can improve the performance of deep neural networks, especially when used in conjunction with dropout. The paper also provides a theoretical analysis of the generalization bounds for the proposed approach, which is a significant contribution to the field.
However, there are some areas that require improvement. The introduction statement about all DRL algorithms repeatedly executing a chosen action for a fixed number of time steps is too strong and should be rephrased to be more precise. The related work section lacks a discussion on the relation to semi-MDPs, which would help readers understand the approach and its comparisons.
To improve the paper, I would suggest adding more details on the experimental setup, such as the number of random seeds used and the specific hyperparameters tuned. Additionally, the paper could benefit from more visualizations, such as plots of the learned activation functions, to help illustrate the results.
Overall, I would accept this paper, as it presents a novel and well-motivated approach to learning activation functions in deep neural networks, with a clear and concise introduction, related work, and empirical results. However, I would suggest some revisions to address the areas mentioned above.
Some questions I would like the authors to answer to clarify my understanding of the paper are:
* Can you provide more details on the experimental setup, such as the number of random seeds used and the specific hyperparameters tuned?
* How do the learned activation functions compare to traditional activation functions, such as ReLU and sigmoid?
* Can you provide more visualizations, such as plots of the learned activation functions, to help illustrate the results?
* How does the proposed approach relate to other methods for learning activation functions, such as evolutionary algorithms or reinforcement learning?