Summary
The paper proposes a neural network architecture and statistical framework for modeling frames in videos using principles inspired by computer graphics pipelines. The approach, called Perception Updating Networks (PUN), explicitly represents "sprites" or percepts inferred from the maximum likelihood of the scene and infers their movement independently of their content. The authors demonstrate the effectiveness of PUN on synthetic datasets, including bouncing shapes and moving MNIST, and show that it can generate videos that are interpretable and better suited for long video generation.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the comparison between Adam and Eve optimizers is not convincing, as the authors only show that Eve converges for a specific learning rate of 0.1, and do not provide a thorough analysis of the hyperparameters. Secondly, the authors claim that Eve always converges, but this is not supported by the experimental results, which only show convergence for a limited set of experiments.
Supporting Arguments
The paper lacks a thorough comparison between the proposed PUN architecture and other state-of-the-art methods, such as Video Pixel Networks. While the authors show that PUN outperforms a baseline LSTM on the moving MNIST dataset, the results are not as good as those obtained by Video Pixel Networks. Additionally, the authors do not provide a clear analysis of the hyperparameters used in the experiments, which makes it difficult to reproduce the results.
Additional Feedback
To improve the paper, the authors should provide a more thorough comparison between PUN and other state-of-the-art methods, including a detailed analysis of the hyperparameters used in the experiments. Additionally, the authors should consider using more advanced techniques, such as PixelCNN decoders, to generate sprites with higher likelihood. The authors should also provide more visualizations and examples to illustrate the effectiveness of PUN in generating interpretable videos.
Questions for the Authors
1. Can you provide more details on the hyperparameters used in the experiments, including the learning rate, batch size, and number of epochs?
2. How do you plan to improve the performance of PUN on more complex datasets, such as real-world videos?
3. Can you provide more visualizations and examples to illustrate the effectiveness of PUN in generating interpretable videos?