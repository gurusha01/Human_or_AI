Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and is designed to address the vanishing and exploding gradient problems in recurrent neural networks. The authors derive the gradients of the Normalized LSTM and propose a scheme to initialize the weight matrices. They evaluate the performance of the Normalized LSTM on two tasks: character-level language modeling and image generative modeling, and show that it compares favorably to other state-of-the-art approaches.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and the results are promising. The paper provides a clear and detailed explanation of the proposed method, and the experiments demonstrate its effectiveness in addressing the vanishing and exploding gradient problems.
Supporting Arguments
The paper tackles a specific and important problem in recurrent neural networks, and the proposed approach is well-placed in the literature. The authors provide a thorough analysis of the gradients of the Normalized LSTM and propose a scheme to initialize the weight matrices, which is a significant contribution. The experimental results are also convincing, showing that the Normalized LSTM compares favorably to other state-of-the-art approaches.
Additional Feedback
To further improve the paper, I would suggest providing more details on the computational complexity of the proposed approach and comparing it to other methods. Additionally, it would be interesting to see more experiments on other tasks and datasets to further demonstrate the effectiveness of the Normalized LSTM. The authors may also want to consider providing more insights into the trade-offs between the different hyperparameters and how they affect the performance of the model.
Questions for the Authors
I would like to ask the authors to clarify the following points:
* How do the authors plan to extend the proposed approach to more complex recurrent neural network architectures, such as gated recurrent units (GRUs) or bidirectional LSTMs?
* Can the authors provide more insights into the effect of the initialization scheme on the performance of the Normalized LSTM, and how it compares to other initialization schemes?
* How do the authors plan to address the potential issue of overfitting in the Normalized LSTM, particularly when using recurrent dropout?