Summary
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) training, which regularizes deep neural networks by pruning and then restoring connections. The authors demonstrate the effectiveness of DSD training on various neural networks, including CNNs, RNNs, and LSTMs, on tasks such as image classification, caption generation, and speech recognition. The results show significant improvements in performance compared to conventional training methods.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's contribution, while novel, is relatively straightforward and does not significantly advance the state-of-the-art in neural network training. Secondly, the paper's evaluation is limited to a few specific tasks and datasets, and it is unclear whether the proposed method will generalize to other domains.
Supporting Arguments
The paper's proposal of DSD training is well-motivated, and the authors provide a clear explanation of the method and its components. However, the evaluation of the method is limited to a few specific tasks and datasets, and it is unclear whether the proposed method will generalize to other domains. Additionally, the paper's comparison to other regularization methods, such as dropout and distillation, is limited, and it is unclear whether DSD training provides significant advantages over these methods.
Additional Feedback
To improve the paper, the authors should consider evaluating the proposed method on a wider range of tasks and datasets, and providing a more comprehensive comparison to other regularization methods. Additionally, the authors should consider providing more insight into the underlying mechanisms of DSD training, and how it relates to other regularization methods. Some specific questions that I would like the authors to address include: How does DSD training relate to other regularization methods, such as dropout and distillation? Can the authors provide more insight into the underlying mechanisms of DSD training, and how it affects the optimization process? How does the choice of sparsity ratio affect the performance of DSD training, and are there any guidelines for selecting this hyperparameter?
Questions for the Authors
I would like the authors to address the following questions: Can you provide more insight into the underlying mechanisms of DSD training, and how it affects the optimization process? How does the choice of sparsity ratio affect the performance of DSD training, and are there any guidelines for selecting this hyperparameter? Can you provide a more comprehensive comparison to other regularization methods, such as dropout and distillation? How does DSD training relate to other regularization methods, and are there any potential advantages or disadvantages of using DSD training in combination with other regularization methods?