Summary of the Paper's Contributions
The paper presents a significant contribution to the field of model-based reinforcement learning (RL) by demonstrating the feasibility of learning a joint model of system dynamics and reward function in environments with high-dimensional visual state spaces. The authors extend recent work on video frame prediction to enable reward prediction, achieving accurate cumulative reward prediction up to 200 frames in five Atari games. This work opens up important directions for model-based RL in complex, initially unknown environments.
Decision and Reasons
Based on the evaluation of the paper, I decide to Reject the paper. The main reasons for this decision are:
1. Lack of novelty: The presented results are considered too incremental and lack novelty, building upon existing methods such as Oh et al. (2015). While the authors extend the work to enable reward prediction, the approach is not surprising or new.
2. Limited contribution: The paper's contribution, although significant in terms of demonstrating the feasibility of joint model learning, is limited in terms of its impact on the field. The results, while impressive, do not significantly advance the state-of-the-art in model-based RL.
Supporting Arguments
The paper is well-written, and the authors provide a clear and detailed explanation of their approach. The empirical evaluation is thorough, and the results are impressive, demonstrating accurate cumulative reward prediction in several Atari games. However, the lack of novelty and limited contribution are significant concerns. The authors' approach, while sensible, is not groundbreaking, and the results, although positive, do not significantly advance the field.
Additional Feedback and Questions
To improve the paper, I suggest the authors consider the following:
1. Provide more context: Clarify how the proposed approach differs from existing methods and what specific challenges it addresses.
2. Highlight the significance: Emphasize the significance of the results and their potential impact on the field of model-based RL.
3. Address limitations: Discuss the limitations of the approach and potential avenues for future work.
Some questions I would like the authors to address:
1. How does the proposed approach compare to other model-based RL methods in terms of data efficiency and performance?
2. Can the authors provide more insight into the learned joint model and its properties?
3. How do the authors plan to extend this work to more complex environments and tasks?