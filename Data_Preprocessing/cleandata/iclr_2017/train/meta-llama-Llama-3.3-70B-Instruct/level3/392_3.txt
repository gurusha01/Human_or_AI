This paper proposes a novel loss framework for language modeling, which utilizes the metric space of word embeddings to generate a more informed data distribution. The authors introduce two improvements: (1) augmenting the conventional cross-entropy loss with an additional term that minimizes the KL-divergence between the model's prediction and an estimated target distribution based on word embeddings, and (2) reusing the input embedding matrix in the output projection layer. The authors provide theoretical justification for the second improvement and empirically validate the effectiveness of both improvements on the Penn Treebank and Wikitext-2 datasets.
I decide to accept this paper for the following two key reasons: (1) the paper tackles a specific and well-motivated problem in language modeling, and (2) the approach is well-supported by theoretical analysis and empirical results.
The paper clearly addresses the problem of improving language modeling by exploiting the metric space of word embeddings. The authors provide a thorough analysis of the limitations of the conventional classification framework and propose a novel loss framework that addresses these limitations. The theoretical justification for reusing the input embedding matrix in the output projection layer is sound, and the empirical results demonstrate the effectiveness of both improvements.
To further improve the paper, I suggest that the authors provide more detailed analysis of the results, including a more thorough comparison with existing state-of-the-art models. Additionally, it would be helpful to provide more insight into the hyperparameter tuning process, particularly with regards to the temperature parameter τ and the weight of the augmented loss α.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How do the authors plan to extend this work to other NLP tasks, such as neural machine translation and text summarization? (2) Can the authors provide more insight into the relationship between the proposed loss framework and other existing loss functions, such as the cross-entropy loss and the KL-divergence loss? (3) How do the authors plan to address the potential issue of overfitting when reusing the input embedding matrix in the output projection layer?