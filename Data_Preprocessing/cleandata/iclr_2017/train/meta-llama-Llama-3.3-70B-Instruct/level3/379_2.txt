This paper introduces a novel approach to visual servoing by combining learned visual features, predictive dynamics models, and reinforcement learning. The authors propose a method that uses pre-trained features from a convolutional neural network (CNN) and a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions.
The paper tackles the specific question of how to learn a visual servoing mechanism that can adapt quickly to new targets with minimal data. The approach is well-motivated, building on the idea that learned visual features can be used for visual servoing, and that predictive dynamics models can improve the robustness of the servoing mechanism.
The paper supports its claims with extensive experimental results on a synthetic car following benchmark, demonstrating substantial improvement over conventional approaches based on image pixels or hand-designed keypoints. The authors also show that their method can learn an effective visual servo using only 20 training trajectory samples for reinforcement learning, which is a significant improvement in sample efficiency over standard model-free deep reinforcement learning algorithms.
However, there are some areas where the paper could be improved. For example, the experimental results on the Stanford Sentiment Treebank are confusing and do not effectively support the paper's claim of improving speed. Additionally, the ensemble variant's state-of-the-art results are likely due to ensemble averaging rather than the framework or model, and clearer argumentation is needed to support this claim.
To improve the paper, I would suggest the following:
* Clarify the experimental results on the Stanford Sentiment Treebank and provide more detailed analysis to support the claim of improving speed.
* Provide more detailed analysis of the ensemble variant's results and argue more clearly why the state-of-the-art results are due to the framework or model rather than ensemble averaging.
* Consider adding more comparisons to other state-of-the-art methods in visual servoing to further demonstrate the effectiveness of the proposed approach.
Overall, I would accept this paper with minor revisions, as it presents a novel and well-motivated approach to visual servoing with promising experimental results.
I have the following questions for the authors:
* Can you provide more details on how the pre-trained features from the CNN are used in the visual servoing mechanism?
* How do you handle cases where the target object is partially occluded or has significant changes in appearance?
* Can you provide more analysis on the computational efficiency of the proposed approach and how it compares to other state-of-the-art methods in visual servoing?