Summary of the Paper's Contributions
The paper proposes a novel extension of Stochastic Gradient Variational Bayes (SGVB) to perform posterior inference for the weights of Stick-Breaking processes, allowing for the definition of a Stick-Breaking Variational Autoencoder (SB-VAE). This model has a latent representation with stochastic dimensionality, which is shown to outperform traditional Gaussian VAEs in terms of discriminative performance. The paper also introduces a semi-supervised variant of the SB-VAE and demonstrates its effectiveness in semi-supervised classification tasks.
Decision: Accept
I decide to accept this paper because it presents a well-motivated and novel approach to extending SGVB to Bayesian nonparametric processes. The paper provides a clear and detailed explanation of the proposed method, and the experimental results demonstrate the effectiveness of the SB-VAE in both unsupervised and semi-supervised settings.
Supporting Arguments
The paper addresses a significant gap in the literature by providing a scalable and differentiable method for performing inference in Bayesian nonparametric models. The use of the Kumaraswamy distribution as an approximate posterior is a key contribution, as it allows for a differentiable and non-centered parametrization of the latent variables. The experimental results are thorough and well-presented, demonstrating the advantages of the SB-VAE over traditional Gaussian VAEs in terms of discriminative performance and robustness to overfitting.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed comparisons with other related methods, such as the Infinite Restricted Boltzmann Machine (iRBM) and the Variational Gaussian Process (VGP). Additionally, it would be helpful to include more visualizations of the learned latent representations, such as t-SNE plots or factorized representations, to provide a better understanding of the SB-VAE's capabilities. Finally, the authors may want to consider exploring the application of the SB-VAE to other domains, such as natural language processing or computer vision, to demonstrate its broader applicability.
Questions for the Authors
1. Can you provide more details on the computational cost of the SB-VAE compared to traditional Gaussian VAEs, and how this cost scales with the size of the dataset?
2. How do you plan to extend the SB-VAE to more complex Bayesian nonparametric models, such as full Dirichlet processes with non-trivial base measures?
3. Can you provide more insights into the choice of the Kumaraswamy distribution as an approximate posterior, and how it compares to other possible choices, such as the Beta distribution?