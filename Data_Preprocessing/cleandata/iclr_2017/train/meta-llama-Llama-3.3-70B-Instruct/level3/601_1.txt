The paper introduces NewsQA, a large-scale machine comprehension dataset consisting of over 100,000 question-answer pairs based on news articles from CNN. The authors aim to create a challenging dataset that requires reasoning and comprehension abilities, going beyond simple word matching and entailment recognition. The dataset is collected through a four-stage process, involving article curation, question sourcing, answer sourcing, and validation.
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper has weaknesses in its preparation, such as inconsistencies in human performance measurements, which raises concerns about the quality of the dataset. Secondly, the design choice of not presenting the news article when soliciting questions may not be effective in encouraging diverse and challenging questions.
The paper's approach is well-motivated, and the authors provide a thorough analysis of the dataset, including answer types and reasoning types required to solve the questions. However, the results and claims made in the paper are not entirely supported by the data. For instance, the human performance measured by the authors is lower than that reported by SQuAD, which raises questions about the quality of the dataset and the potential noise from the QA collection process.
To improve the paper, I suggest that the authors revisit their data collection methodology and consider alternative approaches, such as presenting redacted news articles or engaging an automatic QA system, to encourage more diverse and challenging questions. Additionally, the authors should provide more detailed information about the crowdworkers involved in the data collection process and the validation stage to ensure the quality of the dataset.
I would like to ask the authors to clarify the following points: (1) How did they ensure the quality of the crowdworkers involved in the data collection process, and what measures were taken to prevent noise and inconsistencies in the dataset? (2) Can they provide more detailed information about the validation stage and how it was used to improve the quality of the dataset? (3) How do they plan to address the potential limitations of their design choice, such as not presenting the news article when soliciting questions, and what alternative approaches they considered? Answering these questions would help me better understand the paper and provide additional evidence to support or reject the claims made by the authors.