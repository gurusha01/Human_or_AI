Summary of the Paper's Contributions
The paper proposes a novel approach to few-shot learning using a meta-learning framework, where an LSTM-based meta-learner model is trained to optimize a learner neural network classifier. The meta-learner captures both short-term knowledge within a task and long-term knowledge common among all tasks, allowing it to learn a beneficial initialization for the learner and a successful mechanism for updating the learner's parameters. The paper demonstrates the effectiveness of this approach on the Mini-ImageNet dataset, achieving competitive results with state-of-the-art metric learning methods.
Decision and Key Reasons
I decide to Reject this paper, with two key reasons:
1. Lack of comparison with existing techniques: The paper fails to provide a thorough comparison with existing techniques in computer vision that tackle discovering objects and their relationships, known as contextual models.
2. Limited experimental evaluation: The experimental evaluation is limited to a synthetic dataset with hand-coded ground truth attributes, which restricts the paper's contribution to discovering objects and their relations.
Supporting Arguments
The paper's approach is well-motivated, and the use of an LSTM-based meta-learner model is a novel and interesting idea. However, the lack of comparison with existing techniques makes it difficult to assess the paper's contribution to the field. Additionally, the limited experimental evaluation on a synthetic dataset raises concerns about the model's effectiveness in real-world scenarios.
Additional Feedback and Suggestions
To improve the paper, I suggest the following:
* Provide a thorough comparison with existing techniques in computer vision, such as contextual models, to demonstrate the paper's contribution to the field.
* Evaluate the model on real-world datasets, such as NYUv2, KITTI, and SceneNN, to demonstrate its effectiveness in indoor and outdoor scenes.
* Consider using more advanced architectures, such as attention-based models or graph-based models, to improve the model's performance.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
* Can you provide more details on how the meta-learner model is trained, and how the learner's parameters are updated during training?
* How do you plan to extend the model to more challenging scenarios, such as few-shot learning with many classes or few-shot learning with limited training data?
* Can you provide more visualizations or analyses of the meta-learner's optimization strategy, to gain a better understanding of how the model works?