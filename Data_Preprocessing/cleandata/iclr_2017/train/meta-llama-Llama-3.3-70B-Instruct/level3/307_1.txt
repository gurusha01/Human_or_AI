This paper presents a comprehensive study on the transferability of adversarial examples in deep neural networks, specifically focusing on non-targeted and targeted attacks. The authors conduct an extensive evaluation of various approaches, including optimization-based and fast gradient-based methods, on a large-scale dataset (ImageNet) and multiple state-of-the-art models. The key findings include the prominent transferability of non-targeted adversarial examples, the difficulty in generating targeted adversarial examples with transferable target labels using existing approaches, and the effectiveness of novel ensemble-based approaches in generating transferable targeted adversarial examples.
I decide to reject this paper for the following reasons: 
1. The paper lacks a thorough comparison with existing work, particularly in the context of targeted adversarial examples. The authors claim that existing work has no well-defined measures of performance, which is not entirely true. 
2. The evaluation methodology has some limitations, such as the use of a small test set (100 images) and the lack of diversity in the target labels.
To improve the paper, I suggest the following:
* Provide a more comprehensive comparison with existing work on targeted adversarial examples, including a discussion of the strengths and weaknesses of each approach.
* Increase the size and diversity of the test set to ensure more robust results.
* Consider evaluating the transferability of adversarial examples across different datasets and models to further demonstrate the effectiveness of the proposed ensemble-based approaches.
* Provide more insights into the geometric properties of the models and how they relate to the transferability of adversarial examples.
Some questions I would like the authors to answer to clarify my understanding of the paper include:
* Can you provide more details on the ensemble-based approaches and how they differ from existing methods?
* How do you ensure that the target labels used in the evaluation are diverse and representative of the dataset?
* Have you considered evaluating the robustness of the proposed approaches to different types of attacks, such as adversarial training or input preprocessing?