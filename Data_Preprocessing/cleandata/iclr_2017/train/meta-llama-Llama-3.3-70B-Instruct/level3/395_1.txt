Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and aims to reduce the computational cost of Batch Normalization (BN) while maintaining its benefits. The authors demonstrate the effectiveness of Normalized LSTM on character-level language modeling and image generative modeling tasks, showing similar performance to BN-LSTM and Layer Normalization (LN) while being computationally faster.
Decision
I decide to reject this paper, with the main reason being that the experiments section lacks important details, such as runtime reports, convergence issues, and diagnostic tools for inference problems. Additionally, the paper could benefit from more empirical demonstrations to convince the community of the practicality of the proposed approach.
Supporting Arguments
While the proposed Normalized LSTM has the potential to transform the field, its practicality for real-world use cases is not sufficiently demonstrated. The experiments section is limited to two tasks, and the results, although promising, do not provide a comprehensive understanding of the approach's strengths and weaknesses. Furthermore, the paper could benefit from clearer explanations, signaling, and examples, particularly in the preface and when introducing new concepts.
Additional Feedback
To improve the paper, I suggest adding more evaluated models, comparing performance across different inference tools, and establishing benchmarks for the Model Zoo. Additionally, the authors could provide more details on the computational cost of the proposed approach and its scalability to larger models and datasets. Minor comments include suggestions for table comparisons, handling discrete distributions, and correcting formatting errors.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on the computational cost of Normalized LSTM compared to BN-LSTM and LN?
2. How do you plan to address the potential issue of saturation or explosion regimes due to bad initialization of the rescaling parameters?
3. Can you provide more empirical evidence of the effectiveness of Normalized LSTM on more challenging tasks, such as machine translation or question answering?