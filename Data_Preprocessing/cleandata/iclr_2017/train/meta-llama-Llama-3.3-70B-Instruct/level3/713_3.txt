Summary of the Paper's Contributions
The paper proposes a novel approach to learning reward functions for robotic manipulation tasks from visual demonstrations. The authors leverage the abstraction power of intermediate visual representations learned by deep models to quickly infer perceptual reward functions from a small number of demonstrations. The method is able to identify key intermediate steps of a task and automatically discover the most discriminative features for identifying these steps. The resulting reward functions are dense and smooth, enabling a reinforcement learning agent to learn to perform the task in real-world settings.
Decision: Reject
I am rejecting this paper due to two key reasons. Firstly, the experimental setup and evaluation methodology are not convincing, and the results do not provide sufficient evidence to support the claims made by the authors. Secondly, the paper lacks a clear and thorough analysis of the learned reward functions and their generalization to new, unseen situations.
Supporting Arguments
The paper's experimental evaluation is limited to two real-world tasks, and the results are mostly qualitative. While the authors provide some quantitative analysis, it is not comprehensive, and the metrics used to evaluate the performance of the learned reward functions are not well-defined. Furthermore, the paper does not provide a clear comparison to existing methods, making it difficult to assess the novelty and significance of the proposed approach. Additionally, the authors do not provide a thorough analysis of the learned reward functions, including their robustness to variations in the environment, objects, and lighting conditions.
Additional Feedback
To improve the paper, I would suggest the following: (1) provide a more comprehensive experimental evaluation, including a larger number of tasks and a more detailed quantitative analysis; (2) compare the proposed approach to existing methods, including inverse reinforcement learning and imitation learning; (3) provide a thorough analysis of the learned reward functions, including their generalization to new situations and their robustness to variations in the environment; and (4) consider using more advanced reinforcement learning algorithms, such as deep reinforcement learning, to improve the performance of the learned reward functions.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions: (1) How do the learned reward functions generalize to new, unseen situations, such as changes in the environment, objects, or lighting conditions? (2) Can the authors provide a more detailed analysis of the learned reward functions, including their robustness to variations in the environment and their ability to capture the underlying structure of the task? (3) How does the proposed approach compare to existing methods, including inverse reinforcement learning and imitation learning, in terms of performance, efficiency, and scalability?