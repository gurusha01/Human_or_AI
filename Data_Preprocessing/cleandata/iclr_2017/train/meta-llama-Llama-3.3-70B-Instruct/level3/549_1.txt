Summary
The paper introduces a novel approach to understanding the behavior of deep neural networks, specifically the preimage of activities at a certain level of a deep network. The authors demonstrate how to compute the preimage of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network, disregarding the effects of max-pooling. This concept is shown to be essential in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.
Decision
I decide to reject this paper, with the main reason being that the practical significance of the proposed method is unclear due to the lack of demonstration of general applicability to various CNN architectures and datasets with clear performance gains. Additionally, the performance gain of the proposed method over the previous state-of-the-art is only a small improvement of 0.5% on the CIFAR-10 dataset.
Supporting Arguments
The paper provides a technically sound approach to computing preimages in deep networks, and the concept of preimages is well-motivated and well-placed in the literature. However, the lack of clear performance gains and general applicability to various CNN architectures and datasets raises concerns about the practical significance of the proposed method. Furthermore, the paper's focus on fully connected multi-layer rectifier networks, while useful for understanding the underlying principles, may not be directly applicable to more complex architectures such as convolutional neural networks.
Additional Feedback
To improve the paper, I suggest that the authors provide more experimental results demonstrating the applicability of their approach to various CNN architectures and datasets, as well as a more detailed analysis of the performance gains achieved by their method. Additionally, the authors may want to consider exploring the relationship between preimages and adversarial examples, as well as investigating the potential applications of preimages in designing more efficient training algorithms.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how the proposed method can be applied to convolutional neural networks, and what modifications would be necessary to accommodate the convolutional and pooling layers?
2. How do you plan to address the issue of max-pooling, which is currently disregarded in the computation of preimages?
3. Can you provide more experimental results demonstrating the performance gains achieved by your method on various datasets and architectures, and how do you plan to evaluate the practical significance of your approach?