Summary
The paper proposes an online structure learning technique for Gaussian Sum-Product Networks (SPNs) that does a single pass through the data. The algorithm starts with a fully factorized joint probability distribution and updates the structure and parameters as new data points are processed. The technique is evaluated on several benchmark datasets and compared to other algorithms, demonstrating its effectiveness in learning a suitable network structure and achieving good results.
Decision
I reject this paper, with two key reasons for this choice. Firstly, the paper's focus on text classification is too narrow, and it would benefit from including a variety of other NLP tasks to demonstrate the generalizability of the modifications. Secondly, the experiments and results are concerning, with inconsistent performance benefits from residual connections across different tasks.
Supporting Arguments
The paper presents three improvements to the standard LSTM architecture: Monte Carlo averaging, embed average pooling, and residual connections. However, the experiments and results are inconsistent, with residual connections showing varying performance benefits across different tasks. This inconsistency raises concerns about the effectiveness of the proposed technique. Furthermore, the paper's focus on text classification limits its applicability to other NLP tasks, making it less convincing.
Additional Feedback
To improve the paper, I suggest the authors consider the following points:
* Evaluate the proposed technique on a variety of NLP tasks, such as question answering, machine translation, and sentiment analysis, to demonstrate its generalizability.
* Provide more detailed analysis of the results, including error bars and statistical significance tests, to increase confidence in the findings.
* Consider using more advanced parameter learning algorithms, such as stochastic gradient descent or expectation-maximization, to improve the performance of the proposed technique.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the correlation threshold is chosen, and how it affects the performance of the proposed technique?
* How do you plan to extend the structure learning algorithm to discrete variables, and what challenges do you anticipate in doing so?
* Can you provide more information on the computational resources used to run the experiments, and how the proposed technique scales to larger datasets?