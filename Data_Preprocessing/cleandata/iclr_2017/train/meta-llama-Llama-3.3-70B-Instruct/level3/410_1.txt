Summary
The paper tackles the crucial issue of identifying misclassified or out-of-distribution examples in machine learning, providing a simple yet effective baseline for detecting errors and out-of-distribution examples. The authors demonstrate that softmax prediction probabilities can be used to detect misclassified and out-of-distribution examples, despite being a poor proxy for confidence. The paper also introduces an abnormality module that can improve detection performance in some cases.
Decision
I decide to Accept this paper, with the primary reason being that it provides a valuable contribution to the community by establishing a baseline for error and out-of-distribution detection. The paper is well-motivated, and the approach is well-placed in the literature.
Supporting Arguments
The paper supports its claims through extensive experiments on various tasks, including computer vision, natural language processing, and automatic speech recognition. The results demonstrate the effectiveness of the softmax prediction probability baseline and the abnormality module. The paper also provides a clear and thorough discussion of the limitations and potential avenues for future research.
Additional Feedback
To further improve the paper, I suggest that the authors consider providing more detailed analysis of the abnormality module's performance and its potential applications. Additionally, it would be helpful to include more discussion on the potential limitations and challenges of using softmax prediction probabilities for error and out-of-distribution detection.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more insight into the choice of evaluation metrics, specifically the Area Under the Receiver Operating Characteristic curve (AUROC) and the Area Under the Precision-Recall curve (AUPR)?
2. How do you envision the abnormality module being used in practice, and what are the potential benefits and challenges of integrating it into existing machine learning systems?
3. Are there any plans to explore other approaches for error and out-of-distribution detection, such as using alternative probability distributions or incorporating additional information sources?