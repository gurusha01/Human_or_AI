The paper presents a novel approach to similarity learning using Attentive Recurrent Comparators (ARCs), which leverages attention and recurrence to estimate the similarity of a set of objects. The authors demonstrate the effectiveness of ARCs in various visual tasks, including one-shot learning on the Omniglot dataset, where they achieve state-of-the-art performance surpassing human performance.
However, I decide to reject this paper due to two key reasons. Firstly, the experimental results, although impressive, are not yet convincing due to the limited experimental setup. The authors only evaluate their model on a single dataset (Omniglot) and do not provide a thorough comparison with other approaches. Secondly, the writing is confusing at times, and some techniques could be described more carefully to convey intuition, with clarity issues in derivations and explanations.
To support my decision, I argue that the paper's clarity would benefit from fixes to inaccuracies, such as incorrect descriptions of algorithms and unclear definitions of terms like "deep probabilistic model". Additionally, the experimental evaluation is insufficient, with no demonstrations of the obtained results and a lack of comparison to other approaches, making it impossible to assess the applicability of the proposed technique.
To improve the paper, I suggest that the authors provide a more intuitive high-level description and visualization of the approach to simplify understanding. They should also address the clarity issues and provide more detailed explanations of the techniques used. Furthermore, a more comprehensive experimental evaluation, including comparisons with other approaches and demonstrations of the obtained results, would strengthen the paper.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: (1) Can you provide more details on the attention mechanism used in ARCs and how it differs from other attention mechanisms? (2) How do you plan to address the computational expense of the sequential execution of the recurrent core? (3) Can you provide more insights into the generalization capabilities of ARCs and how they can be applied to other modalities and tasks?