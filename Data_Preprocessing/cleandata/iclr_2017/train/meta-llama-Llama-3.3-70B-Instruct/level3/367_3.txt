Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to adjusting for the variance introduced by dropout in neural networks. The authors derive a new weight initialization technique that corrects for the influence of dropout rates and an arbitrary nonlinearity's effect on neuron output variance. They also introduce a simple method to re-estimate the Batch Normalization variance parameters after training, which improves test performance. The paper claims that these techniques lead to faster and more accurate convergence, and achieve state-of-the-art results on CIFAR-10 and CIFAR-100 without data augmentation.
Decision and Key Reasons
I decide to Accept this paper, with the key reasons being:
1. The paper tackles a specific and important problem in deep learning, namely the variance introduced by dropout, and proposes a well-motivated and theoretically sound solution.
2. The experimental results demonstrate the effectiveness of the proposed techniques, with significant improvements in convergence speed and test accuracy on several benchmark datasets.
Supporting Arguments
The paper provides a clear and detailed derivation of the new weight initialization technique, and the experimental results are thorough and well-presented. The authors also provide a convincing argument for the importance of correcting for dropout variance, and demonstrate the limitations of existing weight initialization techniques in this regard. The simplicity and generality of the proposed techniques are also notable strengths of the paper.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors:
* Provide more discussion on the relationship between the proposed techniques and existing methods for variance stabilization, such as Batch Normalization and weight normalization.
* Consider experimenting with more complex datasets and architectures, such as ImageNet and ResNets, to further demonstrate the effectiveness of the proposed techniques.
* Provide more insight into the computational cost of the proposed techniques, and how they compare to existing methods.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more intuition on why the proposed weight initialization technique is effective in correcting for dropout variance, and how it relates to existing techniques?
* How do the proposed techniques interact with other regularization methods, such as dropout and data augmentation, and are there any potential conflicts or synergies?
* Are there any plans to release the code and models used in the experiments, to facilitate reproduction and further research?