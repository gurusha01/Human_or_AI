This paper proposes a new intermediate stochastic model, called Simplified-SFNN, to efficiently train large-scale stochastic feedforward neural networks (SFNN). The authors establish a connection between three models: DNN → Simplified-SFNN → SFNN, which leads to an efficient training procedure for stochastic models utilizing pre-trained parameters of DNN. The paper provides interesting insights into the properties of SFNN and sheds light on the popularity of certain architectures.
I decide to accept this paper with the following key reasons: 
1. The paper tackles the specific question of developing efficient training methods for large-scale SFNN, which is a well-motivated problem in the field of neural networks.
2. The approach is well-placed in the literature, building upon existing work on SFNN and DNN, and provides a novel solution to the problem.
The paper's thorough analysis, proper experiments, and novel quantification of capacity are major strengths. However, it has some weaknesses, including redundant plots and missing experimental details. To improve the paper, I suggest the authors provide more detailed explanations of the experimental setups and results, and consider adding more visualizations to help illustrate the concepts and results.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
* Can you provide more details on the computational resources used for the experiments and the training time for each model?
* How do you plan to extend the Simplified-SFNN model to more complex tasks and datasets?
* Can you provide more insights into the regularization effect of Simplified-SFNN and how it compares to other regularization techniques?