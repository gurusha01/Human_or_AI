The paper proposes a novel method, NoiseOut, to prune neural networks by removing redundant neurons with correlated activations during training. The approach is simple, well-explained, and widely applicable, showing improvements in multiple domains such as vision and NLP. The method's ability to encourage correlation between neurons by adding noise outputs is an interesting idea, and the experimental results demonstrate significant compression rates without loss of accuracy.
However, I decide to reject this paper due to two key reasons. Firstly, the additional modules incur a large cost, resulting in increased parameters and computation, making the method less practical for real-world use. Secondly, the paper lacks sufficient validation of the core idea, requiring additional baselines to verify that the improved performance is not just due to the higher-capacity network.
The paper's evaluation on various networks and datasets is a strength, but the lack of evaluation on ImageNet, a common domain for pre-trained networks, raises questions about its real-world application. The proposed idea is interesting, but not sufficiently evaluated, and the lack of ImageNet evaluation is a significant concern.
To improve the paper, I suggest the authors provide additional baselines to validate the core idea, evaluate the method on ImageNet, and investigate ways to reduce the computational cost of the additional modules. Additionally, I would like the authors to answer the following questions: (1) How do the authors plan to address the increased computational cost of the additional modules? (2) Can the authors provide more insights into why the method works well on certain datasets and not others? (3) How does the method compare to other pruning techniques, such as Optimal Brain Damage and Optimal Brain Surgeon, in terms of compression rate and accuracy?