The paper proposes a novel approach to sequence-to-sequence transduction by formulating it as a noisy channel decoding problem. This allows the model to condition on both the left and right context, enabling more rapid and accurate decoding. The idea of iterative refinement is crucial and not well-represented in current neural network models, making this paper a valuable contribution to the field.
However, I decide to reject this paper for two main reasons. Firstly, the paper lacks connections to prior work in NLP, MT, and ML, which could provide a more thorough understanding of its contributions. Secondly, the limitations of the model, such as the inability to remove or insert words from a translation, are not well-discussed and need to be addressed.
To support my decision, I argue that the paper's approach, although innovative, requires more in-depth analysis to demonstrate its value. The model's design decisions, such as the use of absolute positional models versus relative positional models, need to be clarified. Additionally, the related work section could be improved by discussing more connections to prior work, such as the DRAW model, conditional adversarial network models, and stochastic hill climbing approaches.
To improve the paper, I suggest that the authors provide more thorough analysis of the model's design decisions and connections to prior work. They should also address the limitations of the model and explore alternative decoding algorithms or analyses, such as pseudo-likelihood objectives. Furthermore, the use of a fixed-sized window for representing the target word in context and the representation of the target sentence in distributional space could be improved for more transparency and flexibility.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: 
1. How does the model handle out-of-vocabulary words, and what strategies can be employed to improve its performance on unseen data?
2. Can the authors provide more details on the training objective and decoding algorithm, and how they relate to the pseudo-likelihood objectives?
3. How does the model's performance compare to other state-of-the-art models in sequence-to-sequence transduction tasks, and what are the potential advantages and disadvantages of the proposed approach?