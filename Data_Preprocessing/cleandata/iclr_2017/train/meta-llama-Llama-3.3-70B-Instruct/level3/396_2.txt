Summary of the Paper's Claims and Contributions
The paper proposes a novel approach to image generation, which involves creating the background first, then generating a foreground mask and appearance, and combining them using an affine transform. The authors claim that their model produces more naturally looking images than a DC-GAN model, with a 68% selection rate by AMTurkers. The paper also explores the use of Bayesian neural networks for predicting learning curves of iterative machine learning methods, and introduces a specialized neural network architecture with a learning curve layer that improves learning curve predictions.
Decision and Key Reasons
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. The paper tackles a specific and well-motivated question, namely, improving the efficiency of hyperparameter optimization for deep neural networks by exploiting the information contained in learning curves.
2. The paper provides a thorough and well-structured evaluation of the proposed approach, including a comparison with several baselines and an analysis of the results.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of hyperparameter optimization and the limitations of current approaches. The authors also provide a detailed description of their proposed approach, including the use of Bayesian neural networks and the introduction of a specialized learning curve layer. The evaluation section is well-structured and provides a thorough comparison with several baselines, including a Gaussian process-based approach and a random forest-based approach.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors provide more details on the implementation of their approach, including the specific architectures and hyperparameters used for the Bayesian neural networks. Additionally, it would be helpful to include more visualizations of the predicted learning curves and the corresponding true learning curves, to provide a better understanding of the performance of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the specific datasets used for the evaluation, including the number of samples and the distribution of the hyperparameters?
2. How did you choose the specific basis functions used in the learning curve layer, and are there any plans to explore other types of basis functions in future work?
3. Can you provide more information on the computational resources required to train the Bayesian neural networks, and how this compares to the computational resources required for the baselines?