Summary of the Paper's Contributions
The paper tackles the problem of understanding the loss surface of deep neural networks, specifically focusing on the existence of bad local minima and the connectedness of level sets. The authors provide theoretical results that quantify the amount of uphill climbing required to progress to lower energy configurations in single hidden-layer ReLU networks and prove that this amount converges to zero with overparametrization under mild conditions. They also introduce a dynamic programming algorithm to efficiently approximate geodesics within each level set, providing a tool to verify the connectedness of level sets and estimate their geometric regularity.
Decision and Key Reasons
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. Well-motivated approach: The paper addresses a fundamental question in the optimization and machine learning communities, and the approach is well-placed in the literature. The authors provide a clear and detailed analysis of the loss surface of deep neural networks, building on existing work and providing new insights.
2. Strong theoretical and empirical support: The paper provides rigorous theoretical results, including proofs and bounds, to support the claims made. Additionally, the authors present empirical results using their dynamic programming algorithm, which demonstrate the effectiveness of their approach in verifying the connectedness of level sets and estimating their geometric regularity.
Supporting Arguments
The paper's theoretical contributions, including Theorem 2.4 and Corollary 2.5, provide a solid foundation for understanding the loss surface of deep neural networks. The authors' use of a dynamic programming algorithm to approximate geodesics within each level set is a novel and effective approach to verifying the connectedness of level sets. The empirical results presented in the paper demonstrate the applicability of the authors' approach to various regression and classification tasks, including polynomial regression, convolutional neural networks, and recurrent neural networks.
Additional Feedback and Questions
To further improve the paper, I suggest that the authors consider the following:
* Provide more detailed explanations of the dynamic programming algorithm and its implementation, including any necessary hyperparameter tuning.
* Discuss potential limitations and extensions of the approach, such as applying it to more complex network architectures or exploring the relationship between the loss surface and generalization error.
* Consider providing additional visualizations or animations to help illustrate the results and facilitate understanding.
Some questions I would like the authors to address include:
* How do the results change when using different activation functions or network architectures?
* Can the authors provide more insight into the relationship between the loss surface and the optimization algorithm used, such as stochastic gradient descent?
* How do the results generalize to more complex tasks, such as image or speech recognition?