Summary of the Paper's Contributions
The paper proposes a novel multi-view learning approach for learning representations of acoustic sequences using bidirectional LSTM with contrastive losses. This approach jointly learns to embed acoustic sequences and their corresponding character sequences, resulting in improved performance over previous work on acoustic word discrimination tasks. The authors also introduce novel objectives, including a cost-sensitive loss that aims to capture orthographic edit distances, and establish new benchmarks for multi-view models.
Decision and Key Reasons
Based on the review, I decide to Accept this paper. The key reasons for this decision are: (1) the paper proposes a well-motivated and novel approach to learning acoustic word embeddings, (2) the authors provide a thorough evaluation of their approach, including comparisons to previous work and ablation studies, and (3) the results demonstrate significant improvements over previous work on acoustic word discrimination tasks.
Supporting Arguments
The paper is well-written and easy to follow, with clear explanations of the proposed approach and its motivations. The authors provide a thorough review of related work and clearly position their contribution within the context of existing research. The experimental evaluation is comprehensive and well-designed, with a clear description of the datasets, models, and metrics used. The results are impressive, with significant improvements over previous work on acoustic word discrimination tasks.
Additional Feedback and Suggestions
To further improve the paper, I suggest that the authors consider open-sourcing their implementation to facilitate replication, comparison, and improvement of their work. Additionally, it would be interesting to see more analysis of the learned embeddings, such as visualizations of the embedding space or studies of the relationships between the acoustic and character sequence embeddings.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions: (1) Can you provide more details on the computational resources required to train the models, and how this might impact the scalability of the approach? (2) How do you plan to extend the approach to handle non-word segments or more complex linguistic structures? (3) Can you provide more insight into the relationships between the acoustic and character sequence embeddings, and how these might be used in downstream applications?