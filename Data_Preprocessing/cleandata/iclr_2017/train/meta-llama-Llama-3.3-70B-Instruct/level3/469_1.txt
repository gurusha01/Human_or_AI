Summary of the Paper's Contributions
The paper proposes a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the expressive power of RNN language models. The NKLM predicts whether a word is based on a fact or not and generates words either from the vocabulary or by copying from the fact description. The model is trained on a new dataset, WikiFacts, and significantly outperforms the RNNLM in terms of perplexity, generating named entities and adapting to changes in knowledge.
Decision and Key Reasons
I decide to accept this paper with the following key reasons: 
1. The paper tackles a specific and well-motivated question of how to effectively incorporate knowledge into language models, which is a significant limitation of current language models.
2. The approach is well-placed in the literature, building on existing work in language modeling and knowledge graphs, and provides a novel and effective solution to the problem.
Supporting Arguments
The paper provides a clear and well-structured presentation of the NKLM model, including its architecture, training, and evaluation. The experiments demonstrate the effectiveness of the NKLM in improving perplexity and generating named entities, and the introduction of the Unknown-Penalized Perplexity (UPP) metric provides a more accurate evaluation of language models for knowledge-related tasks. The paper also provides a thorough discussion of related work and the limitations of current language models.
Additional Feedback
To further improve the paper, I suggest the following:
* Provide more details on the WikiFacts dataset, such as the size of the dataset and the process of aligning Wikipedia descriptions with Freebase facts.
* Consider adding more experiments to evaluate the NKLM on other knowledge-related tasks, such as question answering or dialogue modeling.
* Provide more analysis on the performance of the NKLM on different types of facts and entities, such as rare or common entities.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the fact embeddings are learned and how they are used in the NKLM model?
* How does the NKLM model handle cases where the fact description is not available or is incomplete?
* Can you provide more examples of how the NKLM model generates named entities and adapts to changes in knowledge?