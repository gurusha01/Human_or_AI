Summary
The paper proposes a novel Group Sparse Autoencoder (GSA) for feature extraction and stacks it on top of Convolutional Neural Networks (CNNs) for better question sentence representation in Question Answering (QA) tasks. The authors argue that traditional question classification techniques do not fully utilize the well-prepared answer data, which has great potential for improving question representation. The proposed model, Group Sparse Convolutional Neural Networks (GSCNNs), shows significant improvements over strong baselines on four datasets.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the idea of proposing a new autoencoder concept and stacking it on top of CNNs is somewhat incremental, and the authors could have done more to demonstrate the novelty and significance of their approach. Secondly, the writing needs improvement, and the lack of ablation studies to show the effectiveness of the proposed approach makes it difficult to fully understand the contributions of the paper.
Supporting Arguments
The paper proposes a new GSA concept, which is similar to sparse coding approaches, but with a different approach. The authors demonstrate the learning ability of GSA by visualizing the projection matrix and activations. However, the paper could benefit from more detailed comparisons with existing methods, such as sparse coding approaches, to demonstrate the advantages of GSA. Additionally, the authors could have provided more insights into the effectiveness of the proposed approach by conducting ablation studies.
Additional Feedback
To improve the paper, the authors could consider providing more detailed explanations of the GSA concept and its relationship to sparse coding approaches. They could also conduct more extensive experiments to demonstrate the effectiveness of the proposed approach, including ablation studies and comparisons with other state-of-the-art methods. Furthermore, the authors could improve the writing clarity and organization to make the paper easier to follow.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence, I would like the authors to answer the following questions:
1. Can you provide more details on how the GSA concept differs from sparse coding approaches, and what advantages it offers?
2. How did you select the hyperparameters for the GSA and GSCNNs models, and what sensitivity analysis did you perform to ensure the robustness of the results?
3. Can you provide more insights into the visualization of the projection matrix and activations, and how they relate to the learning ability of GSA?
4. How do you plan to address the lack of ablation studies to demonstrate the effectiveness of the proposed approach, and what additional experiments do you propose to conduct to strengthen the paper?