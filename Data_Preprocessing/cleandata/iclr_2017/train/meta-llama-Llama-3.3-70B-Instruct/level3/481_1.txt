Summary
The paper presents a comprehensive study on improving the performance of gradient descent when utilizing multiple compute resources. The authors propose multiple approaches, including asynchronous layer-wise gradient descent, which maximizes overlap of layer-wise backpropagation with gradient synchronization. They evaluate their approaches on a large-scale CPU-based InfiniBand cluster and NVIDIA's DGX-1 multi-GPU system, using well-known datasets and neural network topologies such as ImageNet with AlexNet and GoogleNet. The results show that delaying gradient updates by one or more iterations is the most effective means of hiding communication latency, achieving a speedup of up to 1.7x compared to synchronous gradient descent.
Decision
I decide to accept this paper, with the primary reason being that it presents a well-motivated and comprehensive study on improving gradient descent performance. The authors provide a clear and detailed explanation of their approaches, and the evaluation results demonstrate the efficacy of their proposed methods.
Supporting Arguments
The paper is well-written and clearly presents a comprehensive comparison of gradient descent methods, providing valuable empirical results to support its findings. The authors identify a significant problem in distributed gradient descent, namely the communication latency, and propose multiple approaches to address it. The evaluation results show that their proposed methods can achieve significant speedups while maintaining accuracy, making it a valuable contribution to the field of deep learning.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the implementation of their approaches, particularly the asynchronous layer-wise gradient descent method. Additionally, it would be helpful to include more analysis on the tradeoff between maintaining equivalence to sequential methods and leveraging computational resources. The authors may also consider providing more results on the convergence rates of their proposed methods.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. Can you provide more details on how you implemented the asynchronous layer-wise gradient descent method, particularly the background thread of computation used to perform gradient averaging?
2. How did you determine the optimal number of iterations to delay gradient updates, and what is the sensitivity of the results to this parameter?
3. Can you provide more analysis on the tradeoff between maintaining equivalence to sequential methods and leveraging computational resources, particularly in terms of the impact on convergence rates and accuracy?