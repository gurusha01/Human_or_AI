Summary
The paper proposes a new perspective on Highway and Residual networks, suggesting that they perform unrolled iterative estimation. This view posits that successive layers within a stage cooperate to compute a single level of representation, with the first layer providing a rough estimate that is then iteratively refined by subsequent layers. The authors derive both Residual and Highway networks from this perspective, providing a unified theory for understanding these architectures. They also present experimental results comparing the performance of Highway and Residual networks on image classification and language modeling tasks.
Decision
I decide to accept this paper, with two key reasons for this choice. Firstly, the paper provides a novel and well-motivated perspective on Highway and Residual networks, which is supported by mathematical derivations and experimental results. Secondly, the authors conduct a thorough comparison of Highway and Residual networks on various tasks, providing valuable insights into the strengths and weaknesses of each architecture.
Supporting Arguments
The paper is well-written and easy to follow, with clear explanations of the proposed perspective and its implications. The mathematical derivations are rigorous and well-supported, providing a solid foundation for the authors' claims. The experimental results are also well-designed and executed, providing a comprehensive comparison of Highway and Residual networks on image classification and language modeling tasks. The authors' analysis of the results is thorough and insightful, highlighting the importance of iterative estimation and the role of gating in Highway networks.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed explanations of the implications of their perspective for other deep learning architectures and tasks. Additionally, they could explore the potential applications of their perspective in other areas, such as computer vision or natural language processing. It would also be helpful to see more visualizations of the iterative estimation process, to provide a clearer understanding of how the layers interact and refine the representation.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How do the authors plan to extend their perspective to other deep learning architectures, such as recurrent neural networks or transformers?
2. Can the authors provide more insights into the role of batch normalization in Residual networks, and how it relates to the iterative estimation perspective?
3. How do the authors think their perspective will impact the design of future deep learning architectures, and what potential applications do they see for their work?