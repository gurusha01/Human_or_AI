Summary of the Paper's Contributions
The paper proposes an extension of a video frame prediction method to learn system dynamics and reward structure, achieving good reward prediction on Atari games within 50 steps. The approach is well-motivated, building on recent work on video frame prediction and model-based reinforcement learning. The paper is well-written, and the experiments are sound, demonstrating the effectiveness of the proposed method in predicting cumulative rewards up to 200 frames.
Decision and Reasons
I decide to reject this paper, with the main reason being that the contribution of the paper is considered minor. While the results are impressive, they are not surprising due to the deterministic link between system state and reward in Atari games. The paper's impact is limited, and the results do not significantly advance the state-of-the-art in model-based reinforcement learning.
Supporting Arguments
The paper's approach is an extension of existing work on video frame prediction, and the results, although good, are not unexpected. The evaluation is limited to Atari games, which have a deterministic reward structure, making it easier to predict rewards. The paper does not provide a thorough analysis of the limitations of the approach or a comparison with other model-based reinforcement learning methods.
Additional Feedback and Suggestions
To strengthen the paper, I suggest including experiments on more complex environments with stochastic reward structures or non-deterministic state transitions. This would demonstrate the robustness and applicability of the proposed method in more realistic scenarios. Additionally, a more detailed analysis of the limitations of the approach and a comparison with other model-based reinforcement learning methods would provide a better understanding of the paper's contributions.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
1. How do the authors plan to address the limitations of the approach in more complex environments with stochastic reward structures or non-deterministic state transitions?
2. Can the authors provide a more detailed comparison with other model-based reinforcement learning methods, highlighting the advantages and disadvantages of their approach?
3. How do the authors plan to extend their work to more realistic scenarios, such as environments with high-dimensional state and action spaces or partial observability?