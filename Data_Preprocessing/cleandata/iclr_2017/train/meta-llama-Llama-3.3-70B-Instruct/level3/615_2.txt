This paper proposes a novel loss framework for language modeling, which aims to improve the learning process by utilizing the metric space of word embeddings. The authors introduce two main improvements: (1) augmenting the conventional cross-entropy loss with an additional term that minimizes the KL-divergence between the model's prediction and an estimated target distribution based on word embeddings, and (2) reusing the input word embedding matrix as the output classification matrix. The paper provides a thorough theoretical analysis of the proposed framework and demonstrates its effectiveness through extensive experiments on the Penn Treebank and Wikitext-2 datasets.
I decide to reject this paper, primarily due to two key reasons. Firstly, the time complexity of the proposed method, L-SR1, is O(mn), which was not mentioned in the paper. This implies that the method would be significantly slower than other existing methods, particularly for large datasets. Secondly, the experimental results show that L-SR1 does not outperform other methods such as Adadelta or Adam, suggesting its inferior performance.
The paper's approach is well-motivated, and the authors provide a clear explanation of the limitations of the conventional classification framework for language modeling. The use of word embeddings to estimate a more informed data distribution is a promising idea, and the theoretical analysis of the proposed framework is thorough. However, the lack of discussion on the time complexity and the inferior performance of L-SR1 compared to other methods are significant drawbacks.
To improve the paper, I suggest that the authors provide a detailed analysis of the time complexity of L-SR1 and discuss its implications on the method's scalability. Additionally, the authors should investigate why L-SR1 does not outperform other methods and provide a more comprehensive comparison with existing methods. It would also be helpful to include more qualitative results to demonstrate the effectiveness of the proposed framework in assigning probabilities to words based on their semantic similarity.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How does the time complexity of L-SR1 affect its scalability for large datasets? (2) Why does L-SR1 not outperform other methods such as Adadelta or Adam? (3) Can the authors provide more qualitative results to demonstrate the effectiveness of the proposed framework in assigning probabilities to words based on their semantic similarity? (4) How does the proposed framework handle out-of-vocabulary words, and what are the implications of reusing the input word embedding matrix as the output classification matrix on the model's ability to handle unseen words?