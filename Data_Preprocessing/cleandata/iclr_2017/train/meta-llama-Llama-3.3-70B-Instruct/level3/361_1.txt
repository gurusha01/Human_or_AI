This paper proposes a novel approach to predicting learning curves by utilizing Bayesian neural networks with a specialized learning curve layer and stochastic gradient Hamiltonian MCMC (SGHMC). The authors demonstrate the effectiveness of their approach in extrapolating partially observed curves and predicting unobserved learning curves on various architectures, showing promise for Bayesian optimization.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and placed in the literature, and (2) the paper supports its claims with thorough empirical evaluations. The authors provide a clear and concise overview of the problem, and their approach is well-founded in the context of Bayesian optimization and learning curve prediction.
The paper provides strong empirical evidence to support its claims, including experiments on various datasets and comparisons to other state-of-the-art methods. The results show that the proposed approach outperforms other methods in predicting learning curves and can be used to improve hyperparameter optimization. The authors also provide additional evaluations, such as predicting unobserved learning curves and extending the Hyperband algorithm, which further demonstrate the effectiveness of their approach.
To improve the paper, I suggest experimenting with handling learning rate decays and evaluating the method on a random subset of data to improve its robustness. Additionally, exploring additional evaluation measures beyond mean squared error (MSE) and log-likelihood (LL), such as identifying the most promising run, would help to better assess the method's practical performance. Minor comments on the paper's formatting, such as increasing font size for legends and axes in figures and clarifying overlapping lines in Figure 6, would also improve the overall readability of the paper.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend their approach to handle more complex learning curves, such as those with multiple local optima? (2) Can the authors provide more insights into the choice of basis functions used in the learning curve layer, and how they affect the performance of the method? (3) How does the authors' approach compare to other methods, such as Gaussian processes or random forests, in terms of computational efficiency and scalability? Answering these questions would provide additional evidence to support the paper's claims and help to further improve the approach.