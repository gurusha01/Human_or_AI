Summary
The paper explores the connection between value-based methods and policy gradients, formalizing the relation between Q-values and a regularized form of policy gradients. The authors propose a new technique, PGQL, which combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. The paper provides theoretical insights and numerical examples demonstrating improved data efficiency and stability of PGQL.
Decision
I decide to Accept this paper, with the main reasons being the strong theoretical contributions and the empirical results showing improved performance of PGQL over existing methods.
Supporting Arguments
The paper provides a clear and well-motivated approach to combining policy gradients and Q-learning. The authors establish a connection between regularized policy gradient techniques and action-value fitting methods, which is a valuable contribution to the field. The numerical experiments demonstrate the effectiveness of PGQL in various environments, including the Atari games suite.
Additional Feedback
To further improve the paper, I suggest the authors consider the following:
* Provide more intuition on why the PGQL update is effective in practice. While the theoretical analysis is thorough, it would be helpful to have a more intuitive understanding of how the update works.
* Compare PGQL to other existing methods that combine policy gradients and Q-learning, such as actor-critic methods with off-policy corrections.
* Consider adding more experiments to demonstrate the robustness of PGQL to different hyperparameters and environments.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the regularization parameter α is chosen in practice?
* How does the choice of η in the PGQL update affect the performance of the algorithm?
* Are there any plans to extend PGQL to more complex environments, such as those with continuous action spaces or partial observability?