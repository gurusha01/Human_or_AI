Summary
The paper proposes a novel approach to designing deep neural networks, called FractalNet, which is based on a self-similar fractal structure. The authors claim that this approach allows for the training of ultra-deep networks without the need for residual connections, and that it matches the performance of residual networks on several benchmark datasets. The paper also introduces a new regularization technique called drop-path, which is designed to prevent co-adaptation of parallel paths in the network.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the paper's approach, although novel, lacks a clear motivation and placement in the literature. The authors do not provide a convincing argument for why the fractal structure is necessary or beneficial, and the connection to other architectures is not well-established. Secondly, the paper's results, although impressive, are not supported by a thorough analysis of the computational efficiency and scalability of the proposed approach.
Supporting Arguments
The paper's approach is considered trivial due to the use of simple random sampling for pruning masks, limiting its novelty and scalability. The experiment results lack practical time consumption measurements, focusing only on classification rate and ideal complexity. The paper fails to provide results on large-scale models, such as ImageNet classification networks, which is more important for improving computational efficiency. The reviewer questions the logical validity of the proposed method, suggesting that training a reduced-size network from scratch could potentially achieve the same accuracy without feature map pruning.
Additional Feedback
To improve the paper, the authors should provide a more detailed analysis of the computational efficiency and scalability of the proposed approach, including measurements of training and inference time. They should also provide results on larger-scale models and datasets, such as ImageNet, to demonstrate the effectiveness of the approach in more challenging scenarios. Additionally, the authors should consider comparing their approach to other state-of-the-art methods, such as DenseNets and Wide ResNets, to provide a more comprehensive evaluation of the proposed approach.
Questions for the Authors
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims:
1. Can you provide a more detailed explanation of the motivation behind the fractal structure, and how it relates to other architectures in the literature?
2. How do you plan to address the scalability and computational efficiency of the proposed approach, particularly for larger-scale models and datasets?
3. Can you provide additional results on larger-scale models and datasets, such as ImageNet, to demonstrate the effectiveness of the approach in more challenging scenarios?