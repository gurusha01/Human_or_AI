Summary
The paper proposes an automatic dialogue evaluation model (ADEM) that learns to predict human-like scores for input responses. The model uses a hierarchical recurrent neural network (RNN) encoder to capture the context and reference response, and a dot-product to compute the score. The authors collect a dataset of human scores for Twitter responses and use it to train ADEM. The results show that ADEM correlates significantly with human judgements at both the utterance and system level, and generalizes to evaluating new models.
Decision
I decide to accept this paper, with two key reasons: (1) the paper tackles a specific and important problem in dialogue research, and (2) the approach is well-motivated and supported by experimental results.
Supporting Arguments
The paper provides a clear and well-written introduction to the problem of automatic dialogue evaluation, and motivates the need for a new approach. The authors provide a thorough review of related work, including word-overlap metrics and reinforcement learning approaches. The experimental results demonstrate the effectiveness of ADEM in correlating with human judgements, and its ability to generalize to new models. The paper also provides a detailed analysis of the results, including a failure analysis and a discussion of the limitations of the approach.
Additional Feedback
To improve the paper, I suggest adding more examples to illustrate the strengths and weaknesses of ADEM. Additionally, the authors could provide more details on the hyperparameter tuning process, and explore the use of other evaluation metrics, such as ROUGE or METEOR. It would also be interesting to see the results of using ADEM to evaluate dialogue models in other domains, such as task-oriented dialogue systems.
Questions for the Authors
I would like to ask the authors to clarify the following points: (1) How do the authors plan to address the issue of generic responses, which are often rated highly by humans but may not be desirable in a dialogue system? (2) Can the authors provide more details on the crowdsourcing process, including the number of workers and the quality control measures used to ensure high-quality annotations? (3) How do the authors plan to extend ADEM to evaluate dialogue systems in other domains, such as task-oriented dialogue systems?