Summary of the Paper's Contributions
The paper presents a novel approach to detecting misclassified or out-of-distribution examples using softmax probabilities from neural network classifiers. The authors demonstrate that the maximum softmax probability can be used as a baseline for detecting errors and out-of-distribution examples, and they provide experimental results on various computer vision, natural language processing, and automatic speech recognition tasks. The paper also introduces an abnormality module, which uses an auxiliary decoder to reconstruct the input and provides superior scores for discriminating between normal and abnormal examples.
Decision and Key Reasons
Based on the review, I would recommend a weak acceptance or weak rejection of the paper, with the need for additional experiments to confirm the approach and improve the paper's score. The key reasons for this decision are: (1) the paper presents an interesting and well-motivated approach to detecting misclassified or out-of-distribution examples, but (2) the experimental part of the paper is lacking, with a limited number of experiments on real datasets and a lack of comparison with other algorithms.
Supporting Arguments
The paper's approach is well-motivated, and the use of softmax probabilities as a baseline for detecting errors and out-of-distribution examples is a simple yet effective idea. The experimental results on various tasks and datasets demonstrate the effectiveness of the approach, and the introduction of the abnormality module provides a promising direction for future research. However, the experimental section seems like a collection of preliminary experiments, and the choice of rank used in the experiments is not explicitly addressed. Additionally, the paper could benefit from more comparisons with other algorithms and a more thorough analysis of the results.
Additional Feedback and Questions
To improve the paper, I would suggest the following: (1) provide more experiments on real datasets, including comparisons with other algorithms; (2) address the choice of rank used in the experiments and provide a more thorough analysis of the results; (3) consider using more advanced evaluation metrics, such as the probability alignment score and soft F1 score, to evaluate the confidence model; and (4) provide more details on the implementation of the abnormality module and its potential applications.
Some questions I would like the authors to answer are: (1) How did you choose the rank used in the experiments, and what is the effect of varying the rank on the results? (2) Can you provide more comparisons with other algorithms, such as those using Bayesian neural networks or uncertainty estimation? (3) How do you plan to extend the abnormality module to more complex tasks and datasets? (4) Can you provide more details on the potential applications of the abnormality module, such as in safety-critical systems or high-stakes decision-making?