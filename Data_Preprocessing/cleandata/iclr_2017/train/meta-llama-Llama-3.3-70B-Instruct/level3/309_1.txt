The paper proposes a novel approach to sequence-to-sequence tasks, specifically text summarization, by introducing a 'Read-Again' model that reads the input sequence twice to form a better representation of each word. Additionally, the authors propose a copy mechanism that allows the model to handle out-of-vocabulary words in a principled manner, reducing the decoder vocabulary size and speeding up inference. The approach is well-motivated, drawing inspiration from human reading behavior and addressing the limitations of current encoder-decoder models.
I decide to accept this paper with two key reasons: (1) the approach is well-motivated and addresses a significant problem in sequence-to-sequence tasks, and (2) the experimental results demonstrate state-of-the-art performance on the Gigaword and DUC2004 datasets.
The paper provides a clear and detailed explanation of the 'Read-Again' model and the copy mechanism, making it easy to follow for readers with a background in deep learning. The authors also provide a thorough analysis of the experimental results, including visualization of the importance weights and examples of the copy mechanism in action. The results show that the proposed approach outperforms existing methods, even with a significantly reduced decoder vocabulary size.
To further improve the paper, I suggest that the authors provide more discussion on the computational resources required to train the model, as well as the potential applications of the approach to other sequence-to-sequence tasks, such as machine translation. Additionally, releasing the source code for the model would be beneficial for the research community.
I would like to ask the authors to clarify the following points: (1) How does the 'Read-Again' model handle input sequences with varying lengths, and are there any limitations to the approach in terms of sequence length? (2) Can the copy mechanism be applied to other tasks, such as machine translation, and if so, what are the potential benefits and challenges? (3) How does the proposed approach compare to other methods that address the out-of-vocabulary problem, such as subword modeling or character-level modeling?