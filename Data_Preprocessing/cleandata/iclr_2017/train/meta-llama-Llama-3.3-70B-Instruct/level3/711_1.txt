Summary
The paper proposes a novel method, RaSoR, for efficiently representing and scoring all possible spans in an extractive QA task. However, the paper provided does not match the description of RaSoR, instead, it introduces Group Sparse Autoencoders (GSA) and Group Sparse Convolutional Neural Networks (GSCNNs) for question classification tasks. The authors argue that traditional question classification techniques do not fully utilize the well-prepared answer data, which has great potential for improving question representation. The proposed model shows significant improvements over strong baselines on four datasets.
Decision
I recommend accepting the paper due to its potential to positively impact other researchers working on question classification tasks. Although the test set results are not state-of-the-art, the idea of utilizing answer data to improve question representation is promising and could lead to further research in this area.
Supporting Arguments
The paper is well-written, with interesting analysis and ablations. The authors provide a clear motivation for their approach, highlighting the unique characteristics of question classification tasks, such as hierarchical and overlapping structures in question categories. The proposed GSA and GSCNNs models are novel and show significant improvements over strong baselines on four datasets. The authors also provide a thorough discussion of related work and the advantages of their approach over traditional sparse coding methods.
Additional Feedback
To improve the paper, the authors could provide more details on the implementation of GSA and GSCNNs, such as the choice of hyperparameters and the optimization process. Additionally, the authors could explore the application of their approach to other NLP tasks, such as text classification or sentiment analysis. It would also be interesting to see a comparison with other state-of-the-art models in question classification tasks.
Questions for the Authors
1. Can you provide more details on the initialization methods for the projection matrix in GSA and how they affect the performance of the model?
2. How do you plan to extend your approach to other NLP tasks, and what potential challenges do you foresee?
3. Can you provide more insights into the visualization of the internal parameters of GSA, such as the projection matrix and hidden activations, and how they relate to the question classification task?