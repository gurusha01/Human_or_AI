Summary
The paper proposes a novel reparametrization of Long Short-Term Memory (LSTM) networks, called Normalized LSTM, which preserves the normalization of the hidden states through time. This approach is inspired by Normalization Propagation (Norm Prop) and is designed to address the vanishing and exploding gradient problems in recurrent neural networks. The authors derive the gradients of the Normalized LSTM and propose a scheme to initialize the weight matrices. They evaluate the performance of the Normalized LSTM on two tasks: character-level language modeling and image generative modeling, and show that it compares favorably to other state-of-the-art approaches.
Decision
I decide to Accept this paper, with the main reason being that it presents a novel and well-motivated approach to addressing the vanishing and exploding gradient problems in recurrent neural networks. The paper is well-written, and the authors provide a clear and detailed derivation of the Normalized LSTM and its gradients.
Supporting Arguments
The paper tackles a specific and important problem in the field of recurrent neural networks, and the proposed approach is well-motivated and supported by theoretical analysis and empirical evaluations. The authors provide a clear and detailed explanation of the Normalized LSTM and its gradients, and the experimental results demonstrate the effectiveness of the approach. Additionally, the paper is well-organized and easy to follow, making it a pleasure to read.
Additional Feedback
To further improve the paper, I suggest that the authors provide more detailed comparisons with other state-of-the-art approaches, such as Recurrent Batch Normalization and Layer Normalization. Additionally, it would be helpful to provide more insights into the initialization scheme and its impact on the performance of the Normalized LSTM. Finally, the authors may want to consider providing more detailed analysis of the computational complexity of the Normalized LSTM and its implications for practical applications.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more details on the initialization scheme and its impact on the performance of the Normalized LSTM?
2. How do you think the Normalized LSTM will perform on more challenging tasks, such as speech recognition or machine translation?
3. Can you provide more insights into the computational complexity of the Normalized LSTM and its implications for practical applications?
Overall, I think that the paper presents a significant contribution to the field of recurrent neural networks, and I look forward to seeing the authors' responses to my questions and feedback.