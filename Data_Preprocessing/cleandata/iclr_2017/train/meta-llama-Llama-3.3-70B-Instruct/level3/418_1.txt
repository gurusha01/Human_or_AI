This paper proposes a novel method for training Generative Adversarial Networks (GANs) that enables the discriminator to retain density information at the global optimum. The approach introduces an additional training signal to the generator, which balances the discriminator signal and allows the generator to converge to the true data distribution. The paper provides a rigorous theoretical analysis of the proposed formulation and demonstrates its effectiveness through experiments on synthetic and real datasets.
The paper tackles a significant problem in GANs, which is the inability of the discriminator to provide sensible energy estimates for samples. The proposed approach addresses this limitation by introducing a novel adversarial learning formulation that results in a discriminator function that recovers the true data energy. The paper also provides a thorough analysis of the optimal discriminator form under the proposed formulation and discusses the connection to existing approaches.
The experimental results demonstrate the effectiveness of the proposed approach in capturing density information and generating high-quality samples. The paper also provides a quantitative comparison of the proposed approach with baseline models, which shows that the proposed approach outperforms the baselines in terms of recovering the true data distribution.
However, there are some limitations to the paper. The first quantitative experiment in section 3.3.1 is unreasonable, as it measures the ability to find a specific z that generates a training example, which does not guarantee high probability or diversity. The metric used in the first experiment is also flawed, as it can be beaten by an identity function and does not prove that the generator is not missing modes. Additionally, the paper lacks quantitative results, such as Inception scores or SSL performance, to evaluate the quality of the generated samples and optimize diversity.
To improve the paper, the authors could consider using alternative metrics, such as training GAN on the tri-MNIST dataset, to demonstrate the ability to generate all modes with equal probability. This would provide a more convincing evaluation of the proposed approach. Furthermore, the authors could provide more detailed analysis of the entropy gradient approximation methods and their impact on the training process.
Overall, the paper presents a novel and well-motivated approach to training GANs, and the experimental results demonstrate its effectiveness. However, there are some limitations and areas for improvement, which the authors could address in future work.
Decision: Reject
Reasons:
1. The paper lacks quantitative results to evaluate the quality of the generated samples and optimize diversity.
2. The first quantitative experiment is unreasonable, and the metric used is flawed.
3. The paper could benefit from more detailed analysis of the entropy gradient approximation methods and their impact on the training process.
Questions to authors:
1. Can you provide more quantitative results, such as Inception scores or SSL performance, to evaluate the quality of the generated samples and optimize diversity?
2. How do you plan to address the limitations of the first quantitative experiment, and what alternative metrics could you use to demonstrate the effectiveness of the proposed approach?
3. Can you provide more detailed analysis of the entropy gradient approximation methods and their impact on the training process?