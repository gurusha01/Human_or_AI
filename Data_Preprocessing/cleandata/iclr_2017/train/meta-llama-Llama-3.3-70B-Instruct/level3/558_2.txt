This paper proposes a novel exploration scheme for reinforcement learning, leveraging locality-sensitive hashing to build a table of visit counts. The approach is remarkably simple compared to current alternatives and demonstrates impressive results across various domains, including classic benchmarks, continuous control tasks, and Atari 2600 games. The authors' use of state-based counts instead of state-action counts is an interesting deviation from theoretical foundations, and their decision to omit the technique's application to DQN seems unusual, given the prevalence of DQN-based variants in the comparisons.
The paper's sensitivity analysis on the granularity of the abstraction is a valuable contribution, but the conclusions drawn from this analysis are somewhat uncertain due to the engineering efforts required to get the approach working. The case study on Montezuma's Revenge, which relies on domain knowledge, feels somewhat disconnected from the rest of the paper and raises questions about the long-term impact of the proposed method.
Despite these concerns, the paper's core idea for exploration is undeniably elegant and simple. However, the reproducibility of the results is a concern due to the numerous "fiddly bits" involved in getting the approach to work. A more thorough comparison to other algorithms, such as DQN, would be necessary to fully evaluate the approach.
Based on these considerations, I would reject this paper, primarily due to the lack of robustness in the conclusions drawn from the sensitivity analysis and the limited comparison to other algorithms.
To improve the paper, I would suggest the following:
1. Provide a more detailed analysis of the robustness of the conclusions drawn from the sensitivity analysis.
2. Include a more comprehensive comparison to other algorithms, such as DQN, to fully evaluate the approach.
3. Consider revising the case study on Montezuma's Revenge to better align with the rest of the paper and provide more insight into the long-term impact of the proposed method.
Questions for the authors:
1. Can you provide more insight into the engineering efforts required to get the approach working, and how these efforts might impact the reproducibility of the results?
2. How do you plan to address the lack of robustness in the conclusions drawn from the sensitivity analysis, and what additional experiments or analysis would you propose to strengthen these conclusions?
3. Can you elaborate on the decision to omit the technique's application to DQN, and how you envision the approach being used in conjunction with other algorithms, such as DQN, in future work?