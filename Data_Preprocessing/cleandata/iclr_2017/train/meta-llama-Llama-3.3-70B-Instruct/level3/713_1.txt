Summary
The paper proposes a novel approach to learn perceptual reward functions from a few visual demonstrations of a task, leveraging the abstraction power of intermediate visual representations learned by deep models. The method is able to identify key intermediate steps of a task and automatically discover the most discriminative features for identifying these steps. The resulting reward functions are dense and smooth, enabling a reinforcement learning agent to learn to perform the task in real-world settings.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the experimental section is lengthy but lacks a clear conclusion, making it unclear whether the approach is a breakthrough or not. Secondly, the theoretical section needs clarification to better understand the proposed approach and its implications.
Supporting Arguments
The paper's approach to learning perceptual reward functions is interesting, but the experimental evaluation is not convincing. The results are mostly qualitative, and the quantitative analysis is limited to a few examples. The comparison to a random baseline is not sufficient to demonstrate the effectiveness of the approach. Furthermore, the paper lacks a clear conclusion, making it difficult to understand the significance of the results.
The theoretical section is also unclear, and the notation is often confusing. The paper assumes a simple, biased model to learn the reward function, but it is not clear why this model is chosen or how it relates to other approaches in the literature. The paper also lacks a clear explanation of how the approach can be applied to other tasks or domains.
Additional Feedback
To improve the paper, the authors should provide a clearer conclusion and a more comprehensive experimental evaluation. The authors should also clarify the theoretical section, providing a more detailed explanation of the approach and its implications. Additionally, the authors should consider comparing their approach to other state-of-the-art methods in the field.
Questions for the Authors
1. Can you provide a clearer conclusion and summary of the results?
2. How does the approach relate to other methods in the literature, such as inverse reinforcement learning?
3. Can you provide more details on the experimental evaluation, including more quantitative results and comparisons to other approaches?
4. How can the approach be applied to other tasks or domains, and what are the limitations of the method?