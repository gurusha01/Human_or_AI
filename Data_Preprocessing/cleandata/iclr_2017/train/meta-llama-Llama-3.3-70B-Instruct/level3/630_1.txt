This paper proposes a novel approach to sequence-to-sequence tasks, specifically text summarization, by introducing a "Read-Again" attention-based representation of the document with a copy mechanism. The main contribution of the paper is the read-again attention mechanism, which reads each sentence twice to create a hierarchical representation, allowing the model to better capture the meaning of the input text. The paper also proposes a simple copy mechanism that enables the model to handle out-of-vocabulary words and reduce the decoder vocabulary size, resulting in faster decoding times and smaller storage costs.
I decide to reject this paper, primarily due to two key reasons. Firstly, the paper's writing needs significant improvement, with several typos and unclear explanations of the model and architecture. Secondly, the proposed model is a simple extension to a previous model and achieves better results than the baselines, but the improvements are not significant, and the contribution to the machine learning field is weak.
The paper lacks strong justifications for the read-again mechanism and the use of additional gating, and the significance of the read-again mechanism is questionable, as it has been explored before in other contexts. The gains achieved by the model may come from the use of pointing rather than the read-again mechanism itself. Furthermore, the paper needs better organization, clearer language, and more precise terminology, with some parts feeling informal or bloated.
To improve the paper, I suggest that the authors try the read-again mechanism on other tasks, such as neural machine translation, to see if the improvements are task-specific. Additionally, the authors should provide more detailed analysis and visualization of the read-again mechanism and the copy mechanism to better understand their effects on the model's performance.
I would like the authors to answer the following questions to clarify my understanding of the paper and provide additional evidence to support their claims: (1) Can you provide more detailed comparisons with other state-of-the-art models to demonstrate the significance of the read-again mechanism? (2) How do you plan to address the issue of long-tail words in the encoder vocabulary, and what are the potential consequences of representing them with extracted embeddings from the model? (3) Can you provide more examples of the copy mechanism in action, and how it handles different types of rare words, such as special entities' names, rare nouns, adjectives, and verbs?