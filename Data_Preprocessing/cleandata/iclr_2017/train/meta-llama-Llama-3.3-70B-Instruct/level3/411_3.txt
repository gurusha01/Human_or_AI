Summary of the Paper
The paper proposes a novel training strategy called Dense-Sparse-Dense (DSD) training for regularizing deep neural networks and achieving better optimization performance. The DSD training flow consists of three steps: dense, sparse, and re-dense. The approach starts with a dense network, prunes the unimportant connections to create a sparse network, and then restores the pruned connections to increase the model capacity. The authors demonstrate the effectiveness of DSD training on various neural networks, including CNNs, RNNs, and LSTMs, and show significant improvements in performance on tasks such as image classification, caption generation, and speech recognition.
Decision
I decide to accept this paper with minor revisions. The main reasons for this decision are the novelty and effectiveness of the proposed DSD training strategy, as well as the thorough experimental evaluation on various neural networks and tasks.
Supporting Arguments
The paper presents a well-motivated approach to addressing the challenges of training deep neural networks. The authors provide a clear explanation of the DSD training flow and demonstrate its effectiveness on various tasks. The experimental results show significant improvements in performance, with an average improvement of 1.1% on ImageNet and 2.0% on WSJ'93. The authors also provide a detailed analysis of the results and discuss the potential benefits of the DSD training strategy, such as escaping saddle points and breaking symmetry.
Additional Feedback
To further improve the paper, I suggest that the authors provide more details on the computational cost of the DSD training strategy and its potential applications to other tasks and domains. Additionally, the authors may want to consider comparing the DSD training strategy to other regularization techniques, such as dropout and weight decay, to provide a more comprehensive evaluation of its effectiveness.
Questions for the Authors
To clarify my understanding of the paper, I have the following questions for the authors:
1. Can you provide more details on the computational cost of the DSD training strategy and how it compares to other training methods?
2. How do you select the sparsity ratio for the sparse training step, and what is the effect of different sparsity ratios on the performance of the DSD training strategy?
3. Can you provide more examples of the captions generated by the NeuralTalk model with and without DSD training to demonstrate the qualitative improvements in performance?