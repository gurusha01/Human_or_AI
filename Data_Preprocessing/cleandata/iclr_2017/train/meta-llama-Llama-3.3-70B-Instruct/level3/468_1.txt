This paper proposes a network quantization method to compress neural network parameters, building upon existing pruning techniques to reduce storage needs. The authors introduce Hessian-weighted k-means clustering to minimize the performance loss due to quantization and relate the network quantization problem to the entropy-constrained scalar quantization (ECSQ) problem in information theory. Two efficient heuristic solutions for ECSQ are proposed: uniform quantization and an iterative solution similar to Lloyd's algorithm.
The paper is well-structured, but it relies heavily on existing work, particularly Han 2015, with only marginal extensions to overcome its drawbacks. The experimental results show good compression performance with minimal accuracy loss, but the comparison with Hang 2015 on ResNet in Table 1 is missing. The paper raises questions about the originality of the procedure in Figure 1 and the impact of approximating the Hessian matrix with a diagonal matrix on compression.
Based on the conference guidelines, I will answer the three key questions:
1. The specific question/problem tackled by the paper is network quantization to reduce the storage needs of deep neural networks while minimizing performance loss.
2. The approach is well-motivated, building upon existing pruning techniques and relating to the ECSQ problem in information theory. However, the paper's limited novelty and heavy reliance on existing work are drawbacks.
3. The paper supports its claims with experimental results, but the lack of comparison with Hang 2015 on ResNet and the questions about the originality of the procedure in Figure 1 and the impact of approximating the Hessian matrix require further clarification.
My decision is to reject the paper, primarily due to its limited novelty and heavy reliance on existing work. The paper's contributions, although well-motivated and supported by experimental results, do not significantly advance the state-of-the-art in network quantization.
To improve the paper, I suggest the authors:
* Provide a more detailed comparison with existing work, particularly Hang 2015 on ResNet.
* Clarify the originality of the procedure in Figure 1 and the impact of approximating the Hessian matrix on compression.
* Consider adding more experimental results to demonstrate the effectiveness of the proposed method on various neural network architectures and datasets.
Questions I would like the authors to answer:
* Can you provide more details on how the Hessian-weighted k-means clustering method differs from existing quantization methods, and what specific advantages it offers?
* How do you plan to address the limited novelty of the paper and demonstrate significant advancements over existing work?
* Can you provide more experimental results to demonstrate the effectiveness of the proposed method on various neural network architectures and datasets?