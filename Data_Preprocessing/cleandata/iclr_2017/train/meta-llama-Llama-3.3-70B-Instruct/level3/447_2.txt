Summary of the Paper's Contributions
This paper presents a comprehensive study on the benefits of dialogue agents asking questions to improve their language understanding and question-answering capabilities. The authors design a simulator and a set of synthetic tasks in the movie question-answering domain, allowing a bot to interact with a teacher to address issues such as question clarification, knowledge operation, and knowledge acquisition. The paper explores both offline supervised and online reinforcement learning settings, demonstrating that the learner improves when asking questions. The authors also validate their approach using real data collected via Amazon Mechanical Turk, showing that a bot asking questions to humans learns to outperform one that only answers them.
Decision and Key Reasons
Based on the provided guidelines, I decide to Reject this paper, with two key reasons:
1. Limited Realism: The tasks and environments designed in the paper are too easy and limited, far from real human-chatbot interactions. The simulation of dialogue mistakes is also limited, only considering word misspellings.
2. Lack of Human Experiments: Although the authors later added Mechanical Turk experiments, the paper's initial approach relied heavily on simulated data, which may not accurately reflect real-world scenarios.
Supporting Arguments
The paper's main strength lies in its comprehensive study of various combinations of environments with missing knowledge, misspelled questions, and different ways to ask for extra information. However, the simplicity of the tasks and the limited simulation of dialogue mistakes undermine the paper's overall impact. The authors' decision to use a "ground-up" approach, starting with simple environments, is worthy of analysis, but more convincing results could be obtained with human experiments from the outset.
Additional Feedback and Questions
To improve the paper, I suggest the authors:
* Discuss plans to scale up to more realistic settings, including more complex dialogue mistakes and tasks.
* Provide more detailed analysis of the differences between simulated and real training data.
* Consider adding more human experiments to validate the approach in various real-world scenarios.
Some questions I would like the authors to answer:
* How do the authors plan to address the limited realism of the tasks and environments in future work?
* Can the authors provide more insights into the challenges of collecting real data via Mechanical Turk and how they addressed these challenges?
* How do the authors envision their approach being applied to more complex and realistic dialogue systems in the future?