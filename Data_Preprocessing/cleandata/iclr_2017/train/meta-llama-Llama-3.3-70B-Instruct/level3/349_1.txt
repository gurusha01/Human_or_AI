Summary
The paper proposes a novel dense-sparse-dense (DSD) training framework for regularizing deep neural networks and achieving better optimization performance. The DSD training flow consists of three steps: dense, sparse, and re-dense. The authors demonstrate the effectiveness of DSD training on various neural networks, including CNNs, RNNs, and LSTMs, and show significant improvements in performance on tasks such as image classification, caption generation, and speech recognition.
Decision
I decide to accept this paper with minor revisions. The paper presents a well-motivated and well-placed approach in the literature, and the experimental results demonstrate the effectiveness of the proposed DSD training framework. However, I have some minor concerns and suggestions for improvement, which I outline below.
Supporting Arguments
The paper tackles the specific problem of optimizing deep neural networks, which is a well-known challenge in the field. The authors propose a novel approach that combines dense and sparse training to regularize the network and improve its performance. The experimental results are thorough and demonstrate the effectiveness of the proposed approach on various tasks and datasets. The paper also provides a clear and well-organized presentation of the methodology and results.
Additional Feedback
To further improve the paper, I suggest the following:
* Provide more analysis on the curriculum learning aspect of the DSD training framework, as this is crucial for effective RL model training.
* Clarify the method of data sample selection during training, including whether it was done randomly or by progressing from simple to multiple depths.
* Consider providing more examples of the captions generated by the NeuralTalk model, as this would help to further demonstrate the effectiveness of the proposed approach.
* Address the potential challenge of handling out-of-vocabulary answers, which could pose a significant challenge to the overall performance of the model.
Questions for the Authors
To clarify my understanding of the paper, I would like the authors to answer the following questions:
* Can you provide more details on how the sparsity ratio is determined in the sparse training step?
* How do you handle the case where the pruned connections are not important for the task at hand?
* Can you provide more analysis on the computational cost of the DSD training framework compared to other regularization techniques?
* How do you plan to extend the DSD training framework to other tasks and domains, such as natural language processing and computer vision?