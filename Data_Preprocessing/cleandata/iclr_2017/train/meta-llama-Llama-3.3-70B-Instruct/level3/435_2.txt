Summary
The paper proposes a simple warm restart technique for stochastic gradient descent (SGD) to improve its anytime performance when training deep neural networks. The authors introduce a new learning rate schedule, called SGDR, which simulates warm restarts by scheduling the learning rate to decrease with a cosine annealing. The approach is evaluated on several datasets, including CIFAR-10, CIFAR-100, and a downsampled version of ImageNet, and achieves competitive results, often requiring 2-4 times fewer epochs than the currently used learning rate schedule schemes.
Decision
I decide to Accept this paper, with the main reason being that the approach is well-motivated and novel, and the experimental results demonstrate its effectiveness in improving the anytime performance of SGD.
Supporting Arguments
The paper tackles a specific question of improving the learning rate schedule for SGD, which is a crucial aspect of deep learning. The approach is well-placed in the literature, building upon existing work on restarts in gradient-free and gradient-based optimization. The experimental results are thorough and demonstrate the effectiveness of SGDR on several datasets, including CIFAR-10 and CIFAR-100, where it achieves new state-of-the-art results. The authors also provide additional experiments on a dataset of EEG recordings and a downsampled version of ImageNet, which further support the claims made in the paper.
Additional Feedback
To further improve the paper, I suggest that the authors provide more analysis on the choice of hyperparameters, such as the initial learning rate and the restart period. Additionally, it would be interesting to see more experiments on other datasets and network architectures to further demonstrate the generality of the approach. The authors may also consider providing more insights into the theoretical aspects of SGDR, such as its convergence properties and the effect of the cosine annealing schedule on the learning process.
Questions for the Authors
To clarify my understanding of the paper, I would like to ask the authors the following questions:
1. Can you provide more intuition on why the cosine annealing schedule is chosen, and how it affects the learning process?
2. How do you choose the initial learning rate and the restart period, and are there any guidelines for selecting these hyperparameters in practice?
3. Have you considered applying SGDR to other optimization algorithms, such as Adam or RMSProp, and if so, what are the results?