Summary
The paper proposes a novel approach to video generation using a generative model that decouples the representation of objects (sprites) and their dynamics in a scene. The model is inspired by computer graphics pipelines and uses a variational auto-encoding Bayesian framework to optimize a lower bound that separates the "what" and "where" of objects in a video. The authors introduce a family of architectures called Perception Updating Networks (PUNs) that can take advantage of this decoupled representation by memorizing sprites and updating their location in a scene independently.
Decision
I decide to reject this paper, with two key reasons for this choice. Firstly, the experimental section is unclear and seems rushed, with some results only alluded to but not provided, even in the appendix. This lack of thoroughness and transparency makes it difficult to fully evaluate the effectiveness of the proposed approach. Secondly, the paper's failure to include experiments with real data is disappointing, and the use of only synthetic datasets (e.g., bouncing shapes and moving MNIST) limits the paper's impact and applicability to real-world problems.
Supporting Arguments
The paper's approach is well-motivated, and the use of a variational auto-encoding Bayesian framework to optimize a lower bound that separates the "what" and "where" of objects in a video is an interesting idea. However, the experimental section is lacking in detail and clarity, making it difficult to fully understand the results and their significance. Additionally, the use of only synthetic datasets limits the paper's impact and applicability to real-world problems. The authors suggest using aerial videos as a potential application where the planar assumption would hold, but this is not explored in the paper.
Additional Feedback
To improve the paper, I suggest that the authors provide more detailed and clear experimental results, including comparisons to baseline methods and evaluations on real-world datasets. Additionally, the authors should consider using more realistic datasets, such as aerial videos or other datasets that are relevant to real-world applications. The paper could also benefit from a more thorough discussion of the limitations and potential applications of the proposed approach.
Questions for the Authors
To clarify my understanding of the paper and provide additional evidence to support my assessment, I would like the authors to answer the following questions:
1. Can you provide more detailed and clear experimental results, including comparisons to baseline methods and evaluations on real-world datasets?
2. How do you plan to extend the proposed approach to more realistic datasets, such as aerial videos or other datasets that are relevant to real-world applications?
3. Can you provide more discussion on the limitations and potential applications of the proposed approach, including potential uses in computer vision, robotics, or other fields?