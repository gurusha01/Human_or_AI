Summary
The paper presents a fascinating study on the eigenvalues of the Hessian of a loss function in deep learning, revealing a two-phased spectrum with a bulk of eigenvalues concentrated around zero and a discrete part that depends on the input data. The authors demonstrate that the Hessian is highly singular, which has significant implications for theoretical work and practical applications. They also show that the singularity of the Hessian can be a desirable property, allowing for the development of separate methods for different directions in the weight space.
Decision
I decide to accept this paper, with two key reasons: (1) the paper tackles a specific and important question in deep learning, providing new insights into the nature of the loss function landscape, and (2) the approach is well-motivated and supported by empirical evidence, demonstrating a clear understanding of the underlying mathematics and computational complexity.
Supporting Arguments
The paper is well-placed in the literature, building on previous work on second-order optimization methods and the geometry of the loss function landscape. The authors provide a clear and concise introduction to the problem, and their methodology is sound, using a combination of theoretical analysis and empirical experiments to support their claims. The results are significant, with implications for both theoretical and practical aspects of deep learning.
Additional Feedback
To further improve the paper, I suggest that the authors provide more context on the potential applications of their findings, such as how the singularity of the Hessian can be exploited in practice. Additionally, it would be interesting to see more experiments on different architectures and datasets to confirm the generality of the results. I also wonder if the authors have considered the relationship between the eigenvalue spectrum and the generalization performance of the model.
Questions for the Authors
Can you provide more insight into how the discrete part of the spectrum depends on the input data? How do you think the results would change if the authors used a different optimization algorithm or a different loss function? Are there any potential applications of the singularity of the Hessian in areas such as adversarial robustness or transfer learning?