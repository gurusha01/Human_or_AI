The paper proposes a novel approach to designing neural network architectures, introducing the concept of fractal networks that are based on self-similarity. The authors demonstrate that these networks can match the performance of standard residual networks on various classification tasks, including CIFAR and ImageNet, without relying on residual connections. This contribution is significant as it challenges the conventional wisdom that residual learning is essential for building ultra-deep neural networks.
I decide to accept this paper, with the primary reason being the novelty and potential impact of the proposed fractal network architecture. The authors provide a well-motivated approach, grounded in a thorough analysis of existing architectures and techniques, and demonstrate its effectiveness through solid experiments. The paper is well-written, and the authors provide a clear and concise explanation of their methodology and results.
The supporting arguments for this decision include the fact that the paper addresses a specific question/problem, namely, the design of ultra-deep neural networks, and provides a well-motivated approach to solving it. The authors also provide a thorough analysis of related work, demonstrating a clear understanding of the existing literature and techniques. The experimental results are convincing, demonstrating the effectiveness of the proposed approach on various tasks and datasets.
To further improve the paper, I would suggest that the authors provide more insights into the training dynamics of fractal networks, particularly in terms of how the self-similar structure affects the optimization process. Additionally, it would be interesting to see more analysis on the robustness of fractal networks to different types of noise and perturbations.
I would like to ask the authors to clarify the following points: (1) How do the authors plan to extend the fractal network architecture to other domains, such as natural language processing or reinforcement learning? (2) Can the authors provide more insights into the computational cost of training fractal networks, particularly in comparison to residual networks? (3) How do the authors envision the fractal network architecture being used in practice, particularly in applications where interpretability and explainability are crucial?