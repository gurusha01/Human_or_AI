This paper proposes a novel Bayesian neural network architecture for predicting learning curve values during machine learning model training. The approach builds upon previous work by incorporating information from all tested hyperparameter settings, allowing for more accurate predictions. The model's performance is overall positive, especially in the initial phase of each curve where sharing knowledge across curves is beneficial.
I decide to accept this paper with the key reason being that the approach is well-motivated and well-placed in the literature. The paper provides a clear and thorough evaluation of the proposed method, including comparisons to other state-of-the-art approaches. The results demonstrate the effectiveness of the proposed method in predicting learning curves and improving hyperparameter optimization.
However, I do have some concerns regarding the timing of the approach, as training the Bayesian network takes around 20-60 seconds, which can be a significant fraction of the total model training time. Additionally, the paper lacks testing in regimes with mostly or fully observed curves, which could provide further insight into the method's performance.
To improve the paper, I suggest providing more detailed analysis of the individual predictions made by the Bayesian network, such as bounding the asymptotic value of the learning curve. Furthermore, clarifying figure labels, prediction targets, and the use of basis functions would enhance the readability and understanding of the paper. I would also like the authors to address the following questions: How does the proposed method handle noisy or missing data? Can the approach be extended to other types of machine learning models or optimization problems? What are the potential applications of this method in real-world scenarios? 
Overall, the paper presents a significant contribution to the field of machine learning and hyperparameter optimization, and with some minor revisions, it has the potential to be a strong addition to the conference proceedings.