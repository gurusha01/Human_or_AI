Summary of the Paper's Contributions
The paper proposes a Neural Knowledge Language Model (NKLM) that combines symbolic knowledge from a knowledge graph with the expressive power of RNN language models. The NKLM significantly outperforms the traditional RNNLM in terms of perplexity and generates named entities that are not observed during training. The model also adapts immediately to changes in knowledge, making it a promising approach for knowledge-related language tasks.
Decision and Reasons
I decide to Accept this paper with the following key reasons:
1. The paper tackles a specific and well-motivated problem in language modeling, namely the limitation of traditional language models in encoding and decoding factual knowledge.
2. The approach is well-placed in the literature, building upon existing work on RNN language models and knowledge graph embeddings.
Supporting Arguments
The paper provides a clear and well-structured presentation of the NKLM model, including its components, training objective, and evaluation metrics. The experimental results demonstrate the effectiveness of the NKLM in improving perplexity and reducing the number of unknown words. The introduction of the Unknown-Penalized Perplexity (UPP) metric is also a valuable contribution, as it provides a more nuanced evaluation of language models in knowledge-related tasks.
Additional Feedback and Questions
To further improve the paper, I suggest the authors provide more analysis on the impact of the knowledge graph embedding method (TransE) on the performance of the NKLM. Additionally, it would be interesting to see experiments on other knowledge-related tasks, such as question answering or dialogue modeling, to demonstrate the versatility of the NKLM.
Some questions I would like the authors to answer:
* How do the authors plan to address the limitation of assuming a known topic for a given description, as mentioned in the conclusion?
* Can the NKLM be applied to other types of knowledge graphs, such as those with more complex relationships or entities?
* How does the NKLM handle out-of-vocabulary words that are not present in the knowledge graph?