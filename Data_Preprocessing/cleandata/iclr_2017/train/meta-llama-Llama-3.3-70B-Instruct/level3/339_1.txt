Summary
The paper proposes a novel approach to augment neural language models with a cache-like memory, allowing the model to adapt to its recent history. The authors introduce the Neural Cache Model, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is efficient, scalable, and can be added to pre-trained recurrent neural language models without fine-tuning. The authors demonstrate the effectiveness of their approach on several language modeling datasets and the LAMBADA dataset, showing significant performance gains.
Decision
I decide to Accept this paper, with the main reason being the simplicity and practicality of the proposed approach, which makes it a useful tool for the NLP community. The authors provide a well-motivated and well-placed approach in the literature, and the empirical results support the claims made in the paper.
Supporting Arguments
The paper tackles a specific question of adapting neural language models to their recent history, and the approach is well-motivated by the need for efficient and scalable memory-augmented neural networks. The authors provide a clear and concise explanation of the Neural Cache Model and its advantages over existing approaches. The empirical results demonstrate the effectiveness of the approach on various datasets, including the challenging LAMBADA dataset.
Additional Feedback
To further improve the paper, I suggest the authors provide more analysis on the trade-offs between the cache size and the performance gains. Additionally, it would be interesting to see experiments on other types of neural language models, such as transformer-based models, to demonstrate the applicability of the Neural Cache Model. The authors may also consider providing more details on the hyperparameter tuning process and the sensitivity of the model to different hyperparameter settings.
Questions for the Authors
I would like to ask the authors to clarify the following points:
1. How do the authors plan to adapt the interpolation parameter between the static model and the cache based on the current vector representation of the history ht, as mentioned in the conclusion?
2. Can the authors provide more insights on the computational cost of the Neural Cache Model compared to other memory-augmented neural networks?
3. How do the authors think the Neural Cache Model can be applied to other NLP tasks, such as machine translation or question answering?