Summary of the Paper's Claims and Contributions
The paper presents a variational approximation to the information bottleneck (IB) method, called Deep Variational Information Bottleneck (Deep VIB). The authors claim that their approach allows for efficient training of stochastic neural networks using the IB objective, which is a fundamental tradeoff between having a concise representation and one with good predictive power. They demonstrate the effectiveness of their method on several experiments, including MNIST and ImageNet, showing improved classification performance and robustness to adversarial attacks.
Decision: Reject
The main reasons for this decision are:
1. Lack of clarity in the definition of universality: The paper fails to provide a clear and well-defined concept of universality, which is a crucial aspect of the IB method. The authors' investigation of the universality property in algorithms is extended to new algorithms, but the dependence of epsilon on N is troubling and requires clarification.
2. Insufficient experimental evaluation: The experiments presented in the paper do not properly test the validity of the universality approximation, as only one parameter is varied and the others are held constant. Additionally, the paper's conclusion that it provides a robust and quantitative way to answer five questions about universality is not supported by the results, which are limited to a few specific algorithms and do not demonstrate robustness.
Supporting Arguments
The paper's approach to approximating the IB objective using variational inference is interesting, but the lack of clarity in the definition of universality and the insufficient experimental evaluation undermine the paper's contributions. Furthermore, the connection between the universal regime and the structure of the landscape is not clearly established, and the question is too vague to be answered in a robust or quantitative way.
Additional Feedback
To improve the paper, the authors should provide a clear and well-defined concept of universality and clarify the dependence of epsilon on N. They should also conduct more comprehensive experiments to test the validity of the universality approximation and demonstrate the robustness of their method. Additionally, the authors should provide more details on the hyperparameter settings and architecture details for the experiments, as well as explore the connections to differential privacy and open universe classification problems.
Questions for the Authors
1. Can you provide a clear and well-defined concept of universality and clarify the dependence of epsilon on N?
2. How do you plan to conduct more comprehensive experiments to test the validity of the universality approximation and demonstrate the robustness of your method?
3. Can you provide more details on the hyperparameter settings and architecture details for the experiments?
4. How do you plan to explore the connections to differential privacy and open universe classification problems?