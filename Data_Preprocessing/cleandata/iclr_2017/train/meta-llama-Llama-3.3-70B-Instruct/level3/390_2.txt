This paper presents a novel approach to automatic dialogue evaluation, which is a crucial component in the development of conversational AI systems. The authors propose a model, ADEM, that learns to predict human-like scores for dialogue responses using a hierarchical recurrent neural network (RNN) encoder and a dot-product based scoring function. The model is trained on a dataset of human-annotated dialogue responses and achieves significant correlations with human judgements at both the utterance and system levels.
The paper tackles the specific question of how to evaluate the quality of dialogue responses in a way that correlates with human judgements. The approach is well-motivated, as existing word-overlap metrics such as BLEU have been shown to be biased and correlate poorly with human evaluations. The authors provide a clear and thorough explanation of the technical background, including the use of RNNs and word-overlap metrics.
The paper supports its claims with extensive experimental results, including correlations with human judgements, system-level evaluations, and generalization to unseen models. The results demonstrate that ADEM outperforms existing word-overlap metrics and achieves significant correlations with human judgements. The authors also provide a detailed analysis of the results, including a failure analysis and an examination of the model's data efficiency.
Based on the results and analysis presented in the paper, I decide to accept this paper. The two key reasons for this decision are: (1) the paper presents a novel and well-motivated approach to automatic dialogue evaluation, and (2) the experimental results demonstrate significant correlations with human judgements and outperform existing word-overlap metrics.
To improve the paper, I suggest that the authors provide more visualizations and examples to illustrate the model's strengths and weaknesses. Additionally, the authors could explore ways to address the model's tendency to be too conservative in predicting response scores and its potential bias towards shorter responses. Finally, the authors could consider providing more details on the computational resources required to train and evaluate the model, as well as its potential applications in real-world dialogue systems.
Some questions I would like the authors to answer to clarify my understanding of the paper include: (1) How do the authors plan to address the issue of generic responses, which are often rated highly by humans but may not be desirable in a conversational AI system? (2) Can the authors provide more details on the VHRED model and its role in pre-training the encoder for ADEM? (3) How do the authors plan to extend the model to other domains and tasks, such as task-oriented dialogue systems or multimodal dialogue systems?