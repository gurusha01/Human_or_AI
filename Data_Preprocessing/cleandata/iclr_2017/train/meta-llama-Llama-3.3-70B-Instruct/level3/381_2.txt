Summary of the Paper's Contributions
The paper proposes a novel method for pruning filters from convolutional neural networks (CNNs) using a Taylor expansion-based criterion. This approach outperforms existing heuristics, such as using activation magnitude, by considering the influence of filter activation on the target loss. The authors thoroughly investigate multiple baselines, including an oracle, and demonstrate the method's elegance, generalizability, and computational feasibility. The work highlights the trade-offs between increased speed and decreased performance, making it useful for practical applications.
Decision and Reasons
Based on the provided guidelines, I decide to Accept this paper. The two key reasons for this choice are:
1. Well-motivated approach: The paper proposes a well-motivated approach based on the Taylor expansion of the loss change, which provides a strong theoretical justification for the pruning criterion.
2. Empirical evidence: The authors provide extensive empirical evidence to support their claims, including comparisons with existing methods and baselines, demonstrating the effectiveness and efficiency of their approach.
Supporting Arguments
The paper provides a clear and concise introduction to the problem of neural network pruning and the motivation behind the proposed approach. The authors thoroughly review existing methods and highlight the limitations of current approaches. The proposed Taylor expansion-based criterion is well-justified and easy to compute, making it a practical solution for pruning CNNs. The experimental results demonstrate the effectiveness of the approach in reducing the number of parameters and floating-point operations while maintaining a high level of accuracy.
Additional Feedback and Questions
To further improve the paper, I suggest the authors consider the following:
* Provide more insights into the relationship between the Taylor criterion and other pruning methods, such as Optimal Brain Damage.
* Investigate the potential for combining the Taylor criterion with other criteria, such as activation magnitude or mutual information, to further improve pruning performance.
* Consider applying the proposed approach to other types of neural networks, such as recurrent neural networks or fully connected networks.
I would like the authors to answer the following questions to clarify my understanding of the paper:
* Can you provide more details on the computational cost of computing the Taylor criterion compared to other pruning methods?
* How do you handle the case where the Taylor criterion is zero or very close to zero for a particular filter?
* Have you considered applying the proposed approach to other computer vision tasks, such as object detection or segmentation?