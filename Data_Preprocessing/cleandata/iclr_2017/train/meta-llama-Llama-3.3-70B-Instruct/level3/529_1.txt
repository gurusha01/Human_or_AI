This paper proposes a novel approach to music generation by combining Long Short-Term Memory (LSTM) networks with a handcrafted reward function using Reinforcement Learning (RL). The authors aim to refine the LSTM model to generate more pleasing and harmonious melodies while maintaining the information learned from the training data. The approach shows promise in improving the quality of generated music, but there are some concerns regarding the scalability of the method and the choice of LSTM as the underlying model.
The paper tackles the specific question of whether RL can be used to constrain a sequence learner to generate sequences that adhere to a desired structure, in this case, music theory rules. The approach is well-motivated, building on existing work in RL and sequence modeling. The authors provide a clear and detailed explanation of their method, including the derivation of the RL objective and the implementation of the RL Tuner framework.
The results presented in the paper demonstrate that the RL Tuner approach can improve the quality of generated music, with the Q-learning and generalized Î¨-learning methods outperforming the Note RNN baseline. The user study also shows that the melodies generated by the RL Tuner models are preferred by human listeners. However, the results may be cherry-picked, and the method's scalability to more complicated reward functions is uncertain.
I decide to accept this paper, with the primary reason being that it presents a novel and promising approach to music generation using RL. The paper provides a clear and detailed explanation of the method, and the results demonstrate the effectiveness of the approach.
To improve the paper, I suggest that the authors provide more discussion on the limitations of their approach, including the potential drawbacks of using LSTM as the underlying model. Additionally, the authors could explore alternative models, such as dilated convolution-based approaches, to improve the quality of generated music. The authors could also provide more analysis on the user study results, including the demographics of the participants and the criteria used to evaluate the melodies.
Some questions I would like the authors to answer include: How do the authors plan to address the scalability of the method to more complicated reward functions? What are the potential applications of the RL Tuner approach beyond music generation? How do the authors plan to evaluate the quality of generated music in a more objective and systematic way? What are the implications of using RL to refine sequence models, and how can this approach be applied to other domains?