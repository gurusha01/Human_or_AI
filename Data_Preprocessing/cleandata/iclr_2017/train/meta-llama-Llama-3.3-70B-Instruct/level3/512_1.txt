This paper presents a novel framework for nonparametrically learning activation functions in deep neural networks, providing a theoretical justification for the approach. The authors demonstrate the effectiveness of their method on image recognition datasets, achieving up to a 15% relative increase in test performance compared to the baseline. The paper is well-written and introduces novel theoretical techniques for learning activation functions.
I decide to accept this paper for several reasons. Firstly, the paper tackles a specific and relevant problem in the field of deep learning, which is the limitation of current activation functions. The approach is well-motivated, and the authors provide a clear explanation of the theoretical justification for their method. The experimental results demonstrate the effectiveness of the approach, and the paper is well-organized and easy to follow.
The key reasons for my decision are:
1. The paper presents a novel and well-motivated approach to learning activation functions in deep neural networks.
2. The authors provide a clear and concise explanation of the theoretical justification for their method.
3. The experimental results demonstrate the effectiveness of the approach, achieving significant improvements in test performance compared to the baseline.
To further improve the paper, I suggest that the authors provide more details on the implementation of the two-stage training process and the initialization of the nonparametric activation functions. Additionally, it would be helpful to include more visualizations of the learned activation functions and to discuss the potential applications of this approach to other domains.
I would like the authors to answer the following questions to clarify my understanding of the paper:
1. Can you provide more details on the choice of the Fourier basis expansion for the activation functions?
2. How do you initialize the nonparametric activation functions, and what is the effect of different initialization methods on the performance of the network?
3. Can you discuss the potential limitations of this approach and how it can be extended to other types of neural networks, such as recurrent neural networks or generative models?