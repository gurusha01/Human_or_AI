This paper presents a novel training methodology to bridge the gap between backpropagation-based deep learning and neuromorphic hardware constraints, such as spiking neurons and low-precision synapses. The authors propose a probabilistic interpretation of spikes and synaptic states, enabling the use of backpropagation while maintaining compatibility with hardware like the TrueNorth (TN) chip. The method achieves impressive results on the MNIST dataset, with a high-performance configuration reaching 99.42% accuracy and a low-energy configuration achieving 92.7% accuracy at 0.268 µJ per classification. The paper's main contribution lies in its "constrain-then-train" approach, which directly maps the trained network to hardware without post-training discretization, addressing a critical challenge in neuromorphic computing.
Strengths:
1. Relevance and Practicality: The proposed method is highly relevant for neuromorphic hardware platforms, addressing real-world constraints like reduced precision synapses and limited connectivity. The energy-efficient results on the TN chip demonstrate the method's practical applicability.
2. Technical Soundness: The probabilistic framework for training is well-justified, and the authors provide detailed derivations and experimental results to support their claims.
3. Clarity: The paper is generally well-written and organized, with clear explanations of the training methodology and deployment process.
4. Significance: The work addresses a critical bottleneck in neuromorphic computing, providing a pathway for integrating deep learning with energy-efficient hardware. The results are competitive with state-of-the-art spiking neural networks.
Weaknesses:
1. Overstated Claims: Section 1 overstates the novelty of running the network on the TN platform, which serves more as validation than a novel contribution.
2. Terminology and Clarity Issues: Terms like "0.15 bits per synapse" and the network topology are insufficiently explained, leaving room for ambiguity.
3. Limited Generalizability: While the method is effective for TN-specific constraints, its applicability to other hardware platforms remains unclear.
4. Experimental Gaps: The low performance of the 30-core network and the consistent results for the 5-core network raise questions about the training process and hardware limitations. Additionally, input spike rates and whether the input is a spike train require clarification.
5. Comparison with Related Work: A comparison with other spiking backpropagation methods, such as SpikeProp, is missing and would enhance the paper's originality and contextualization.
6. Resource Requirements: Achieving high accuracy with a 64-ensemble approach requires significant hardware resources, potentially limiting scalability.
Suggestions for Improvement:
1. Reword claims in Section 1 to avoid overstating novelty and focus on the method's contributions.
2. Provide clearer explanations for ambiguous terms and network topology.
3. Include a discussion of training time and hardware-specific limitations to address experimental inconsistencies.
4. Compare the proposed method with existing spiking backpropagation techniques to highlight its unique contributions.
5. Correct minor issues, such as the typo on line 399 ("approach approach").
Recommendation:
While the paper has significant strengths, including its relevance and technical contributions, the weaknesses—particularly the lack of clarity in some areas and the limited generalizability—need to be addressed. I recommend acceptance with minor revisions, contingent on the authors addressing the clarity, experimental, and comparative gaps outlined above.