Review of the Paper
Summary
This paper addresses a critical issue in batch learning from logged bandit feedback (BLBF), specifically the propensity overfitting problem inherent in the conventional counterfactual risk estimator. The authors propose a novel self-normalized risk estimator that avoids these anomalies and introduce a new learning algorithm, Norm-POEM, based on the Counterfactual Risk Minimization (CRM) principle. The paper demonstrates that Norm-POEM consistently outperforms the conventional POEM algorithm on multi-label classification tasks, both in terms of generalization performance and robustness. The work is positioned within the broader context of BLBF, causal inference, and off-policy evaluation, building on prior research while addressing a key limitation of existing methods.
Strengths
1. Novelty and Originality: The introduction of a self-normalized risk estimator to address propensity overfitting is a significant contribution. The paper builds on well-established techniques like importance sampling and control variates, but applies them in a novel way to BLBF.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, including proofs of consistency and boundedness for the proposed estimator. The use of diagnostic tools like propensity weight sums (Ŝ(h)) to detect overfitting is insightful.
3. Empirical Validation: The experimental results are compelling, showing consistent improvements of Norm-POEM over POEM across multiple datasets. The analysis of propensity overfitting and its mitigation through the self-normalized estimator is well-supported by empirical evidence.
4. Practical Significance: The proposed method is computationally efficient, with runtimes comparable to existing approaches, and demonstrates robustness across varying dataset properties and experimental setups. This makes it highly relevant for real-world applications like recommendation systems and ad placement.
5. Clarity of Results: The authors clearly demonstrate the benefits of Norm-POEM, including its robustness to loss transformations and its ability to avoid catastrophic failures seen in POEM.
Weaknesses
1. Clarity of Presentation: While the paper is generally well-written, some areas require clarification:
   - It is unclear whether Norm-POEM also rescales δ to [-1,0] and what "δ > 0" signifies in Table 1 experiments. This ambiguity could confuse readers unfamiliar with the experimental setup.
   - The feedback δ(x, y) is treated deterministically in Section 3, which does not account for noisy labeling scenarios. Treating feedback as a random variable would improve generality.
2. Generalization of Risk Setup: The paper could benefit from including an expectation over δ's randomness in the true risk setup, which would make the theoretical framework more robust.
3. Notational Inconsistencies: There are minor errors in the notation:
   - Line 170-171: The ratio should be \( h(y|x) / h_0(y|x) \), and prior work [1] should be explicitly referenced for non-linearity.
   - Line 276: Replace "argmax" with "argmin" as the objective is to minimize risk.
   - Line 319: Replace bars with hats to maintain consistency in notation.
Arguments for Acceptance
- The paper addresses a critical and well-motivated problem in BLBF, offering a theoretically sound and empirically validated solution.
- The proposed method advances the state of the art in counterfactual learning by mitigating propensity overfitting, a previously underexplored issue.
- The results are significant, demonstrating clear improvements over the baseline POEM algorithm and aligning with the growing interest in batch learning from logged bandit feedback.
Arguments Against Acceptance
- The paper requires minor revisions for clarity and consistency, particularly in the experimental setup and notation.
- The theoretical framework could be expanded to account for noisy feedback scenarios, which are common in real-world applications.
Recommendation
This paper makes a strong scientific contribution to the field of BLBF and is highly relevant to the NeurIPS community. While there are minor issues with clarity and notation, these do not detract from the overall quality and significance of the work. I recommend acceptance with minor revisions to address the noted concerns.