The paper introduces the Elastic Averaging Momentum Stochastic Gradient Descent (EAMSGD) algorithm, an innovative approach to distributed stochastic gradient descent (SGD) for deep learning. By leveraging an elastic force mechanism to link local worker parameters with a central variable stored on a parameter server, the algorithm reduces communication overhead while enabling greater exploration of the parameter space. This balance between exploration and exploitation is particularly beneficial in deep learning, where local optima are abundant. The authors provide both synchronous and asynchronous variants of the algorithm, as well as a momentum-based extension (EAMSGD). Theoretical analysis, including stability conditions and comparisons with ADMM, is presented, alongside empirical evaluations on CIFAR and ImageNet datasets, demonstrating faster convergence and superior performance compared to baseline methods like DOWNPOUR.
Strengths:
1. Quality and Theoretical Contribution: The paper is technically sound, with rigorous theoretical analysis and stability proofs for the asynchronous EASGD variant. The comparison with ADMM highlights the algorithm's stability advantages, particularly under communication constraints.
2. Empirical Results: The experiments on CIFAR and ImageNet datasets convincingly demonstrate the algorithm's efficacy. EAMSGD achieves faster convergence and better test error performance, especially under larger communication periods, showcasing its practical utility.
3. Originality: While inspired by prior work on SGD and ADMM, the elastic force mechanism and its application to modern deep learning models represent a novel contribution. The momentum-based extension further adds to the originality.
4. Significance: The work addresses a critical challenge in distributed deep learning—communication efficiency—making it highly relevant for both researchers and practitioners in the field.
Weaknesses:
1. Clarity: Although the paper is well-structured, the algorithm's formulations could be presented more clearly. For instance, a table summarizing the differences between EASGD, EAMSGD, and other distributed SGD approaches (e.g., DOWNPOUR) would enhance readability.
2. Experimental Details: While the empirical results are strong, the paper could benefit from more detailed documentation of hyperparameter settings and sensitivity analyses. This would aid reproducibility and provide deeper insights into the algorithm's behavior under varying conditions.
3. Broader Context: The paper could better situate its contributions within the broader landscape of distributed optimization methods, referencing more recent works from the NIPS community.
Recommendation:
I strongly recommend this paper for publication at NIPS, as it makes a significant contribution to distributed deep learning. The combination of theoretical rigor, empirical validation, and practical relevance ensures its appeal to both researchers and engineers. Addressing the minor clarity and experimental detail issues would further strengthen the paper. 
Arguments for Acceptance:
- Novel and effective algorithm for distributed SGD.
- Strong theoretical and empirical results.
- High relevance to the growing field of distributed deep learning.
Arguments Against Acceptance:
- Minor clarity issues in algorithm presentation.
- Limited discussion of hyperparameter sensitivity and broader context.
Overall, the paper represents a high-quality contribution and aligns well with the scope and standards of NIPS.