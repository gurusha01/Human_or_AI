The paper introduces the Poisson Gamma Belief Network (PGBN), a novel deep generative model designed to infer multilayered representations of high-dimensional count data. By leveraging gamma distributions and augmentation techniques, the authors develop a tractable upward-downward Gibbs sampler to jointly train all layers of the network. The PGBN builds on existing work in deep learning and topic modeling, such as sigmoid belief networks and Poisson factor analysis, but extends these by employing gamma-distributed nonnegative hidden units. This approach allows the model to capture correlations across layers and handle overdispersed counts effectively. The paper also proposes a layer-wise training strategy to infer the network structure, offering a principled way to balance layer depth and width under computational constraints.
Strengths:
1. Primary Contribution: The introduction of a Poisson-Gamma hierarchy is a meaningful contribution, providing an alternative to existing deep generative models for count data. The use of gamma-distributed hidden units is novel and theoretically justified.
2. Technical Soundness: The paper is technically robust, with detailed derivations for the Gibbs sampler and augmentation techniques. The authors also provide insights into the relationship between the network depth, layer widths, and the ability to model overdispersed counts.
3. Scalability: The layer-wise training strategy is a practical addition, enabling the model to infer network structure adaptively while managing computational resources.
4. Quantitative Results: The experiments demonstrate the PGBN's ability to improve classification accuracy and perplexity as network depth increases, showcasing its potential for unsupervised feature learning.
Weaknesses:
1. Connection to Rectified Linear Units (ReLUs): While the gamma units subsume the linear regime of ReLUs, this connection is only briefly mentioned. A more explicit discussion and comparison with ReLU-based architectures would strengthen the paper.
2. Scalability Concerns: The Gibbs sampling procedure appears computationally expensive, especially for deeper architectures. Training times for networks with more than five layers are not clearly reported, leaving scalability for large datasets uncertain.
3. Experimental Comparisons: The classification experiments suffer from inconsistencies in training vocabularies, making comparisons with competing algorithms less meaningful. A more rigorous experimental setup is needed.
4. Qualitative Analysis: Claims about topic specialization and synthetic document generation are not well-supported. Including examples or supplementary material would enhance the qualitative evaluation.
5. Clarity: While the technical content is thorough, the paper is dense and could benefit from improved organization and clearer explanations, particularly for readers less familiar with hierarchical Bayesian models.
Recommendation:
The PGBN is an interesting and technically sound contribution to deep generative modeling for count data. However, its shortcomings in experimental rigor and qualitative analysis reduce its overall impact. To improve, the authors should address scalability concerns, provide clearer experimental comparisons, and include qualitative evidence to support their claims. Despite these issues, the paper advances the state of the art and is likely to stimulate further research in this area.
Arguments for Acceptance:
- Novel and theoretically grounded approach to modeling count data.
- Demonstrated improvements in classification and perplexity metrics.
- Practical layer-wise training strategy for inferring network structure.
Arguments Against Acceptance:
- Limited discussion of connections to ReLUs and other deep learning paradigms.
- Computational scalability for deeper networks remains unclear.
- Experimental design and qualitative analysis are insufficiently rigorous.
Overall, I recommend acceptance with minor revisions to address the identified weaknesses.