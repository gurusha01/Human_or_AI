The paper presents several enhancements to the stochastic variance-reduced gradient (SVRG) method, focusing on improving its computational efficiency and practical applicability. The authors propose strategies such as growing-batch approaches, mixed SG/SVRG iterations, support vector exploitation, and alternative mini-batch selection techniques. They also analyze the generalization error of SVRG and provide theoretical justifications for regularized SVRG updates. While the paper makes strong theoretical contributions, its experimental validation and practical considerations leave room for improvement.
Strengths:
1. Theoretical Contributions: The paper provides a solid theoretical foundation for the proposed modifications to SVRG, including convergence rate analysis under inexact gradient approximations. The analysis of regularized SVRG and mini-batch strategies is particularly noteworthy.
2. Novelty: The exploration of support vectors to reduce gradient evaluations and the mixed SG/SVRG strategy are innovative. These ideas could inspire further research in optimizing stochastic methods.
3. Clarity in Theory: The theoretical sections are well-organized, with clear derivations and proofs that demonstrate the robustness of the proposed methods.
4. Potential Impact: The proposed modifications, if implemented effectively, could make SVRG more practical for large-scale machine learning problems, particularly in scenarios with limited memory.
Weaknesses:
1. Non-Uniform Sampling: While the paper mentions non-uniform sampling (NUS), it does not adequately compare its proposed methods to prior NUS-based approaches, especially in practical scenarios where \( L_i \) is unknown. This omission weakens the discussion of the relative advantages of the proposed methods.
2. Experimental Section: The experimental results are the weakest part of the paper. Key issues include:
   - Lack of error bars, which makes it difficult to assess the statistical significance of the results.
   - Poor plot resolution and unclear labels, which hinder interpretability.
   - Minimal observed differences between uniform and Lipschitz sampling in Experiment 3, which raises questions about the practical utility of the proposed sampling strategies.
3. Challenging Domains: The experiments are limited to logistic regression and `l2`-regularized HSVM. Testing the methods on more challenging domains, such as deep learning or structured prediction tasks, would strengthen the paper's claims.
4. Limitations: The paper does not sufficiently discuss the limitations of the proposed methods, such as scenarios where the growing-batch strategy or support vector exploitation may not be effective.
5. Open-Source Code: Releasing open-source code for the proposed methods would significantly enhance the paper's impact and reproducibility.
6. Typos and Formatting: Several typos and formatting issues (e.g., L19, L53, L96, etc.) detract from the paper's overall polish and readability.
Recommendation:
The paper is strong in its theoretical contributions but is weakened by its experimental section and lack of practical considerations. To improve, the authors should:
- Provide a more comprehensive comparison to prior NUS-based approaches.
- Strengthen the experimental section by addressing the issues mentioned above.
- Explore more challenging problem domains.
- Discuss limitations and release open-source code.
Arguments for Acceptance:
- Strong theoretical contributions with novel ideas.
- Potential for significant impact on practical implementations of SVRG.
Arguments Against Acceptance:
- Weak experimental validation.
- Insufficient comparison to prior work and lack of discussion on limitations.
In conclusion, while the paper has substantial theoretical merit, its practical contributions are underdeveloped. If the experimental section and practical considerations are improved, this work could make a meaningful contribution to the field.