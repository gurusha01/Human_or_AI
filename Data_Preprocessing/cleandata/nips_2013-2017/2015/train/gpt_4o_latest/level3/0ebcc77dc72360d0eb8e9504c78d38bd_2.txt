This paper introduces a novel Bayesian optimization algorithm, Infinite-Metric GP Optimization (IMGPO), which achieves exponential convergence without requiring auxiliary optimization or the impractical δ-cover sampling. The proposed method leverages a space/interval partitioning approach and uses Gaussian Process (GP) priors with Upper Confidence Bound (UCB) for sample selection. The authors provide a rigorous theoretical analysis, demonstrating an exponential regret bound, which represents a significant improvement over prior methods such as GP-UCB and BaMSOO. The algorithm is well-described, and the illustrative example aids in understanding its mechanics.
Strengths:
1. Novelty and Theoretical Contribution: The paper addresses a long-standing challenge in Bayesian optimization by achieving exponential convergence without δ-cover sampling or auxiliary optimization. This is a meaningful advancement over existing methods.
2. Clarity and Presentation: The algorithm is clearly described, with detailed pseudocode and an illustrative example. The theoretical analysis is thorough, and the remarks provide valuable insights into the algorithm's advantages and limitations.
3. Theoretical Rigor: The regret analysis is robust, and the authors provide a detailed explanation of how the algorithm benefits from considering infinitely many possible bounds.
4. Practical Implications: By eliminating auxiliary optimization, the method reduces computational overhead, making it more practical for real-world applications.
Weaknesses:
1. Experimental Limitations: The experiments are primarily conducted on 1D test functions, with limited exploration of higher-dimensional problems. This raises concerns about the scalability of the interval partitioning approach, as the resolution requirements grow exponentially with dimensionality.
2. Scalability Challenges: The relationship to the DIRECT algorithm, which struggles with high-dimensional problems, suggests that IMGPO may face similar difficulties. While the authors acknowledge this, the paper lacks a concrete discussion or empirical validation of scalability strategies.
3. Acquisition Function Generality: The paper exclusively uses UCB for sample selection. It would be helpful to clarify whether other acquisition functions, such as Expected Improvement (EI), could be integrated into the framework.
4. Minor Clarity Issues: Some statements (e.g., on pages 3 and 4) require clearer explanations to improve accessibility for readers less familiar with the topic.
Arguments for Acceptance:
- The paper makes a novel and theoretically sound contribution to Bayesian optimization, addressing a key limitation of prior methods.
- The exponential regret bound is a significant improvement over existing approaches, advancing the state of the art.
- The algorithm is clearly described, and the theoretical insights are valuable for the broader optimization community.
Arguments Against Acceptance:
- The experimental evaluation is limited, particularly for higher-dimensional problems, which are critical for assessing the algorithm's practical utility.
- Scalability concerns are acknowledged but not adequately addressed, leaving open questions about the method's applicability to real-world, high-dimensional tasks.
Suggestions for Improvement:
1. Extend the experimental evaluation to include higher-dimensional test functions and real-world applications to validate scalability.
2. Discuss potential strategies for addressing scalability challenges, such as leveraging additional assumptions or dimensionality reduction techniques.
3. Clarify whether UCB is essential or if other acquisition functions could be used within the proposed framework.
4. Improve the clarity of certain statements to make the paper more accessible to a broader audience.
Recommendation:
This paper is a strong theoretical contribution to Bayesian optimization and merits acceptance, provided the authors address the scalability concerns and clarify minor points. The work is likely to inspire further research and practical advancements in the field.