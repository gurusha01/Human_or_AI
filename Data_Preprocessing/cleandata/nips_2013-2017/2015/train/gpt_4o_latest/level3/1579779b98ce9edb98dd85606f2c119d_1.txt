The paper introduces a novel approach for global optimization of decision trees using a nonconvex relaxation technique. Unlike traditional greedy methods that optimize split functions one node at a time, this work proposes a framework for jointly optimizing all split functions and leaf parameters under a global objective. The authors establish a connection between decision tree optimization and structured prediction with latent variables, leveraging a convex-concave upper bound on empirical loss. The proposed method employs stochastic gradient descent (SGD) for optimization, enabling efficient training even for deep trees. Experimental results on standard classification benchmarks demonstrate that the non-greedy decision trees outperform greedy baselines in terms of generalization performance.
Strengths:  
The paper addresses a long-standing limitation of greedy decision tree induction by proposing a global optimization framework. The theoretical contribution of linking decision tree learning to structured prediction with latent variables is significant and novel. The use of a convex-concave upper bound and efficient SGD-based optimization ensures scalability to large datasets and deep trees. The authors also provide a detailed analysis of computational complexity, introducing a fast loss-augmented inference algorithm that significantly reduces runtime. The paper is clearly written, well-organized, and provides sufficient technical detail for reproducibility. Furthermore, the results indicate that the proposed method achieves better generalization and is less prone to overfitting compared to greedy baselines.
Weaknesses:  
Despite its theoretical elegance, the experimental results are somewhat underwhelming. While the proposed method outperforms greedy baselines, the improvements are modest and dataset-dependent. The paper does not adequately explore whether global optimization inherently increases the risk of overfitting, a concern raised in prior work. Additionally, the authors could have strengthened their results by incorporating the suggested improvement of optimizing the last split and leaf node in the standard way after global optimization. The choice of benchmarks, while standard, does not fully showcase the potential of the method in more challenging or diverse scenarios.
Suggestions for Improvement:  
1. Investigate whether global optimization leads to overfitting by conducting experiments with varying levels of regularization and tree depths.  
2. Implement the suggested refinement of optimizing the last split and leaf node using a standard greedy approach to evaluate its impact on performance.  
3. Expand the experimental evaluation to include more challenging datasets or real-world applications to better demonstrate the method's practical utility.  
Pro/Con Arguments for Acceptance:  
- Pro: The paper presents a novel and theoretically sound approach to a well-known problem, with potential to inspire further research in decision tree optimization.  
- Con: The experimental results, while promising, are not groundbreaking, and the practical benefits of the approach remain somewhat unclear.  
Conclusion:  
Overall, the paper makes a meaningful contribution to the field of decision tree optimization and is deserving of publication despite its limitations. The theoretical insights and methodological advancements outweigh the modest experimental results, making this work a valuable addition to the conference.