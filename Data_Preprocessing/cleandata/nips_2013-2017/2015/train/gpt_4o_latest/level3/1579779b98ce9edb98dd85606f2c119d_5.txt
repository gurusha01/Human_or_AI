This paper introduces a novel algorithm for learning decision trees with linear split functions through global optimization, moving beyond the traditional greedy approaches. The authors propose a convex-concave upper bound on the tree's empirical loss, regularized using L2 constraints, and optimize it using stochastic gradient descent (SGD). A key contribution is the connection drawn between decision tree optimization and structured prediction with latent variables, which provides a theoretical foundation for their approach. Empirical results demonstrate that the globally optimized trees outperform greedy baselines in some cases, though the improvements are limited and come at a higher computational cost.
Strengths:
1. Theoretical Contribution: The paper provides an elegant formulation linking decision tree learning to structured prediction with latent variables. This is a novel perspective that could inspire further research in tree-based methods.
2. Optimization Framework: The use of a convex-concave upper bound and L2 regularization is well-motivated and addresses overfitting, a common issue in decision tree learning.
3. Scalability: The authors reduce the computational complexity of gradient updates for deep trees from \(O(2^dp)\) to \(O(d^2p)\), making the approach feasible for larger datasets.
4. Empirical Evaluation: The paper evaluates the proposed method on multiple benchmark datasets, demonstrating its potential to improve generalization performance and reduce overfitting compared to greedy baselines.
Weaknesses:
1. Empirical Results: While the method achieves marginal improvements over greedy baselines, the gains are not substantial enough to justify the significantly higher computational cost. This limits the practical impact of the proposed approach.
2. Clarity Issues: The description of the stable version of SGD (SSGD) and the assignment of data points to leaves is insufficiently detailed. Including pseudo-code for SSGD would improve reproducibility.
3. Figure 2 Ambiguity: The distinction between SGD and stable SGD in Figure 2 is unclear, as is the definition of "active" leaves. These ambiguities hinder the reader's ability to fully understand the results.
4. Projection onto Simplex: The projection of \(\Theta\) onto a simplex in Algorithm 1 is not well-explained. The practical implementation and purpose of this step require clarification.
5. Writing Quality: The paper contains minor issues such as unclear phrasing, typos, and inconsistencies in equations and terminology. These detract from the overall readability and polish of the manuscript.
6. Supplementary Material: Additional typographical errors and unclear notations in the supplementary material further reduce clarity.
Recommendation:
While the paper makes a solid theoretical contribution and proposes an interesting optimization framework, its practical significance is limited by the modest empirical gains and high computational cost. The lack of clarity in key sections also detracts from its overall quality. To strengthen the paper, the authors should:
- Provide pseudo-code for SSGD and clarify its role in the optimization process.
- Elaborate on the projection step in Algorithm 1.
- Address ambiguities in figures and definitions, particularly regarding "active" leaves.
- Improve the clarity and consistency of the writing, including the supplementary material.
Arguments for Acceptance:
- The paper introduces a novel theoretical perspective on decision tree optimization.
- The optimization framework is well-founded and could inspire future extensions.
- It addresses a challenging problem in decision tree learning with a principled approach.
Arguments Against Acceptance:
- The empirical results fail to convincingly demonstrate the practical benefits of global optimization.
- The computational cost is significantly higher than that of greedy methods.
- Clarity issues and ambiguities reduce the paper's accessibility and reproducibility.
Overall, this paper is a valuable theoretical contribution but falls short in practical impact and clarity. It is borderline for acceptance, contingent on addressing the identified weaknesses.