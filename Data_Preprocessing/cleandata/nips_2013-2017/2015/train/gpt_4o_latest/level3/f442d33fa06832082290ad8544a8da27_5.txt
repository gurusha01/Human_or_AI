The paper presents "skip-thoughts," a novel approach for learning unsupervised sentence embeddings by generalizing the skip-gram model to sentences. Using an encoder-decoder RNN framework, the model predicts the preceding and succeeding sentences from a given sentence, enabling unsupervised training on raw text. The authors evaluate the resulting sentence embeddings on eight diverse tasks, demonstrating competitive performance against many baselines with minimal feature engineering. However, the model does not achieve state-of-the-art results, and several limitations are evident in its experimental setup and methodology.
Strengths:
1. Conceptual Simplicity and Technical Sensibility: The paper builds on well-established RNN-based encoder-decoder frameworks and adapts the skip-gram objective to sentences, providing a clear and intuitive approach to unsupervised sentence representation learning. The vocabulary expansion strategy is particularly appreciated for its practicality in addressing out-of-vocabulary words.
2. Robustness Across Tasks: The skip-thought vectors perform consistently well across diverse tasks, including semantic relatedness, paraphrase detection, and classification benchmarks, highlighting the generality of the learned representations.
3. Minimal Feature Engineering: The approach requires little task-specific tuning, making it an attractive off-the-shelf solution for practitioners.
4. Scalability: The use of a large corpus (BookCorpus) and the ability to encode nearly a million words post-vocabulary expansion demonstrate the scalability of the method.
Weaknesses:
1. Lack of Novelty: While the approach is technically sound, it lacks significant innovation compared to existing methods like paragraph vectors. The use of RNNs and auxiliary tasks for representation learning is well-trodden ground.
2. Baseline Comparisons: The absence of baselines trained on the same dataset (BookCorpus) makes it unclear whether the reported performance gains stem from the model architecture or the dataset's size and quality. Comparisons with simpler baselines, such as bag-of-words or word embeddings trained on BookCorpus, are notably missing.
3. Unaddressed RNN Limitations: The paper does not address known limitations of RNN encoder-decoder models, such as the bottleneck of summarizing input into a single vector. Alternatives like attention mechanisms, which could mitigate these issues, are not explored.
4. Insufficient Experimental Details: The paper lacks adequate discussion on hyperparameter tuning, sensitivity analysis, and performance scaling with data size. These omissions hinder reproducibility and a deeper understanding of the model's behavior.
5. Evaluation Gaps: The experimental evaluation suffers from data mismatch issues between baselines and the proposed approach. Additionally, comparisons with paragraph vector models under matched conditions are recommended but not provided.
Recommendation:
While the paper introduces a technically sound and practical approach for unsupervised sentence representation learning, its lack of novelty, insufficient baseline comparisons, and unaddressed limitations reduce its impact. The work is a valuable contribution to the field, particularly for its robustness across tasks and scalability, but it falls short of advancing the state of the art. I recommend acceptance with the expectation that the authors address the evaluation gaps and provide more detailed analyses in future iterations. 
Arguments for Acceptance:
- Robust performance across diverse tasks.
- Practical and scalable approach with minimal feature engineering.
- Clear and technically sound methodology.
Arguments Against Acceptance:
- Limited novelty compared to existing methods.
- Insufficient baseline comparisons and experimental details.
- Lack of exploration of alternatives to address known RNN limitations. 
Final Verdict: Weak Accept