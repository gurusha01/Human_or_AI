The paper presents several innovative strategies to enhance the performance of Stochastic Variance-Reduced Gradient (SVRG) methods, addressing key limitations such as the computational cost of gradient evaluations. The authors propose three main contributions: (i) a growing batch size strategy to reduce gradient evaluations in early iterations, (ii) a mixed SGD/SVRG method to leverage the rapid initial progress of SGD, and (iii) a novel heuristic for identifying support vectors to reduce gradient computations in later iterations. Additionally, the paper explores alternative mini-batching schemes and provides a theoretical justification for the commonly used regularized SVRG update. These contributions are supported by theoretical analyses and empirical evaluations on logistic regression and Huberized hinge loss problems.
Strengths:
1. Innovative Modifications: The proposed approaches, particularly the growing batch size strategy and the mixed SGD/SVRG method, are well-motivated and address practical inefficiencies in SVRG. The heuristic for identifying support vectors is novel and could have significant implications for problems with sparse solutions.
2. Theoretical Rigor: The paper provides detailed theoretical analyses, including convergence guarantees for the proposed methods. These analyses are thorough and demonstrate the robustness of SVRG under inexact gradient computations.
3. Empirical Validation: The experimental results, though limited to specific datasets, demonstrate the effectiveness of the proposed methods in reducing computational costs while maintaining or improving convergence rates.
4. Clarity in Presentation: The paper is well-organized, with clear explanations of the proposed methods and their theoretical underpinnings. The inclusion of pseudo-code for the algorithms enhances reproducibility.
Weaknesses:
1. Lack of Empirical Comparisons: While the paper compares variants of SVRG, it does not provide a direct empirical comparison against a well-tuned SGD implementation. Such a comparison is crucial to contextualize the practical benefits of the proposed methods.
2. Unclear Metrics: The calculation of "time to reach Eest + Eopt for FG and SVRG" is not sufficiently explained. The dependencies and assumptions underlying these results should be clarified to ensure transparency.
3. Limited Scope of Experiments: The experiments focus primarily on logistic regression and Huberized hinge loss. While these are relevant benchmarks, additional experiments on more diverse machine learning models (e.g., deep neural networks) would strengthen the paper's claims about general applicability.
4. Mixed Results for Some Methods: The mixed SGD/SVRG strategy showed inconsistent performance across datasets, which raises questions about its robustness and practical utility.
Recommendation:
The paper makes significant contributions to the optimization literature, particularly in improving the efficiency of SVRG methods. However, the lack of empirical comparisons with well-tuned SGD and the limited diversity of experimental benchmarks are notable gaps. Addressing these issues would enhance the paper's impact and provide a more comprehensive evaluation of the proposed methods.
Arguments for Acceptance:
- The paper introduces novel and theoretically sound modifications to SVRG.
- The growing batch size and support vector heuristics are particularly promising for practical applications.
- The theoretical analyses are rigorous and provide valuable insights into the behavior of SVRG under inexact gradient computations.
Arguments Against Acceptance:
- The absence of a direct comparison with well-tuned SGD limits the practical relevance of the results.
- The inconsistent performance of the mixed SGD/SVRG method suggests that further refinement is needed.
- The experimental evaluation is somewhat narrow in scope.
In conclusion, the paper is a strong contribution to the field of optimization and stochastic methods, but it would benefit from additional empirical comparisons and broader experimental validation. I recommend acceptance with minor revisions to address these concerns.