The paper introduces Rectified Factor Networks (RFNs), a novel approach to factor analysis that enforces non-negativity and normalization constraints on the posterior using posterior regularization. This method is positioned as a significant advancement in unsupervised feature learning, particularly for constructing sparse, non-linear, and high-dimensional representations. The authors provide rigorous theoretical contributions, including proofs of convergence and correctness, and propose a scalable learning algorithm based on generalized alternating minimization (GAM). The work is innovative, offering a fresh perspective on sparse coding and generative modeling, and aligns well with recent trends in deep learning.
Strengths:  
The paper's primary strength lies in its methodological novelty. By leveraging posterior regularization, RFNs address limitations of existing unsupervised methods like autoencoders and RBMs, which often fail to guarantee sparse and non-negative representations. The theoretical analysis is robust, with detailed proofs of convergence and correctness, lending credibility to the proposed approach. The scalability of the RFN algorithm, demonstrated through efficient E-step updates, is a notable contribution, particularly for large datasets. The supplementary material is thorough, providing additional insights and implementation details. Furthermore, the application of RFNs to pharmaceutical datasets highlights their practical utility in detecting rare and small events, which other methods missed.
Weaknesses:  
The empirical evaluation is a significant weakness. While the authors compare RFNs to various baseline methods, the results are underwhelming and lack clarity. The omission of test set likelihood evaluations for one-layer and multi-layer models is a critical gap, as it hinders a comprehensive assessment of the model's performance. Synthetic data results are difficult to interpret, and the classification baselines used are outdated, limiting the relevance of the comparisons. Additionally, the paper makes imprecise claims about the limitations of unsupervised methods like RBMs and autoencoders, which could benefit from more nuanced discussion. The success of non-sparse representations, such as maxout networks, is not addressed, and the treatment of normalizing constraints requires further elaboration. Finally, the projected Newton method, a key component of the algorithm, is not explicitly defined in the main text, which could affect reproducibility.
Pro and Con Arguments:  
Pro:  
1. Innovative approach with strong theoretical foundations.  
2. Scalable algorithm suitable for large datasets.  
3. Practical applications in drug discovery demonstrate real-world relevance.  
Con:  
1. Weak empirical evaluation with outdated baselines.  
2. Lack of test set likelihood evaluations and unclear synthetic data results.  
3. Insufficient discussion of related methods and alternative approaches.  
Recommendation:  
Despite its empirical limitations, the paper makes a sound theoretical contribution and introduces a scalable method with potential for significant impact. Post-rebuttal, the authors' argument against likelihood usage is somewhat convincing, though deeper models should ideally outperform shallow ones. Emphasizing CIFAR results in the main text would improve alignment with deep learning benchmarks. I recommend acceptance with a score of 7, acknowledging the method's soundness and scalability while urging the authors to address the empirical shortcomings in future work.