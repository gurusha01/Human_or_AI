The paper introduces the Projected Langevin Monte Carlo (PLMC) algorithm, a novel method for sampling from log-concave distributions with compact support. This work is significant as it provides the first Markov chain algorithm with provable guarantees that relies on a first-order oracle, contrasting with existing methods like hit-and-run (H&R), ball walk, and lattice walk, which require zeroth-order oracles. The authors establish a polynomial-time bound for sampling with prescribed accuracy, a notable theoretical contribution to computational statistics and machine learning. The proof leverages elegant stochastic calculus techniques, particularly the Skorohod embedding, which will likely resonate with mathematically inclined readers.
Strengths:
1. Originality and Significance: The introduction of PLMC addresses a gap in the literature by focusing on first-order oracles, which are more practical in many machine learning applications. The polynomial-time convergence bound is a significant theoretical advancement.
2. Technical Rigor: The proof is mathematically sound and well-structured, employing sophisticated tools like reflected Brownian motion and Wasserstein distance. The use of Skorohod embedding is particularly innovative.
3. Contextualization: The paper situates its contributions within a broad context, connecting PLMC to stochastic gradient descent (SGD) and existing Markov Chain Monte Carlo (MCMC) methods.
4. Preliminary Experiments: The experimental comparison of PLMC with H&R provides initial evidence of its practical viability, showing comparable accuracy and faster runtime in some cases.
Weaknesses:
1. Clarity: While the theoretical exposition is rigorous, certain sections, such as the introduction of the pair \((x, \phi)\) in Section 2.1, are unnecessarily complex and could be simplified for better readability.
2. Discussion of Assumptions: The reliance on both a first-order oracle and a projector to the compact set is not sufficiently discussed. Practical implications and computational challenges of these requirements should be elaborated.
3. Comparison with Zeroth-Order Oracles: The relationship between PLMC and H&R's zeroth-order oracle, particularly in the context of sampling from restricted distributions on segments, needs clarification.
4. Minor Issues: The manuscript contains minor typos (e.g., "Dirac," "paths," \(\bar{W}_T\)) and notational inconsistencies. Additionally, precise references for the Khintchine inequality and the uniform distribution as a stationary distribution for reflected Brownian motion should be included.
Recommendation:
The paper makes a valuable theoretical contribution and is well-suited for the conference. However, revisions are necessary to improve clarity, address practical implications of the algorithm's assumptions, and correct minor errors. 
Arguments for Acceptance:
- Novel and significant theoretical result on the computational complexity of sampling.
- Rigorous and elegant proof techniques.
- Relevance to machine learning and Bayesian inference, with potential for future extensions.
Arguments Against Acceptance:
- Clarity issues in some sections.
- Insufficient discussion of practical implications and computational challenges.
- Limited experimental validation.
Overall, I recommend acceptance with minor revisions, as the paper advances the state of the art in sampling algorithms and provides a strong foundation for future research. Confidence: 4/5.