The paper introduces "Highway Networks," a novel neural network architecture inspired by Long Short-Term Memory (LSTM) networks, designed to address the challenges of training very deep networks. By incorporating adaptive gating mechanisms, Highway Networks enable unimpeded information flow across layers, facilitating the training of networks with hundreds of layers using standard gradient descent. The authors provide both theoretical insights and empirical evidence, demonstrating the superior optimization and generalization capabilities of Highway Networks compared to traditional "plain" networks. The paper also highlights the dynamic, input-dependent routing of information through the network, a feature that distinguishes it from hard-wired shortcut connections.
Strengths:
1. Well-Described Idea and Convincing Experiments: The paper clearly articulates the motivation behind Highway Networks and provides a strong theoretical foundation. The experimental results, particularly the comparisons with plain networks and Fitnets, convincingly demonstrate the advantages of the proposed architecture.
2. Analysis of Learned Networks: The analysis of gate output patterns and layer importance is insightful, offering a deeper understanding of how Highway Networks utilize depth dynamically based on the complexity of the task.
3. Practical Contributions: The ability to train very deep networks directly with gradient descent is a significant advancement, as it simplifies training and eliminates the need for complex initialization or multi-stage training procedures.
Weaknesses:
1. Incorrect Gradients in Equation 5: The use of a sigmoid activation for the gate unit \(T\) leads to incorrect gradients in Equation 5. The authors need to clarify the backpropagation process to ensure the theoretical soundness of their claims.
2. Initialization Method: The proposed initialization method using constant negative biases is unsatisfactory. A performance plot showing the sensitivity of the network to different initial bias values would provide valuable insights.
3. Unclear Experimental Comparisons: The differences in experimental setups are not explicitly clarified, which raises concerns about the validity of the comparisons. A more detailed description of the setups is necessary.
4. Statistical Reporting: Reporting only "max" accuracies in tables is statistically unreasonable. The inclusion of mean accuracy and standard deviation would provide a more robust evaluation of the results.
5. Lack of Theoretical Perspective: While the empirical results are strong, a learning theory perspective on how gate units enable selective identity layers would enhance the paper's contribution.
Arguments for Acceptance:
- The paper addresses an important and challenging problem in deep learning, advancing the state of the art in training very deep networks.
- The proposed architecture is novel and supported by strong empirical evidence.
- The analysis of gate behavior and layer importance provides valuable insights for the community.
Arguments Against Acceptance:
- The incorrect gradients and lack of clarity in backpropagation raise concerns about the technical correctness of the approach.
- The initialization method and experimental comparisons need further refinement and clarification.
- The statistical reporting of results is suboptimal and needs to be improved.
Recommendation:
The paper makes a significant contribution to the field and is likely to inspire further research on deep architectures. However, the authors must address the issues related to gradients, initialization, experimental clarity, and statistical reporting. With these revisions, the paper would be a strong candidate for acceptance.