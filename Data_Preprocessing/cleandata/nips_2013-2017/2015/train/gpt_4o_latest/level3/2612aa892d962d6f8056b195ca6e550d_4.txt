The paper presents a method that combines trajectory optimization with policy learning to enable interactive control of complex characters, addressing a challenging problem in high-dimensional continuous control. The approach alternates between optimizing trajectories and training a neural network policy, leveraging existing methods for each subproblem. This joint optimization framework is validated through experiments on diverse tasks, including swimming, flying, and bipedal/quadrupedal locomotion, demonstrating impressive results across various morphologies.
Strengths:
1. Technical Sophistication and Practicality: The paper showcases a technically sound and well-executed approach. By alternating between trajectory optimization and policy learning, the method reuses established techniques, making it practical and scalable. The use of sensory noise during training and the perturbation-based feedback gain computation are particularly compelling, as they enhance robustness and generalization.
2. Impressive Results: The method achieves realistic and stable behaviors for a wide range of tasks and morphologies, including 3D locomotion, which is significantly more complex than prior 2D work. The supplementary video convincingly demonstrates the quality of the learned behaviors.
3. Clarity: The paper is well-written, with effective organization and cross-referencing. The technical details are presented with sufficient depth, enabling reproducibility for expert readers.
4. Validation: Ablation studies and comparative evaluations (e.g., against static trajectory datasets and model-predictive control) provide strong evidence for the effectiveness of the joint optimization approach and noise injection.
Weaknesses:
1. Limited Novelty: While the results are impressive, the paper lacks significant theoretical contributions. The approach primarily combines existing techniques (trajectory optimization and neural network regression) rather than introducing fundamentally new ideas.
2. Motivation for Extra Optimization Step: The rationale for re-optimizing at every timestep (Section 7) is not clearly articulated. Questions remain about whether this step is necessary due to residual trajectory costs or policy regression errors. Additionally, re-optimization raises concerns about physical constraints, which are only softly enforced.
3. Related Work: The paper could strengthen its argument by referencing similar recent approaches (e.g., line 153) that advocate for small updates in parameter space. This would better contextualize the contributions.
4. Typographical Error: A minor typo is noted at line 89: "to unity" should be "to unify."
Pro and Con Arguments for Acceptance:
Pros:
- Demonstrates a practical and scalable method for interactive control of complex characters.
- Produces state-of-the-art results for diverse and challenging tasks.
- Clearly written and well-validated through experiments and ablation studies.
Cons:
- Limited novelty in terms of theoretical insights.
- Unclear necessity and implications of the extra optimization step.
- Missing references to closely related work.
Conclusion:
This paper represents a strong contribution to the field of interactive control and character animation, with significant practical value and impressive results. However, the lack of theoretical novelty and unclear motivation for certain design choices slightly diminish its impact. I recommend acceptance, provided the authors address the concerns about the extra optimization step and include additional references to related work.