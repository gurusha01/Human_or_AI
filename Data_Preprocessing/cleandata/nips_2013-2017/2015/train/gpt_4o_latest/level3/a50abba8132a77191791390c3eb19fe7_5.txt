The paper introduces three novel approaches to enhance the convergence speed of the Stochastic Variance Reduced Gradient (SVRG) algorithm, presenting a comprehensive analysis and practical improvements. The authors begin by providing a clear and concise introduction to SVRG, its limitations, and the motivation for their enhancements. The paper is well-written, logically structured, and accessible to readers familiar with optimization techniques in machine learning.
The first contribution of the paper is demonstrating that SVRG does not require a highly accurate gradient approximation in the early iterations, which allows for batching strategies with growing batch sizes. This insight is leveraged to propose a batching SVRG algorithm that maintains the same convergence rate while reducing computational overhead. The second contribution is a mixed stochastic gradient/SVRG approach, which combines the rapid initial progress of stochastic gradient methods with the linear convergence of SVRG. The authors provide a rigorous proof of convergence for this hybrid method. The third contribution is a speed-up technique for Huberized hinge-loss support vector machines (HSVMs) by identifying and exploiting support vectors, which reduces the number of gradient evaluations in later iterations.
The paper also extends convergence analysis to regularized SVRG algorithms and explores alternative mini-batching strategies. These theoretical contributions are complemented by extensive simulations on logistic regression and HSVM tasks, demonstrating the effectiveness of the proposed methods. Notably, the growing batch strategy consistently outperforms the original SVRG in terms of test error and training objective, while the mixed SG/SVRG approach shows mixed results depending on the dataset.
Strengths:  
1. The paper is technically sound, with well-supported claims through theoretical analysis and empirical validation.  
2. The proposed methods are practical, computationally efficient, and relevant to large-scale machine learning problems.  
3. The work is original, addressing key limitations of SVRG and introducing novel batching and hybrid strategies.  
4. The paper is clearly written and well-organized, making it easy to follow the theoretical derivations and experimental results.  
Weaknesses:  
1. While the mixed SG/SVRG approach is theoretically justified, its empirical performance is inconsistent across datasets, warranting further investigation.  
2. The heuristic for identifying support vectors, while effective in practice, lacks a formal theoretical guarantee.  
3. Minor typos and formatting issues need to be addressed for a polished presentation.
Pro Acceptance Arguments:  
- The paper advances the state of the art in stochastic optimization, a core topic for the NIPS community.  
- The proposed methods are both theoretically rigorous and practically impactful.  
- The extensive experiments provide strong evidence of the effectiveness of the contributions.  
Con Acceptance Arguments:  
- The mixed SG/SVRG approach requires further empirical refinement.  
- The support vector heuristic, while promising, could benefit from additional theoretical analysis.  
In summary, this paper makes significant contributions to the field of stochastic optimization, addressing practical challenges in SVRG and offering novel solutions with strong theoretical and empirical support. It is highly relevant to the NIPS community and should be accepted with minor revisions to address the noted weaknesses.