Review
Summary
This paper presents a significant theoretical advancement in the analysis of the Frank-Wolfe (FW) optimization algorithm and its variants. Specifically, it proves global linear convergence for four FW variants—away-steps FW, pairwise FW, fully-corrective FW, and Wolfe's minimum norm point algorithm—under weaker conditions than previously established. The authors introduce a novel geometric quantity, the pyramidal width, which, when combined with the classical condition number of the objective function, determines the convergence rate. The paper also provides a unified framework for analyzing these algorithms, addressing limitations of prior work that relied on stronger assumptions such as Robinson's condition or the location of the optimum within the domain. Additionally, the paper highlights practical applications of these FW variants in machine learning, such as submodular optimization and sparse regression.
Strengths
1. Quality: The paper is technically sound and provides rigorous proofs for its claims. The introduction of the pyramidal width as a geometric measure is both novel and insightful, offering a deeper understanding of the convergence behavior of FW variants.
2. Clarity: The theoretical contributions are well-organized, and the proofs are detailed. The use of illustrative experiments, such as constrained Lasso regression and video co-localization, effectively demonstrates the practical relevance of the proposed methods.
3. Originality: The work offers a unified perspective on FW variants and introduces a weaker sufficient condition for linear convergence, which is a notable departure from prior analyses. The connection between the convergence rate and the geometry of the constraint set is particularly innovative.
4. Significance: The results have broad implications for constrained convex optimization, especially in machine learning applications where FW methods are widely used. The paper serves as a valuable reference for researchers and practitioners working with FW algorithms.
Weaknesses
1. Clarity of Algorithmic Variants: While the paper provides an overview of the algorithmic differences between the FW variants, the descriptions are dense and could benefit from clearer exposition. For instance, the distinctions between away-steps FW and pairwise FW are not immediately intuitive. Moving some of the detailed algorithmic steps to the appendix and focusing on high-level differences in the main text would improve readability.
2. Empirical Evaluation: Although the experiments demonstrate the linear convergence of the variants, the empirical section is relatively brief. A more comprehensive comparison across a wider range of datasets and applications would strengthen the practical impact of the work.
3. Practical Implications of Pyramidal Width: While the pyramidal width is a compelling theoretical concept, its practical computation and implications for real-world problems are not fully explored. More discussion on how to estimate or bound this quantity in practice would enhance the paper's applicability.
Arguments for Acceptance
- The paper makes a substantial theoretical contribution by proving linear convergence under weaker conditions, which advances the state of the art in FW analysis.
- The unified framework and introduction of the pyramidal width provide a fresh perspective on FW variants.
- The results are highly relevant to machine learning and optimization, ensuring the paper's significance to the NIPS community.
Arguments Against Acceptance
- The clarity of algorithmic differences could be improved, as the current presentation may be challenging for readers unfamiliar with FW variants.
- The empirical evaluation is somewhat limited, leaving room for a more thorough exploration of the practical benefits of the proposed methods.
Recommendation
I recommend acceptance of this paper. Its theoretical contributions are both novel and significant, and the work is a valuable addition to the optimization and machine learning literature. However, the authors should consider revising the presentation of algorithmic details and expanding the empirical evaluation to further strengthen the paper.