The paper presents a kernel-based algorithm for cross-domain instance matching, embedding features from two domains into a shared Reproducing Kernel Hilbert Space (RKHS) using kernel embeddings of distributions. The authors propose a joint feature learning approach that minimizes the Maximum Mean Discrepancy (MMD) distance between paired instances while maximizing the likelihood of their correspondence. The method is evaluated on multilingual document matching, document-tag, and image-tag tasks, outperforming standard approaches like Canonical Correlation Analysis (CCA) and Kernel CCA (KCCA). While the use of kernel embeddings for bag-of-features is not novel, the application to cross-domain matching with a probabilistic framework is a notable contribution.
Strengths:
1. Technical Contribution: The paper introduces a novel probabilistic model leveraging kernel embeddings for cross-domain matching, which is a significant extension of existing methods like KCCA. The use of latent vectors in a shared space to represent features and instances is well-motivated.
2. Experimental Performance: The proposed method demonstrates superior performance across diverse datasets, including multilingual Wikipedia articles, document-tag, and image-tag matching tasks. The quantitative results consistently outperform baseline methods.
3. Writing Quality: The paper is well-written, organized, and easy to follow, with clear explanations of the methodology and experimental setup.
4. Relevance: The problem of cross-domain matching is important in fields like natural language processing, information retrieval, and computer vision, making the paper a valuable contribution to the community.
Weaknesses:
1. Motivation and Justification: The paper does not clearly articulate the limitations of KCCA or other baseline methods, which weakens the justification for the proposed approach. For instance, the claim that the method learns a "more complex representation" is not experimentally demonstrated.
2. Likelihood Function: The likelihood function in Equation 8 appears ad hoc, and its choice is not well-motivated. Simpler alternatives could potentially improve computational efficiency.
3. Computational Cost: The paper does not address the computational cost of the proposed method compared to KCCA, raising concerns about scalability for larger datasets.
4. Qualitative Insights: The lack of qualitative comparisons with KCCA (e.g., visualizations or case studies) limits the interpretability of the results.
5. Impact of Latent Dimension: The effect of varying the latent dimension (q) on precision and runtime is not explored, which is critical for understanding the method's robustness.
6. Hyper-Parameter Selection: The process for selecting hyper-parameters via "development data" is unclear, and the term itself is not well-defined.
7. Minor Issues: There are inconsistencies in reference formatting, unclear terminology for MMD, and insufficient clarity on the role of latent vectors in experiments.
Pro and Con Arguments:
- Pro: The method is technically sound, achieves state-of-the-art performance, and addresses a significant problem in cross-domain matching.
- Con: The lack of clear motivation, computational analysis, and qualitative insights weakens the overall impact of the work.
Recommendation:
While the paper introduces an interesting and effective method, the lack of clear motivation, computational analysis, and qualitative insights limits its overall contribution. I recommend acceptance with minor revisions, provided the authors address the concerns related to motivation, computational cost, and qualitative comparisons.