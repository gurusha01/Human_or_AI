The paper presents a general framework for the Expectation-Maximization (EM) algorithm in high-dimensional settings, addressing both parameter estimation and inferential procedures. The authors propose a high-dimensional EM algorithm that incorporates sparsity through a truncation step, achieving near-optimal statistical rates of convergence under appropriate initialization. Additionally, they introduce a decorrelated score test for hypothesis testing in high-dimensional latent variable models. The paper claims to provide the first computationally feasible approach for simultaneous optimal estimation and inference in such settings.
Strengths:  
1. Novelty in High-Dimensional EM: The proposed truncation step in the EM algorithm is a notable contribution, enabling sparsity enforcement and improving statistical rates of convergence. This is a significant advancement over prior work, which primarily focused on low-dimensional settings.  
2. Unified Framework: The paper provides a unified theoretical framework that integrates computational and statistical guarantees, which is valuable for both parameter estimation and inference.  
3. Broad Applicability: The framework is applied to Gaussian mixture models and regression mixtures, demonstrating its generalizability to a broad class of latent variable models.  
4. Theoretical Guarantees: The authors establish rigorous convergence results, including geometric convergence rates and asymptotic normality of the proposed score statistic, which is optimal in the semiparametric sense.  
Weaknesses:  
1. Initialization Conditions: The paper lacks clarity on the conditions required to find the initial solution \(\beta^{\rm init}\) (line 60, page 2). This is critical since the theoretical guarantees hinge on appropriate initialization.  
2. Algorithmic Guidance: While two algorithms (exact and gradient ascent M-step implementations) are proposed, the paper does not provide practical guidance on when to use each. This limits the usability of the methods in real-world applications.  
3. Hyperparameter Selection: The paper does not detail how to choose \(\hat{s}\) (sparsity parameter) and \(\lambda\) (tuning parameter) in practice. A discussion on their sensitivity or data-dependent selection methods is missing.  
4. Citation and Novelty: The decorrelated score test appears to be identical to prior work but is not adequately cited. The authors should clarify their novel contributions in its application to high-dimensional latent variable models.  
5. Technical Conditions: The theoretical results rely on numerous technical conditions, but their applicability to real-world latent variable models is not sufficiently discussed.  
6. Main Paper vs. Supplementary Material: The main paper serves more as a "trailer" for the supplementary material, with key technical challenges and proofs deferred. This reduces the accessibility of the work for readers.  
Pro vs. Con Arguments for Acceptance:  
- Pro: The paper addresses a significant gap in the literature by extending the EM algorithm to high-dimensional settings with rigorous theoretical guarantees. The proposed methods are novel and have the potential to advance the state of the art in latent variable modeling.  
- Con: The lack of practical guidance, clarity on initialization, and insufficient discussion of hyperparameter selection and technical conditions limit the immediate applicability of the work. Additionally, the reliance on supplementary material for key details detracts from the paper's self-contained nature.  
Recommendation:  
While the paper makes a strong theoretical contribution, the practical limitations and lack of clarity in certain areas warrant a major revision before acceptance. The authors should address the issues related to initialization, algorithmic guidance, hyperparameter selection, and citation of prior work. Including more details in the main paper and discussing the applicability of the technical conditions would also enhance the paper's impact.