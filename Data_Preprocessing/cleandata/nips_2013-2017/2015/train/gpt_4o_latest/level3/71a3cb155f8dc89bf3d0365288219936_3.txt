The paper addresses the estimation of sparse precision matrices with a Kronecker product structure for tensor-valued data, extending prior works on matrix graphical models to tensor graphical models. The authors propose an alternating minimization algorithm for this non-convex optimization problem and provide theoretical guarantees for its statistical consistency, including minimax-optimal rates of convergence. Notably, the paper claims that the proposed method achieves estimation consistency with a single tensor sample, a result not observed in previous work. The authors also present numerical experiments to validate the performance of their method.
Strengths:
1. Extension to Tensor Data: The paper successfully extends the sparse graphical model framework from matrices to tensors, addressing an important gap in the literature. This is particularly relevant for applications like fMRI and recommendation systems, where tensor-valued data is common.
2. Theoretical Guarantees: The authors provide rigorous theoretical results, including minimax-optimal rates of convergence in Frobenius, max, and spectral norms. The proof of model selection consistency is a noteworthy contribution.
3. Practical Implications: The ability to achieve estimation consistency with a single tensor sample is a significant result for real-world scenarios with limited data.
4. Computational Efficiency: The proposed Tensor Lasso (Tlasso) algorithm is computationally efficient, as demonstrated in the experiments, and outperforms alternative methods in terms of runtime and estimation accuracy.
Weaknesses:
1. Restrictive Assumptions: The theoretical results rely on strong assumptions, such as the irrepresentable condition and bounded eigenvalues, which may limit the practical applicability of the method. The initialization condition (3.4) is also restrictive, and the paper lacks clarity on how to find suitable initial estimates in high-dimensional settings.
2. Incremental Novelty: While the extension to tensor graphical models is valuable, the proposed alternating minimization algorithm and its theoretical guarantees are not fundamentally novel. The minimax-optimal results in Theorem 3.5 have been addressed in prior works [5, 8], and the comparisons in Remark 3.6 are incomplete.
3. Lack of Clarity: The paper does not adequately explain how the suggested initial values for precision matrices satisfy condition (3.4). This omission raises concerns about the practical implementation of the algorithm.
4. Limited Advancement: The contribution over prior works like [9] is incremental, as the paper primarily adapts existing methods to tensor data without introducing significant methodological innovations.
Pro and Con Arguments for Acceptance:
- Pro: The extension to tensor graphical models is timely and relevant, with rigorous theoretical guarantees and strong empirical performance.
- Con: The reliance on restrictive assumptions and the lack of substantial novelty limit the significance of the contribution.
Recommendation: While the paper provides a solid extension of existing methods to tensor graphical models, the incremental nature of the contribution and the reliance on restrictive assumptions reduce its impact. I recommend weak reject, encouraging the authors to address the clarity issues, provide more comprehensive comparisons, and explore ways to relax the assumptions in future revisions.