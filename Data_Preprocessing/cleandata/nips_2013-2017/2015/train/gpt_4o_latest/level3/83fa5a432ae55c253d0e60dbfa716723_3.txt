This paper introduces Principal Differences Analysis (PDA), a novel framework for identifying differences between high-dimensional distributions, and its sparse variant, SPARDA. The approach leverages the Cramer-Wold device to reduce the problem to univariate projections and uses the Wasserstein metric to quantify differences between distributions. The authors address the computational challenges of the resulting non-convex optimization problem through semidefinite relaxation and a tightening procedure. Empirical validations on synthetic datasets and a bioinformatics application demonstrate the method's utility, particularly in identifying sparse features responsible for distributional differences.
Strengths:
1. Novelty and Relevance: The paper addresses a critical problem in high-dimensional data analysis, proposing a novel combination of techniques. The use of the Wasserstein metric, known for its robustness in comparing distributions, is particularly compelling.
2. Theoretical Contributions: The authors provide rigorous theoretical guarantees, including concentration results and sparsistency properties for SPARDA. These results enhance the credibility of the proposed framework.
3. Algorithmic Innovation: The combination of semidefinite relaxation and a tightening procedure is well-motivated and effectively addresses the non-convexity of the problem. The RELAX algorithm is particularly noteworthy for its scalability.
4. Empirical Validation: The experiments are thorough, covering both synthetic and real-world datasets. The bioinformatics application is a strong demonstration of the method's practical utility, revealing biologically meaningful insights.
5. Clarity: The paper is well-written and organized, with a clear exposition of the problem, methodology, and results.
Weaknesses:
1. Terminology: The use of the term "Cramer-Wold device" is non-standard and could confuse readers familiar with its traditional interpretation. Clarifying this usage would improve accessibility.
2. Figure Descriptions: The figures lack sufficient detail, making it difficult for readers to fully interpret the results without extensive cross-referencing to the text.
3. Computational Complexity: While the authors address scalability, the computational cost of the semidefinite relaxation remains high, particularly for large datasets. A more detailed discussion of runtime trade-offs would be beneficial.
4. Comparison to Related Work: While the paper references related methods, such as sparse PCA and DiProPerm, the experimental comparisons could be expanded to include more baselines, particularly those tailored to high-dimensional two-sample testing.
Suggestions for Improvement:
- Provide a clearer explanation of the "Cramer-Wold device" in the context of this work to avoid confusion.
- Enhance figure captions to make them self-contained and more informative.
- Discuss the computational trade-offs of the proposed algorithms in greater detail, particularly for large-scale applications.
- Explore connections between the induced divergences and standard divergences in high-dimensional settings, as suggested for future work.
Arguments for Acceptance:
- The paper addresses a relevant and challenging problem with a novel and theoretically grounded approach.
- The empirical results are compelling, demonstrating the method's practical utility and superiority over existing techniques in certain scenarios.
- The theoretical contributions are significant and provide a strong foundation for future extensions.
Arguments Against Acceptance:
- The computational cost of the semidefinite relaxation may limit the method's applicability to very large datasets.
- The experimental comparisons could be more comprehensive, particularly in benchmarking against other high-dimensional two-sample testing methods.
Recommendation:
Overall, this paper makes a strong contribution to the field of high-dimensional data analysis. While there are minor issues that could be addressed in a revision, the novelty, theoretical rigor, and empirical validation make it a valuable addition to the conference. I recommend acceptance.