This paper presents a novel approach to decision tree (DT) optimization by reformulating it as a structured prediction problem using latent sign vectors for internal node decisions. The authors propose a differentiable upper bound on the non-differentiable, non-convex empirical loss of the tree, enabling optimization via stochastic gradient descent (SGD). The key contributions include the structured prediction formulation, efficient loss-augmented inference algorithms, and sparse gradient updates that reduce computational complexity. The method also introduces the intriguing possibility of growing infinitely deep trees by initializing weights to zero and gradually increasing complexity.
Strengths:
1. Novelty: The paper establishes a clear link between decision tree optimization and structured prediction with latent variables, a perspective that is both innovative and theoretically grounded.
2. Technical Contributions: The use of a convex-concave upper bound and efficient algorithms for loss-augmented inference demonstrates strong technical rigor. Sparse gradient updates are computationally efficient and open interesting avenues for tree growth.
3. Performance: Experimental results show that the proposed non-greedy trees outperform greedy baselines across multiple datasets, with improved generalization and reduced susceptibility to overfitting.
4. Scalability: The fast loss-augmented inference algorithm significantly reduces computational complexity, making the method feasible for deep trees and large datasets.
Weaknesses:
1. Experimental Details: The experimental section lacks sufficient baselines, particularly comparisons with alternative global optimization methods and pruning strategies. The absence of experiments with forests, despite the title mentioning them, is misleading.
2. Interpretability: Dense weight vectors resulting from L2 regularization may reduce the interpretability of decision trees, which is a key advantage of DTs. While L1 regularization could address this, its impact on performance remains unexplored.
3. Ambiguities: The handling of zero weights, critical for pruning and decision-making at nodes, is not clearly explained. This raises questions about the method's robustness and practical applicability.
4. Clarity: Certain sections, particularly those describing inference (e.g., 6.2 and 6.4), are difficult to follow and could benefit from clearer explanations and diagrams.
5. Practical Justification: While the method is theoretically compelling, its practical utility is undermined by interpretability concerns and limited experimental evidence.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound approach to a challenging problem.
- The proposed method achieves state-of-the-art performance on several benchmarks.
- Sparse gradient updates and efficient algorithms contribute to scalability and computational efficiency.
Arguments Against Acceptance:
- Insufficient experimental comparisons and lack of clarity in key sections.
- Practical concerns regarding interpretability and the handling of zero weights.
- The title's reference to forests is misleading, as the paper does not address this case.
Recommendation:
The paper is a strong theoretical contribution with significant potential, but it requires additional experimental validation and clarity in presentation. Post-rebuttal, I emphasize the importance of analyzing zero-weight initialization and encourage further exploration of interpretability and pruning strategies. Pending these improvements, I recommend acceptance with revisions.