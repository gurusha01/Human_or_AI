This paper addresses the challenging problem of regret minimization in non-stochastic multi-armed bandit (MAB) problems, focusing on high-probability performance guarantees. The authors generalize the principle of implicit exploration (IX) from prior work (Koc√°k et al., 2014) and propose a novel approach that avoids the explicit exploration component traditionally thought necessary for achieving high-probability regret bounds. The key theoretical contribution is a general concentration inequality for the sum of losses (Lemma 1), which enables tighter regret bounds and cleaner analyses for various MAB settings. The paper demonstrates the broad applicability of its results by improving regret bounds for three non-stochastic bandit problems, achieving a factor of 2 improvement in pre-factors compared to prior work. Additionally, the proposed algorithm, EXP3-IX, outperforms the classical EXP3 in experiments, showing more robust regret estimation and reduced sensitivity to hyperparameters.
Strengths:
1. Technical Contribution: The paper provides a significant theoretical advancement by proving high-probability regret bounds without explicit exploration. This challenges a longstanding assumption in the field and opens new avenues for algorithm design.
2. General Applicability: The results extend to multiple MAB settings, including bandits with expert advice, tracking the best arm, and bandits with side observations, showcasing the flexibility and broad relevance of the proposed approach.
3. Improved Regret Bounds: The paper achieves tighter bounds with better constants (e.g., a leading factor of \(2\sqrt{2}\) compared to \(5.15\) in prior work), advancing the state-of-the-art in high-probability guarantees.
4. Experimental Validation: The experiments demonstrate the practical utility of the proposed method, with EXP3-IX outperforming EXP3 and EXP3.P in robustness and empirical performance.
5. Clarity and Organization: The paper is well-written, with a logical progression from theoretical results to applications and experiments. The analysis is rigorous yet accessible.
Weaknesses:
1. Comparison with Related Work: While the paper highlights the limitations of EXP3, it does not compare EXP3-IX with EXP3.SIX, which is specifically designed for switching bandit problems. Such a comparison would strengthen the empirical evaluation.
2. Parameter Tuning: Although the authors recommend setting \(\gammat = \etat/2\), the practical difficulty of tuning these parameters remains a concern, particularly in real-world applications.
3. Typographical Error: A minor typo is noted in Algorithm 1 (line 4-5), where "\(\hat{}\)" should be replaced with "\(\sim\)." This should be corrected for clarity.
4. Limited Scope of Experiments: The experiments, while illustrative, are relatively simple. Additional benchmarks on more complex or real-world datasets would enhance the empirical validation.
Recommendation:
This paper makes a strong theoretical contribution to the field of non-stochastic bandits and provides practical insights through its experiments. Its results are broadly applicable and advance the state-of-the-art in high-probability regret bounds. However, the lack of comparison with EXP3.SIX and the limited scope of experiments slightly detract from its empirical impact. I recommend acceptance with minor revisions to address the typo and expand the experimental evaluation. 
Pros: Significant theoretical contribution, improved regret bounds, broad applicability, strong experimental results.  
Cons: Limited empirical scope, lack of comparison with EXP3.SIX, parameter tuning challenges.