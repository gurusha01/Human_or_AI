The manuscript presents a novel approach to approximating the predictive distribution in Bayesian regression for deep neural networks using a tractable parametric model. This method addresses the computational inefficiencies of Monte Carlo sampling, such as high memory usage and slow prediction times, by distilling the posterior predictive density into a single neural network. The authors employ stochastic gradient Langevin dynamics (SGLD) to generate posterior samples and minimize an averaged KL divergence between the true predictive distribution and the surrogate model. The proposed method is evaluated on toy and real-world datasets, demonstrating superior performance compared to state-of-the-art methods like expectation propagation (EP) and variational Bayes (VB).
Strengths:
1. Technical Soundness: The paper is technically solid, with a clear derivation of the proposed method and its connection to Bayesian inference. The use of SGLD for posterior sampling and the distillation process is well-motivated and effectively implemented.
2. Empirical Performance: The method outperforms competitors such as EP and VB in terms of predictive accuracy and calibration, as evidenced by simulation results on both classification and regression tasks. The reduction in computational cost compared to SGLD is particularly noteworthy.
3. Clarity: The manuscript is well-organized and provides sufficient details for reproducibility, including algorithmic descriptions and hyperparameter choices.
4. Significance: The work addresses a critical challenge in Bayesian deep learning—scalability—making it relevant for applications requiring uncertainty quantification, such as active learning and bandits.
Weaknesses:
1. Parameter Choices: The manuscript does not adequately discuss the sensitivity of the method to various hyperparameters, such as the priors for the teacher and student networks, learning rates, and noise levels in data generation. This omission limits the practical applicability of the method.
2. Originality: While the idea of distilling a Bayesian posterior into a single model is sensible and natural, it lacks surprising elements. Similar ideas, such as model distillation and teacher-student frameworks, have been explored in prior work, albeit in different contexts.
3. Scope of Evaluation: The experiments, though compelling, are limited in scope. For instance, the method's utility in large-scale or adversarial settings is not explored, and comparisons to more recent Bayesian neural network methods are missing.
Arguments for Acceptance:
- The method is a practical and scalable solution to Bayesian inference for neural networks, with strong empirical results.
- The paper is well-written, technically sound, and addresses a relevant problem in the field.
- The approach has potential applications in uncertainty-aware tasks, making it significant for the community.
Arguments Against Acceptance:
- The lack of discussion on hyperparameter sensitivity and robustness is a notable gap.
- The method, while effective, does not introduce fundamentally new concepts, which might limit its impact.
- The evaluation could be more comprehensive, particularly in challenging or large-scale settings.
Recommendation:
Overall, the paper makes a meaningful contribution to Bayesian deep learning by proposing a practical method to approximate predictive distributions. While it lacks groundbreaking novelty, its empirical performance and clarity make it a valuable addition to the field. I recommend acceptance, provided the authors address the hyperparameter sensitivity and expand the discussion on the method's limitations in the final version.