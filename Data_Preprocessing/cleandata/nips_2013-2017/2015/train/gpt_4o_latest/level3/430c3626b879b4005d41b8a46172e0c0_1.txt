The paper introduces "Equilibrated Stochastic Gradient Descent" (ESGD), a novel adaptive learning rate method for non-convex optimization, leveraging the equilibration preconditioner. The authors argue that ESGD addresses the limitations of the Jacobi preconditioner, particularly in the presence of both positive and negative curvature, and demonstrate its theoretical and empirical advantages. The proposed method is evaluated on two benchmarks, MNIST and CURVES, where ESGD outperforms RMSProp and standard SGD in terms of convergence speed and training error. The authors also provide insights into the similarity between RMSProp and the equilibration preconditioner, offering a potential explanation for RMSProp's success in practice.
Strengths:
1. Novelty and Theoretical Contribution: The paper presents a novel adaptive learning rate method (ESGD) and provides theoretical evidence for the superiority of the equilibration preconditioner over the Jacobi preconditioner in non-convex optimization. The connection between the equilibration preconditioner and the absolute Hessian is particularly insightful.
2. Empirical Validation: The experiments demonstrate that ESGD outperforms RMSProp and Jacobi-preconditioned SGD on the MNIST and CURVES datasets, supporting the theoretical claims.
3. Scalability: The authors propose a computationally efficient implementation of ESGD, making it practical for large-scale neural networks.
4. Insight into RMSProp: The observation that RMSProp's success may stem from its similarity to the equilibration preconditioner is an interesting contribution to understanding adaptive learning rates.
Weaknesses:
1. Experimental Limitations: The use of MNIST and CURVES datasets is a significant limitation. These datasets are relatively simple and may not provide meaningful insights into ESGD's performance on more complex, real-world problems.
2. Focus on Training Error: The evaluation focuses on training error (MSE) rather than test accuracy, which is more relevant for neural network applications. This limits the practical significance of the results.
3. Clarity Issues: The paper is difficult to follow in places. Algorithm 1 lacks clarity, and several notations (e.g., recalculation of $H$, interpretation of $D^{-1}$, and mismatch between $R(H,v)$ and Equation (11)) are ambiguous. Section 4's explanation of $D^E$ reducing the condition number also requires clearer articulation.
4. Lack of Convergence Rate Analysis: The absence of a convergence rate theorem for ESGD weakens its theoretical foundation and makes it harder to compare with other optimization methods rigorously.
5. Incomplete Comparisons: The experimental results plot performance against epochs rather than training time, which is a more practical metric for comparing optimization methods.
Recommendation:
While the paper introduces a promising method with strong theoretical underpinnings, its experimental evaluation is insufficient to demonstrate the broader applicability of ESGD. To strengthen the paper, the authors should:
1. Evaluate ESGD on more challenging datasets and tasks.
2. Include test accuracy as a performance metric.
3. Clarify the algorithm and notations to improve readability.
4. Provide a convergence rate analysis to solidify the theoretical contribution.
5. Report training time in addition to epochs for a more comprehensive comparison.
Pro Acceptance:
- Novel and theoretically sound method.
- Promising empirical results.
- Insightful connection between RMSProp and equilibration.
Con Acceptance:
- Limited experimental scope and relevance.
- Lack of clarity and convergence rate analysis.
- Focus on training error rather than test accuracy.
Final Decision: Weak Accept (conditional on addressing experimental and clarity issues).