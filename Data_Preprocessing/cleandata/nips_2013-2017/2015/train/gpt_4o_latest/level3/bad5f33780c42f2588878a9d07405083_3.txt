The paper introduces approximate max-information, a novel information-theoretic quantity designed to study generalization in adaptive data analyses. This concept is particularly relevant for scenarios where data is reused adaptively, such as hyperparameter tuning or repeated holdout set validation. The authors argue that algorithms with low approximate max-information produce outputs that are "almost" independent of the data, thereby mitigating overfitting and enabling valid subsequent analyses. Notably, the paper demonstrates that differentially private algorithms, small cardinality range algorithms, and compositions of low max-information algorithms exhibit low approximate max-information. The authors also highlight applications such as generalization guarantees for differentially private algorithms, the Thresholdout method for adaptive holdout reuse, and SparseValidate for multiple hypothesis testing.
Strengths:
1. Novelty and Versatility: Approximate max-information unifies differential privacy and description length approaches, offering a versatile framework for generalization in adaptive settings. Its composition property makes it more adaptable than prior stability measures.
2. Practical Contributions: The paper introduces Thresholdout and SparseValidate, two practical algorithms for adaptive holdout reuse, and demonstrates their utility in preventing overfitting.
3. Theoretical Rigor: The authors provide strong theoretical guarantees, including bounds on approximate max-information and its implications for generalization. The adaptive composition property is particularly compelling.
4. Experimental Validation: The synthetic experiments effectively illustrate the dangers of standard holdout reuse and the benefits of the proposed reusable holdout methods.
5. Significance: The work addresses a critical issue in modern data analysis—adaptive reuse of datasets—and provides tools that could influence both theoretical research and practical applications.
Weaknesses:
1. Lack of Comparisons: The paper does not adequately compare its contributions to earlier work, such as Freund's self-bounding algorithms or Blum and Langford's micro-choice bounds. This omission makes it difficult to contextualize the novelty of approximate max-information.
2. Clarity Issues: The distinction between description length results and Occam/MDL-type bounds is unclear and requires further elaboration. Additionally, terms like "quadratic" and "exponential" are used imprecisely in some sections (e.g., Lines 119-120).
3. Interplay Between Parameters: The relationship between parameters like τ and n in Thresholdout is underexplored, especially for cases like τ = O(1/√n). This could limit the practical applicability of the method.
4. Technical Gaps: Several technical aspects, such as probability statements, definitions, and Theorem 9, require clarification. The presentation of these results could benefit from greater precision and organization.
5. Contextualization of Adaptivity: The paper could better contextualize the quantitative aspects of adaptivity and the "free" evaluation of "good" functions, which are central to its claims.
Recommendation:
Overall, the paper provides a novel and impactful perspective on generalization in adaptive data analysis. Despite some clarity and contextualization issues, its contributions are significant and well-supported by theoretical and experimental evidence. I recommend acceptance, provided the authors address the highlighted weaknesses, particularly the lack of comparisons to prior work and the need for clearer exposition of key concepts.