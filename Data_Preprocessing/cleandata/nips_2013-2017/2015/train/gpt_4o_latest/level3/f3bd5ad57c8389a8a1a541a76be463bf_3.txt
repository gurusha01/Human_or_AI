The paper introduces Stochastic Expectation Propagation (SEP), a novel variant of Expectation Propagation (EP), designed to address the memory scalability challenges of EP in large-scale Bayesian learning. By assuming identical local factor approximations, SEP reduces memory requirements from \(O(ND^2)\) to \(O(D^2)\), making it well-suited for large datasets and models. The authors demonstrate SEP's performance across various canonical learning problems, including probit regression, Gaussian mixture models, and Bayesian neural networks, showing that it achieves accuracy comparable to EP while significantly reducing memory usage.
Strengths:
1. Proposed Method: The introduction of SEP is a meaningful contribution to the field of approximate Bayesian inference. The algorithm effectively balances the local computation benefits of EP with the global approximation benefits of Variational Inference (VI), offering a practical solution for large-scale applications.
2. Experimental Validation: The experiments are thorough and diverse, covering synthetic and real-world datasets, as well as different models. Results consistently show that SEP approximates EP closely in terms of accuracy while addressing its memory limitations.
3. Connections to Prior Work: The paper consolidates various EP variants (e.g., ADF, Averaged EP) and relates SEP to VI and Stochastic Variational Inference (SVI), providing a clear theoretical context for the proposed method.
4. Clarity of Writing: The paper is well-written and organized, with detailed explanations of the algorithms and experimental setups. The inclusion of theoretical extensions (e.g., parallel SEP, distributed SEP) and latent variable handling adds depth to the work.
Weaknesses:
1. Failure Regimes: While the paper highlights SEP's advantages, it does not adequately explore or illustrate scenarios where SEP (or its assumptions) might fail. Simple examples of such failure cases would strengthen the paper by providing a more balanced evaluation.
2. Clarity of Analogies: The analogies between SEP, SVI, and VI, as well as the limits of PEP and site approximation complexities, could be better explained. These sections may be challenging for readers unfamiliar with the nuances of EP and its variants.
3. Incremental Contribution: SEP's assumption of identical local factors is conceptually similar to Averaged EP (AEP) and ADF. While the consolidation and empirical validation are valuable, the novelty of the method may be perceived as incremental rather than groundbreaking.
4. Granularity of Approximation: The discussion on distributed SEP and the granularity of approximations is promising but underexplored. More empirical results on this aspect would help validate its practical utility.
Arguments for Acceptance:
- SEP addresses a critical limitation of EP, making it applicable to large-scale problems.
- The paper provides a solid theoretical foundation and empirical evidence for SEP's effectiveness.
- The work is well-situated within the broader context of EP and VI, offering insights into their relationships.
Arguments Against Acceptance:
- The novelty of SEP may be seen as incremental, given its similarity to existing methods like AEP and ADF.
- The lack of detailed analysis of failure regimes and certain theoretical aspects limits the paper's completeness.
Recommendation:
Overall, this paper makes a meaningful contribution to the field of approximate Bayesian inference by proposing a practical and scalable extension to EP. While the novelty is somewhat incremental, the thorough experimental validation and theoretical connections make it a valuable addition to the literature. I recommend acceptance with minor revisions, particularly addressing the clarity of analogies and failure case analysis.