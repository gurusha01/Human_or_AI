This paper presents a significant technical contribution by unifying a large body of work on isotonic regression (IR) through the use of fast solvers for linear systems. The authors propose improved algorithms for computing IR under weighted `p-norms and `âˆž-norms, with rigorous performance guarantees and faster runtimes compared to prior methods. The paper also introduces a novel optimization-based framework for these algorithms, which could potentially extend to other convex programming problems. While the theoretical results are compelling, the practical impact on machine learning (ML) applications remains limited, as the algorithms do not demonstrate substantial improvements for structured DAGs in common ML tasks like probabilistic calibration or learning single-index models (SIMs).
Strengths:
1. Technical Depth and Novelty: The paper provides a unified framework for IR algorithms, extending interior point methods to `p-objectives and introducing fast solvers for a new class of matrices. These contributions are technically solid and advance the state of the art in algorithmic efficiency.
2. Improved Runtimes: The proposed algorithms achieve faster runtimes for general DAGs and specific graph families, such as 2D grids and d-dimensional point sets, compared to previous methods.
3. Clarity of Theoretical Contributions: The paper clearly articulates its theoretical contributions, including detailed proofs and performance guarantees.
4. Potential for Broader Applications: The framework could be extended to other problems, such as learning generalized linear models and multi-index models, which is a promising direction for future work.
Weaknesses:
1. Limited ML Relevance: The paper's focus on faster IR algorithms, while valuable for theoretical computer science, may not resonate with the broader ML community. The lack of direct improvements for structured DAGs in ML applications limits its practical significance.
2. Insufficient Highlighting of ML Applications: The authors briefly mention potential applications in learning Lipschitz monotone functions and class probability estimation but fail to provide concrete examples or experimental results demonstrating significant advances in these areas.
3. Presentation Issues: The paper could improve its readability by formatting the four points about Program (5) on page 6 into bullet points. Additionally, minor typos on pages 1, 4, 6, and 7 should be corrected.
4. Incomplete Citation of Related Work: The paper does not cite work on the PAV algorithm's optimality for general loss functions and should clarify that [14] works with the standard L2 norm, not a general Lp norm.
Recommendation:
While the paper makes a strong technical contribution, its relevance to an ML-focused conference like NeurIPS is questionable. The work may be better suited for an algorithms or theoretical computer science venue. To strengthen its case for acceptance, the authors should explicitly highlight ML applications where their algorithms provide significant advances and include experimental results demonstrating these benefits.
Arguments for Acceptance:
- Strong theoretical contributions and improved runtimes for IR.
- Potential for broader applications in optimization and learning.
Arguments Against Acceptance:
- Limited direct relevance to ML tasks.
- Lack of experimental validation for ML-specific applications.
In summary, this paper is a high-quality theoretical contribution but may not align well with the focus of NeurIPS.