The paper addresses the important problem of generating natural language descriptions for sequences of images, a task that extends single-image captioning to a more complex multimodal domain. The authors propose a novel architecture, the Coherent Recurrent Convolutional Network (CRCN), which integrates convolutional neural networks (CNNs) for image representation, bidirectional recurrent neural networks (BRNNs) for sentence modeling, and an entity-based coherence model to ensure fluent transitions between sentences. The approach is evaluated on a dataset of blog posts, demonstrating superior performance over state-of-the-art methods using metrics like BLEU, CIDEr, and user studies via Amazon Mechanical Turk (AMT).
The paper is well-written and easy to follow, with a clear explanation of the CRCN model and its components. The integration of coherence modeling into a multimodal architecture is a notable contribution, as it addresses the challenge of maintaining fluency and logical progression in sentence sequences. The use of blog posts as a training dataset is also innovative, leveraging real-world, user-generated content for a challenging task. However, the dataset's crawling and preprocessing methods lack transparency, and the dataset itself is not disclosed, raising concerns about reproducibility and anonymity. This is a significant limitation, as it prevents other researchers from verifying or building upon the work.
While the CRCN model effectively captures relationships between image streams and natural sentences, the method for generating sentence sequences is somewhat primitive. It relies on direct associations and simple concatenation, which may limit its ability to handle more complex or nuanced scenarios. Additionally, Equation (2) appears to be incorrect, suggesting a possible flaw in the network's connectivity. This should be clarified or corrected in future revisions.
The paper does not adequately cite related works on video content description, which share similarities with the proposed task. This omission weakens the discussion of originality and positioning within the broader research landscape. While the authors argue that their work is distinct due to the inclusion of a coherence model, a more thorough comparison with video-sentence methods would strengthen their claims.
In summary, the paper is technically sound, with a novel combination of techniques and promising results. However, the ad-hoc nature of sentence generation, the lack of dataset transparency, and the omission of related work are notable weaknesses. The paper is suitable for acceptance as a poster, as it provides a meaningful contribution to the field but requires further refinement to address its limitations.
Pros:
1. Novel integration of coherence modeling into a multimodal architecture.
2. Strong experimental results with quantitative and user study validation.
3. Clear and well-organized presentation of the methodology.
Cons:
1. Lack of transparency in dataset creation and absence of dataset disclosure.
2. Primitive sentence generation method and possible error in Equation (2).
3. Insufficient citation of related works on video content description.
Recommendation: Accept as a poster.