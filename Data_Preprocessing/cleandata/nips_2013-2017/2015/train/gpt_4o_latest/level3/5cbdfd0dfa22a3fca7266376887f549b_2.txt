The paper introduces a novel tensor decomposition algorithm tailored for sparse noise settings, leveraging tensor contraction to reduce the problem to matrix decompositions. This approach is computationally efficient, avoiding expensive tensor unfolding, and is theoretically grounded with rigorous guarantees for exact recovery under specific conditions. The authors demonstrate the algorithm's scalability and modularity, extending it to higher-order tensors and other settings, such as block sparsity and tensor completion. The theoretical contributions, including incoherence conditions and recovery guarantees, are significant and advance the state of the art in robust tensor decomposition.
Strengths:
1. Novelty: The paper presents a novel algorithm that extends Leurgans' method to sparse noise settings, combining it with convex optimization techniques. The theoretical analysis, including recovery guarantees and incoherence conditions, is rigorous and well-justified.
2. Scalability: The method is computationally efficient, with complexity scaling as \(O(n^3)\), significantly outperforming tensor unfolding-based approaches. The extension to higher-order tensors is seamless and well-articulated.
3. Practicality: The algorithm is modular, easily implementable, and adaptable to various settings, such as block sparsity and tensor completion. This makes it a promising tool for real-world applications.
4. Clarity: The paper is well-written, with clear explanations of the algorithm, theoretical results, and extensions. The use of numerical experiments, though limited, supports the claims of the paper.
Weaknesses:
1. Experimental Evaluation: The experimental validation is limited in scope. The authors only test on synthetic data with a narrow range of parameters. Additional experiments on real-world datasets and under diverse noise settings would strengthen the paper.
2. Evaluation Metric: The use of "accurate probability" as an evaluation measure is unconventional. The authors should clarify why this metric was chosen over more standard measures like mean squared error (MSE).
3. Optimization Details: The paper lacks sufficient details on the optimization algorithm for equation (6). A deeper discussion of implementation challenges, parameter selection, and convergence properties would be beneficial.
4. Related Work: While the paper references prior work, it lacks a thorough discussion of how the proposed method compares to existing tensor decomposition techniques, particularly in terms of computational efficiency and robustness.
5. Minor Errors: There is a minor mistake in the model description in Section 2.2, which should be corrected for clarity.
Arguments for Acceptance:
- The paper addresses a challenging and important problem in tensor decomposition, providing a novel and theoretically sound solution.
- The proposed algorithm is computationally efficient, scalable, and adaptable to various settings, making it highly practical.
- The theoretical contributions, including recovery guarantees and incoherence conditions, are significant and advance the field.
Arguments Against Acceptance:
- The experimental evaluation is limited, and the choice of evaluation metric is unconventional and insufficiently justified.
- The lack of detailed discussions on the optimization algorithm and related work weakens the paper's overall impact.
Recommendation: Accept with minor revisions. The paper makes a strong theoretical contribution and proposes a practical algorithm, but the authors should address the experimental limitations, clarify the evaluation metric, and provide more details on the optimization process.