The paper presents a novel optimization algorithm, HONOR, designed to address the computational challenges of non-convex sparse learning formulations, which have demonstrated theoretical and practical superiority over convex formulations. HONOR leverages second-order information via a hybrid optimization scheme combining Quasi-Newton (QN) and Gradient Descent (GD) steps to achieve faster convergence without requiring explicit Hessian inversion. The authors provide rigorous convergence analysis, demonstrating that HONOR guarantees convergence to Clarke critical points for non-convex problems, a significant theoretical contribution given the inherent difficulties of non-convex optimization. Empirical evaluations on large-scale datasets further validate HONOR's efficiency, showing substantial improvements in convergence speed compared to state-of-the-art algorithms like GIST.
Strengths:
1. Technical Soundness: The paper is technically robust, with detailed convergence proofs and a well-structured algorithm. The use of Clarke subdifferentials and hybrid optimization to address non-convexity is innovative and well-justified.
2. Practical Relevance: HONOR addresses a critical challenge in sparse learningâ€”scalability to large datasets with millions of samples and features. The empirical results convincingly demonstrate its practical advantages.
3. Originality: The hybrid scheme combining QN and GD steps is a novel approach to balancing computational efficiency and theoretical guarantees. The avoidance of explicit Hessian inversion is particularly noteworthy.
4. Significance: The ability of HONOR to escape high-error plateaus in high-dimensional non-convex problems is promising and could inspire further research in optimization and sparse learning.
Weaknesses:
1. Clarity: While the paper is mathematically rigorous, certain aspects could be better explained for broader accessibility. For instance, the intuition behind the hybrid scheme and the role of parameter \(\epsilon\) in balancing QN and GD steps could be elaborated.
2. Local Convergence Rate: The paper lacks a discussion of HONOR's local convergence rate guarantees, which would provide a more complete picture of its performance.
3. Generality: Although HONOR is demonstrated on specific non-convex penalties (LSP, MCP, SCAD), its applicability to other sparsity-inducing penalties like group Lasso and fused Lasso is not explored.
4. Empirical Comparisons: The experiments focus on comparisons with GIST but do not include other relevant methods like SparseNet or DC-PN, limiting the scope of the evaluation.
Arguments for Acceptance:
- The paper addresses a significant and challenging problem in sparse learning, presenting a novel and well-validated solution.
- The theoretical contributions, particularly the convergence analysis for non-convex problems, are substantial.
- The empirical results are compelling, demonstrating clear advantages over state-of-the-art methods.
Arguments Against Acceptance:
- The lack of clarity in explaining key design choices and parameters may hinder reproducibility and broader adoption.
- The omission of local convergence rate analysis and extension to other penalties leaves some questions unanswered.
- A more comprehensive empirical evaluation would strengthen the paper's claims.
Recommendation:
I recommend acceptance, contingent on minor revisions to improve clarity and address the concerns raised. The paper makes a strong contribution to the field of optimization and sparse learning, and its practical and theoretical insights are likely to benefit both researchers and practitioners.