The paper introduces Stochastic Expectation Propagation (SEP), a novel generalization of Expectation Propagation (EP) aimed at addressing EP's memory overhead issues in large-scale Bayesian learning. By maintaining a global posterior approximation instead of per-data-point approximations, SEP significantly reduces memory requirements while preserving the local computation advantages of EP. The authors extend SEP to parallel, distributed, and latent variable models, demonstrating its scalability and versatility. Experimental results across canonical domains, including Bayesian probit regression, mixture of Gaussians, and probabilistic backpropagation, validate SEP's performance as competitive with EP while achieving substantial memory savings.
Strengths:
1. Quality: The method is robust and well-justified, with theoretical connections to existing techniques like Assumed Density Filtering (ADF), Variational Inference (VI), and Averaged EP (AEP). The authors provide a thorough experimental evaluation, showing that SEP achieves comparable accuracy to EP while reducing memory consumption by a factor of \(N\). The inclusion of both synthetic and real-world datasets strengthens the empirical validation.
   
2. Clarity: The paper is exceptionally well-written and organized. The authors clearly articulate the motivation, methodology, and experimental results. The algorithmic steps for SEP and its extensions are presented in a structured manner, making the work accessible to readers with varying expertise levels.
3. Originality: The approach is innovative, offering a principled stochastic generalization of EP that addresses a critical limitationâ€”scalability in large datasets. The extensions to parallelism, mini-batches, and latent variable models further enhance the novelty of the contribution.
4. Significance: The contributions are impactful, providing a scalable alternative to EP for inference in complex, high-parameter models. The memory reduction achieved by SEP makes it a practical choice for large-scale Bayesian learning, which is increasingly relevant in modern machine learning applications. The connections drawn between SEP and other inference methods (e.g., SVI, ADF) provide a unifying perspective that could inspire further research.
Weaknesses:
1. Theoretical Analysis: While the paper establishes connections between SEP and existing methods, the theoretical understanding of SEP's convergence properties is limited. This is acknowledged by the authors as an area for future work.
   
2. Granularity of Approximation: The experiments suggest that SEP's coarse-grained approximation may underperform EP in heterogeneous datasets, particularly when likelihood contributions vary significantly. While Distributed SEP (DSEP) is proposed as a remedy, its practical utility requires further exploration.
3. Computational Overhead: Although SEP reduces memory requirements, the computational cost of moment matching and damping in large models or datasets is not fully analyzed. A more detailed discussion of computational trade-offs would strengthen the paper.
Arguments for Acceptance:
- The paper addresses a critical challenge in EP, offering a scalable and memory-efficient alternative.
- The method is well-validated across diverse datasets and models, demonstrating its practical utility.
- The work is highly original and advances the state of the art in approximate Bayesian inference.
Arguments Against Acceptance:
- Theoretical guarantees for SEP's convergence are incomplete, leaving open questions about its behavior in certain scenarios.
- The performance of SEP in highly heterogeneous datasets may be suboptimal without additional tuning or extensions like DSEP.
Recommendation:
Overall, the paper makes a significant contribution to the field of approximate Bayesian inference. Its innovative approach, strong empirical validation, and practical relevance outweigh the minor limitations. I recommend acceptance with minor revisions to address the theoretical and computational concerns.