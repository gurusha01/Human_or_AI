The paper introduces Scheduled Sampling (SS), a novel training heuristic for Recurrent Neural Networks (RNNs) aimed at addressing the discrepancy between training and inference in sequence prediction tasks. The approach gradually transitions from using ground-truth labels during training to using the model's own predictions, thereby improving robustness to errors that accumulate during inference. The method is evaluated on tasks such as image captioning, constituency parsing, and speech recognition, showing consistent improvements over baseline RNN models. Notably, the proposed method contributed to the authors' winning entry in the 2015 MSCOCO image captioning challenge.
Strengths:
1. Novelty and Practical Relevance: While noise injection in training is not new, the focus on mitigating the training-inference mismatch is an important and underexplored area. The curriculum learning-inspired approach is intuitive and demonstrates promising results across diverse tasks.
2. Empirical Performance: SS shows significant improvements over baselines in image captioning, parsing, and speech recognition. The experimental results are compelling, with detailed comparisons to baselines and ablation studies (e.g., uniform sampling and "always sampling").
3. Wide Applicability: The method is tested across multiple domains, showcasing its generalizability.
4. Clarity: The paper is well-written and provides sufficient detail for reproduction, including the decay schedules used for SS.
Weaknesses:
1. Lack of Theoretical Analysis: The paper does not provide a rigorous probabilistic or theoretical explanation for why SS works, particularly its potential regularization effect. This limits the scientific understanding of the heuristic.
2. Baseline Comparisons: While the paper includes several baselines, it lacks a comparison to models with varying beam widths, which could help assess SS's effectiveness in mitigating search errors during inference.
3. Hyperparameter Sensitivity: The decay schedule and hyperparameter \( k \) are not thoroughly analyzed. The paper does not discuss how sensitive the results are to these choices, which could impact reproducibility and practical deployment.
4. Limited Exploration of Supervision Levels: Experiments varying the amount of supervision (e.g., partial labels) could provide deeper insights into the benefits of SS.
5. Post-Author Response Concerns: Despite the authors' clarifications, concerns remain about the difficulty of tuning the sampling schedule and the lack of deeper analysis.
Arguments for Acceptance:
- The heuristic is novel, practical, and demonstrates clear empirical benefits across multiple tasks.
- The paper addresses a critical problem in sequence prediction, making it relevant to the NIPS audience.
- The results are significant and could inspire further research into bridging the training-inference gap.
Arguments Against Acceptance:
- The lack of theoretical grounding and sensitivity analysis weakens the scientific contribution.
- Some baseline comparisons (e.g., varying beam widths) are missing, leaving questions about the robustness of the claims.
- The heuristic's reliance on hyperparameter tuning without clear guidelines may limit its practical utility.
Recommendation:
While the paper has notable strengths in its empirical contributions and practical relevance, it falls short in theoretical rigor and experimental depth. I recommend acceptance with minor revisions, emphasizing the need for additional analysis of hyperparameters, theoretical insights, and broader baseline comparisons.