This paper presents several modifications to the stochastic variance-reduced gradient (SVRG) method aimed at improving its computational efficiency and convergence properties. The authors propose strategies such as growing-batch approaches, mixed SG/SVRG updates, support vector exploitation, and alternative mini-batch selection techniques. They also analyze the regularized SVRG update and its implications for convergence rates. While the paper is well-written and provides a comprehensive theoretical analysis alongside experimental results, several concerns limit its impact.
Strengths:
1. Clarity and Organization: The paper is clearly written, with a logical flow of ideas and detailed explanations of the proposed methods. The inclusion of a comparative bounds table in Section 8 is particularly helpful for understanding the theoretical implications of the work.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis for their proposed modifications, including proofs of convergence rates under various conditions. The results, such as the robustness of SVRG to inexact gradient computations, are insightful.
3. Practical Relevance: The paper addresses computational bottlenecks in SVRG, such as the high cost of gradient evaluations, which is a critical issue for large-scale machine learning problems.
4. Experimental Validation: The experiments are thorough, covering multiple datasets and comparing different variants of SVRG. The results demonstrate the potential benefits of the proposed strategies, particularly the growing-batch approach.
Weaknesses:
1. Lack of Baseline Comparisons: The paper primarily focuses on self-comparisons (e.g., different variants of SVRG) and does not adequately benchmark its methods against other state-of-the-art stochastic or mini-batch optimization techniques. This omission makes it difficult to assess the practical significance of the proposed methods relative to existing approaches.
2. Novelty and Contribution: While the paper introduces several modifications to SVRG, the novelty of these contributions is somewhat incremental. Many of the ideas, such as mini-batching and support vector exploitation, have been explored in prior work. The authors should clarify how their methods advance the state of the art beyond existing techniques.
3. Significance of Results: The improvements in convergence and computational efficiency, while theoretically justified, appear modest in practice. For a competitive field like stochastic optimization, the results may not be substantial enough to warrant publication in a top-tier venue like NIPS.
4. Crowded Research Area: The field of stochastic optimization is highly competitive, with numerous recent advancements. The paper does not sufficiently position its contributions within this crowded landscape, nor does it address why SVRG, as opposed to other methods, is the most suitable framework for these modifications.
Pro vs. Con for Acceptance:
- Pro: The paper is technically sound, well-written, and provides useful insights into improving SVRG. The theoretical analysis is rigorous, and the experimental results are comprehensive.
- Con: The lack of baseline comparisons, incremental novelty, and questionable significance of the improvements limit the paper's impact. The work does not convincingly demonstrate a substantial advancement over existing methods.
Recommendation: While the paper is a solid piece of work, its contributions are incremental, and the lack of baseline comparisons weakens its case for acceptance. I recommend rejection but encourage the authors to address these issues in a future submission. Specifically, they should benchmark their methods against state-of-the-art techniques, better articulate the novelty of their contributions, and demonstrate more significant practical improvements.