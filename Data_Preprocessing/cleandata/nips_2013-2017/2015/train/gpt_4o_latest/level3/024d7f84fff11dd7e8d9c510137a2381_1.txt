The paper presents a significant contribution to risk-sensitive reinforcement learning (RL) by extending policy gradient methods to the entire class of coherent risk measures, encompassing both static and dynamic settings. This unified approach generalizes prior work, which focused on specific risk measures like variance or Conditional Value at Risk (CVaR). The authors derive analytical expressions for policy gradients under static coherent risk measures and propose a sampling-based algorithm for cases where analytical solutions are unavailable, ensuring consistency in gradient estimates. For dynamic risk measures, the paper introduces a policy gradient theorem for Markov coherent risks and outlines an actor-critic style algorithm. These contributions are novel and valuable, advancing the state of the art in risk-sensitive decision-making.
Strengths:
1. Novelty and Generality: The unified derivation of policy gradients for coherent risk measures is a notable advancement, providing a framework that subsumes and extends prior methods.
2. Theoretical Rigor: The paper is grounded in solid theoretical foundations, leveraging the dual representation of coherent risks and the envelope theorem to derive gradient formulas.
3. Sampling-based Algorithm: The proposed sampling-based approach broadens the applicability of the method to scenarios where analytical solutions are infeasible, with consistency guarantees.
4. Dynamic Risk Measures: The extension to Markov coherent risks and the introduction of a policy gradient theorem for dynamic settings are important contributions.
5. Clarity in Mathematical Derivations: The mathematical exposition is detailed and well-structured, making the theoretical contributions accessible to readers familiar with the field.
Weaknesses:
1. Limited Empirical Validation: The lack of experimental comparisons between the sample-based and analytical policy gradients (e.g., for CVaR) weakens the practical impact of the proposed methods. Empirical results demonstrating the viability and efficiency of the sampling-based approach are necessary.
2. Clarity and Focus: The paper attempts to address both static and dynamic risk measures, leading to a dense presentation that sacrifices clarity. A narrower focus or splitting the paper into two parts could improve readability and allow for more comprehensive evaluations.
3. Algorithmic Details for Dynamic Risk: The actor-critic algorithm for dynamic risk measures is presented at a high level, with insufficient details on implementation and convergence analysis. This limits reproducibility and practical adoption.
4. Practical Relevance of Sampling-based Approach: While theoretically sound, the practical utility of the sampling-based approximation scheme is unclear without empirical evidence.
5. Limited Analytical Solutions: The reliance on Lagrangian saddle-points restricts analytical solutions to specific risk measures, reducing the general applicability of the approach.
Recommendation:
The paper makes a strong theoretical contribution to risk-sensitive RL, but its practical impact is limited by the lack of empirical validation and clarity in algorithmic details. I recommend acceptance conditional on the authors addressing the following:
1. Include experimental comparisons between sample-based and analytical gradients, particularly for CVaR.
2. Provide more detailed algorithmic descriptions and convergence analysis for the actor-critic approach.
3. Consider narrowing the scope to static risk measures or splitting the paper for better clarity and focus.
Pro and Con Arguments for Acceptance:
- Pro: Novel and general framework for coherent risk measures; rigorous theoretical contributions; potential to significantly impact risk-sensitive RL.
- Con: Limited empirical validation; insufficient algorithmic details for dynamic risk; clarity issues due to broad scope.
Overall, the paper is a valuable theoretical contribution but requires additional empirical and practical insights to maximize its impact.