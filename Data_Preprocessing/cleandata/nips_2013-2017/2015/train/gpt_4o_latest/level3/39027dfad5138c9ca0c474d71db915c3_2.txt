This paper addresses a critical issue in batch learning from logged bandit feedback (BLBF) by identifying and mitigating the problem of propensity overfitting in counterfactual risk estimation. The authors propose replacing the conventional unbiased risk estimator with a self-normalized risk estimator, which avoids the anomalies of the former. This leads to the development of a new learning algorithm, Norm-POEM, which demonstrates superior generalization performance in multi-label classification tasks. The paper is well-motivated, technically rigorous, and provides empirical evidence supporting the proposed method.
Strengths:
1. Novelty and Significance: The paper identifies a previously unaddressed issue—propensity overfitting—in counterfactual risk estimation and provides a principled solution. This is a significant contribution to the field of BLBF and counterfactual learning.
2. Theoretical Soundness: The self-normalized risk estimator is rigorously derived and shown to resolve the anomalies of the conventional estimator. The theoretical properties, including strong consistency and boundedness, are well-supported.
3. Empirical Validation: The experimental results convincingly demonstrate that Norm-POEM outperforms POEM and approaches the performance of full-information CRFs. The results are consistent across multiple datasets and experimental setups.
4. Practical Impact: The proposed method has direct applications in interactive systems like ad placement, web search, and recommendation, making it highly relevant to real-world scenarios.
5. Clarity of Writing: The paper is well-organized, with clear explanations of the problem, methodology, and experimental results.
Weaknesses:
1. Incorrect Commentary on Equation (2): The commentary and notations for the main equation (2) are misleading, as it incorrectly identifies the loss as the expected quantity. This flaw propagates to Example 1, which is based on a faulty presumption and does not hold.
2. Estimator Sensitivity: The estimator in equation (2) is weak to its denominator, \(p_i\), which can be zero for unobserved samples, leading to instability. While the modified estimator in equation (7) addresses this, the weakness of the original estimator could have been more thoroughly analyzed.
3. Computational Complexity: Although Norm-POEM converges faster in practice, the paper does not provide a detailed analysis of the computational trade-offs introduced by the self-normalized estimator.
4. Limited Discussion of Related Work: While the paper references prior work, it could better contextualize its contributions within the broader literature on counterfactual learning and variance reduction techniques.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a critical problem in BLBF, provides a novel and theoretically sound solution, and demonstrates significant empirical improvements. The proposed method has practical relevance and advances the state of the art.
- Con: The misleading commentary on equation (2) and the flawed Example 1 detract from the paper's clarity and rigor. Additionally, the computational trade-offs of the proposed method are not thoroughly analyzed.
Recommendation: Accept with minor revisions. The paper is a strong contribution to the field, but the authors should address the issues with equation (2) and provide more clarity on computational trade-offs.