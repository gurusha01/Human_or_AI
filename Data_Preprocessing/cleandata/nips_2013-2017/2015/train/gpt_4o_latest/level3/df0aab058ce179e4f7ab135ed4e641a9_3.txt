This paper introduces Rectified Factor Networks (RFNs), a novel generative unsupervised model designed to produce sparse, non-negative, and high-dimensional representations of input data. By extending factor analysis with posterior regularization, the authors enforce desirable properties in the learned representations. The proposed RFNs outperform existing methods such as autoencoders, restricted Boltzmann machines (RBMs), and independent component analysis (ICA) in terms of sparsity, reconstruction error, and covariance modeling. Additionally, RFNs demonstrate strong performance as a pretraining method for deep networks and uncover biologically relevant insights in gene expression datasets that other methods fail to detect.
Strengths:  
The paper makes a significant contribution to unsupervised learning by addressing key limitations of existing methods. The combination of factor analysis with posterior regularization is novel and well-motivated, enabling the model to enforce sparsity and non-negativity effectively. The theoretical analysis is rigorous, with proofs of convergence and correctness provided, which strengthens the validity of the proposed approach. Empirical results are comprehensive, spanning synthetic datasets, standard vision benchmarks (e.g., MNIST, CIFAR-10), and real-world applications in drug discovery. The results convincingly demonstrate RFNs' superiority in sparsity, reconstruction error, and utility for pretraining deep networks. The availability of the RFN implementation further enhances the paper's reproducibility and practical impact.
Weaknesses:  
While the paper is technically sound, some areas could benefit from additional clarification. The rationale for employing five different gradient descent methods in the E-step is not adequately justified. It would be helpful to explain why this diversity of methods is necessary and whether certain methods consistently outperform others. Additionally, the paper lacks an ablative analysis to isolate the contributions of specific constraints, such as non-negativity and normalization, to the overall performance. This would provide deeper insights into the importance of each design choice. Finally, while the paper is generally well-written, the dense mathematical exposition may pose challenges for readers unfamiliar with posterior regularization or factor analysis.
Pro and Con Arguments for Acceptance:  
Pros:  
- Novel and theoretically grounded approach to sparse generative modeling.  
- Strong empirical results across diverse datasets and tasks.  
- Demonstrates practical utility in real-world applications, particularly in drug discovery.  
- Well-suited for the NIPS audience, given its focus on unsupervised learning and representation learning.  
Cons:  
- Insufficient justification for the use of multiple gradient descent methods.  
- Lack of ablative analysis to disentangle the contributions of individual constraints.  
- Dense presentation may limit accessibility for non-experts.
Conclusion:  
This paper makes a compelling case for the adoption of RFNs as a robust method for unsupervised representation learning. Despite minor shortcomings in justification and analysis, the paper's contributions are significant and well-aligned with the interests of the NIPS community. I recommend acceptance, with minor revisions to address the noted weaknesses.