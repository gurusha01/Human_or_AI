The paper presents a significant contribution to risk-sensitive reinforcement learning (RL) by extending policy gradient methods to the general class of coherent risk measures. This work builds on prior studies that focused on specific risk measures, such as variance or Conditional Value at Risk (CVaR), and offers a unified framework applicable to both static and dynamic risk measures. The authors propose novel RL algorithms, including a sampling-based approach for static coherent risk measures and an actor-critic style algorithm for dynamic Markov coherent risk measures. These methods are supported by rigorous theoretical derivations, including new gradient formulas and convergence guarantees.
Strengths:
1. Technical Soundness and Clarity: The paper is well-written, logically structured, and technically robust. The derivation of gradient formulas for both static and dynamic coherent risk measures is thorough and builds on foundational concepts such as the envelope theorem and likelihood-ratio methods. The clarity of the exposition is above average, making the complex mathematical formulations accessible to readers familiar with RL and optimization.
2. Originality and Scope: By generalizing risk-sensitive RL to the entire class of coherent risk measures, the paper offers a novel and non-trivial extension to existing methods. This unified framework is a significant step forward, as it allows practitioners to tailor risk measures to specific problem domains, addressing limitations of prior work that focused on individual risk measures.
3. Significance and Practical Relevance: The proposed methods have broad applicability in domains where managing cost variability is critical, such as finance and operations research. The numerical example in the finance domain effectively demonstrates the flexibility of the approach and its ability to capture diverse risk preferences.
4. Experimental Validation: Although the experiments are limited to a toy example, they validate the algorithm's effectiveness and highlight the importance of selecting appropriate risk measures. The example underscores the practical implications of the proposed framework, such as avoiding counterintuitive behavior in risk-averse policies.
Weaknesses:
1. Limited Experimental Scope: The experiments focus on a static risk problem with a toy example in finance. While the results are illustrative, the lack of more complex, real-world scenarios limits the empirical validation of the proposed methods. Expanding the experiments to dynamic risk problems or large-scale MDPs would strengthen the paper.
2. Computational Complexity: The reliance on convex programming and sampling-based methods may pose scalability challenges for high-dimensional or continuous state-action spaces. While the authors acknowledge this limitation, a more detailed discussion of computational trade-offs and potential optimizations (e.g., importance sampling) would be beneficial.
3. Practical Implementation Details: The actor-critic algorithm for dynamic risk measures is only briefly outlined, with key details deferred to supplementary material. A more comprehensive presentation of the algorithm and its convergence properties would enhance the paper's practical utility.
Recommendation:
I recommend acceptance of this paper. Its contributions to risk-sensitive RL are substantial, and the theoretical advancements are both novel and impactful. While the experimental evaluation could be more extensive, the paper's strengths in originality, clarity, and technical rigor outweigh its limitations. The proposed framework has the potential to inspire further research and practical applications in risk-sensitive decision-making.
Arguments for Acceptance:
- Significant theoretical contribution to risk-sensitive RL with coherent risk measures.
- Unified framework generalizes and extends prior work, offering flexibility across risk types.
- Clear and technically sound presentation of methods and results.
Arguments Against Acceptance:
- Limited experimental validation and scalability concerns.
- Insufficient detail on the actor-critic algorithm for dynamic risk measures.
Overall, the paper makes a meaningful contribution to the field and addresses an important problem in RL.