The paper introduces an innovative approach to approximate Bayesian neural network learning using online student-teacher distillation, addressing key challenges in Bayesian inference for neural networks, particularly memory and computational efficiency. The primary contribution lies in the simultaneous online training of the teacher (via SGLD) and the student network, eliminating the need to store sampled models. This approach significantly reduces storage and test-time computational costs, making Bayesian inference more practical for large-scale problems. The method builds on prior work in model distillation but adapts it for Bayesian settings, where the teacher provides probabilistic predictions rather than deterministic outputs.
Strengths:
1. Novelty and Practicality: The proposed method cleverly combines online Monte Carlo sampling with model distillation, offering a scalable and efficient solution to Bayesian neural network learning. The reduction in storage and test-time costs is a significant practical advantage.
2. Technical Soundness: The paper provides a clear mathematical formulation of the method and demonstrates its effectiveness through experiments on both synthetic and real-world datasets.
3. Significance: The approach addresses a critical limitation of Bayesian methods—computational inefficiency—making it relevant for applications requiring uncertainty quantification, such as active learning and reinforcement learning.
4. Comparative Analysis: The authors compare their method against state-of-the-art approaches like expectation propagation (EP) and variational Bayes (VB), showing superior performance in terms of log-likelihood and calibration of predictions.
Weaknesses:
1. Computational Overhead: While the method reduces test-time costs, the frequent training of the student during the MCMC process introduces additional computational overhead. This could be prohibitive for large datasets, and the paper does not adequately address this trade-off.
2. Dataset Selection: The choice of the auxiliary training dataset \( D' \) for the student network is not thoroughly discussed. The lack of clarity on how \( D' \) impacts generalizability and performance raises concerns about the method's robustness.
3. Experimental Limitations: The experimental evaluation, while promising, is incomplete. The absence of direct comparisons with some baseline methods (due to unavailable code) weakens the empirical validation. Additionally, larger-scale experiments would strengthen the claims of scalability.
4. Clarity: While the paper is generally well-written, some sections, particularly the description of the algorithm and its hyperparameter choices, could benefit from greater clarity and detail.
Arguments for Acceptance:
- The paper addresses an important and challenging problem in Bayesian neural networks with a novel and practical solution.
- The method demonstrates competitive performance and significant computational advantages over existing approaches.
- The work has the potential to inspire further research in scalable Bayesian methods and uncertainty-aware deep learning.
Arguments Against Acceptance:
- The computational overhead during training and the limited discussion on dataset selection \( D' \) leave open questions about the method's scalability and generalizability.
- The incomplete experimental comparisons and lack of large-scale benchmarks reduce the strength of the empirical evidence.
Recommendation:
Overall, the paper presents a valuable contribution to the field of Bayesian neural networks, particularly in terms of practical applicability. However, to strengthen its impact, the authors should address the computational trade-offs, provide more detailed guidance on \( D' \), and expand the experimental evaluation. I recommend acceptance with minor revisions.