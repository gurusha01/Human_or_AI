The paper introduces Rectified Factor Networks (RFNs), a novel approach to constructing sparse, non-linear, and high-dimensional input representations within a generative framework. The authors emphasize the utility of RFNs for identifying rare and small events, minimizing reconstruction error, and explaining data covariance structures. The proposed learning algorithm leverages posterior regularization to enforce non-negativity and normalization, and its convergence and correctness are theoretically proven. RFNs are compared against unsupervised methods such as autoencoders, RBMs, PCA, and ICA, and are also evaluated as a pretraining technique for deep networks. Promising results are demonstrated on synthetic datasets, vision benchmarks (e.g., MNIST, CIFAR-10), and real-world pharmaceutical datasets.
Strengths:
1. Novelty and Theoretical Contributions: The paper presents a novel combination of sparse coding and posterior regularization, with theoretical guarantees for convergence and correctness. This is a notable contribution to unsupervised learning.
2. Sparse Representations: RFNs achieve very sparse codes, outperforming other methods in sparsity, reconstruction error, and covariance approximation on synthetic datasets.
3. Real-World Applicability: The application of RFNs to pharmaceutical datasets is compelling, showcasing their ability to detect rare and biologically significant patterns missed by other methods.
4. Efficiency: The proposed generalized alternating minimization (GAM) algorithm is computationally efficient, with significant speedups over traditional quadratic solvers.
5. Open-Source Availability: The availability of the RFN package for GPU/CPU enhances reproducibility and accessibility.
Weaknesses:
1. Unaddressed Comparisons: The authors emphasize non-negativity but fail to explain why PReLUs, which are not non-negative, outperform ReLUs in some cases. This weakens the argument for the necessity of non-negativity.
2. Contradictory Results in Table 1: PCA is reported to have lower reconstruction error, contradicting the claim that RFNs perform best. This inconsistency undermines the experimental rigor.
3. Limited Benchmarks: Table 2 omits stronger baselines like Schmidhuber's convolutional autoencoders, which outperform RFNs on MNIST and CIFAR-10. This omission is misleading.
4. Noise Robustness: Figure 2 claims robustness to noise, but the filters appear less robust compared to denoising autoencoders.
5. Lack of Comprehensive Comparisons: The experiments do not include explicit sparse coding methods or real-world datasets for sparsity and reconstruction error. Pretraining comparisons focus on non-deep-net methods, neglecting competitive deep-net pretraining approaches.
6. Classification Tasks: RFNs are not directly compared to other unsupervised methods for classification tasks, leaving a gap in evaluation.
Pro vs. Con Acceptance:
- Pro: The paper introduces a novel method with theoretical guarantees, demonstrates strong results on synthetic and pharmaceutical datasets, and provides an efficient algorithm.
- Con: The experimental evaluation is incomplete, with contradictory results, omitted baselines, and insufficient comparisons to state-of-the-art methods.
Recommendation: While the paper has significant theoretical and practical contributions, the experimental shortcomings and lack of clarity in some claims weaken its overall impact. I recommend a major revision to address the inconsistencies, expand the benchmarks, and provide clearer justifications for the design choices.