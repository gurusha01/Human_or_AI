The paper introduces HONOR, a hybrid optimization algorithm designed to address non-convex, non-smooth regularized optimization problems with a focus on sparsity. The authors propose a novel combination of quasi-Newton (QN) and gradient descent (GD) steps, inspired by the OWL-QN algorithm, to efficiently solve such problems. By constraining iterates to remain in the same orthant and leveraging the Clarke sub-differential, the algorithm addresses the challenges posed by non-convexity and non-differentiability. The authors provide a rigorous convergence analysis and demonstrate the algorithm's empirical superiority over state-of-the-art methods like GIST on large-scale datasets.
Strengths:
1. Technical Soundness: The paper is mathematically rigorous, with detailed convergence proofs and theoretical guarantees. The use of the Clarke sub-differential for non-convex problems is well-justified and carefully analyzed.
2. Algorithm Design: The hybrid approach of alternating between QN and GD steps is innovative and effectively balances computational efficiency with theoretical robustness. The use of second-order information without explicitly forming the inverse Hessian is a notable contribution.
3. Empirical Validation: The experimental results on large-scale datasets convincingly demonstrate the faster convergence of HONOR compared to GIST, particularly when the QN step dominates.
4. Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical analysis, and experimental setup.
Weaknesses:
1. Practical Relevance of the Regularizer: The non-convex regularizers considered (e.g., SCAD, MCP, LSP) are not widely used in practice compared to simpler convex alternatives like L1 regularization. The paper does not sufficiently justify the practical importance of these regularizers in real-world applications.
2. Lack of Linear Convergence Guarantees: While the convergence to a Clarke critical point is established, the absence of linear convergence guarantees, as seen in DC programming, may limit the algorithm's appeal for certain applications.
3. Parameter Tuning: The algorithm requires manual tuning of the tradeoff parameter (Îµ) between QN and GD steps. This trial-and-error approach could be a barrier to adoption, especially for practitioners.
4. Scope of Impact: The significance of the work to the broader machine learning community is unclear. While the algorithm is technically sound, its applicability to widely studied problems or its potential to advance the state-of-the-art in machine learning is not well-demonstrated.
Pro vs. Con for Acceptance:
- Pro: The paper is mathematically rigorous, introduces a novel hybrid algorithm, and demonstrates empirical improvements over existing methods.
- Con: The practical relevance of the problem and regularizer is questionable, and the algorithm lacks linear convergence guarantees or an automated parameter-tuning mechanism.
Conclusion: While the paper makes a strong theoretical and algorithmic contribution, its practical significance and broader impact on the machine learning community are limited. I recommend acceptance for a specialized audience interested in non-convex optimization but suggest the authors address the practical relevance and parameter tuning in future work.