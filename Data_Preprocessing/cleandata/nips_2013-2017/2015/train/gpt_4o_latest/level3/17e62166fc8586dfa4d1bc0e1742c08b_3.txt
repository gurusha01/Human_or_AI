This paper introduces a novel approach for generating coherent sentence sequences from image streams using a Coherent Recurrent Convolutional Network (CRCN). The CRCN architecture integrates convolutional neural networks (CNNs) for image representation, bidirectional recurrent neural networks (BRNNs) for sentence modeling, and an entity-based local coherence model to ensure fluent transitions between sentences. The proposed method is evaluated on large datasets derived from blog posts, demonstrating its ability to outperform state-of-the-art baselines in both quantitative metrics (e.g., BLEU, CIDEr) and user studies.
Strengths  
1. Novelty: The paper addresses an underexplored problem of generating sentence sequences for image streams, extending the input-output dimensions from single images and sentences to sequences. This is a meaningful step forward in multimodal learning.  
2. Architecture Design: The integration of CNNs, BRNNs, and a coherence model is innovative and well-motivated. The coherence model, in particular, addresses a critical gap in ensuring smooth transitions between sentences, which is often overlooked in related works.  
3. Evaluation: The authors conduct extensive experiments using both quantitative metrics and user studies via Amazon Mechanical Turk (AMT). The results convincingly demonstrate the superiority of CRCN over baselines, particularly in retrieval tasks and coherence for longer sequences.  
4. Dataset: The use of blog posts as a text-image parallel corpus is creative and scalable, showcasing the method's applicability to real-world, unstructured data.
Weaknesses  
1. Missing References: The paper omits key related works on video-to-sentence tasks, such as "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework." Including such references would better situate the work within the broader context of multimodal research.  
2. Significance Test: While the quantitative results are strong, the lack of statistical significance testing undermines the reliability of the observed improvements, particularly given the small performance margins in some metrics.  
3. User Study Results: Although the CRCN outperforms the baseline RCN in user studies, the differences are marginal for shorter sequences. This raises questions about the practical impact of the coherence model in real-world scenarios with shorter image streams.  
4. Clarity: While the technical details are thorough, the paper could benefit from improved organization and conciseness, particularly in the experimental section. Some explanations, such as the training process, are overly detailed and could be streamlined.
Arguments for Acceptance  
- The paper addresses a novel and important problem, advancing the state of the art in multimodal learning.  
- The proposed CRCN architecture is well-designed and demonstrates strong performance across multiple evaluation metrics.  
- The use of blog datasets highlights the method's scalability and potential for real-world applications.
Arguments Against Acceptance  
- The omission of related works on video-to-sentence tasks weakens the paper's positioning within the field.  
- The lack of significance testing raises concerns about the robustness of the reported improvements.  
- The marginal gains in user studies for shorter sequences may limit the practical impact of the coherence model.
Recommendation  
Overall, this paper makes a meaningful contribution to the field of multimodal learning and is likely to inspire future research on coherent text generation for image streams. However, addressing the weaknesses—particularly the missing references and significance testing—would strengthen the paper. I recommend acceptance with minor revisions.