The paper addresses a critical limitation in training recurrent neural networks (RNNs) for sequence prediction tasks, such as machine translation, image captioning, and speech recognition. Current training methods minimize perplexity on gold-standard sequences but fail to account for the discrepancy between training and inference, where the model must condition on its own predictions. This mismatch often leads to error accumulation during inference. The authors propose "Scheduled Sampling," a curriculum learning strategy that gradually transitions the model from conditioning on ground truth tokens to conditioning on its own predictions. This is achieved through a probabilistic sampling mechanism with a decay schedule, which is simple, well-defined, and empirically validated.
The paper's strengths lie in the clarity and simplicity of the proposed method, which is easy to implement and integrate into existing RNN-based models like LSTMs. The empirical results demonstrate significant improvements across diverse tasks, including image captioning, parsing, and speech recognition. Notably, the method contributed to the authors' winning entry in the 2015 MSCOCO image captioning challenge. The scheduling mechanism is shown to be a key factor in performance gains, and the experiments are thorough, exploring variations such as token-level vs. sequence-level sampling and alternative sampling strategies.
However, the paper has notable weaknesses. First, it lacks sufficient baseline comparisons to related methods addressing similar issues, such as SEARN and early-update perceptron with beam search. While the authors dismiss these approaches as slower or less applicable, they do not provide empirical evidence to substantiate these claims. SEARN, for instance, also interpolates between predicted and gold data but learns a policy dynamically, contrasting with the fixed schedule in Scheduled Sampling. A direct comparison would have strengthened the paper's contributions. Additionally, the parsing results lag behind state-of-the-art methods, likely due to reduced input representation, which could have been addressed with more robust modeling.
Another limitation is the lack of discussion on how the choice of token-level vs. example-level flipping impacts performance, particularly in terms of worst-case distances. This is a critical design decision that warrants deeper analysis. Furthermore, while the method is effective, the paper does not explore back-propagating through sampling decisions, which could further enhance the approach.
In summary, the paper presents a valuable contribution to sequence prediction tasks by addressing a fundamental training-inference mismatch. Its simplicity, empirical rigor, and demonstrated effectiveness make it a strong candidate for acceptance. However, the lack of baseline comparisons and engagement with related work slightly undermines its broader impact. Future work addressing these gaps would further solidify its significance. Pros: Simple, effective method; strong empirical results; practical contributions. Cons: Insufficient baseline comparisons; limited engagement with related work; some unexplored design choices.