This paper presents several modifications to the stochastic variance-reduced gradient (SVRG) algorithm, aiming to improve its computational efficiency and practical applicability. The authors propose strategies such as corrupted gradient analysis, mixed SGD-SVRG updates, mini-batch sampling, and leveraging support vectors. They provide theoretical guarantees, demonstrating linear convergence under smooth and strongly convex assumptions, and validate their methods through experiments on logistic regression and Huberized hinge loss problems.
The paper builds on prior work on SVRG and related stochastic optimization methods, such as SAG and proximal-SVRG, by addressing computational inefficiencies in early and late iterations. For example, the growing-batch strategy reduces gradient evaluations in early iterations, while the support vector heuristic aims to identify and skip non-contributing examples in later iterations. These ideas are supported by clear theoretical proofs, which extend existing SVRG analyses to account for inexact gradient computations and mini-batching. The experimental results generally confirm the theoretical findings, showing modest improvements in training and test objectives.
Strengths:
1. Theoretical Rigor: The paper provides clear and detailed proofs for the proposed modifications, ensuring that the linear convergence rate of SVRG is preserved under the new strategies.
2. Practical Relevance: The proposed methods, such as growing-batch strategies and support vector heuristics, address real-world computational challenges in large-scale optimization.
3. Experimental Validation: The authors validate their methods on multiple datasets and demonstrate improvements in both training objectives and test error, albeit limited in magnitude.
4. Clarity: The paper is well-organized, with a logical flow from theoretical analysis to experimental results.
Weaknesses:
1. Limited Novelty: While the proposed strategies are practical, they represent incremental improvements rather than groundbreaking innovations. The proof techniques and theoretical results closely mirror those of the original SVRG.
2. Unresolved Issues: The paper does not address key limitations of SVRG, such as its reliance on strong convexity assumptions. Extending the method to non-strongly convex settings remains an open problem.
3. Performance Gains: The experimental improvements are modest, and the mixed SGD-SVRG strategy sometimes underperforms, raising questions about its robustness.
4. Notation Errors: Minor inconsistencies in the appendix detract from the overall polish of the paper.
Arguments for Acceptance:
- The paper provides a solid theoretical foundation and practical insights for improving SVRG, which is widely used in machine learning.
- The proposed modifications are simple yet effective, making them easy to adopt in practice.
- The work is relevant to the NIPS community, addressing large-scale optimization problems central to machine learning.
Arguments Against Acceptance:
- The contributions lack significant novelty and primarily extend existing work with minor modifications.
- The experimental results, while supportive, show limited practical impact, which may reduce the significance of the work.
- The paper does not address broader limitations of SVRG, such as its applicability to non-strongly convex problems.
Recommendation:
I recommend borderline acceptance. While the paper lacks groundbreaking contributions, its theoretical rigor and practical relevance make it a useful addition to the literature on stochastic optimization. However, the limited novelty and modest performance gains temper its overall impact. Addressing non-strongly convex settings or demonstrating more substantial empirical improvements would strengthen the paper significantly.