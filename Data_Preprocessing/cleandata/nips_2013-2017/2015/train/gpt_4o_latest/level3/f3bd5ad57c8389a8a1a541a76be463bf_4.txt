This paper introduces Stochastic Expectation Propagation (SEP), a novel modification of Expectation Propagation (EP) designed to reduce memory requirements in large-scale Bayesian learning tasks. SEP achieves this by maintaining a global posterior approximation, akin to Variational Inference (VI), while retaining EP's local update mechanism. The authors position SEP as a middle ground between EP's accuracy and VI's scalability, offering a significant reduction in memory consumption by a factor of \(N\), albeit sometimes at the cost of reduced accuracy and increased computation. The paper also explores connections between SEP and related algorithms, such as Assumed Density Filtering (ADF), Stochastic Variational Inference (SVI), and Averaged EP (AEP), providing a broader theoretical context.
The paper's strengths lie in its extensive experimental validation, which convincingly demonstrates SEP's practicality across a range of synthetic and real-world datasets. The experiments highlight SEP's ability to approximate EP's accuracy while drastically reducing memory usage, particularly in large-scale settings such as probabilistic backpropagation for neural networks. Additionally, the exploration of algorithmic extensions, such as parallel SEP and distributed SEP, adds depth to the work and suggests promising directions for future research.
However, the paper has several weaknesses. The presentation is marred by unclear terminology and sloppy writing, which detracts from its clarity. For instance, the abstract and introduction conflate "memory" and "computation," two distinct concepts, leading to confusion. The description of SEP lacks clarity regarding the number of iterations, and Table 1 does not specify whether SEP was run to convergence or manually stopped. The term "converges in expectation" in Section 4.1 is vague and requires further elaboration. Additionally, Theorem 1 is not proven; the appendix merely restates the claim without providing a valid proof. The claim in Section 4.4 about not storing messages relies on an assumption that does not hold for certain models, such as Latent Dirichlet Allocation (LDA), where either message storage or increased computation becomes necessary. Lastly, Appendix B.2 should explicitly clarify that memory is reduced by a factor of \(N/K\), and in Section 5.1, the claim that ADF collapses to the true posterior mode, rather than the mean, is incorrect.
Arguments for Acceptance:
1. SEP addresses a critical limitation of EP, making it more scalable for large datasets.
2. The paper provides extensive experimental evidence supporting SEP's utility.
3. Connections to related algorithms are well-explored, offering a broader theoretical perspective.
Arguments Against Acceptance:
1. The paper suffers from significant clarity and presentation issues, including vague terminology and unproven claims.
2. Some experimental details, such as convergence criteria, are missing or unclear.
3. The theoretical contributions, such as Theorem 1, lack rigorous proof.
In conclusion, while the paper makes a valuable contribution to scalable Bayesian learning, its clarity and rigor need improvement. I recommend acceptance conditional on addressing the presentation issues and providing a valid proof for Theorem 1.