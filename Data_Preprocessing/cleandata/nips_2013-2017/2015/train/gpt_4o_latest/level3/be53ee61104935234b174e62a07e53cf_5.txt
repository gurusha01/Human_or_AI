The paper addresses the problem of Isotonic Regression under weighted $\ellp$-norms ($1 \leq p \leq \infty$) on directed acyclic graphs (DAGs), a well-studied problem in statistics and machine learning. The authors present improved algorithms for this task, achieving significant reductions in time complexity for $1 \leq p < \infty$ and $\ell\infty$ norms, except for $\ell1$ in 2D space with additional edge constraints. The key technical contribution lies in the use of an approximate interior point method (\textsc{ApproxIPM}), with a novel subroutine (\textsc{BlockSolve}) for efficient Hessian inverse approximation. Additionally, the $\ell\infty$ and Strict Isotonic Regression problems are reduced to Lipschitz Learning and solved using prior algorithms, achieving linear-time solutions in certain cases.
Strengths:  
The paper makes a solid theoretical contribution by improving time complexity bounds for Isotonic Regression, a problem with applications in diverse fields such as signal processing, statistics, and machine learning. The use of \textsc{ApproxIPM} and the development of efficient solvers for Hessian systems are notable algorithmic innovations. The reduction of $\ell\infty$-norm and Strict Isotonic Regression to Lipschitz Learning is elegant and leverages existing techniques effectively. The authors also provide a unified framework for $\ellp$-norms, generalizing previous results and demonstrating the versatility of their approach. The theoretical analysis is rigorous, and the results are well-supported by proofs.
Weaknesses:  
The paper has several presentation issues that hinder its accessibility. The main algorithm and its analysis are relegated to the supplementary material, making it difficult for readers to follow the core contributions without additional effort. Additionally, the paper contains several typos and undefined notations, which detract from clarity and readability. The experimental validation is preliminary and lacks comparisons with state-of-the-art methods, raising concerns about the practical utility of the proposed algorithms. Furthermore, the work appears incremental, as it builds on existing techniques (e.g., interior point methods and Lipschitz Learning) rather than introducing fundamentally new paradigms.
Pro and Con Arguments for Acceptance:  
Pros:  
1. Significant theoretical improvements in time complexity for Isotonic Regression.  
2. Novel algorithmic contributions, particularly in the use of approximate solvers for Hessian systems.  
3. Generalization to $\ell_p$-norms and reductions to Lipschitz Learning demonstrate versatility.  
Cons:  
1. Poor presentation, with critical details missing from the main body.  
2. Insufficient experimental validation and lack of comparison with prior methods.  
3. Incremental nature of the work, with limited originality in the broader context of Isotonic Regression research.  
Recommendation:  
While the paper makes a valuable theoretical contribution, its weaknesses in presentation and experimental validation limit its impact. I recommend acceptance with major revisions, contingent on restructuring the paper to include the main algorithm and analysis in the main body, addressing typos and notational issues, and providing more comprehensive experimental results with comparisons to existing methods.