The paper addresses two significant problems in game theory and online learning: (1) comparing social welfare under regret minimization to optimal welfare, and (2) improving regret bounds when all players use regret minimization algorithms. The authors extend prior work, such as that of Rakhlin and Sridharan (2013) and Daskalakis et al. (2011), which focused on two-player zero-sum games, to the more general and practically relevant setting of multi-player non-zero-sum games. The paper introduces a class of no-regret algorithms with recency bias that achieve faster convergence rates to approximate efficiency and coarse correlated equilibria.
The authors prove a 1/T convergence rate to optimal welfare under smoothness assumptions for non-zero-sum games, leveraging algorithms with the Regret bounded by Variation in Utilities (RVU) property. They demonstrate that both the Optimistic Mirror Descent (OMD) algorithm and an optimistic variant of Follow the Regularized Leader (OFTRL) satisfy the RVU property. Notably, the paper achieves individual regret bounds of \(O(T^{1/4})\) for players using the OFTRL variation, supported by theoretical analysis and simulations. A doubling trick ensures that regret grows as \(O(T^{1/4})\) when all players use OFTRL, while maintaining \(O(T^{1/2})\) bounds under adversarial rewards. This improves upon the standard \(O(\sqrt{T})\) regret bounds in general games, advancing the state of the art.
Strengths:
1. Technical Contributions: The paper provides a significant theoretical advancement by generalizing fast convergence results to multi-player non-zero-sum games. The introduction of the RVU property and its application to OMD and OFTRL is novel and impactful.
2. Practical Relevance: The results are applicable to important settings like auctions and routing games, where decentralized no-regret dynamics are common.
3. Clarity and Organization: The paper is well-written and logically structured, with clear explanations of the algorithms, theoretical results, and their implications.
4. Experimental Validation: The simulations on auction games effectively demonstrate the practical benefits of the proposed algorithms, such as faster regret convergence and more stable dynamics compared to standard methods like Hedge.
Weaknesses:
1. Limited Experimental Scope: While the simulations are insightful, they are restricted to a specific auction game. Broader empirical validation across diverse game settings would strengthen the paper.
2. Assumptions on Smoothness: The results rely on smoothness assumptions, which may not hold in all real-world games. The paper could benefit from a discussion of how robust the algorithms are to deviations from these assumptions.
3. Comparison to Vanilla Algorithms: The paper does not fully explore whether the modifications to satisfy the RVU property are necessary or merely sufficient. A deeper analysis or counterexamples would clarify this point.
Pro/Con Arguments:
- Pro: The paper addresses a challenging and important problem, provides novel theoretical insights, and demonstrates practical improvements in convergence rates.
- Con: The experimental evaluation is somewhat narrow, and the reliance on smoothness assumptions may limit the generality of the results.
Recommendation: Accept. The paper makes a strong scientific contribution by extending fast convergence results to multi-player general games, improving regret bounds, and introducing the RVU property. Its theoretical and practical implications are significant, and the work is well-suited for the conference audience. However, the authors are encouraged to expand the empirical evaluation and discuss the necessity of the RVU modifications in future work.