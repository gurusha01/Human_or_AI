The paper introduces the Poisson Gamma Belief Network (PGBN), a novel deep belief network designed to model high-dimensional count data. The key innovation lies in using gamma-distributed nonnegative hidden units and a Poisson likelihood to construct a multilayer representation. The authors propose an upward-downward Gibbs sampling method for joint training and a layer-wise training strategy to infer the network structure. The model is applied to text analysis as a "deep belief topic model," with experiments conducted on datasets like 20 Newsgroups and NIPS12 to evaluate its performance in feature extraction, classification, and perplexity reduction.
Strengths:
1. Novelty and Originality: The use of gamma-distributed hidden units and the Poisson likelihood is a significant departure from traditional binary or sigmoid-based deep belief networks. This approach is well-suited for count data, addressing overdispersion and correlations in latent features.
2. Technical Soundness: The derivation of the upward-downward Gibbs sampler and the propagation of Dirichlet and gamma distributions is rigorous and well-documented. The layer-wise training strategy is a practical addition for inferring network structure.
3. Significance: The model demonstrates clear advantages in handling high-dimensional count data, with empirical results showing improvements in classification accuracy and perplexity over single-layer models.
4. Clarity: The paper is generally well-written, with detailed mathematical derivations and clear descriptions of the experimental setup.
Weaknesses:
1. Empirical Validation: While the experiments demonstrate the potential of PGBN, the empirical analysis lacks comparisons with strong baselines like SVMs, ridge regression, or other deep topic models. This omission makes it difficult to contextualize the model's performance relative to existing methods.
2. Layer Size Budget: The fixed budget approach for the first layer's width is questionable. The authors do not adequately justify why this constraint is necessary or explore adaptive alternatives.
3. Marginal Gains with Depth: Adding layers beyond the second shows only marginal performance improvements, with high variance in results. This raises questions about the practical utility of deeper networks in this framework.
4. Background Context: The paper does not sufficiently discuss related work on neural networks with weights outside [0,1], which would provide better context for the novelty of gamma-distributed hidden units.
5. Stopword Removal: The necessity of stopword removal is unclear, as topic models like LDA typically handle stopwords inherently.
6. Clarity in Experiments: The experimental section introduces tasks in a somewhat confusing manner. Additionally, there is a minor typo ("budge" â†’ "budget").
Arguments for Acceptance:
- The paper presents a novel and technically sound approach to deep belief networks for count data.
- The upward-downward Gibbs sampler and layer-wise training strategy are valuable contributions.
- The model demonstrates promising results in text analysis tasks, indicating its potential impact.
Arguments Against Acceptance:
- The empirical evaluation is incomplete, lacking comparisons with competitive baselines.
- The marginal performance gains with additional layers and the fixed budget constraint reduce the practical significance of the proposed method.
- Insufficient discussion of related work and unclear necessity of certain preprocessing steps (e.g., stopword removal).
Recommendation:
While the paper introduces an interesting and novel idea, it requires stronger experimental validation and better contextualization within existing literature. I recommend acceptance only if the authors address the empirical shortcomings and provide more robust comparisons with baselines. Otherwise, it may be better suited for revision and resubmission.