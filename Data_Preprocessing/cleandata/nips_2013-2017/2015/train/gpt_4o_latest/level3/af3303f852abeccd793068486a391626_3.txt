The paper presents a novel approach to Bayesian parameter estimation for deep neural networks by combining online Monte Carlo methods (specifically SGLD) with model distillation, resulting in a single neural network that approximates the posterior predictive distribution. This "Bayesian dark knowledge" approach is a natural extension of prior work on Bayesian neural networks, particularly building on Snelson and Ghahramani's 2005 work, and updates it with neural networks and online MCMC methods. The proposed method is simpler to implement than alternatives like expectation propagation (EP) and variational Bayes (VB) while achieving competitive or superior performance in terms of log-likelihood and predictive uncertainty.
Strengths:
1. Clarity and Presentation: The paper is well-written, clearly organized, and provides sufficient detail for reproducibility. The method is straightforward to implement, making it accessible to practitioners.
2. Novelty and Impact: The combination of SGLD and distillation is a novel contribution, and the method has the potential to significantly impact Bayesian neural network research by reducing the computational and memory overhead of traditional MCMC methods.
3. Empirical Evaluation: The paper demonstrates the effectiveness of the method on a variety of tasks, including classification (MNIST) and regression (Boston Housing), and compares favorably to EP, VB, and SGD in terms of log-likelihood and calibration of predictions.
4. Efficiency Gains: By distilling the posterior predictive distribution into a single network, the method reduces test-time costs by a factor of 10,000 compared to SGLD, without significant loss in accuracy.
Weaknesses:
1. Posterior Accuracy: The paper acknowledges that SGLD provides only approximate posterior samples, which may not respect the posterior locally during extended exploration. This limitation is not adequately addressed.
2. Incorrect Posterior Visualization: Figure 2 shows an incorrect posterior, but the paper does not explicitly discuss or resolve this issue.
3. Misleading Claims: The claim that priors are equivalent to L_2 regularization is misleading and should be removed. Additionally, the use of simple spherical Gaussian priors is underwhelming given the rich prior literature on Bayesian neural networks.
4. Training Time: Training times are not explicitly discussed but are likely slower than other methods, as hinted in the abstract. This omission weakens the practical evaluation of the method.
5. Application Demonstration: The paper does not demonstrate the utility of the method in specific applications where predictive uncertainty is critical, such as active learning or contextual bandits.
6. Comparisons to Regularization: The paper lacks comparisons to standard regularization techniques and overfitting prevention methods, which would provide a more comprehensive evaluation.
7. Title and Formatting: The title does not accurately reflect the paper's contributions, and typesetting for numerical values (e.g., $5e-6$) should be improved. References also need to follow the NIPS style guide with consistent formatting.
Recommendation:
While the paper presents a promising and impactful method, several issues need to be addressed before acceptance. These include correcting the posterior visualization, clarifying misleading claims, expanding comparisons to regularization techniques, and demonstrating utility in specific applications. If these weaknesses are addressed, the paper could make a strong contribution to the field of Bayesian neural networks.
Arguments for Acceptance:
- Novel and impactful method with significant efficiency gains.
- Clear presentation and empirical results demonstrating competitive performance.
Arguments for Rejection:
- Incorrect posterior visualization and misleading claims weaken the scientific rigor.
- Lack of application demonstrations and comparisons to standard regularization techniques.
- Potentially slower training times not adequately discussed.
Rating: Weak Accept (conditional on addressing key weaknesses).