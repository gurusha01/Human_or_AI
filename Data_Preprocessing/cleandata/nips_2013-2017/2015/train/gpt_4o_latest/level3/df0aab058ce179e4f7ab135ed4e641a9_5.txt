The paper introduces Rectified Factor Networks (RFNs), a novel deep factor analysis model that employs posterior constraints for unsupervised representation learning. The authors propose a projected EM algorithm to enforce these constraints, ensuring non-negativity and normalization of posterior means. This approach is claimed to improve computational efficiency and representation quality compared to existing methods. The paper demonstrates RFNs' effectiveness in generating sparse, non-linear, high-dimensional representations and compares their performance against popular unsupervised methods like autoencoders, RBMs, and PCA. Additionally, RFNs are applied as a pretraining technique for deep networks and in pharmaceutical gene expression studies, showcasing their practical utility.
Strengths:  
The paper makes a significant contribution by being the first to apply posterior regularization to a deep factor analysis model using a simple projected EM algorithm. The theoretical guarantees of convergence and correctness are well-supported, and the proposed method is computationally efficient, with a detailed complexity analysis provided. Experimental results indicate that RFNs achieve sparser codes, lower reconstruction errors, and better covariance approximation than competing methods. The application of RFNs to real-world gene expression data is particularly compelling, as it highlights the model's ability to detect rare and small events that other methods miss. The availability of an open-source implementation further enhances the paper's impact and reproducibility.
Weaknesses:  
Despite its strengths, the paper has several shortcomings. Section 2 is poorly written and overly complicated, making it difficult to follow the derivation of the RFN model. The claimed speedup over Newton's method lacks clarity, as the paper does not provide step-by-step timing comparisons with other optimization techniques. The experimental evaluation, while promising, has limitations. The first experiment uses a trivial synthetic dataset, and while RFNs outperform RBMs in reconstruction, the significance of differences in sparsity is not adequately explained. The second experiment shows only marginal improvement (0.5%) on MNIST and poor performance on CIFAR-10, raising concerns about the model's scalability to more complex, real-world datasets. A broader evaluation on larger datasets would strengthen the claims. Additionally, the paper lacks a thorough discussion of related work, particularly in comparing RFNs to other sparse coding methods.
Pro and Con Arguments for Acceptance:  
Pro:  
- Novel application of posterior regularization to deep factor analysis.  
- Strong theoretical guarantees and computational efficiency.  
- Demonstrated utility in real-world applications (e.g., gene expression analysis).  
- Open-source implementation supports reproducibility.  
Con:  
- Poor clarity and organization in key sections (e.g., Section 2).  
- Limited experimental evaluation on complex datasets.  
- Insufficient explanation of claimed speedup and sparsity differences.  
Conclusion:  
The paper presents a novel and promising approach to unsupervised representation learning, with strong theoretical foundations and practical applications. However, the clarity of presentation and experimental evaluation need improvement. I recommend acceptance contingent on revisions to address these issues, as the paper has the potential to make a meaningful contribution to the field.