This paper addresses the challenging problem of decomposing a third-order tensor \( Z \) into a low-rank tensor \( X \) and a sparse corruption tensor \( Y \). The authors propose an algorithm that builds upon Leurgans' algorithm for tensor factorization, extending it to handle sparse corruptions via tensor contractions. The method leverages convex optimization techniques to impose low-rank and sparsity constraints, using the nuclear norm for \( X \) and the \(\ell_1\) norm for \( Y \). The paper also provides theoretical guarantees for exact recovery under specific incoherence conditions and validates the approach with synthetic experiments.
Strengths:  
The paper tackles an important and underexplored problem in tensor factorization, particularly for non-symmetric tensors. By extending simultaneous diagonalization methods to tensors, the authors introduce a novel algorithmic framework that avoids computationally expensive tensor unfolding. The use of tensor contractions to reduce the problem to matrix decomposition is both elegant and computationally efficient, with complexity scaling as \( O(n^3) \) for third-order tensors. The theoretical analysis is rigorous, offering deterministic and probabilistic guarantees for recovery under well-defined conditions. The modularity of the algorithm is another strength, as it can be extended to higher-order tensors, block sparsity, and tensor completion. The experiments on synthetic data demonstrate the algorithm's ability to recover low-rank and sparse components accurately, even for large tensors.
Weaknesses:  
Despite its strengths, the paper has several limitations. First, the experimental evaluation is limited to synthetic data, which may not fully capture the complexities of real-world applications. Testing the algorithm on real-world datasets, such as those from recommendation systems or neuroscience, would strengthen its practical relevance. Second, while the theoretical guarantees are robust, the proofs are dense and difficult to follow, which may hinder accessibility for a broader audience. Additionally, the algorithm's steps, particularly in Section 2, could benefit from clearer explanations and more intuitive descriptions. Finally, the paper does not explore the impact of noise beyond sparse corruption, which could be a limitation in practical scenarios.
Major Comments:  
1. The extension of simultaneous diagonalization to non-symmetric tensors is a significant contribution, but the paper should clarify the stability of the method when multiple contractions are performed.  
2. The lack of real-world experiments is a notable gap. Evaluating the algorithm on practical datasets would better demonstrate its utility and robustness.  
3. The clarity of the algorithm's presentation could be improved, particularly for readers less familiar with tensor contractions and simultaneous diagonalization.
Conclusion:  
This paper makes a valuable contribution to the field of tensor decomposition by addressing the sparse and low-rank decomposition problem with a novel and computationally efficient approach. While the theoretical foundations are solid, the paper would benefit from additional real-world experiments and clearer exposition of the algorithm. Overall, the work is original and relevant, advancing the state of the art in robust tensor factorization. I recommend acceptance, contingent on addressing the clarity and evaluation concerns.