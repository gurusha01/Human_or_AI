Review of the Paper
The paper proposes a kernel-based method for cross-domain matching of bag-of-words data, leveraging a shared latent space and probability distributions to capture semantically similar features across domains. The method represents each feature as a latent vector in a shared space and uses kernel embeddings of distributions to measure differences between instances. The approach is validated on tasks such as matching multilingual Wikipedia articles, documents with tags, and images with tags, demonstrating superior performance compared to existing methods.
Strengths:
1. Significance and Applicability: The proposed method addresses an important problem in cross-domain matching, with applications in natural language processing, information retrieval, and computer vision. The use of kernel embeddings of distributions to represent instances as probability distributions is a novel and effective approach.
2. Experimental Validation: The experiments are thorough, covering diverse datasets (e.g., multilingual Wikipedia, document-tag, and image-tag datasets). The method consistently outperforms baseline approaches, demonstrating its robustness and effectiveness.
3. Clarity of Writing: The paper is well-written and provides sufficient background on kernel embeddings, latent space representations, and related methods. The inclusion of detailed experimental results and examples adds to its clarity.
4. Broader Impact: The method has potential for broader use, such as in representation learning with hierarchical latent spaces or generative models like RBMs and deep neural networks.
Weaknesses:
1. Technical Depth: While the method is novel, the technical depth could be improved. For example, the paper does not provide a detailed theoretical analysis of why the proposed approach outperforms existing methods, particularly kernel CCA or bilingual topic models. A more rigorous exploration of the method's limitations and theoretical guarantees would strengthen its contribution.
2. Clarity of Certain Concepts: Some points require further clarification:
   - The role and definition of "development data" are not explicitly explained.
   - The motivation for the choice of Gaussian priors and their connection to L2 regularization needs elaboration.
   - The relationship between the proposed method and kernel CCA could be more explicitly discussed, especially in terms of computational complexity and scalability.
3. Redundancy in Equations: Equations (15) and (16) are redundant with (12) and (14) and could be removed to save space.
Originality:
The idea of using kernel embeddings of distributions for cross-domain matching is novel, though it builds on prior work such as kernel CCA and bilingual topic models. The focus on matching rather than classification distinguishes it from related methods like those in Yuya et al. (NIPS 2014). However, the novelty is somewhat incremental, as the method adapts existing techniques (e.g., kernel embeddings) to a new application.
Significance:
The method advances the state of the art in cross-domain matching and has practical implications for tasks involving heterogeneous data. Its ability to handle non-linear relationships and represent instances as distributions in a shared latent space is a notable contribution.
Pro and Con Arguments for Acceptance:
- Pro: The method is novel, well-validated, and addresses an important problem with broad applicability.
- Con: The paper lacks sufficient technical depth and theoretical analysis, and certain aspects of the method require clarification.
Recommendation: Accept with minor revisions. The paper makes a meaningful contribution to cross-domain matching, but addressing the noted weaknesses would enhance its impact.