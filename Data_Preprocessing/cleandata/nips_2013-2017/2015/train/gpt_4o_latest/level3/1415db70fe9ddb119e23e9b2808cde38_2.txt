The paper introduces a modified Expectation-Maximization (EM) algorithm tailored for high-dimensional, sparse parameter estimation, alongside a statistical framework for hypothesis testing in such settings. The authors claim that their method achieves near-optimal statistical convergence rates and provides computationally feasible solutions for parameter estimation and inference in high-dimensional latent variable models. The introduction of a truncation step to enforce sparsity is a novel addition, and the proposed decorrelated score test addresses the challenge of hypothesis testing in the presence of high-dimensional nuisance parameters.
Strengths:
1. Technical Contributions: The paper makes significant theoretical advances by extending the EM algorithm to high-dimensional settings, where traditional methods fail. The derivation of convergence conditions and statistical rates is rigorous and well-documented.
2. Novelty: The proposed truncation step in the EM algorithm is a creative approach to enforce sparsity, and the decorrelated score test is a meaningful contribution to statistical inference in high dimensions.
3. Comprehensive Analysis: The authors provide a unified theoretical framework, covering both computational and statistical guarantees. The application to Gaussian mixture models and mixture of regression models demonstrates the versatility of the approach.
4. Clarity in Related Work: The paper situates itself well within the existing literature, highlighting gaps it aims to address and differentiating itself from prior works, such as those focusing on low-dimensional settings or tensor methods.
Weaknesses:
1. Clarity and Accessibility: While the paper is well-organized, sections 3 and 4 are dense and mathematically intricate, making it challenging for readers, particularly those without a strong theoretical background in high-dimensional statistics or optimization.
2. Minimax-Optimal Claim: The claim of achieving the minimax-optimal rate (√(s*log(d/n))) is compelling but lacks a formal proof in the main manuscript, which undermines its credibility.
3. Practical Guidance: The paper does not provide sufficient practical advice on selecting critical parameters, such as the sparsity level (^s) and the initial value (β^init), which are crucial for real-world applications.
4. Truncation Step: The role and importance of the truncation step in the algorithm remain underexplained. A deeper discussion or empirical analysis would strengthen the paper.
5. Real-World Applicability: The method's applicability to practical machine learning problems is unclear. The lack of empirical results on real-world datasets limits its appeal to practitioners.
Post-Rebuttal Comments:
The authors provided satisfactory responses to some of the concerns, such as clarifying the truncation step and addressing the minimax-optimal claim. However, the paper remains highly theoretical and challenging for the broader machine learning audience. Additionally, the lack of practical guidance and real-world validation continues to raise questions about its applicability.
Recommendation:
While the paper makes significant theoretical contributions and advances the state of the art in high-dimensional EM algorithms, its accessibility and practical relevance are limited. I recommend acceptance with revisions to address the clarity and applicability concerns. The paper is a strong fit for NIPS, given its focus on advancing computational and statistical methods, but it would benefit from additional empirical validation and clearer exposition. 
Arguments for Acceptance:
- Novel theoretical contributions with rigorous analysis.
- Addresses a critical gap in high-dimensional latent variable modeling.
- Potential to inspire further research in high-dimensional inference.
Arguments Against Acceptance:
- Dense and challenging for non-expert readers.
- Limited practical guidance and real-world validation.
- Unproven minimax-optimal claim in the main text.