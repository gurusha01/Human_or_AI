This paper introduces Principal Differences Analysis (PDA), a novel dimensionality reduction framework that leverages the squared Wasserstein distance to identify differences between high-dimensional distributions. The authors propose a semidefinite programming (SDP) relaxation to solve the underlying optimization problem and extend the method to a sparse variant (SPARDA) for feature selection. The approach is evaluated on synthetic data, benchmark datasets, and real-world single-cell RNA-seq data, demonstrating its utility in identifying meaningful differences between populations.
Strengths:
1. Technical Soundness: The proposed method is well-grounded in theory, with a rigorous derivation of the SDP relaxation and convergence guarantees. The authors also provide a detailed discussion of the algorithmic implementation, including the RELAX and tightening procedures, which are computationally efficient and scalable.
2. Novelty: The use of the Wasserstein distance in conjunction with SDP for dimensionality reduction and feature selection is innovative. While related to Transfer Component Analysis (TCA), the method generalizes beyond specific parametric assumptions and captures broader distributional differences.
3. Experimental Validation: The method outperforms existing approaches like sparse PCA, LDA, and logistic lasso in identifying relevant features and detecting differences in high-dimensional settings. The application to single-cell RNA-seq data highlights its practical significance in real-world scenarios.
4. Clarity: The paper is well-written and organized, with clear explanations of the methodology, theoretical results, and experimental findings. The inclusion of supplementary materials further enhances reproducibility and accessibility.
Weaknesses:
1. Baseline Comparisons: While the method is compared to several baselines, a simple Lasso-based baseline for linearly related distributions (X and Y) is missing. Adding this baseline would strengthen the experimental validation and provide a clearer picture of the method's advantages in linear settings.
2. Nonlinear Extensions: The current formulation is limited to linear projections. Extending the method to handle nonlinear relationships would significantly broaden its applicability, particularly in complex real-world datasets where nonlinear interactions are prevalent.
3. Alternative Baselines: The authors note that TCA can be adapted to find the most different direction. Including this as a baseline would provide a more comprehensive evaluation of the proposed method's performance relative to closely related approaches.
Suggestions for Improvement:
- Incorporate a Lasso-based baseline in the experiments to assess performance in linear cases.
- Explore extensions of the algorithm to nonlinear settings, potentially through kernel methods or deep learning frameworks.
- Compare the method against an adapted TCA baseline to better contextualize its contributions.
Arguments for Acceptance:
- The method is technically sound, novel, and impactful, addressing an important problem in machine learning.
- The experimental results are compelling, demonstrating superior performance in both synthetic and real-world datasets.
- The paper is well-written and accessible, making it a valuable contribution to the community.
Arguments Against Acceptance:
- The lack of a Lasso-based baseline and nonlinear extensions limits the scope of the evaluation.
- The absence of a TCA-based baseline leaves a gap in the comparative analysis.
Recommendation:
Overall, this paper makes a significant contribution to the field of dimensionality reduction and distributional analysis. While there are some areas for improvement, the strengths of the work outweigh the weaknesses. I recommend acceptance, with minor revisions to address the suggested improvements.