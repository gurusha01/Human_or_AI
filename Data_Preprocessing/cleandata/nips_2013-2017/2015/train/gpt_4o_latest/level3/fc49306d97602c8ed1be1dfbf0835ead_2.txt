The paper proposes a kernel-based method for cross-domain instance matching, leveraging kernel embeddings of distributions to represent instances as distributions in a shared latent space. The method aims to address the challenge of measuring similarity between instances in different domains, such as multilingual documents or images with tags, by learning latent vector representations for features. The authors demonstrate their approach on tasks like matching multilingual Wikipedia articles, documents with tags, and images with tags, claiming superior performance over existing methods like kernel CCA and bilingual topic models.
Strengths:
1. Innovative Use of Kernel Embeddings: The paper employs kernel embeddings of distributions in a novel way to represent instances as distributions in a shared latent space, enabling non-linear cross-domain matching.
2. Diverse Applications: The method is applied to multiple domains (e.g., multilingual text, document-tag, image-tag), showcasing its versatility.
3. Empirical Performance: The experimental results indicate that the proposed method outperforms baseline methods like kernel CCA and bilingual latent Dirichlet allocation (BLDA) on several datasets.
4. Theoretical Foundation: The use of maximum mean discrepancy (MMD) to measure differences between distributions is well-grounded in statistical theory.
Weaknesses:
1. Lack of Comparison with Simpler Alternatives: The paper does not compare its method to natural alternatives like kernel CCA on pre-trained word embeddings (e.g., word2vec), which are widely used in NLP. This omission makes it difficult to assess the practical advantages of the proposed approach.
2. Incremental Novelty: While the method builds on kernel embeddings of distributions, its novelty appears incremental compared to prior work ([18] and [19]). The application to new domains is interesting but not groundbreaking without stronger experimental evidence.
3. Limited Engagement with Recent Literature: The paper fails to engage with recent advances in multilingual embeddings and deep learning-based approaches for cross-domain tasks, such as neural machine translation or multimodal embeddings.
4. Scalability Concerns: The method is tested only on small datasets, and its scalability to larger datasets or real-world applications is not discussed.
5. Clarity and Organization: While the technical details are thorough, the paper could benefit from clearer explanations of its contributions and a more structured discussion of related work.
6. Evaluation Gaps: The evaluation lacks comparisons with state-of-the-art multilingual embedding systems and deep learning-based methods for tasks like automatic caption generation, which are highly relevant to the proposed approach.
Recommendation:
The paper presents an interesting extension of kernel embeddings of distributions for cross-domain matching. However, its incremental novelty, lack of comparisons with simpler and more recent alternatives, and limited engagement with relevant literature weaken its contribution. Stronger experimental evidence, particularly on larger datasets and against state-of-the-art methods, is necessary to justify publication. Additionally, scalability and practical applicability should be addressed.
Arguments for Acceptance:
- Novel application of kernel embeddings to cross-domain matching tasks.
- Promising empirical results on diverse datasets.
- Solid theoretical foundation.
Arguments Against Acceptance:
- Incremental contribution over prior work.
- Lack of comparisons with simpler and state-of-the-art alternatives.
- Limited discussion of scalability and real-world applicability.
- Insufficient engagement with recent literature.
Final Recommendation: Weak Reject. While the method is promising, the paper requires significant improvements in evaluation, clarity, and positioning within the broader research landscape to meet the standards of a top-tier conference.