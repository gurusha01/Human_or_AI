The paper introduces Principal Differences Analysis (PDA) and its sparse variant (SPARDA) as novel methods for characterizing differences between high-dimensional distributions. By leveraging the Wasserstein divergence and the Cramer-Wold device, the approach identifies linear projections that maximize distributional differences without requiring parametric assumptions. The sparse variant, SPARDA, further isolates features responsible for these differences, making the method particularly useful for exploratory analysis in complex datasets. The authors provide rigorous theoretical foundations, including convergence guarantees, and propose efficient algorithms based on semidefinite relaxation and gradient-based tightening. Applications to synthetic datasets, real-world genomic data, and comparisons with existing methods demonstrate the utility and robustness of the approach.
Strengths:
1. Originality: The paper presents a novel and generalizable framework for comparing high-dimensional distributions. Unlike traditional methods such as LDA or sparse PCA, PDA and SPARDA are designed to uncover arbitrary differences, including those beyond mean shifts or covariance structures.
2. Algorithmic Sophistication: The proposed algorithms, particularly the semidefinite relaxation and tightening procedures, are well-motivated and address the inherent non-convexity of the optimization problem. The RELAX algorithm is particularly innovative in balancing computational efficiency and solution quality.
3. Empirical Validation: The experiments are comprehensive, spanning synthetic datasets, benchmark challenges (e.g., MADELON), and real-world single-cell RNA-seq data. The results convincingly demonstrate the superiority of SPARDA in identifying relevant features and maintaining statistical power in high-dimensional settings.
4. Theoretical Contributions: The paper provides concentration results, sparsistency guarantees, and conditions under which the semidefinite relaxation is tight, offering a strong theoretical foundation for the proposed methods.
5. Significance: The ability to identify subtle, non-parametric differences between distributions has broad applicability across fields such as genomics, neuroscience, and social sciences. The method advances the state of the art in exploratory data analysis.
Weaknesses:
1. Clarity: While the paper is algorithmically and theoretically dense, some sections (e.g., the RELAX algorithm and its dual formulation) are difficult to follow without significant prior expertise. Additional diagrams or pseudocode could improve accessibility.
2. Comparison to Related Work: Although the authors reference related methods like DiProPerm and sparse PCA, a more detailed empirical comparison across diverse datasets would strengthen the case for PDA/SPARDA's superiority.
3. Scalability: The computational complexity of the RELAX algorithm (e.g., solving optimal transport problems) may limit its applicability to very large datasets. While the authors mention incremental subgradient methods, these are not explored in detail.
4. Reviewer Confidence: Due to limited familiarity with the relevant literature on high-dimensional distributional comparisons, I have low confidence in fully assessing the novelty and scope of the contributions.
Recommendation:
I recommend acceptance of this paper, as it introduces a significant methodological advancement with strong theoretical and empirical support. However, the authors should consider improving the clarity of the algorithmic descriptions and providing additional computational benchmarks to address scalability concerns.
Arguments for Acceptance:
- Novel and generalizable framework for high-dimensional distributional comparisons.
- Strong theoretical guarantees and algorithmic innovations.
- Demonstrated utility across synthetic and real-world datasets.
Arguments Against Acceptance:
- Limited clarity in some algorithmic details.
- Potential scalability issues for very large datasets.
- Reviewer's low confidence in assessing the full novelty of the work.