This paper presents a method for training recurrent neural networks (RNNs) as near-optimal feedback controllers for interactive character control in physics simulators. The proposed approach combines supervised learning with trajectory optimization, enabling the generation of stable and realistic behaviors across diverse dynamical systems, such as swimming, flying, and biped/quadruped locomotion. Notably, the method does not rely on motion capture or task-specific features, making it broadly applicable to various body morphologies and cyclic behaviors. The authors leverage Contact-Invariant Optimization (CIO) to generate training data and inject noise during training to improve robustness and generalization. The resulting neural network policies are capable of real-time execution, offering a promising framework for applications in robotics, animation, and biomechanics.
Strengths:
1. Technical Soundness: The integration of trajectory optimization with neural network training is well-motivated and technically robust. The use of noise injection and distributed training demonstrates thoughtful design to address overfitting and scalability.
2. Generality: The method's ability to handle diverse body types and tasks without hand-crafted controllers is a significant contribution, as evidenced by its successful application to swimming, flying, and walking characters.
3. Real-Time Execution: The focus on real-time policy execution is a practical advancement over traditional trajectory optimization, which is typically offline.
4. Reproducibility: The paper provides detailed descriptions of the algorithm, training setup, and architecture, which should facilitate replication by other researchers.
Weaknesses:
1. Positioning and Novelty: While the paper builds on prior work in trajectory optimization and neural network control, it lacks a clear differentiation from related research, particularly that of Pieter Abbeel and Sergey Levine. The claimed advancements (e.g., handling 3D locomotion) are not sufficiently emphasized or quantitatively compared to prior methods.
2. Ambiguity in Explanations: Key terms such as "large and ill-conditioned feedback gains" and "softer feedback gains" are not well-defined, leaving room for misinterpretation. Additionally, the rationale for not using LQG is deferred and insufficiently discussed.
3. Clarity of Notation: The notation in lines 226-240 is visually dense and could benefit from reorganization or additional explanation to improve readability.
4. Evaluation Gaps: While the results are promising, the paper lacks a systematic comparison with alternative methods (e.g., model-predictive control or direct policy learning) on metrics like runtime efficiency, robustness, and task variety.
Pro and Con Arguments:
Pro:
- The method addresses a challenging problem in interactive character control and demonstrates generalizability across tasks and morphologies.
- The combination of trajectory optimization and neural networks is a meaningful contribution to the field.
- The paper provides a strong foundation for future work in both animation and robotics.
Con:
- The lack of clear novelty compared to prior work may limit its impact.
- Ambiguities in terminology and notation reduce the paper's accessibility.
- The evaluation could be more comprehensive, particularly in comparing against state-of-the-art methods.
Recommendation:
While the paper makes a solid technical contribution and demonstrates promising results, the lack of clarity in positioning and some ambiguities in explanation weaken its overall impact. I recommend acceptance with revisions, contingent on the authors addressing the issues of novelty, clarity, and evaluation.