This paper presents an innovative approach to unsupervised learning of sentence representations, extending distributional methods from words to sentencesâ€”a significant and timely contribution to the field. The authors propose "skip-thought vectors," inspired by the skip-gram model, to encode sentences by predicting their surrounding sentences in a corpus. Using a GRU-based encoder-decoder architecture, the model produces generic sentence embeddings that are robust and versatile across multiple tasks. The paper also introduces a vocabulary expansion technique, allowing the model to encode words not seen during training, which is a practical enhancement for real-world applications.
The experiments are thorough and evaluate the proposed method across eight diverse tasks, including semantic relatedness, paraphrase detection, image-sentence ranking, and sentiment classification. The results demonstrate that skip-thought vectors perform competitively with task-specific supervised models, highlighting their robustness and generalizability. Notably, the method achieves state-of-the-art or near state-of-the-art results in semantic relatedness and paraphrase detection, while also performing well in image-sentence ranking and classification benchmarks. The use of linear classifiers to evaluate the embeddings ensures that the results reflect the quality of the learned representations rather than task-specific fine-tuning.
Strengths:
1. Originality: The extension of distributional semantics to sentences via an unsupervised learning approach is novel and impactful. The use of a skip-gram-inspired objective at the sentence level is a creative adaptation.
2. Thorough Evaluation: The experiments are well-designed, covering a wide range of tasks and providing strong evidence for the generalizability of the proposed method.
3. Practical Contributions: The vocabulary expansion technique is a valuable addition, addressing a common limitation in embedding models.
4. Clarity: The paper is well-written and organized, with clear explanations of the model architecture, training procedure, and experimental setup.
Weaknesses:
1. Significance: While the results are robust, the performance on certain tasks, such as sentiment classification, lags behind task-specific supervised models. This indicates that the embeddings may not fully capture domain-specific nuances.
2. Scalability: Training the model on a large corpus for two weeks may limit its accessibility to researchers with constrained computational resources.
3. Naming Clarity: The term "skip-thought vectors" could be misleading, as it does not immediately convey the sentence-level nature of the embeddings. A more descriptive name, such as "skip-sentence vectors," is recommended for clarity.
Pro Acceptance Arguments:
- The paper addresses a longstanding challenge in NLP: learning unsupervised, generic sentence representations.
- The proposed method is novel, well-motivated, and demonstrates strong empirical performance across diverse tasks.
- The vocabulary expansion technique enhances the model's practical utility.
Con Acceptance Arguments:
- The computational cost of training may hinder adoption.
- The embeddings, while general, do not outperform task-specific models in all cases, limiting their significance for certain applications.
In conclusion, this paper makes a meaningful contribution to the field of sentence representation learning. While there are areas for improvement, such as computational efficiency and task-specific performance, the proposed method is a valuable step forward. I recommend acceptance, with a suggestion to rename the approach to "skip-sentence vectors" for improved clarity.