This paper addresses a critical issue in batch learning from logged bandit feedback (BLBF) by identifying and proposing a solution to the problem of "propensity overfitting," which arises when the conventional counterfactual risk estimator is used in complex hypothesis spaces. The authors introduce a self-normalized risk estimator as an alternative, which they demonstrate avoids the anomalies of the conventional estimator. They further propose a new learning algorithm, Norm-POEM, based on the Counterfactual Risk Minimization (CRM) principle, and validate its effectiveness through empirical evaluations on multi-label classification tasks.
Strengths:
1. Novel Problem Identification: The paper identifies "propensity overfitting" as a previously unaddressed issue in BLBF, which is a meaningful contribution to the field. This insight highlights the limitations of existing unbiased estimators and provides a clear motivation for the proposed solution.
2. Proposed Solution: The self-normalized risk estimator is theoretically well-motivated and addresses the identified issues, such as unbounded variance and lack of equivariance. The authors provide a strong theoretical foundation for its consistency and boundedness.
3. Empirical Validation: The experimental results convincingly demonstrate that Norm-POEM outperforms the baseline POEM algorithm in terms of generalization performance, robustness to propensity overfitting, and computational efficiency. The inclusion of results across multiple datasets strengthens the claims.
4. Practical Utility: The availability of code and the use of real-world datasets for evaluation make the work accessible and reproducible, which is highly appreciated.
5. Clarity of Writing: The paper is generally well-written and organized, with a clear exposition of the problem, solution, and experimental results.
Weaknesses:
1. Limited Explanation of Key Concepts: The relationship between propensity overfitting and the large variance of weights could be elaborated further. While the authors provide examples, a more intuitive explanation or additional theoretical insights would enhance understanding.
2. Unclear Point (l. 199): The logic behind a specific point (line 199) is unclear and warrants clarification. This could hinder the reader's ability to fully grasp the argument.
3. Fairness of Claims: The claim that prior works use unbiased estimators without addressing propensity overfitting may not be entirely fair, as techniques like clipping are commonly employed to mitigate such issues. A more nuanced discussion of related work would strengthen the paper.
4. Verification of Novelty: Due to time constraints, I could not thoroughly verify the novelty of the proposed estimator or its technical details. A deeper comparison with existing variance reduction techniques and control variate methods would be beneficial.
Recommendation:
Despite the aforementioned weaknesses, the paper presents an interesting and potentially impactful contribution to BLBF. The identification of propensity overfitting and the introduction of the self-normalized risk estimator are valuable additions to the field. The empirical results support the claims, and the proposed Norm-POEM algorithm shows promise for practical applications. I recommend acceptance, provided the authors address the unclear point and refine their discussion of related work.