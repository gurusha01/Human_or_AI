The paper introduces HONOR, a novel optimization algorithm tailored for non-convex regularized problems, which effectively integrates second-order information to accelerate convergence. By combining a Quasi-Newton (QN) step and a Gradient Descent (GD) step, the algorithm addresses the challenges of non-convexity and non-smoothness in sparse learning formulations. The authors provide rigorous theoretical guarantees, proving that every limit point of HONOR is a Clarke critical point, which is a significant contribution given the inherent difficulties of analyzing non-convex problems. Empirical results on large-scale datasets demonstrate HONOR's superior convergence speed compared to state-of-the-art methods like GIST, particularly when leveraging second-order information.
Strengths:
1. Theoretical Soundness: The paper establishes a strong theoretical foundation by proving convergence to Clarke critical points, a challenging task for non-convex optimization problems. This ensures the algorithm's reliability and robustness.
2. Efficiency: HONOR's hybrid scheme effectively incorporates second-order information without explicitly forming the inverse Hessian matrix, striking a balance between computational efficiency and convergence speed.
3. Empirical Validation: The experimental results are compelling, showcasing HONOR's significant performance gains over GIST on large-scale datasets. The sensitivity analysis of the parameter \( \epsilon \) further highlights the algorithm's adaptability.
4. Clarity and Organization: The paper is well-written and logically structured, making it accessible to both theoretical and applied researchers. The detailed convergence analysis and supplementary materials enhance reproducibility.
Weaknesses:
1. Initial Point Dependency: The algorithm's reliance on keeping iterates within the same orthant raises concerns about its sensitivity to the choice of the initial solution. While the authors use random initialization in experiments, a deeper discussion on how the initial point affects performance and convergence is warranted.
2. Memory Overhead: HONOR's use of second-order information, particularly the L-BFGS matrix, may lead to increased memory usage for highly non-convex or extremely large-scale problems. This trade-off between memory and speed is not sufficiently explored.
3. Approximation of the Hessian: The potential discrepancies between the L-BFGS approximation and the true Hessian in non-convex settings could impact the algorithm's performance. A more detailed analysis of this limitation would strengthen the paper.
Arguments for Acceptance:
- The paper addresses a critical and challenging problem in non-convex sparse learning, advancing the state of the art.
- Theoretical contributions, including convergence guarantees, are robust and non-trivial.
- Empirical results convincingly demonstrate the algorithm's practical utility.
Arguments Against Acceptance:
- Concerns about initial point dependency and its impact on convergence remain unresolved.
- The memory overhead associated with second-order information could limit scalability in some applications.
Recommendation:
Overall, the paper makes a significant contribution to the field of optimization for non-convex problems. While there are some concerns about initial point sensitivity and memory usage, these do not overshadow the algorithm's theoretical and empirical strengths. I recommend acceptance, with minor revisions to address the aforementioned weaknesses.