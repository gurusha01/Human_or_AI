The paper introduces the HONOR algorithm, a hybrid optimization method for solving non-convex sparse learning problems. By combining Quasi-Newton (QN) and Gradient Descent (GD) steps, HONOR addresses the challenges of non-convexity and non-smoothness in large-scale optimization. A key innovation is its use of L-BFGS to approximate second-order information without explicitly computing the Hessian, ensuring scalability. The authors provide a rigorous convergence analysis, proving that HONOR converges to a Clarke critical point, a significant result given the inherent difficulties of non-convex optimization. Empirical evaluations demonstrate HONOR's superior convergence speed compared to state-of-the-art methods on large-scale, high-dimensional datasets.
Strengths:
1. Technical Depth and Rigor: The paper provides a mathematically rigorous convergence analysis, which is both correct and extendable to generalized non-convex objectives. This is a notable contribution, as convergence guarantees for non-convex problems are challenging to establish.
2. Scalability: By leveraging L-BFGS, HONOR avoids the computational overhead of explicitly evaluating the Hessian, making it suitable for large-scale problems.
3. Empirical Validation: The experimental results convincingly demonstrate HONOR's faster convergence compared to existing methods, particularly on large datasets with millions of features.
4. Hybrid Approach: The combination of QN and GD steps is well-motivated and efficiently implemented, with a computationally inexpensive condition to switch between the two methods.
5. Clarity in Presentation: The paper is well-organized, with detailed explanations of the algorithm, theoretical analysis, and experimental setup.
Weaknesses:
1. Limited Diversity in Datasets: While the experiments are conducted on large-scale datasets, the paper does not explicitly test HONOR on datasets with known local minima. This would provide additional insights into its robustness in escaping poor solutions.
2. Parameter Tuning Guidance: The paper lacks detailed guidelines for selecting the parameter \( \epsilon \), which significantly impacts the ratio of QN to GD steps and, consequently, the algorithm's performance. Providing such guidance would enhance the algorithm's usability.
3. Comparison with Broader Methods: The empirical evaluation excludes other relevant algorithms like SparseNet and DC-PN, citing implementation challenges. Including these comparisons, even qualitatively, would strengthen the claims of superiority.
Arguments for Acceptance:
- The paper addresses a significant problem in non-convex sparse learning and provides a novel, well-validated solution.
- The theoretical contributions are robust and extendable, advancing the state of the art.
- The empirical results are compelling and demonstrate practical utility.
Arguments Against Acceptance:
- The lack of experiments on datasets with local minima limits the understanding of HONOR's robustness.
- The absence of parameter tuning guidelines may hinder adoption by practitioners.
Recommendation:
Overall, the paper is a strong contribution to the field of optimization and sparse learning. Its combination of theoretical rigor and practical efficiency makes it a valuable addition to the conference. I recommend acceptance, with minor revisions to address the weaknesses outlined above.