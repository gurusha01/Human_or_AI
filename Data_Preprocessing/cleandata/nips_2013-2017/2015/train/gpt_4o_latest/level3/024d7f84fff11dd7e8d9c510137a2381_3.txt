The paper addresses the problem of risk-sensitive reinforcement learning (RL) by extending policy gradient methods to the entire class of coherent risk measures, encompassing both static and dynamically consistent risk measures. This generalization builds upon prior work on specific risk measures like CVaR, offering a unified framework for risk-sensitive optimization in Markov decision processes (MDPs). The authors propose new gradient formulas for static and dynamic coherent risk measures, combining sampling techniques with convex programming for static measures and an actor-critic approach for dynamic measures. These contributions are significant as they provide a broader and more flexible framework for managing cost variability in RL, with potential applications in finance, operations research, and beyond.
Strengths
1. Technical Soundness: The paper is technically rigorous, and the results appear correct based on the reviewer's assessment. The derivation of gradient formulas for both static and dynamic coherent risk measures is well-supported by theoretical analysis, and the algorithms are grounded in established techniques like sampling and convex programming.
2. Clarity and Organization: The paper is well-written and easy to follow, with a logical progression from problem formulation to algorithm development and numerical illustration. The authors provide sufficient background on coherent risk measures and related work, making the paper accessible to readers familiar with RL and optimization.
3. Originality: The extension of policy gradient methods to the entire class of coherent risk measures is novel and represents a meaningful generalization of prior work. This flexibility allows decision-makers to tailor risk measures to specific problem contexts, which is a valuable contribution.
4. Significance: The results have practical importance, particularly in domains where managing risk is critical. The unified framework and algorithms could inspire further research and applications in risk-sensitive RL.
Weaknesses
1. Notation in Theorem 2.1: The notation in Theorem 2.1 is somewhat confusing and could benefit from alignment with established references, such as Theorem 6.6 in [26]. This would improve clarity and make the results more accessible to readers familiar with the literature.
2. Intractability in Section 3: The paper should clarify that the intractability of the problem arises from policy parameterization rather than the risk measure itself. This distinction is important for understanding the scope of the proposed methods.
3. Minor Errors: A typo was noted in Line 141 ("risk enevlop[e]"), which should be corrected.
4. Scope of Contribution: While the extension to coherent risk measures is valuable, the contribution is incremental in the sense that it builds on existing work on CVaR and other specific measures. The practical impact of the proposed methods would benefit from more extensive empirical validation.
Recommendation
Overall, this paper provides a minor but solid and important extension to the field of risk-sensitive RL. The theoretical contributions are sound, and the algorithms are well-motivated. However, addressing the weaknesses mentioned above would further strengthen the paper. I recommend acceptance, provided the authors revise the notation in Theorem 2.1, clarify the intractability issue in Section 3, and correct the noted typo.