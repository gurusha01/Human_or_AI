The paper proposes a novel adaptive learning rate scheme, Equilibrated Stochastic Gradient Descent (ESGD), to address optimization challenges caused by saddle points in deep neural network training. Saddle points, characterized by flat and ill-conditioned objective functions, can significantly slow down training. The authors introduce a preconditioning method using a diagonal matrix \( D \), which builds on insights from the Jacobi preconditioner and RMSProp. While the Jacobi preconditioner is shown to perform poorly in mixed curvature scenarios, the authors derive the theoretically optimal preconditioner \( D^E = |H|^{-1} \), which is computationally infeasible. ESGD is presented as an efficient approximation, requiring only two additional gradient calculations every ~20 iterations. Empirical results demonstrate that ESGD outperforms SGD, Jacobi preconditioning, and RMSProp on deep autoencoder benchmarks, particularly for the MNIST dataset.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous analysis of preconditioning in non-convex optimization, highlighting the limitations of the Jacobi preconditioner and deriving the optimal preconditioner for indefinite matrices. This theoretical grounding is a significant contribution to understanding adaptive learning rates.
2. Practical Algorithm: ESGD is computationally efficient, requiring minimal overhead compared to standard SGD. The authors demonstrate its scalability and robustness across challenging benchmarks.
3. Empirical Validation: The experimental results are compelling, showing that ESGD achieves faster convergence and lower training error compared to RMSProp and other baselines. The analysis of cosine similarity between preconditioners adds depth to the empirical findings.
4. Novel Insights: The observation that RMSProp's success may stem from its similarity to the equilibration matrix is intriguing and could inspire further research into adaptive learning rate methods.
Weaknesses:
1. Clarity of Presentation: While the paper is generally well-written, the notation is occasionally ambiguous. For instance, distinguishing between element-wise and matrix-wise operators (e.g., \( A{i,.} \), \( qi qj \) vs. \( q{j,i} \)) is unclear. Equations such as Eqn. 7 and Eqn. 10 require more explicit explanations.
2. Supplementary Material: Some steps in the supplementary material, such as the inequality transformation in Proposition 3, are insufficiently detailed, which may hinder reproducibility.
3. Limited Benchmarks: The evaluation is restricted to deep autoencoders on two datasets (MNIST and CURVES). While these are standard benchmarks, additional experiments on other architectures (e.g., convolutional or transformer models) would strengthen the paper's generalizability.
Pro and Con Arguments for Acceptance:
Pro:
- Strong theoretical foundation and practical contribution.
- Demonstrates clear empirical improvements over existing methods.
- Addresses a critical problem in deep learning optimization.
Con:
- Ambiguities in notation and supplementary material reduce clarity.
- Limited scope of empirical evaluation.
Recommendation:
Overall, this paper makes a significant contribution to the field of optimization in deep learning by introducing ESGD, a theoretically motivated and empirically validated adaptive learning rate scheme. Despite minor issues with clarity and scope, the paper is of high quality and relevance to the NIPS community. I recommend acceptance, with the suggestion that the authors clarify the ambiguous notation and expand the empirical evaluation in future work.