The paper introduces Rectified Factor Networks (RFNs), a novel unsupervised learning method designed to construct sparse, non-linear, high-dimensional representations of input data. The authors claim that RFNs outperform existing methods like autoencoders, RBMs, and PCA in terms of sparsity, reconstruction error, and capturing data covariance structures. Additionally, RFNs are shown to excel in detecting rare and small events, with applications in vision datasets and pharmaceutical gene expression data. The proposed learning algorithm is based on posterior regularization, ensuring non-negative and normalized posterior means, and is proven to converge and be correct. The authors provide experimental evidence of RFNs' superiority in benchmarks and real-world applications, and they make their implementation publicly available.
Strengths:
1. Novelty and Significance: The paper addresses a critical gap in unsupervised learning by focusing on sparse, non-negative, and non-linear representations, which are particularly useful for detecting rare and small events. This is a meaningful contribution to the field, especially for applications in bioinformatics and drug discovery.
2. Theoretical Rigor: The authors provide a solid theoretical foundation for their method, including proofs of convergence and correctness. This adds credibility to their claims and distinguishes the work from heuristic-based approaches.
3. Comprehensive Evaluation: RFNs are compared against a wide range of unsupervised methods across multiple benchmarks. The results convincingly demonstrate RFNs' advantages in sparsity, reconstruction error, and covariance approximation.
4. Practical Utility: The application of RFNs to pharmaceutical datasets highlights their real-world relevance. The discovery of rare gene modules that informed decision-making in drug discovery is particularly compelling.
5. Efficiency: The proposed algorithm is computationally efficient, with significant speedups over traditional quadratic solvers, making it scalable for large datasets.
Weaknesses:
1. Clarity: While the paper is technically sound, the dense mathematical exposition may be challenging for readers unfamiliar with posterior regularization or factor analysis. Simplifying or summarizing key equations could improve accessibility.
2. Limited Discussion of Limitations: The paper does not adequately discuss the potential limitations of RFNs, such as their reliance on hyperparameter tuning or sensitivity to initialization. Acknowledging these would provide a more balanced perspective.
3. Comparative Analysis: Although RFNs outperform existing methods, the paper could benefit from a deeper analysis of why competing methods (e.g., RBMs or autoencoders) fail in specific scenarios. This would strengthen the argument for RFNs.
4. Generalization to Other Domains: While the results in vision and bioinformatics are promising, the applicability of RFNs to other domains (e.g., natural language processing) is not explored.
Recommendation:
I recommend acceptance of this paper, as it presents a significant and well-supported contribution to unsupervised learning. The theoretical rigor, practical utility, and strong experimental results make it a valuable addition to the conference. However, I encourage the authors to improve the clarity of their presentation and include a discussion of limitations in future revisions. 
Pro Arguments:
- Novel and impactful method for sparse coding.
- Strong theoretical and experimental validation.
- Demonstrated real-world utility in drug discovery.
Con Arguments:
- Dense presentation may hinder accessibility.
- Limited discussion of limitations and broader applicability. 
Overall, the strengths of the paper far outweigh its weaknesses, making it a strong candidate for inclusion in the conference.