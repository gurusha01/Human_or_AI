The paper proposes a novel curriculum learning strategy, termed "Scheduled Sampling," to address the discrepancy between training and inference in sequence prediction tasks using Recurrent Neural Networks (RNNs). The authors identify the key issue: during training, models are conditioned on true previous tokens, while during inference, they rely on their own predictions, leading to error accumulation. Scheduled Sampling gradually transitions the training process from using true tokens to model-generated tokens, thereby aligning training with inference. The approach is validated through experiments on three tasks: image captioning, constituency parsing, and speech recognition, demonstrating significant performance improvements.
Strengths:
1. Significance and Practical Impact: The paper addresses a critical problem in sequence prediction, with applications in machine translation, image captioning, and speech recognition. The proposed solution is simple yet effective, making it accessible for practitioners and researchers alike.
2. Novelty: The Scheduled Sampling approach is a novel application of curriculum learning to sequence prediction tasks, offering a principled way to mitigate the training-inference mismatch.
3. Empirical Validation: The method is rigorously evaluated on diverse tasks, including the MSCOCO image captioning challenge, where it achieved state-of-the-art results. The inclusion of ablation studies (e.g., comparing different sampling strategies) strengthens the claims.
4. Clarity: The paper is well-organized and provides sufficient technical details, including equations, training schedules, and experimental setups, allowing for reproducibility.
5. Acknowledgment of Limitations: The authors acknowledge that back-propagating through sampling decisions was not implemented and suggest it as future work, demonstrating an honest evaluation of their approach.
Weaknesses:
1. Theoretical Analysis: While the empirical results are strong, the paper lacks a deeper theoretical analysis of why Scheduled Sampling works and under what conditions it might fail. For example, the impact of different decay schedules on convergence is not fully explored.
2. Limited Scope of Tasks: Although the tasks are diverse, the experiments are limited to relatively well-studied domains. Testing on more challenging or less conventional sequence prediction tasks (e.g., dialogue generation or reinforcement learning) could further validate the generalizability of the approach.
3. Baseline Comparisons: While the paper compares Scheduled Sampling to a few baselines, it does not benchmark against other recent methods addressing the same problem, such as SEARN or beam search with ranking losses.
4. Interpretability of Results: The paper does not provide qualitative examples (e.g., generated captions or parse trees) to illustrate the improvements, which would help readers better understand the practical impact.
Pro and Con Arguments for Acceptance:
Pros:
- Addresses a critical problem in sequence prediction with a novel and practical solution.
- Demonstrates strong empirical results across multiple tasks.
- Well-written and sufficiently detailed for reproducibility.
Cons:
- Lacks theoretical insights and broader task coverage.
- Limited comparisons with alternative methods.
Recommendation:
I recommend acceptance of this paper, as it provides a significant contribution to the field of sequence prediction with a novel and empirically validated approach. However, future iterations of this work should include theoretical analysis and broader task evaluations to strengthen its impact further.