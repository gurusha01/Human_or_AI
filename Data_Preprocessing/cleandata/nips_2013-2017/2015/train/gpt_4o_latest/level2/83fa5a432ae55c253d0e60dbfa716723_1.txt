The paper introduces Principal Differences Analysis (PDA) and its sparse variant (SPARDA) as novel methods for analyzing differences between high-dimensional distributions. The core contribution is the use of the Wasserstein divergence to identify a projection that maximizes the difference between two distributions, leveraging the Cramer-Wold device to reduce the problem to a univariate setting. The sparse variant, SPARDA, identifies features responsible for these differences, making it particularly useful for exploratory analysis in high-dimensional data. The authors provide algorithms for both the minimax formulation and a semidefinite relaxation, along with theoretical guarantees and empirical demonstrations, including applications to single-cell RNA-seq data.
Strengths:
1. Novelty and Relevance: The paper addresses a significant challenge in high-dimensional data analysisâ€”comparing distributions without restrictive parametric assumptions. The use of Wasserstein divergence and the incorporation of sparsity are innovative and relevant to fields like genomics and biomedical data analysis.
2. Theoretical Contributions: The authors derive convergence results and provide statistical guarantees for the empirical divergence-maximizing projection. The sparsistency guarantees for SPARDA are particularly noteworthy.
3. Algorithmic Development: The RELAX algorithm and the tightening procedure are well-motivated and demonstrate practical utility. The semidefinite relaxation is a clever approach to handle the non-convexity of the problem.
4. Empirical Validation: The experiments convincingly demonstrate the advantages of SPARDA over existing methods like sparse PCA, LDA, and logistic lasso. The application to single-cell RNA-seq data highlights the method's practical utility and interpretability.
5. Clarity of Presentation: The paper is well-organized, with clear explanations of the methodology, theoretical results, and experimental setup.
Weaknesses:
1. Computational Complexity: While the RELAX algorithm is well-designed, the computational cost of solving semidefinite relaxations and performing iterative tightening may become prohibitive for very large datasets. The paper could benefit from a more detailed discussion of scalability and potential optimizations.
2. Limited Comparison to Related Work: Although the authors compare SPARDA to several baseline methods, the discussion of related work, particularly in the context of Wasserstein-based methods, could be expanded. For instance, recent advances in optimal transport and its applications to high-dimensional data could provide additional context.
3. Practical Guidance: The choice of hyperparameters (e.g., sparsity level, regularization) is not thoroughly discussed. While cross-validation is mentioned, providing more concrete guidelines or heuristics would enhance the method's accessibility to practitioners.
4. Generalization Beyond Wasserstein Divergence: While the authors mention that PDA can be extended to other divergences, the paper focuses exclusively on the Wasserstein distance. A brief empirical comparison with other divergences would strengthen the claim of generalizability.
Arguments for Acceptance:
- The paper makes a significant contribution to high-dimensional data analysis by introducing a novel framework with strong theoretical underpinnings and practical utility.
- The empirical results are compelling, demonstrating clear advantages over existing methods in both synthetic and real-world datasets.
- The method addresses a broad range of applications, from feature selection to exploratory data analysis, making it highly relevant to the NIPS audience.
Arguments Against Acceptance:
- The computational complexity of the proposed algorithms may limit their applicability to very large datasets.
- The discussion of related work and practical implementation details could be more comprehensive.
Recommendation:
I recommend acceptance of this paper, as it presents a novel and impactful contribution to the field of high-dimensional data analysis. While there are areas for improvement, the strengths of the paper far outweigh its weaknesses. Addressing the scalability and practical implementation concerns in future work would further enhance its impact.