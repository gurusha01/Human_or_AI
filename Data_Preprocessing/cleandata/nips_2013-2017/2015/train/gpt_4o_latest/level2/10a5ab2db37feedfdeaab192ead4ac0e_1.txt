The paper presents a novel approach to bridging the gap between backpropagation-based deep learning and neuromorphic hardware by introducing a probabilistic training methodology that maps directly to spiking neural networks with low-precision synapses. The authors propose a "constrain-then-train" paradigm, where the network is first constrained to match the hardware's limitations and then trained using backpropagation. The method is validated on the MNIST dataset using IBM's TrueNorth chip, achieving state-of-the-art accuracy (99.42%) for spiking neural networks and unprecedented energy efficiency (0.268 ÂµJ per classification).
Strengths:
1. Novelty and Contribution: The paper introduces a significant innovation by reconciling backpropagation with neuromorphic constraints, such as spiking neurons and low-precision synapses. This is a clear advancement over prior work, which addressed subsets of these constraints but not their combination.
2. Experimental Validation: The results are robust and impressive, with the method achieving both high accuracy and energy efficiency. The comparison with prior work is thorough, demonstrating clear improvements.
3. Practical Relevance: The approach is highly relevant for real-world applications, particularly in embedded systems where energy efficiency is critical. The use of TrueNorth as a deployment platform highlights the method's practicality.
4. Clarity of Presentation: The paper is well-organized and provides sufficient detail about the methodology, training process, and hardware mapping, enabling reproducibility.
5. Broader Impact: The discussion section outlines potential extensions to more complex datasets and architectures, such as convolutional networks, making the work broadly applicable.
Weaknesses:
1. Limited Dataset: While MNIST is a standard benchmark, it is relatively simple compared to real-world datasets. Demonstrating the method on more complex datasets (e.g., CIFAR-10 or ImageNet) would strengthen the paper's claims of scalability and generalizability.
2. Hardware-Specific Focus: The approach is tailored to TrueNorth, which may limit its applicability to other neuromorphic platforms. While the authors suggest generalizability, no experiments are provided to support this claim.
3. Sparse Discussion of Limitations: The paper does not sufficiently discuss the trade-offs of the "constrain-then-train" approach compared to "train-then-constrain," particularly in terms of training complexity and convergence behavior.
4. Ensemble Dependence: The highest accuracy is achieved using a 64-member ensemble, which may not be practical for all applications. The paper could explore ways to improve single-network performance.
Suggestions for Improvement:
1. Extend the experiments to more complex datasets to demonstrate scalability.
2. Provide empirical evidence of the method's applicability to other neuromorphic hardware platforms.
3. Include a more detailed discussion of the limitations of the "constrain-then-train" approach.
4. Investigate methods to improve single-network performance without relying on large ensembles.
Recommendation:
This paper makes a strong contribution to the field of neuromorphic computing and is highly relevant to the NIPS community. While there are some limitations, the novelty, practical relevance, and experimental results outweigh these concerns. I recommend acceptance, with minor revisions to address the weaknesses noted above.