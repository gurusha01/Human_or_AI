The paper presents a comprehensive analysis of several variants of the Frank-Wolfe (FW) optimization algorithm, including away-steps FW (AFW), pairwise FW (PFW), fully-corrective FW (FCFW), and Wolfe's minimum norm point (MNP) algorithm. The authors claim to provide the first global linear convergence guarantees for these variants under a weaker condition than strong convexity, introducing a novel geometric quantity related to the constraint set's condition number. This work is positioned as a significant improvement over prior results, which were limited to specific cases or stronger assumptions.
Strengths:
1. Novelty and Contribution: The paper makes a strong contribution by proving linear convergence for FW variants under less restrictive conditions. The introduction of the geometric condition number of the constraint set is particularly innovative and could have broader implications for complexity theory.
2. Clarity of Variants: The authors provide a clear exposition of the FW variants, including detailed algorithms and their practical implications. This clarity is valuable for both theoretical understanding and practical implementation.
3. Comprehensive Analysis: The paper rigorously compares the FW variants, offering insights into their relative performance and convergence properties. The inclusion of theoretical bounds, such as the pyramidal width, adds depth to the analysis.
4. Practical Relevance: The authors demonstrate the utility of these algorithms in real-world applications, such as sparse optimization and video co-localization, supported by empirical experiments that validate their theoretical claims.
5. Broader Impact: The results could influence other areas of optimization, particularly in domains where FW algorithms are commonly used, such as submodular optimization and machine learning.
Weaknesses:
1. Experimental Scope: While the experiments demonstrate linear convergence, they are limited in scope. Additional benchmarks on diverse datasets and problem types would strengthen the empirical validation.
2. Complexity of Analysis: The theoretical proofs, while rigorous, are dense and may be challenging for readers unfamiliar with advanced convex optimization concepts. Simplifying or summarizing key results could improve accessibility.
3. Practical Feasibility: The paper does not sufficiently address the computational overhead of some variants, such as FCFW, which involve more complex correction steps. A discussion on trade-offs between theoretical guarantees and practical runtime would be helpful.
4. Comparison with Related Work: Although the paper references related work, it could provide a more detailed comparison with recent advancements, particularly in accelerated methods for FW algorithms.
Recommendation:
This paper makes a significant theoretical contribution to the field of optimization and is well-suited for the NIPS audience. Its results are both novel and impactful, addressing a long-standing question about the convergence of FW variants. However, the authors should consider expanding the experimental section and providing a more detailed discussion of practical trade-offs. Overall, I recommend acceptance, with minor revisions to improve clarity and broaden the empirical evaluation.
Arguments for Acceptance:
- Strong theoretical contributions with novel insights.
- Clear exposition of FW variants and their convergence properties.
- Practical relevance demonstrated through experiments.
Arguments Against Acceptance:
- Limited experimental scope.
- Dense theoretical presentation may hinder accessibility.
Final Rating: 8/10 (Accept with minor revisions).