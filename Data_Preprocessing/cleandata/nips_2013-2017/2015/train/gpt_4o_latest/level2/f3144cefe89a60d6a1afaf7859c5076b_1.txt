The paper introduces the Poisson Gamma Belief Network (PGBN), a novel deep generative model designed for high-dimensional count data. It extends traditional deep networks by employing gamma-distributed nonnegative real hidden units instead of binary units, which allows for better modeling of overdispersed counts and correlations across layers. The authors propose an upward-downward Gibbs sampler for efficient joint training of all layers and a layer-wise training strategy to infer the network structure, including the widths of hidden layers. The PGBN is shown to outperform Poisson Factor Analysis (PFA) and other shallow models in tasks such as text classification and held-out word prediction, demonstrating the benefits of deeper architectures.
Strengths:
1. Novelty and Contribution: The PGBN introduces a significant innovation by replacing binary hidden units with gamma-distributed ones, addressing limitations in traditional deep networks for count data. The integration of the gamma-negative binomial process for layer-wise structure inference is particularly noteworthy.
2. Technical Soundness: The paper is technically rigorous, with detailed derivations of the Gibbs sampler and theoretical insights into the model's properties. The augmentation techniques and propagation of latent counts are well-justified.
3. Experimental Validation: The experiments, particularly on the 20 Newsgroups dataset, demonstrate the practical utility of PGBN. The results show clear improvements in classification accuracy and perplexity as the network depth increases, validating the model's claims.
4. Scalability: The proposed layer-wise training strategy is computationally efficient and allows for automatic inference of network structure, making the model scalable to large datasets.
5. Clarity of Results: The paper provides comprehensive quantitative and qualitative analyses, including insights into inferred network structures and synthetic document generation, which enhance interpretability.
Weaknesses:
1. Clarity of Presentation: While the technical content is thorough, the paper is dense and challenging to follow, especially for readers unfamiliar with gamma processes or deep generative models. Simplifying the exposition or providing more intuitive explanations could improve accessibility.
2. Comparison to Related Work: Although the paper briefly mentions related models like deep exponential families and deep PFA, a more detailed comparison, both conceptually and experimentally, would strengthen the claims of novelty and superiority.
3. Limited Scope of Experiments: The experiments focus primarily on text data. Evaluating PGBN on other types of count data (e.g., biological or network data) would demonstrate its broader applicability.
4. Hyperparameter Sensitivity: The paper does not discuss the sensitivity of the model to hyperparameters like the upper bound on the first-layer width (K1 max) or the number of layers (Tmax). This could be a potential limitation in practical applications.
Recommendation:
The paper makes a strong case for the PGBN as a significant advancement in deep generative modeling for count data. Its technical rigor, novelty, and experimental results justify its acceptance. However, the authors should address the clarity of presentation and provide more detailed comparisons to related work in the final version.
Arguments for Acceptance:
- Novel and technically sound approach with clear improvements over existing methods.
- Strong experimental results demonstrating practical utility.
- Efficient training strategy suitable for large-scale applications.
Arguments Against Acceptance:
- Dense and complex presentation may hinder understanding for a broader audience.
- Limited evaluation on non-textual count data.
Overall, I recommend acceptance, with minor revisions to improve clarity and broaden the scope of experiments.