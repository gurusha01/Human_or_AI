This paper presents a significant extension to risk-sensitive reinforcement learning (RL) by developing policy gradient algorithms for the entire class of coherent risk measures, encompassing both static and dynamic settings. The authors propose a unified framework that generalizes and simplifies existing approaches, such as those targeting specific risk measures like Conditional Value at Risk (CVaR) or mean-variance models. The main contributions include a novel gradient formula for static coherent risk, an actor-critic algorithm for dynamic Markov coherent risk, and a sampling-based approach for gradient estimation. These contributions are well-supported by theoretical derivations, numerical illustrations, and connections to prior work.
Strengths:
1. Novelty and Generalization: The paper's focus on coherent risk measures is a notable advancement, as it unifies and extends prior work on specific risk functions. The generality of the approach is a major strength, enabling its application to a wide range of RL problems.
2. Technical Rigor: The derivation of the gradient formulas for both static and dynamic risk measures is thorough and mathematically sound. The use of convex programming and sampling-based methods is well-justified and aligns with the state-of-the-art in policy gradient methods.
3. Practical Relevance: The proposed algorithms address real-world challenges, such as handling large state spaces and incorporating risk sensitivity into decision-making. The numerical example involving financial decision-making effectively demonstrates the utility of the approach.
4. Clarity of Contributions: The paper clearly identifies its contributions, such as the new policy gradient theorem for Markov coherent risk and the sampling-based gradient estimation algorithm. The connections to prior work are well-documented, showing how this work builds on and generalizes existing methods.
Weaknesses:
1. Limited Empirical Validation: While the numerical example is illustrative, the paper would benefit from more extensive empirical evaluations across diverse domains. For instance, testing the algorithms on large-scale RL benchmarks or real-world applications could strengthen the claims of scalability and practical utility.
2. Complexity of Implementation: The reliance on convex programming and saddle-point computations may pose challenges for practitioners. The paper could provide more guidance or benchmarks on the computational overhead of the proposed methods.
3. Discussion of Limitations: Although the authors acknowledge the need for a simulator and the challenges of single-trajectory rollouts, the discussion of limitations is relatively brief. A deeper exploration of the trade-offs between flexibility and computational complexity would be valuable.
4. Risk-Shaping Potential: While the authors highlight the potential of risk shaping for addressing model uncertainty, this concept remains underexplored. A concrete example or preliminary results in this direction would enhance the paper's impact.
Recommendation:
Overall, this paper makes a strong theoretical contribution to risk-sensitive RL by extending policy gradient methods to coherent risk measures. The work is technically sound, original, and relevant to the field. However, the limited empirical validation and practical implementation challenges slightly detract from its immediate applicability. I recommend acceptance with minor revisions, with suggestions to expand the empirical evaluation and provide more practical insights into the computational aspects of the proposed algorithms.
Arguments for Acceptance:
- The paper advances the state of the art in risk-sensitive RL by addressing a broad class of risk measures.
- The theoretical contributions are well-founded and have the potential to inspire future research in risk-aware decision-making.
- The numerical example, though limited, effectively demonstrates the importance of flexibility in risk criteria.
Arguments Against Acceptance:
- The empirical validation is limited, and the practical scalability of the methods is not fully demonstrated.
- The computational complexity of the algorithms may hinder their adoption by practitioners without additional guidance.
This paper is a valuable contribution to the field and aligns well with the conference's focus on advancing RL methodologies.