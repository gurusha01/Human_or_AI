The paper presents improved algorithms for computing Isotonic Regression on directed acyclic graphs (DAGs) under weighted `p-norms, with rigorous performance guarantees. The authors claim advancements in computational efficiency, particularly for general DAGs, and provide new algorithms for `∞-norm and Strict Isotonic Regression. The work is positioned as a significant improvement over prior results, with theoretical contributions and practical implementations.
Strengths:
1. Technical Depth and Novelty: The paper introduces a unified optimization-based framework for Isotonic Regression under `p-norms, extending existing Interior Point Methods (IPMs) to handle `p-objectives. This is a novel contribution, particularly in its use of approximate solvers for linear systems, which reduces computational overhead.
2. Improved Complexity: The proposed algorithms achieve better time complexity compared to prior work. For example, the `p-norm algorithm improves upon the previous best bounds for general DAGs, and the `∞-norm algorithm achieves linear time complexity, which was not known before.
3. Practical Implementation: The authors provide a practical implementation of their `2-norm algorithm, demonstrating its efficiency on real-world DAGs. The inclusion of experimental results and a GitHub repository for reproducibility adds significant value.
4. Comprehensive Comparison: The paper thoroughly compares its results with prior work, highlighting improvements in both theoretical bounds and practical applicability. The detailed discussion of related work ensures clarity on the novelty of the contributions.
5. Broader Impact: The techniques introduced, such as the IPM framework and fast solvers for new matrix classes, have potential applications beyond Isotonic Regression, including learning Generalized Linear Models and constructing Class Probability Estimation models.
Weaknesses:
1. Clarity: While the technical content is robust, the paper is dense and could benefit from clearer explanations of key concepts, particularly for readers less familiar with IPMs or Isotonic Regression. For example, the reduction to Lipschitz learning could be explained more intuitively.
2. Experimental Scope: The experiments focus primarily on `2-norm Isotonic Regression. While the results are promising, additional experiments for other norms (`p and `∞) would strengthen the practical validation of the proposed algorithms.
3. Limitations Discussion: The paper does not explicitly discuss the limitations of the proposed methods, such as potential scalability issues for extremely large graphs or the impact of approximate solvers on solution accuracy in edge cases.
4. Accessibility: The reliance on advanced mathematical concepts (e.g., self-concordant barriers, lexicographic ordering) may limit accessibility to a broader audience. Simplified explanations or an extended appendix with examples could address this.
Recommendation:
The paper makes a strong theoretical and practical contribution to the field of optimization and machine learning. It advances the state of the art in Isotonic Regression and provides tools that can be applied to other problems. However, the clarity of presentation and experimental breadth could be improved. I recommend acceptance with minor revisions to address these concerns.
Arguments for Acceptance:
- Significant theoretical advancements and improved complexity bounds.
- Practical implementation and reproducibility.
- Potential for broader impact in related fields.
Arguments Against Acceptance:
- Limited experimental validation for norms other than `2.
- Dense presentation that may hinder accessibility for a wider audience.
Overall, this is a high-quality submission with substantial contributions to the field.