The paper introduces Stochastic Expectation Propagation (SEP), a novel extension to Expectation Propagation (EP) aimed at addressing EP's prohibitive memory requirements in large-scale Bayesian learning problems. SEP combines the local update advantages of EP with the global posterior approximation of Variational Inference (VI), reducing memory overhead from \(O(ND^2)\) to \(O(D^2)\), where \(N\) is the number of data points and \(D\) is the parameter dimensionality. The authors demonstrate that SEP achieves comparable accuracy to EP while being significantly more scalable, making it well-suited for large datasets and complex models.
Strengths:
1. Clear Contribution: The paper identifies a critical limitation of EP—its memory inefficiency—and proposes SEP as a well-motivated solution. The authors provide theoretical insights and practical extensions, such as parallel SEP and distributed SEP (DSEP), which enhance its applicability.
2. Empirical Validation: The experiments are comprehensive, covering synthetic and real-world datasets, and include diverse models like Bayesian probit regression, mixture of Gaussians, and probabilistic backpropagation. SEP consistently performs on par with EP while significantly reducing memory usage, validating its utility.
3. Theoretical Connections: The paper establishes meaningful connections between SEP and other methods like VI, stochastic VI (SVI), and averaged EP (AEP), situating SEP within a broader context of approximation algorithms. This theoretical framing enhances the paper's significance.
4. Practical Relevance: The reduction in memory footprint is substantial, enabling SEP to handle large-scale datasets (e.g., MNIST, Protein, and Year datasets) that would be infeasible with EP. This makes SEP highly relevant for practitioners in machine learning and Bayesian inference.
Weaknesses:
1. Limited Theoretical Guarantees: While the paper provides some theoretical insights, the convergence properties of SEP remain underexplored. For example, it is unclear under what conditions SEP converges to the same fixed points as EP or how robust it is to hyperparameter choices like damping rates.
2. Granularity Trade-offs: The discussion on the granularity of approximations (e.g., DSEP) is promising but underdeveloped. The experiments suggest potential benefits in heterogeneous datasets, but the paper lacks a systematic exploration of when and how to partition data effectively.
3. Clarity and Accessibility: While the paper is technically sound, the dense presentation of algorithms and theoretical connections may be challenging for readers unfamiliar with EP or VI. Simplified explanations or visual aids could improve accessibility.
4. Scalability of Computation: Although SEP reduces memory requirements, the computational cost of moment matching and updates for large models (e.g., neural networks) is not thoroughly analyzed. A deeper discussion of runtime trade-offs would strengthen the paper.
Recommendation:
I recommend acceptance of this paper. Its contributions are both novel and practically significant, addressing a well-known limitation of EP while maintaining accuracy. The empirical results are convincing, and the method is likely to have a broad impact on scalable Bayesian learning. However, further work on theoretical guarantees and practical guidelines for data partitioning would enhance the method's robustness and usability.
Arguments for Acceptance:
- Novel and well-motivated contribution addressing a critical limitation of EP.
- Comprehensive empirical validation across diverse datasets and models.
- Significant practical relevance due to reduced memory requirements.
Arguments Against Acceptance:
- Limited theoretical analysis of convergence and robustness.
- Dense presentation may hinder accessibility for a broader audience.
In summary, the paper presents a strong contribution to scalable Bayesian inference and is a valuable addition to the field.