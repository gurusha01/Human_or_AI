The paper introduces a novel adaptive learning rate method, ESGD, based on the equilibration preconditioner, aimed at addressing the challenges of non-convex optimization in deep learning. The authors argue that saddle points, rather than local minima, are the primary obstacles in training deep networks, and propose that preconditioning methods tailored to handle negative curvature can improve convergence. The paper provides theoretical justifications for the equilibration preconditioner, demonstrates its advantages over the Jacobi preconditioner, and empirically validates ESGD's performance on deep autoencoder benchmarks, showing it outperforms RMSProp and plain SGD.
Strengths:
1. Novelty and Theoretical Contributions: The paper provides a fresh perspective on adaptive learning rates by leveraging the equilibration preconditioner, which is relatively underexplored in the context of deep learning. The theoretical analysis linking equilibration to the absolute Hessian and its robustness in handling indefinite matrices is compelling.
2. Empirical Validation: The experiments on MNIST and CURVES datasets convincingly demonstrate ESGD's superior performance, particularly in terms of convergence speed and final training error. The comparison with RMSProp and Jacobi SGD is thorough and highlights the practical utility of the proposed method.
3. Clarity of Motivation: The paper effectively motivates the need for better preconditioners in non-convex optimization, grounding its claims in recent literature on saddle points and ill-conditioning in deep networks.
4. Insight into RMSProp: The observation that RMSProp's success may stem from its similarity to the equilibration preconditioner is an interesting contribution that could inspire further research.
Weaknesses:
1. Limited Scope of Experiments: While the benchmarks used are standard, the evaluation is restricted to autoencoders on two datasets. Broader experimentation on diverse architectures (e.g., convolutional or transformer models) and tasks (e.g., classification or reinforcement learning) would strengthen the generalizability of the results.
2. Reproducibility Concerns: Although the paper provides a detailed algorithm and implementation notes, key hyperparameters (e.g., the frequency of equilibration updates) are not extensively analyzed. A more comprehensive ablation study would enhance reproducibility.
3. Comparison with State-of-the-Art: The paper does not compare ESGD with other recent adaptive optimizers like Adam or AdaDelta, which are widely used in practice. This omission limits the impact of the findings.
4. Acknowledgment of Limitations: The paper does not explicitly discuss the computational overhead of ESGD compared to simpler methods like RMSProp, which could be a concern for large-scale applications.
Pro Acceptance Arguments:
- The paper introduces a novel and theoretically grounded method for adaptive learning rates.
- ESGD demonstrates clear empirical improvements over baseline methods.
- The work contributes to a deeper understanding of preconditioning in non-convex optimization.
Con Acceptance Arguments:
- The experimental scope is narrow, limiting the applicability of the findings.
- The paper lacks comparisons with more recent optimizers and does not fully explore computational trade-offs.
Recommendation: Accept with minor revisions. The paper makes a meaningful contribution to the field of optimization in deep learning, but broader experimentation and a more detailed discussion of limitations would enhance its impact.