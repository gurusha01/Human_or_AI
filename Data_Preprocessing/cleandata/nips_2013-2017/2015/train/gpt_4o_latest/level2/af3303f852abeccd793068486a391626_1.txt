The paper addresses the problem of Bayesian parameter estimation for deep neural networks (DNNs), focusing on scenarios with limited data or the need for accurate posterior predictive densities. The authors propose a novel method that combines online Monte Carlo sampling (via stochastic gradient Langevin dynamics, SGLD) with model distillation to create a single, compact neural network that approximates the posterior predictive distribution. This approach, termed "Bayesian dark knowledge," aims to overcome the computational and memory inefficiencies of traditional Bayesian methods like expectation propagation (EP) and variational Bayes (VB), while retaining the benefits of Bayesian inference.
Strengths
1. Novelty and Practicality: The proposed method is innovative in combining SGLD with distillation to achieve Bayesian inference with reduced computational and memory costs. This is particularly useful for real-world applications where resources are constrained.
2. Empirical Validation: The paper provides extensive experimental results across diverse tasks, including toy classification and regression problems, MNIST digit classification, and the Boston housing dataset. The results demonstrate that the proposed method achieves comparable or superior performance to EP, VB, and even Hamiltonian Monte Carlo (HMC), which is considered the gold standard.
3. Efficiency Gains: By distilling the posterior predictive distribution into a single neural network, the method significantly reduces storage and test-time computational costs compared to SGLD, without substantial loss in accuracy.
4. Clarity of Results: The paper includes detailed comparisons of log-likelihood scores, error rates, and visualizations of predictive distributions, which effectively illustrate the advantages of the proposed approach.
5. Acknowledgment of Limitations: The authors acknowledge potential biases introduced by SGLD and propose future directions to address these, such as using exact MCMC methods or improving data generation for the student network.
Weaknesses
1. Limited Scope of Applications: While the method is promising, the paper does not demonstrate its utility in end-to-end tasks where predictive uncertainty is critical, such as active learning or contextual bandits. This limits the practical impact of the work.
2. Comparative Analysis: Although the paper compares its method to EP, VB, and HMC, some comparisons rely on previously published results rather than direct experiments. This weakens the rigor of the evaluation.
3. Reproducibility Concerns: The paper lacks sufficient details on hyperparameter tuning and experimental setups, which may hinder reproducibility. For example, the choice of data perturbations for training the student network is not fully justified or explored.
4. Scalability: While the method is shown to work on moderately sized datasets like MNIST, its scalability to very large datasets or models with billions of parameters remains untested.
Recommendation
I recommend acceptance of this paper, as it presents a novel and practically useful approach to Bayesian inference for DNNs, with strong empirical results and significant efficiency gains. However, the authors should address the limitations by (1) demonstrating the method's utility in real-world tasks requiring predictive uncertainty, (2) providing more detailed experimental setups for reproducibility, and (3) discussing scalability to larger datasets and models. These additions would strengthen the paper's impact and applicability.