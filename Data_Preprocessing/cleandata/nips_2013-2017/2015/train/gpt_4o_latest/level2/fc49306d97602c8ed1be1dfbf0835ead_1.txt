The paper presents a kernel-based method for cross-domain instance matching, addressing the challenge of finding correspondences between instances in different domains, such as multilingual documents or images and tags. The authors propose embedding features from both domains into a shared latent space, representing instances as distributions of these features. The framework of kernel embeddings of distributions is employed to measure differences between these distributions efficiently and nonparametrically. The method is evaluated on tasks involving multilingual Wikipedia articles, document-tag matching, and image-tag matching, demonstrating superior performance compared to existing methods like CCA, kernel CCA, and bilingual topic models.
Strengths:
1. Novelty and Innovation: The proposed method introduces a novel approach by combining kernel embeddings of distributions with a shared latent space representation. This allows for capturing relationships between features across domains, addressing limitations of existing methods like kernel CCA, which struggle with non-linear relationships and semantically similar but distinct features.
2. Empirical Validation: The paper provides extensive experimental results across diverse datasets (e.g., multilingual Wikipedia, social bookmarking, and image-tag datasets). The proposed method consistently outperforms baselines, demonstrating its robustness and generalizability.
3. Practical Relevance: The method addresses real-world problems in natural language processing, information retrieval, and computer vision, such as cross-lingual document alignment and image annotation. Its ability to handle bag-of-words representations makes it broadly applicable.
4. Theoretical Soundness: The use of kernel embeddings of distributions in a reproducing kernel Hilbert space (RKHS) is well-motivated, and the mathematical formulation is rigorous. The optimization process is clearly described, with gradients derived for efficient learning.
Weaknesses:
1. Clarity: While the paper is technically sound, it is dense and could benefit from clearer explanations, particularly for readers unfamiliar with kernel embeddings or RKHS. For instance, the intuition behind the kernel embedding framework and its advantages over traditional methods could be elaborated further.
2. Scalability: The computational complexity of the method, especially for large datasets with high-dimensional features, is not discussed in detail. The reliance on gradient-based optimization and pairwise comparisons may pose challenges for scalability.
3. Comparison Methods: While the paper compares against several baselines, it does not include recent deep learning-based approaches for cross-domain matching, which could provide a more comprehensive evaluation.
4. Limitations: The paper does not explicitly discuss the limitations of the proposed method, such as potential sensitivity to hyperparameters (e.g., dimensionality of the latent space, kernel parameters) or challenges in handling noisy or sparse data.
Recommendation:
The paper makes a significant contribution to the field of cross-domain matching and is well-suited for the conference. However, the authors should consider improving the clarity of the presentation, particularly for non-expert readers, and addressing scalability concerns. Including comparisons with deep learning-based methods would further strengthen the evaluation. Overall, I recommend acceptance with minor revisions.
Pro and Con Arguments:
Pros:
- Novel and theoretically sound approach.
- Strong empirical results across diverse datasets.
- Addresses practical and impactful problems in multiple domains.
Cons:
- Dense presentation, requiring clearer explanations.
- Limited discussion on scalability and computational efficiency.
- Lack of comparison with recent deep learning methods.
Final Score: 7/10 (Accept with Minor Revisions)