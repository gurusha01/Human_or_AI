The paper presents HONOR, a novel optimization algorithm for solving non-convex regularized sparse learning problems. The authors propose a hybrid scheme that combines Quasi-Newton (QN) and Gradient Descent (GD) steps to address the challenges of non-convexity and non-smoothness in large-scale data. The main contributions include: (1) an efficient use of second-order information without explicitly forming the inverse Hessian matrix, (2) a rigorous convergence analysis guaranteeing Clarke critical point convergence, and (3) empirical validation demonstrating significant speed improvements over state-of-the-art algorithms.
Strengths:
1. Technical Soundness: The paper is well-grounded in theory, offering a rigorous convergence analysis for non-convex problems, which is notably challenging. The use of Clarke subdifferentials to handle non-convexity is appropriate and well-explained.
2. Practical Significance: HONOR demonstrates substantial performance gains over existing methods like GIST in empirical evaluations on large-scale datasets. The hybrid optimization scheme effectively balances computational efficiency and convergence guarantees.
3. Novelty: The hybrid QN-GD approach is innovative, particularly in leveraging second-order information while avoiding computationally expensive operations like solving regularized quadratic subproblems.
4. Clarity: The paper is generally well-written, with clear explanations of the algorithm, theoretical foundations, and experimental setup. The inclusion of detailed assumptions and proofs enhances reproducibility and transparency.
5. Comprehensive Evaluation: The experiments are conducted on large-scale, high-dimensional datasets with multiple non-convex regularizers, providing robust evidence of HONOR's effectiveness. The sensitivity analysis on the parameter ϵ is insightful.
Weaknesses:
1. Limited Scope of Comparison: While the authors compare HONOR against GIST, other relevant methods like SparseNet and DC-PN are excluded due to implementation challenges or prior performance issues. This limits the breadth of the evaluation.
2. Computational Overhead: The paper acknowledges that HONOR's performance degrades when GD steps dominate, particularly due to additional computational overhead. This could be a limitation in scenarios where second-order information is less effective.
3. Practical Usability: The algorithm's reliance on parameter tuning (e.g., ϵ) may hinder its adoption in real-world applications where such tuning is non-trivial.
4. Parallelization: While the authors mention plans for parallel/distributed variants, the current algorithm is not optimized for very large-scale datasets that require distributed computation.
Pro and Con Arguments for Acceptance:
Pros:
- Rigorous theoretical contributions addressing a challenging problem in non-convex optimization.
- Significant empirical improvements over state-of-the-art methods.
- Novel hybrid optimization approach with potential for broader applicability.
Cons:
- Limited comparison with other non-convex solvers.
- Computational overhead in GD-dominated scenarios.
- Lack of immediate scalability to distributed settings.
Suggestions for Improvement:
1. Include comparisons with additional non-convex solvers, even if limited to specific problem settings, to strengthen the empirical evaluation.
2. Provide more practical guidance on parameter selection, particularly for ϵ, to enhance usability.
3. Explore strategies to reduce computational overhead in GD steps, such as adaptive switching criteria or lightweight approximations.
Recommendation:
The paper makes a substantial contribution to the field of non-convex optimization and sparse learning, with strong theoretical and empirical results. While there are some limitations in scope and scalability, the novelty and significance of the proposed approach justify acceptance. I recommend acceptance with minor revisions to address the noted weaknesses.