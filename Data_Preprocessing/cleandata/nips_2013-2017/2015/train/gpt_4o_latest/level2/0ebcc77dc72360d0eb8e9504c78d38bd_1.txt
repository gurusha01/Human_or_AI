This paper introduces a novel Gaussian Process-based Bayesian optimization algorithm, Infinite-Metric GP Optimization (IMGPO), which achieves exponential convergence without requiring auxiliary optimization or the impractical δ-cover sampling procedure. The authors address a significant limitation in existing Bayesian optimization methods by proposing a theoretically grounded approach that leverages an unknown semi-metric bound to guide optimization. By considering all possible bounds rather than relying solely on the Gaussian Process (GP) prior, the algorithm balances global and local search effectively, achieving superior performance in terms of regret bounds and practical applicability.
Strengths:
1. Novelty and Contribution: The paper makes a clear and significant contribution by eliminating the need for δ-cover sampling and auxiliary optimization, which have been longstanding challenges in Bayesian optimization. The proposed IMGPO algorithm is the first to achieve exponential convergence under these relaxed conditions.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, including regret bounds and insights into the benefits of considering infinite bounds. The regret bound \(O(\lambda^{N+N_{gp}})\) (with \(\lambda < 1\)) is a substantial improvement over existing methods like BaMSOO, which only achieve polynomial regret.
3. Practical Relevance: The algorithm is computationally efficient compared to traditional GP-based methods, as demonstrated in the experiments. The authors also provide publicly available source code, enhancing reproducibility and accessibility.
4. Experimental Validation: The paper includes comprehensive experiments across a range of benchmark functions, showing that IMGPO consistently outperforms competing methods like SOO, BaMSOO, GP-PI, and GP-EI. The results are robust, even when hyperparameters and kernels are not fine-tuned.
5. Clarity of Algorithm Description: The pseudocode and detailed explanation of the algorithm's steps are well-presented, making the methodology easy to follow.
Weaknesses:
1. Scalability to High Dimensions: While the paper acknowledges the scalability challenge for higher-dimensional problems, the proposed solution (e.g., REMBO) is only briefly discussed. A more detailed analysis or experiments on high-dimensional problems would strengthen the paper.
2. Dependence on GP Hyperparameters: Although the authors use empirical Bayesian methods to update hyperparameters, the performance of IMGPO may still depend on the choice of the initial kernel and hyperparameters. This aspect could have been explored further.
3. Limited Discussion of Limitations: The paper does not explicitly discuss potential trade-offs, such as the computational overhead introduced by considering infinite bounds or the impact of suboptimal GP priors on performance.
Pro and Con Arguments for Acceptance:
Pro:
- The paper addresses a critical gap in Bayesian optimization by removing impractical assumptions while maintaining exponential convergence.
- Theoretical contributions are well-supported by rigorous proofs and practical insights.
- Experimental results demonstrate clear advantages over state-of-the-art methods.
Con:
- Scalability to high-dimensional problems remains a challenge, and the proposed solutions are not explored in depth.
- The reliance on GP priors may limit performance in cases where the prior is poorly suited to the objective function.
Recommendation:
Overall, this paper makes a significant contribution to the field of Bayesian optimization by proposing a novel algorithm with strong theoretical guarantees and practical utility. While there are minor areas for improvement, the strengths far outweigh the weaknesses. I recommend acceptance for this conference.