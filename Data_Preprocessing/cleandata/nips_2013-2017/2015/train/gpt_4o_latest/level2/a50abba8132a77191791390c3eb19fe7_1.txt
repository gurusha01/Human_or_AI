Review of the Paper
This paper presents several strategies to improve the performance of stochastic variance-reduced gradient (SVRG) methods, which are widely used for optimizing large-scale machine learning problems. The authors propose techniques such as growing-batch strategies, mixed SG/SVRG methods, leveraging support vectors, and alternative mini-batch selection strategies. They also analyze the generalization error of SVRG and provide theoretical justifications for their proposed modifications. The paper is well-motivated, as SVRG is a memory-efficient algorithm with linear convergence, making it suitable for a variety of machine learning tasks.
Strengths:
1. Novelty and Contributions: The paper introduces several innovative extensions to the SVRG algorithm, including growing-batch strategies and mixed SG/SVRG methods. The use of support vectors to reduce gradient evaluations is particularly interesting and novel in the context of linearly-convergent stochastic gradient methods.
2. Theoretical Rigor: The authors provide detailed theoretical analyses for their proposed modifications, including convergence guarantees under inexact gradient calculations and mini-batch sampling strategies. The results are well-supported by mathematical proofs in the appendices.
3. Practical Relevance: The proposed strategies address key limitations of SVRG, such as its inefficiency in early iterations and the computational cost of gradient evaluations. These improvements have clear practical implications for large-scale machine learning problems.
4. Experimental Validation: The experimental results demonstrate the effectiveness of the proposed methods on logistic regression and Huberized hinge loss problems. The growing-batch strategy consistently outperforms the original SVRG in terms of test error and training objective, validating its utility.
5. Clarity of Presentation: The paper is well-organized, with a logical flow from problem formulation to theoretical analysis and experimental results. The inclusion of pseudo-code for the algorithms enhances reproducibility.
Weaknesses:
1. Limited Scope of Experiments: While the experiments are thorough for logistic regression and Huberized hinge loss, the paper does not explore other machine learning models or datasets. This limits the generalizability of the results.
2. Mixed SG/SVRG Performance: The mixed SG/SVRG strategy shows inconsistent performance across datasets, sometimes improving and sometimes degrading results. The paper could benefit from a deeper analysis of when this method is likely to succeed.
3. Support Vector Heuristic: The heuristic for identifying support vectors is not rigorously analyzed. While it shows empirical benefits, its theoretical properties and limitations are not well-understood.
4. Complexity of Mini-Batch Strategies: Some of the proposed mini-batch strategies, such as fixed/random set combinations, are complex and may not be practical in real-world applications. The marginal gains observed in experiments may not justify the added complexity.
Arguments for Acceptance:
- The paper makes significant theoretical and practical contributions to improving SVRG, a widely used optimization method.
- The proposed strategies are well-supported by theoretical analysis and empirical results.
- The work addresses important challenges in large-scale optimization, such as reducing gradient computation costs and improving early iteration performance.
Arguments Against Acceptance:
- The experimental evaluation is somewhat narrow, focusing primarily on logistic regression and Huberized hinge loss.
- Some proposed methods, such as the mixed SG/SVRG strategy and mini-batch strategies, show inconsistent or marginal improvements.
Recommendation:
I recommend acceptance of this paper, as it provides valuable insights and practical improvements to SVRG. However, the authors should consider expanding the experimental evaluation and providing more analysis of the mixed SG/SVRG method and support vector heuristic in a future revision. The paper is a strong contribution to the field of optimization for machine learning.