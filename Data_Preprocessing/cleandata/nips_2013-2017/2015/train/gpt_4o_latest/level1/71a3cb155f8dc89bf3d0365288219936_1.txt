This paper addresses the estimation of sparse graphical models for high-dimensional tensor-valued data, proposing a novel alternating minimization algorithm for penalized maximum likelihood estimation under the assumption of a tensor normal distribution with a Kronecker product covariance structure. The authors prove that their algorithm achieves minimax-optimal statistical rates of convergence and consistent graph recovery, even with a single tensor sample, a significant advancement over prior work. The theoretical contributions are supported by rigorous proofs and extensive numerical experiments, demonstrating the method's computational efficiency and superior accuracy compared to existing approaches.
Strengths:
1. Theoretical Contributions: The paper provides strong theoretical guarantees, including minimax-optimal rates of convergence in Frobenius, max, and spectral norms, as well as model selection consistency. The results extend prior work by addressing the computational-statistical gap in tensor graphical models.
2. Novelty: The alternating minimization algorithm is novel in its application to tensor graphical models and achieves results that were previously unattainable, such as estimation consistency with a single tensor sample.
3. Practical Relevance: The method is well-suited for real-world applications, such as fMRI and gene expression analysis, where tensor data are common and sample sizes are often limited.
4. Empirical Validation: The numerical experiments are thorough, comparing the proposed method (Tlasso) with two alternatives (Glasso and P-MLE). Results demonstrate Tlasso's computational efficiency and superior performance in estimation accuracy and variable selection.
5. Clarity of Presentation: The paper is well-organized, with detailed explanations of the model, algorithm, and theoretical results. The inclusion of illustrative examples and simulation results enhances understanding.
Weaknesses:
1. Assumptions and Conditions: The theoretical results rely on several assumptions, such as bounded eigenvalues and irrepresentability, which may not always hold in practice. While these are standard in the literature, their practical implications could be discussed more thoroughly.
2. Initialization Sensitivity: Although the authors argue that the algorithm's performance is insensitive to initialization, this claim could benefit from additional empirical validation, especially for higher-order tensors.
3. Comparison with Non-Lasso Penalties: The paper focuses on the lasso penalty, but it would be interesting to explore how the method performs with other sparsity-inducing penalties, such as SCAD or MCP, which may relax the irrepresentability condition.
4. Limited Real-World Applications: While the simulations are comprehensive, the paper lacks real-world application examples to demonstrate the method's practical utility beyond synthetic data.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution to the field of tensor graphical models, addressing a long-standing gap between computational and statistical guarantees.
- The proposed algorithm is computationally efficient and achieves state-of-the-art performance in both theory and practice.
- The work is highly relevant to the NeurIPS community, given its focus on high-dimensional data and machine learning applications.
Arguments Against Acceptance:
- The reliance on strong theoretical assumptions may limit the method's applicability in some practical scenarios.
- The lack of real-world application examples leaves questions about the method's performance in diverse, noisy datasets.
Recommendation:
I recommend acceptance of this paper, as it represents a substantial advancement in the field of tensor graphical models with strong theoretical and empirical results. Addressing the noted weaknesses in a future revision would further enhance its impact.