The paper introduces Stochastic Expectation Propagation (SEP), a novel algorithm designed to address the memory limitations of Expectation Propagation (EP) in large-scale Bayesian learning problems. EP, while accurate and computationally advantageous, suffers from a memory overhead that scales with the number of datapoints \(N\), making it impractical for large datasets. SEP mitigates this by maintaining a global posterior approximation, akin to Variational Inference (VI), while retaining the local update structure of EP. The authors demonstrate that SEP reduces memory consumption by a factor of \(N\) compared to EP, making it suitable for large models and datasets. Experimental results on synthetic and real-world datasets show that SEP achieves comparable accuracy to EP while significantly reducing memory requirements.
Strengths:
1. Significant Contribution: The paper addresses a critical limitation of EP—its scalability to large datasets—by proposing a memory-efficient alternative. This is a meaningful advancement for Bayesian learning.
2. Theoretical Foundations: The authors provide a solid theoretical grounding for SEP, connecting it to existing methods like Assumed Density Filtering (ADF), Stochastic Variational Inference (SVI), and Averaged EP (AEP). These connections highlight SEP's novelty and its place within the broader landscape of approximate inference methods.
3. Experimental Validation: The experiments are thorough, covering a range of models (probit regression, mixture of Gaussians, Bayesian neural networks) and datasets (synthetic, UCI, MNIST, and large-scale regression datasets). Results consistently show that SEP performs comparably to EP while drastically reducing memory usage.
4. Practical Relevance: The proposed method is highly practical, especially for applications like probabilistic backpropagation in neural networks, where memory constraints are a bottleneck.
Weaknesses:
1. Clarity: While the paper is well-organized, some sections, particularly the algorithmic details and theoretical results, are dense and may be challenging for readers unfamiliar with EP or related methods. Simplifying or summarizing key steps could improve accessibility.
2. Limited Theoretical Guarantees: The authors acknowledge that the theoretical understanding of SEP's convergence properties is incomplete. This limits confidence in the algorithm's robustness across diverse settings.
3. Granularity Trade-offs: While the paper explores finer-grained approximations (e.g., Distributed SEP), the discussion on when and how to partition datasets effectively is limited. This could be crucial for practitioners aiming to apply SEP to heterogeneous data.
4. Comparison with SVI: Although SEP is positioned as an alternative to VI and SVI, the experimental comparisons with SVI are minimal. A more detailed analysis of SEP's advantages over SVI in terms of accuracy, convergence, and computational efficiency would strengthen the paper.
Arguments for Acceptance:
- SEP addresses a critical scalability issue in EP, offering a practical and theoretically motivated solution.
- The experimental results convincingly demonstrate the algorithm's effectiveness across diverse models and datasets.
- The connections to existing methods provide a strong theoretical foundation, situating SEP as a natural extension of EP.
Arguments Against Acceptance:
- The lack of detailed theoretical guarantees for SEP's convergence is a notable limitation.
- The paper could benefit from clearer exposition and more comprehensive comparisons with SVI.
Recommendation:
I recommend acceptance of this paper, as it presents a significant and practical contribution to scalable Bayesian inference. While there are areas for improvement, particularly in clarity and theoretical analysis, the strengths of the proposed method and its potential impact on the field outweigh these concerns.