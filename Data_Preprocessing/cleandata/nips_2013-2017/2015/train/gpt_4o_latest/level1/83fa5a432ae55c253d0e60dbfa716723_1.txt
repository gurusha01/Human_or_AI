Review
Summary:
The paper introduces Principal Differences Analysis (PDA), a novel framework for identifying differences between high-dimensional distributions by leveraging the Wasserstein divergence. PDA operates by finding a projection that maximizes the divergence between univariate projections of two distributions, relying on the Cramer-Wold device to ensure generality. A sparse variant, SPARDA, is proposed to identify the features most responsible for these differences. The authors provide both theoretical guarantees and practical algorithms, including a semidefinite relaxation and a tightening procedure for optimization. The method is evaluated on synthetic datasets, the MADELON feature selection challenge, and single-cell RNA-seq data, demonstrating its effectiveness in uncovering meaningful differences in both simulated and real-world high-dimensional settings.
Strengths:
1. Novelty and Originality: The use of the Wasserstein divergence in a projection-based framework for high-dimensional distribution comparison is innovative. The sparse variant (SPARDA) is particularly valuable for feature selection in exploratory analysis, addressing a gap in existing methods like sparse LDA or PCA.
2. Theoretical Contributions: The paper provides rigorous theoretical guarantees, including concentration bounds, sparsistency results, and conditions under which the semidefinite relaxation is tight. These results enhance the credibility and robustness of the proposed approach.
3. Algorithmic Development: The RELAX algorithm and the tightening procedure are well-designed and computationally efficient, with clear connections to existing techniques like sparse PCA and semidefinite programming.
4. Empirical Validation: The experiments are comprehensive, covering synthetic datasets, benchmark challenges, and real-world applications in single-cell RNA-seq. The results convincingly demonstrate the superiority of SPARDA in identifying sparse, interpretable differences between distributions.
5. Broader Applicability: The framework is flexible, allowing for the use of other divergences beyond the Wasserstein distance, making it adaptable to diverse applications.
Weaknesses:
1. Clarity: While the paper is technically sound, it is dense and difficult to follow in places. For example, the derivation of the semidefinite relaxation and the tightening procedure could be better explained with additional intuition or visual aids. The notation is also complex, which may hinder accessibility for readers unfamiliar with optimization techniques.
2. Computational Complexity: Although the authors address scalability, the RELAX algorithm's runtime (O(d³n²) per iteration) may become prohibitive for very large datasets. This limitation is partially mitigated by the tightening procedure but could still restrict the method's applicability in extremely high-dimensional or large-sample settings.
3. Comparison to Related Work: While the paper references related methods like sparse LDA, PCA, and DiProPerm, the experimental comparisons could be expanded. For example, it would be helpful to include runtime comparisons or analyze performance on additional real-world datasets.
4. Assumptions: The theoretical results rely on simplifying assumptions (e.g., compact support, sub-Gaussian tails), which may not hold in all practical scenarios. The authors acknowledge this but do not provide extensive discussion on how violations of these assumptions might affect performance.
5. Limited Exploration of Divergences: Although the framework is general, the paper focuses almost exclusively on the Wasserstein divergence. Exploring other divergences (e.g., f-divergences) in the experiments could strengthen the claim of generality.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded framework for a challenging problem in high-dimensional data analysis.
- The empirical results demonstrate clear advantages over existing methods in terms of interpretability and power.
- The theoretical contributions, particularly the sparsistency guarantees and semidefinite relaxation, are significant and advance the state of the art.
Arguments Against Acceptance:
- The paper's clarity and accessibility could be improved, particularly in the algorithmic and theoretical sections.
- The computational complexity of the RELAX algorithm may limit scalability.
- The focus on the Wasserstein divergence, while justified, limits the exploration of the broader applicability of the framework.
Recommendation:
I recommend acceptance of the paper, as it makes a strong scientific contribution to the field of high-dimensional distribution comparison. However, the authors should address the clarity issues and provide additional discussion on scalability and the impact of assumptions in the final version.