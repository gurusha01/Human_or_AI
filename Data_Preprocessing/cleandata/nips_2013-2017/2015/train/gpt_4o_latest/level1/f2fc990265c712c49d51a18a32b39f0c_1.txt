The paper introduces HONOR, a novel optimization algorithm for solving non-convex regularized sparse learning problems. The authors address the challenges of non-convexity and non-smoothness in large-scale data by proposing a hybrid optimization scheme that alternates between a Quasi-Newton (QN) step and a Gradient Descent (GD) step. HONOR leverages second-order information for faster convergence while avoiding computationally expensive operations like forming the inverse Hessian matrix. The paper provides rigorous theoretical convergence guarantees, showing that HONOR converges to a Clarke critical point, a significant achievement given the inherent difficulties of non-convex optimization. Empirical results on large-scale datasets demonstrate that HONOR outperforms state-of-the-art algorithms like GIST in terms of convergence speed.
Strengths:
1. Technical Soundness: The paper is technically robust, with detailed theoretical analysis proving the convergence of HONOR. The use of the Clarke subdifferential and the hybrid scheme demonstrates a deep understanding of non-convex optimization challenges.
2. Practical Relevance: HONOR is designed for large-scale, high-dimensional datasets, making it highly relevant for real-world applications in sparse learning.
3. Empirical Validation: The experimental results are compelling, showing significant improvements in convergence speed over GIST. The sensitivity analysis on the parameter \( \epsilon \) adds depth to the evaluation.
4. Novelty: The hybrid optimization scheme combining QN and GD steps is innovative. Unlike prior methods, HONOR effectively incorporates second-order information without the computational overhead of solving complex subproblems.
5. Clarity of Contributions: The paper clearly delineates its contributions, including theoretical guarantees, practical efficiency, and empirical superiority.
Weaknesses:
1. Limited Comparisons: While HONOR is compared to GIST, other relevant methods like SparseNet and DC-PN are excluded, with only brief justifications. Including these would strengthen the empirical claims.
2. Computational Overhead: The authors note that HONOR's performance degrades when the GD step dominates, particularly for large \( \epsilon \). This raises questions about its scalability in extreme cases.
3. Scope of Applications: The experiments focus on logistic regression with specific non-convex regularizers. Broader evaluations across different loss functions and regularizers would enhance the generalizability of the results.
4. Clarity: While the paper is well-organized, some technical details (e.g., the pseudo-gradient definition and the hybrid scheme) could benefit from additional explanation or visual aids for accessibility to a broader audience.
Arguments for Acceptance:
- The paper addresses a critical challenge in non-convex optimization with a novel and theoretically sound approach.
- HONOR demonstrates significant empirical improvements over a state-of-the-art baseline.
- The hybrid scheme and convergence analysis are innovative and advance the state of the art.
Arguments Against Acceptance:
- The empirical evaluation could be more comprehensive, particularly with additional baselines and broader application scenarios.
- The computational overhead in GD-dominated scenarios may limit scalability in certain cases.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a strong theoretical and practical contribution to non-convex sparse learning. Addressing the noted weaknesses, particularly by expanding the empirical evaluation, would further solidify its impact.