This paper introduces ESGD, a novel adaptive learning rate method based on the equilibration preconditioner, aimed at addressing the challenges of optimizing non-convex loss functions in deep learning. The authors argue that saddle points, rather than local minima, dominate the optimization landscape of deep networks, leading to ill-conditioning and slow convergence. They critique the popular Jacobi preconditioner for its inadequacy in handling both positive and negative curvature and propose equilibration as a superior alternative. The paper provides theoretical justifications for equilibration's effectiveness in reducing the condition number of indefinite matrices and demonstrates its empirical advantages over RMSProp and Jacobi SGD on deep autoencoder benchmarks. ESGD achieves faster convergence and lower training error, particularly on the MNIST dataset, while maintaining computational efficiency comparable to other adaptive methods.
Strengths:
1. Novelty and Originality: The paper introduces a new adaptive learning rate method, ESGD, which leverages the equilibration preconditioner. This approach is novel and addresses a significant gap in understanding and improving optimization for non-convex problems.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis, including proofs and upper bounds, to justify the advantages of equilibration over Jacobi preconditioning. The connection to the absolute Hessian is particularly insightful.
3. Empirical Validation: The experiments on deep autoencoders are well-designed and demonstrate clear improvements in convergence speed and training error compared to RMSProp and Jacobi SGD. The analysis of cosine distances between preconditioners adds depth to the empirical findings.
4. Clarity of Motivation: The paper effectively motivates the need for better preconditioners in the context of saddle points and provides a compelling argument for why equilibration is better suited to non-convex optimization.
Weaknesses:
1. Limited Scope of Experiments: While the results on MNIST and CURVES are promising, the evaluation is limited to deep autoencoders. Broader benchmarks, such as classification tasks or larger-scale datasets, would strengthen the claims of general applicability.
2. Practical Impact: Although ESGD shows theoretical and empirical promise, its practical adoption may be hindered by the additional computational overhead of estimating curvature information, even if amortized.
3. Comparative Analysis: The paper does not compare ESGD with more recent adaptive optimizers like Adam, which could provide a more comprehensive evaluation of its effectiveness.
4. Clarity for Non-Experts: While the paper is well-written for experts, some sections (e.g., the derivation of preconditioners) may be difficult for readers unfamiliar with numerical optimization or Hessian-based methods.
Recommendation:
Arguments for Acceptance:
- The paper makes a significant theoretical and empirical contribution to adaptive learning rate methods for non-convex optimization.
- ESGD demonstrates clear improvements over RMSProp and Jacobi SGD in the provided benchmarks.
- The insights into the relationship between RMSProp and equilibration preconditioning could inspire further research in adaptive optimization.
Arguments Against Acceptance:
- The experimental scope is narrow, limiting the generalizability of the results.
- The paper does not compare ESGD with state-of-the-art optimizers like Adam, which is a notable omission.
Final Decision:
I recommend acceptance with minor revisions. The paper presents a meaningful advancement in adaptive optimization methods and provides both theoretical and empirical support for its claims. Expanding the experimental evaluation and including comparisons with Adam would further strengthen the paper.