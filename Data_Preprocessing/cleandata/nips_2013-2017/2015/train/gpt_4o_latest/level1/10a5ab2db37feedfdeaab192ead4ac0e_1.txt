Review
This paper addresses the critical challenge of bridging the gap between backpropagation-based training of deep neural networks and the deployment of such networks on energy-efficient neuromorphic hardware. The authors propose a novel "constrain-then-train" approach, where spikes and discrete synapses are treated as continuous probabilities during training, enabling the use of standard backpropagation. The trained network is then mapped to neuromorphic hardware (TrueNorth) by sampling these probabilities. The method achieves state-of-the-art accuracy on the MNIST dataset (99.42%) with a high-performance network and demonstrates unprecedented energy efficiency (0.268 ÂµJ per classification) with a high-efficiency network. The paper also introduces a progressive mixing topology to ensure constrained connectivity, making it compatible with neuromorphic hardware. This work is a significant contribution to the field, as it integrates algorithmic advancements in deep learning with the operational efficiency of neuromorphic systems.
Strengths:
1. Technical Soundness and Quality: The proposed method is technically robust, with well-supported theoretical analysis and experimental results. The authors provide detailed equations, training procedures, and deployment strategies, ensuring reproducibility.
2. Originality: The paper introduces a novel training methodology that reconciles the incompatibility between backpropagation and neuromorphic hardware. The probabilistic treatment of spikes and synapses is innovative and distinct from prior work.
3. Significance: The results demonstrate a significant advancement in energy-efficient neural networks. The method achieves the best accuracy to date for spiking neural networks on MNIST and sets a new benchmark for energy efficiency.
4. Clarity: The paper is well-organized and clearly written, with sufficient detail for experts to reproduce the results. The inclusion of comparisons with prior work highlights the novelty and impact of the approach.
Weaknesses:
1. Scope of Evaluation: While the results on MNIST are impressive, the paper does not explore more complex datasets or tasks, limiting its generalizability to real-world applications.
2. Hardware Dependency: The method is demonstrated on TrueNorth, and while the authors claim it can generalize to other neuromorphic hardware, this is not empirically validated.
3. Training Complexity: The proposed training method introduces additional complexity compared to standard backpropagation, which may limit its adoption in broader contexts.
Arguments for Acceptance:
- The paper addresses a timely and important problem, advancing the integration of deep learning and neuromorphic computing.
- The results are state-of-the-art in both accuracy and energy efficiency, with clear potential for real-world applications.
- The methodology is novel and well-supported by theoretical and experimental evidence.
Arguments Against Acceptance:
- The evaluation is limited to MNIST, a relatively simple dataset, which raises questions about scalability to more complex tasks.
- The dependency on TrueNorth hardware may restrict the broader applicability of the approach.
Recommendation: Accept with minor revisions. The paper makes a significant contribution to the field, but the authors should address the limitations of their evaluation and discuss potential extensions to more complex datasets and hardware platforms.