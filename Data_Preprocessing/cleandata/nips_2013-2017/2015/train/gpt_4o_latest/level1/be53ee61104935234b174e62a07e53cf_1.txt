This paper presents significant advancements in algorithms for Isotonic Regression on directed acyclic graphs (DAGs) under weighted `p-norms, including `∞-norms and Strict Isotonic Regression. The authors propose a unified, optimization-based framework that improves upon prior computational bounds, achieving rigorous performance guarantees. For `p-norms, the proposed algorithm runs in \(O(m^{1.5} \log^2 n \log n/\delta)\), improving the previous best results. For `∞-norms, the authors achieve a linear-time algorithm, which is a substantial improvement over prior \(O(m \log n)\) methods. Additionally, the paper introduces an \(O(mn)\)-time algorithm for Strict Isotonic Regression, improving upon earlier results for specific cases.
Strengths:
1. Technical Contributions: The paper provides a robust theoretical framework for solving Isotonic Regression problems efficiently. The use of Interior Point Methods (IPMs) with approximate solvers is novel and well-justified, extending prior work on convex programming.
2. Improved Complexity: The algorithms achieve state-of-the-art time complexity for general DAGs and special graph families, which is a significant contribution to the field.
3. Practicality: The authors emphasize the practical applicability of their algorithms, supported by experimental results demonstrating efficiency on real-world graph structures.
4. Clarity of Results: The paper includes detailed comparisons with prior work, clearly highlighting improvements in computational bounds and practical feasibility.
5. Broader Impact: The techniques introduced have potential applications in machine learning, such as learning Lipschitz monotone functions and constructing class probability estimation models.
Weaknesses:
1. Clarity: While the technical content is rigorous, the paper is dense and may be challenging for readers unfamiliar with convex optimization or Isotonic Regression. Simplifying the exposition or providing more intuitive explanations would improve accessibility.
2. Experimental Evaluation: The experiments focus primarily on `2-norm Isotonic Regression. While the results are promising, additional empirical evaluations for other norms (e.g., `∞) and larger datasets would strengthen the practical claims.
3. Scope of Applications: Although the paper mentions potential applications, it does not explore these in depth. Demonstrating the algorithms' utility in real-world machine learning tasks would enhance the paper's significance.
4. Dependency on Prior Work: The algorithms for `∞-norms and Strict Isotonic Regression rely heavily on reductions to existing Lipschitz learning techniques. While this is valid, the novelty in these cases is less pronounced.
Arguments for Acceptance:
- The paper advances the state of the art in Isotonic Regression, both theoretically and practically.
- The proposed algorithms are efficient, with rigorous performance guarantees and broad applicability.
- The work addresses a fundamental problem in machine learning and optimization, making it relevant to the NIPS community.
Arguments Against Acceptance:
- The dense technical presentation may limit accessibility to a broader audience.
- Experimental results could be more comprehensive, particularly for norms other than `2.
Recommendation:
I recommend acceptance of this paper, as it makes a substantial contribution to the field of optimization and machine learning. The strengths outweigh the weaknesses, and the work is likely to inspire further research and applications. However, the authors should consider improving the clarity and expanding the experimental evaluation in the final version.