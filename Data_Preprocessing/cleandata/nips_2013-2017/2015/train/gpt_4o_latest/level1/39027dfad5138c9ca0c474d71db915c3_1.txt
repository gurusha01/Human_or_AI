This paper addresses a critical issue in batch learning from logged bandit feedback (BLBF), specifically the propensity overfitting problem inherent in the conventional counterfactual risk estimator used in Counterfactual Risk Minimization (CRM). The authors propose a novel self-normalized risk estimator to mitigate this issue and introduce a new learning algorithm, Norm-POEM, which leverages this estimator. The paper is well-motivated, as BLBF is a key framework for training systems like recommendation engines and ad placement algorithms using historical logs. The authors demonstrate both theoretically and empirically that their approach avoids the pitfalls of propensity overfitting and achieves superior generalization performance compared to the state-of-the-art POEM algorithm.
Strengths:
1. Novelty and Originality: The paper makes a significant contribution by identifying the propensity overfitting problem, which has not been previously discussed in the BLBF literature. The introduction of the self-normalized risk estimator is a novel solution that builds on classic variance reduction techniques from importance sampling.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, proving that the self-normalized estimator is strongly consistent and avoids the anomalies of the conventional estimator. The theoretical results are well-supported by mathematical derivations and proofs.
3. Empirical Validation: The experimental results are compelling, showing that Norm-POEM consistently outperforms POEM across multiple datasets and settings. The authors also demonstrate that the self-normalized estimator effectively mitigates propensity overfitting, as evidenced by the stability of the propensity weight sums and improved test set performance.
4. Practical Relevance: The proposed method is computationally efficient, with runtime comparable to existing methods, and the authors provide open-source software, increasing the practical impact of their work.
Weaknesses:
1. Clarity: While the paper is technically sound, it is dense and could benefit from clearer explanations in some sections. For instance, the derivation of the self-normalized estimator and its variance estimator may be challenging for readers unfamiliar with importance sampling.
2. Limited Scope of Experiments: Although the experiments are robust, they are restricted to multi-label classification tasks. It would strengthen the paper to evaluate Norm-POEM on other BLBF applications, such as ad placement or recommendation systems, to demonstrate broader applicability.
3. Hyperparameter Sensitivity: The paper briefly mentions hyperparameter tuning but does not provide an in-depth analysis of how sensitive Norm-POEM is to these parameters. This could be a concern for practitioners.
Arguments for Acceptance:
- The paper addresses a well-motivated and important problem in BLBF.
- It introduces a novel and theoretically sound solution with strong empirical results.
- The proposed method has practical implications and is computationally efficient.
Arguments Against Acceptance:
- The clarity of the presentation could be improved, particularly for readers outside the immediate subfield.
- The experiments, while strong, are somewhat narrow in scope.
Recommendation:
I recommend acceptance of this paper. It makes a significant contribution to the field of BLBF by identifying and addressing a critical issue, and it provides a theoretically grounded and empirically validated solution. Improvements in clarity and broader experimental validation could further enhance the paper, but these are secondary to its core contributions.