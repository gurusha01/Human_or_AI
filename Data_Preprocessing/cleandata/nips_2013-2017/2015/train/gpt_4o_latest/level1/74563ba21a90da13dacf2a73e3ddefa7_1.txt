This paper addresses the statistical treatment of persistence diagrams, a key representation in topological data analysis (TDA), by proposing a universal kernel for embedding these diagrams into reproducing kernel Hilbert spaces (RKHS). The authors build on prior work by Reininghaus et al. (2015), who introduced a positive definite kernel for persistence diagrams, and extend it by proving universality for a modified version of this kernel. This universality enables the principled use of persistence diagrams in statistical tasks such as two-sample hypothesis testing. The paper also demonstrates the utility of the proposed kernel through experiments on synthetic and real-world datasets, including applications in neuroscience.
Strengths:
1. Technical Contribution: The paper makes a significant theoretical contribution by proving the universality of the proposed kernel. This is a meaningful step forward, as universality ensures that the kernel can distinguish between different probability distributions of persistence diagrams, a limitation in prior work.
2. Relevance and Novelty: The work is novel in its approach to embedding persistence diagrams into RKHS for statistical computations. While related methods like persistence landscapes (Bubenik, 2015) and Wasserstein metrics have been explored, this kernel-based approach offers a complementary and theoretically robust framework.
3. Experimental Validation: The experiments are well-designed and demonstrate the practical utility of the kernel in two-sample hypothesis testing. The synthetic data experiments effectively illustrate the kernel's ability to capture topological differences, while the real-world applications highlight its potential in neuroscience.
4. Clarity and Organization: The paper is well-organized, with a clear exposition of background material, theoretical results, and experimental findings. The inclusion of related work situates the contribution within the broader context of TDA and kernel methods.
Weaknesses:
1. Practical Limitations: The kernel's reliance on compactness assumptions for the metric space of persistence diagrams may limit its applicability to certain datasets. While the authors acknowledge this, further discussion on relaxing these constraints would strengthen the paper.
2. Choice of Parameters: The experiments involve tuning kernel scales (Ïƒ) and heat kernel signature (HKS) times, but the paper provides limited guidance on how to select these parameters in practice. This could hinder reproducibility and generalizability.
3. Limited Scope of Applications: While the experiments are compelling, they focus primarily on two-sample hypothesis testing. Additional applications, such as clustering or regression, would better demonstrate the broader utility of the kernel.
4. Comparison with Alternatives: The paper does not provide a direct empirical comparison with alternative methods like persistence landscapes or Wasserstein-based approaches. Such comparisons would clarify the advantages and trade-offs of the proposed kernel.
Arguments for Acceptance:
- The theoretical contribution is significant and advances the state of the art in statistical TDA.
- The proposed kernel has broad applicability in machine learning and statistics, with potential for impact in various domains.
- The experiments are thorough and demonstrate the kernel's practical utility.
Arguments Against Acceptance:
- The reliance on compactness assumptions may limit the kernel's general applicability.
- The lack of empirical comparisons with alternative methods leaves open questions about relative performance.
- The scope of applications explored is somewhat narrow.
Recommendation:
I recommend acceptance of this paper, as it provides a meaningful theoretical and practical contribution to TDA and kernel-based learning. However, the authors should address the noted weaknesses, particularly by discussing parameter selection and providing empirical comparisons in future work.