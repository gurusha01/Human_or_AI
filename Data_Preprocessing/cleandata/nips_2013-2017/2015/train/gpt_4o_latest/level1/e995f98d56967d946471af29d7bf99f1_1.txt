Review
This paper proposes a novel curriculum learning approach, termed "Scheduled Sampling," to address the discrepancy between training and inference in sequence prediction tasks using recurrent neural networks (RNNs). The authors highlight that during training, RNNs are conditioned on the true previous token, whereas during inference, they rely on their own predictions, leading to error accumulation. Scheduled Sampling gradually transitions the training process from using true tokens to model-generated tokens, thereby better aligning training with inference. The approach is evaluated on three tasks: image captioning, constituency parsing, and speech recognition, demonstrating consistent improvements. Notably, the method contributed to the authors' winning entry in the 2015 MSCOCO image captioning challenge.
The paper builds on prior work addressing the training-inference mismatch, such as SEARN and beam search-based approaches, but distinguishes itself by offering an online, computationally efficient solution tailored to RNNs. The authors also situate their work within the broader context of curriculum learning and reinforcement learning, providing a clear theoretical motivation for their method.
Strengths:
1. Quality: The paper is technically sound, with a well-motivated problem statement and a clear description of the proposed solution. The experimental results are robust, spanning multiple tasks and datasets, and demonstrate significant performance gains. The use of diverse decay schedules for sampling probability is a thoughtful design choice.
2. Clarity: The paper is well-written and organized, with detailed explanations of the methodology, training process, and experimental setup. The inclusion of ablation studies (e.g., "Always Sampling" and "Uniform Scheduled Sampling") strengthens the validity of the results.
3. Originality: The proposed approach is novel in its application of curriculum learning to sequence prediction tasks. While related work exists, the paper provides a unique perspective by integrating sampling strategies directly into the training process.
4. Significance: The method addresses a fundamental challenge in sequence modeling and has practical implications for a wide range of applications, including machine translation, image captioning, and speech recognition. The strong empirical results, including the MSCOCO challenge win, underscore its impact.
Weaknesses:
1. Theoretical Analysis: While the empirical results are compelling, the paper lacks a rigorous theoretical analysis of why Scheduled Sampling works, particularly in comparison to related methods like SEARN. A deeper exploration of the underlying dynamics could strengthen the contribution.
2. Generality: The approach is evaluated on specific tasks, but its generalizability to other sequence prediction problems (e.g., conversational AI or reinforcement learning) is not fully explored.
3. Gradient Backpropagation: The authors acknowledge that they did not backpropagate through the sampling decisions, which could potentially improve performance. This omission leaves room for further optimization.
Arguments for Acceptance:
- The paper addresses a critical and widely recognized issue in sequence modeling.
- The proposed method is novel, computationally efficient, and demonstrates strong empirical results across diverse tasks.
- The clarity and organization of the paper make it accessible to both researchers and practitioners.
Arguments Against Acceptance:
- The lack of theoretical analysis limits the understanding of the method's broader implications.
- The experiments, while diverse, could benefit from additional tasks to further validate generalizability.
Recommendation: Strong Accept  
This paper makes a significant contribution to the field of sequence modeling by proposing a novel, practical solution to a well-known problem. Its strong empirical results and clear exposition make it a valuable addition to the conference. Further theoretical exploration and broader task evaluations could enhance its impact.