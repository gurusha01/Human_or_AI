This paper presents a novel non-greedy method for decision tree induction, addressing the limitations of traditional greedy approaches. The authors propose a framework that jointly optimizes split functions and leaf parameters under a global objective, leveraging a connection between decision tree optimization and structured prediction with latent variables. The key contribution is a convex-concave upper bound on the empirical loss, which allows efficient optimization using stochastic gradient descent (SGD). The proposed method demonstrates superior performance over greedy baselines on several classification benchmarks, achieving better generalization and reduced susceptibility to overfitting, particularly for deep trees.
Strengths:
1. Technical Innovation: The paper introduces a principled, non-greedy approach to decision tree learning, which is a significant departure from traditional greedy methods. The connection to structured prediction with latent variables is insightful and well-motivated.
2. Efficiency: By reducing the computational complexity of gradient updates for deep trees from \(O(2^d p)\) to \(O(d^2 p)\), the authors make training deep trees feasible, which is a notable technical contribution.
3. Empirical Validation: The experiments are thorough, comparing the proposed method against multiple baselines (e.g., axis-aligned trees, OC1, CO2) across diverse datasets. The results consistently show that the non-greedy approach achieves higher test accuracy and better generalization.
4. Regularization: The use of norm constraints on split parameters provides a principled way to control overfitting and implicitly prunes the tree, which is a thoughtful addition to the framework.
5. Scalability: The introduction of a fast loss-augmented inference algorithm ensures scalability for deep trees, making the method applicable to large datasets.
Weaknesses:
1. Clarity: While the technical content is rigorous, the paper is dense and challenging to follow, particularly for readers unfamiliar with structured prediction or convex-concave optimization. Simplifying the exposition or adding more intuitive explanations would improve accessibility.
2. Limited Scope of Evaluation: Although the method is evaluated on several benchmarks, the datasets are relatively standard. Testing on more diverse or real-world datasets, such as those with imbalanced classes or noisy features, would strengthen the claims of general applicability.
3. Initialization Dependency: The method relies on a greedy initialization (e.g., axis-aligned splits), which may limit its performance if the initialization is suboptimal. This dependency is not thoroughly analyzed.
4. Computational Overhead: While the fast loss-augmented inference reduces complexity, the overall training time for non-greedy trees is still higher than traditional greedy methods. A more detailed comparison of computational costs across datasets would be helpful.
Arguments for Acceptance:
- The paper addresses a long-standing challenge in decision tree learning by proposing a novel and theoretically grounded approach.
- The empirical results demonstrate clear improvements over existing methods, both in accuracy and generalization.
- The connection to structured prediction opens new avenues for extending decision tree learning to more complex settings.
Arguments Against Acceptance:
- The dense and technical presentation may hinder comprehension for a broader audience.
- The reliance on greedy initialization and limited evaluation on diverse datasets may raise questions about the robustness and generalizability of the method.
Recommendation:
I recommend acceptance of this paper, as it makes a significant contribution to decision tree learning by introducing a non-greedy optimization framework with strong theoretical underpinnings and empirical validation. However, the authors should consider revising the paper for clarity and addressing the computational overhead in future work.