This paper addresses the challenge of efficiently implementing non-convex sparse learning formulations. Recent research has demonstrated that non-convex sparse learning formulations often outperform their convex counterparts in both theoretical and practical settings. However, solving non-convex sparse optimization problems efficiently for large-scale datasets remains a significant challenge. The authors propose a novel algorithm, HONOR, which is designed to handle a broad spectrum of non-convex sparse learning formulations.
A central innovation of HONOR is its use of second-order information to accelerate convergence. Unlike most existing second-order methods, HONOR avoids solving a regularized quadratic programming problem and instead relies solely on matrix-vector multiplications without explicitly computing the inverse Hessian matrix. This design ensures low computational complexity per iteration, making HONOR scalable to large-scale problems.
Analyzing and establishing convergence for non-convex problems is inherently difficult. A major contribution of this work is the rigorous convergence analysis provided for HONOR, which guarantees convergence even for non-convex problems. The analysis hinges on a hybrid optimization scheme that alternates between Quasi-Newton steps and Gradient Descent steps in each iteration. This analysis is nontrivial, and it would be beneficial to include a high-level explanation of the intuition behind the hybrid scheme.
The empirical evaluation of HONOR is compelling. The authors test the algorithm on large-scale datasets, some of which contain up to millions of samples and features, with two datasets exceeding 20 million features. Such large-scale datasets are essential for assessing the performance of the algorithm. The results clearly demonstrate that HONOR achieves significantly faster convergence compared to state-of-the-art algorithms. Providing guidance on the selection of \(\epsilon\) would enhance the practical usability of the algorithm.
The paper mentions that HONOR has the potential to escape high-error plateaus, which are common in high-dimensional non-convex problems. It would be valuable to explore the theoretical properties of the solutions obtained by HONOR in future work.
While the proposed algorithm demonstrates rapid empirical convergence, it would be interesting to investigate whether there is a guarantee on its (local) convergence rate. Additionally, it would be worth exploring whether the algorithm can be extended to accommodate other sparsity-inducing penalties, such as group Lasso and fused Lasso.