The typical approach to training recurrent neural networks involves making each prediction based on the current hidden state and the previous correct token from the training set. However, during testing, the expectation is that the trained RNN will generate the entire sequence by making predictions based on its own previously generated tokens. This paper proposes a training strategy where the model is progressively encouraged to generate the entire sequence by increasingly relying on its own predictions for the previous token.
Quality
The paper is technically sound, and the effectiveness of the proposed scheduled sampling approach is well-supported.
Clarity
The manuscript is well-written and logically organized.
Significance
The core idea is compelling and well-motivated. The proposed training methodology has the potential to significantly influence the field of recurrent neural network training.
Minor Comments
- Do you have any insights into the differences among the three decay schedules? How do these schedules manifest in the training process?  
- Does the training process risk becoming stuck in sub-optimal solutions? Training recurrent networks can be challenging due to numerous factors (e.g., momentum, gradient clipping, RMSProp, etc.).  
- Please provide additional details about the training process to ensure the experiments are reproducible. Additionally, report the training set cost.  
- Would scheduled sampling also facilitate optimization?  
In summary, this is a strong paper that introduces a simple yet effective scheduled sampling strategy to address the mismatch between training and inference in recurrent neural networks for sequence generation tasks. The proposed approach enables trained RNNs to outperform robust baselines in applications such as image captioning, constituency parsing, and speech recognition.