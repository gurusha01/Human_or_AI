This paper introduces an algorithm, HONOR, designed for a broad range of non-convex sparse learning formulations.
The HONOR algorithm integrates a Quasi-Newton (QN) step with a standard Gradient Descent (GD) step to approximate the objective's first-order behavior without explicitly computing the Hessian for the QN step. Instead, it leverages L-BFGS to scale effectively for large-scale objectives. The authors demonstrate that any limit point to which the algorithm converges is a Clarke critical point of the objective, and the sequence generated by the algorithm leads to such a limit point.
Analyzing convergence for non-convex problems is generally challenging.
This paper successfully provides a rigorous convergence analysis, proving that the algorithm converges to a limit point guaranteed to be a Clarke critical point. The analysis is mathematically intricate and appears to be correct. It would be interesting to explore whether this approach can be extended to more generalized non-convex objectives.
The empirical evaluation is reasonable, with experiments demonstrating the decrease in objective value over time across various large-scale and high-dimensional datasets. The results indicate that the algorithm converges faster than comparable methods. It would be helpful to include an example dataset, even synthetic, with a local minimum to evaluate the algorithm's performance in such scenarios. Additionally, guidance on selecting parameters, particularly \(\gamma\), to ensure faster convergence would be valuable. This paper focuses on a specific class of regularized non-convex problems common in machine learning. The authors propose a hybrid algorithm that combines second-order information via Quasi-Newton steps and gradient descent steps, determined by a computationally inexpensive condition at each iteration. They provide a thorough analysis, addressing the challenges posed by non-convexity, and prove that their algorithm converges to a Clarke critical point. Furthermore, the analysis is extendable to other general non-convex settings.