Acknowledge the author's rebuttal.
I largely maintain my overall perspective on the paper and look forward to further discussions regarding how this work relates to traditional neural and deep networks.
---
In this paper, the authors propose a deep belief network where the intermediate hidden layers are constrained to have nonnegative weights.
They apply this framework to text documents, referring to it as a "deep belief topic model" (my own terminology).
The authors derive an inference and update procedure, where Dirichlet vectors are propagated upward through the network and Gamma weights are propagated downward, and they provide an empirical evaluation of the model.
Overall, the paper is written with reasonable clarity.
While I found the paper to be generally acceptable, I believe there is room for improvement in the empirical analysis (which I will elaborate on shortly).
Additionally, I would be surprised if neural networks with weights outside the range [0, 1] have not been explored previously; it would be beneficial to include some discussion of related background work, even if it did not directly inspire this model.
I also have a few minor comments:
 - It is unclear whether using a fixed budget is an appropriate approach for determining the size of the various layers. Without additional details about the method, I remain skeptical.
This approach seems prone to producing layers that are either too large or too small. I suspect that using a held-out subset of data could help dynamically determine whether to expand or contract the model.
 - It is not evident whether adding layers beyond the second layer provides meaningful benefits.
The performance trends in the figures suggest marginal improvements, but the high variance in the plots (with the possible exception of Figure 3) makes it difficult to draw firm conclusions.
Additionally, the topic descriptions from sample words (line 417) do not appear to differ significantly; the authors' observation that topics become more specific at higher layers may simply be a result of the top layer being the largest.
 - The authors describe a method for jointly training all layers, as opposed to a greedy layer-wise training approach (line 264).
In this regard, the method resembles traditional neural network training from two decades ago, i.e., forward and backward propagation, albeit with samples.
 - The paper could potentially introduce the Gamma-Poisson belief network without relying on an application involving the Chinese Restaurant Process (CRT). The application itself is complex, and it is unclear whether such complexity is necessary to evaluate the proposed neural network architecture.
Regarding the experimental validation, I believe there is significant room for improvement.
First, it would be helpful to include more comparisons with baseline methods.
For instance, in the classification task, it would be valuable to compare the proposed model against simpler baselines such as SVM or ridge regression using word counts as features, which are known to outperform topic models in classification tasks. (Note: I do not expect the proposed model to necessarily outperform these baselines.)
It would also be helpful to compare the proposed model against a standard neural network with weights constrained to the range [0, 1], potentially adapting the bottom layer to accommodate Poisson/Multinomial observations.
In particular, claims such as the one on line 145 ("...clearly shows that the gamma distributed nonnegative hidden units could carry richer information than the binary hidden units and model more complex nonlinear functions") should be substantiated with a direct comparison to a model using binary hidden units.
It is not evident that a binary network, with an appropriately designed architecture, could not express similar nonlinear functions.
Additionally, is it necessary to remove stopwords? Topic models like LDA can handle stopwords adequately, and having a stopword topic does not necessarily undermine the evaluation.
Minor issues:
 - In the experiment section (line 349), the authors discuss classification before introducing the actual task on line 353, which caused some confusion.
 - Line 56: "budge" should be corrected to "budget."
This paper presents an intriguing variation of a standard deep network. The core idea is interesting, and the presentation is mostly clear, but the experimental validation requires further refinement.