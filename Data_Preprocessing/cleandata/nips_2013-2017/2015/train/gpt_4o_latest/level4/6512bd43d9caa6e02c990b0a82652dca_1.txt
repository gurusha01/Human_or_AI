Summary of Paper ======================================== The paper aims to explore the relationship between algorithmic stability and generalization performance. Prior work has proposed various notions of algorithmic stability and demonstrated their connection to the generalization performance of learning algorithms [6,11,13,14], as well as their importance for learnability [14].
For bounded loss functions, [12] showed that the generalization of ERM is equivalent to the probabilistic leave-one-out stability of the learning algorithm. Additionally, [14] established that a learning problem is learnable in Vapnik's general learning framework if and only if there exists an asymptotically stable ERM procedure.
This paper builds on these foundations by proving that, in Vapnik's general learning framework, a probabilistic notion of stability is both necessary and sufficient for the uniform convergence of training losses to test losses across all distributions. The paper also includes discussions on how this stability notion can be interpreted in terms of the capacity of the function class or the size of the population.
Questions ======================================== - The practical utility of the paper's main result is unclear. Does the paper aim to address cases where training-to-test error convergence occurs despite the absence of uniform convergence? - Can the authors provide an example of a learning problem where uniform generalization fails, yet the training errors of a stable algorithm still converge to the test errors? - Could the results of [14] be leveraged to derive a similar conclusion? Specifically, [14] demonstrates that learnability is equivalent to the existence of a stable AERM. Let f* denote the population risk minimizer, fERM the empirical risk minimizer, and fAERM a stable asymptotic empirical risk minimizer as defined in [14]. Let R and R^ represent the population and empirical risk functionals, respectively. Then, the following holds:
R(f_AERM) ≤ R(f*) + ε (for sufficiently large sample sizes, since the problem is learnable)
≤ R^(f) + 2ε (via point convergence on f using Hoeffding's inequality)
≤ R^(fERM) + 2ε (since fERM minimizes the empirical risk)
≤ R^(f_AERM) + 3ε (for sufficiently large sample sizes, as the algorithm is an AERM)
This establishes that R(fAERM) - R^(fAERM) ≤ ε for any ε, given a sufficiently large sample size. This result appears to recover the main claim of the paper. - Why should the difference between training and test errors be of primary interest? The focus in learning theory is typically on learnability and test error, and [14] already establishes that learnability is equivalent to the existence of uniformly RO-stable asymptotic empirical risk minimizers. - A detailed discussion comparing the paper's contributions to prior work is necessary. - The paper does not adequately relate its proposed notion of stability to existing stability concepts. This is critical, as established notions like uniform RO-stability [14] have already been used to derive similar results. - Section 5.1 is vague, and the results are not formally stated. Dropout is a widely used heuristic, and recent theoretical analyses have sparked significant interest. The discussion on dropout's theoretical merits feels like a missed opportunity without formal statements. - The results in Section 5.2 lack proper instantiation. While the notion of ESS is introduced, it is not applied to bound the effective size of an observation space, except in the simple case of finite spaces in Corollary 1. - Similarly, Section 5.3 suffers from insufficient instantiation. The results are only demonstrated for the simple case of finite hypothesis classes in Theorem 3. Theorem 4 introduces an alternative method for calculating the VC dimension of a function class that is not necessarily a classification class. However, no examples are provided to show whether this notion of VC dimension is practically useful or how it connects to other established capacity measures, such as uniform entropy or Rademacher averages. - Overall, it is challenging to assess the significance of the results in Section 5 or determine whether they provide meaningful new insights into the topic.
Quality ======================================== While the paper may contain interesting contributions, these are neither clearly articulated nor sufficiently contextualized with respect to prior work. The motivation for focusing on the difference between training and test errors, particularly in the absence of uniform convergence, is not well-justified.
Clarity ======================================== The paper is well-written and clear.
Originality ======================================== The paper employs established techniques in information complexity, such as the data processing inequality. The proposed notion of stability, defined via the total variation distance between distributions, appears novel and intriguing.
Significance ======================================== In its current form, the paper may not generate significant interest within the learning theory community, as algorithmic stability is already known to be equivalent to learnability. The additional results in Section 5 are poorly instantiated, making it unclear whether they offer novel insights. The primary contribution of the paper lies in establishing a connection between algorithmic stability and generalization performance. While the probabilistic stability notion introduced is new, the paper does not sufficiently integrate its findings with existing results in the field.