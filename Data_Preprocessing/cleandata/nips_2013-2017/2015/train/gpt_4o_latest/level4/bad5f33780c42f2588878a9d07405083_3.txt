The manuscript introduces and investigates an information-theoretic measure, approximate max-information, which serves as a tool for quantifying the generalization properties of adaptive data analyses. The output of a data analysis algorithm with low approximate max-information can be interpreted as being "almost" independent of the dataset in a specific sense that facilitates reasoning about subsequent analyses of the data that may rely on the algorithm's output. Differentially private algorithms exhibit low approximate max-information, as do algorithms with a small cardinality range, and compositions of algorithms with low approximate max-information also maintain relatively low approximate max-information. The paper provides several applications: generalization guarantees for differentially private algorithms (Corollaries 19 and 20), a mechanism for reusing holdout sets in machine learning (Thresholdout), and a method for conducting multiple hypothesis testing (or similar activities) on the same dataset (SparseValidate).
I recommend accepting the paper, as it offers a compelling perspective on generalization. While it shares similarities with algorithmic stability concepts, I find that approximate max-information encapsulates a property that appears to be more practical and versatile than prior stability notions, particularly due to its composition property. The presented applications are also highly engaging and impactful.
However, the paper does not adequately compare its contributions to earlier work on generalization in learning theory. For example, Freund's work on self-bounding learning algorithms and Blum and Langford's research on micro-choice bounds seem particularly relevant but are not discussed. Additionally, I found it challenging to discern the distinction between the description length results in this paper and prior Occam/MDL-type bounds (again, see Blum and Langford's work).
The authors should also elaborate on the relationship between τ and n in Thresholdout. Typically, we aim for τ = O(1/√n), which would imply budgets of constant size or smaller. While the results remain interesting in this regime due to the allowance for adaptivity and the "free" evaluation of "good" functions, highlighting this quantitative aspect is crucial for contextualizing the results.
Additional comments are as follows:  
- Page 6, line 289: "k = I_\infty^\beta(S; A(S)) = k"  
- Page 6, lines 318 and 320: The probability statements appear to be inverted.  
- Page 7, line 338: Define $\mathcal E{Sh}[\phi]$.  
- Page 7, line 341: Do you mean "true expectation $\mathcal P[\phi]$"?
Further remarks:  
- Another reviewer noted an issue with the statement of Theorem 9. Did you mean something like "For all i and t, Pr{ ai \neq \bot AND |ai - P[\phi_i]| > T + (t+1)\tau } \leq ..."? Clarifying this would be helpful.  
- It would also be beneficial to make various qualitative claims more explicit (e.g., the number of queries can be exponential in n). While the user may not know when the budget will be exhausted, they may desire simultaneous validity of all non-\bot queries. This suggests that Theorem 9 should be applied with a union bound over the m queries. (Perhaps a more sophisticated approach than a straightforward union bound could be considered.)  
- Additionally, why is it important for S_t (the training set) to be an i.i.d. sample?
---
Post-rebuttal remarks:
One significant source of confusion in the text is Lines 119-120. It seems to suggest that m = 2^{cn} and B = cn^2 can hold simultaneously for some positive c, but this clearly leads to trivial results in Theorem 9. While it may be acceptable to use terms like "quadratic" and "exponential" somewhat loosely, the paper should state these relationships precisely and explicitly at least once.  
Overall, I maintain that the paper should be accepted. It provides an intriguing perspective on generalization. While it shares some overlap with algorithmic stability concepts, approximate max-information captures a property that is both more practical and versatile (largely due to its composition property). Additionally, the applications presented in the paper are highly compelling.