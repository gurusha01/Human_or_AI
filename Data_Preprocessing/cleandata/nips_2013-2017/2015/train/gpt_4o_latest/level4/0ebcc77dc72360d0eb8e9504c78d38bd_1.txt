The paper introduces a novel algorithm for global optimization that circumvents delta-cover sampling and achieves exponential regret, thereby generalizing the work of Freitas et al., 2012, which relies on an impractical sampling procedure.
The manuscript is well-written, easy to follow, and effectively situates the proposed algorithm within the broader context of bounded-based search methods. In this regard, I believe the work merits attention from the community.
I assign a score of 5, but not a higher one, as I find the experimental section to fall short of the standards typically expected in a Bayesian Optimization (BO) paper. My primary concerns with this section are as follows:
- The authors do not evaluate the methods under multiple initializations of f, which is standard practice in BO. As a result, no meaningful statistical comparison of the methods can be drawn.
- Despite proposing a bounded-based search method, the authors fail to benchmark their results against state-of-the-art BO approaches. Notably, information-theoretic methods such as Entropy Search are absent from the experiments.
- All experiments are conducted on synthetic functions, with a maximum dimensionality of 6. The lack of real-world experiments, such as wet-lab parameter tuning, limits the demonstration of the method's applicability to practical scenarios.
In summary, the paper presents an interesting global optimization algorithm that avoids delta-cover sampling and achieves exponential regret. While the theoretical contributions are notable, the experimental section does not meet the standards expected for a NIPS paper.