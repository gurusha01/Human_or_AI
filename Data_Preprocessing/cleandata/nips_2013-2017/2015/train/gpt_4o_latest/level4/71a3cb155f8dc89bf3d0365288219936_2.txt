After rebuttal:
Given that one of the primary contributions of this paper is the claim that the initialization is irrelevant, it is perplexing that the authors have chosen to use the same initialization as prior works. This raises significant concerns regarding the novelty and robustness of the proposed approach.
============
In my view, this is a technically sound paper that presents nontrivial results. Building on recent research trends involving alternating minimization for various statistical problems, the authors derive compelling results on recovering precision matrices from tensor normal distributions. This work can thus be seen as another demonstration of the utility of alternating minimization in statistical settings. Theorem 3.1, while the simplest result in the paper, encapsulates the core idea and is arguably the most critical insight for future research. The proofs of the other theorems, though more technical, are sufficiently clear for me to follow and do not appear to contain any insurmountable issues. However, the proof of the final theorem is missing.
Below, I outline some errors and limitations I identified in the paper:
(1) The authors assert that a single sample is sufficient to achieve strong statistical guarantees. However, the current proof does not substantiate this claim. Specifically, in line 616 of the Supplementary Material, the Frobenius norm of the error is only bounded for sufficiently large \( n \).
This issue could potentially be resolved by considering fixed \( n \) and growing dimensions such that the radius in the definition of \( \mathbb{A} \) approaches zero. For instance, the scaling law discussed in line 294 of the main text satisfies this condition, which would preserve the validity of the conclusion. Only minor modifications to certain parts of the proof would be necessary.
(2) Several small errors and ambiguities are present in the Supplementary Material proofs:
2.1) Line 607: The term "boundary" of \( \mathbb{A} \) is unclear. Since \( \mathbb{A} \) is already a sphere-like set, discussing its boundary seems unnecessary.
2.2) Line 661: There is a typo in the second term of the first equality.
2.3) Line 669: The application of the inequality is unclear. Based on my understanding, it would suffice to use the sub-multiplicativity of the Frobenius norm, which directly leads to line 667.
2.4) Line 699: The term "primal-dual witness" appears to be incorrectly formatted.
2.5) In the proof of Lemma B.1, the constant \( \varepsilon \) is not explicitly defined. I recommend including its definition in the lemma statement to emphasize that the bound in Lemma B.1 depends on \( \| \varepsilon - \varepsilon^* \|_F \).
2.6) The main text mentions that the proof relies on Talagrand's concentration inequality. However, this inequality does not appear to be explicitly used. For example, Lemma C.2 is not Talagrand's concentration inequality but rather a classical Gaussian concentration result, which can be derived using tools like the Gaussian log-Sobolev inequality.
(3) Line 303 in the main text: The authors claim that their bound is minimax optimal. However, it is unclear with respect to which class this minimax optimality is defined.
(4) Line 323: The Hessian is defined as \( \Sigma^{-1} \otimes \Sigma^{-1} \)? This should be clarified.
This paper investigates the estimation problem under the assumption that the observations follow a tensor normal distribution with a separable covariance structure (\( \Sigma^ = \Sigma1^ \otimes \cdots \otimes \SigmaK^* \)). The authors analyze the classical penalized maximum likelihood estimator, which leads to a non-convex optimization problem. Remarkably, the main result of the paper demonstrates that this non-convexity can be effectively ignored by employing alternating minimization, yielding an estimator with strong statistical performance.