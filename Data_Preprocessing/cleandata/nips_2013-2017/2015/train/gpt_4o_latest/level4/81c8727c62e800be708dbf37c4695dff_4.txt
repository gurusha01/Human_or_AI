Paraphrased Review
Summary  
This paper provides a theoretical investigation into non-regularized supervised metric learning formulations. The authors examine two primary frameworks. The first, referred to as the distance-based framework, focuses on algorithms that optimize a distance function based on class label information. The second, termed the classifier-based framework, involves algorithms that optimize a metric tailored to specific prediction or ranking tasks.
For both frameworks, the authors derive PAC-style bounds (sample complexity bounds) to quantify the gap between the true risk and the empirical risk of the learned metric. Additionally, they analyze the discrepancy between the true risk of the learned metric and the true risk of the optimal in-class metric, emphasizing the significant dependence on the dimensionality of the data. In the latter part of the paper, the authors explore regularized formulations using the Frobenius norm as a regularizer. They refine their theoretical results in this context and argue that incorporating regularization improves the guarantees. Finally, the authors present an empirical evaluation of two widely known metric learning methods (ITML and LMNN), demonstrating that adding a regularization term enhances performance.
The paper is generally clear and well-written. The results are novel and contribute to the theoretical understanding of metric learning.
Comments  
- Proof of Lemma 2: The proof of Lemma 2 should be revised to eliminate its dependence on the dimensionality. Additionally, the authors should include comments linking Theorems 1 and 2. Currently, these theorems are presented in isolation, with formulations that do not allow readers to directly infer the implications of dimensional dependence (D).  
- Extension of Results: The authors should note that the results of Jin et al. (NIPS 2009) can be extended to replace the dependence on dimensionality (d) with a term based on data concentration (term B in the authors' paper), applicable to any distribution and convex regularized formulations. This extension can be achieved using the techniques of Bousquet & Elisseeff (JMLR 2002), provided the formulation is convex to ensure algorithmic stability. Relevant references include Bellet et al., Metric Learning (Morgan & Claypool, 2015, Chapter 8).  
- Non-Convex Loss: One of the paper's claims is the generalization of prior results to potentially non-convex losses. However, in such cases, there is no guarantee of finding the optimal model (matrix) that minimizes the risk. This limitation renders the results less informative in terms of practical applicability. It is also worth noting that even for convex formulations, handling all the quadratic constraints inherent in classical metric learning formulations is computationally expensive, often necessitating heuristics to reduce the number of constraints.  
- Impact of Dimensionality: An interesting direction for future work would be to investigate whether the dependence on dimensionality (D) has a more pronounced effect on non-convex, non-regularized formulations.  
- True Rank vs. Dimensionality: While the paper focuses on the dependence on the representation dimension (D), it would be valuable to also consider the true rank of the matrix (M) rather than the data dimension. This perspective could establish connections with metric learning approaches based on Cholesky decomposition of the learned metric and methods employing low-rank regularization. This point should at least be discussed as a potential avenue for future work.  
- Classifier-Based Framework: Regarding the classifier-based framework, the bibliography lacks references to methods that learn linear classifiers from well-designed similarity functions, even if these methods address different settings. Relevant references include Balcan et al. (Improved Guarantees for Learning via Similarity Functions, COLT 2008) and Bellet et al. (Similarity Learning for Provably Accurate Sparse Linear Classification, ICML 2012). This topic is also explored in the paper by Guo and Ying, which the authors have already cited.  
Overall, the results are compelling, and providing theoretical guarantees for classifiers leveraging learned metrics is crucial. Further research in this direction is warranted.  
The current results rely on covering number and fat-shattering dimension arguments. Is there potential to explore alternative frameworks, such as those based on Rademacher complexity?  
About the Experiments  
While the theoretical analysis does not address noisy data, the authors augment datasets with noise to evaluate the impact of regularization. The experiments suggest that regularization mitigates overfitting, a well-known issue, by selecting an appropriate regularizer. However, this conclusion is not particularly novel.  
- The scaling of the x-axis differs across the three figures. What is the rationale behind this choice? Specifically, what happens beyond 120/150 for the Iris and Ionosphere datasets?  
- For the comparison with ITML, it would be informative to report the accuracy achieved using only the rank-1 matrix in the distance metric to isolate the effect of the regularizer. The choice of this matrix appears to play a significant role.  
- The experimental evaluation is limited in scope, with only three datasets of relatively low dimensionality and few methods for comparison. Additional datasets, particularly those with higher dimensions, would strengthen the empirical results. For instance, the authors could consider datasets from UCI or those referenced in Shi et al. (Sparse Compositional Metric Learning, AAAI 2014).  
- No experiments are conducted for the classifier-based framework. Including such experiments would enhance the paper's comprehensiveness.  
- The authors could also present how the theoretical bounds evolve with respect to the data and dimensionality.  
Other Comments  
- Line 075: The authors state that they consider binary classification, but the loss function described later supports a multiclass setting. Moreover, some datasets used in the experiments are multiclass.  
- The definition of the true risk in the classifier-based framework uses a margin of 1/2, whereas the empirical risk allows the margin to vary between 0 and 1/2. Why is this discrepancy present?  
- Line 219: Typographical error: "do now know" should be corrected to "do not know."  
- In the proof of Lemma 4, the notion of covering is extensively used. Could the results be extended to other regularization norms?  
Pros:  
- Novel and original bounds for metric learning.  
- Interesting analysis with respect to the hypothesis space.  
- Theoretical results provided for the classifier-based framework.  
Cons:  
- Limited experimental evaluation.  
- Insufficient discussion of regularization and its connections to related work.