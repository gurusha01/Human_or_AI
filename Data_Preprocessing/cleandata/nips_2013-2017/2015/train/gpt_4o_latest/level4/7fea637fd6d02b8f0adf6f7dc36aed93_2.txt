The paper addresses two key problems:
(1) How does the social welfare of players employing regret minimization algorithms compare to the optimal welfare?  
(2) Can improved regret bounds be achieved when all players adopt a regret minimization algorithm?
Previous work has established that, in zero-sum games, the social welfare converges to the optimal welfare at a rate of 1/T. However, the question remained unresolved for general non-zero-sum games. The authors demonstrate that, under certain smoothness assumptions on the game (as introduced by Roughgarden), the same convergence rate of 1/T can be achieved when all players use a regret minimization algorithm satisfying the RVU property. Furthermore, they show that the optimistic mirror descent algorithm proposed by Rahklin possesses this property. Additionally, they identify that an optimistic variant of FTRL (OFTRL) with a recency bias also satisfies the RVU property.
For problem (2), the authors establish that the regret of each individual player is bounded by O(T^{1/4}) when employing their variation of FTRL, as corroborated by their simulations. Moreover, they enhance their algorithm using a doubling trick, ensuring that when all players use OFTRL, their regret grows at a rate of O(T^{1/4}). At the same time, the regret does not exceed O(T^{1/2}) when players encounter adversarial rewards.
The paper is exceptionally well-written and addresses a compelling problem. I particularly appreciate the generality of the results derived from the RVU property. The work focuses on analyzing regret bounds for regret minimization algorithms in games. While the standard regret bounds for such algorithms are typically O(\sqrt{T}), this assumes that the learner faces a fully adversarial opponent. However, it is more natural to consider scenarios where all participants in a game employ regret minimization algorithms. The authors convincingly show that, in this setting, regret bounds as tight as O(T^{1/4}) can be achieved for general games.