The manuscript presents the covariance-controlled adaptive Langevin thermostat (CCAdL), a Bayesian sampling approach leveraging stochastic gradients (SG) to address correlated errors arising from the SG approximation of the true gradient. The authors provide evidence that CCAdL outperforms other SG-based methods in terms of accuracy and robustness across a variety of test cases.
Overall, the paper is well-written, though it may be somewhat challenging to follow for readers unfamiliar with this class of sampling algorithms. The introduction begins with a review of several SG-based methods for efficient Bayesian posterior sampling (SGDL, mSGDL, SGHMC, SGNHT). To enhance clarity, the inclusion of a table or figure summarizing these SG variants and emphasizing their similarities and differences would be highly beneficial.
CCAdL builds on previously proposed concepts, particularly those from SGHMC and SGNHT, and incorporates the following key components:
- A stochastic approximation of the true gradient of the log posterior, obtained via data sub-sampling, which introduces "noise" in the gradient that must be managed.
- An estimation of the covariance matrix of the gradient noise, computed as a running average of the Fisher scores. For high-dimensional problems, this is approximated using a diagonal matrix.
- The use of a thermostat mechanism to address the inefficiencies of the Metropolis Monte Carlo acceptance/rejection step employed in standard HMC.
As someone not deeply specialized in this domain, it is somewhat unclear what the precise novelty of CCAdL is. The method appears to integrate elements from SGHMC and SGNHT with an existing estimator for the noise covariance matrix.
The authors evaluate CCAdL on a logistic regression problem, demonstrating that it converges significantly faster to higher log-likelihood values. Additionally, CCAdL remains stable for friction values smaller than those required by SGHMC and SGNHT. By examining the marginal distributions of parameter pairs, the authors show that CCAdL closely approximates the "true" posterior distribution obtained via HMC on the full likelihood. In a second large-scale experiment, the authors train and test discriminative RBMs on three datasets, again finding that CCAdL outperforms SGHMC and SGNHT across most step sizes and friction constants.
The results presented are compelling and merit publication. However, improving the paper's accessibility for non-experts and addressing minor issues would enhance its overall quality. For instance, some symbols, such as $\mu$ and $dW_A$, are introduced without adequate explanation. Clarifying these details would improve the manuscript's readability.