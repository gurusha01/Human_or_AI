I have a concern regarding the fact that, in the proof of Theorem 2, Î» and L are defined to depend on d. What are the consequences of this assumption? Is it a practical or realistic choice?  
For a potentially relevant additional reference, the following paper addresses the same loss function as the one presented in (4) and proposes an efficient algorithm for solving it globally:  
"Scalable Metric Learning for Co-embedding," Mirzazadeh et al., 2015, ECML.  
This paper is well-written and offers novel theoretical insights into the sample complexity of metric learning, as well as the benefits of incorporating additional regularization. It thoroughly examines the topic both in general and specific scenarios from a theoretical perspective. Furthermore, the empirical results are concise yet systematic, and they effectively validate these findings on real-world datasets.