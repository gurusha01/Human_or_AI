The paper addresses an algorithm for solving the optimization problem
\[
\min l(x) + r(x)
\]
where \( l(x) \) is a smooth function and \( r(x) = \sum \rho(|x_i|) \), with \( \rho \) being a concave smooth function. The authors propose a hybrid algorithm that integrates a quasi-Newton (QN) step with a gradient descent (GD) step. The core idea of the algorithm is inspired by the OWL-QN algorithm, wherein the iterates are constrained to remain within the same quadrant to handle the non-smooth nature of the regularizer. However, due to the non-convexity of the regularizer, subgradient properties cannot be directly applied. Instead, the theoretical analysis relies on the properties of the Clarke sub-differential.
I find the problem addressed in the paper to be non-trivial, as the non-convexity of the regularizer adds a significant layer of complexity to the already challenging issue of non-smoothness. That said, I have identified three key concerns with the paper:
1. While the authors critique DC programming approaches for non-convex optimization, those methods offer linear convergence guarantees. In contrast, the proposed algorithm does not provide any explicit convergence rate guarantees.
2. To ensure convergence, the algorithm necessitates a gradient descent step. However, determining the optimal balance between the GD step and the QN step appears to require running the algorithm with various tradeoff parameters, which may be computationally expensive and impractical.
3. From a practitioner's perspective, I have neither encountered nor utilized non-convex regularizers of the type described in this paper. While this may be due to the specific nature of my applications, I question whether this problem is of significant relevance to the broader machine learning community.
Overall, the paper tackles the minimization problem under a non-convex regularizer designed to promote sparsity. The writing is clear, and the mathematical derivations are correct. However, I remain uncertain about the practical relevance and impact of the results for the machine learning community.