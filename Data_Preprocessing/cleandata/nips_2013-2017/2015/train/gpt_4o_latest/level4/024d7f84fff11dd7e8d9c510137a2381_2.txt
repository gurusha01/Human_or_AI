This paper introduces methods for estimating policy gradients in risk-sensitive scenarios using coherent risk measures and proposes reinforcement learning (RL) algorithms based on these methods. The work extends existing approaches for specific risk measures to the broader class of coherent risk measures, thereby offering a unified framework. The proposed algorithm is demonstrated using a toy example in the finance domain, where each asset is characterized by its own return distribution.
The paper is well-motivated, with the problem formulation and algorithms logically stemming from the stated motivation and prior research. While the experiment is relatively simple, being limited to a static risk problem, the results effectively highlight the algorithm's risk-averse behavior and its adaptability to various types of risk measures.
Quality: The paper is well-written, with technically sound equations and comprehensive references to relevant literature.
Clarity: The paper is exceptionally clear, with clarity well above the average standard.
Originality: The paper introduces novel algorithms for risk-sensitive RL problems, building on prior work in risk-sensitive planning and RL. The extension to the general class of coherent risk measures is non-trivial, and the paper demonstrates sufficient originality in its contributions.
Significance: Risk-sensitive RL is a critical and engaging area of research, and this paper makes meaningful contributions to the field. It provides well-defined algorithms for addressing risk-sensitive RL with coherent risk measures and showcases the behavior of trained policies under different risk settings in a static risk problem. The proposed approach is well-motivated and represents a natural extension of existing algorithms and frameworks, including policy gradient, actor-critic methods, and coherent risk measure optimization.