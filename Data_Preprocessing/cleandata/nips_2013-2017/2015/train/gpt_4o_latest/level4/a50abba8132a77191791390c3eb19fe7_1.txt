This paper builds upon the stochastic optimization algorithm SVRG introduced in recent years by incorporating several modifications, including: analyzing the convergence of SVRG with corrupted full gradients; combining SGD and SVRG iterations; adopting a mini-batch strategy; and leveraging support vectors. For each modification, the authors provide rigorous proofs and demonstrate linear convergence under smooth and strongly convex assumptions. However, the paper's novelty is limited. The improvement in convergence rate is marginal, and the proof structure closely resembles that of the original SVRG. Moreover, critical issues, such as extending support to non-strongly convex loss functions, remain unresolved. Additionally, minor errors in the notations are present in the appendix. 
This work introduces practical enhancements to SVRG, supported by theoretical analysis and experimental validation. Nonetheless, the contributions in terms of novelty and performance gains are modest. The proof techniques largely mirror those of the original SVRG, and significant challenges, like addressing non-strongly convex losses, are not tackled.