The problem
This paper investigates the problem of Isotonic Regression under the $\ell_p$-norms, where $1 \leq p \leq \infty$. Given a directed acyclic graph (DAG) $G(V,E)$, observations $y \in \mathbb{R}^{|V|}$, and a weight vector $w$, the isotonic regression problem is formulated as the following minimization problem (stated in line 053):  
\begin{eqnarray}  
\minx \|x-y\|{w,p} \quad \text{such that} \quad xu \leq xv \quad \text{for all} \quad (u,v) \in E,  
\end{eqnarray}  
where $\|\cdot\|{w,p}$ represents the weighted $\ellp$-norm.
The results
By allowing a small error $\delta$ from the optimal solution, Theorem~2.1 establishes a time complexity bound of $O(m^{1.5} \log^2 n \log(npw{\max}^p/\delta))$, which holds with high probability for $1 \leq p < \infty$. For $p < \infty$, Table 1 compares the time complexities of this work with prior results that require exact solutions. For $p = \infty$ and a variant called Strict Isotonic Regression (defined in line 079), Theorems 1.2 and 1.3 provide upper bounds on the time complexity for computing exact solutions. These bounds improve upon previous results, except for the $\ell1$-norm in two-dimensional space ($V \subset \mathbb{R}^2$), where an additional constraint on the number of edges $|E|$ applies.
The authors reformulate the original regression problem into an instance solvable by an approximate interior point method (\textsc{ApproxIPM}). By demonstrating the efficiency and accuracy of a key subroutine of \textsc{ApproxIPM}, called \textsc{BlockSolve}, which computes an approximate Hessian inverse, the proposed algorithm achieves improved time complexity for $1 \leq p < \infty$. The paper's contribution lies in generalizing a result for linear programs from [23] to $\ellp$ objectives and providing an enhanced analysis. For the $\ell\infty$ Isotonic Regression and Strict Isotonic Regression, the authors reduce these problems to Lipschitz Learning problems (defined in [29]) and apply the algorithms from [29] to compute solutions.
The paper also includes preliminary experimental results on the proposed algorithm, presented in Table 2.
Comments
The theoretical contributions of this paper are incremental. The main contributions are the problem reductions and the design and analysis of the critical subroutine \textsc{BlockSolve}, which efficiently computes an approximate Hessian inverse. Many of the mathematical techniques used in the analysis are standard in convex optimization, interior point methods, and the referenced works cited in the paper.
The classification of this paper as either theoretical or experimental is unclear. The experiments in Table 2 are preliminary and lack comparisons with state-of-the-art algorithms. Furthermore, the main algorithm and its analysis are not clearly presented in the main body of the paper, though they are available in the supplementary material. The paper would benefit from restructuring to improve clarity and presentation.
Typos and undefined notations
- Line 056: The condition should be $m \geq n-1$ for a connected graph, rather than $n \geq m-1$.  
- Line 342: The failure probability should be $n^{-3}$ instead of $n^3$.  
- Line 663: $\textsc{Solve}{HF}$ is undefined, making the proof of Theorem 2.7 difficult to follow.  
- Line 716: \textsc{Solve} is undefined, making the proof of Lemma A.5 unclear.  
- Line 722: There may be an extraneous $z$.
Quality
From a theoretical standpoint, this paper is acceptable. However, the experimental results are insufficient for publication. The overall presentation of the paper (considering only the main body and excluding the supplementary material) places it on the borderline between acceptance and weak rejection.
Clarity
The main algorithm, along with its critical analysis and key ideas, is missing from the main body of the paper. This omission makes it challenging for readers to fully understand the work.
Originality and Significance
This work represents an incremental contribution to the study of Isotonic Regression. The paper provides modest improvements for Isotonic Regression under $\ell_p$-norms. The experimental results are preliminary, and the main contributions related to the algorithm's design and analysis are not included in the main body (though they are available in the supplementary material).