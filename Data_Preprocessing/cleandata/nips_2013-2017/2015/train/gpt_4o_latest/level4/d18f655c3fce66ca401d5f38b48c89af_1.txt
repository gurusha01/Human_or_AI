In this paper, the authors introduce a distributed stochastic gradient learning framework designed to accommodate infrequent synchronization. They argue that the non-convex nature of the problem renders existing algorithms less robust to distant synchronization intervals. Additionally, the proposed method's tolerance to discrepancies in parameter values across nodes promotes exploration, ultimately leading to improved solutions.
The proposed algorithm is straightforward and demonstrates promising performance in the experiments. The authors benchmarked their approach against several existing algorithms and explored a range of values for tau. On a minor note, while the paper discusses deep learning in general terms, it would have been valuable to include experiments on architectures beyond CNNs.
My primary concern with this paper is that, while it revisits a well-established method (refer to section 4.1 of "Notes on Big-n Problems" by Mark Schmidt, an excellent survey of the field), the discussion of prior implementations of similar methods is limited. These are briefly mentioned in the introduction, but the detailed analysis is largely confined to ADMM.
I believe that achieving positive results in distributed stochastic optimization for non-convex loss functions could have significant implications. However, the authors need to provide a more thorough review of the existing literature rather than merely citing it. While the algorithm appears to outperform current distributed techniques, the paper lacks sufficient comparisons with distribution methods outside the deep learning domain, which have been extensively studied in the optimization literature.