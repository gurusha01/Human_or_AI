"Interactive control of diverse complex characters with neural networks" demonstrates how a compact and efficient neural network can represent a control policy for interactive character control within a physics simulator.
The proposed method leverages trajectory optimization during the training phase to develop a neural network policy. The primary contribution lies in showcasing that the same neural network architecture is effective across a variety of cyclic and non-cyclic behaviors, as well as across different body types.
General Comments
My interpretation of this paper is that it applies deep learning techniques to enhance existing (albeit recent) methods in computer graphics.
Previous research has already established the capability of trajectory optimization to generate realistic control signals, as acknowledged in the citations provided by the authors.
Similarly, the use of neural networks to approximate step-by-step policy actions that replicate trajectories derived from trajectory optimization has been explored in earlier works (e.g., [9]).
As such, the authors need to better contextualize their contributions in relation to these prior approaches.
The most comparable body of work, in my opinion, is the recent research by Pieter Abbeel and Sergey Levine. How does the proposed approach compare to theirs? Is it designed to achieve faster learning? To accommodate a broader range of body morphologies? To reduce computational overhead during runtime? Or to enhance robustness against environmental variability?
Specific Comments
- What do the authors mean by "we found the resulting process can produce very large and ill-conditioned feedback gains"? Please elaborate or clarify.
- Consider postponing the discussion of why LQG was not used until after the presentation of your proposed strategy.
- The notation in lines 226-240 is unclear. For instance, you introduce \( s(X) = \bar{s} \), followed by a cost function defined in terms of \( s(X) - \bar{s} \). The visual similarity between the bars and squiggles makes this notation confusing.
- What is meant by "our approach produces softer feedback gains according to parameter \(\lambda\)"? Softer compared to what? Additionally, what does it mean for feedback gains to be "soft" or "not soft," and why is this distinction significant?
- Why is altering the initial state as part of your procedure advantageous?
While the work is technically sound, the novelty and contributions relative to other recent, closely related research remain insufficiently clear.