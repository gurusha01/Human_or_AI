This paper introduces the use of LSTM units as a technique for training extremely deep neural networks.
The paper is of high quality and is well-written. The proposed idea is novel, and the results are highly compelling.
In the field of machine learning, researchers often observe improved performance by increasing the depth of their neural networks. However, challenges such as initialization issues and the vanishing/exploding gradient problem have posed significant barriers. These challenges have arguably hindered the widespread adoption of deep neural networks for nearly three decades. This work directly tackles these issues and presents a method capable of training networks with 100 layers.
Notably, the experiments that highlight the routing of information and the significance of individual layers are particularly impressive.
Specific comments:
In Section 4.1, the statement 'most transform blocks are active on average' appears to be incorrect and should likely be revised to 'not active on average.'
Additionally, one could consider initializing each layer of a very deep network with the identity matrix (potentially with a small amount of noise). It may be valuable to discuss how transform gates in highway networks differ from this straightforward initialization approach.
Overall, this is an excellent paper that showcases a method for training very deep neural networks. It was a pleasure to review.