This paper presents several strategies aimed at accelerating the convergence of the well-known stochastic gradient method, stochastic variance-reduced gradient (SVRG).
The manuscript is well-structured and the arguments are presented in a clear and comprehensible manner. The authors begin with a concise introduction to the SVRG method and then detail three distinct approaches to enhance its convergence speed.
The paper opens with a key proposition demonstrating that SVRG does not require a highly precise approximation of the total gradient of the objective function used in the SVRG algorithm. Building on this result, the authors develop a batching SVRG algorithm that achieves the same convergence rate as the original SVRG. Next, they introduce a hybrid stochastic gradient/SVRG approach and provide a formal proof of its convergence. Additionally, as an alternative acceleration strategy, the authors propose a speed-up technique tailored for the Huberized hinge-loss support vector machine.
Beyond these acceleration techniques and their convergence analyses, the paper also addresses the regularized SVRG algorithms, offering similar convergence analyses for these cases. The authors present propositions regarding the convergence of mini-batching strategies. Through extensive simulations, they effectively demonstrate the performance improvements achieved by their proposed methods.
Aside from the minor comments listed below, I do not have any significant suggestions for improving the manuscript.
Minor Comments:  
* On the right-hand side of the inequality in line 77, an expectation should be included.  
* The question marks in the first line and the middle of page 10 should be corrected.  
The topic of this paper is highly relevant to the NeurIPS community, and the theoretical analyses and simulation results are executed flawlessly.