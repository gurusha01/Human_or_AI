Paraphrased Review:
Summary:  
This paper introduces a method for jointly optimizing all parameters of decision trees, including decision splits and posterior probabilities. This is a significant contribution as most existing approaches train decision trees in a greedy, layer-by-layer fashion. The authors reformulate the task as a single optimization problem, which is then upper-bounded and approximated to derive a tractable solution.
Quality:  
The manuscript is well-written, though there are potential issues in the reasoning, as detailed below.  
Clarity:  
The derivation is presented clearly, but the experiments and their presentation could be enhanced.  
Originality:  
The exploration of joint training for decision trees addresses an important and underexplored topic.  
Significance:  
The proposed approach is valuable, though it appears to have some limitations. See the comments below for more details.
Comments:  
- Potential issue with the derivation of the empirical loss bound: The argument in lines 216f is at least unclear, if not flawed. Specifically, the statement "`... for all values of $\bg \neq \sgn(W\bx)$, the right-hand side can only become larger than when $\bg = \sgn(W\bx)$ ...`" is confusing. Consider the following example:  
  \( W\bx = [5; -5] \)  
  \( \hat{\bh} = [1; -1] = \sgn(W\bx) \)  
  \( \hat{\bh}^T W\bx = 10 \)  
  If we choose \( \bg = [1; 1] \), then \( \bg^T W\bx = 0 \). Consequently, the right-hand side of Eq. (7) becomes:  
  \( \bg^T W\bx - \hat{\bh}^T W\bx + \ell(...) = -10 + \ell(...) \),  
  which could potentially be smaller than \( \ell(...) \) alone, thereby failing to serve as a valid upper bound. Furthermore, the proof of Proposition 1 in the supplementary material does not address this scenario, leaving it incomplete.  
- Unassigned data points in some leaves: It is unclear why certain leaves are not assigned any data points. This behavior seems unusual and might suggest an issue with the objective function or the initialization strategy. The fixed assignment of data points to tree leaves, as proposed, appears suboptimal. Could this be better justified using a framework like expectation maximization or a concave-convex procedure, as is common in other clustering methods?
- Efficiency of loss-augmented inference: The authors emphasize the design of efficient loss-augmented inference, but its necessity is unclear. Given that the tree depth is at most 16, evaluating all \( 2^{16} = 65,536 \) possible values seems computationally feasible. How much does the restricted search space imposed by the Hamming ball affect performance? While one result is provided in the supplementary material, does this generalize? Additionally, the term "`leaves`" is ambiguous and could benefit from clarification.
- Missing baseline: The paper lacks a baseline that trains decision trees with non-axis-aligned split functions in the standard greedy manner, i.e., without initialization. Including such a baseline would provide a more comprehensive comparison.
- Tree depth specification: The need to predefine the tree depth is a limitation that warrants further discussion. A more thorough experimental evaluation of the impact of the depth parameter would also strengthen the paper.
Additional Suggestions:  
- Thank you to the authors for providing a detailed explanation of the reasoning behind the loss bound (Eq. 7). You may want to replace \(\arg\max\) with \(\max\) for clarity.  
- An experiment comparing runtime performance would add value to the paper.  
- A more detailed evaluation of the proposed method would benefit readers.
Conclusion:  
This submission tackles the compelling problem of jointly training all parameters of decision trees. While the idea is promising, the experimental evaluation is somewhat limited, and there may be a flaw in the derivation. I am open to revising my score based on a thorough explanation in the rebuttal.