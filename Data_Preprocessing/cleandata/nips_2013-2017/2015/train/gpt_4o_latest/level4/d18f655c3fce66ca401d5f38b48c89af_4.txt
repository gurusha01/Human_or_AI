The paper presents the EAMSGD algorithm, designed to accelerate the convergence of distributed SGD. The approach builds upon several foundational works in distributed optimization and establishes a novel connection with deep learning models. Experimental results on public datasets, including CIFAR and ImageNet, are robust and clearly presented.
Quality: This is a high-quality paper featuring strong ideas and solid experimental results.
Clarity: The paper is well-organized; however, more explicit and concise descriptions of the algorithm would enhance readability. For example, the discussion of different distributed SGD variants at the beginning of Section 5 lacks clarity. A table format or a categorized list of the available algorithms could make this section more comprehensible and highlight the results more effectively.
Originality: While the core idea is derived from established classical works, its application to modern deep learning models and datasets is novel and represents a meaningful contribution.
Significance: Given the growing importance of distributed deep learning in both industry and academia, this paper is likely to attract significant interest from engineers and researchers.
The proposed elastic averaging method for distributed SGD demonstrates strong empirical performance in image classification tasks. Additionally, the authors provide theoretical insights and mathematical formulations by drawing parallels to ADMM. The research is innovative, and the results are compelling. Apart from minor experimental details that could be refined, this is a strong paper, and I recommend it for publication at NIPS.