A sparse EM algorithm is introduced, which extends the classical EM by truncating the parameter vector at each iteration to enforce a desired sparsity level. Additionally, the authors propose an asymptotically normal test statistic, derived from a decorrelated score, to assess whether a specific parameter entry is non-zero.
The sparse EM is shown to converge geometrically to the true parameter value (in terms of iterations), up to a statistical error term. This error term scales with the square root of the sparsity level and the infinity norm of the statistical error in the M-step, which is typically only logarithmic in the ambient dimension.
This work builds on recent results for EM in low-dimensional settings. Similar to prior studies, it assumes that the Q function is sufficiently smooth and approximately quadratic within a certain basin of attraction, and that the algorithm is initialized within this basin. A new requirement is introduced: control over the infinity norm of the error in estimating M. While this condition applies only to sparse vectors, for many models, estimating M for all vectors (sparse or not) is no more challenging than for sparse vectors alone (as demonstrated in the Gaussian mixture example and the non-gradient version of the mixture of regression model). This raises a question: is the sparse EM algorithm strictly necessary? Could comparable guarantees be achieved by applying the standard EM algorithm under the same conditions, followed by a single truncation step? EDIT: The authors' argument that linear regression is a special case here provides a compelling counterpoint to the single truncation step approach.
There may be relevant prior work in sparse optimization that warrants discussion, such as "Sparse online learning via truncated gradient" by Langford et al. (2009), as one example.
The experimental section should clarify the initialization method used. A sparse EM variant is presented for high-dimensional parameter estimation, and the convergence guarantees are a significant contribution to high-dimensional learning. However, the necessity of the sparse EM variant for achieving these results remains uncertain.