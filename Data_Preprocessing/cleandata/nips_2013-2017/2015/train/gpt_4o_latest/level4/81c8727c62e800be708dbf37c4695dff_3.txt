This work represents one of the most rigorous and well-executed studies I have encountered on sample complexity for metric learning. As the authors highlight, it offers several distinct advantages over prior research in this domain.
Notably, I appreciate that the authors address multiple significant scenarios: both the "distance-based" and "classifier-based" frameworks. These effectively illustrate the dependence on dimensionality and also consider cases where noisy or uninformative dimensions may be present.
The paper is well-written, and its results appear to be sound.
Given the relative scarcity of research of this nature within the metric learning sub-community, I believe this work merits publication.
While the experiments are somewhat underwhelming, this is primarily a theoretical paper, so this limitation is not a major concern.
That said, I did notice a couple of points worth mentioning: there has been considerable prior work on the type of regularizer proposed in this study (the ||M^T M||_F^2 regularizer). For examples, see section 2.4.1 of "Metric Learning: A Survey." It might be beneficial to discuss these connections in more detail.
Additionally, both ITML and LMNN can already be interpreted as incorporating regularization on the M matrix. For instance, ITML is often associated with LogDet regularization on M^T M, while LMNN is typically viewed as using a weighted trace norm regularization on M^T M. This raises the question of whether these were the most appropriate baseline methods for evaluating the proposed regularization.
Furthermore, the paper states that ITML is initialized with a rank-one metric. Given that the LogDet regularizer preserves rank (i.e., the rank cannot change from the initialization), this would result in a low-rank solution, which might be undesirable depending on the experimental setup. If my understanding is correct, this aspect could benefit from clarification.
These are relatively minor points, however, and they do not detract from the overall quality of the work. The paper provides a clear and insightful exploration of sample complexity for metric learning, presenting several important results on Mahalanobis metric learning. It is a valuable contribution to the metric learning literature and deserves to be published.