%%%% post-rebuttal comment %%%%
I have raised the score from 6 to 7 to more accurately reflect my updated impression of this paper.
%%%%
== Summary ==
This paper introduces a kernel-based approach for cross-domain matching of bag-of-words data. The challenge lies in the fact that features across domains may differ, making direct comparison infeasible. A common solution involves learning a "shared latent space," as seen in methods like CCA, KCCA, and topic modeling. However, this paper takes a different perspective by modeling the data—such as documents and images—as probability distributions over vectors in the shared latent space. This enables the proposed method to account for the co-occurrence of distinct but semantically related features across instances. The latent vectors are derived by maximizing the posterior probability, which is expressed using the RKHS distance between instances (i.e., maximum mean discrepancy). The experimental results, conducted on several real-world datasets such as bilingual documents, document-tag matching, and image-tag matching, are highly promising.
In summary, the paper presents a straightforward and intuitive approach to the cross-domain matching problem, supported by strong empirical results.
== Quality ==
The main limitation of this paper lies in its technical depth.
== Clarity ==
The paper is well-written and clear overall, though I have a few minor suggestions for improvement.
In the experiments section, you refer to "development data." Does this term correspond to validation data? [Line 185–189] This paragraph seems to imply that Xi and Yj belong to different latent spaces. Please clarify. Eq. (9): What is the rationale behind this prior? The motivation should be explained. In Eq. (11), it would be helpful to mention that the chosen prior corresponds to L_2 regularization on x and y. This aligns with the formulation used in Yuya et al. (NIPS 2014). Again, the reasoning behind this choice should be clarified.
== Originality ==
The proposed method appears to be conceptually similar to Yuya et al. (NIPS 2014), but it focuses on cross-domain matching rather than classification. Nonetheless, the idea of treating data as probability distributions over a latent space is intriguing and has potential applications in various domains.
== Significance ==
The approach proposed in this paper has broader applicability. For instance, it could be extended to "representation learning," where the latent space might exhibit a hierarchical structure (e.g., a tree) or represent parameters of a generative model such as an RBM or a deep neural network.
Relevant papers:
Y. Li, K. Swersky, R. Zemel. Generative Moment Matching Networks. ICML 2015.  
G.K. Dziugaite, D. Roy, Z. Ghahramani. Training generative neural networks via Maximum Mean Discrepancy optimization. UAI 2015.
== Minor Comments ==
Eqs. (15) and (16) appear redundant, as they are essentially equivalent to Eqs. (12) and (14). Consider removing them to conserve space.  
Overall, this is a simple and intuitive approach for cross-domain matching of bag-of-words data, supported by strong empirical results.