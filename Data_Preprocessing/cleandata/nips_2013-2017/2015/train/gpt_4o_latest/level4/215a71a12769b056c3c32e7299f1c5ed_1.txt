I appreciate the paper, as the idea is clearly articulated, and the experiments are reasonably convincing. What stands out most to me is the authors' effort to analyze the learned networks in terms of the gate output patterns.
(1) The gate unit T employs a standard sigmoid activation function, which means the response values are never precisely 0 or 1 (as acknowledged by the authors). Consequently, the gradients in Equation 5 are incorrect. The authors should provide a detailed explanation in the paper regarding how backpropagation is specifically implemented in these networks.
(2) While analyzing the effect of initialization is inherently challenging, the authors' proposal to use constant negative biases based on the number of levels is not entirely satisfactory. A plot showing the performance as a function of the initial bias value would be helpful. In this context, the initial bias acts as a form of regularization, where a strongly negative bias favors networks with "longer highways" (a comparison I find particularly insightful).
(3) The phrase "report by recent studies in similar experimental settings" is unsatisfactory. Either the methods share the same experimental setup, enabling direct comparison, or they differ, in which case a direct comparison is invalid. The authors should explicitly clarify the distinctions between the experimental setups in the paper.
(4) Reporting "max" accuracies in tables is uncommon and statistically questionable. The authors should instead provide the mean accuracy and standard deviation for each method (where applicable).
(5) It would be valuable to explore the proposed approach from a learning theory perspective. The paper outlines how additional gate units in neural networks enable layers to function as identities. Since the gate unit response is input-dependent, this mechanism facilitates the propagation of inputs through a large number of highly selective layers.