The primary contribution of this paper is a more theoretically grounded formulation of the on-the-job learner introduced in [22]. Initially, the learner relies on crowd input to generate predictions but progressively transitions to making its own predictions, querying the crowd only sparingly. The decision to query the crowd for a given input is framed as a decision problem, which is addressed using Monte Carlo Tree Search.
The presentation is generally clear but lacks critical details in certain areas. The empirical results, when compared to a threshold-based heuristic, are somewhat underwhelming. Nevertheless, the proposed approach appears promising and warrants further exploration.
Please address or clarify the following points:
1. Equation (2) is unclear. The left-hand side should also be conditioned on \( s \). Additionally, how does \( y \) in \( p{R}(ri | y, q_i) \) disappear?
2. What does \( N(s) \) represent in Algorithm 1?
3. How were the values 0.3 and 0.88 determined for the threshold baseline?
4. I am not familiar with the specific tasks used in this study. Are the "online" systems considered the best machine-learned systems for these tasks? If not, why? If they are, then any improvement observed is likely attributable to human annotation. Could you provide examples of such cases? An error analysis would be helpful.
This paper proposes a theoretical framework for an "on-the-job learner" that begins as a crowdsourcing system and gradually evolves into a self-reliant model, querying the crowd only for low-confidence inputs. While the empirical results are mixed, the work opens up an interesting avenue for discussion and further investigation.