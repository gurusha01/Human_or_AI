The paper tackles the significant challenge of parallel optimization. The proposed algorithm demonstrates improved tolerance to staleness, potentially reducing the communication overhead compared to state-of-the-art methods. Additionally, it achieves superior learning outcomes as evidenced by lower test error. Overall, the paper is well-written and enjoyable to read.
In the current EASGD framework, the center variable is updated using a symmetric elastic force (Eq.4). However, given the objective in Eq.2, a more intuitive approach might involve always computing the exact average of all local variables (still in an online manner). I am curious whether the authors have explored this alternative and would appreciate its discussion in the paper.
Have the authors conducted any experiments to analyze the impact of rho? Specifically, how does the exploration parameter influence performance?
Regarding the line under Eq.4: does it refer to the stochastic gradient of "F"? The paper introduces a parallel algorithm designed to promote simultaneous exploration across computing nodes, thereby enhancing the optimization of objectives with numerous local optima. The proposed approach is well-motivated, clearly articulated, and supported by comprehensive experimental evaluations.