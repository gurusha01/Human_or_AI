While the irrepresentability condition (IC) has been extensively utilized in prior literature [29,30], it would be beneficial to include a brief explanation of the IC in the specific context of learning Gaussian MRFs, as discussed in [30].
The authors should also clarify the dependence of Theorem 3.5 on \alpha in condition (3.4). A short discussion on how a random initialization satisfies this condition would be highly valuable. It is possible that the assumption is straightforward, with \alpha merely acting as a multiplicative factor in the result of Theorem 3.5, but this is not immediately evident from the current presentation in the paper.
Furthermore, the authors are strongly encouraged to explore potential extensions of their method to their specific problem and possibly to other machine learning applications.
There are two statements in the paper that appear to suggest that learning a simple Gaussian MRF with a single sample guarantees consistency. Specifically: Line 060: "statistical rate of convergence in Frobenius norm, which is minimax-optimal since this is the best rate one can obtain even when the rest K-1 true precision matrices are known [4]." Line 066: "our alternating minimization algorithm can achieve estimation consistency in Frobenius norm even if we only have access to one tensor sample." These statements seem to require further qualification, as it appears that either both need additional clarification or one of them is incorrect (e.g., certain parts of the proof might only hold for sufficiently large n, or some assumptions might impose dimension-independent bounds on the Frobenius norm of the Gaussian MRF matrices).
Minor typos (with additions in [ ] and deletions in { }): Line 177: "is [a] bi-convex problem" Line 181: "by alternatively updat{e}[ing] one precision matrix with [the] other matrices fixed" Line 190: "corresponds to estimating [a] vector-valued Gaussian graphical model."
  The paper presents strong theoretical results on sample complexity and a well-designed experimental setup.