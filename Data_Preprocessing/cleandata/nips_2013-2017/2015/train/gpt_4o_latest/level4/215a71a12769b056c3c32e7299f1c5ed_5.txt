I also considered employing the LSTM-inspired technique of gradient trapping to enable the construction of deeper networks, but I have not yet pursued this line of work. I commend the authors for being the first to successfully implement this approach!
There might be an alternative perspective worth considering: a very deep network can be viewed as a not-so-deep network where each layer is composed of sub-layers, resulting in more complex non-linear behavior compared to traditional rectifier or sigmoid layers. This perspective aligns with how I interpret the inception of architectures like GoogLeNet, the 3 stacked 3x3 convolutions in VGG, and potentially the highway network as well. I wonder if this viewpoint is applicable here.
I would suggest one additional experiment for the authors to explore: using a single thin and tall network to train on both MNIST and CIFAR datasets, with a fork at the top to handle the two domains. My interest lies in observing whether, during inference, the network exhibits similar patterns to those seen in networks trained separately for each domain. Specifically, I am curious to see if the parameters underutilized by MNIST are effectively leveraged by CIFAR. This could provide insight into how parameter sharing occurs in a very deep hierarchical network and whether it is optimized for efficiency.
I have no significant criticisms of the paper. I encourage the authors to test their network on larger datasets, such as ImageNet, to further validate its scalability and performance.
This paper presents an idea, inspired by LSTM, that facilitates the training of very deep feedforward networks. The proposed approach is both simple and elegant, and the accompanying analysis is thorough and well-executed.