The authors propose an innovative approach to factor analysis by leveraging posterior regularization, as introduced by Ganchev et al., to enforce non-negativity constraints on the posterior. They provide proofs of convergence and correctness, alongside a scalable framework for inference and learning in stacked constrained factor analysis models.
The proposed method is conceptually sound and represents a promising direction, offering a refreshing departure from much of the existing literature. To my knowledge, this is the first instance where posterior regularization has been applied in the context of deep learning and unsupervised feature learning. While I did not examine the supplementary material in exhaustive detail, it is impressively thorough and reflects a high degree of rigor.
My primary concern lies with the empirical evaluation, which feels somewhat underwhelming when compared to the community's standards. This makes it challenging to fully assess the contribution from an empirical standpoint.
Specifically, one question that arises is whether these models, particularly in the single-layer case (and likely in the multi-layer case as well), could be evaluated using test set likelihood. For example, the quantitative evaluation of unsupervised methods on MNIST is often framed in terms of test set likelihood. While synthetic data results are presented using metrics designed to showcase the model's ability to meet the design goals of RFNs, these metrics are somewhat difficult to interpret. Additionally, the classification baselines used appear slightly outdated. For instance, Komer et al. (2014) report a test set error of 11.7% on CONVEX using a combination of PCA preprocessing and a polynomial SVM. Although this result is relatively obscure, unsupervised learning on datasets like CIFAR has seen significant progress, often leveraging domain-specific architectures.
- The statement "Current unsupervised deep learning approaches like autoencoders or restricted Boltzmann machines (RBMs) do not model specific structures in the data" is unclear. Individual units in these models can indeed capture "specific structures." Additionally, the subsequent reference to "generative models" is imprecise, as RBMs and deep belief networks derived from them are generative models. Greater clarity is needed in this section.  
- I am curious about the authors' perspective on the success of representations that abandon sparsity altogether yet perform exceptionally well, such as maxout networks (Goodfellow et al., 2013).  
- Further elaboration on the role of normalizing constraints would enhance the exposition.  
- I assume the "projected Newton method" refers to a Newton step projected onto the constraint surface, finding the closest point (in terms of L2 distance) that satisfies the constraint. Explicitly stating this would be beneficial.
Post-Rebuttal Comments:  
- The authors' argument against using likelihood is somewhat persuasive, although it seems plausible that a deep model could recover the performance trade-offs introduced by posterior regularization in a shallow model.  
- I suggest moving the CIFAR results to a more prominent position in the main text, alongside the drug design results, while relegating the current table to the supplementary material. Since CIFAR10/100 is a widely recognized benchmark in the deep learning community, emphasizing these results in the main text would make the paper more accessible and impactful.
I have revised my score to a 7. This work presents a compelling approach to factor analysis, incorporating non-negativity constraints on the posterior to achieve sparse representations. The method is well-founded, with detailed implementation discussions that enable scalability. However, the empirical evaluation could be strengthened, as some benchmarks are either outdated or less commonly used.