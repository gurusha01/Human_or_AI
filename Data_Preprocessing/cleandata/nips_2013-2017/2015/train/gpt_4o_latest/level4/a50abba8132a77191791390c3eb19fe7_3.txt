The authors should clearly articulate what this new method contributes to the existing body of work in stochastic optimization (e.g., Hu, Kowk, and Pan, 2009) and related mini-batch optimization approaches (e.g., Konecny et al., 2013 and 2014), as well as Smola's work (e.g., Zinkevich, Weimer, Li, and Smola, 2010). This includes employing such methods as baselines in the experimental comparisons.
Comparing the proposed method solely to its own variants is inadequate.
This is a highly competitive and well-established field.
It is essential for the authors to explicitly highlight their novel contributions.
The paper is reasonably well-written, and the comparative bounds table in Section 8 is a valuable addition.
The work explores the stochastic variance-reduced gradient method to accelerate convergence in optimizing a sum of strongly convex Lipschitz continuous functions. While the method trades off inexact computation for speed, it remains unclear whether it offers a significant enough improvement over the current state of the art in stochastic optimization to justify publication at NIPS.