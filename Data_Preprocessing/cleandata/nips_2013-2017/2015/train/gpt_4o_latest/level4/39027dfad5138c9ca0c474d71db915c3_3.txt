Summary
This paper addresses the challenge of batch learning from logged bandit feedback, a highly pertinent issue in domains such as ad ranking. The authors pinpoint an "overfitting" issue within the recently introduced CRM principle and propose a remedy using multiplicative control variates, which leads to a biased estimator.
Quality
The work is of high quality and well-justified. However, given that it builds upon [1], I anticipated more extensive experiments and a deeper analysis of the training objective's optimization process (beyond the brief discussion in the paragraph on line 415).
Clarity
The paper is clearly presented and well-written.
Originality
While this is a continuation of [1], it introduces a modification to the risk estimator, making it self-normalizing. This adjustment appears to yield significant improvements in the experiments conducted by the authors.
Significance
The task of learning from logged data is undeniably important, but it remains unclear how the notable experimental gains translate to broader real-world applications.
Various Remarks
365: If multiple repetitions have already been conducted, please provide a measure of variance. The paper identifies a limitation in the recently proposed Counterfactual Risk Minimization (CRM) principle and offers a solution to address it. Overall, the paper is solid, well-written, and the contributions are both relevant and novel. That said, I would have liked to see more extensive experiments, particularly involving applications beyond those in [1], including real-world use cases.