This paper introduces an architecture for very deep networks that allows the input to pass through a layer unchanged via gates that are conditioned on the input itself.
This approach facilitates the training of much deeper networks than would be feasible with a standard feedforward architecture.
The authors conduct a compelling analysis of the trained networks, examining how the gates select inputs across different layersâ€”a focus that aligns perfectly with what I was hoping to see.
The paper is of high quality, well-written, and demonstrates originality.
While there are some natural parallels to recent work (notably FitNets and LSTMs), the distinctions are clearly articulated, and comparisons are provided where relevant.
The results achieved are promising.
I have no recommendations for improvement.
Note the reference to "Tables 3 and 3" (p5).
This is a strong paper. The proposed method is innovative, yields good results, and is supported by thorough and insightful analysis.