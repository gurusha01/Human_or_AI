This paper introduces an algorithm for constructing decision trees with linear split functions at the nodes. Unlike the traditional greedy approach to tree learning, this algorithm globally optimizes the tree parameters, including the weights of the split functions and the output variable distributions at the leaves. The authors propose an upper bound on the tree's empirical loss, which is regularized using L2 regularization on the split function weights to facilitate smoother optimization. The algorithm employs stochastic gradient descent (SGD) to minimize this upper bound and determine the optimal tree parameters.
The formulation of the tree optimization problem is elegant, as it draws a connection to structured prediction with latent variables. However, the empirical results presented in the paper fail to convincingly demonstrate that global optimization is a superior alternative to the greedy approach for learning decision trees. The globally optimized tree shows significant performance improvements over the greedy tree on only a few datasets, while also being computationally more demanding.
The paper is well-organized, but certain aspects require further clarification:
- The description of the stable version of SGD is somewhat unclear. Specifically, how are data points assigned to leaves? Including pseudo-code could enhance clarity.  
- In Figure 2, are the results derived from applying SGD or stable SGD? Additionally, what is the precise meaning of an "active" leaf?  
- In line 10 of Algorithm 1, the parameter \(\Theta\) is updated and subsequently projected onto a simplex. How is this projection problem solved in practice? It might also be helpful to note that this projection ensures each row of \(\Theta\) sums to one.  
Minor comments and typographical issues:  
- Lines 407-408: You mention tuning the "number of features evaluated when building an ordinary axis-aligned decision tree." This is unclear, as standard decision trees evaluate all features at each node.  
- In equations (7), (8), and (9), "argmax" should be replaced with "max."  
- Figure 1 is not referenced in the main text.  
- Line 141: "...to the index of the leaf on by this path." This sentence appears to have an error.  
- Line 238: "...the solution to loss-augmented inference..." should be "the solutions to..."  
- Line 301: "An key observation..." should be "A key observation..."  
- Line 360: What does "FSGD" refer to?  
- Line 404: "...a tree with minimum training errors." Did you mean "tuning errors"?  
In the supplementary material:  
- Line 29: "max{g \in sgn...}" should be "max{g = sgn...}"  
- Line 30: "sgn(Wx)^T Wx + l(\Theta^Tf(g), y)..." should be "sgn(Wx)^T Wx + l(\Theta^Tf(sgn(Wx)), y)"  
While the tree optimization problem is formulated in an elegant manner, the empirical results provided do not convincingly support the proposed approach.