In this paper, the authors present an algorithm for real-time control of 3D models, leveraging a neural network (NN) trained offline to replicate the output of Contact-Invariant Optimization (CIO) for generating animations. A key aspect of the approach is the interdependence between CIO and NN training, which are performed alternately, with the output of one influencing the optimization criterion of the other. Another notable feature of the method is the introduction of noise, achieved through data augmentation with small perturbations and additive noise in the NN's hidden layers. Experimental results demonstrate that the proposed network can generate realistic and stable control policies across a variety of character models.
This is an intriguing approach, particularly given the challenges associated with real-time control of arbitrary models. While the method might initially seem straightforward ("train a NN to predict CIO outputs"), the additional innovations—such as joint training and noise injection—represent significant contributions, as evidenced by their positive impact on generalization in the experiments.
However, the experimental results could be strengthened by using a larger dataset. Since the training trajectories were generated from only "between 100 to 200 trials, each with 5 branched segments," it is unsurprising that the "no noise" variant suffers from overfitting. Similarly, the "no joint" variant might also benefit from more diverse training trajectories, though this is less immediately apparent.
The comparative evaluation remains the paper's weakest aspect (as acknowledged in the introduction: "A systematic comparison of these more direct methods with the present trajectory-optimization-based methods remains to be done"). The only comparison with an alternative method is a brief one with MPC near the end of the paper, and this method lacks a proper reference.
The mathematical formulation is, for the most part, well-explained and motivated. However, readers attempting to replicate the results will need to consult additional references, as certain aspects are only superficially addressed (e.g., the CIO step—see Eq. 6, where some notations are undefined). Providing the code would be a valuable contribution to the community. One section I found particularly difficult to follow was Section 3.2, which discusses generating optimal actions for noisy inputs. The notations became confusing, and it was unclear what was being done (e.g., whether the time index "t" is still implied for some quantities after being dropped).
It would also be helpful to include time measurements for real-time control computations, as this could be a significant bottleneck. Additionally, understanding how these computations scale with the complexity of the 3D model would be valuable, especially for applications like video games where CPU resources for animating a single character are limited.
Additional minor comments:  
- "biped location": did you mean "locomotion"?  
- "It is our aim to unity these disparate approaches": should this be "unite" or "unify"?  
- Does the target only involve x, y, z coordinates, with no constraints on the character's facing angle at the target? If so, would it be straightforward to add a target angle, as this seems practically important?  
- The term "noise injection" for generating additional data from noisy inputs seems misleading, as the target is recomputed to match the modified inputs. This step is more accurately described as "data augmentation" and is distinct from the concept of denoising autoencoders.  
- Are the initial (random) network weights θ used in the first iteration to compute the initial X's in Algorithm 1? Or are the X's computed without the regularization term initially?  
- What is the rationale for using the same hyperparameter η in both steps of Algorithm 1?  
- "it is non-trivial to adapt them to asynchronous and stochastic setting we have": is "the" missing?  
- The acronym LQG is introduced one line before it is defined.  
- "While we can use Linear Quadratic Gaussian (LQG) pass": should this be "a pass"?  
- Line 231: the star in the argmin is misplaced, and is \bar{s} missing in \tilde{C}?  
- "the minimizer of a quadratic around optimal trajectory": is a word missing here?  
- "sX and aX are subsets of \phiX": why is this guaranteed?  
- "a single pass over the data to reduce the objective in (4)": this seems to refer to the objective from Algorithm 1, as only θ is optimized.  
- The asynchronous joint optimization scheme in Section 6 differs significantly from the alternating scheme in Algorithm 1. It would be helpful to mention this distinction earlier.  
- Line 315: should f be f^{des}?  
- "In the current work, the dynamics constraints are enforced softly and thus may include some root forces in simulation.": this sentence is unclear.  
- The "no noise" variant in experiments could be split into "no data augmentation" and "no hidden noise injection" for clarity.  
- It would be interesting to investigate whether α can be reduced to zero during optimization (for CIO only). Does this work primarily due to a "curriculum learning" effect (easier tasks initially), or does maintaining trajectories close to what the network can learn always provide benefits?  
- The caption of Table 1a mentions 4 layers, but I believe this should be 3.
In summary, this is an impressive application of neural networks to character control, but its practical benefits and broader applicability remain to be fully established.