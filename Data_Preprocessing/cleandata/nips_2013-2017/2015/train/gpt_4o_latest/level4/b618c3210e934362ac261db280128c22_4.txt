This paper introduces a variational inference framework for training Recurrent Neural Networks (RNNs) with stochastic transitions between hidden states. The proposed model functions as a stochastic nonlinear dynamical system, akin to a nonlinear state-space model, where the process noise is not identically distributed but instead depends on the hidden state. The parameters of the generative model are optimized jointly with a recognition model by maximizing the evidence lower bound.
Overall, this is a strong paper that effectively connects RNNs with nonlinear state-space models. The exposition is clear, focusing on the overarching concepts rather than delving deeply into technical details. The model is straightforward to describe, and the variational training approach is now a standard methodology. However, while the results are compelling, it seems that significant engineering effort was likely required to achieve them, which is not readily apparent from the paper. Additionally, there is no discussion of the computational resources or training times involved.
The use of subindices "< t" in the notation occasionally obscures the Markovian structure of the model, which could be helpful for interpreting the factorizations of joint distributions.
The literature review appears to omit discussion of nonlinear state-space models (i.e., nonlinear extensions of linear state-space models, such as "Kalman filters"). These models are widely used in fields like engineering, robotics, and neuroscience, among others. Explicitly highlighting the connection between the proposed model and nonlinear state-space models could broaden its impact beyond the machine learning community.
A few questions for clarification:
1. If VRNN-Gauss was unable to generate well-formed handwriting, are all the plots labeled VRNN actually referring to VRNN-GMM?
2. Was any form of regularization applied to the model? If not, how was overfitting mitigated?
3. Equation (1) defines \( ht \) as a function of \( xt \), while Equation (2) specifies a distribution over \( xt \) as a function of \( ht \). Is this discrepancy intentional?
In conclusion, this is a well-executed paper that successfully bridges the gap between RNNs and nonlinear state-space models. The experiments are robust, and the presentation is clear and polished.