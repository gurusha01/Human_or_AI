The authors propose a method to reconcile the high precision requirements of continuous variables in backpropagation algorithms with the use of spike-based neural network devices featuring binary synapses. They introduce the concept of treating analog variables as probabilities, which are sampled to map the neural network trained offline onto the spiking network chip used for application deployment. The authors adapt the backpropagation algorithm to align with this approach and validate their method on the MNIST benchmark. They demonstrate the mapping of the trained network onto two distinct architectures, optimizing for both size and accuracy, and provide performance metrics obtained from the chip. The manuscript is of very high quality, with excellent clarity. The work is original and has the potential to benefit other approaches and hardware platforms. However, the authors should consider referencing Qiao et al., Frontiers in Neuroscience 2015, and the paper available at http://arxiv.org/abs/1506.05427 when discussing alternative hardware platforms. Additionally, the paper does not sufficiently highlight the significance of the work. It remains unclear how the proposed method facilitates the deployment of the TrueNorth chip in practical applications and which specific applications would benefit. Furthermore, the advantages of this method over other alternatives, such as those cited in [9] to [12], are not explicitly addressed.
The authors present a technique for applying backpropagation to spiking neural networks with binary synapses and validate it by mapping the trained network onto the TrueNorth device. The paper is well-organized, clear, and concise. While the results are compelling, some details are missing, such as the process for converting inputs into spikes, the frequencies used, the shared weight values, and the neuron models.