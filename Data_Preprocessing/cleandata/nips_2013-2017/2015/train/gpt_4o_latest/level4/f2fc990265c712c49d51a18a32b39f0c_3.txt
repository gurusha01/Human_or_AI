The paper introduces an efficient optimization algorithm (HONOR) for tackling nonconvex regularized problems by leveraging second-order information to accelerate convergence. While convergence analysis for nonconvex optimization is generally more challenging than for convex methods, the authors have successfully demonstrated that every limit point of the sequence generated by HONOR is a Clarke critical point. The proposed approach is both theoretically robust and computationally efficient. Nevertheless, I have a few concerns regarding the algorithm.
1: By utilizing the property that each decomposable component function of the nonconvex regularizer is only non-differentiable at the origin, the authors have devised an algorithm that ensures the current iterate remains in the same orthant as the previous iterate, preventing the segment between consecutive iterates from crossing any axis. However, this design appears to make the algorithm reliant on the initial point \( x^0 \).
   + Can the authors clarify how the choice of the initial solution affects the outcomes of HONOR?
   + What specific vector was used as the initial solution in the numerical experiments?
   + Do the results presented in Figure 1 vary when different initial solutions are employed?
2: A key advantage of HONOR is its use of second-order information to enhance convergence speed. However, this may come at the cost of increased memory usage. For highly nonconvex problems, the positive definite matrix \( H^k \) derived from L-BFGS might significantly deviate from the true Hessian matrix of the nonconvex optimization problem. Do the authors have any insights or comments regarding these potential issues?
The paper is exceptionally well-written, and the development of an efficient algorithm with theoretical guarantees for nonconvex problems is highly valuable. Nonetheless, I remain concerned about certain aspects of the proposed algorithm.