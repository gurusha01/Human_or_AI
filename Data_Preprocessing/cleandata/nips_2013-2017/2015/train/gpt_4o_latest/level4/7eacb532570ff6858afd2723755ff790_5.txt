In this paper, the authors propose a dueling bandit approach where the arms are assumed to follow a ranking based on the Plackett-Luce distribution.
While regret analysis is a standard component of bandit problems, this paper focuses on sample complexity: specifically, the number of samples required to identify an \(\epsilon\)-optimal arm with a probability of \(1 - \delta\). However, there is no discussion regarding the regret bounds of the proposed algorithms. Additionally, it is unclear how the regret performance of the proposed algorithm compares to that of other existing dueling bandit algorithms. This aspect is not adequately addressed in the paper.
A significant limitation of the work is that the experiments are conducted solely on synthetic data. The paper does not clarify under what circumstances the Plackett-Luce distribution assumption is valid or to which real-world problems the proposed approach might be applicable. As a result, the experiments appear overly artificial.
Additional comments:
- Line 202: What does "end" after [M] signify?
- Line 7, Algorithm 2: Should \(N\) be \(\hat{N}\)?
- The algorithm is presented in the supplementary material, while its analysis is in the main paper. This structure seems counterintuitive and should likely be reversed.
Overall, this work introduces yet another approach for the dueling bandit problem. However, the experimental evaluation is weak, and the paper does not convincingly demonstrate how the proposed method outperforms existing approaches.