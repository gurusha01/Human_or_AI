In this paper, the authors introduce a novel recurrent convolutional encoder-decoder network designed to learn how to apply out-of-plane rotations to 3D objects, such as human faces and 3D chair models. The proposed architecture begins with a basic model, where the encoder network separates the input image into identity units and pose units. By applying action units to the pose units to control the rotation direction, the decoder network—comprising convolutional layers and upsampling—reconstructs the rotated object's image and its corresponding mask from the identity and pose representations. To handle longer rotation trajectories, the network is extended with a recurrent architecture. In this extension, the encoded identity unit of the input image remains fixed, while the pose unit evolves through a sequence of action units. The updated identity and pose units are then passed to the decoder to generate the final rotated image.
A key contribution of this work lies in its ability to disentangle identity/appearance and pose representations. The identity units are demonstrated to be discriminative, view-invariant features in a cross-view object recognition task. Furthermore, this disentanglement enhances the network's ability to predict more accurate renderings, particularly when longer rotation trajectories are introduced during the curriculum training stages of the recurrent convolutional encoder-decoder network.
The paper is well-written, easy to understand, and provides clear motivation for the various components of the proposed method. The qualitative results for rendered rotated images and the quantitative evaluations on the cross-view object recognition task strongly support the effectiveness of the approach, particularly in terms of the disentangled representations of pose and identity factors.
However, there are some minor weaknesses that the authors could address during the rebuttal period:
- The proposed network currently supports only discrete rotation angles, which are determined by the set of angles included in the training data. Do the authors have any preliminary ideas on how to extend the method to handle continuous rotation angles?
- The recurrent convolutional encoder-decoder network is trained with a fixed sequence length, which seems inconsistent with the general flexibility of recurrent neural networks.
Overall, this paper presents a novel recurrent convolutional encoder-decoder network trained end-to-end for rendering rotated objects from a single input image. The primary contribution—disentangling identity and pose factors through the recurrent rotation prediction objective—is convincingly demonstrated through both qualitative and quantitative evaluations.