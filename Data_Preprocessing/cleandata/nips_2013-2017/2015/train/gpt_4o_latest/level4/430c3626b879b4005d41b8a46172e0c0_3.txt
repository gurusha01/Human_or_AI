This paper introduces a novel adaptive learning rate scheme aimed at optimizing nonlinear objective functions encountered during the training of deep neural networks. The central premise is grounded in recent findings that suggest the primary challenge in optimization arises from saddle points rather than local minima along the optimization trajectory.
Saddle points hinder training progress because the objective function tends to exhibit flatness in multiple directions and poor conditioning in the vicinity of these points. To address this, the authors propose a method to mitigate the issue of pathological curvature by "preconditioning" the objective function. This involves a linear transformation of variables, effectively left-multiplying the gradient descent update step by a learned preconditioning matrix, denoted as D. The authors focus on the specific case where D is diagonal and demonstrate how this diagonal D corresponds to parameter-specific learning rate methods, such as the Jacobi preconditioner or RMSProp.
This framework provides a compelling perspective for examining various adaptive learning rate schemes. However, as a side note, it remains unclear whether this connection has been previously established or if it represents a novel contribution of this paper.
The paper's primary theoretical contribution is its analysis of the Jacobi preconditioner, highlighting its undesirable behavior in the presence of both positive and negative curvature. The authors derive the optimal preconditioner, termed the "equilibration preconditioner" D^E, defined as |H|^-1. However, since D^E depends on the Hessian, which has a computational complexity of O(n^2) with respect to the number of network weights, both its computation and storage are infeasible in practice. The key practical contribution of the paper is an efficient approximate algorithm, "Equilibrated Stochastic Gradient Descent" (ESGD). ESGD estimates the optimal D^E using the R-operator, requiring approximately two additional gradient computations (further amortized by performing this step every ~20 iterations) and sampling a vector from a Gaussian distribution.
The theoretical and practical contributions are validated through empirical results, which demonstrate that ESGD outperforms standard SGD, Jacobi preconditioning, and RMSProp in training deep auto-encoder networks.
Overall, the paper is relatively well-written, though I encountered some ambiguities in the notation. Specifically, there were challenges in distinguishing between element-wise operators and matrix operators, as well as instances where matrices appeared to be defined element-wise but were equated with full matrices, or vice versa. For example:
- In Eqn 7 and the definition of D^-1 = 1 / ||A{i, .}||2 just above it, does this mean that the i'th element of D^-1 is defined in this way? Otherwise, what does the subscript i in A_{i,.} represent?  
- In Eqn 10, does this imply that each diagonal element i of D^E is defined as the norm of the i'th row of the Hessian, ||H_{i,.}||? Is that correct?
It would be beneficial to include a section at the beginning of the paper that clarifies the notation, explicitly specifying which operators act element-wise (e.g., |A|) and how matrices are indexed. For instance:
- What is the distinction between A{i,.} and Ai? (I assume one refers to a row from a full matrix, while the other refers to an element from a diagonal matrix.)  
- What is the difference between qi qj (e.g., below Eqn 11) and q_{j,i} (as seen in the supplementary material)? Are these equivalent?
Additionally, in the supplementary material, Proposition 3 was unclear. Specifically, after applying (q^2)^-{1/2}, how does q on the RHS of the inequality remain squared? Shouldn't this result in q^{-1}?
In conclusion, this is a well-executed paper that introduces a new adaptive learning rate scheme for deep neural network training. It establishes theoretical connections between various methods within a preconditioning framework, derives an efficient approximation to the optimal preconditioner, and provides empirical evidence demonstrating its superiority over three alternative methods in practice.