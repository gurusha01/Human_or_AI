Counterfactual risk minimization is a method designed for learning in a bandit setting using logged data, with bias correction for the generation of logged examples. While this approach is consistent, the paper highlights an overfitting issue where the hypothesis class can overfit based on the generation probabilities of examples. To address this, the authors introduce a multiplicative parameter that controls the variance while preserving consistency. Experimental results demonstrate that the proposed multiplicative (or normalized) counterfactual technique outperforms the non-normalized approach.
The paper introduces a method for learning a self-normalized estimator for counterfactual learning in the logged bandit setting. It tackles the challenge of overfitting, where the hypothesis may overfit by placing excessive weight on the logged data. This is a strong paper and merits acceptance, though I am not entirely certain.