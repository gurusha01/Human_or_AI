I am curious about the trade-off introduced by the discretized time interval discussed in Section 3.1. Is this approach exact? Similarly, the same time discretization could be applied to CTMC; what implications would this have?
The writing throughout the paper is quite dense.
I believe that an excessive amount of space is dedicated to explaining prior work, extending up to line 235, which is 4.5 pages into the paper.
In Section 5.1, the results with simulated data assume that the mean of the observations from each state differs by 1.0, with a standard deviation of \(\sigma = 0.25\). With states separated by \(4\sigma\), the results in this section exhibit minimal noise. I question how these results effectively demonstrate the advantages of the proposed method compared to CTMC.
Additionally, the error metric "2-norm error" is not defined. I assume this refers to the state sequence? As mentioned earlier, where could the state errors have originated?
In Section 2 (Background), it would be helpful to clarify the relative contributions of references [8, 9, 10, 12, 13]. This would aid readers in better understanding and conceptualizing the topic.
One recent paper that should have been cited and discussed is: V. Rao and Y. W. Teh. "Fast MCMC sampling for Markov jump processes and extensions." Journal of Machine Learning Research, 13, 2014. This paper introduces two inference methods for continuous-time hidden Markov models (CT-HMM) by building on recent inference techniques for continuous-time Markov chains (CTMC). The algorithmic contribution is somewhat novel, and the applications are relevant and moderately innovative (e.g., disease progression modeling, which is of interest to the NIPS community). The primary concerns with this submission lie in the quality of the writing and the strength of the results.