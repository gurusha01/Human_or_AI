Review - Paper Title: Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling  
Paper Summary: This paper introduces a novel approach, termed the "covariance-controlled adaptive Langevin thermostat," for MCMC posterior sampling in Bayesian inference. Similar to prior work in scalable MCMC, the proposed method is a stochastic gradient sampling technique. The primary goal of the method is to reduce parameter-dependent noise, thereby accelerating convergence to the invariant distribution of the Markov chain and improving sampling efficiency, while preserving the desired invariant distribution. Like other stochastic gradient MCMC methods, this approach is designed for large-scale machine learning scenarios, such as Bayesian inference with extensive datasets. The authors evaluate their method on three models—a normal-gamma model, Bayesian logistic regression, and a discriminative restricted Boltzmann machine—and demonstrate its superiority over two existing methods: stochastic gradient Hamiltonian Monte Carlo (SGHMC) and stochastic gradient Nose-Hoover thermostat (SGNHT).
Comments:  
- This paper makes a meaningful contribution to the field of stochastic gradient MCMC methods and effectively situates its approach within the context of related work, specifically SGHMC and SGNHT. However, the contribution feels somewhat incremental, both in terms of the novelty of the ideas and the results presented.  
- The experimental evaluation is limited to comparisons with SGHMC and SGNHT. It would strengthen the paper to include results for additional recently developed mini-batch MCMC methods, such as stochastic gradient Langevin dynamics or stochastic gradient Fisher scoring. Furthermore, comparisons with methods that do not rely on stochastic gradients, such as those proposed in (Bardenet, Remi, Arnaud Doucet, and Chris Holmes. "On Markov chain Monte Carlo methods for tall data." arXiv preprint arXiv:1505.02827 (2015)) or (Maclaurin, Dougal, and Ryan P. Adams. "Firefly Monte Carlo: Exact MCMC with subsets of data." arXiv preprint arXiv:1403.5693 (2014)), would provide additional context for the performance of the proposed method.  
- In Figure 1, the inset "peaks" contribute little to the overall figure. They appear to be only a marginal zoom into the main plot, offering minimal additional insight.  
- The quality of writing in the paper could be improved in several areas. For instance, the abstract contains sentences that are somewhat ambiguous, such as "one area of current research asks how to utilise the computational benefits of stochastic gradient methods in this setting." In the introduction (second paragraph), the order of presenting stochastic gradient methods feels unintuitive, as the general collection of methods is described first, followed by the earliest developed method. In Section 2, some terms, such as "temperature" and "Boltzmann constant," are introduced without sufficient explanation, which could confuse a machine learning audience. Providing brief descriptions or intuitive explanations for these terms would enhance clarity.  
Overall, I believe that advancing scalable Bayesian inference methods is a valuable research direction, and this paper successfully combines the strengths of SGHMC and SGNHT to achieve improved performance. However, the contributions feel incremental, and the paper would benefit from broader comparisons and more careful attention to writing and presentation.