The authors introduce a hierarchical model that provides a multilayer representation of count vectors. This model incorporates hidden layers composed of gamma variables sampled from distributions with factorized shape parameters.
The proposed inference method, based on Gibbs sampling, effectively learns the widths of the hidden layers, with the first-layer width serving as an upper limit. Leveraging this approach, the experimental results demonstrate that deeper networks achieve superior classification accuracy and perplexity compared to single-layer models (equivalent to the model in [12]). This improved performance is attributed to the ability to capture correlations between hidden units and account for overdispersion.
The model and algorithm are presented with clarity.
The paper highlights the advantages of the multilayer model over the single-layer counterpart, which can be viewed as a nonparametric extension of the PFA model introduced by Zhou et al. (2015). The experimental results are compelling.
While the model is innovative — employing nonnegative hidden units instead of binary ones and automatically learning layer widths — the paper does not thoroughly address its advantages over the leading competing method, the over-replicated softmax model [21], which is a two-layer DBM. The authors acknowledge that the classification accuracy is lower than that of the over-replicated softmax model [21] when the first-layer budget is set to 512, attributing this to potential issues with word preprocessing.
Additionally, on page 8, the authors discuss observations regarding layer width decay rates, but these findings are derived from experiments on a single dataset. It remains unclear whether these observations are influenced by the dataset itself or by the first-layer budget. The claim on page 2 regarding the relationship between these factors requires validation across multiple datasets.
In summary, the authors propose a novel multilayer model for count vectors with nonnegative hidden units and Gibbs sampling-based inference. The paper convincingly demonstrates the benefits of the model over a (potentially wider) single-layer alternative, and the experimental results are well-supported. However, the paper does not establish that the proposed model represents the state-of-the-art in a specific domain (e.g., topic modeling) or that it outperforms the leading competing method — the over-replicated softmax model of Srivastava et al. (2013), a deep Boltzmann machine with binary hidden units. Furthermore, the conclusions are drawn from experiments conducted on a single dataset.