This paper introduces a hybrid methodology that combines crowdsourced labels with an incrementally (online) trained model to tackle prediction problems. The central concept is to rely heavily on the crowd during the system's initial phase, learn from the collected labels, and gradually reduce dependence on the crowd as the model gains confidence. The approach is formulated as a stochastic game, leveraging a CRF-based prediction model where both the system and the crowd act as players. The system issues one or more queries \( q \) for tokens \( x \) (with true label \( y \)), eliciting responses \( r \), and assigns a utility \( U(q, r) \) to each outcome. The system's goal is to maximize expected utility by selecting optimal actions. Queries are issued incrementally at times \( s \), with responses arriving at times \( t \), and utility is maximized under a \( t_{\text{deadline}} \), which determines the number, rate, and timing of queries.
To compute the expected utility, the authors employ a simulation dynamics model \( P(y, r, t | x, q, s) \) to evaluate utilities as described in Equation (4). While the optimal action could theoretically be derived from these utility values, the continuous-time nature of the problem renders direct optimization intractable. Instead, the authors propose an approximation using Monte Carlo Tree Search and TD learning (Algorithm 1). The methodology is evaluated on three tasks: named entity recognition (NER), sentiment recognition, and face identification, using four methods: majority vote from \( n \) human judges (1, 3, 5), online learning with oracle labels, a "threshold" baseline (the authors' model without continuous time, where \( m \) queries are issued until model uncertainty drops below a threshold), and the full proposed model (LENSE). Results indicate that the full model outperforms all but the 5-vote crowdsourced baseline in terms of precision/recall/F1, though the "threshold" baseline achieves comparable performance with lower latency for NER (Table 2). Additionally, the authors demonstrate that their model requires fewer crowdsourced labels over time as the system matures (Figure 3) and achieves high accuracy from the outset by leveraging crowd expertise during high-uncertainty phases (Figure 4).
This is an innovative and thought-provoking paper that has the potential to spark significant discussion, inspire further research, and see practical adoption. While heuristic approaches to hybrid learning from crowds have been explored, this work presents the first comprehensive and principled framework. The results against reasonable baselines are compelling, making the paper highly original and impactful. I strongly recommend its inclusion in the conference. However, the paper's clarity is a notable weakness, which could hinder its broader impact. Many of these issues are addressable, even in a camera-ready version, if the authors are willing to refine the presentation and adjust the tone regarding the "threshold" baseline versus the full continuous-time model.
The primary clarity issue arises with the introduction of continuous time in Section 3. While the motivation—operating within time and cost constraints—is valid, the experimental results suggest minimal performance benefits, yet the added complexity significantly complicates the formulation, discussion, and optimization process. For instance, in Table 2, the "threshold" baseline achieves lower latency on the NER task with comparable performance and even outperforms the full model on the face ID task. The authors attribute this to the simpler nature of face ID as a single-label task (line 314), but the NER task, which involves substantial label interaction, only shows marginal gains.
For many practical applications, the "threshold" baseline is likely sufficient, offering a principled hybrid learning system that already surpasses active and online learning. The paper might have greater impact if the authors emphasized this point, developing the core method without continuous time for simplicity and presenting continuous time as an extension. Similarly, the discussion of results could better highlight the strength of the "threshold" baseline, even in tasks with sequence dependencies. While this is a suggestion rather than a requirement, it could enhance the paper's accessibility and adoption.
There are additional issues with the results and their presentation. For example, the introduction claims the model "beats human performance," but this is not true for the 5-vote NER case. Additionally, the 5-vote results are missing for the face ID and sentiment tasks in several tables and figures (e.g., the right half of Tables 2, Table 3, and Figure 4) and should be included. In Table 2, the 5-vote results should be bolded as they represent the best performance, as should the latency value for the "threshold" baseline in NER and the LENSE value for face ID. Furthermore, the "threshold" baseline is omitted from the sentiment experiment, leaving it absent from Table 3 and Figure 4, which should also be rectified.
Minor methodological clarifications include:  
- The discussion of \( t_{\text{deadline}} \) (lines 183–187) does not specify whether it was used in the experiments or how the system determined when to report an answer—was it based on an entropic threshold like the baseline or some other criterion?  
- In Equation (2), the distribution should include \( s \) as a variable, i.e., \( P(y, r, t | x, q, s) \).  
- In Equation (5), the use of \( q \) in \( F(q) \) is confusing, as it appears to refer to a query but actually represents a nominal distribution function. A different variable should be used for clarity.  
- In the description of the "threshold" baseline (lines 246–252), the inequality appears to be incorrect. The right-hand side should likely be \( (1 - 0.88) \), as increasing \( m \) reduces uncertainty.  
In summary, this paper proposes a hybrid approach combining crowdsourced labels and online learning, framed as a stochastic game that incorporates query timing and response dynamics. The results are impressive, particularly compared to active and online learning baselines, and the work is highly original and significant. However, the introduction of continuous time adds complexity with limited performance gains, and the paper's clarity could be improved. Addressing these issues would enhance the paper's impact and accessibility, though it is already strong enough to merit acceptance.