This paper addresses high-dimensional estimation problems under sub-exponential design and noise. The authors derive an estimation error bound using the exponential width argument, demonstrating that the estimation error is at most \(\sqrt{\log p}\) times worse than in the sub-Gaussian case.
The first key result establishes a connection between Gaussian and exponential widths, showing that the exponential width is upper bounded by the Gaussian width by a factor of \(c \sqrt{\log p}\).
The second significant result demonstrates that achieving the restricted eigenvalue condition for sub-exponential design requires the same sample complexity as in the sub-Gaussian case. The authors present two proof techniques, showing that the bound derived via the exponential width argument is \(\log p\) worse than the one obtained using the VC dimension approach.
The authors also provide simulations to validate the derived sample complexity. However, I find Figure 1 to be less informative. To better illustrate that the sample complexity is \(O(s \log^2 p)\) rather than \(O(s \log p)\), it would be more effective to plot the normalized sample size (\(n / s \log^2 p\)) against the probability of success, allowing for a clearer comparison of curves across different dimensions.
Overall, this paper presents substantial contributions to high-dimensional estimation under sub-exponential design and noise. The proof techniques are both novel and insightful. I believe this work is a strong candidate for NIPS. The paper establishes high-dimensional estimation error bounds under sub-exponential conditions and provides interesting and valuable analyses.