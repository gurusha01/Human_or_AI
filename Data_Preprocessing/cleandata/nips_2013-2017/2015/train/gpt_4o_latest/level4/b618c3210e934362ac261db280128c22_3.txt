The paper presents a generative model for sequences, with its novelty lying in the incorporation of explicit latent stochastic variables that condition data generation. These latent variables are allowed to depend on past information.
Strengths: The work extends recent latent variable models to the domain of sequences.
The authors demonstrate that the proposed model performs competitively with other sequence models across several datasets.
Weaknesses: Certain claims lack sufficient support. The experiments are primarily proof-of-concept, relying on raw signals without any preprocessing, which complicates direct comparisons with state-of-the-art results on these well-established datasets.