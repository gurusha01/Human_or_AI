Including the running times for all the algorithms would enhance the evaluation. Employing five different algorithms to compute a gradient step toward the constrained posterior appears excessive and may reduce the algorithm's appeal. Is each of these steps strictly necessary? The paper introduces sparsity-enforcing posterior regularization to transform inference in factor analyzers into a nonlinear process, enabling the stacking of these models to achieve hierarchical representations. This is an intriguing approach, but I question its scalability given its batch-based design. While the results seem reasonable, the algorithm appears significantly more computationally expensive than the baselines used for comparison.