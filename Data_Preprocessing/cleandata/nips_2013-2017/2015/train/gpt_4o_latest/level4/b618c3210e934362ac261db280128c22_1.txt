This paper explores the integration of the variational autoencoder (VAE) into a recurrent model. The introduction provides strong motivation for the work, but the clarity of the paper diminishes as it progresses (e.g., there are issues with equations 1 and 2). The core contribution of the paper begins in the middle of page 4.
It is worth noting that the first two sentences of the "Generation" paragraph on this page succinctly summarize the essence of the paper. These sentences could be introduced earlier in the manuscript, potentially even in a revised version of the introduction.
The primary concern lies in the experimental section and table 1. The evaluation metric is stated as the average log-probability. If this is indeed the case, all the results in table 1 should be negative, not just the first three scores in the first row. Alternatively, if the metric is the negative log-probability, then lower values would indicate better performance. As it stands, the numbers in table 1 are unclear and difficult to interpret. It is important to note that these are the only experimental results presented in the paper.
Comments (in reading order):
Summarizing the work of Kingma and Welling (2014) in less than one page is admittedly challenging. However, the resulting summary is not always easy to follow. It might be more effective to directly describe the VAE in the context of a recurrent model.
- Page 2: "extend the variational autoencoder (VAE) into an recurrent" should be corrected to "extend the variational autoencoder (VAE) into a recurrent."  
  Similarly, "offers a interesting combination highly" should be revised to "offers an interesting combination of highly..." Additionally, this sentence is overly long and contains too many "and"s, which could be simplified for better readability.
- There appears to be a potential issue with equations 1 and 2: in equation (1), xt depends on ht, while in equation (2), ht depends on xt. To avoid this cyclic dependency, a "t-1" term might be required. Is this interpretation correct?
- Page 6: The statement "We use truncated backpropagation through time and initialize hidden state with the final hidden state of previous mini batch, resetting to a zero-vector every four updates" is somewhat unclear. Could the authors clarify whether this approach is necessary for the conventional model, the model with latent variables, or both?
In summary, the introduction is well-motivated, and the idea of applying the VAE at each time step of a recurrent network is compelling. However, the experimental results are problematic and difficult to interpret, which raises significant concerns about the validity of the findings.