The combination of ideas from screening and approximate regularization paths, as proposed in this paper, represents a completely novel and pleasantly surprising approach to achieving the important goal of test error guarantees. Although the results are currently limited to binary linear classification, this work could serve as a valuable stepping stone toward extending these ideas to more general machine learning models. The paper is well-written, and the experimental results are promising.
To enhance clarity for readers, it would be helpful to more explicitly distinguish between the definitions of eps-approximate solutions in terms of training error (1) and validation error (2), as this would reduce potential confusion. Additionally, the term 'eps-approximate regularization parameter' might be better rephrased as 'eps-approximate best regularization parameter' for improved precision.
The authors should provide a more detailed discussion of the theoretical complexity of the proposed method as a function of eps. Specifically, assuming that the C values are chosen naively and equally spaced, what implications do the results have for the interval size? Expanding on this point would also create an opportunity to discuss the connection to path methods for training error (as opposed to the text-based discussion provided here) in greater depth.
Minor comments: l061: replace "the" with "this"; l264: correct "soluitons" to "solutions."
(This is a light review.) The paper introduces a novel method for obtaining guarantees on cross-validation error across the entire regularization path. This represents a highly relevant initial step toward the development of more robust hyperparameter search methods that could eventually provide test error guarantees.