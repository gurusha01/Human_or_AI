The paper addresses the challenge of generating a sequence of coherent sentences to describe a sequence of images extracted from blog posts. To achieve this, the authors propose a method that retrieves the K=5 most similar images and their corresponding sentences from the training set for each query image. The primary contribution of the paper lies in devising a mechanism to select the most relevant sentences for the given image sequence, ensuring a coherent narrative. Specifically, the sentences are embedded into vectors, and their sequence is modeled using a bidirectional LSTM. The output of the bidirectional LSTM is processed through a ReLU activation and a fully connected layer, followed by scoring with a compatibility metric between the image and sentence. Furthermore, a local coherence model [1] is integrated to ensure consistency between consecutive sentences.
Strengths / Positive Aspects:  
- The paper introduces a novel and effective architecture for retrieving coherent sentences corresponding to an image sequence.  
- It includes comprehensive quantitative, qualitative, and human evaluations, demonstrating the superiority of the proposed approach over several baselines that do not incorporate coherence. Additionally, an ablation study is provided, removing the local coherence model [1].  
- The authors commit to releasing the source code and dataset, which will benefit the research community.  
Weaknesses / Questions / Unclarities:  
1. Line 94: The paper states that [5] lacks "a mechanism for coherence between sentences." While this may not be the primary contribution of [5], it is worth noting that [5] predicts an intermediate semantic representation of videos, which inherently ensures coherence across sentences by modeling the topic of the multi-sentence description.  
2. Correctness/Clarity: Figure 2b does not appear to align with the description in Section 3.3 or Equation (2). While Figure 2b suggests that the fully connected layers are connected to all sentences, Equation (2) implies that the parameters are shared across sentences and connected only to the vector representing a single sentence.  
3. A more suitable metric for automatically evaluating the generated sentences would be METEOR (http://www.cs.cmu.edu/~alavie/METEOR/), particularly when there is only a single reference sentence, as opposed to BLEU.  
4. In Equation (2), why are two linear functions (W{f2}, W{f1}) applied sequentially? Since two consecutive linear functions are equivalent to a single linear function, the benefit of this design choice is unclear. An ablation study demonstrating the utility of these functions would strengthen the paper.  
5. Why are the same parameters for the fully connected layers used for both the BRNN output (o_t) and the local coherence model q (Equation 2)?  
6. Is the paragraph vector [16] fine-tuned during training, or is it kept fixed?  
=== Post Rebuttal ===  
After reviewing the rebuttal, I recommend the paper for acceptance. The authors have effectively addressed concerns related to the formulation, evaluation, and discussion of related work.  
Please ensure that the promised revisions are incorporated into the final version and clarify the following remaining point:  
6. Is the paragraph vector [16] fine-tuned during training, or is it kept fixed?  
Overall, the paper presents an innovative model for retrieving coherent sentences corresponding to an image sequence, supported by convincing evaluations. However, several clarifications are necessary to further strengthen the paper.