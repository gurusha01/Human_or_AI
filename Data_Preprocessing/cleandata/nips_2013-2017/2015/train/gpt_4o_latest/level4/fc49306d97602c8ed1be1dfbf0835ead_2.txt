Paraphrased Review:
Comments after response:
While [18] demonstrated only modest absolute gains over the two-stage approach, I do not believe it is evident that any combined approach would outperform two-stage methods to the extent that running the experiment becomes unnecessary. Considering that many contemporary NLP systems already utilize word embeddings, I find it crucial to include comparisons with natural approaches that incorporate them.
In addition to applying CCA on separate word embeddings, you should also evaluate a baseline that matches by identifying the nearest candidate word within an existing multilingual word embedding system. Since your method can be interpreted as a technique for multilingual word embedding, I believe it is equally important to compare it against the relevant literature in that domain.
I would be inclined to recommend publication of a revised version of this paper that includes comprehensive experimental comparisons to these related approaches, provided the results are promising. While the novelty relative to [18] and [19] may not be particularly substantial, extending their application to a new domain is a reasonable contribution. However, without seeing these results, it is difficult for me to fully endorse the paper. I have, nonetheless, raised my score from 4 to 5.
---
The paper introduces a method for cross-domain matching by embedding instances as sets and optimizing the likelihood of a softmax model based on MMD distances between these sets, given a limited number of should-match training pairs.
An alternative algorithm for addressing this problem would involve generating word2vec-style embeddings for each domain, followed by applying kernel CCA to these representations. This approach is highly intuitive, addresses the paper's criticisms of kernel CCA, and yet is not mentioned here.
More broadly, the paper appears to overlook the extensive recent literature on embeddings. A quick search yielded five highly relevant papers specifically focused on the NLP domain:
- Bollegala, Maehara, and Kawarabayashi. Unsupervised Cross-Domain Word Representation Learning. ACL 2015.  
- Shimodaira. A simple coding for cross-domain matching with dimension reduction via spectral graph embedding. arXiv:1412.8380.  
- Yang and Eisenstein. Unsupervised Domain Adaptation with Feature Embeddings. ICLR 2015 workshop (arXiv:1412.4385).  
- Al-Rfou, Perozzi, and Skiena. Polyglot: Distributed Word Representations for Multilingual NLP. arXiv:1307.1662.  
- Nguyen and Grishman. Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction. ACL 2014 short paper.  
Additionally, the numerous deep learning papers from last year on automatic caption generation are highly relevant. While some of these methods rely on more paired training examples than those used here, your paper would benefit from a more thorough evaluation of this distinctionâ€”or at least a brief acknowledgment of it.
Although the techniques proposed in this paper are interesting (albeit somewhat incremental compared to prior work), the lack of discussion regarding their relationship to the relevant literature makes it difficult for readers to assess how the learned embeddings compare to those produced by actual competing systems, rather than simplistic baselines.
Furthermore, as with the previous two papers in this series, the scalability of the proposed method appears to be a significant challenge. This issue is not addressed, despite the fact that all datasets evaluated are relatively small. While the proposed method is intriguing, its advantages over straightforward alternatives (which are not discussed in the paper) remain unclear, and the vast body of related work is only superficially addressed.