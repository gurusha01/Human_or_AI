The discussion and notations regarding the primary equation—(2)—appear to be incorrect or, at the very least, misleading:
An expectation is a quantity of the form \sum f(x) p(x). The expected quantity, f(x), is distinct from p(x), which represents the probability measure.
In equation (2), the loss is clearly not the expected quantity. Consequently, the statement on page 2, second paragraph, claiming that (2) exhibits an "anomaly" due to its lack of invariance to loss translation, is technically inaccurate. Furthermore, the subsequent Example 1 is built upon this flawed assumption, which, unsurprisingly, does not hold.
From a practical perspective, the estimator in (2) is sensitive to its denominator, pi, which can become 0 if the sample has not been observed in the prior. The modified estimator in (7) appears to be a principled improvement, as it mitigates the influence of pi through the use of a double denominator.
I sincerely hope the authors will consider revising their commentary if the paper is accepted.
The experimental results demonstrate that Norm-POEM outperforms POEM across all datasets and performs comparably to a CRF that uses full multi-label information. This paper introduces a method for "counterfactual learning," a learning paradigm that is a specialized form of multi-label learning. In traditional multi-label learning, the ground-truth annotation for a sample provides a 0/1 label for each class; in this case, however, only the label for a single class (i.e., bandit feedback) is available. Nevertheless, this serves merely as a justification preamble. The paper's actual focus is precise: to enhance an estimator (POEM) that was recently introduced in reference [1] at ICML 2015. Overall, I find the paper compelling and recommend it for publication, albeit with some reservations outlined in the Comments to authors section.