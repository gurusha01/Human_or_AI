This paper introduces the CCAdL method, which enhances the SGNHT method in scenarios where the variance of stochastic gradients varies across the parameter space.
The trade-off is that the CCAdL method requires explicit estimation of the stochastic gradient variance. This estimation can be performed on minibatches, which is computationally efficient. However, it introduces additional noise stemming from the variance estimator itself (as acknowledged by the authors on line 193). While the thermostats can stabilize the system by neutralizing constant noise, the noise introduced by the estimator is evidently parameter-dependent.
As a result, there remains some residual error in the system that cannot be fully eliminated, similar to the SGNHT. Nevertheless, the strong experimental results suggest that the impact of this residual error may be less significant compared to the original error from the stochastic gradients. It would be valuable if the authors could further investigate and characterize this residual error in comparison to the stochastic gradient error.
The paper is well-written, and the experimental results are promising, albeit limited to small-scale experiments. However, without a more thorough analysis of the error introduced by the new stochastic noise, the contribution of the paper may be considered incremental in terms of overall novelty. The proposed algorithm builds upon SGNHT by incorporating an estimator for the variance of stochastic gradients, but it remains an incremental improvement with an unresolved issue regarding the introduced noise.