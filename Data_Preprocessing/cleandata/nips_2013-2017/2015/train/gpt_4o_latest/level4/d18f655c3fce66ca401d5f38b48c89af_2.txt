This paper investigates general-purpose training algorithms for deep learning and introduces a novel family of methods termed elastic averaging SGD (EASGD). The concept is innovative, and the paper is of exceptional quality.
The study addresses the challenge of training large-scale deep learning models under communication constraints. This is a difficult problem due to the prevalence of local optima in non-convex problems, such as those encountered in deep learning. The optimization problem is framed as a global variable consensus problem to ensure that local workers do not converge to different local optima. The gradient update rules are then reinterpreted in terms of elastic forces between local and global parameters, which is the basis for naming the proposed family of algorithms elastic averaging SGD. The family consists of four variants: a synchronous version, an asynchronous version, and their respective momentum-based versions. These algorithms are shown to be promising both theoretically and experimentally.
In the theoretical analysis, the authors provide a compelling illustrative example where the widely-used ADMM can exhibit instability with an unknown stability condition, whereas the stability condition for EASGD is straightforward. The writing in this section is particularly clear and engaging. While the main paper includes only a brief portion of the theoretical analysis, with the full theorems relegated to the supplementary material, the organization is seamless and easy to follow (though I did not thoroughly examine the supplementary material). I have a few minor questions here: What is the primary factor contributing to the superior stability of EASGD? Could it be related to the symmetry of EASGD's maps compared to the asymmetry of ADMM's maps? Additionally, is it possible to construct an alternative illustrative example where EASGD might exhibit greater instability than ADMM?
The experimental results demonstrate the effectiveness of the proposed algorithms in training convolutional neural networks, outperforming state-of-the-art SGD methods on CIFAR and ImageNet datasets. The findings highlight that EASGD performs better than other SGD methods as communication periods increase, as illustrated in Figure 2. Furthermore, Figures 3 and 4 show that EASGD achieves superior speedup compared to baseline methods when the number of workers increases. In our own experiments, we implemented similar gradient update rules adapted to our computational clusters (based on the arXiv version of the paper), which successfully improved our baseline performance. The resulting models have been, or are planned to be, deployed online, benefiting approximately one-fifth of the global population. This underscores the significance of the paper. 
In summary, this paper presents a novel and high-quality contribution to the field of deep learning optimization through the introduction of the elastic averaging SGD family of algorithms.