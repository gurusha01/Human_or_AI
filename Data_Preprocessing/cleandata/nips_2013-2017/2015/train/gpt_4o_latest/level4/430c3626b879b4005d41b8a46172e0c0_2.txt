The authors present a method called pre-conditioning, a numerical technique that employs a linear change of variables to facilitate gradient descent in a better-conditioned parameter space. Specifically, they introduce the use of a pre-conditioner termed "equilibration," which involves row-normalizing a matrix, to pre-condition the loss Hessian. The authors further propose a practical numerical method to approximate the equilibration pre-conditioner with minimal additional computational cost compared to standard gradient-based training. Through experiments, they demonstrate that this approach lowers the condition number of random Hessians, and they provide a theoretical argument showing that it reduces an upper bound on the Hessian's condition number, although a more direct proof is absent. They also show that the resulting algorithm, termed "Equilibrated Gradient Descent" (EGD), accelerates convergence for deep neural networks on certain datasets. Moreover, they suggest that the widely used heuristic algorithm RMSProp can be interpreted as an approximation of equilibrated EGD.
In my view, this is a clear and insightful paper that offers a well-rounded analysis. The work is original and has the potential to be immediately beneficial for neural network training. The paper is well-written, with precise experimental results and solid theoretical contributions on a topic of considerable significanceâ€”providing both a justification for the effectiveness of one of the most commonly used online training algorithms for neural networks and a new, relatively practical online training method.