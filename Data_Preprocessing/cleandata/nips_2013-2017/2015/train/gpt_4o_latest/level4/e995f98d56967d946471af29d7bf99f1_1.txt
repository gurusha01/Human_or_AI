Recent advancements in neural machine translation and other text generation tasks have focused on training models to directly minimize perplexity or the negative log-likelihood of observed sequences. While this approach has demonstrated highly promising results, it overlooks a critical practical issue: during inference, the model conditions on its own generated symbols rather than the gold-standard symbols. This discrepancy can lead to conditioning on contexts that differ significantly from those seen in the training data.
This paper addresses this issue by incorporating generated sequences into the training process. Instead of always conditioning on the gold-standard context, the method alternates between conditioning on generated and gold contexts. However, early iterations of this approach produce low-quality outputs, so the authors introduce a "scheduled sampling" strategy. This strategy alternates between the two training methods based on a predefined decay schedule, inspired by curriculum learning.
The paper's primary strength lies in its simplicity and the breadth of its empirical evaluation. The authors clearly define the key model and inference assumptions, while appropriately omitting unnecessary details about the internal architecture. This clarity makes the approach straightforward to implement on LSTMs or other non-Markov models.
Empirical results indicate that the method performs well, showing significant improvements across several diverse tasks. The scheduling mechanism appears to play a crucial role, as always sampling from generated contexts performs poorly in comparison. 
That said, the parsing results remain well below state-of-the-art, though this may be due to the use of a highly simplified input representation (lacking features). Similarly, the speech results are obtained in a somewhat unique experimental setup, but the observed improvements in this domain are substantial.
One point of interest is how performance varies based on the distinction mentioned in footnote 1. Flipping at the token level seems fundamentally different from flipping at the example level, as the worst-case distance between gold tokens is much smaller. Clarifying this distinction would be valuable.
The paper's main weakness is its lack of comparison to other methods that aim to address similar challenges. 
First, the authors dismiss early-update perceptron (Collins and Roark, 2004) with beam search too quickly, claiming it is inapplicable because "the state sequence cannot easily be factored." While factoring is indeed used in parsing, beam search itself does not inherently require this assumption. In fact, beam search is often employed precisely when dynamic programming is infeasible, as noted on line 130. The continuous nature of the state should not preclude the use of this algorithm. Moreover, a related paper presented at this year's ACL, "Structured Training for Neural Network Transition-Based Parsing," applies a similar method to a neural network model with comparable assumptions.
Second, the paper does not adequately distinguish its approach from SEARN and reinforcement learning algorithms. The related work section describes these as "batch" methods, but this characterization is overly restrictive. While the SEARN paper may have used a batch multi-class classifier for efficiency, the approach itself is not inherently incompatible with stochastic gradient descent. The core idea of SEARN—interpolating between the model's current predictions and gold-standard outputs to generate sampled trajectories—bears similarities to the scheduled sampling strategy. The primary distinction is that SEARN learns the policy, whereas this paper relies on a predefined schedule.
In summary, this paper introduces a simple yet effective method that improves performance across multiple sequence modeling tasks. However, the lack of rigorous baseline comparisons and the dismissal of related methods as inapplicable are notable concerns.