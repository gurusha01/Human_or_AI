I appreciate that this paper evaluates classifier accuracy in comparison to human performance. It is intriguing in Table 2 to observe that the LENSE system surpasses the 3-vote baseline but falls short of the 5-vote baseline. This provides an important comparison regarding the relative effectiveness of de-noising crowd responses through additional consensus versus the LENSE system's approach. However, the example provided in the paragraph beginning at Line 154 feels abstract and somewhat challenging to grasp. To enhance clarity, it might help to include a concrete query example with crowd responses and timings that closely resemble a real-world scenario. Additionally, in Tables 2 and 3, the inconsistent terminology—where the threshold baseline is alternately referred to as "entropic" and "threshold"—is confusing. I am also unclear on the rationale for placing the related work in Section 5. Overall, the paper addresses an important and compelling problem, but some of the explanations lack clarity.
Line 086: Figure ?? This paper addresses the challenge of building high-accuracy classifiers starting from zero labeled examples. A key subproblem explored is managing noisy crowd responses, and while this issue has been extensively studied in the literature, the paper's focus on timing and latency optimization is both novel and engaging.