Review - Paper Title: Bayesian Dark Knowledge
Paper Summary: This paper introduces a method for approximating Bayesian neural network models while addressing the significant storage demands during training and computational overhead during prediction. Conventional Bayesian approaches rely on generating samples and constructing a sample-based approximation to the posterior predictive distribution. However, this typically necessitates storing multiple parameter sets, which can be prohibitive for high-parameter neural networks. Additionally, predictions on test data require evaluating all sampled models, which is computationally expensive. This work proposes a solution by training a "student" model to approximate the predictions of a Bayesian "teacher" model, leveraging a process known as "distillation" (or "model compression" in prior literature). Importantly, the proposed method trains the student and teacher models simultaneously in an online fashion, eliminating the need to store large collections of samples even during training. The paper demonstrates the approach on synthetic datasets (a 2D binary classification problem and a 1D regression problem), as well as on the MNIST classification dataset and a housing regression dataset.
Comments:
- Developing computational tools for practical Bayesian inference in neural networks, particularly those that address the storage and memory challenges associated with sampling in high-parameter Bayesian models, is a valuable research direction. I believe this paper makes meaningful progress in this area.
- While the core concept of this work—student/teacher learning or model distillation/compression—is not novel, the paper's primary contribution lies in the innovative execution of simultaneous online training of the teacher and student models without requiring sample storage. This online distillation/compression approach is noteworthy because the teacher is never fully trained (i.e., it does not provide complete posterior-predictive labels for the student), yet the student successfully learns to approximate the fully trained teacher's model.
- A potential computational concern with the proposed algorithm is that the student must be trained to mimic the teacher on the training dataset D' at every iteration of the MCMC algorithm. This likely demands significantly more training of the student compared to standard distillation methods, which typically do not require multiple passes through the training dataset D'. This could pose challenges, particularly if a large D' is necessary for good performance. The paper does not provide sufficient discussion regarding the training set D', including how its specific characteristics might impact the method's performance and the details of its use in the experiments.
- The experimental section of the paper feels somewhat incomplete and could benefit from further refinement. The authors acknowledge that they were unable to fully compare their method to others on all datasets due to limitations such as "the open source code for the EP approach only supports regression" and "we did not get access to the code for the VB approach in time for us to compare to it." While I appreciate the transparency, it would significantly strengthen the paper to address these limitations and present a more comprehensive experimental evaluation. 
Overall, I find the goal of this paper—developing methods for Bayesian neural networks that avoid the need to store and evaluate multiple copies of the model—to be important. The proposed method is a clever application of existing ideas, but the experimental section requires further polishing to fully realize the paper's potential.