This paper investigates linear estimation with a nonlinear link function. Building on the recent work of Plan and Vershynin, it analyzes the asymptotic estimation error using generalized lasso for arbitrary link functions with \(\mu \neq 0\). The key finding, as presented in Theorem 2.1, is that the asymptotic error remains identical regardless of whether the link function is linear or nonlinear.
Theorem 2.1 is derived under the assumption that the linear measurement vector is sampled from a normal distribution, which could be restrictive in certain scenarios. The authors may need to clarify why this assumption is essential for the validity of their conclusions.
A few questions arise: 1. How should the regularizer be selected based on the set \(\mathcal{K}\) in which \(x_0\) resides? While the paper discusses the assumption of a convex regularizer, this specific issue does not appear to be addressed. 2. Although the results demonstrate equivalence between linear and nonlinear measurements, the conclusions depend on \(\mu\) and \(\sigma\), which characterize the difficulty of estimation from the nonlinear function \(g\). It would be helpful to understand whether the authors have considered the necessity of these dependencies.
Additionally, some references are missing. 1. The single index model and sufficient dimension reduction literature examine linear vector/subspace estimation under unknown link functions, but the paper does not cite any works in this domain. 2. A recent paper, "Optimal linear estimation under unknown linear estimation" by Yi et al., introduces a novel algorithm for sparse recovery, even when the link function has \(\mu = 0\). This work establishes the equivalence of high-dimensional linear estimation with linear and nonlinear link functions under appropriate conditions, supported by rigorous theoretical proofs and empirical evidence. Overall, this paper presents well-written and intriguing results.