The authors introduce a kernel-based algorithm designed for the cross-domain matching task, where the objective is to retrieve a corresponding instance in the target domain given an instance in the source domain. Each instance from the two domains is represented as a bag of features (e.g., words) and mapped to a shared space (RKHS) using kernel embeddings for the purpose of matching. The features of the two domains are jointly learned by maximizing a likelihood that is inversely related (intuitively) to the distance (i.e., MMD) between the embedded instances in RKHS. On real-world datasets, the proposed approach demonstrates superior performance compared to standard methods such as CCA and kernel CCA in tasks like document-document, document-tag, and tag-image matching.
The paper is well-written and easy to follow. The experimental results are compelling, as the proposed method consistently outperforms other approaches across all three tasks. In the field of machine learning (excluding information retrieval), cross-domain matching is predominantly addressed using methods like CCA or kernel CCA. Thus, this work provides a valuable starting point for further exploration in this area.
Regarding originality, while the concept of embedding a bag of hidden features using kernels and learning the features is not novel (as explored in [18], "Latent Support Measure Machines for Bag-of-Words Data Classification," NIPS 2014), the originality of this work lies in applying such an approach to cross-domain matching using a probabilistic model akin to kernel logistic regression. However, despite the strong experimental results, the paper lacks sufficient motivation, justification, and detailed explanation of the method's advantages, which would enhance its clarity and impact.
My comments and questions, ranked by importance, are as follows:
1. Lines 70-78: The limitations of kernel CCA are not clearly articulated. This section is critical for motivating the proposed approach and should be improved.
2. The learned latent vectors \(xf\) and \(yg\) seem to play a more significant role in the algorithm than the kernel \(k\). How does the Gaussian kernel \(k\) compare to a linear kernel when the latent dimension \(q\) is large? My intuition suggests that with a sufficiently large \(q\), the latent vectors may have enough flexibility to perform the task even with a linear kernel, potentially rendering the kernel unnecessary.
3. What is the rationale behind the likelihood in Eq. 8? It appears to be an ad-hoc choice. Why not use \(\|m(X) - m(Y)\|^2\) as the loss in Eq. 11? Removing the log term in Eq. 11 would simplify and reduce the computational cost of optimization.
4. How does the proposed method compare qualitatively (rather than quantitatively) to kernel CCA? I recommend shortening Sections 4.1 and 4.3 to provide more detailed qualitative insights.
5. It would be valuable to analyze the effect of varying the latent dimension \(q\) on both precision and runtime. The current experiments focus solely on precision.
6. How does the computational cost of the proposed method compare to kernel CCA? Kernel CCA appears to be more efficient. The gradient in Eq. 12 is likely expensive to compute, and the objective function is highly non-convex. This issue should be addressed in the paper.
7. Lines 284-286: Are the hyperparameters selected via cross-validation? Does "development data" refer to a validation set?
8. In the experiments, does kernel CCA use the same learned features \(xf\) and \(yg\) from the proposed method? If not, what kernel is used for kernel CCA (e.g., Gaussian kernel applied to what)? What are the results if the same features are used?
9. Abstract, lines 27-28: The phrase "while keeping unpaired instances apart" is unclear. How is this criterion implemented in the method?
10. Lines 100-102: The claim that the method "can learn a more complex representation" is vague. Can this be demonstrated experimentally?
Minor comments:
1. Eq. 3: \(\|m(X) - m(Y)\|^2\) is not a distance; \(\|m(X) - m(Y)\|\) (or MMD) is.
2. It should be explicitly mentioned in the experiments that \(x_f\) represents a latent vector for feature \(f\).
3. The formatting of the references is inconsistent, with some entries using abbreviated author names.
===== After Rebuttal =====  
I have reviewed the authors' rebuttal, which addressed only a subset of my questions.
The paper presents an interesting kernel-based algorithm for cross-domain matching, supported by experiments on three real-world tasks. However, the motivation for the approach remains unclear, and the qualitative explanation of the method is insufficient.