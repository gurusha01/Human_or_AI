Paper 921 presents a novel variant of the adaptive SGD method for non-convex optimization, referred to as "equilibrated gradient descent" (ESGD). The authors claim that the equilibration preconditioner outperforms the Jacobi preconditioner both theoretically and empirically.
Regarding the experimental evaluation, the use of MNIST and CURVES datasets is insufficient, as these are relatively simple tasks and do not provide substantial insights into the effectiveness of the proposed algorithm. I recommend conducting additional experiments on more challenging datasets. Furthermore, when employing neural networks, the primary focus should be on test accuracies rather than solely on "training error (MSE)." Additionally, comparing ESGD with SGD using "epoch" as the x-axis is not sufficient. It would be beneficial to include another figure where the x-axis represents training time, measured in "minutes" or "hours."
The paper is somewhat difficult to follow in certain sections. (1) In Algorithm 1, should $H$ be recalculated at every iteration? If so, a subscript for $H$ should be included. Moreover, in line 277 (the update of $\theta$), should the term be $\sqrt{D / i}$? (2) In Section 4, the paper appears to argue that $D^{E}$ significantly reduces the condition number $\kappa(D^{E-1}H)$, and this point should be explicitly clarified. (3) In line 178, the meaning of $D^{-1}=\Vert A{I,.}\Vert2$ is unclear and requires further explanation. (4) In line 267, $R(H,v)=...$ is presented as a vector, whereas (11) is a scalarâ€”this discrepancy needs clarification regarding the intended meaning. Lastly, since no theorem is provided to establish a convergence rate for ESGD, a more detailed explanation of its theoretical properties is necessary.
In summary, I recommend including more rigorous and convincing experiments as well as providing clearer explanations of the proposed ESGD method to enhance the paper's overall clarity and impact.