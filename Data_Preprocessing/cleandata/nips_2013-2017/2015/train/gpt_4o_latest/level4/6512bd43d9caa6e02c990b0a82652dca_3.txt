SUMMARY: Building on the foundational approach to algorithmic stability introduced in [6], this paper explores the connection between generalization and stability. Unlike prior work, the authors tackle this problem in the broader context of general learning settings. The contribution is noteworthy as it provides necessary and sufficient conditions for (uniform) generalization, rather than merely establishing bounds. However, the primary limitation lies in the "uniform" aspect: it remains unclear whether this condition is overly stringent, as it requires generalization with respect to any parametric bounded loss.
DETAILED COMMENTS: The paper is well-written and presents an intriguing contribution.
As the authors themselves note, this work complements the findings in [14] by focusing on necessary and sufficient conditions for generalization, rather than learnability, within the general learning framework.
It is worth noting that the machine learning community has historically been divided between those who prioritize learnability and those who emphasize generalization. Personally, I find both perspectives valuable and distinct enough to warrant separate lines of inquiry. Consequently, the topic of this paper is undoubtedly relevant to a segment of the NIPS community, even though it does not address learnability.
That said, the paper has certain shortcomings. The most significant issue is the limited depth in the discussion of its implications (e.g., Remarks 1-2 and Examples 2-3). The finite VC dimension case is the only one that is clearly articulated, and some remarks and comments give the impression of mild overselling. This lack of depth, combined with the unconventional definition of algorithmic stability (relative to a portion of the ML community), may restrict the paper's accessibility and impact. This is particularly important because the definition of uniform generalization (Definition 3) appears quite strong. Its justification hinges on the authors' claim that the corresponding algorithmic stability condition (Definition 5) is "rather weak," implying that most algorithms would satisfy it and, therefore, generalize even with respect to any bounded loss. However, this assertion is presented in vague and intuitive terms rather than in a manner that would resonate with the broader ML community.
While this is a concern, I do not believe it warrants rejection.
As I understand it, the loss function considered here is independent of the algorithm employed. The algorithm itself is not required to be an ERM or to rely on ERM in any capacity. As such, there are no concrete grounds to deem the uniform generalization condition "too strong," apart from subjective intuition. On the other hand, the algorithmic stability condition does seem weak to me and, more importantly, has a compelling representation in certain cases (e.g., finite VC dimension).
Another minor issue, also noted by other reviewers, is that the bounds are provided only in expectation. However, I consider this a relatively minor drawback. Many results based on algorithmic stability are also in expectation, yet they have successfully inspired new algorithms and techniques.
If the paper is accepted, I strongly encourage the authors to include more detailed examples and/or quantifications of algorithmic stability in additional interesting cases. This would enhance the impact and accessibility of their work.
In conclusion, I find the results to be correct, novel, and sufficiently mature for publication at NIPS. The paper establishes a necessary and sufficient condition for (uniform) generalization using a novel notion of algorithmic stability in the general learning setting. It also provides a few examples and implications. While a deeper discussion of the implications would strengthen the work, the paper is clearly written and makes a significant contribution that merits acceptance.