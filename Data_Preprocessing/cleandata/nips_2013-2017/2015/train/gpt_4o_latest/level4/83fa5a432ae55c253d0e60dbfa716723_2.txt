The authors introduced a novel dimensionality reduction technique aimed at identifying the most divergent direction between input datasets X and Y. The key innovation of the proposed method lies in leveraging the squared Wasserstein distance as a measure of discrepancy, which is optimized using semidefinite programming. Experimental results demonstrate that the proposed approach performs favorably in comparison to existing methods.
Quality: The method is technically sound.
Clarity: The manuscript is well-written and straightforward to understand.
Originality: The proposed approach is innovative. The problem addressed in this paper is closely related to that of Transfer Component Analysis, which seeks to identify a subspace that minimizes the discrepancy between two datasets.
Significance:  
The combination of Wasserstein distance with semidefinite programming is intriguing and has the potential to make a meaningful impact within the machine learning community.
Detailed Comments:  
1. The problem could be reformulated using a simple Lasso-based approach. For instance, by assigning positive pseudo-labels to X and negative pseudo-labels to Y, and solving the problem \( ||Y - Z^T \beta||2^2 + \lambda ||\beta||1 \), one might achieve comparable results. However, this approach is only effective when X and Y are linearly related, making it a suitable baseline for comparison.  
2. Could the proposed algorithm be extended to handle nonlinear relationships?  
3. Transfer Component Analysis (TCA) could also be adapted to identify the most divergent direction. Although TCA was originally designed to find a common subspace, it can be easily modified for the task at hand. Refer to [this paper](http://www.cse.ust.hk/~qyang/Docs/2009/TCA.pdf) for more details.  
The proposed formulation is compelling. Including a simple Lasso-based baseline would further strengthen the paper.