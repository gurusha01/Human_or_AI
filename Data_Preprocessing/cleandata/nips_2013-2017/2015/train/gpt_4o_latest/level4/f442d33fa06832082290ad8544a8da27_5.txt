TL; DR This paper introduces a natural extension of neural word embeddings: sentence embeddings. The proposed "skip-thoughts" model is trained to predict the preceding and succeeding sentences based on the encoding of the current sentence. This unsupervised training approach leverages raw text data, and the resulting embeddings can be utilized as features for downstream tasks. The authors evaluate the model on eight tasks, and while it does not achieve state-of-the-art performance, it surpasses several prior baselines with reduced feature engineering.
The "skip-thoughts" model represents a clever adaptation of the increasingly popular sequence-to-sequence RNN framework. It has been well-established (e.g., Ando & Zhang, 2005) that learning representations of observed data X through auxiliary tasks can enhance downstream predictions of Y. Applying this concept at the sentence level is an interesting variation, albeit not entirely novel (e.g., the cited paragraph vector work and earlier references therein). The technical approach, which builds on existing RNN frameworks, appears sound, and I particularly appreciated the vocabulary expansion strategy.
However, I have concerns regarding the experimental evaluation. The key question is whether the proposed method offers a superior approach to learning from large unlabeled datasets (X) compared to alternatives, in terms of downstream task performance (Y). Unfortunately, this question is not adequately addressed in the experiments, as there is no baseline that uses the same dataset (Book11K). Consequently, it is unclear whether the reported results stem from (a) leveraging a larger dataset or (b) the model capturing meaningful patterns beyond surface-level word co-occurrence statistics.
A comparison with a strong baseline trained on the same dataset would have been valuable. In the absence of this, even a weak baseline, such as a bag-of-words model, would have been informative. Another option would have been to train word embeddings on Book11K and incorporate them into more engineered baselines to ensure comparable data conditions.
One common limitation of RNNs in the encoder-decoder framework is the need for the encoder to condense the entire input into a single vector. Techniques such as reversing token order or using attention mechanisms have been proposed to address this issue. A discussion of this limitation would have been helpful. Additionally, more details on the following points would have strengthened the paper:
- The tuning process for the proposed approach and its sensitivity to hyperparameters.
- Any alternative architectures explored, such as predicting only the next sentence from the current one (it is not immediately clear why predicting sentences in reverse is beneficial, though I am open to the idea as it effectively doubles the training data).
- How performance scales with increasing amounts of unlabeled data (this could potentially be inferred from checkpoints of prior training runs).
UPDATE AFTER AUTHOR RESPONSE:
Including a baseline with word-level embeddings trained on Book11K would enhance the paper. However, the most natural comparison would be with the paragraph vector model [1], under matched training and testing conditions. The absence of this comparison is puzzling and should be clarified in the paper.
[1] http://arxiv.org/pdf/1405.4053v2.pdf
The "skip-thoughts" model is an inventive application of the sequence-to-sequence RNN framework. However, I remain concerned that the experimental evaluation primarily examines scenarios where there is a data mismatch between the baselines and the proposed method.