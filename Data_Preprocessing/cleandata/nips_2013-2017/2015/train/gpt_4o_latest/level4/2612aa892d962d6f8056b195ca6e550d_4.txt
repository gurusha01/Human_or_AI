This paper introduces a method that integrates trajectory optimization with policy learning to facilitate interactive control of complex characters. The core idea is to jointly optimize trajectories and policies, with the additional empirical observation that decoupling these into alternating subproblems i) allows leveraging established, well-documented methods and ii) performs adequately in practice. While I was able to follow the paper's content, I lack sufficient expertise in this specific domain to fully assess its potential impact. On the surface, the work appears to demonstrate considerable technical sophistication, though it does not seem to offer fundamentally novel theoretical insights.
The authors exhibit an impressive level of technical depth across all aspects of the work—from conceptual formulation to implementation—yielding equally impressive results. The benefits of i) incorporating sensory noise during training and ii) jointly learning trajectories and policies are convincingly demonstrated in the experiments.
The paper is well-written, with an appropriate level of detail given the complexity of the problem and its implementation. Each section addresses a distinct aspect of the problem, and the cross-references between sections are particularly helpful for understanding the overall framework.
That said, I believe the rationale for the additional optimization step in Section 7 could have been explained more clearly. Based on the preceding sections, I had expected the policy to operate independently without requiring further optimization. Is this additional step necessary due to residual trajectory costs or policy regression errors? If so, does feeding the policy's action into the physical simulator lead to error accumulation? I had assumed that the forward dynamics inherently respect physical constraints, preventing any physical implausibility, but perhaps I am overlooking something? I do recognize, however, that re-optimizing at every timestep enables the use of larger timesteps, which is a practical advantage.
Line 153: "similar to recent approaches..." Could you provide one or two references here? I initially found it counterintuitive that keeping updates small in parameter space would generally be effective. However, I later understood its utility in your scenario, where alternating between subproblems is a key aspect of the method.
Typo: Line 89: "to unify." Overall, the results presented in this paper are quite impressive, and the proposed optimization scheme and implementation appear sound. However, as this research area is somewhat outside my expertise, I find it challenging to confidently evaluate the novelty and significance of the contributions.