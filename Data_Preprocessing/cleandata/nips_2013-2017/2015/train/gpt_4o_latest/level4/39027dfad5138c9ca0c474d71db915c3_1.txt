Since I am currently traveling, I had very limited time to complete this (additional) light review. This is unfortunate because I found the paper quite engaging. It is possible that I may have overlooked or misunderstood certain points, and if so, I apologize.
Given data observed under one hypothesis, we may be interested in evaluating a different hypothesis. When these hypotheses are non-deterministic (e.g., conditional distributions), the loss under the hypothesis being evaluated can be estimated by reweighting the data points, as shown in the equation between (1) and (2).
The authors appear to tackle a problem that is related to, but slightly distinct from, the issue of large variance in the weights. They refer to this issue as "propensity overfitting." The examples provided suggest that this problem arises when there is insufficient explorationâ€”i.e., for a given x, we observe very few or even no y values corresponding to the new hypothesis being evaluated. Since a similar reasoning underpins the large variance of the weights, it would be helpful if the relationship between these two problems were elaborated on in greater detail.
I did not have the opportunity to thoroughly examine the technical details or assess the novelty of the proposed solution, but the underlying idea appears reasonable.
- l. 199: I find this unclear. If n is small relative to k, we observe only a few data points where xi = yi, correct? In that case, wouldn't \hat R(h^) be even smaller than -2? Am I misunderstanding something?
- "unbiased counterfactual risk estimator used in prior works on BLBF [4, 5, 1]." I am unsure if this is an entirely accurate statement. While I am not familiar with all three papers, I suspect that they employ techniques like clipping, which introduce bias to reduce variance.
- I appreciate the availability of the code.
In summary, I was unable to verify the technical correctness of the paper. However, it presents intriguing ideas that I would like to explore further. I therefore recommend acceptance.