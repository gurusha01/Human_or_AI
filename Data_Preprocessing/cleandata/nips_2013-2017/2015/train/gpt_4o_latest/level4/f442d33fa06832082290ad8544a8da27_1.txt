To clarify after reviewing the authors' response: the pre-trained sentence vectors are straightforward to use and will likely prove to be highly practical. My earlier point was that retraining the model would not be as straightforward as using a tool like word2vec.
The GitHub package shared by the authors does not appear to include the training component at this time. The paper presents an intriguing application of sequence-to-sequence models by adapting the skip-gram concept, where the current sentence is used to predict the preceding and succeeding sentences. While the experiments demonstrate the potential of this approach, the model does not surpass the state-of-the-art performance in any of the evaluated scenarios. Considering the significant time required to train the model, it may pose challenges for users attempting to retrain the vectors on their own domain-specific corpus.