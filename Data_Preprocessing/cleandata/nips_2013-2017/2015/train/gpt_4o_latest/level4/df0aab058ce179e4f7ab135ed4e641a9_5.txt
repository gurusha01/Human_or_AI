This paper tackles the challenge of learning effective representations in an unsupervised manner. It introduces a 'deep' (stacked) variant of the factor analysis model, incorporating posterior constraints. The optimization process employs an EM algorithm with a projection step in the E-step to ensure the constraints are consistently satisfied. The proposed approach is compared against several popular unsupervised methods, demonstrating consistent improvements.
To the best of my knowledge, this work is the first to apply posterior regularization to a deep factor analysis model and solve it using a straightforward projected version of the EM algorithm.
However, Section 2 is not clearly written and appears unnecessarily complex.
The authors claim significant speedup for their algorithm compared to the standard Newton method, but it is unclear how the number of steps mentioned in line 145 is computed. Additionally, it would be beneficial to include timing comparisons across different methods.
In the first experiment, since PCA achieves zero reconstruction error with 100/150 code units, the dataset appears to be trivial. When comparing RFN and RBM, RFN performs better in terms of reconstruction, while RBM excels in sparsity. It is difficult to conclude that RFN is significantly better, and further explanation of the reported numbers is needed, particularly regarding what constitutes a substantial difference.
In the second experiment on pre-training, the method shows an improvement of approximately 0.5%. However, on MNIST, such a small difference is not particularly meaningful, and the proposed approach performs poorly on CIFAR. It would be valuable to include experiments on larger and more realistic datasets.
Overall, this paper presents a novel unsupervised learning model for sparse, non-linear, high-dimensional representation learning. The experimental results highlight the model's effectiveness.