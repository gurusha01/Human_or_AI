Review - Paper Title: Variational Consensus Monte Carlo
Paper Summary:  
This paper introduces a novel approach for aggregating samples in a low-communication MCMC framework. In this framework, a large dataset is distributed across multiple cores, with each core performing MCMC on a subset of the data (i.e., drawing samples from a subposterior distribution). The samples are then aggregated using a combination function to approximate the full posterior distribution. Unlike prior methods that rely on a fixed combination function, this work extends the Consensus Monte Carlo (CMC) approach [Scott et al., 2013] by defining a family of combination functions and employing variational inference to optimize over this family. This optimization identifies the combination function that yields the best approximation (within the family) to the full posterior distribution. A key advantage of this method is its potential to facilitate the aggregation of more complex mathematical objects (beyond simple vectors) due to the generalized formulation. The proposed method is evaluated on a Bayesian probit regression model, a normal-inverse Wishart model, and a Gaussian mixture model.
Comments:  
- The development of scalable Bayesian inference methods that achieve accurate posterior approximations is an important research direction, and this paper makes a meaningful contribution toward this goal. I appreciate the innovative perspective of framing an existing low-communication parallel MCMC strategy as an optimization problem over a variational objective, effectively bridging two distinct approaches in approximate Bayesian inference.
- The primary objective of low-communication parallel MCMC methods is to accelerate inference while preserving a high-quality posterior approximation, particularly in comparison to communication-intensive parallel MCMC methods. This paper successfully demonstrates that its proposed method achieves a strong posterior approximation relative to existing techniques. However, the experimental evaluation is somewhat limited in assessing whether the method retains the speed advantages of low-communication MCMC. While the paper includes a comparison of speedups with the CMC method for one model, a more comprehensive analysis across additional models and scenarios would strengthen the evaluation.
- I am curious about how the proposed VCMC method compares to serial variational inference methods in terms of both inference speed and approximation error. Although the method is presented as a "variational Bayes algorithm," the comparisons in the paper are exclusively against MCMC methods, not variational Bayes approaches. Since the method fundamentally involves optimizing an approximate posterior derived from distributed sampling, it seems natural to benchmark it against variational inference methods. This comparison is particularly relevant given the growing popularity of scalable variational inference techniques, such as stochastic gradient variational inference. Additionally, recent advancements in low-communication parallel variational inference would provide valuable points of comparison. For instance, the following works could be considered:  
  (1) Broderick, Tamara, et al. "Streaming variational bayes." Advances in Neural Information Processing Systems. 2013.  
  (2) Campbell, Trevor, and Jonathan P. How. "Approximate Decentralized Bayesian Inference." UAI. 2014.  
- One limitation of the proposed method is its lack of guarantees regarding the correctness of the aggregated samples. Unlike some existing scalable MCMC methods that attempt to provide such guarantees, this approach simply selects the best combination function from a predefined family. However, this limitation is not necessarily worse than that of most variational inference methods, which also optimize over a restricted family of approximations. Moreover, the proposed method should generally outperform the CMC method [Scott et al., 2013], assuming the weighted-average combination function is included in the predefined family.
- The paper includes some theoretical results, such as blockwise concavity under specific conditions and its implications. However, the purpose and significance of these results are not thoroughly discussed. It would be helpful to elaborate on why these results are important and how they relate to the broader variational inference literature, where certain types of concavity results are commonly analyzed. Currently, these results are presented without sufficient context or explanation.
In summary, this paper makes notable progress toward the development of scalable approximate Bayesian inference methods, particularly in the context of low-communication parallel MCMC. The idea of optimizing over the sample aggregation function is both clever and effective, as evidenced by the improved results compared to CMC. However, the paper would benefit from a more extensive empirical evaluation of inference times and speedups, as well as comparisons with other methods, especially variational inference approaches.