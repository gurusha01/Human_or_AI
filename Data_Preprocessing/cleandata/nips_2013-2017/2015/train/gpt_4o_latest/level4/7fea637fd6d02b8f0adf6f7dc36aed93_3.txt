The paper presents a novel approach to regret minimization in multiplayer normal-form games. In particular, it extends existing algorithms (Optimistic Mirror Descent and Optimistic Follow the Regularized Leader) from the setting of two-player zero-sum games to the more general case of multiplayer general-sum games.
The experimental results, when compared to Hedge, are impressive. The paper's content is likely to resonate with the NIPS community.
I have several specific questions and comments:
- Page 3: Why is the square root removed from the logarithmic term in the claim "converges to O(n log(d) sqrt(T))," whereas in r_i(T) the square root includes the logarithmic term? Is this a typographical error?
- Page 6: The statement "This is the first fast convergence result to CCE using natural, decoupled no-regret dynamics" raises two concerns: 1. The terms "natural" and "decoupled" are not clearly defined. Additionally, how is it "decoupled"? Corollary 12, which underpins this claim, requires all players to adopt OFTRL with specific choices for M_i^t and \eta. 2. The claim seems overly strong. What about Hart & Mas-Colell's regret-matching approach (e.g., [13])? Please clarify, rephrase, or consider removing this assertion.
Some broader considerations:
- Do you have any thoughts or comments on whether these results could be extended straightforwardly to the partial-information setting using sampling techniques (e.g., Optimistic Exp3)?
Minor points for the camera-ready version, if accepted: - Page 1: The phrase ".. a chink that hints"â€”what does "chink" mean in this context? - Page 3: Remove the extra space after "PoA" in "(PoA )". - Page 4: At the end of the proof of Theorem 4, is there a missing max{j \in N} in the rightmost inequality? Perhaps the j subscript should be removed after eliminating the \sum{j \neq i}. - Ensure that "argmax" is formatted correctly so that the set being maximized over is fully contained under the "argmax" (i.e., avoid separating "arg" from "max"). - Several items in the bibliography need corrections. For instance, volume, issue number, and page details are missing for [4], and the full conference names should be written out for [19] and [20].
This is a well-written paper that provides meaningful new contributions to the challenging domain of no-regret learning in multiplayer general-sum games. It extends prior results, demonstrates that players using the same algorithm (with the RVU property) can achieve faster convergence than in adversarial settings, and shows that in adversarial scenarios, the worst-case O(log(T) sqrt(T)) regret remains achievable using a parameterized step-size and the doubling trick.