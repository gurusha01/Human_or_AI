The authors introduce a variational objective designed to minimize the KL divergence to a "population posterior," which is defined as the expectation of the standard posterior under an empirical distribution. Using this framework, they derive a streaming variational algorithm where the objective is parameterized by an empirical distribution.
The introduction appears to suggest that online Bayesian posteriors converge to a point mass. However, asymptotically, isn't this simply a result of consistency? Is the intended point that such convergence can occur prematurely? If so, to what extent is this issue attributable to the variational (or other) posterior approximation, which is known to underestimate uncertainty? In other words, is the core problem with Bayesian inference itself or with the specific approximation being employed?
Eq (3): Should "min" be replaced with "argmin"? Overall, this is an excellent treatment of streaming Bayesian inference through variational methods. The experiments are compelling, and the formalism is both clear and elegant.