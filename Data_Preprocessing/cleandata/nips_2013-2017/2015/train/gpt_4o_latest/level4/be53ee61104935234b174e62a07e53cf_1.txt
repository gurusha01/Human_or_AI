The paper presents a solid technical contribution by unifying a significant body of work on isotonic regression (IR). The core idea, which involves leveraging techniques from fast solvers of linear systems, is intuitive and technically sound. From the standpoint of novelty and technical rigor, I find no issues with the work (though I must acknowledge my limited background in verifying the proofs).
However, my primary concern is that the paper might be more appropriate for an algorithms or theoretical computer science venue, such as those where the prior work it builds upon ([16]–[20]) and the techniques it employs ([21]–[29]) were published. It is not clear whether the results presented would resonate strongly with the broader NIPS community. Specifically:
- While IR has recently found some interesting applications in learning problems, it does not appear to be a central machine learning tool for which a faster algorithm alone would generate widespread interest. For an ML-focused paper, I believe there needs to be an additional learning-specific insight or extension. For example, [12] introduced a faster algorithm for Lipschitz IR fits, where the Lipschitz constraint was motivated by statistical considerations, and the resulting algorithm demonstrated clear statistical benefits over standard IR. 
- In the IR applications I am most familiar with, such as probabilistic calibration ([0], [-1]) and learning single index models (SIMs) ([10], [12]), the proposed algorithms do not seem to offer runtime improvements, as these applications typically involve highly structured DAGs. While faster runtimes for general DAGs are of significant algorithmic interest, I feel that a more direct impact on an ML-specific problem is necessary for this venue. If there are other learning applications where the proposed algorithms provide a substantial advance, these should be highlighted and discussed more explicitly.
Additional comments:  
- There is prior work on showing that the PAV algorithm is optimal for a general class of loss functions ([-2] and references therein), which may be worth citing.  
- From my initial reading, it seems that [14] focuses on the standard L2 norm rather than a general Lp norm.  
- On page 6, consider formatting the four points about Program (5) as bullet points for clarity.
Typos:  
- Page 1: "IF it is a weakly"  
- Page 4: "ACCOMPANYING"  
- Page 6: "show that the $D$ factor"  
- Page 7: "Regression on a DAG"
References:  
[0] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '02). ACM, New York, NY, USA, 694–699.  
[-1] Harikrishna Narasimhan and Shivani Agarwal. On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation. In NIPS 2013.  
[-2] Niko Brummer and Johan du Preez. The PAV Algorithm optimizes binary proper scoring rules.  
In summary, the paper introduces new algorithms for solving weighted isotonic regression problems under general Lp norms, with favorable complexity compared to existing methods. However, the machine learning implications of the work remain somewhat unclear.