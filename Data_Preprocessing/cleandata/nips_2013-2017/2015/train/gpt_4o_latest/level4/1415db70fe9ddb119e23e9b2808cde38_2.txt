In this work, the authors present a modification of the EM algorithm tailored for scenarios where the true parameter is both sparse and high-dimensional. They establish certain conditions under which the algorithm converges to the global solution and derive its rate of convergence. Additionally, the authors develop a statistical testing framework to assess the sparsity of the estimator.
The paper is well-structured, and the related literature is adequately reviewed. However, the content is quite dense, and certain sections (primarily Sections 3 and 4) are challenging to follow. Simplifying these parts would enhance readability for the audience.
- While the authors claim in line 066 that sqrt(s*log(d/n)) is minimax-optimal, I was unable to locate a proof for this assertion in the main manuscript (I did not review the supplementary material). Could you provide a clear proof for this claim?
- In Eq. (3.8), ^s depends on s, but s is unknown in practical scenarios. How should ^s be selected in such cases? Furthermore, how can one determine an appropriate initial value beta^init such that ||beta^init - beta*||_2 is less than R/2 (as mentioned in line 275) in practice?
- I am unclear about the significance of the truncation step in Algorithm 1. Could you elaborate on its importance and clarify what would occur if this step were omitted from the EM algorithm?
 After rebuttal 
Thank you for your detailed response. It addressed my concerns, and I have raised my score from 6 to 7. This is a solid statistics paper, though it remains somewhat difficult to digest for a machine learning researcher. Additionally, the practical applicability of the proposed method to real-world scenarios is still somewhat uncertain.