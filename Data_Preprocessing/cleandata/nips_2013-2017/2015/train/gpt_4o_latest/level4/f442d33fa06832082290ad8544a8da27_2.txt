The authors present an encoder-decoder framework for learning vector representations of entire sentences. Analogous to how CBOW leverages the distributional semantics of words, the authors hypothesize that sentences in natural discourse exhibit similar distributional properties. If this hypothesis is valid, training a model to predict a sentence's neighboring sentences should yield effective sentence representations. This approach is particularly compelling because, like CBOW/SkipGram, it allows for unsupervised training, using the data itself to guide the learning process.
The paper further explores a specific implementation of this idea, employing gated RNNs for the encoding and decoding functions. As the authors point out, similar architectures have been utilized in machine translation, and this model can be interpreted as "translating" a given sentence into its preceding and succeeding sentences. The authors evaluate their model across eight tasks, demonstrating that the learned embeddings (encoder hidden states) are semantically meaningful and effective for training classifiers, achieving performance that is competitive with or superior to many existing systems.
Overall, the paper is well-written, though I have a few comments:
*In the experimental results section, it is not always clear whether the reported numbers for certain systems are taken from the literature or reproduced by the authors. For instance, in Table 3, it is evident that the challenge numbers are sourced from the literature, but in Table 6, it is unclear whether the paragraph vector was re-trained on the same dataset as the skip-vector model (the book data) or if the reported figures are also drawn from the literature.
*Another potential limitation of the gated RNN implementation of skip-thoughts is its training complexity compared to paragraph vectors. Training the model appears to take significantly longer (potentially more than a week). While paragraph vectors require inference to embed new sentences, is this inference process truly more computationally expensive than having the gated RNN process a sentence token by token?
*It may be beneficial to expand Table 4 with additional sentence pairs that illustrate cases where skip-thoughts outperform the baseline. A useful proxy for this could involve identifying sentence pairs with low lexical similarity but high semantic similarity (both predicted and actual semantic similarity).
In summary, this is a well-written paper with a novel contribution and a comprehensive experimental evaluation against multiple baselines across eight tasks.