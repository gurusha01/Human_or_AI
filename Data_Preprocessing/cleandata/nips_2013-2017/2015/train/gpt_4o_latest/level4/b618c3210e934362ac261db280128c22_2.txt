This paper demonstrates empirically that incorporating latent variables into an RNN enhances its generative capabilities, as evidenced by high-quality samples and improved likelihoods (or tighter lower bounds on likelihoods) compared to prior approaches for modeling distributions over continuous variables.
The manuscript is generally well-written and clear, though the figure is somewhat confusing and caused some initial difficulty in understanding. As acknowledged by the authors, their model represents a minor modification of, for example, [1]. The authors appear to have conducted a fair comparison with the model proposed in [1], and they effectively illustrate how introducing additional structure to the prior over latent variables z_t (by making the mean and variance functions of the previous hidden state) improves generation performance.
My primary concern with the paper is its incremental nature. Since the main contribution lies in improving the prior over zt, I would have appreciated a deeper analysis of why this modification is beneficial (beyond empirical results). Additionally, unlike prior work, the authors use the same hidden state ht for both generation and inference. What is the rationale behind this design choice?
For the speech experiments, could the authors provide more details about the windowing process for the 200 waveform samples? Specifically, was there any overlap between consecutive samples?
The authors might consider citing DRAW as another example or application of a VRNN-like architecture. While the idea of adding latent variables to RNNs has been explored in recent work, this paper is well-written and introduces a minor modification that achieves superior performance compared to earlier efforts at combining the variational AE framework with RNNs.