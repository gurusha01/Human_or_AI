This paper introduces a pixelwise classification framework leveraging recurrent convolutional connections.
The proposed method builds upon a multiscale convolutional network akin to the approach by Farabet et al., but distinguishes itself by incorporating shared weights and additive skip-connections across layers within the networks applied at each scale.
The model's performance is assessed on the Stanford Background and SIFT Flow datasets.
While the approach demonstrates commendable results for a model trained from scratch, I believe several aspects warrant deeper exploration and evaluation.
* First, the impact of sharing weights across recurrent connections could be contrasted with a configuration where distinct convolution kernels are used for each iteration. Although this modification would increase the number of parameters, it might also enhance performance without significantly altering computational cost (i.e., maintaining the same number of connections).
This comparison was partially addressed, but the CNN2 model has fewer total layers and a smaller receptive field.
Additionally, the findings regarding varying gamma (enabling/disabling skip connections) might differ under these conditions.
* Second, the influence of N and T could be further investigated.
As far as I can discern, all RCNN networks are configured with T=3, except RCNN-large, which uses T=4.
However, RCNN-large is only evaluated for N=3.
How would it perform with N=4 or N=5?
* Furthermore, in Table 2, RCNN is reported to achieve a performance of 83.5/35.8 PA/CA.
Yet, in Table 1, RCNN-large appears to deliver superior performance with 83.4/38.9 PA/CA (last row).
Is there a specific reason why the latter configuration was not included in Table 2?
* The related work section mentions that [4] and similar networks rely on postprocessing techniques, such as superpixels, to enforce consistency in neighboring pixel labels.
This suggests that the proposed model may not require such postprocessing, but this claim is neither substantiated through experiments nor illustrated with qualitative examples.
No qualitative predictions are provided, leaving the nature of the outputs unclear.
* Additionally, there are some recent works in this domain that could be discussed or compared against, such as Chen et al.'s "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs" and Eigen & Fergus's "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture."
While the approach achieves promising results for a model trained from scratch, I believe certain aspects could benefit from more thorough exploration and evaluation.