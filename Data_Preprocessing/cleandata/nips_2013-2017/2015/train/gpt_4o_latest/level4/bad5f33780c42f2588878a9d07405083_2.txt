Summary
This paper examines a scenario where an analyst has access to both a training set and a hold-out set drawn from the same distribution. The analyst designs methods using the training set but also repeatedly evaluates the hold-out set to make critical decisions. While this repeated use of the hold-out set can generally lead to overfitting, the authors demonstrate that overfitting can be avoided if the analyst employs the methods proposed in the paper. This is supported by the main results, Theorems 9 and 10, which extend prior work based on differentially private methods.
Positive Points
A key strength of this work is that it does not impose a priori restrictions on the class of functions (e.g., limiting it to a class with bounded VC dimension) from which the analyst can select checks. In such cases, standard uniform concentration inequalities for VC classes would suffice. This is discussed in lines 226-232 of the paper.
Another notable contribution is the generalization beyond differentially private methods to include algorithms whose outputs can be encoded using a small number of bits. This unification is achieved through the concept of (approximate) max-information. Unlike algorithmic stability, which is closely tied to generalization, max-information enables the composition of multiple algorithms in sequence. This is elaborated in lines 234-243.
Additionally, the high-level exposition in the paper is generally clear and well-written.
Negative Points
Despite the positive aspects highlighted above, I have significant concerns regarding Theorem 9, one of the main results. These concerns lead me to believe that the paper requires substantial revision before it can be considered for acceptance.
Main Concerns
Regarding Theorem 9:  
1. The theorem makes a deterministic claim "for all i such that ai ≠ ⊥," but the condition "ai ≠ ⊥" is inherently random. It depends on the adaptively chosen functions φ_i, the randomness in the data, and the randomness introduced by the algorithm.  
2. The in-probability statement bounds the likelihood that ai deviates significantly from the expectation of φi for a fixed i. However, if the analyst adaptively makes decisions based on the algorithm's output, ai must remain close to the expectation of φi for all i simultaneously. Otherwise, the analysis could go astray at some point.  
3. I could not verify the claim in lines 119-120 of the introduction, which I paraphrase as follows: the number of queries m can be exponential in n as long as the budget B is at most quadratic in n. Specifically, the result does not seem to degrade with m, which I suspect is related to point 2 above. For the result to be practically useful, I would expect τ to decrease with n, for instance, as τ ~ 1/√n. However, if B is quadratic in n, then having τ approach 0 would cause β to diverge, rendering the result vacuous. Consequently, I am uncertain about the appropriate parameter choices for the theorem.
Regarding the experiments: why is the classifier constructed to use only weights of +1 and -1 based on the signs of correlations? Is there a realistic scenario where such a method would be employed in practice?
Minor Remarks
As noted by the authors in their proof of Theorem 23 in the supplementary material, the extension to algorithms with outputs encoded in a small number of bits can also be derived using a union bound argument. While this is hinted at in lines 171-179 of the introduction, the discussion there may not make this point sufficiently clear.
A Potential Strengthening
I would like to suggest a potential strengthening of the results, which may or may not be of interest.  
The results are fundamentally based on a change of measure from the joint distribution P of a sample and an algorithm's output on that sample to the independent product distribution Q of the two (Theorem 4). If P and Q are sufficiently close in terms of "max-information," the sample can essentially still be treated as fresh even after observing the algorithm's output.  
It may be worth noting that, for β = 0, the bound on max-information can be relaxed. Specifically, max-information can be interpreted as the Rényi divergence Dα(P‖Q) of order α = ∞. Since Rényi divergence increases with α (see, e.g., [1]), bounding D∞(P‖Q) imposes the strongest restriction. Theorem 4 states that a change of measure is possible if D∞(P‖Q) is small. However, it is also possible to change measures under the weaker condition that Dα(P‖Q) is small for any α > 1. For α = 1, we recover standard mutual information, which, as the authors note, is insufficient for a change of measure.  
This result is essentially a special case of Lemma 1 from [2], but the connection may not be immediately apparent. To clarify, let X be an indicator random variable that equals 1 if (S, A(S)) ∈ 𝒪 and 0 otherwise. Then, applying Lemma 1 from [2] with EP[X] in place of LP(h, f), EQ[X] in place of LQ(h, f), and M = max X = 1, we obtain:  
EP[X] ≤ (2^{Dα(P‖Q)}  E_Q[X])^{(α-1)/α}  M^{1/α},  
which simplifies to:  
P[(S, A(S)) ∈ 𝒪] ≤ (2^{D_α(P‖Q)} * Q[(S, A(S)) ∈ 𝒪])^{(α-1)/α}.  
As α → ∞, this recovers Theorem 4, but the result also holds for smaller α.  
References:  
1. Van Erven, Harremoës, "Rényi Divergence and Kullback-Leibler Divergence," IEEE Transactions on Information Theory, 2014.  
2. Mansour, Mohri, Rostamizadeh, "Multiple Source Adaptation and the Rényi Divergence," UAI 2009.  
Minor Issues (Typos, etc.)
- Line 114: Replace "training set" with "hold-out set" (if my understanding is correct).  
- Theorem 4: Two issues: "k = I∞^β(S; A(S)) = k" should likely be "I∞^β(S; A(S)) ≤ k."  
- Theorem 8: The sentence "Let Y = ... on input S." can be omitted.  
- Lines 314-323: The bounds mentioned describe the probability that the empirical estimate deviates by more than τ from the true accuracy.  
In conclusion, while the paper has several strong points, my significant concerns regarding Theorem 9 lead me to believe that substantial revisions are necessary before the paper can be accepted.