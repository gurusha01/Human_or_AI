The authors introduce a novel framework for estimating a lower bound on cross-validation (CV) errors as a function of the regularization parameter. However, the primary motivating question of the manuscript ("how many grid points are necessary for cross-validation?") has already been extensively explored from both theoretical and empirical perspectives. For instance, see (Rifkin et al., MIT-CSAIL-TR-2007-025) or (Rasmussen and Williams, Gaussian Processes for Machine Learning, Chapter 5).
The proposed score and validation error bounds are technically sound, as they are a direct application of concepts from safe screening (bounding Lagrangian multipliers at the optimal solution). While this connection is clearly explained in the supplemental material, including a similar clarification in the main text (e.g., Section 3) would enhance the manuscript's readability for the audience.
The experimental setup used to evaluate the theoretical approximation of the regularization parameter is not adequately described. For instance, how exactly is the grid search conducted? Is it based on 10-fold cross-validation on the training set to determine the optimal parameters, followed by final evaluation on the validation set? If so, it seems that the training set's E_{v} should have been reported in Fig. 3 instead of the validation set's results.
While the proposed score and validation error bounds are technically sound, the novelty of the work is incremental, and the experimental setup and results are underwhelming.