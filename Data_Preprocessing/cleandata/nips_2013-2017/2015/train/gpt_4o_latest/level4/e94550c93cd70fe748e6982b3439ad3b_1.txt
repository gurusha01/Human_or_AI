Consensus Monte Carlo (CMC) is a framework designed to parallelize MCMC for posterior inference on large datasets. The approach involves splitting the posterior distribution into sub-posteriors, each associated with a subset of the data, sampling from these sub-posteriors in parallel, and subsequently combining the sub-posterior samples via an aggregation function to approximate the true posterior. Existing methods for aggregation are either overly simplistic, leading to significant bias, or computationally expensive, which limits the practical applicability of CMC. This paper introduces a more systematic approach to aggregation by leveraging variational inference to optimize the aggregation function.
Clarity: The paper is well-written and straightforward to understand.
Significance: Addressing Bayesian inference for large datasets is a critical challenge. This work is significant as it proposes a practical improvement for parallel MCMC using CMC.
Originality: The paper's main contribution lies in employing variational inference to optimize CMC aggregation functions rather than relying on fixed, predefined ones. While this idea is relatively straightforward, the difficulty lies in estimating the entropy term, which the authors address by minimizing a lower bound instead. This is a meaningful innovation, as it enables the proposed method to significantly reduce errors in posterior expectation estimation compared to simple baselines.
Quality: While the method appears theoretically sound, the experimental evaluation is somewhat lacking. The experiments are limited to toy problems and small datasets. Although these examples effectively demonstrate key aspects of the algorithm, such as its ability to aggregate structured samples, more realistic experiments are needed. Parallel MCMC is particularly valuable for large datasets, yet the largest dataset used in the paper contains only 50K datapoints. Additionally, the experiments focus on relatively simple models, such as Gaussian mixture models and normal-Wishart distributions. Evaluating the method on more complex and practical models, such as LDA, would make the work more relevant to practitioners, especially since large, real-world datasets are available for such models. Furthermore, the authors compare their method only against very simple baselines. It would be helpful to see comparisons with more sophisticated alternatives, such as Neiswanger et al.'s method or the Weierstrass sampler by Wang and Dunson, in scenarios where these methods are applicable. Comparisons with serial mini-batch algorithms like Stochastic Gradient Langevin Dynamics (SGLD) would also be valuable. While the proposed method could incorporate SGLD for sub-posterior sampling, it remains unclear whether the additional computational cost of aggregating samples and optimizing the aggregation function offsets its advantages over standard SGLD. Finally, the experiments would be more compelling if they demonstrated the method's utility in scenarios where Bayesian inference offers a clear advantage over point estimates, such as cases where point estimates overfit or where Bayesian uncertainty is explicitly utilized.
In summary, this is a solid and well-written paper, but the experimental section leaves room for improvement. While I am inclined to recommend acceptance, rejecting the paper to allow for more comprehensive and realistic experiments would not be a significant loss.