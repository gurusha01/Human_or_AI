I found the methodology and experiments presented in this paper to be quite persuasive, with only a few minor comments to address.
If the focus is on predictive inference, as is often the case for applications involving large datasets, it might be more appropriate to aggregate functions of the parameters relevant for prediction rather than the entire set of parameters.
For instance, in the case of probit regression, one could aggregate the fitted probabilities, while for the mixture of Gaussians example, a similar approach could be applied to cluster membership probabilities.
I am curious about whether the results are more or less sensitive to the choice of aggregation method in such scenarios.
Have the authors conducted any experiments to explore this aspect?
Another question that came to mind pertains to structured aggregation in the context of positive semidefinite matrices.
There are alternative reparametrizations that could be employed here, such as Cholesky factorization, and I am curious why the aggregation is restricted to the D(Î›_k) matrices.
Is there a specific reason why this approach was deemed the most suitable?
Minor comment:
There appears to be a missing integration in equation (3).
This paper enhances recently proposed consensus Monte Carlo algorithms from the literature by incorporating variational Bayes methods into the aggregation step of these algorithms. The relaxation of the variational objective function proposed by the authors is innovative, and I found the experimental results compelling, demonstrating that the suggested approach offers improvements over simpler methods.