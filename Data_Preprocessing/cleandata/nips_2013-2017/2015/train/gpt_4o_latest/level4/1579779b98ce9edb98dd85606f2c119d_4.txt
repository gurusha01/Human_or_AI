The paper introduces an algorithm for global optimization of decision trees by reformulating the problem as (latent) structured prediction. Here, the latent structured variables are represented as sign vectors that encode the decisions at each internal node of the tree.
The authors propose a non-differentiable and non-convex loss function, which they approximate with a differentiable (but still non-convex) upper bound.
Optimization of this loss using SGD involves solving a sequence of loss-augmented inference problems, for which the authors provide efficient algorithms. Experimental results are presented, comparing the proposed method to a standard greedy training approach.
The main contributions of the paper are the reformulation of global decision tree optimization as structured prediction and the development of efficient algorithms to optimize the resulting objective.
The approach is novel and thought-provoking, raising several interesting questions.
However, I have concerns regarding some practical aspects, particularly the motivation for the method and the significance of its performance improvements over simpler baseline methods.
As a method for global decision tree optimization, the approach has intriguing aspects.
One notable advantage of framing the problem as structured prediction is the ability to perform SGD by solving a sequence of loss-augmented optimization problems that exploit the problem's structure.
That said, the most compelling result of this approach, in my view, is the sparsity of the gradient updatesâ€”if I understand correctly, each gradient step modifies only one node's weight vector.
While the paper primarily highlights this as a computational advantage, I believe this property opens up more interesting possibilities that are not fully explored in the paper.
For instance, one could envision optimizing over an infinitely deep tree by initializing all weights to zero and progressively growing the tree by activating more weights, allowing the model's complexity to adapt naturally to the data.
This raises intriguing questions: for example, would such a method resemble or diverge from known greedy strategies?
This might be the case if it turns out that introducing a new node is always preferable to revising an existing node's weights.
Alternatively, with suitable regularization, could the method converge to a tree with a limited number of nodes (as suggested by Fig. 2)?
This issue strikes at the heart of what makes the proposed method compelling compared to naive decision trees and warrants a more thorough analysis than is currently provided.
For example, a critical ambiguity remains: what happens if a node's weight is zero?
The discussion on page 3 implies that the right branch is taken in such cases, which seems questionable.
A more reasonable approach might be to output a decision at this node, with the possibility of later activating the weight to split the node.
Clarifying this behavior is crucial, as it directly parallels pruning strategies in greedy training.
I also have a significant practical concern regarding the motivation for the work.
Plain decision trees are valued primarily for their interpretability; if interpretability is not a priority, other methods typically outperform decision trees.
As described, the proposed method seems likely to produce non-interpretable results, as L2 regularization would yield dense weight vectors.
Switching to L1 regularization might improve interpretability, but it is unclear whether performance would suffer if the model were forced to produce exactly 1-sparse splits.
This aspect requires empirical validation.
Another concern is the lack of detail in the experimental comparisons with baselines.
The paper does not specify which pruning strategies were used for the naive approach.
Given that the proposed method was tested with a range of regularization parameters, I would expect the naive method to be similarly tested with various pruning strategies and parameters.
Additionally, it would be fair to compare against other simple global optimization strategies, such as training a greedy method on random subsets of the data and selecting the model that minimizes the desired loss on the full training or validation set.
Regarding clarity, the paper is generally well-written, though some sections could be more explicit.
For example, Section 6.2 could be summarized as first maximizing the objective for all g corresponding to a given leaf, followed by maximizing over all leaves.
Similarly, Section 6.4 could benefit from greater clarity.
In conclusion, this is a clever and innovative approach that raises several interesting questions.
However, its practical utility remains uncertain due to the interpretability issue (why use decision trees if they are not interpretable?) and the lack of detailed experimental comparisons.
PS: Although the title mentions forests, the paper does not address this case.
POST-REBUTTAL COMMENTS
I maintain that the case of zero weights is a critical issue that warrants further exploration, as outlined in my initial review.
Based on the rebuttal, it appears that weights are likely initialized randomly, which explains why this issue does not arise in practice.
However, I believe the method would become more compelling if weights were initialized to zero and gradually increased.
I encourage the authors to explore this direction further. The paper presents a creative reformulation of global decision tree optimization as structured prediction, along with an efficient optimization algorithm. Nevertheless, some aspects of the motivation and experimental results remain unconvincing.