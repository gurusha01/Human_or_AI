The authors present a compelling paper on a backpropagation-based training method that leverages spike probabilities while addressing the hardware constraints of a specific platform featuring spiking neurons and discrete synapses. This topic is highly relevant to the ongoing efforts to map deep networks onto multi-neuron hardware platforms.
Quality: The proposed training method is particularly valuable in light of the constraints imposed by emerging multi-neuron hardware platforms.
Clarity: The paper is well-written and easy to follow. However, the claims at the end of Section 1 could benefit from rephrasing, as they might be misinterpreted to suggest that this is the first-ever training methodology involving spiking neurons and reduced-precision synapses. Additionally, I would not classify the demonstration of running the network on a specific hardware platform (in this case, TN) as a novel contribution. Instead, this should be framed as a validation of the proposed training method.
In Section 2, the meaning of "0.15 bits per synapse" is unclear and requires clarification. The network topology is not comprehensively described in one location, which would help the reader. Further, how much additional training time is incurred due to the probabilistic synaptic connection updates? Why does the single ensemble of the 30-core network yield such low results compared to the 5-core network? Why are the results across different ensembles of the 5-core network so consistent? What are the spike rates for the inputs during testing, and is the input represented as a spike train? Additionally, Ref. 12 on line 399 incorporates constraints (e.g., bias=0) during training, so it is not purely a "train-then-constrain" approach (there is also a typographical error on this line: "approach approach").
Originality: Could the authors include a discussion or comparison with other spiking backpropagation-based methods, such as SpikeProp?
Significance: Developing training methods that account for hardware constraints is a critical area of research. However, the constraints addressed in this paper are specific to the TN architecture and may not generalize to other platforms. Moreover, if the results are based on a 64-ensemble configuration, this implies that significant hardware resources are required to achieve the desired network accuracy. Overall, this is an intriguing paper that explores a backpropagation-based training method while considering the hardware constraints of a platform with spiking neurons and discrete synapses.