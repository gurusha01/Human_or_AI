Here's the paraphrased version of your review:
Given CRF potentials, one can derive the marginal predictions by simply passing the initial messages from factors to variables. This paper integrates that parametric formulation into a neural network and directly optimizes it to minimize training error. The proposed architecture demonstrates empirical success on a significant benchmark. However, I found the framing of the work somewhat misleading: Based on my understanding, the reported cost function focuses solely on pixel-wise errors rather than structured prediction, and no actual CRF model is being trained.
In my view, "structured CRF prediction" implies the existence of a joint distribution over the labels conditioned on the input. With such a model, one could generate plausible joint configurations of labels or identify the most probable joint labeling.
The "Intersection over Union" score, however, does not account for whether the vector of labels is jointly plausible. Maximizing this score involves predicting the most probable label for each pixel independently. While training a CRF model would involve learning a joint distribution, the focus here is on maximizing predictive performance, which does not depend on the joint distribution. Furthermore, it is unclear to me whether the "messages" described in this paper always correspond to realizable factors, let alone likely ones.
That said, the work is undoubtedly interesting. The concept is innovative and serves as an effective heuristic for defining the parametric form of the output layer in a neural network to achieve strong performance. I simply found the framing to be somewhat misleading and initially confusing.
Line 332: "One advantage of message learning is that we are able to explicitly incorporate the expected number of inference iterations into the learning procedure." -- However, only a single iteration appears to have been implemented and tested. I am not convinced that this claim has been fully substantiated or explored in sufficient detail.
Typo: line 54: massage -> message
There are several instances of incorrect word endings, often involving singular/plural mismatches. I stopped noting them after realizing a spell checker had not been used: line 66: potenials, line 120: sematic, line 265: convlutional.
In summary, this is an interesting approach to designing the output layer of a neural network for pixel labeling, inspired by the parametric form of CRF messages. However, as far as I can tell, it does not involve fitting a structured prediction model as claimed. Nothing in the rebuttal has changed my perspective: this paper does not perform structured prediction as advertised, though it does appear to be effective for pixel-wise labeling.