In this paper, the authors investigate the estimation of sparse graphical models, which is framed as a nonconvex optimization problem. To address this, they propose an alternating minimization algorithm. The paper provides a discussion on the optimal statistical rate of convergence and the consistent graph recovery properties. The manuscript is well-structured and clearly presented. However, I have a few questions:
(1) Regarding the sparsity of the solution \(\Omega\): Problem (2.2) is designed to promote the sparsity of each precision matrix \(\Omega_i\). However, in the simulation section, there is no explicit sparsity result reported. On page 8, lines 414-415, it is mentioned that "Tlasso tends to include more non-connected edges than other methods." Does this imply that the computational results of Tlasso are not particularly sparse? Could you provide an explanation for this observation?
(2) Concerning the initialization of Algorithm 1 (page 4, lines 206-210): You state that the obtained estimators are relatively insensitive to the choice of initialization. However, in your simulations, using \(1_{mk}\) as the initialization led to better numerical performance. Why does this initialization perform better? How much worse can the results be with random initialization? Could you include a brief discussion on this point?
In summary, the authors study the estimation of sparse graphical models, framed as a nonconvex optimization problem, and propose an alternating minimization algorithm to solve it. The paper discusses the optimal statistical rate of convergence and consistent graph recovery properties. Overall, the manuscript is well-organized and clearly written.