- The authors argue for the importance of non-negative transforms, such as ReLUs, in successful models by stating that "Representations learned by ReLUs are not only sparse but also non-negative" and "Correctness means that the RFN codes are non-negative, sparse, have a low reconstruction error, and explain the covariance structure of the data." However, this argument is weakened by the existence of MSR's work on PReLUs, which outperform ReLUs despite not being non-negative. The only compelling advantage of ReLUs appears to be their non-saturating nature.
- The authors summarize their goals as follows: "In summary, our goal is to construct input representations that (1) are sparse, (2) are non-negative, (3) are non-linear, (4) use many code units, and (5) model structures in the input data." The significance of goal 2 (non-negativity) is unclear, and goal 4 (using many code units) is questionable—effective representations should ideally capture the same structure with fewer hidden units. A more meaningful goal, such as achieving low reconstruction error, is notably absent.
- The authors claim, "Current unsupervised deep learning approaches like autoencoders or restricted Boltzmann machines (RBMs) do not model specific structures in the data. On the other hand, generative models explain structures in the data but their codes cannot be enforced to be sparse and non-negative." This statement is ambiguous—are they implying that RBMs are not generative models, or is this simply unclear writing? Hopefully, it is the latter.
- Table 1 is perplexing. While the authors claim it demonstrates their method achieves the lowest reconstruction error, PCA is shown to have lower reconstruction error in the table.
- Figure 2 is intended to illustrate the robustness of the method to background noise. However, the filters appear significantly less robust compared to those produced by denoising autoencoders.
- Table 2 is misleading. It compares classification accuracy on a computer vision task across deep learning models that leverage pretraining, showing the authors' method as competitive with the state of the art. However, they omit Schmidhuber's convolutional autoencoders (http://people.idsia.ch/~ciresan/data/icann2011.pdf), which outperform their method on MNIST and significantly outperform it on CIFAR-10. For instance, the authors report a test error of 41% on CIFAR-10, while Schmidhuber's method achieves 22%—and that result is from 2011. The current state of the art achieves a test error of 9%.
For the experiments, the following improvements would strengthen the paper:
- In the "RFNs vs. other unsupervised methods" section, the authors should have included explicit comparisons with sparse coding methods (e.g., l1-regularized regression). Additionally, they should have evaluated sparsity and reconstruction error on real-world datasets rather than relying solely on toy datasets.
- In the "RFN pretraining for deep nets" section, the comparisons would have been more compelling if the authors had included a broader range of pretraining methods. Instead, they primarily compare their approach to non-deep-net methods, which misses the point of evaluating pretraining strategies for deep learning.
- It would also have been valuable to present classification results directly on the output of RFNs compared to the output of other unsupervised methods.
The authors have made commendable progress in applying posterior regularization to factor analysis, and the results are promising for unsupervised learning applications. However, the results are less encouraging for pretraining strategies in deep learning models. Specifically, classification accuracy on CIFAR-10 is significantly worse than that achieved by convolutional autoencoders and far below the current state of the art.