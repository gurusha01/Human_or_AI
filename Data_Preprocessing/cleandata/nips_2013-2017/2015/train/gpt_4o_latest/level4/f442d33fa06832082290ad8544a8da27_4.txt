The authors propose a technique inspired by Tomas Mikolov's Skip-gram model to encode sentence representations. The encoded representation corresponds to the final layer in the sentence sequence of a GRU RNN. Using this representation, the model is trained on a Billion Words corpus (sourced from books) to predict both the preceding and succeeding sentences in a paragraph, functioning as a GRU RNN Language Model conditioned on the encoded representation. Although the model is trained on a relatively modest vocabulary size (20K), the authors introduce a straightforward method to extend it to 1M words using word2vec embeddings and a linear mapping. They conduct an extensive set of experiments across various NLP tasks, utilizing the raw sentence representation as a feature.
I found the paper highly compelling and believe it serves as a strong foundation for future research in learning sentence representations, with the potential to extend to paragraph-level representations. One possible critique of the paper is that the proposed method does not achieve a new state-of-the-art performance. However, the research community is well aware of the challenges involved in developing representations that generalize effectively across a broad spectrum of NLP tasks.
In my opinion, the only missing experiment is fine-tuning the GRU RNN from the sentence representation on a few datasets mentioned in the paper to assess how close the resulting improvements would come to the state of the art. Regarding the bidirectional model, do you have results for a single model that integrates both the forward and backward passes, rather than concatenating the representations?
Minor comments:
- In the 2nd and 3rd paragraphs of the Introduction, references to Fig. 2 and Table 2 should be replaced with Fig. 1 and Table 1.
- Section 1, last paragraph: "such an experimental is begin" needs revision.
- Section 3.2, 2nd paragraph: "learned representation fair against heavily" requires correction.
Training details:
- How many words per second is the model able to process during training using Adam?
- How many passes through the entire dataset are performed during the two weeks of training?
Overall, I thoroughly enjoyed the paper and believe it is a promising starting point for advancing research in sentence representation learning, with potential applications at the paragraph level.