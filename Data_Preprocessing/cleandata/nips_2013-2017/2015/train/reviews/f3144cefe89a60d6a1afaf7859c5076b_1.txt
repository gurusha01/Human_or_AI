The authors propose a multilayer representation of count vectors using a hierarchical model. The model has hidden layers consisting of gamma variables drawn from distributions with factorized shape parameters.
The proposed Gibbs sampling based inference is capable of learning the widths of the hidden layers, with the first-layer width acting as limit. Using this, the experimental results show that a deeper network has better classification accuracy and perplexity than a single-layer model (which is equivalent to the model of [12]). The superior performance is attributed to capturing correlations between hidden units and modeling overdispersion.
The model and algorithm are clearly presented.
 The paper draws out the advantage of the multilayer over the single layer model -- equivalent to a nonparametric extension to the PFA presented in Zhou et al. (2015) -- and the experimental results are convincing.
 While the model is novel -- using nonnegative hidden units instead of binary, and automatically learning the widths -- the advantage over the leading competing method, that of the over-replicated softmax [21] -- a two layer DBM model -- is not addressed in any depth. The authors note that
the classification accuracy is worse than the over-replicated softmax [21] when the first-layer budget is 512. This is noted to possibly due to word preprocessing.
 Further, on pg 8., the authors draw observations about layer width decay rates, but these come from inference on a single data set. To what extent is this a function of the data vs. the budget of the first-layer? The claim on pg.2 that you reveal this relationship surely needs analysis on more than 1 dataset.
  A novel multilayer model of count vectors with nonnegative hidden units and Gibbs sampling-based inference is proposed. The merits over a (possibly wider) single-layer model has been clearly presented, and the experimental results are convincing. However, the paper does not demonstrate that the model is the state-of-the-art in a domain (e.g., topic modeling); neither does it demonstrate merits over the leading competing method -- the overreplicated softmax model of Srivastava et al. (2013), a deep Bolztman machine with binary hidden units.Further, the authors draw conclusions from results on a single data set.