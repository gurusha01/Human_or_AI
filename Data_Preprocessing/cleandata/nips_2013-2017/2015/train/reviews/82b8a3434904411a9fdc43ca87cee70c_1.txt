For the important goal of test error guarantees, the combination of ideas from screening together with approximate regularization paths as proposed here is completely novel, and nicely surprising. While the result here is only for binary linear classification, it might be a useful first step towards more general ML models. The paper is clearly written, and the experimental results are encouraging.
For the reader it might be nice to more clearly distinguish explicit definitions of eps-approximate solutions in terms of training error (1) and validation error (2), to avoid confusion. (and also 'eps-approximate regularization parameter'. maybe better call it 'eps-approximate best regularization parameter'?)
The authors should discuss the theoretical complexity of the proposed approach as a function of eps more in detail. More concretely in this sense, assume the C values would be chosen naively and equally-spaced, what do your results say about the interval size? This would also be a good contact point to discuss the connection to the path methods for training error (vs text as done here) more in detail.
minor: l061: the -> this l264 soluitons
(this is a light review) The paper contributes a novel approach to compute guarantees on the cross-validation error along the entire regularization path. This is a highly relevant first step towards much stronger methods for hyperparameter search, such that those methods could in the future come with guarantees for test error.