The authors propose an encoder-decoder approach to learning vector representations of entire sentences. In much the same way that CBOW exploits the distributional semantics of words, the hope is that in natural discourse, sentences will obey similar distributional properties. If the hypothesis holds then training a sentence to predict its neighboring sentences would produce good learned representations of sentences. This approach is particularly appealing, because like CBOW/SkipGram, the model can be trained in an unsupervised fashion using the data itself to constrain the weights.
 They also propose and study a particular embodiment of this idea in which they employ gated RNNs for encoding and decoding functions. As noted by the authors, similar models have been used for machine translation, and you can think of this model as "translating" a current sentence into a previous and next sentence. The authors evaluate the model on eight different tasks, showing the learned embeddings (the hidden states of the encoder) are semantically meaningful and useful for training classifiers (competitive or better than many existing systems).
Overall, the paper is well written, but I have a few comments:
*In the experimental results section, it's not always clear if the number reported for a system is from the literature or if it's reproduced from scratch. For example, in the first results table (Table 3), it's clear that the numbers from the challenge are ones reported in the literature, but in Table 6 it's not clear if paragraph vector was re-trained on the same data as the skip-vector model (the book data), or if the reported number is from the literature.
*Another potential weakness of the particular gated RNN implementation of skip-thoughts is that compared to paragraph vectors, it is much more difficult to train (more than a week to train?). While I can see that paragraph vector requires inference to embed new sentences, is this inference really that much more expensive than having the gated RNN consume the sentence one token at a time?
*It might be worth extending Table4 with additional sentence pairs that show the cases in which skip-thoughts improve performance over the baseline. A proxy for this would be to search for sentences with poor lexical similarity, but high semantic similarity (both predicted and true semantic similarity). A well written paper with a novel contribution and thorough experimental evaluation against numerous baselines on eight different tasks.