This paper addresses the problem of learning a good representation, unsupervised. It proposes a 'deep'(stacked) version of factor analysis model, with a posterior constraints. The optimization is done by EM with a projection in E step making sure constraints are always satisfied. It compares with several other popular unsupervised method and show a consistent improvement.
To the best of my knowledge, it's the first to introduce posterior regularization to a deep factor analysis model and solve it using a simple projected version of EM algorithm.
Section 2 is not clearly written which reads much more complicated than it should be.
The author claims huge speedup for their algorithm over regular Newton method but how is the number of steps computed at line 145? and it would be better to compare timing for different methods as well.
For first experiment, as the PCA gives 0 error on reconstruction with 100/150 code units, the dataset seems to be trivial. While comparing RFN and RBM, RFN is better in terms of reconstruction but RBM is better in terms of sparsity. It's hard to say RFN is much better and also it would be better to explain more about the numbers themselves, i.e. how much difference should be considered huge.
For second experiment on pre-training, it shows improvement of around 0.5%, but on MNIST, this difference doesn't mean much. And the proposed method doesn't do well on CIFAR. So it would be great to see experiments on more real and larger dataset. This paper proposes a novel unsupervised learning model for learning sparse, non-linear, high-dimensional representation. Experimental results demonstrate the effectiveness of the model.