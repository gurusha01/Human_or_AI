This paper presents methods for estimating policy gradients in risk-sensitive problems with coherent risk measures, and proposes RL algorithms based on them. The paper particularly extends the existing methods for specific risk measures to the general class of coherent risk measures, and thus provides a unified view. The algorithm is illustrated with a toy example in finance domain, where each asset has its own return distribution.
 The motivation is very clear, and the problem formulation and algorithms follow naturally from this motivation and previous works. The experiment is relatively simple as it is done in only a static risk problem. However, the results seem to support the quality of the algorithm in terms of its risk-aversive behavior as well as the flexibility in different risk types.
Quality: the paper is well written. The equations are technically sound and all relevant references are given.
Clarity: the clarity of this paper is well above average.
Originality: The paper proposes new algorithms for risk-sensitive RL problems based on the related works on risk-sensitive planning and RL. The extension to proposed approach is not straightforward. The originality of this paper seems sufficient to me.
Significance: Risk-sensitive RL is an interesting and important problem, and this paper provides significant contributions.  This paper presents clear algorithms which solve risk-sensitive RL with coherent risk measures, and demonstrates the behavior of trained policies with different risk settings on a static risk problem. The proposed approach is clearly motivated by the authors, and a natural extension to existing algorithms and problems (policy gradient, actor-critic and coherent risk measure optimization).