The paper discusses high-dimensional sparse/group sparse estimation with l1/l1-l2 regularized least squares estimation. As opposed to the usual setting with Gaussian measurements and noise, the author consider isotropic sub-exponential measurements and i.i.d. subexponential noise. The main result of the paper is that the error/sample complexities bounds are essentially the same as for the usual setting, modulo an extra log factor. The results are based on advanced techniques from empirical processes.
In that analysis, the extra log factor results from the transition to the Gaussian width (usual setting) to the exponential width. Altogether the results can be seen as a meaningful extension of earlier results (Negahban et al[18], Rudelson & Zhou [20], and others).
Quality/Clarity: The technical quality of the paper is consistently high. It is well-written and organized.
 Originality/Significance: The paper studies a domain that has been extensively studied in the past years. The contribution of the paper is significant for experts in that domain, but would probably be considered as minor for the general audience. It is also not clear to what extent the techniques are novel given a series of related works on compressed sensing with measurements having heavier tails.
Specific comments/questions: - the authors study only isotropic measurements, wheres the non-isotropic sub-Gaussian case has been dealt with successfully in Rudelson and Zhou [20].
 Maybe the authors can comment on how their results could be extended in this direction - Gaussian-width based analysis is used excessively in Vershynin's tutorial on high-dimensional estimation. In my view, it is helpful to that cite that work - the experiments are particularly disappointing as there is no comparison between sub-Gaussian and sub-Exponential measurements. Such comparison would allow one to assess the sharpness of the authors' main result.  A valuable paper for people interested in the theory of compressed sensing and high-dimensional statistics.