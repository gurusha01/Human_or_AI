Comments:
- Tricks like non-uniform sampling and then correcting by importance sampling have been around for much longer, see e.g. [1] which claims a speed-up of 3x. Those older approaches chose the probability based on error instead of Li -- could you discuss/compare such an approach, at least in practice? I'd suspect this may be useful in practice where the Li may not be known.
- Channeling Leon Bottou: please discuss the limitations, and the cases where your method may not be applicable.
- I would really appreciate an open-source code release accompanying the paper, which would increase its impact.
- The experimental section and figures are clearly the weakest part of the paper, I'd recommend a serious second pass. For example, all plots should have error bars, state over how many runs they were averaged, labeled more clearly (SVRG -> full batch in top plots), the test-error plot does not have the resolution to be readable (maybe log-scale is better even if it won't go to zero). The same holds for the figures in the appendix. I do not understand why there is no difference at all between uniform sampling and Lipschitz sampling (in experiment 3)? Of course, a more challenging domain than logistic regression would be appreciated, but that can rightfully be shown in another paper.
 [1] Geoff Hinton, "To recognize objects, first learn to generate images", 2007.
Typos:
L19 a variant L53 increases over L77 expectation missing L96, L132 expectation symbol misformatted L164 B^s L289 can be used L316 superscript instead of subscript L325 sometimes L416 sometimes often? This is a solid and important paper about variance-reduced gradient descent. It has many good, new ideas, explains them clearly, and determines convergence rates for all its proposed variants. The paper's weakness is the experimental section.