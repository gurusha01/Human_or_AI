Overall 9 This paper develops and generalizes the principle of implicit exploration proposed in "Efficient learning by implicit exploration in bandit problems with side observations", NIPS 2014. . Implicit exploration consists in using a lower confidence bound to estimate the true loss.
While the previous analysis was done with respect to the expected regret, here, the regret bounds are given with high probability.
 The main claim of the paper is to demonstrate the advantage of high confidence algorithms based on an implicit exploration. The main result is a general concentration inequality concerning the sum of losses, which is provided in Lemma 1. Then, this result is applied to three variants of the non-stochastic bandit problems: the MAB problem with expert advices, tracking the best sequence of arms, the MAB problem with side-information. For these three problems, the analysis of the regret bounds shows an improvement of around a factor 2 of the pre-factors. Moreover, although that in practice Exp3.P using an explicit exploration is outperformed by standard Exp3, Exp3-IX outperforms Exp3 on the single but interesting experience done: Exp3-IX seems to be less sensible to the value of eta, and provides more robust estimation of the regret.
Remark 1: Exp3 does not work for the switching bandit problem (i. e. on the best sequence of arms). It will be interesting to add a comparison of Exp3.S and Exp3.SIX when the best arm changes several times. Remark 2: there is a typo line 4-5 Algorithm 1: \hat in place of \sim as stated in the first lines of proof of Lemma 1.  This paper is simply excellent. It is particularly well written. The analysis is gradual and relevant. The theoretical results involve a large class of MAB problem and can be re-used by the machine learning community.An experiment illustrates the practical interest of the implicit exploration.