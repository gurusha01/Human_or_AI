Summary: A method for jointly training all parameters (decision splits and posterior probabilities) of decision trees is proposed. This is an important topic since current methods train trees in a greedy manner, layer by layer. The task is phrased as a single optimization problem which is then upper-bounded and approximated to obtain a tractable formulation.
Quality - The paper is well written but there might be flaws in the reasoning. See below for details. Clarity - The derivation is clean but experiments and their presentation could be improved. Originality - Investigating joint training for decision trees is an important topic Significance - The proposed solution is valuable although there seem to be some shortcomings. See below for details.
Comments: - Flaw in the derivation of the empirical loss bound? At the very least the following argument in l.216f is confusing: `... for all values of $\bg \neq \sgn(W\bx)$, the right hand side can only become larger than when $\bg = \sgn(W\bx)$ ...' Let's look at the following example to illustrate why I'm confused: W\bx = [5;-5]
--> \hat \bh = [1;-1] = \sgn(W\bx)
--> \hat \bh^T W\bx = 10
Choosing g = [1;1] results in g^T W\bx = 0. Therefore the right hand side of Eq. (7) is
g^T W\bx - \hat \bh^T W\bx +
\ell(...) = -10 + \ell(...)
which could possibly be smaller than \ell(...) and hence not a valid upper bound. Note that the proof of Prop. 1 in the supplementary material does not cover this case and is therefore incomplete.
- Why are some of the leaves not assigned any data points? This seems strange and could indicate a problem with the objective or the initialization. The suggested heuristic of a fixed assignment of data-points to tree leaves seems sub-optimal but could possibly be motivated via expectation maximization/concave-convex procedure, similar to other clustering techniques?
- I'm wondering about the importance for the design of the efficient loss-augmented inference. The depth of the tree seems to be at most 16 and greedily checking 2^16 = 65536 values still seems feasible. How much did the restricted search space introduced by the Hamming ball impact performance? One result is given in the supplementary material but does this generalize? And what means `leaves'?
- I'm missing a baseline which trains non-axis aligned split functions of trees in a standard greedy manner, i.e., without any initialization.
- The depth of the tree needs to be specified ahead of time. I think this limitation should be discussed more carefully. The authors could also provide a more careful experimental evaluation regarding the depth parameter.
--------------- Thanks to the authors for carefully explaining the reasoning behind the loss bound (Eq. 7). You might want to replace \arg\max with \max. As stated in my summary I adjusted my score.
An experiment comparing the runtime could strengthen the paper. A more careful evaluation of the method would be desirable for the reader.  The submission considers an interesting problem of jointly training all parameters of decision trees. The experimental evaluation is somewhat limited and there could potentially be a flaw in the derivation. I'm happy to adjust my score given a careful explanation in the rebuttal.