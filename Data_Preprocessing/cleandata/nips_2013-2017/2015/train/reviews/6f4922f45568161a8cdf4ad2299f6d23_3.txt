The paper introduces covariance-controlled adaptive Langevin thermostat (CCAdL), a Bayesian sampling method based on stochastic gradients (SG) that aims to account for correlated errors introduced by the SG approximation of the true gradient. The authors demonstrate that CCAdL is more accurate and robust than other SG based methods on various test problems.
 In general, the paper is well written but sometimes a bit hard to follow for someone who is not familiar with these type of sampling algorithms. The paper starts by reviewing various SG methods for efficient Bayesian posterior sampling (SGDL, mSGDL, SGHMC, SGHNT). It would be quite helpful if the authors could provide, for example, a table or figure that gives on overview over the different SG variants and highlights their commonalities and differences.
 CCAdL combines ideas that have been proposed previously (mainly SGHMC and SGNHT):
- approximation of the true gradient of the log posterior with a stochastic version obtained by sub-sampling the data; this introduces "noise" in the gradient which has to be dealt with
- an estimate of the covariance matrix of the gradient noise; the estimate is a running average over the Fisher scores; in high-dimensional problems the authors replace it by the diagonal matrix
 - the use of a thermostat in order to account for the inefficiency of Metropolis Monte Carlo acceptance/rejection used, e.g., in standard HMC
Given that I'm not an expert in the field, I'm not sure what the real novelty of CCAdL is. It apparently combines ideas from SGHMC and SGHNT with a previously proposed estimator for the noise covariance matrix.
 The authors demonstrate CCAdL on a logistic regression problem and show that it converges significantly faster to higher log likelihood values. CCAdL also works for friction values smaller than those needed by SGHMC and SGHNT to result in stable sampling. By looking at the marginal distribution of pairs of parameters, the authors show that CCAdL produces posterior distributions that are close to the "true" distribution obtained by HMC on the full likelihood. As a second large scale example the authors train and test discriminative RBMs on three datasets. Again, they find that CCAdL performs superior to SGHMC and SGHNT for most stepsizes and friction constants.
 These results are quite promising and deserve publication. It would be nice if the authors could improve the paper in terms of readability for the non-expert and correct some details. For example, some symbols are not explained when they are introduced, e.g. $\mu$ and $dW_A$.  The paper introduces covariance-controlled adaptive Langevin thermostat (CCAdL) a Bayesian sampling method that is based on stochastic gradients (SG) and aims to account for correlated errors introduced by the SG approximation of the true gradient. The authors demonstrate that CCAdL is more accurate and robust than other SG based methods.