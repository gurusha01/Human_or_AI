In this paper, the authors propose a modification of the EM algorithm for the case that the true parameter is sparse and high-dimensional. The authors show some conditions for its convergence to the global solution and derive the rate of convergence. Also, a statistical test framework is constructed, which enables to check the sparsity of the estimator.
The paper is well organized. The related work is sufficiently summarized. However, the contents are very dense and some parts (mainly, section 3 and 4) are hard to follow. More simplification would be helpful for readers.
- Although in line 066 the authors argue that sqrt(s*log(d/n)) is minimax-optimal, I couldn't find the proof of this from the main manuscript (I didn't read the supplementary material.) Can you prove this clearly?
- In Eq.(3.8), ^s depends on s, but we don't know s in real situations. How should ^s be chosen in practice? Also, how can we find an appropriate initial value beta^init satisfying ||beta^init - beta*||_2 being less than R/2 (see line 275) in practice?
- I didn't understand why the truncation step in Algorithm 1 is important. Can you explain what does happen if we skip the truncation step in your EM algorithm?
 After rebuttal 
Thank you for your response. It was satisfactory for me and I increased my score from 6 to 7. A solid stats paper, but not easy to read for a MLresearcher. Also, the usefulness of the proposed method for realapplications is somewhat questionable.