The paper uses an online approximation to MCMC to draw parameters for a Bayesian neural network. The predictive distribution under these samples is then fitted using stochastic approximation. The comparisons are to recent work on approximate Bayesian inference applied to the same models and example problems. The claim being that it's a better method (at least in some ways). The paper does not yet present demonstrate that these methods will push forward any particular application.
The paper is a fairly natural extension of existing work. It is to-the-point, done well, presented clearly, and would be easy for others to try, so could have considerable impact.
Some indication of training times would be useful. Presumably the training time is much slower than the other methods, as the abstract makes a point specifically about test time. However, this issue is simply dodged.
I think it should be made clear in the paper that stochastic gradient Langevin dynamics only draws approximate samples from the posterior. Not just in the usual MCMC sense of requiring burn-in, but it doesn't even respect the posterior locally when exploring some mode for a long time. While there are various developments (cited), none of these offer the same sort of results as traditional batch MCMC on small problems. It's true that on large problems there isn't a better option, but I think it's important not to over-promise. The eagle-eyed reader will notice the posterior is wrong in Figure 2, but it's never really pointed out.
I would remove the statement that the priors are equivalent to L_2 regularization. That's only if doing MAP estimation, which this work is emphatically not! It seems a shame to use simple spherical Gaussian priors, given all the work on Bayesian neural networks in the 1990s. It's hard to take these priors too seriously, especially in a very large and deep network.
In an engineering sense, the overall procedure is useful. Although future comparisons would be required to see if it really beats standard regularization, early stopping, drop-out, and various other hacks to avoid overfitting. In applications where the predictive distributions are the goal of inference, more work is also required to make approximate Bayesian inference at this scale trustworthy. However, this is an interesting step in the process.
 Minor:
I personally don't think the title really captures the contribution. It may sound cool, but there isn't any discussion of eeking out "hidden" knowledge in the paper.
I don't think the fitting algorithm is really plain SGD. The subsequent theta samples in the final line are dependent, so presumably some stochastic approximation argument needs to be made.
$5e-6$ is ugly typesetting. I suggest $5\!\times\!10^{-6}$, and $1e-5$ could just be $10^{-5}$.
References: The NIPS styleguide says the references should use the numerical "unsrt" style. The references could have more complete details. Some proper nouns need capitalizing. Some authors only have initials, while most names are listed in full. This paper is a natural extension of Snelson and Ghahrahmani's "Compact approximations to Bayesian predictive distributions" (ICML 2005), using neural networks and online methods for the MCMC and gradient-based fitting. It's well done, bringing existing neat ideas up to date.