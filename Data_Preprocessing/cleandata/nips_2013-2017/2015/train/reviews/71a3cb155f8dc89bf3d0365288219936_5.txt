While the irrepresentability condition (IC) has been largely used in the literature [29,30], it might be useful to include just a few lines regarding the IC for the specific problem of learning Gaussian MRFs, as in [30].
It is important that the authors clarify how Theorem 3.5 depends on \alpha in condition (3.4). A small discussion on how a random initialization satisfies the condition will also be very useful. It might be the case that the assumption is trivial, i.e., \alpha is just a multiplicative factor in the result in Theorem 3.5, but this does not seem clear to me from the paper.
Additionally, the authors are highly encouraged to discuss extensions of their technique on their particular problem and perhaps some other machine learning problems.
There are two statements that somehow seem to imply that learning a simple Gaussian MRF with one sample allows for consistency. Specifically: Line 060: "statistical rate of convergence in Frobenius norm, which is minimax-optimal since this is the best rate one can obtain even when the rest K-1 true precision matrices are known [4]." Line 066: "our alternating minimization algorithm can achieve estimation consistency in Frobenius norm even if we only have access to one tensor sample". It seems that either both statements need qualification or one of the statements is false (e.g., some parts of the proof hold for sufficiently large n, some assumptions bound the Frobenius norm of the Gaussian MRF matrices with dimension-independent constants, etc.)
Few typos (I include things to be added inside [], things to be removed inside {}): Line 177: "is [a] bi-convex problem" Line 181: "by alternatively updat{e}[ing] one precision matrix with [the] other matrices fixed" Line 190: "corresponds to estimating [a] vector-valued Gaussian graphical model"
  Good theoretical results on sample complexity. Good experimental setup.