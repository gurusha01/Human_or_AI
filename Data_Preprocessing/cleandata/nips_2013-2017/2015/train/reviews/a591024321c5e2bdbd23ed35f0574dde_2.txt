I wonder what trade-off the discretized time interval (Sec 3.1) introduces, is this exact? Analogously, the same discretization in time can be applied to CTMC, what will this do?
The writing of this paper is quite dense overall.
I feel that too much space is spent explaining prior art (until line 235, 4.5 pages into the paper).
 In sec 5.1 results with simulated data - the mean of the observations from each state is 1.0 apart, with standard deviation \sigma=0.25. With states 4\sigma apart, the results from this section has hardly any noise. I wonder how this result helps show the efficacy of the proposed scheme, as opposed to CTMC.
 Furthermore, the error metric "2-norm error" is not defined, assume this is on the state sequence? as said above, where could the errors in states have come from?
In section 2 background, it is better to clarify the contributions of [8,9,10,12,13] relative to each other, it will help the reader conceptually appreciate this topic more.
 One recent paper should have been cited and commented on: V. Rao and Y. W. Teh. Fast MCMC sampling for Markov jump processes and extensions. Journal of Machine Learning Research, 13, 2014. This paper proposes two inference schemes for continuos-time hidden markov models (CT-HMM) by extending recent inference methods for continuous-time markov chains (CTMC). The algorithmic contribution is a little bit novel. Applications relevant and somewhat novel (to the NIPS community, from disease progression modeling).The main concerns are in writing quality and quality of results.