In this paper, the authors present dueling bandit approach where the arms are assumed to be ranked according to Placket-Luce distribution.
Typically, bandit problems have regret analysis. What is presented in this paper is sample complexity: the number of samples required to obtain an \epsilon-optimal arm with probability 1 - \delta. I did not see any discussion about the regret bounds of the proposed algorithms. Also, how does the regret bound of the proposed algorithm compare with that of other existing
dueling bandit algorithms? It was not clear from reading the paper.
I think that one major limitation of the paper is that the experiments are based on synthetic data. It is not clear when the PL distribution assumption holds and to what problems, the proposed approach is applicable. The experiments seem too artificial.
Other comments:
Line 202: what is "end" after [M],?
Line 7, algorithm 2, N should be \hat{N}?
The authors present an algorithm in Supplementary material and its analysis in the main paper. I think it should be other way round.  Yet another approach for dueling bandit. Experiments are weak, it is not clear from the paper how the proposed method is better than existing approaches.