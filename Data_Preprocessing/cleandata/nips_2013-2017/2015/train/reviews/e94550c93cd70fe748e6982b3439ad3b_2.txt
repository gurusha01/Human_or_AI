I found the methodology and experiments presented in this paper fairly convincing and have only one some minor comments.
If (as is the case for many applications involving large datasets) there is a focus on predictive inference it may make more sense to aggregate functions of the parameters of relevance for prediction rather than the full set of parameters.
For example, in the probit regression one could aggregate fitted probabilities and in the mixture of Gaussians example one could do something similar for cluster membership probabilities.
I wonder whether the results are more or less sensitive to the aggregation method in such a case.
Have the authors done any experiments along these lines?
Another question that occurred to me was about the structured aggregation for the case of positive semidefinite matrices;
there are certainly different reparametrizations that could be used here (such as a Cholesky factorization) and I wondered why the aggregation is only being done on the D(\Lambda_k) matrices.
Is there any reason why this would be the best choice?
Minor comment:
there is a missing integration in equation (3).
 This paper refines recently suggested consensus Monte Carlo algorithms suggested in the literature by using variational Bayes methods within the aggregation step of these algorithms.The relaxation of the variational objective function suggested is clever and I found the experiments convincing that the proposed approach produces some improvements over simpler methods.