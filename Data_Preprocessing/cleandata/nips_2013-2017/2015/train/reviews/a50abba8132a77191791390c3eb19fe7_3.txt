The authors should be explicit on what this new methods adds to existing stochastic optimization (e.g., Hu Kowk and Pan, 2009) and related mini-batch optimization (e.g. Konecny et al, 2013 and 2014), or Smola's work (e.g., M Zinkevich, M Weimer, L Li, AJ Smola, 2010), including using such methods as baseline on the experiments.
Merely comparing the method to variants of itself is insufficient.
This is a crowded field.
It behooves the authors to explicitly delineate their novel contributions.
 The paper is relatively well written, and the comparative bounds table (sec 8) is useful.
 The paper discusses the stochastic variance-reduced gradient method to speed convergence in optimization of a sum of strongly convex Lipschitz continuous functions. The method trades off inexact computation for speed, and it is not clear whether it represents a sufficient improvement over the state of the art in stochastic optimization to merit publication in NIPS.