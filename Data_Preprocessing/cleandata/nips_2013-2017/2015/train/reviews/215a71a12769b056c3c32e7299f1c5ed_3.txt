This paper proposes the use of LSTM units as a method of training very deep networks.
 This is a high-quality, well-written paper. The idea is original, and the results are quite interesting.
 Machine learning researchers frequently find that they get better results by adding more and more layers to their neural networks, but the difficulties of initialization and decaying/exploding gradients have been severely limiting. Indeed, the difficulties of getting information to flow through deep neural networks arguably kept them out of widespread use for 30 years. This paper addresses this problem head on and demonstrates one method for training 100 layer nets.
In particular, the experiments showing the routing of information and layer importance are very cool.
 Specific comments:
It seemed that in Section 4.1, 'most transform blocks are active on average' should be changed to 'not active on average.'
One could initialize each layer of a very deep network with the identity matrix (possibly with a small amount of noise). It might be worth commenting how transform gates differentiate highway networks from this simple scheme.  This is a great paper, demonstrating a method for training very deep neural networks, and was a pleasure to read.