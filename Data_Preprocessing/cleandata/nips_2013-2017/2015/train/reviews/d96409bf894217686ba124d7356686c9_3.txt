This paper proposes to directly learn the messages in message-passing inference (as in inference machines[8]) using CNNs. The paper shows state-of-the-art results on PASCAL VOC semantic segmentation.
First, note that [12] also learn messages in one-step message-passing inference. The math is not described well, and the details are different (that paper tackles pose estimation) but the insight that learning messages is easier than learning potentials, and that CNNs can be used to learn messages, is similar. This paper is missing an acknowledgement of this, and a discussion of what this paper adds.
Second, let's take a closer look at what the approach is actually doing: the final score vector for a pixel is the sum of the scores output by each factor->pixel message, and each factor->pixel message is computed by a separate deep network. This means that the method is eerily close to a simple ensemble of four models trained with different initializations.
All the other methods compared to (except ContextDCRF, which suffers from a similar problem) have only one network making the prediction. This suggests that an ensemble model is a natural baseline. I would encourage the authors to try such a model.
Given these two points I am somewhat skeptical of the originality and significance. In terms of clarity, the paper is well written, and the math is very clearly described. I would still recommend the paper for acceptance because this idea of learning messages using CNNs is important, and because the results are state-of-the-art.
 The notion that one can train a CNN to predict the messages is quite interesting and is described well. However, [12] also directly learn the messages (for a one-step message passign inference) using CNNs, albeit for a different task. The ensemble effect coming from multiple networks is also not disentangled (see below)