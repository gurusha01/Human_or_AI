Since I am currently traveling, I only had very little time to finish this (additional) light review. It's unfortunate since I found this paper quite interesting to read. It might very well be that I have missed or misunderstood some points. If this is the case, I apologize.
Observing data from a given hypothesis, we might be interested in evaluating a different hypothesis. If these hypotheses are non-deterministic (e.g. conditional distributions), we can estimate the loss under the hypothesis under investigation by reweighting the data points, see eq. between (1) and (2).
The authors seem to address a problem that is related but somewhat different (see below) from the problem of large variance of the weights. The authors call this problem "propensity overfitting". The examples indicate that this problem occurs if there is not enough exploration, i.e. for a given x, we see only very few or even not a single y that would correspond to the new hypothesis that is to be evaluated. Since a similar argument explains the large variance of the weights, it would be nice to explain the relation between these two problems in a bit more detail.
I didn't have time to look into the details or check novelty of the proposed solution but its idea seems sensible.
- l. 199: I don't get this, if n is small compared to k, we see only few data points with xi = yi, right? Then, \hat R(h^) should be even smaller than -2? Am I missing sth.?
- "unbiased counterfactual risk estimator used in prior works on BLBF [4, 5, 1]". I am not sure whether this is a fair statement. I don't know all three papers but I doubt that they are not using sth. like clipping which leads to biased estimation with reduced variance.
 - I appreciate that code is available.
 Summarizing, I could not check whether the paper is technically sound. But it definitely contains interesting ideas that I would like to think about. I therefore suggest acceptance.  -