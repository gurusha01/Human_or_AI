The main contribution of this paper is to provide a more theoretically sound formulation of an on the job learner proposed in [22]. The learner starts by queuing the crowd to produce its output, but gradually starts making its own predictions and querying the crowd infrequently. The choice of whether to query the crowd for any given input is modeled as a decision problem solved via a Monte Carlo Tree Search.
 The presentation is mostly good, although lacking in important details. The empirical results vis a vis a threshold-based heuristic are somewhat underwhelming. But it certainly seems like a reasonable approach and merits further investigation.
 Please address/clarify the following:
1. Equation (2) isn't making much sense. The LHS should also be conditioned on s. And how does the y in p{R}(ri | y, q_i) vanish?
2. What is N(s) in Algorithm 1?
3. How did you get the 0.3 and 0.88 in the threshold baseline?
 4. I am not familiar with the exact tasks used here, but are the "online" systems the best machine learned systems on this task? If not why not? If so, then any gain on top of it is purely as a result of human annotation. What are some such cases? Some error analysis would be handy.  This paper presents a theoretical formulation of an "on the job learner" that starts as a pure crowdsourcing system and gradually transitions to a learned model that only queries the crowd on low confidence inputs. The empirical results are somewhat of a mixed bag, but it will be good to get the discussion going along these lines.