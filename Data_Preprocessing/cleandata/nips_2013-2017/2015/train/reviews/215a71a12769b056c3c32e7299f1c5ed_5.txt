I, too, thought about using the LSTM trick of trapping the gradient to grow a network to deeper, but didn't start the work just yet. Congratulate the authors for getting there first!
There is possibly a different perspective: a very deep network is a not-so-deep network where layers have sub-layers. Each "layer" has more complex non-linear behavior than a traditional rectifier/sigmoid layer. This is how I personally see inception of GoogLeNet, 3 stacked 3x3 in VGG etc., and I wonder if this is a valid way of looking at highway network as well.
There is one experiment that I would suggest the authors to try, using one thin&tall network to train both MNIST & CIFAR (you can fork at the top for the two domains). What I am interested in is whether, at inference time, the network exhibit the same pattern as found in the networks that are trained separately for each domain, whether the model parameters that are underutilized in MNIST is picked up by CIFAR. In other words, I would love to see how sharing happen in a very deep hierarchy, and whether it is as efficient as possible.
No major complaints. I encourage the authors to try the network on bigger dataset such as ImageNet etc.
  This paper describes an idea inspired by LSTM that allows the training of very deep feedforward network. The idea is simple and elegant, and the analysis are solid.