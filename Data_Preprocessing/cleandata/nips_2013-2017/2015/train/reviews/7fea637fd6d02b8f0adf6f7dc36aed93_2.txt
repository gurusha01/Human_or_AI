The paper is concerned with two problems:
 (1)
How does the social welfare of players using regret minimization algorithms compare to the optimal welfare. (2)
Can one obtain better regret bounds when all players use a regret minimization algorithm
It has been shown in the past that the social welfare converges at a rate of 1/T to the optimal welfare for zero-sum games. However, the question remained open for general non-zero sum games. The authors show that under some smoothness assumptions on the game (introduced by Roughgarden) the same rate of convergence can be achieved when all players use a regret minimization algorithm that has the so called RVU property. Moreover, the authors show that the optimistic mirror descent algorithm of Rahklin admits this property. The authors also show that an optimistic variation of FTRL (OFTRL) with a recency bias also admits this property.
 For problem (2), the authors show that the regret of each individual player is in O(T^{1/4}) when using their variation of FTRL and this can be seen also in their simulations. Furthermore, they enhance their algorithm by using a doubling trick in such a way that if all players use OFTRL, then their regret grows like T^{1/4} and yet the regret does not grow faster than O(T^1/2) if the players observe adversarial rewards.
 The paper is really well written and solves an interesting problem. I particularly liked the generality of the result in terms of the RVU property.  The paper deals with bounds on regret minimization algorithms in games. The usual regret bounds on these algorithms is in O(\sqrt(T)). However, this assumes that the learner faces a completely adversarial opponent. However, it is natural to assume that on a game everyone will play a regret minimization algorithm and the question is whether or not one can obtain better rates in this scenario. The authors show that regret in O(T^{1/4}) is achievable for general games.