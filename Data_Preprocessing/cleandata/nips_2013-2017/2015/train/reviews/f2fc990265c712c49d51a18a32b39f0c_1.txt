This paper presents an algorithm HONOR applicable for a wide range of non-convex sparse learning formulations.
 The HONOR method combines a Quasi Newton step with a standard GD step for a first order approximation of the
objective without evaluating the actual Hessian for the QN step and using L- BFGS to scale it up for large scale objectives. The authors show that any limit point the algorithm might converge to is a Clarke critical point of the objective and the sequence generated by the objective leads to a limit point.
The convergence for non-convex problems is typically hard to analyze rigorously.
This paper succeeds in showing the analysis of convergence to a proved limit point which is guaranteed to be a Clarke critical point. This analysis is pretty deep and the mathematics looks correct. It would be good to see whether the same kind of method can be applied to more generalized non-convex objectives.
The empirical evaluation looks pretty reasonable with experiments analyzing the decrease in objective value over time for different large scale and high dimensional data sets. The results show that the algorithm converges faster than other comparable methods. It would be good to see some example of a dataset where there might be a local minima (even if synthetic) and see how the algorithm performs in presence of such an objective as well some guidance to how the different parameters particuarly \gamma can be chosen to ensure faster convergence. This paper looks at a specific kind of regularized non-convex problem which appears in different problems in machine learning. They give a hybrid algorithm combining second order information using Quasi newton steps as well as gradient descent steps depending on a certain condition which is cheap to compute at every iteration. They provide an extensive analysis, pointing out the steps that are complicated due to lack of convexity and prove that their algorithm converges to a Clarke critical point. The analysis is also extendable to other non-convex general scenarios.