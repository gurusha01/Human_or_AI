Paper 921 introduces a new variant of the adaptive SGD method for non-convex optimization called "equilibrated gradient descent". The paper states that the equilibration preconditioner is better than Jacobi preconditioner theoretically and empirically.
 For the experiments, MNIST and CURVES are two easy tasks and it cannot say too much insight about the proposed algorithm. I think more experiments should be implemented. When we use neural network, what we really cares are the test accuracies rather than "training error (MSE)". On the other hand, I think it is not enough to compare the ESGD with SGD when the x-axis is "epoch".
There should be another figure whose x-axis is the training time measured by "minute" or "hour".
 The paper is not easy to understand clearly. (1) In Algorithm 1, should $H$ be re-calculated every iteration? If yes, there should be a subscript for $H$. Also, in line 277 (the update of $\theta$), should it be $\sqrt{D / i}$? (2) I think in Section 4, paper 921 tries to state that it is the $D^{E}$ that can greatly reduce the condition number $\kappa(D^{E-1}H)$ and I think it should be declared clearly. (3) In line 178, what does $D^{-1}=\Vert A{I,.}\Vert2$ mean? (4) In line 267, $R(H,v)=...$ is a vector while (11) is a scalar, so what do you want to express here? Since there is no theorem that can provide a convergence rate for the proposed ESGD, I think a clearer explanation is needed.
 Overall, I expect more convincing experiments and a clearer explanation of the proposed ESGD.  Overall, I expect more convincing experiments and a clearer explanation of the proposed ESGD.