This paper empirically shows how adding latent variables to an RNN improves its generative power by showing nice samples and better likelihoods (or lower bound on likelihoods) than previous attempts at generating distributions over continuous variables.
The paper is well written and clear - except for the figure, which is quite confusing and confused me for a while. As the authors admit, their model is a slight modification to e.g. [1]. The authors did, as far as one can tell, a fair comparison with the model presented in [1], and showed how adding more structure to the prior over latent variables z_t (by means of making the mean / variances of those a function of the previous hidden state) helped generation.
My main concern about the paper is that is rather incremental. Given that the main novelty involves a better prior over zt, I would have liked to see more analysis on why that helped (other than through empirical evaluation). Also, unlike previous work, the authors seem to use the same hidden state ht at generation and for inference. What is the motivation behind this?
Regarding the experiments with speech, could the authors clarify how exactly the windowing was made for the 200 waveform samples (was it overlapping from sample to sample?).
The authors could reference DRAW as another application / use case of a VRNN-like architecture. Despite the fact that adding latent variables to RNNs has already been explored in previous (recent) work, this paper is well written and presents a slight modification that outperforms all previous attempts at integrating the variational AE framework with RNNs.