The authors describe the approach of pre-conditioning, a numerical method that uses a linear change of variables to perform gradient descent in a better conditioned parameter space. The authors propose using a pre-conditioner known as "equilibration", which row-normalizes a matrix, to pre-condition the loss Hessian. They then propose a practical numerical technique to estimate the equlibration pre-conditioner, which costs little more than normal gradient-based training. They show experimentally that this approach reduces the condition number of random Hessians, and they show theoretically that it reduces an upper-bound on the condition number of the Hessian, though a more direct proof is lacking. They further show that the resulting algorithm "Equilibrated Gradient Descent" (EGD) improves the convergence time of deep neural networks on some data sets. Additionally, they argue that the heuristic algorithm RMSProp is an approximation to equilibrated EGD.
I think this is a clear, insightful paper with a well-rounded analysis. It is original and may be immediately useful for neural network training. This is a well-written paper with clear and precise experimental results and good theory on a topic of reasonable importance -- a justification of the success of one of the most common online training algorithms for neural networks and also the provision of a new, fairly practical online training algorithm.