Summary: This paper considers tensor graphical models aiming to estimating sparse precision matrices if the overall precision matrix admits a Kronecker product structure with sparse components. It proves various good properties of the alternating minimization algorithm.
Quality: The paper extends some of the early works in the literature by examining tensor data rather than matrix data, noticeably [5, 6, 7, 8].
- The results depend critically on a few assumptions. The first is the irrepresentable condition, which is understood to be quite restricted. The second is (3.4). How can one come up with an initial estimate which lies closes to the truth, when the dimension grows?
- The discussion following Theorem 3.5: The minimax-optimal results have also appeared in [5, 8] when K=2. Thus, the claim that the phenomenon was first discovered
by this paper is not entirely correct. In Remark 3.6, a fair comparison should be made to [8].
- Initial values for the precision matrices: I don't see how and why the suggested initial values (or after iteration) would satisfy (3.4).
- The proposal method in (2.3) is standard. The iterative algorithm is widely used. The theory appears to be standard.
Clarity: The paper is clear.
Originality: The paper makes some contribution to the literature by extending matrix graphical model to tensor graphical model (e.g. in [9]). It is known that the rate in [9] is not optimal in light of [8]. The paper can be best seen as tightening up the results in [9], by using different techniques than those in [8].
 Significance: The contribution of the paper is incremental at best.
 A nice attempt was made to solve an interesting problem, but the assumptions are wrong potentially. In some sense the algorithm works in a way similar to [8]. However, how may one find initial values satisfying (3.4) when dimensionality grows?