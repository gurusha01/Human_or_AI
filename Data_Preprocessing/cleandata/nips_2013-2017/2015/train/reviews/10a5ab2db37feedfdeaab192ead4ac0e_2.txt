The authors propose a way to reconcile the need for high precision of continuous variables in backpropagation algorithms with the desire to use spike-based neural network devices with binary synapses. They introduce the idea of considering the analog variables as probabilities that are sampled to map the neural network used off-line for training to the spiking network chip used for deployment of the application. They show how to adapt the backpropagation algorithm to be compatible with this strategy and validate the method proposed on the MNIST benchmark. They map the trained network onto two different architectures, optimizing for size and accuracy, and present performance figures measured from the chip. The quality of the manuscript is very high, as well as its clarity. The work is quite original and can be useful for other approaches and other hardware platforms. BTW, in reviewing the different HW platforms available, the authors should mention also Qiao et al., Frontiers in Neuroscience 2015, and the paper at http://arxiv.org/abs/1506.05427 The significance of the work is not highlighted in the paper. It is not clear how the method proposed will be useful to deploy the TrueNorth chip in practical applications, and in what specific applications. It is not clear if and to what extent this specific method has advantages over other alternative options, such as those cited in [9] to [12].
 The authors present a method for applying backpropagation to spiking neural networks with binary synapses and validate it by successfully mapping the trained network onto the TrueNorth device. The paper is well structured, clear and to the point. The results are convincing, but lack some details (e.g. how are the inputs converted into spikes, what are the frequencies used, the shared weight values, the neuron models, etc.)