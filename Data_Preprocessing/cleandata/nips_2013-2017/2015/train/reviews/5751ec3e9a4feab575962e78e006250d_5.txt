The authors propose a variational objective that minimises the KL to a "population posterior", that is the expectation of the usual posterior under an empirical distribution. They then use this formulation to derive a streaming variational algorithm where the objective is parameterised by an empirical distribution.
 The introduction seems to claim that online Bayesian posteriors converge to a point mass: asymptotically, is this not correct and a consequence of consistency? Is the point the convergence can be premature? If so, how much of this is due to the variational (or other) posterior approximation which tends to underestimate uncertainty? i.e., is the problem really with Bayesian inference or with the approximation taken?
Eq (3): min -> argmin?  This is a very nice treatment of streaming Bayesian inference via variational methods. The experiments are strong, and the formalism is quite elegant.