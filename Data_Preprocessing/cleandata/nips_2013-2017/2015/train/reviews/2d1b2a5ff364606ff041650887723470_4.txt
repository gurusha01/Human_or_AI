This paper considers about linear estimation with nonlinear link function. Inspired by the recent work of Plan and Vershynin, it studies the asymptotical estimation error using generalized lasso for arbitrary link function with \mu \ne 0. It turns out that no matter the link function is linear or nonlinear, the asymptotical error is exactly the same as shown in theorem 2.1.
 Theorem 2.1 holds when the linear measurement vector is sampled from normal distribution, which might be strict in some cases. The author might need to explain why this assumption is necessary for the conclusion to hold.
A few questions: 1. How to choose regularizer based on set \kai in which x_0 lies? While the assumption of convex regularizer is discussed in this paper, it seems the aforementioned problem is not addressed yet. 2. While the result shows nonlinear is equal to linear measurement, but the conclusion involves with \mu and \sigma which characterizes the hardness of estimation from nonlinear function g. I would like to know if the authors have any thought on the necessity of these dependences.
There are some references missing. 1. Single index model and sufficient dimension reduction study linear vector/subspace estimation under unknown link function. However, the paper does not review any paper in this area. 2. A recent paper "Optimal linear estimation under unknown linear estimation" by Yi et al. introduces new algorithm for sparse recovery even with link function that has zero \mu. The paper establishes the equivalence of high dimensional linear estimation with linear and nonlinear link function under proper conditions. The conclusion is supported by careful theoretical proof and empirical results. This is a well written paper with fairly interesting results.