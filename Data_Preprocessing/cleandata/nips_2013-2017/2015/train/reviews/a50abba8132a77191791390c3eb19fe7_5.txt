This paper introduces several approaches to increase the convergence speed of a well-known stochastic gradient method called stochastic variance-reduced gradient (SVRG).
 The paper is well-written and it is very easy to follow the arguments of the paper. The authors start with a short introduction of SVRG method and subsequently explain three approaches for improving the convergence speed of SVRG.
This paper starts with a key proposition showing that SVRG does not require a very accurate approximation of the total gradient of the objective function needed by SVRG algorithm. The authors use this proposition to derive a batching SVRG algorithm with the same convergence rate as that of original SVRG. Then, the authors propose a mixed stochastic gradient/SVRG approach and give a convergence proof for such a scheme. As a different approach of speeding up, the authors proposed a speed-up technique for Huberized hinge-loss support vector machine.
 In addition to speeding-up strategies and their convergence analysis, the authors include similar analysis for the case of regularized SVRG algorithms. The authors provide propositions for the convergence of mini-batching strategies. Through extensive simulations, the authors show the improvements obtained by their methods.
Except minor comments stated below, I do not have any major comments for further improving the manuscript.
 Minor Comments:  On the right hand side of inequality in line 77, there should be an expectation  Question marks in the first line and middle of page 10 should be corrected The topic of the paper is of interest for nips community and the analysis and simulation results were done perfectly.