The paper proposes an efficient optimization algorithm (HONOR) for nonconvex regularized problems by incorporating the second-order information to speed up the convergence. Although the convergence analysis for nonconvex optimization methods is typically more difficult than convex methods, the authors have proved that every limit point of the sequence obtained by HONOR is a Clarke critical point. The proposed method is not only theoretically sound, but also computationally efficient. However, I still have a few concerns about the proposed algorithm.
1: Using the fact that each decomposable component function of the non-convex regularizer is only non-differentiable at the origin, the authors have designed an algorithm which can keep the current iterate in the same orthant of the previous iterate so that the segment between any two consecutive iterates do not cross any axis. The strategy seems to make the algorithm dependent on an initial point x^0.
 + Can the authors show how the solution of HONOR is influenced by the choice of the initial solution?
 + Which vector did the authors use as the initial solution in the numerical experiments?
 + Does the results shown in Figure 1 change by using different the initial solution?
2: One of the advantages of HONOR is to incorporate the second-order information to speed up the convergence but HONOR might sacrifice memory usage. When the given problem is highly nonconvex, the positive definite matrix H^k given by L-BFGS might be completely different from the Hessian matrix of the nonconvex optimization problem. Do the authors have any thoughts about those issues?  The paper is very well-written and an efficient algorithm with theoretical guarantee for nonconvex problems is of great significance. However, I have some concerns about the proposed algorithm.