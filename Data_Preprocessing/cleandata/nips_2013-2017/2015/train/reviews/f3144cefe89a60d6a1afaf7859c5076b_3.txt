Summary: A poisson gamma hierarchy is presented for modeling counts data.
The primary contribution lies in utilizing a hierarchy of gamma distributions and using recently proposed clever augmentation tricks to develop a simple, tractable Gibbs sampler in spite of non conjugacy. Coupled with a Poisson likelihood, the authors demonstrate the utility of the hierarchy for modeling counts data.
 The paper is technically sound. The problem of extracting unsupervised multilayered representations of data is interesting, and the developed model provides an interesting alternative for counts data.
 Detailed comments:
1) While it is true that RBMs have traditionally used binary hidden units, recent work [1] has found that using rectified linear nonlinearities leads to better representations. The non linearities induced by the proposed gamma units appear to subsume the linear regime of rectified linear nonlinearity (with the linear regime being recovered in expectation, when the expected rate parameter is 1). This is cool and it would be nice to discuss this connection more explicitly in the text.
2) Scalability appears to be a big concern for the inference procedure. It isn't completely clear, but it appears that a 1000 Gibbs sampling iterations are run after adding each new layer, sweeping over all the variables in the network. This would be difficult to scale to deeper architectures. How long does it currently take to train the 5 layer networks employed for multi-class classification? A discussion of these computational issues would be useful.
3) The external classification comparisons, to DocNADE and over replicated softmax are sloppy. The classification numbers are not really comparable, considering the competing algorithms are trained on distinct vocabularies. The paragraph on qualitative analysis is not useful at all to the reader and should be reworked. The authors claim that the discovered topics specialize as one traverses the hierarchy downwards, but present very little evidence in support of this claim. It would be interesting for the reader to explore the per layer discovered topics more closely and they could be made available in a supplement. The authors also claim, without supporting
evidence, that they can generate interpretable synthetic documents from the trained network. It would be interesting to see these synthetic documents generated by the network. Again something that could be easily provided in an appendix.
[1] Nair, Vinod, and Geoffrey E. Hinton. "Rectified linear units improve restricted boltzmann machines." Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.  Overall, this is an interesting paper. I would have scored the paper higher, if not for sloppy experimental evaluations.