In this paper, the authors propose a distributed stochastic gradient learning scheme amenable to infrequent synchronization. Their claim is that, due to the non-convexity, existing algorithms are not very robust to distant synchronizations. Further, their resilience to differences between parameter values across different nodes favors exploration, which leads to better solutions.
The algorithm is simple and seems to work well on the experiments. The authors compared with many existing algorithms and for several values of tau. To nitpick, since the authors talk about deep learning without more details, I would also have appreciated experiments on other architectures than CNNs.
My main gripe with this paper is that, while they revisit an old method (see section 4.1 of "Notes on Big-n Problems" by Mark Schmidt, an excellent review of the literature), their coverage of existing implementations of these methods is scarce. Basically, they are referenced in the introduction but the in-depth analysis is limited to ADMM.
I believe positive results on distributed stochastic optimization in the context of nonconvex losses can definitely be of great significance. However, the authors should do a better job at reviewing the existing literature and not just referencing it. The algorithm seems to outperform existing distributed techniques but is very slim on the comparisons with non-deep learning oriented distribution techniques which have been widely studies in the optimization literature.