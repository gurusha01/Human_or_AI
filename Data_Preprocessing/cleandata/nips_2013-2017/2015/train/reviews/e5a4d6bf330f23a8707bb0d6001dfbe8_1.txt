A new loss estimation procedure for adversarial multi-armed bandit is proposed which allows to bypass explicit exploration to obtain high probability bounds. Technically this leads to improved constants as well as simpler proofs. It's a good paper which deserved to be published. It is unfortunate that the author does not comment on whether this idea can be used for the linear bandit case, where a lot of work has gone into building good exploration distribution. Nice observation that escaped everyone else in the bandit community.