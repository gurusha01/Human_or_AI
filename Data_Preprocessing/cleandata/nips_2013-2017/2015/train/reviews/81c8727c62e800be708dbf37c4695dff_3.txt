This is the cleanest piece of work I have seen on sample complexity for metric learning and, as noted by the authors, presents several advantages over related work in this area.
In particular, I like that the authors present several important cases: both the "distance-based" and "classifier-based" frameworks, which highlight dependence on the dimensionality, as well as the regime in which there may be noisy or uninformative dimensions.
It's clearly written and appears to be correct.
Given that there is a relative dearth of work of this sort in the metric learning sub-community, I think it's worth publishing.
The experiments are somewhat underwhelming, though since this is more of a theoretical paper, it's not a key drawback.
A couple things I noticed were: there has actually been quite a bit of work on using the type of regularizer advocated in this work (the ||M^T M||_F^2 regularizer); see, e.g., "Metric Learning: A Survey", section 2.4.1 for examples).
It might be good to discuss these a bit.
Also, both ITML and LMNN can be viewed as already having a regularizer on the M matrix; for instance, ITML is typically viewed as applying a LogDet regularization on M^T M and LMNN is often viewed as applying a weighted trace norm regularization on M^T M.
Perhaps these were not necessarily the best choices of methods to add the proposed regularization?
Also, it's stated that ITML is initialized with a rank-one metric; given that the LogDet regularizer maintains rank (the rank cannot change from the initialization), this would lead to a low-rank solution, which would probably be undesirable (if I'm understanding the experimental setup correctly).
These are all fairly minor things, however, and shouldn't detract from the fact that the paper is a clear and effective look at sample complexity for metric learning. This is a nice paper that presents several key results regarding the sample complexity for Mahalanobis metric learning.It is a good addition to the literature on metric learning, and deserves to be published.