The paper addresses the important problem of parallel optimization. The proposed algorithm is shown to have better tolerance to staleness and thus carries potentially smaller communication burden than the state-of-the-art. It also obtains better learning results as measured by test error. The paper is overall well-written and a pleasure to read.
In the current EASGD, the center variable is updated by following a symmetric elastic force (Eq.4). However given the objective in Eq.2, a more natural solution seems to be always taking the exact average of all the local variables (still in an online fashion). I wonder whether the authors have investigated this variant and would like to see some discussion in the paper.
Is there any experimental study on the effect of rho? More specifically, how does the exploration affect the performance?
the line under Eq.4: the stochastic gradient of "F"? The paper proposes a parallel algorithm that encourages simultaneous exploration among computing nodes for more effectively optimizing objectives with many local optima. The proposed idea is well motivated, clearly presented and supported by extensive experimental studies.