The usual The usual approach to training recurrent nets, each prediction is all based on the current hidden state and the previous correct token (from the training set). But for test, we actually expect the trained rnn could generate the whole sequence by make prediction based on the previous self-generated token. The paper suggests during the training we should force the model to gradually generate the whole sequence (the previous token more and more likely generate by the model self).
 Quality
 Technically sound and the usefulness of scheduled sampling is supported well.
Clarity
 The paper is well written and organized.
Significance:
The main idea is well motivated and interesting. The new training process could have important impacts on the study of recurrent net training.
Minor comments Do you have any intuition of the differences of the three decay schedules? How those different decay schedules represent on the training set? Does the training easily remain stuck in sub-optimal solution? Training recurrent nets could very tricky (there are lots of choices, momentum, gradient clipping, rmsprop and so on). Please provide more details of the training and make the experiments reproducible. Please also report the cost on the training set. Would the scheduled sampling be helpful for optimization?  It is a good paper that proposed a simple and straightforward scheduled sampling strategy for alleviating the discrepancy between training and inference of recurrent nets applied on generating sequences. The trained recurrent nets by this scheduled sampling outperform some fairly strong baselines on image captioning, constituency parsing and speech recognition.