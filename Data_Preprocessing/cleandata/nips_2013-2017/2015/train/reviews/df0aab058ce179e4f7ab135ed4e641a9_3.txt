The paper proposes a novel generative unsupervised model aimed at obtaining representations with a variety of properties which are excepted as desirable for representations -- sparse, non-negative, high in dimentionality. The model builds upon factor analysis by enforcing in addition the desired properties by posterior regularization.
 The model shows very good performance as autoencoder for reconstruction as well as a pre-training for classification networks. The model is well studied both analytically and empirically. The results are convincing of its merits.
 A few comments: -- A better explanation is needed why one needs to try five different gradient descend methods in the E-step of the learning (lines 136 - 140) in the given order? This seems a bit arbitrary. -- It would be informative to do an ablative analysis of RFN by removing all constraints (normalization is being removed but no non-negativity for example).  The paper introduces a convincing, both analytically as well as empircally, method which deserves the attention of the NIPS audience.