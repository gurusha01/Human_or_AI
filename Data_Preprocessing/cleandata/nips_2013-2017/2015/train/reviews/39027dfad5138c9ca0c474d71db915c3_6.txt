Counterfactual risk minimization is a technique aimed at learning in a bandit setting but from logged data by correcting for the bias of generating logged examples. While consistent, the paper shows that it suffers from an overfitting problem where a hypothesis class can overfit based on the generation probability of an example. The authors propose to fix this using a multiplicative parameter bounds the variance while maintaining consistency. The experimental results show that the multiplicative (or normalized) counter factual technique outperforms the non-normalized one.
 The paper presents a technique for learning self-normalized estimator for counter-factual learning in the logged bandit setting. It presents ideas for dealing with a form of overfitting where the hypothesis can overfit by having stronger support for the logged data. This is a good paper and should be accepted although I am not absolutely certain.