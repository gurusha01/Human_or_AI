The commentary and notations concerning the main equation - (2) - seem wrong or at least misleading:
an expectation is a quantity of the type \sum f(x) p(x). The expected quantity, f(x), is everything else than p(x), the probability measure.
In equation (2), the loss is so obviously not the expected quantity! Therefore, the following comment (page 2, second paragraph) that (2) has an "anomaly" because it is not invariant to translation of the loss is technically incorrect. Again, the ensuing Example 1 is based on this wrong presumption - that, unsurprisingly, does not hold.
In practical terms, the estimator in (2) is weak to its denominator, pi, which can be 0 if the sample has never been observed in the prior. The modified estimator in (7) seems better in principle because the effects of pi are mollified through the double denominator.
 I really hope that the authors will be so gracious to modify their commentary should the paper be accepted.
The experimental results show that Norm-POEM outperforms POEM on all datasets and is not far from a CRF using full multi-label information. This paper presents an approach for "counterfactual learning", a learning scenario that is a specialisation of multi-label learning. In multi-label learning, the ground-truth annotation for a sample provides a 0/1 label for each of the classes; here, instead, only the label for one class (i.e., bandit feedback).However, the above is only a justification preamble. De facto, this paper has a precise aim: to improve an estimator (POEM) recently presented by reference [1] at ICML 2015. I would say that, overall, the paper is convincing and worth publication, but with some remarks that I raise in the Comments to authors field.