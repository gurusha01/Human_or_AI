This paper extends the stochastic optimization algorithm SVRG proposed in recent years. These modifications mainly includes: the convergence analysis of SVRG with corrupted full gradient; Mix the iteration of SGD and SVRG; the strategy of mini-batch; Using support vectors etc. For each modification, the author makes clear proofs and achieves linear convergence under smooth and strong convex assumptions. However, this paper's novelty is not big enough. The improvement of convergence rate is not obvious and the proof outline is very similar to the original SVRG. The key problem such as the support for non-strongly convex loss is still unsolved. Besides, there exists some small mistakes on the notations in the appendix.
 This paper makes some practical improvements based on the stochastic optimization algorithm SVRG proposed in recent years. These improvements are proved to be effective by the theory and experiment results. However, the novelty and improvements of performance are not big enough. The proof idea is very similar to the original SVRG and key problems such as the support for non-strongly convex loss is still unsolved.