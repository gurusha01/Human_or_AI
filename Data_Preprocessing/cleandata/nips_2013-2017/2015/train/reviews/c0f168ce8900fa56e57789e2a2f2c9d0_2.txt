The paper presents a finite-time analysis of new MC algorithm called Projected LMC.
The optimization counterpart of Projected LMC is stochastic gradient descent (SGD). The main theoretical result states that for an appropriately chosen step-size and after a large number of iterations, the distribution of the samples from Projected LMC is arbitrarily close to the target distribution log-concave density.
The main contribution here is to show that the step-size of Projected LMC is similar to the step-size in SGD and that the maximum number of iteration of Projected-LMC depend polynomially on the dimension of the space (up to a logarithmic factor).
Further, the proposed algorithm does not involve evaluation of the density, but its gradient.
This is extremely useful in cases where the normalization constant is not tractable.
The paper is clear and well-written. It introduces a new theoretical analysis of Projected LMC algorithm for sampling from distributions with log-concave densities that involves evaluation of the first order oracle.  This work introduces a finite-time analysis of the projected Langevin Monte Carlo (LMC) algorithm.The approach first theoretical analysis of a MC algorithm to sample from a log-concave distribution that only requires values of the gradient of log-density.