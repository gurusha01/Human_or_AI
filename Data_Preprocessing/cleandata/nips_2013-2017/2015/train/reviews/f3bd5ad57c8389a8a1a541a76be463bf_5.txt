Summary: The paper provides a stochastic generalization of expectation propagation. In EP local contributions are removed and updated for each data point - the down side being that sufficient statistics must be saved for each data point. For models with many parameters this becomes expensive. Instead of removing the sufficient statistics for each data point, instead the authors remove the global sufficient statistic, weighted by the sample size being updated. The method is generalized to other EP variants such as parallel EP and distributed EP for when there is heterogeneity in the data set, and to models with latent variables. For the experiments the method is applied to probit regression, gaussian mixture models, and Bayesian inference in neural networks. Empirically they show that their method provides computational savings without sacrificing performance.
Quality: The method is sound, well motivated and experimentally validated in three different, but canonical, applied domains.
 Clarity: Easily readable and clear. Excellent writing style.
Originality: The core idea behind the paper is simple but clever, elegantly solving the memory overhead in EP. Novel extensions of the stochastic EP method to parallel, mini-batches for better approximations, and latent variable models are also developed and explored. To my knowledge this approach and its extensions are all completely original.
Significance: The contributions are significant as they provide scalable alternatives for EP. I expect the paper to be as important to EP as SVI was to VB. The paper is well motivated, extremely clear, and the research contributions are important for scaling EP inference for complex models with many parameters. Extensions and connections with existing EP and VB techniques are explored for an overall very thorough and mature paper.