To clarify after reading author response: the pre-trained sentence vectors are going to be easy to use and thus will be most certainly useful. What I meant was that to retrain the model would not be as easy as using, say, word2vec.
The package shared on GitHub does not seem to include the training part yet. The paper extends an interesting application of the sequence to sequence models by borrowing the idea of skip-gram, using the current sentence to predict the sentence before and after. Although the experiments do show the potential of this model, the approach didn't really outperform the state-of-the-art on any of the tested cases. Given that the model takes a lot of time to train, it might be difficult for users to retrain the vectors using their, domain-specific, corpus.