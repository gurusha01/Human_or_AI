SUMMARY: Following the traditional approach to algorithmic stability pioneered in [6], the authors study the relationship between generalization and stability. However, contrary to previous approaches, they consider the general setting of learning. The result is interesting because they have necessary and sufficient condition to (uniform) generalization, not just bounds. However, the main drawback is the "uniform" part: it is not clear if it is a too strong condition, i.e. being able to generalize wrt to any parametric bounded loss.
DETAILED COMMENTS: The paper is very interesting and clearly written.
In a sense, as the authors' say, this work complements the results in [14], finding necessary and sufficient conditions to generalization, rather than learnability, in the general setting of learning.
I have to say that there has been always some kind of division in the ML community about people that care about learnability and the ones that care about generalization. Personally, I find both interesting and different enough to generate different approaches and papers. Hence, the topic is the paper is clearly interesting, at least for a part of the NIPS community, even if it does not cope with learnability.
There are also some drawbacks in this paper. The main problem for me is the lack of depth in the discussion of the implications (see for example Remarks 1-2 and Examples 2-3). The only clear case seems to be the one for finite VC dimension and you also get a feeling of mild overselling in reading some remarks and comments. This lack of depth and the unconventional (for a part of the ML community) definition of the algorithmic stability could make the paper accessible to a small niche of people and reduce its impact. This is also important because the definition of uniform generalization (def. 3) appears quite strong, and it can be only justified if we buy the authors' thesis that the equivalent algorithmic stability (def. 5) is "rather weak". That is to say, most of the algorithms will satisfy it, so we have that they will generalize, even wrt any bounded loss. However, this "weakness" is stated only in vague and intuitive terms, rather than in a more ML-friendly way.
This seems problematic, but I don't think this can be a reason for rejection.
In fact, as I understand it, the loss function here has no relationship with the algorithm used. Indeed, the algorithm itself does not have to be ERM nor use it in any part of it. Overall, it seems to me that there are no concrete elements to judge the condition of "uniform generalization" as "too strong", besides one's intuition. On the other hand, the condition of algorithmic stability does appear weak to me, and, more importantly than any "feeling" and/or "intuition", seems to have nice representation in some cases (e.g. finite VC dimension).
Another problem, as also found by the other reviewers, is the fact that the bounds are only in expectation, but I find this a minor thing. Indeed, many of the results based on algorithmic stability are only in expectation, yet they have succeeded in inspiring new algorithms and techniques.
If the paper is accepted, I encourage the authors to provide more detailed examples and/or quantification of the algorithmic stability in other interesting cases, if only to increase the impact of their work.
Overall, I find the result correct, novel and mature enough to be published in NIPS. The paper presents a necessary and sufficient condition to (uniform) generalization, through a novel notion of algorithmic stability in the generic setting of learning. A few implications and examples are shown as well.I found the paper clearly written and interesting enough to be accepted, even if a more in-depth discussion of the implications is needed.