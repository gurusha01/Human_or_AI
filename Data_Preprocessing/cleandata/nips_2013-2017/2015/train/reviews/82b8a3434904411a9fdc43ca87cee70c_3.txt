Interesting paper, with a potentially useful result.
It is not clear that the bound would remain valid if the method were kernelized and the kernel parameters also tuned using cross-validation (as the kernel and regularization parameters tend not to be independent).
The experimental evaluation is a little weak as there are only a few of small datasets.
The size of the datasets is especially an issue as the paper discusses the computational practicality of the bound.
It is not clear that the bound enables better models to be selected, than would be selected simply by optimizing the cross-validation error directly (within some computational budget).
The method may be useful in avoiding over-tuning the regularization parameter (i.e. over-fitting the cross-validation estimate).  The paper presents a bound on the cross-validation error for linear models and a procedure for finding a value with guaranteed CV error.The paper is interesting, and well written, but the experimental evaluation is a little weak.