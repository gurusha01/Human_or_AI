This paper presents an algorithm for learning a decision tree with linear split functions at the nodes. The algorithm optimizes the tree parameters (i.e. the weights in the split functions and the distribution of the output variable at each leaf) in a global way, as opposed to the standard greedy way of learning trees. An upper bound on the tree's empirical loss is proposed, and to achieve a smoother optimization problem, this upper bound is regularized, via a L2 regularization of the split function weights. The algorithm then consists in using a stochastic gradient descent in order to identify the tree parameters that minimize the upper bound.
 The tree optimization problem is elegantly formulated, by establishing a link with structured prediction with latent variables. However, the empirical results that are shown in the paper do not really convince that a global optimization is a better alternative to a greedy learning of the tree. The globally optimized tree outperforms significantly the greedy tree on a couple of datasets only, while being more intensive computationally.
 The paper is well structured, but some points need to be clarified:
- The stable version of SGD is not described very clearly. How are data points assigned to leaves? It would probably help to have a pseudo-code.
- Figure 2: are these results obtained when applying SGD or stable SGD? What does an "active" leaf mean exactly?
- In line 10 of Algorithm 1, \Theta is updated and then projected on a simplex. In practice, how do you solve this projection problem? It would also maybe be worth mentioning that this projection ensures that the each line of \Theta sums up to one.
 Minor comments / typos:
- In lines 407-408, you say that you tune the "number of features evaluated when building an ordinary axis-aligned decision tree". I am not sure to understand, since all the features are evaluated at each node when building a standard decision tree. - In equations (7), (8) and (9), "argmax" must be replaced with "max". - Figure 1 is never mentioned in the main text. - Line 141: "...to the index of the leaf on by this path." Something is wrong at the end of this sentence. - Line 238: "...the solution to loss-augmented inference..." -> the solutions to... - Line 301: "An key observation..." -> A key... - Line 360: What is FSGD? - Line 404: "...a tree with minimum training errors." I suppose you mean "tuning" errors?
In the supplementary material:
- Line 29: max{g \in sgn...} -> max{g = sgn...} - Line 30: sgn(Wx)^T Wx + l(\Theta^Tf(g), y)... -> sgn(Wx)^T Wx +
l(\Theta^Tf(sgn(Wx)), y)
 The tree optimization problem is elegantly formulated, but the results shown are not convincing.