This paper consider the high-dimensional estimation problems with sub-exponential design and noise. The authors establish estimation error bound via the exponential width argument, show that the estimation error will be at most \sqrt{\log p} times worse than the case for sub-Gaussian.
The first result is the connection between Gaussian and exponential widths. It is shown that the exponential widths is upper bounded by Gaussian widths by a fact of c \sqrt{\log p}.
The second important result shows to obtain restricted eigenvalue condition, for sub-exponential design the sample complexity is the same as sub-Gaussian case. The authors outlined two prooof techniques and shows that the bound based on exponential widths argument is \log p worse than the one obtained by VC dimension.
The authors also conducted some simulations to verify the proved sample complexity. However, I don't think the way Figure 1 presented is very informative: to show the sample complexity is O(s \log^2 p) instead of O(s \log p), its better to plot the normalized sample size (n/s \log^2 p) versus the probability of success to see whether the curves for different dimensions are close enough.
Overall, I think this paper established some substantial results for high dimensional estimation under exponential design and noise. The proof technique is novel and interesting as well. I would like to see it appear in NIPS.  This paper establish the high-dimensional estimation error bound with sub-exponential design and noise, interesting and useful analysis were presented.