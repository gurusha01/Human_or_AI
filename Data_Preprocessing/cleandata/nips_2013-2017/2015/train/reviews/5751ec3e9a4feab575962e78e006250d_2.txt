This paper proposed the population posterior distribution for Bayesian modeling of streams of data and showed how stochastic optimization could be used to find a good approximation.
The proposed framework and algorithm were demonstrated on both latent Dirichlet allocation and Dirichlet process mixture models on text and geolocation data and were shown to perform better than previous work in some cases.
Overall, I think the main idea of the paper is very interesting and it would fit in well at NIPS.
There are a few aspects of the paper that could use some more discussion though.
First, the authors were very careful throughout the paper to use the term "Bayesian modeling", except the title uses "Bayesian inference", which this paper definitely does not provide a method for.
The title should really use "Bayesian modeling" instead.
Also, the notation used in Eqs. 3 and 4 for the local variables is confusing as they're being optimized to the expectation of a population average.
However, they're local to a particular data point.
Perhaps there's a better way to write this because as written it looks like the learned local variational parameters will just be mess because they'll all be averaged together.
I see how everything works in the actual algorithm, I'm just hoping there's a clean way to write this in Eqs. 3 and 4.
Also, the step-size for gradient-ascent was never introduced in the algorithm.
Finally, in the paragraph around line 153, the authors say that optimizing the F-ELBO is not guaranteed to minimize the KL, but in the sentence immediately after they say they show that it does in Appendix A.
This needs to be explained better, because these sentences say opposite things.
A quick discussion about the \alpha parameter is given in the experiments, however, the fact that it controls the level of uncertainty in the approximate posteriors is extremely important (one of the selling points of the method is that the posterior doesn't collapse to a point).
It would be great to have some discussion of this earlier on, especially since it is essentially a dataset size.
Additionally, there's no discussion of whether or not the algorithm converges to anything and what that means.
One selling point of the population posterior by the authors is that since there's always model mismatch the posterior concentrating on a point in the parameter space is a bad thing.
But this statement seems to have the underlying assumption that people think that
their model is converging to the data generating distribution as more data arrives.
But I'm not certain people actually think this.
Having a fixed level of uncertainty (at least a lower-bound on it) through the \alpha parameter seems really useful for streaming data, I just don't think model mismatch is a good selling point.
The experiments section is well done and the experiments are convincing.
One question is whether some discussion can be made on why SVB does worse.
Is it local optima?
Additionally, the authors should state the actual step-size schedules that they used.
Are the results sensitive to the step-size schedule?
Lastly, how many replicates of permuting the order of the data did you use and can error bars be included?
The rest of my comments are minor:
 - There are a lot of typos that need to be fixed.
 - There is no future work in the "Discussion and Future Work" section.
Definitely include some because this is really interesting work.
I would like to reiterate that I thoroughly enjoyed this paper and the ideas it proposed.
I hope the authors address my concerns, especially those regarding clarity of presentation, and I think it would be a great addition to the proceedings. This paper proposed an interesting method for Bayesian modeling of streaming data.It would be a nice addition to the NIPS proceedings.