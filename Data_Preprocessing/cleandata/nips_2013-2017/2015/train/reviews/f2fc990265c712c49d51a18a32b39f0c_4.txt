The paper considers an algorithm for the optimization problem
\min l(x) + r(x)
where l is a smooth function and r(x) = \sum(\rho(|x_i|) for a concave smooth function \rho. The authors present a hybrid algorithm that combines a quasi newton step and a gradient descent step. The main idea of the algorithm is borrowed from the OWL-QN algorithm. That is, in order to deal with the non-smoothness of the regularizer, the iterates of the algorithm are constrained to remain on the same quadrant. However, the fact that the regularizer is non-convex does not allow the use of subgradient properties. Instead, the proofs are based on the properties
Clarke sub-differential.
 I believe that the problem being tackled is not trivial. Dealing with non-convexity is an added difficulty to the non-smoothness of the regularizer. There are three problems that I see with this paper.
(1) In spite of the criticism of DC programming for non-convex optimization, this algorithm provides linear convergence guarantees whereas the proposed algorithm does not provide any rates of convergence.
 (2) In order to obtain convergence the algorithm requires a gradient descent step. However, it seems that the only way to find the optimal tradeoff between the GD step and the QN step is by actually running the algorithm with different tradeoff parameters.
(3) As a practitioner I have never used a non-convex regularizer like the ones described on this paper nor have I seen them in any publications. Probably this is only because of the type of applications I do but I do want to make sure that this problem is indeed relevant to the machine learning community.
 The paper deals with minimizationunder a non-convex regularizer that promotes sparsity. The paper is well written and the math is correct. However I am not entirely sure of the relevance of the results for the learning community.