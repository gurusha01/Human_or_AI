I like that this paper compares classifier accuracy to human performance. It is interesting in Table 2 to see that the LENSE system outperforms the 3-vote baseline, but underperforms the 5-vote. This indicates a significant comparison about the relative efficiencies of de-noising crowd responses via additional consensus vs. the LENSE system approach. The example given in the paragraph starting at Line 154 is rather abstract and difficult to follow. For easy readability, it might be useful to use a specific query example with crowd responses and times similar to what they would be in a real case. In Tables 2 and 3, it is confusing that the threshold baseline is sometimes referred to as entropic and sometimes as threshold. I do not understand why the related work is in Section 5. Overall, a compelling problem but the explanations were somewhat opaque.
 Line 086: Figure ?? This paper tackles the problem of creating high-accuracy classifiers beginning with zero labeled examples. A component problem in this paper is handling noisy crowd respondents, and while this topic has be considered from many angles in the literature, the timing and latency optimization factors of this paper are novel and interesting.