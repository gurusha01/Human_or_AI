This paper presents an elegant modification of EP that reduces memory requirements at cost of reduced accuracy (and sometimes more computation).
The paper explores connections with related algorithms.
The extensive set of experiments give a convincing argument for the practicality of the algorithm.
The only weakness of the paper is some sloppiness in the presentation.
The abstract and introduction use the word "computation" when they actually mean "memory".
Memory does not equal computation.
SEP is actually adding computation in some cases, in exchange for less memory.
 The description of SEP is unclear about the number of iterations.
In Table 1, was SEP run to convergence (if such a concept exists), or was it manually stopped?
 In section 4.1, it is not clear what is meant by "converges in expectation".
Exepctation of what?
With what being random?
Theorem 1 has a similar issue.
Furthermore, this claim is not proven.
The "proof" of theorem 1 in appendix A is not a proof---it is simply re-asserting the theorem.
If the authors cannot make a precise statement with a real proof then this claim should not appear in the paper.
The claim in section 4.4, that messages to latent variables associated with a single data point need not be stored, requires a special assumption.
The assumption is that the term p(xn | hn, theta) can be updated in one step.
This is true for the model in section 5.2, but it is not true in general.
For example, LDA has a latent variable for each document (the topic proportions) but it cannot be updated in one step.
So these messages will need to be stored, unless you want to significantly increase the amount of computation.
Appendix B.2 should say "This procedure still reduces memory by a factor of N/K".
In section 5.1, ADF should be collapsing to a delta function at the true posterior mode, not mean.  An elegant modification of EP, a nice discussion of connections to other algorithms, and extensive experimental results.