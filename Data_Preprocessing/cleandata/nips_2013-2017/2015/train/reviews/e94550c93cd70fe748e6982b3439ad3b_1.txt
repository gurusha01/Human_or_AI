Consensus Monte Carlo (CMC) is a method for parallelizing MCMC for posterior inference over large datasets. It works by factorizing the posterior distribution into sub-posteriors each of which depend on only a subset of datapoints, sampling from each of these sub-posteriors in parallel, and then transforming samples from the sub-posteriors using an aggregation function to samples from the real posterior. Existing works use very naive methods of aggregation which result in high bias, or are computationally very expensive, which make it difficult to use Consensus Monte Carlo in practice. This paper proposes a more principled way of combining samples by optimizing over aggregation functions using variational inference.
Clarity: The paper is well written and easy to follow.
Significance: Bayesian inference for big datasets is a very important problem. This paper is significant because it presents a way of making parallel MCMC using CMC more practical.
Originality: The novelty in the paper is the use of variational inference to optimize over CMC aggregation functions instead of using a fixed one. This is rather straightforward, except that the entropy term is difficult to estimate and the authors propose minimizing a lower bound of it instead. Nevertheless, this is an important difference and allows the proposed method to achieve a large reduction in error in estimating posterior expectations compared to very simple baselines.
Quality: The method itself seems sound, but I am not quite satisfied with the experiments. All experiments are on toy problems and datasets. Although it is nice to have these toy examples as they illustrate important aspects of the algorithm like the ability to aggregate structured samples, more realistic experiments are also necessary. Parallel MCMC is useful mainly for large datasets, but the authors compare on very small datasets, the largest of which only had 50K datapoints. I also wish the authors had experiments on more interesting models, e.g. LDA rather than on a Gaussian mixture model or normal-wishart, so that their method is of more direct interest to practitioners (plus there are large, real datasets for LDA) Also, the authors compare only against very simple baselines. How does this compare to Neiswanger et al or the Weierstrass sampler method of Wang and Dunson (for settings where these methods are also applicable)? How does this compare to serial mini-batch algorithms like Stochastic Gradient Langevin Dynamics (SGLD)? Although the proposed method could also use SGLD for sampling from sub-posteriors, does the additional time in aggregating samples and optimizing the aggregating function prevent it from having an advantage over vanilla SGLD? Finally, I really wish these methods were applied to a problem where there is a clear advantage of using Bayesian inference as compared to using a point estimate, e.g. where point estimates overfit, or the uncertainty obtained from Bayesian inference is actually used for something.
 This is a good paper and is well written, but the experimental section could be a lot better. I lean towards accepting it, but rejecting the paper to wait for more convincing and realistic experiments would not be a big loss.