The authors present a technique inspired by the Skip-gram model of Tomas Mikolov to encode a sentence representation. The representation is the last layer in the sentence sequence of a GRU RNN. From this representation, the model is trained on a Billion words corpus (extracted from books) to generate the previous and next sentence in a paragraph as a GRU RNN Language Model conditioned on the encoded representation. Even if the model is trained on a relatively small vocabulary (20K), they present an easy way to extend it to 1M using word2vec and a linear mapping. They present an extensive set of experiments across many tasks for NLP, using the raw sentence representation as feature.
I really liked the paper and I think it is a good starting point for upcoming research in learning sentence representation, potentially learning representation at the paragraph level. Potential criticism of the paper would be that there is no new state of the art from the method but the research community knows how hard it is to find a representation that generalizes well across a wide range of NLP tasks.
The only experiment missing in my opinion would be to fine-tune through the GRU RNN from the sentence representation on a few datasets described in the paper to verify how far the obtained improvement would be from the state of the art. Regarding the bidirectional model, do you have results with a single model that would take the forward and backward pass rather than concatenating the representation?
 minor comments: 2nd 3rd paragraph in Introduction - Fig 2 and Table 2 should be replaced by Fig 1 and Table 1 Sec 1 last paragraph 'such an experimental is begin' Sec 3.2 2nd paragraph 'learned representation fair against heavily'
training details: how many words per seconds the model is able to process for training using adam? how many passes thourgh the whole dataset do you perform in the 2 weeks training? I really liked the paper and I think it is a good starting point for upcoming research in learning sentence representation, potentially learning representation at the paragraph level.