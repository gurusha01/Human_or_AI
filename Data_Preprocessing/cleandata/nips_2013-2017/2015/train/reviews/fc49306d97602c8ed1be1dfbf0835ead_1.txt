%%%% post-rebuttal comment %%%%
I have increased the score from 6 to 7 to better reflect my impression of this paper.
%%%%
== Summary ==
This paper proposes a kernel-based method for cross-domain matching for bag-of-words data. The difficulty is that the data may be represented using features across domains so that they cannot be compared directly. A common approach is to learn a "shared latent space". Existing algorithms such as CCA, KCCA, and topic modeling can be thought in this way. Unlike previous works, this paper looks at the data, e.g., documents and images, as a probability distribution over vectors in the shared latent space. As a result, the proposed method is able to capture the occurrence of different but semantically similar features in two distinct instances. The latent vectors are estimated by maximizing the the posterior probability of the latent vectors expressed in terms of the RKHS distance between instances (aka maximum mean discrepancy). Experimental results on several real-world datasets, e.g., bilingual documents, document-tag matching, and image-tag matching, are very encouraging.
In summary, the paper proposes a simple and intuitive idea for cross-domain matching problem with strong empirical results.
== Quality ==
The key weakness of this paper is the technicality.
== Clarity ==
The paper is clearly written. I only have minor comments on clarity.
In the experiment section, you mentioned the "development data". Does it refer to the validation data? [Line 185--189] This paragraph seems to suggest that Xi and Yj live in different latent spaces. It should be clarified. Eq. (9) Why do you consider this prior? What is the motivation? It should be clarified. In Eq. (11), perhaps you could mention that the prior you chose will correspond to the L_2 regularization on x and y. This is also in line with the formulation considered in Yuya et al., (NIPS2014). Again, it should be clarified why this is a good choice.
== Originality ==
The proposed idea seems to be quite similar to Yuya et al., (NIPS2014), but considers the cross-domain matching problem instead of classification. Nevertheless, I think the idea of treating data as probability distributions over latent space is quite interesting and has potential applications in many areas.
== Significance ==
I think the idea presented in this work can be applied more broadly. For example, this could be used for "representation learning" in which the latent space may have a hierarchical structure, e.g., tree, or represent the parameters of some generative model, e.g., RBM or deep neural network.
 Relevant papers:
Y. Li, K. Swersky, R. Zemel. Generative Moment Matching Networks. ICML 2015. G.K. Dziugaite, D. Roy, Z. Ghahramani. Training generative neural networks via Maximum Mean Discrepancy optimization. UAI 2015.
Minor comments
Eq. (15) and (16) seem to be redundant as they are basically the same as Eq. (12) and (14). Maybe they can be removed to save space.  A simple and intuitive approach for cross-domain matching of bag-of-words data with strong empirical results.