- the authors try to make the case for non-negative transforms, like ReLUs, being important for successful models when they say "Representations learned by ReLUs are not only sparse but also non-negative" and "Correctness means that the RFN codes are non-negative, sparse, have a low reconstruction error, and explain the covariance structure of the data." In that case, how does one explain MSR's paper with PReLUs outperforming ReLUs when PReLUs are not non-negative. This is a weak argument, the only serious advantage is the non-saturating nature of the transform.
- The authors explicitly state that "In summary, our goal is to construct input representations that (1) are sparse, (2) are non-negative, (3) are non-linear, (4) use many code units, and (5) model structures in the input data" as mentioned above. Not sure what the importance of goal2 is. Goal4 is a questionable goal - if anything, good representation will capture the same structure in the data with less hidden units. Low reconstruction error would be a good goal - it's missing. - The authors state "Current unsupervised deep learning approaches like autoencoders or restricted Boltzmann machines (RBMs) do not model specific structures in the data. On the other hand, generative models explain structures in the data but their codes cannot be enforced to be sparse and non-negative." - are they trying to say that RBMs are not generative? or is it unclear writing? Hopefully the latter. - Table 1 is confusing. The authors state it demonstrates their method has lowest reconstruction error and yet PCA has lower reconstruction methods in the table? - Figure 2 supposedly shows the method is robust to background noise. However, the filters look significantly less robust than what I've seen in denoising autoencoders - Table 2 is misleading: it compares accuracy on computer vision task across deep learning models that leverage pretraining and the authors show that they are competitive with state of the art. Except they seem to have omitted schmidhuber's convolutional autoencoders (http://people.idsia.ch/~ciresan/data/icann2011.pdf) which beat their method on MNIST and really outperform on CIFAR 10 - the author's test test error is 41% , Schmidhuber gets 22% and that's from 2011, today the state of the art is 9%.
For the experiments, I'd have liked to see: - "RFNs vs. other unsupervised methods" should have included some explicit sparse coding methods (e.g., l1 regularized regression). They should have done comparisons of sparsity and reconstruction error on real-world examples, instead of just on the toy dataset.
- "RFN pretraining for deep nets" would have been stronger if the authors compared their methods to more other methods for pre-training. Instead they mostly do comparisons against other non-deep-net methods, which is not the point.
- It would also have been nice to see the results of classification tasks done directly on the output of the RFNs vs. on the output of other unsupervised methods.  The authors work out many details needed to efficiently apply posterior regularization to factor analysis. The results shown in this paper are promising applications to unsupervised learning and unpromising applications to pre-training strategies of deep learning models - classification accuracy on CIFAR10 is significantly worse than Convolutional Neural Networks pre-trained with convolutional auto-encoders and far worse than state of the art.