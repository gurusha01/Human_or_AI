(This is a "heavy" review.)
== SUMMARY ==
This paper proves that a certain notion of algorithmic stability (in the learning algorithm) is a necessary condition for "uniform generalization". In the paper, "algorithmic stability" is a probabilistic definition, which loosely means that the (random) hypothesis output by a (randomized) learning algorithm should have decreasing dependence on any single example as the size of the training set approaches infinity. In this context, uniform generalization means that there exists a minimum training size such that the difference of the empirical and expected losses (in expectation over hypotheses output by the learning algorithm) is upper-bounded, uniformly over all "parametric" loss functions and data distributions. Uniform generalization is a stronger condition than regular generalization -- which, using the paper's definition, only needs to hold asymptotically, for a given loss function. The main theorem is interpreted in various ways: to explain the benefit of methods like dimensionality reduction and dropout; to analyze the relationship between the effective size of the domain and algorithmic stability (hence, generalization); and to make connections between algorithmic stability and VC dimension.
All in all, I find this to be a well-crafted, insightful theory paper. There are well-placed examples that ground the theory in practice, and the implications of the main theorem are interesting. (I especially liked the analysis of the effective size of the domain.) The writing could use some minor polishing (see detailed notes), but, overall, it's pretty good.
My only concern is that it probably won't have much impact for practitioners; there are no take-away messages, no "prescriptions". The paper gives us a better understanding of learning (in theory), but it doesn't propose anything that we should be doing differently (in practice). It would be a much stronger paper if its insights motivated a new technique.
== HIGH-LEVEL COMMENTS ==
I was thrown off by the term "inference process" used in the abstract. This usually refers to prediction (i.e., infer Y given X), but it is equivalent to learning in this paper (i.e., infer H given S_m). The paper could be clearer about this distinction, or (my preference) you could just avoid calling it inference. On a related note, it would interesting to analyze whether stability in the predictor's inference is a necessary condition for generalization.
The main result hinges on the subtle distinctions between "learnability", "consistency", "uniform convergence" and "(uniform) generalization". I found the discussion of these concepts (Sec 2) to be a bit tortured and would have appreciated some discussion of why these distinctions matter. Is there a class of learning algorithms (or hypotheses) that only supports learnability but not generalization (or vice versa)? Why should the reader care about these distinctions?
On a related note, the characterization of "learnability" via the excess risk seems strange to me. I've never read this definition anywhere. Normally (to me), learnability means PAC learnability; that is, there exists a poly(\epsilon,\delta,m)-time algorithm such that, with probability at least 1-\delta over draws of m examples, the learned hypothesis has error at most \epsilon. This definition doesn't appear to be equivalent to yours. I guess what I'm saying is you should make it clearer that the definition you're using is taken from source XYZ, and may not be the definition that the reader is accustomed to.
I was a bit confused by the application of the data processing inequality in the proof of Theorem 1. If the Markov chain is
 ( Sm -> H -> L(.;H) ) == ( A -> B -> C ) then what is Ztrn? Is it considered A in this construction? The inequalities on line 242 seem to use the fact that
 S(L(.;H);Ztrn) > S(H;Ztrn), but doesn't this mean that S(C;A) > S(B;A), and isn't this a different inequality from Lemma 1?
== DETAILED COMMENTS ==
- Lines 19 and 50 : "new novel" -> "new". - Line 25 : "justification to" -> "justification for". - Lines 34 - 44 read in like a tutorial on machine learning, which is a bit too rudimentary for the intended audience. - Line 40 : "such two objectives" -> "these two objectives". - Line 50 : "example to such approach is the development of the SVM" -> "example of such an approach is the SVM". - Line 52 : "subtle" is the wrong word here, but I can't figure out what is meant here, so I can't suggest a better word. - Line 55 : "such rich theories" -> "these rich theories". - Line 80 : "such result" -> "this result". - Line 107 : I've only ever seen generalization defined for a given hypothesis class; never for a learning algorithm. Can you provide a citation for this definition? - Line 197 : "we define stability" -> "we define the stability". - Line 302 : "bleak" seems like too strong a word. - Line 312 : "such notion" -> "this notion". - Line 422 : "vanish" -> "vanishes". - Line 424 : "choice of the" -> "choice of". - Line 425 : "such result" -> "this result".
 == POST-RESPONSE ASSESSMENT ==
I'm still not convinced that this paper will have any impact. The relationship between stability and generalization has been well-known for over a decade, so the high-level message is nothing new. This paper basically just expands stability theory and confirms a conjecture for a more general learning setting. The results don't motivate any specific new technique, and the implications for existing data processing heuristics (Ex. 2 & 3) are a bit hand-wavy. Nonetheless, it's a well-written paper, and an interesting read from the theory perspective; just because I don't see the practical value does not mean that others won't. Therefore, I recommend acceptance. The paper proves that algorithmic stability is a necessary condition for uniform generalization. The theory seems technically sound (although a bit confusing), the insights are intriguing, and the presentation is good, but the result probably won't have much impact.