1) The authors should at least mention the algorithm in section 3.2 is deeply similar to the cited references
[14] J. Zhao, J. Shawe-Taylor, and M. van Daalen, "Learning in stochastic bit stream neural networks," Neural Networks, vol. 9, no. 6, pp. 991 - 998, 1996. and [15] Soudry, Daniel, Itay Hubara, and Ron Meir. "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights." Advances in Neural Information Processing Systems. 2014.
in its final form (which is a hybrid of both algorithms), and was derived using identical approximations.
2) The first paragraph in the discussion gives the impression the "constrain-then-train" methodology (learning a weight distribution and then sampling from it to produce a fixed network ensemble at test time) is novel, even though it was already suggested and tested in ref [15].
3) Novelty claim (i) in the last paragraph of the introduction is also misleading. Ref. [15] "expectation backpropgation" algorithm is a backpropgation update rule (using expectation propagation), implemented for binary neurons (i.e., with a sign activation function, in which zero input is never reached exactly) with binary weights, and can easily have constrained connectivity.
4) The cited previous state-of-the-art are wrong. To the best of my knowledge, the previous best results for binary weights (and neurons) were achieved by the algorithm in ref. [15], which was tested on the mnist (http://arxiv.org/pdf/1503.03562v3.pdf) without data augmentation, as was done in this paper.
%%% Edited after author's feedback Thank you for agreeing to address these concerns.  This paper has quite strong and novel experimental results, in terms of accuracy and power consumption. However, both the general methodology and the algorithm itself are not as novel as claimed in the paper.