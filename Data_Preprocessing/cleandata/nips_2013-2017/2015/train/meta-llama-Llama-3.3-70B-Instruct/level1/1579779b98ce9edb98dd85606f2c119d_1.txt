This paper proposes a novel approach to learning decision trees, deviating from the traditional greedy methods that optimize split functions one node at a time. Instead, the authors formulate a global objective that jointly optimizes the split functions at all levels of the tree, along with the leaf parameters. This non-greedy approach is shown to outperform greedy decision tree baselines on several classification benchmarks.
The paper's main contribution is the establishment of a link between decision tree optimization and structured prediction with latent variables. The authors propose a convex-concave upper bound on the tree's empirical loss, which acts as a surrogate objective that can be optimized using stochastic gradient descent (SGD). The use of SGD enables effective training with large datasets, and the authors demonstrate that their approach can learn deep trees efficiently.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The related work section is thorough, and the authors adequately reference previous contributions in the field. The experimental results are convincing, demonstrating the superiority of the non-greedy approach over greedy baselines.
One of the strengths of the paper is its ability to handle any general loss function, without sacrificing the efficiency of inference imparted by the tree structure. The authors also provide a natural way to regularize the joint optimization of tree parameters, which helps to prevent overfitting.
However, one potential weakness of the paper is the complexity of the optimization problem. The authors acknowledge that the optimization problem is non-convex, and the use of SGD may not guarantee convergence to a global optimum. Additionally, the paper assumes that the tree depth is fixed, which may not always be the case in practice.
Overall, the paper presents a significant contribution to the field of decision tree learning, and the non-greedy approach has the potential to improve the accuracy and efficiency of decision tree-based models. The authors provide a clear and well-written paper, and the experimental results are convincing.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of decision tree learning.
* The non-greedy approach has the potential to improve the accuracy and efficiency of decision tree-based models.
* The authors provide a clear and well-written paper, with a thorough related work section and convincing experimental results.
* The approach can handle any general loss function, without sacrificing the efficiency of inference imparted by the tree structure.
Arguments con acceptance:
* The optimization problem is non-convex, and the use of SGD may not guarantee convergence to a global optimum.
* The paper assumes that the tree depth is fixed, which may not always be the case in practice.
* The approach may be sensitive to the choice of hyperparameters, such as the regularization constant and the learning rate.
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 8/10
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of decision tree learning, and the non-greedy approach has the potential to improve the accuracy and efficiency of decision tree-based models. However, the authors should be aware of the potential limitations of their approach, and consider addressing these limitations in future work.