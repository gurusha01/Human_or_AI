This paper proposes a novel approach to batch learning from logged bandit feedback (BLBF) by introducing a self-normalized risk estimator that avoids the propensity overfitting problem inherent in the conventional unbiased risk estimator. The authors demonstrate that the conventional estimator suffers from severe anomalies, including unbounded variance and non-equivariance, which can lead to degenerate risk estimates. In contrast, the self-normalized estimator is shown to be equivariant, bounded, and strongly consistent, making it a more reliable choice for BLBF.
The paper is well-structured, and the authors provide a clear and concise introduction to the problem of BLBF and the limitations of existing approaches. The related work section is thorough, and the authors provide a detailed discussion of the connections between BLBF, causal inference, and importance sampling.
The technical contributions of the paper are significant, and the authors provide a rigorous analysis of the self-normalized estimator, including a proof of its strong consistency. The experimental evaluation is also comprehensive, and the authors demonstrate the effectiveness of their approach, called Norm-POEM, on several multi-label classification problems.
The strengths of the paper include:
* A clear and concise introduction to the problem of BLBF and the limitations of existing approaches
* A thorough discussion of the related work and connections to causal inference and importance sampling
* A rigorous analysis of the self-normalized estimator, including a proof of its strong consistency
* A comprehensive experimental evaluation demonstrating the effectiveness of Norm-POEM
The weaknesses of the paper include:
* The paper assumes a certain level of familiarity with BLBF and importance sampling, which may make it challenging for readers without a strong background in these areas to follow
* Some of the technical proofs and derivations are relegated to appendices or external references, which may make it difficult for readers to fully appreciate the technical contributions of the paper
Overall, the paper is well-written, and the authors provide a significant contribution to the field of BLBF. The self-normalized estimator proposed in the paper has the potential to improve the robustness and reliability of BLBF algorithms, and the experimental evaluation demonstrates the effectiveness of Norm-POEM in practice.
Arguments for acceptance:
* The paper proposes a novel approach to BLBF that addresses a significant limitation of existing methods
* The technical contributions of the paper are rigorous and well-supported by theoretical analysis and experimental evaluation
* The paper has the potential to impact the field of BLBF and related areas, such as causal inference and importance sampling
Arguments against acceptance:
* The paper assumes a certain level of familiarity with BLBF and importance sampling, which may limit its accessibility to readers without a strong background in these areas
* Some of the technical proofs and derivations are not fully presented in the paper, which may make it challenging for readers to fully appreciate the technical contributions of the paper.