This paper presents a significant contribution to the field of risk-sensitive reinforcement learning by extending the policy gradient method to the whole class of coherent risk measures. The authors provide a unified approach to risk-sensitive reinforcement learning, generalizing and extending previous results that focused on specific risk measures such as variance or conditional value at risk (CVaR).
The paper is well-written, and the authors demonstrate a clear understanding of the previous work in the area, including the limitations and challenges of existing approaches. The introduction provides a thorough motivation for the research, highlighting the importance of managing variability in cost, in addition to its expected outcome, in various applications such as finance and operations research.
The technical contributions of the paper are substantial, including the derivation of a new formula for the gradient of static coherent risk measures and a policy gradient theorem for Markov coherent dynamic risk measures. The authors also propose a sampling-based algorithm for estimating the gradient of dynamic risk measures, which is a significant improvement over existing methods that rely on dynamic programming.
The paper is well-organized, and the authors provide a clear and concise explanation of the technical results, including the assumptions and proofs. The numerical illustration in Section 6 demonstrates the importance of flexibility in designing risk criteria and shows how the proposed approach can be used to select an appropriate risk measure that suits both the user's risk preference and the problem-specific properties.
The strengths of the paper include:
* A unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results
* A new formula for the gradient of static coherent risk measures
* A policy gradient theorem for Markov coherent dynamic risk measures
* A sampling-based algorithm for estimating the gradient of dynamic risk measures
* A clear and concise explanation of the technical results
The weaknesses of the paper include:
* The paper assumes that the risk envelope is given in a canonical convex programming formulation, which may not always be the case
* The algorithm for estimating the gradient of dynamic risk measures requires a simulator for sampling multiple state-transitions from each state, which may not be feasible in all cases
* The paper does not provide a comprehensive comparison with existing methods, which would be useful for evaluating the performance of the proposed approach
Overall, the paper is well-written, and the technical contributions are significant. The authors demonstrate a clear understanding of the previous work in the area and provide a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.
Arguments for acceptance:
* The paper presents a significant contribution to the field of risk-sensitive reinforcement learning
* The technical results are substantial and well-motivated
* The paper is well-written and easy to follow
* The numerical illustration demonstrates the importance of flexibility in designing risk criteria
Arguments against acceptance:
* The paper assumes that the risk envelope is given in a canonical convex programming formulation, which may not always be the case
* The algorithm for estimating the gradient of dynamic risk measures requires a simulator for sampling multiple state-transitions from each state, which may not be feasible in all cases
* The paper does not provide a comprehensive comparison with existing methods, which would be useful for evaluating the performance of the proposed approach.