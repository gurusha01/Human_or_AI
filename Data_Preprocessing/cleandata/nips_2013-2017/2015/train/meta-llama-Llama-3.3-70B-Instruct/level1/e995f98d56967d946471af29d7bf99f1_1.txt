This paper proposes a novel approach to training recurrent neural networks (RNNs) for sequence prediction tasks, called Scheduled Sampling. The main idea is to gradually change the training process from a fully guided scheme, where the true previous token is used, to a less guided scheme, where the generated token is used instead. This approach aims to bridge the gap between training and inference, where the model is typically used to generate sequences of tokens one at a time, conditioned on the state and the previous token.
The paper is well-written and clearly explains the motivation behind the proposed approach. The authors provide a thorough review of related work and highlight the differences between their approach and existing methods. The experimental results are impressive, showing significant improvements in performance on several sequence prediction tasks, including image captioning, constituency parsing, and speech recognition.
The strengths of the paper include:
* The proposed approach is novel and addresses a significant problem in sequence prediction tasks.
* The experimental results are thorough and demonstrate the effectiveness of the approach.
* The paper is well-written and easy to follow.
The weaknesses of the paper include:
* The approach is not theoretically grounded, and the authors do not provide a clear explanation of why it works.
* The experimental results are not compared to other state-of-the-art methods, making it difficult to assess the significance of the improvements.
* The paper does not provide a clear discussion of the limitations of the approach and potential avenues for future work.
Arguments for acceptance:
* The paper proposes a novel approach to training RNNs for sequence prediction tasks.
* The experimental results demonstrate significant improvements in performance.
* The paper is well-written and easy to follow.
Arguments against acceptance:
* The approach is not theoretically grounded, and the authors do not provide a clear explanation of why it works.
* The experimental results are not compared to other state-of-the-art methods.
* The paper does not provide a clear discussion of the limitations of the approach and potential avenues for future work.
Overall, I would recommend accepting the paper, as it proposes a novel approach to training RNNs for sequence prediction tasks and demonstrates significant improvements in performance. However, the authors should be encouraged to provide a clearer explanation of the theoretical grounding of the approach and to compare their results to other state-of-the-art methods. Additionally, the authors should discuss the limitations of the approach and potential avenues for future work. 
Quality: 8/10
The paper is technically sound, and the experimental results are impressive. However, the approach is not theoretically grounded, and the authors do not provide a clear explanation of why it works.
Clarity: 9/10
The paper is well-written and easy to follow. The authors provide a clear explanation of the proposed approach and the experimental results.
Originality: 8/10
The proposed approach is novel, but it builds on existing work in the field. The authors do not provide a clear explanation of how their approach differs from existing methods.
Significance: 9/10
The paper demonstrates significant improvements in performance on several sequence prediction tasks, making it a significant contribution to the field.