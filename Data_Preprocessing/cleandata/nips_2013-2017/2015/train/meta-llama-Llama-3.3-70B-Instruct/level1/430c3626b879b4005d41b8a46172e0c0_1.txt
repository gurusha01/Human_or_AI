This paper proposes a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner, which is shown to be better suited for non-convex optimization problems than the popular Jacobi preconditioner. The authors provide theoretical and empirical evidence to support their claims, demonstrating that ESGD outperforms RMSProp on two deep autoencoder benchmarks.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of optimizing large deep networks, highlighting the challenges of ill-conditioning and the limitations of existing adaptive learning rate methods. The related work section provides a thorough overview of previous research in this area, including the Jacobi preconditioner and other adaptive learning rate methods.
The technical contributions of the paper are significant, and the authors provide a detailed analysis of the equilibration preconditioner and its properties. The experimental results are convincing, demonstrating the effectiveness of ESGD in practice. The comparison with RMSProp is particularly interesting, as it shows that ESGD can outperform a widely used and well-established method.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem and the proposed solution
* A thorough analysis of the equilibration preconditioner and its properties
* Convincing experimental results demonstrating the effectiveness of ESGD
* A detailed comparison with RMSProp, providing insight into the similarities and differences between the two methods
The weaknesses of the paper include:
* The paper assumes a significant amount of background knowledge in optimization and deep learning, which may make it difficult for non-experts to follow
* Some of the technical details, such as the proof of Proposition 1, are relegated to the appendix, which may make it difficult for readers to fully appreciate the technical contributions of the paper
* The paper could benefit from a more detailed discussion of the potential limitations and future directions of the proposed method
Overall, I believe that this paper makes a significant contribution to the field of optimization and deep learning, and I would recommend it for acceptance.
Arguments pro acceptance:
* The paper proposes a novel and effective adaptive learning rate scheme, which has the potential to improve the performance of deep neural networks
* The technical contributions of the paper are significant, and the authors provide a detailed analysis of the equilibration preconditioner and its properties
* The experimental results are convincing, demonstrating the effectiveness of ESGD in practice
Arguments con acceptance:
* The paper assumes a significant amount of background knowledge in optimization and deep learning, which may make it difficult for non-experts to follow
* Some of the technical details are relegated to the appendix, which may make it difficult for readers to fully appreciate the technical contributions of the paper
* The paper could benefit from a more detailed discussion of the potential limitations and future directions of the proposed method.