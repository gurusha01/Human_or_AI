This paper proposes a new algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for stochastic optimization in deep learning under communication constraints. The algorithm is designed for parallel computing environments where local workers compute parameters and a master updates a center variable. The key idea is to introduce an elastic force that links local parameters to the center variable, allowing for more exploration and reducing communication overhead.
The paper relates to previous work on parallelizing stochastic gradient descent, such as Google's DistBelief system and the DOWNPOUR method. The authors also discuss the challenges of parallelizing deep learning models on multiple GPU cards and the need for efficient communication protocols.
The strengths of the paper include:
* The proposal of a new algorithm that addresses the communication constraints in parallel computing environments
* The provision of a stability analysis of the asynchronous EASGD algorithm in the round-robin scheme, which shows its advantage over the ADMM algorithm
* The empirical demonstration of the algorithm's effectiveness in training deep neural networks on benchmark datasets, such as CIFAR-10 and ImageNet
The weaknesses of the paper include:
* The lack of a clear comparison with other state-of-the-art methods, such as synchronous stochastic gradient descent and decentralized stochastic gradient descent
* The limited analysis of the algorithm's convergence properties and the effect of hyperparameters on its performance
* The need for further experimentation to fully understand the trade-off between exploration and exploitation in the algorithm
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses an important problem in deep learning
* The algorithm has been empirically shown to be effective in training deep neural networks
* The stability analysis provides a theoretical foundation for the algorithm's performance
Arguments con acceptance:
* The paper lacks a comprehensive comparison with other state-of-the-art methods
* The analysis of the algorithm's convergence properties is limited
* The experimental results are based on a limited number of datasets and may not generalize to other settings
Overall, the paper makes a significant contribution to the field of deep learning by proposing a new algorithm that addresses the communication constraints in parallel computing environments. While there are some limitations to the paper, the strengths outweigh the weaknesses, and the paper is worthy of acceptance. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, the analysis of the algorithm's convergence properties is limited, and the experimental results are based on a limited number of datasets.
Clarity: 9/10
The paper is well-written, and the ideas are clearly presented. The notation is consistent, and the algorithms are well-explained.
Originality: 9/10
The paper proposes a novel algorithm that addresses an important problem in deep learning. The idea of introducing an elastic force to link local parameters to the center variable is new and innovative.
Significance: 9/10
The paper makes a significant contribution to the field of deep learning by proposing a new algorithm that addresses the communication constraints in parallel computing environments. The algorithm has the potential to improve the training of deep neural networks in a wide range of applications.