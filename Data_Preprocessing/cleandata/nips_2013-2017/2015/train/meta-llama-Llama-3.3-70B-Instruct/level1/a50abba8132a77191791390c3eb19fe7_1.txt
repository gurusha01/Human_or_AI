This paper presents several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods, a class of stochastic optimization algorithms that have gained popularity in recent years due to their ability to achieve linear convergence rates. The authors show that the convergence rate of SVRG can be preserved even when the full gradient is approximated using a decreasing sequence of errors, and use this result to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations.
The paper is well-written and clearly organized, with a thorough introduction to the background and motivation of the work. The authors provide a detailed analysis of the convergence rate of SVRG under various assumptions, including the use of inexact approximations to the full gradient. They also propose several new variants of SVRG, including a mixed SG/SVRG method that combines the benefits of stochastic gradient (SG) and SVRG methods.
The strengths of the paper include its thorough analysis of the convergence rate of SVRG, its proposal of several new variants of SVRG, and its experimental evaluation of these variants on several datasets. The authors also provide a clear and detailed explanation of the intuition behind their methods, which makes the paper easy to follow.
One potential weakness of the paper is that some of the proposed variants of SVRG may not be widely applicable, as they rely on specific assumptions about the problem or the data. For example, the use of support vectors to reduce the number of gradient evaluations requires that the problem has a specific structure, and may not be effective for all types of problems.
Here is a list of arguments pro and con acceptance:
Pros:
* The paper presents a thorough analysis of the convergence rate of SVRG under various assumptions.
* The authors propose several new variants of SVRG that have the potential to improve its performance in practice.
* The experimental evaluation of the proposed variants is thorough and well-designed.
* The paper is well-written and clearly organized, making it easy to follow.
Cons:
* Some of the proposed variants of SVRG may not be widely applicable, as they rely on specific assumptions about the problem or the data.
* The paper could benefit from a more detailed comparison with other stochastic optimization algorithms, such as stochastic average gradient (SAG) and semi-stochastic gradient (SSG) methods.
* Some of the notation and terminology used in the paper may be unfamiliar to readers who are not experts in stochastic optimization.
Overall, I believe that this paper makes a significant contribution to the field of stochastic optimization, and that its results have the potential to improve the performance of SVRG in practice. I therefore recommend acceptance.
Quality: 9/10
The paper is technically sound, and the analysis of the convergence rate of SVRG is thorough and well-done. The experimental evaluation is also well-designed and thorough.
Clarity: 9/10
The paper is well-written and clearly organized, making it easy to follow. The authors provide a clear and detailed explanation of the intuition behind their methods.
Originality: 8/10
The paper presents several new variants of SVRG, and the analysis of the convergence rate of SVRG under various assumptions is original and significant. However, some of the ideas presented in the paper may not be entirely new, and the paper could benefit from a more detailed comparison with other stochastic optimization algorithms.
Significance: 9/10
The paper has the potential to make a significant impact on the field of stochastic optimization, as its results could improve the performance of SVRG in practice. The paper is also well-written and clearly organized, making it accessible to a wide range of readers.