This paper presents a comprehensive analysis of the Frank-Wolfe (FW) optimization algorithm, a popular method for constrained convex optimization. The authors highlight and clarify several variants of the FW algorithm, including away-steps FW, pairwise FW, fully-corrective FW, and Wolfe's minimum norm point algorithm. They prove that all these variants enjoy global linear convergence under a weaker condition than strong convexity of the objective, with a constant in the convergence rate that has an elegant interpretation as the product of the condition number of the function and a novel geometric quantity, the pyramidal width of the constraint set.
The paper is well-organized, and the authors provide a clear and concise introduction to the FW algorithm and its variants. The technical analysis is rigorous and thorough, with a detailed proof of the global linear convergence rate for each variant. The authors also provide a discussion of the related work, including the connection to other optimization methods, such as projected gradient and proximal methods.
The strengths of the paper include:
* A clear and concise introduction to the FW algorithm and its variants
* A rigorous and thorough technical analysis of the global linear convergence rate for each variant
* A discussion of the related work and connections to other optimization methods
* A novel geometric quantity, the pyramidal width, which provides a tighter bound on the convergence rate than previous results
The weaknesses of the paper include:
* The paper assumes a strong background in convex optimization and linear programming, which may make it difficult for non-experts to follow
* Some of the technical proofs are lengthy and may require careful reading to understand
* The paper could benefit from more numerical experiments to illustrate the performance of the FW variants in practice
Overall, the paper is a significant contribution to the field of convex optimization and provides a comprehensive analysis of the FW algorithm and its variants. The results are likely to be of interest to researchers and practitioners working in optimization, machine learning, and related fields.
Arguments pro acceptance:
* The paper presents a rigorous and thorough analysis of the FW algorithm and its variants
* The results are novel and provide a tighter bound on the convergence rate than previous results
* The paper has the potential to impact the field of convex optimization and related areas
Arguments con acceptance:
* The paper assumes a strong background in convex optimization and linear programming, which may limit its accessibility to non-experts
* The technical proofs are lengthy and may require careful reading to understand
* The paper could benefit from more numerical experiments to illustrate the performance of the FW variants in practice.