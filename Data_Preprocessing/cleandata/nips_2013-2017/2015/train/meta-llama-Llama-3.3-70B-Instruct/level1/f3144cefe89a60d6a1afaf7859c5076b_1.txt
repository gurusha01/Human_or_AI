This paper proposes the Poisson gamma belief network (PGBN), a deep directed network with gamma distributed nonnegative real hidden units, to infer a multilayer representation of high-dimensional count vectors. The PGBN factorizes the observed count vectors into the product of a factor loading matrix and the gamma distributed hidden units of each layer, and employs an upward-downward Gibbs sampler to jointly train all its hidden layers. The network structure is learned using a layer-wise training strategy, which allows the PGBN to automatically infer the width of each layer given a fixed budget on the width of the first layer.
The paper is well-written and clearly explains the proposed model, its properties, and the experimental results. The authors provide a detailed comparison with existing deep networks, such as sigmoid and deep belief networks, and demonstrate the advantages of the PGBN in modeling overdispersed counts. The experimental results on text analysis tasks, including feature learning for binary and multi-class classification, and perplexities for heldout words, show that the PGBN outperforms existing models, including the gamma-negative binomial process Poisson factor analysis.
The strengths of the paper include:
* The proposal of a novel deep network model that can handle high-dimensional count vectors and model overdispersed counts.
* The development of an efficient upward-downward Gibbs sampler to jointly train all the hidden layers of the PGBN.
* The introduction of a layer-wise training strategy to automatically infer the network structure.
* The demonstration of the advantages of the PGBN in various text analysis tasks.
The weaknesses of the paper include:
* The complexity of the model and the computational cost of the upward-downward Gibbs sampler, which may limit its applicability to large-scale datasets.
* The need for careful tuning of the hyperparameters, such as the upper bound of the width of the first layer and the number of layers.
* The lack of a clear understanding of the relationships between the hyperparameters and the performance of the model.
Arguments pro acceptance:
* The paper proposes a novel and interesting model that addresses an important problem in machine learning.
* The experimental results demonstrate the advantages of the PGBN in various text analysis tasks.
* The paper is well-written and clearly explains the proposed model and its properties.
Arguments con acceptance:
* The complexity of the model and the computational cost of the upward-downward Gibbs sampler may limit its applicability to large-scale datasets.
* The need for careful tuning of the hyperparameters may make it difficult to apply the model in practice.
* The lack of a clear understanding of the relationships between the hyperparameters and the performance of the model may limit the interpretability of the results.
Overall, I recommend accepting the paper, as it proposes a novel and interesting model that addresses an important problem in machine learning, and demonstrates its advantages in various text analysis tasks. However, the authors should be encouraged to address the weaknesses of the paper, such as the complexity of the model and the need for careful tuning of the hyperparameters, to make the model more applicable and interpretable in practice. 
Quality: 8/10
The paper is technically sound, and the proposed model is well-motivated and clearly explained. However, the complexity of the model and the computational cost of the upward-downward Gibbs sampler may limit its applicability to large-scale datasets.
Clarity: 9/10
The paper is well-written, and the authors provide a clear explanation of the proposed model, its properties, and the experimental results.
Originality: 9/10
The paper proposes a novel deep network model that can handle high-dimensional count vectors and model overdispersed counts, and introduces a layer-wise training strategy to automatically infer the network structure.
Significance: 8.5/10
The paper demonstrates the advantages of the PGBN in various text analysis tasks, and shows that it can outperform existing models, including the gamma-negative binomial process Poisson factor analysis. However, the complexity of the model and the need for careful tuning of the hyperparameters may limit its applicability to large-scale datasets.