This paper presents a novel recurrent CNN architecture for multi-frame super resolution, incorporating three types of convolutional filters: feed-forward, recurrent, and conditional convolutions. While the overall architecture is innovative, it appears to be a relatively straightforward integration of existing deep learning components.
The technical quality of the paper is marginal, as it combines established deep learning modules in a straightforward manner. However, the novelty of the approach lies in its unprecedented application to multi-frame super resolution.
To strengthen their submission, the authors should address the following questions in their rebuttal: (i) What is the total number of parameters in the proposed model? (ii) Was a pre-trained SR-CNN used for comparison, or was SR-CNN re-trained on the new dataset? (iii) Why does SR-CNN exhibit slower runtime compared to BRCN? (iv) What do the images generated by SR-CNN look like in Figures 3 and 5?
The originality of the paper is incremental, with the authors demonstrating the contribution of individual architectural components in Table 2. Although the BRCN {v,r} variant outperforms existing methods, it is well-established that increasing architectural complexity and parameter count can improve accuracy, as evident from the incremental performance gains of BRCN {v,t}, {v,r,t}, and {v,r,t,b} in Table 2.
The significance of this paper for the NIPS audience is debatable. While the introduced architecture is novel, it represents an incremental advancement in the field of deep learning. The paper proposes a new architecture for multi-frame super resolution, building upon previous work on single-frame CNNs for super resolution [6]. Although some experimental details are missing, the paper is generally well-structured and thorough. Ultimately, its suitability for the NIPS audience is open to discussion.