This manuscript presents a novel approach that integrates trajectory optimization with policy learning, enabling interactive control of complex characters. The core contribution lies in the joint learning of trajectories and policies, with the added empirical observation that decomposing the optimization problem into alternating subproblems allows for the reuse of established methods and yields satisfactory results in practice. Although the paper is well-structured and easy to follow, I lack the requisite expertise in this domain to fully assess its potential impact, which, at first glance, appears to be a technically impressive achievement, albeit without groundbreaking theoretical contributions.
The authors demonstrate a high level of technical expertise, from conceptualization to implementation, which is matched by the impressive results they obtained. The claimed benefits of incorporating sensory noise during training and learning trajectories jointly with the policy are convincingly demonstrated. The manuscript is well-written, with an appropriate level of detail considering the complexity of the problem and its implementation. Each section focuses on a distinct aspect of the problem, and the cross-references between sections are particularly useful.
However, I found the motivations behind the additional optimization step in Section 7 to be somewhat unclear. Given the preceding sections, I had expected the policy to function without the need for extra optimization. I presume this is due to the residual trajectory cost or policy regression error, which would accumulate errors when feeding the policy's actions into the physical simulator. While I understand that re-optimizing at every timestep allows for larger timesteps, I would have appreciated a more detailed explanation.
On line 153, the authors mention that their approach is similar to recent methods; providing one or two references would be helpful. I initially found it counterintuitive that keeping updates small in parameter space would be beneficial, but I eventually understood its utility in the context of alternating between subproblems.
There is a typo on line 89, where "to unify" is mentioned. The paper's results appear impressive, and the proposed optimization scheme and implementation seem sensible. Nevertheless, as this research area is not my primary expertise, I am hesitant to comment on the novelty and significance of the work.