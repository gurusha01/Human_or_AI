I found the methodology and experiments presented in this paper to be generally persuasive, with only a few minor suggestions for improvement.
In applications where large datasets are involved and predictive inference is a primary focus, it might be more practical to aggregate specific parameter functions that are relevant to prediction, rather than aggregating the entire set of parameters.
For instance, in the context of probit regression, aggregating the fitted probabilities could be a viable approach, and similarly, for the mixture of Gaussians example, aggregating cluster membership probabilities could be explored.
I am curious to know whether the results would be more or less sensitive to the aggregation method in such scenarios, and if the authors have conducted any experiments to investigate this.
Another point that caught my attention was the structured aggregation approach for positive semidefinite matrices, where various reparametrizations, such as Cholesky factorization, could be applied. I wondered why the aggregation is limited to the D(Î›_k) matrices, and if there is a specific reason for this choice.
On a minor note, I noticed that equation (3) is missing an integral.
This paper builds upon recently proposed consensus Monte Carlo algorithms by incorporating variational Bayes methods into the aggregation step, offering a refinement of these existing methods. The proposed relaxation of the variational objective function is ingenious, and the experiments convincingly demonstrate that the suggested approach yields improvements over simpler alternatives.