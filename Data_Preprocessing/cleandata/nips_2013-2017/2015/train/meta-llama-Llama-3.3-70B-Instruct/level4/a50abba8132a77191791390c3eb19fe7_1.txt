This manuscript presents an extension of the stochastic optimization algorithm SVRG, introducing several key modifications. These include an analysis of SVRG's convergence under corrupted full gradients, a hybrid iteration approach combining SGD and SVRG, the incorporation of mini-batch strategies, and the utilization of support vectors. The author provides rigorous proofs for each modification, demonstrating linear convergence under assumptions of smoothness and strong convexity. Nevertheless, the paper's originality is somewhat limited, with the convergence rate improvements being modest and the proof framework closely mirroring that of the original SVRG algorithm. A significant challenge, namely supporting non-strongly convex loss functions, remains unaddressed. Additionally, minor errors in notation are present in the appendix. Overall, while the manuscript offers practical enhancements to SVRG, substantiated by both theoretical and experimental results, the incremental advancements in novelty and performance are not substantial, and core issues persist unresolved.