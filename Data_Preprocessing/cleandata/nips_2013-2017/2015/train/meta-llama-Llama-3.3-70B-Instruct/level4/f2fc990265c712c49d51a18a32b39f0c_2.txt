This manuscript explores the efficient implementation of non-convex sparse learning formulations, which have been demonstrated to outperform their convex counterparts in both theoretical and practical aspects. Nevertheless, solving non-convex sparse optimization problems efficiently for large-scale data remains a significant challenge. The authors introduce a novel algorithm, HONOR, designed to address a broad spectrum of non-convex sparse learning formulations.
A crucial aspect of HONOR is its incorporation of second-order information to substantially accelerate convergence. Unlike most existing second-order methods, HONOR avoids the need to solve regularized quadratic programming and only requires matrix-vector multiplications, without explicitly computing the inverse Hessian matrix. This approach enables HONOR to maintain a low computational complexity at each iteration, making it scalable for large-size problems.
Establishing convergence for non-convex problems is inherently challenging. A major contribution of this paper is the rigorous convergence analysis of HONOR, which guarantees convergence even for non-convex problems. The hybrid optimization scheme, choosing between a Quasi-Newton step and a Gradient Descent step per iteration, is pivotal to this analysis. While the presented analysis is complex, providing a high-level overview of the intuition behind the proposed hybrid scheme would be beneficial.
The empirical evaluation in the paper is compelling, with the authors testing the proposed algorithm on large-scale datasets containing up to millions of samples and features, including datasets with over 20 million features. The results show that HONOR achieves significantly faster convergence than state-of-the-art algorithms. Guidance on selecting the parameter epsilon would be useful for practitioners.
The potential of HONOR to escape high error plateaus, commonly encountered in high-dimensional non-convex problems, is noted. Investigating the theoretical properties of the solutions obtained by HONOR could yield interesting insights.
Given the empirical fast convergence of the proposed algorithm, it would be valuable to establish a guarantee on the local convergence rate of HONOR. Furthermore, exploring the possibility of extending the algorithm to other sparsity-inducing penalties, such as group Lasso and fused Lasso, could broaden its applicability. Overall, the paper presents a significant advancement in the efficient implementation of non-convex sparse learning formulations, with HONOR demonstrating potential as a versatile and efficient algorithm for solving large-scale non-convex sparse optimization problems.