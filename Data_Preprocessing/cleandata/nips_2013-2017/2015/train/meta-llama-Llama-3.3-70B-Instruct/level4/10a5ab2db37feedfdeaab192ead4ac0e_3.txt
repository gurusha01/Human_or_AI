The authors have submitted a compelling paper that explores a backpropagation training approach utilizing spike probabilities, with a focus on the hardware limitations of a specific platform featuring spiking neurons and discrete synapses. This topic is particularly relevant in the context of mapping deep networks to multi-neuron hardware platforms, an area of ongoing interest.
Quality: The proposed training method demonstrates significant utility, particularly in the context of emerging multi-neuron hardware platforms with inherent constraints.
Clarity: The paper is generally well-written and easy to follow. However, the claims presented at the conclusion of Section 1 could be rephrased to avoid suggesting that the paper introduces a novel training methodology employing spiking neurons and reduced-precision synapses for the first time. The demonstration of the network's operation on a specific hardware platform, in this case, TN, is more accurately viewed as a validation of the training method rather than a novel feature. 
In Section 2, the meaning of 0.15 bits per synapse requires clarification. A concise description of the network topology is lacking. The increase in training time resulting from the probabilistic synaptic connection update is not specified. Furthermore, the relatively low performance of a single ensemble of the 30-core network, especially when compared to the 5-core network, warrants explanation. The consistency of results across different ensembles of the 5-core network also requires discussion. Additionally, the spike rates for the inputs during testing should be provided, along with clarification on whether the input is a spike train. Reference 12 on line 399 mentions constraints (such as bias=0) during training, indicating that the approach is not solely train-then-constrain (a typo is noted as "approach approach").
Originality: A discussion or comparison with other spiking backprop-type rules, such as SpikeProp, would be beneficial to contextualize the authors' contribution.
Significance: The development of novel training methods that account for the constraints of hardware platforms is of considerable interest. However, the constraints considered in this paper are specific to the TN architecture and may not be universally applicable to other platforms. If the results are based on a 64-ensemble setup, this implies a significant dedication of hardware resources to achieve the desired network accuracy. Overall, the paper presents an interesting exploration of a training method using backpropagation that considers the hardware constraints of a platform with spiking neurons and discrete synapses.