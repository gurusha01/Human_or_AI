The authors have introduced a novel dimensionality reduction technique that identifies the most distinct direction between input datasets X and Y. A key innovation of this approach is the utilization of the squared Wasserstein distance as a discrepancy measure, which can be efficiently solved using semidefinite programming. Experimental results demonstrate the proposed method's superiority over existing techniques.
Quality: The technical aspects of the method are sound.
Clarity: The paper is well-organized and easy to comprehend.
Originality: The proposed approach is innovative, although it shares similarities with Transfer Component Analysis, which aims to find a subspace with minimal discrepancy between two datasets.
Significance: The formulation combining Wasserstein distance with semidefinite programming is noteworthy and is likely to have an impact on the machine learning community.
Detailed comments: 
1. The problem could be alternatively formulated using a simple Lasso approach, such as assigning positive pseudo-labels to X and negative pseudo-labels to Y, and then solving the optimization problem ||Y - Z^t \beta||2^2 + \lambda ||\beta||1. This method may yield similar results, particularly when X and Y are linearly related, making it a viable baseline for comparison.
2. It would be interesting to explore the possibility of extending the algorithm to handle nonlinear cases.
3. Transfer Component Analysis could be applied to find the most distinct direction, as it can be adapted for this purpose despite originally being designed for finding common subspaces (http://www.cse.ust.hk/~qyang/Docs/2009/TCA.pdf).
The proposed formulation is intriguing, and incorporating a simple Lasso-based baseline would enhance the study.