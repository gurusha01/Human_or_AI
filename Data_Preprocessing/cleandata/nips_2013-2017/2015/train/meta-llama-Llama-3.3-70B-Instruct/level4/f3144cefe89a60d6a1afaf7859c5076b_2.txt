I acknowledge the authors' response to my initial review. 
Overall, my stance on the paper remains unchanged, and I look forward to further discussion on how this work relates to traditional neural and deep networks.
The authors propose a deep belief network with nonnegative weights in the intermediate hidden layers, which they apply to text documents as a "deep belief topic model". They derive an inference and update step, where Dirichlet vectors are propagated up the network and gamma weights are propagated down, and provide an empirical evaluation of the model.
The paper is generally well-written, but I believe the empirical analysis can be enhanced. I would like to see a more comprehensive background discussion, even if it's not directly related to the model's motivation, as it's unlikely that neural networks with non-standard weight ranges have not been explored before.
I have several minor comments: 
- The use of a fixed budget to determine the size of the layers is unclear, and I am skeptical about its effectiveness. It might be beneficial to use a held-out subset of data to decide whether to adjust the model size.
- The addition of layers beyond the second layer may not provide significant improvements, as the performance gains in the figures are marginal and difficult to discern due to high variance. Furthermore, the topic descriptions from sample words do not substantially differ, which may indicate that the observed specificity of topics at the top layer is due to its larger size.
- The authors' joint training process resembles traditional neural network training methods, such as forward and backward propagation, but with sampling.
- The introduction of the Gamma-Poisson belief network could be presented without relying on a complicated application, as the neural network architecture itself is the primary focus.
Regarding experimental validation, I think it could be improved. More comparisons with baselines, such as SVM or ridge regression using word counts as covariates, would be beneficial. Additionally, a comparison with a typical neural network using weights in [0,1] and adapted to suit the Poisson/Multinomial observations would be insightful. Claims about the gamma-distributed nonnegative hidden units should be supported by comparisons with models using binary hidden units.
It is unclear whether removing stopwords is necessary, as topic models like LDA can handle them without issues.
Minor issues include: 
- The discussion of classification in the experiment section precedes the introduction of the task, which is confusing.
- A typo in line 56, where "budge" should be "budget".
This paper presents an interesting variant of a typical deep network, with an intriguing idea and mostly clear presentation. However, the experimental validation could be improved to strengthen the paper's contributions.