The authors should clearly articulate the advancements their new method brings to the existing body of work in stochastic optimization, such as that of Hu, Kowk, and Pan (2009), and mini-batch optimization approaches like those of Konecny et al (2013, 2014) and Smola's contributions (e.g., Zinkevich et al, 2010), including the incorporation of these methods as baselines in their experimental comparisons. 
Simply comparing their method to its own variants is not sufficient for a comprehensive evaluation. Given the crowded nature of this field, it is imperative for the authors to explicitly outline their novel contributions and differentiate their work from existing literature.
The paper is generally well-written, with the comparative bounds table in Section 8 being a notable highlight. The discussion on the stochastic variance-reduced gradient method for optimizing sums of strongly convex and Lipschitz continuous functions is also of interest, as it explores the trade-off between computational precision and speed. However, it remains unclear whether this method offers a substantial enough improvement over current state-of-the-art stochastic optimization techniques to warrant publication in a prestigious conference like NIPS.