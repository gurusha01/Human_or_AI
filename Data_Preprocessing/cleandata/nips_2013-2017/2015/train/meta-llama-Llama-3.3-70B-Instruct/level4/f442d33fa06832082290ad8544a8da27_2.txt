The authors present a novel encoder-decoder framework for learning sentence embeddings, leveraging the distributional semantics of sentences in natural discourse. This approach, analogous to CBOW, aims to capture the semantic properties of sentences by training a model to predict neighboring sentences. The proposed method is attractive, as it can be trained unsupervised, utilizing the data itself to constrain the model's weights.
The authors also explore a specific implementation of this concept, employing gated RNNs for encoding and decoding functions. This model can be viewed as "translating" a sentence into its preceding and succeeding sentences, similar to machine translation models. The evaluation of the model across eight tasks demonstrates that the learned embeddings (encoder's hidden states) are semantically meaningful and effective for training classifiers, often outperforming existing systems.
Overall, the paper is well-written, but some clarifications are needed:
* In the experimental results section, it is unclear whether the reported numbers for certain systems are taken from the literature or re-computed from scratch. For instance, in Table 3, the numbers from the challenge are clearly from the literature, whereas in Table 6, it is ambiguous whether the paragraph vector was re-trained on the same data as the skip-vector model or if the reported number is from the literature.
* A potential limitation of the gated RNN implementation of skip-thoughts is its difficulty in training, requiring over a week. In comparison, paragraph vectors, although requiring inference to embed new sentences, may not be significantly more expensive than the gated RNN's token-by-token processing.
* Consider expanding Table 4 with additional sentence pairs to illustrate cases where skip-thoughts outperform the baseline. This could involve searching for sentences with low lexical similarity but high semantic similarity, both predicted and true. The paper presents a well-written, novel contribution with thorough experimental evaluation against numerous baselines across eight tasks.