The authors present a new framework for calculating a lower bound of the cross-validation errors in relation to the regularization parameter. However, the primary question driving the manuscript, which concerns the difficulty of determining the required number of grid-points for cross-validation, has already been thoroughly explored from both theoretical and empirical standpoints, as seen in works such as Rifkin et al (MIT-CSAIL-TR-2007-025) and Rasmussen and Williams (Gaussian Processes for Machine Learning, Chapter 5).
The proposed scoring and validation error bounds are technically robust, representing a direct application of concepts from safe screening, specifically the bounding of Lagrangian multipliers at the optimal solution. Although this is clearly mentioned in the supplemental material, adding a similar note in the main manuscript, such as in Section 3, would enhance readability for the audience.
The methodology for evaluating the theoretical approximation of the regularization parameter is not clearly outlined. Specifically, the details of the grid search procedure are unclear: is it based on 10-fold cross-validation on the training set to find optimal parameters, followed by a final evaluation on the validation set? If so, it would be expected that the training set's E_v should be reported in Figure 3 instead of the validation set's.
While the proposed score and validation error bounds are technically sound, the novelty of the work is incremental, and the experimental setup and results are underwhelming.