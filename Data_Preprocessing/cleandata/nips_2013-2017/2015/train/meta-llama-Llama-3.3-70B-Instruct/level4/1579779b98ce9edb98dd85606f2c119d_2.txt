Summary: The authors propose a novel approach to jointly train all parameters of decision trees, including decision splits and posterior probabilities, by formulating the task as a single optimization problem. This is a significant improvement over current methods, which train trees in a greedy manner, layer by layer. The optimization problem is upper-bounded and approximated to obtain a tractable formulation.
Quality: While the paper is well-written, there are potential flaws in the reasoning that require further clarification. The clarity of the derivation is commendable, but the presentation of experiments could be improved. The originality of investigating joint training for decision trees is noteworthy, and the proposed solution is valuable, although it may have some shortcomings.
Originality and Significance: The topic of joint training for decision trees is important, and the authors' approach is a significant contribution to the field. However, the solution may have some limitations that need to be addressed.
Comments: 
- The derivation of the empirical loss bound appears to be flawed, particularly in the argument presented in lines 216f. A counterexample can be constructed to demonstrate that the right-hand side of Equation (7) may not always be an upper bound. For instance, consider the case where W\bx = [5;-5], resulting in \hat \bh = [1;-1] = \sgn(W\bx) and \hat \bh^T W\bx = 10. Choosing g = [1;1] yields g^T W\bx = 0, which could potentially lead to a smaller value of the right-hand side of Equation (7). The proof of Proposition 1 in the supplementary material does not cover this case and is therefore incomplete.
- The assignment of data points to tree leaves seems suboptimal, with some leaves not being assigned any data points. This could indicate a problem with the objective or initialization. The suggested heuristic of fixed assignment of data points to tree leaves may not be the best approach and could be improved using techniques such as expectation maximization or concave-convex procedures.
- The importance of the efficient loss-augmented inference is unclear, particularly given the relatively small depth of the tree (at most 16). The impact of the restricted search space introduced by the Hamming ball on performance is not well understood and requires further evaluation. The result presented in the supplementary material may not be generalizable, and the meaning of "leaves" is unclear.
- A baseline that trains non-axis aligned split functions of trees in a standard greedy manner is missing. This would provide a more comprehensive comparison of the proposed approach.
- The depth of the tree needs to be specified ahead of time, which is a significant limitation. The authors should discuss this limitation more carefully and provide a more thorough experimental evaluation of the depth parameter.
The authors are thanked for their careful explanation of the reasoning behind the loss bound (Equation 7). However, the use of \arg\max could be replaced with \max for clarity. The score has been adjusted accordingly. An experiment comparing the runtime of the proposed approach with other methods would strengthen the paper, and a more careful evaluation of the method would be desirable for the reader. The submission considers an interesting problem, but the experimental evaluation is limited, and there may be a flaw in the derivation. A careful explanation in the rebuttal would be appreciated, and the score may be adjusted accordingly.