A modified EM approach is presented, which adapts the traditional EM method by truncating the parameter vector at each iteration to achieve a specified level of sparsity. Additionally, a statistically normal test statistic is introduced, leveraging a decorrelated score to determine whether a particular parameter vector entry is non-zero.
The sparse EM method is guaranteed to converge geometrically to the true parameter value with each iteration, subject to a statistical error term that scales with the square root of the sparsity level and the infinity norm of the M-step error. This error term typically exhibits logarithmic dependence on the ambient dimension, building upon recent findings for EM in low-dimensional settings.
The analysis relies on assumptions regarding the smoothness and quasi-quadratic nature of the Q function within a certain basin of attraction, as well as initialization within this basin. A key condition introduced here is the control of the infinity norm of the error in estimating M, which is specifically required for sparse vectors. However, for various models, including the Gaussian mixture example and a non-gradient version of the mixture of regression model, estimating M for all possible vectors is comparable to estimating it for sparse vectors.
This raises the question of whether the truncated EM algorithm is indispensable. It is conceivable that similar guarantees could be obtained by performing standard EM under the same conditions, followed by a single truncation step. Nevertheless, the authors' observation that linear regression represents a special case provides a compelling argument against relying on a single truncation step.
Potential connections to sparse optimization literature, such as "Sparse online learning via truncated gradient" by Langford et al (2009), may warrant discussion. The experimental section should specify the initialization method employed. The proposed sparse EM variant offers a significant contribution to parameter estimation in high-dimensional settings, with the convergence guarantees provided being a notable result. However, the necessity of the sparse EM variant in achieving these results remains uncertain.