This paper presents an intriguing result with potential utility. However, it remains uncertain whether the proposed bound would hold if the method were extended to incorporate kernelization, with kernel parameters optimized via cross-validation, given the interdependence of kernel and regularization parameters. 
The empirical assessment of the method is somewhat limited, relying on a small number of datasets, which is particularly concerning given the discussion on computational efficiency. The modest size of the datasets raises questions about the practical applicability of the bound. 
Furthermore, it is unclear whether the bound offers a significant advantage in model selection over directly optimizing cross-validation error within a given computational constraint. Nonetheless, the method might prove valuable in preventing over-tuning of the regularization parameter, thereby avoiding over-fitting of the cross-validation estimate. 
The paper introduces a bound on cross-validation error for linear models along with a procedure to find a value that guarantees a certain level of CV error. While the paper is well-written and interesting, its experimental evaluation could be strengthened.