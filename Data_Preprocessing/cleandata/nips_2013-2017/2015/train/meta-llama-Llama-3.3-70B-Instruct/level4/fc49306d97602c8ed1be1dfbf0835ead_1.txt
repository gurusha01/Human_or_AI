I have revised my evaluation from a score of 6 to 7, as it better reflects my overall assessment of this paper.
== Summary ==
This paper presents a kernel-based approach for cross-domain matching of bag-of-words data, addressing the challenge of comparing data represented by features across different domains. The proposed method views data, such as documents and images, as a probability distribution over vectors in a shared latent space, enabling the capture of semantically similar features in distinct instances. Unlike existing algorithms like CCA, KCCA, and topic modeling, this approach estimates latent vectors by maximizing the posterior probability expressed in terms of the RKHS distance between instances. The experimental results on several real-world datasets, including bilingual documents, document-tag matching, and image-tag matching, are highly encouraging.
In essence, the paper proposes a straightforward and intuitive concept for cross-domain matching with robust empirical results.
== Quality ==
The primary limitation of this paper lies in its technical aspects.
== Clarity ==
The paper is well-written, with only minor suggestions for improvement.
In the experiment section, the term "development data" is used; it would be beneficial to clarify whether this refers to the validation data [Line 185-189]. Additionally, the paragraph discussing Xi and Yj appears to suggest that they inhabit different latent spaces, which requires clarification. The motivation behind the prior considered in Eq. (9) should also be explained. In Eq. (11), it would be helpful to mention that the chosen prior corresponds to L_2 regularization on x and y, aligning with the formulation in Yuya et al. (NIPS2014). Clarification on why this choice is suitable would be appreciated.
== Originality ==
Although the proposed idea bears similarities to Yuya et al. (NIPS2014), it is applied to the cross-domain matching problem rather than classification. The concept of treating data as probability distributions over latent space is intriguing and has potential applications in various areas.
== Significance ==
The idea presented in this work has broader applicability, such as in "representation learning" where the latent space may have a hierarchical structure (e.g., tree) or represent the parameters of a generative model (e.g., RBM or deep neural network).
Relevant papers include:
Y. Li, K. Swersky, R. Zemel. Generative Moment Matching Networks. ICML 2015.
G.K. Dziugaite, D. Roy, Z. Ghahramani. Training generative neural networks via Maximum Mean Discrepancy optimization. UAI 2015.
Minor comments:
Equations (15) and (16) seem redundant, as they are essentially the same as Equations (12) and (14), and could be removed to conserve space. Overall, the paper presents a simple and intuitive approach for cross-domain matching of bag-of-words data with strong empirical results.