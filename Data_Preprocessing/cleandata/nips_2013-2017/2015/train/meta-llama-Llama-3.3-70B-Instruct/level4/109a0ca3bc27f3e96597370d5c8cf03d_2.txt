This paper presents a novel approach to learning out-of-plane rotations for 3D objects, such as human faces and chair models, using a recurrent convolutional encoder-decoder network. The network architecture begins with a basic model that disentangles the input image into separate units representing identity and pose, which are then manipulated by action units to control rotation direction. The decoder network, comprising convolutional and upsampling layers, combines the identity and pose units to generate an image of the rotated object along with its corresponding mask. To accommodate longer rotation trajectories, the network is extended to incorporate a recurrent architecture, where the encoded identity unit remains fixed while the pose unit is sequentially updated by action units, ultimately producing the resulting image.
A key contribution of this work lies in its ability to disentangle representations of identity/appearance and pose factors, yielding view-invariant features that demonstrate discriminative power in cross-view object recognition tasks. Furthermore, this disentangling capability enhances predictive rendering performance, particularly when utilizing longer rotation trajectories during curriculum training stages for the proposed network.
The paper is well-structured and clearly motivated, with qualitative results for predicted renderings of rotated images and quantitative evaluations on cross-view object recognition tasks providing strong support for the methodology, especially the disentangled representations of pose and identity factors.
However, a few minor limitations are noted, which the authors may address in their rebuttal:
- The current implementation only supports discrete rotation angles, limited by the range of angles present in the training data. It would be beneficial to explore potential extensions to enable support for continuous rotation angles.
- The proposed recurrent convolutional encoder-decoder network is trained with fixed-length sequences, which contrasts with the typical variable-length sequences handled by general recurrent neural networks.
Overall, this paper introduces a novel recurrent convolutional encoder-decoder network trained end-to-end for the task of rendering rotated objects from a single image, with a primary contribution being the generative disentangling of identity and pose factors. The effectiveness of this approach is well-demonstrated through qualitative and quantitative evaluations.