The authors present a novel approach to reconciling the requirement for high precision in continuous variables within backpropagation algorithms with the goal of utilizing spike-based neural network devices equipped with binary synapses. By treating analog variables as probabilities that are sampled, they facilitate the mapping of a neural network trained offline to a spiking network chip deployed for application execution. The adaptation of the backpropagation algorithm to align with this strategy is demonstrated, and its efficacy is validated using the MNIST benchmark. The trained network is successfully mapped onto two distinct architectures, with optimizations for both size and accuracy, and performance metrics obtained from the chip are presented. The manuscript exhibits high quality and clarity, with the work being sufficiently original and potentially beneficial for other approaches and hardware platforms. However, a comprehensive review of existing hardware platforms should also include references to Qiao et al. (Frontiers in Neuroscience, 2015) and the paper available at http://arxiv.org/abs/1506.05427. 
A notable omission in the paper is a clear emphasis on the significance of the work, specifically how the proposed method enhances the deployment of the TrueNorth chip in practical applications and the unique advantages it offers over alternative methods, such as those referenced in [9] to [12]. The authors effectively demonstrate the application of backpropagation to spiking neural networks with binary synapses, validating their method by successfully deploying the trained network on the TrueNorth device. The paper's structure and clarity are commendable, with convincing results. Nevertheless, certain details are lacking, including the conversion of inputs into spikes, the frequencies utilized, shared weight values, neuron models, and other pertinent information that would enhance the comprehensiveness of the presentation.