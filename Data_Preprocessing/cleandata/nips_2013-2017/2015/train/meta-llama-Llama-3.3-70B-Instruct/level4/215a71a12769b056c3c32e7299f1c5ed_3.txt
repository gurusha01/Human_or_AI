This paper presents a novel approach to training exceptionally deep networks by utilizing Long Short-Term Memory (LSTM) units.
The paper is of high quality, well-structured, and clearly written. The concept introduced is innovative, and the findings are noteworthy.
A common challenge in machine learning research is the limitation imposed by initialization and vanishing/exploding gradients when attempting to add multiple layers to neural networks. Historically, this issue has hindered the widespread adoption of deep neural networks for approximately 30 years. This study directly addresses this problem and successfully demonstrates a method for training neural networks with up to 100 layers.
Notably, the experiments illustrating information routing and layer importance are particularly intriguing.
Specific suggestions and observations:
In Section 4.1, it appears that the statement 'most transform blocks are active on average' may be more accurately rephrased as 'most transform blocks are not active on average.'
An alternative approach to initializing very deep networks could involve using the identity matrix (with optional added noise) for each layer. It would be beneficial to discuss how the transform gates in this paper differentiate from this straightforward method, particularly in relation to highway networks. Overall, this is an excellent paper that effectively demonstrates a viable method for training deep neural networks and is engaging to read.