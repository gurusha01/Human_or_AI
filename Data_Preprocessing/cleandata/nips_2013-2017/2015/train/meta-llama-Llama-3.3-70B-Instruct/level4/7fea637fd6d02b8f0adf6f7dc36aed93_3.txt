This paper presents a novel approach to regret minimization in multiplayer normal-form games, building upon existing algorithms such as Optimistic Mirror Descent and Optimistic Follow the Regularized Leader, which were previously limited to two-player zero-sum games. The authors successfully generalize these algorithms to multiplayer general-sum games, yielding impressive experimental results compared to Hedge. The content of the paper is likely to resonate with the NIPS community.
I have several specific questions and comments regarding the paper:
- On page 3, the removal of the root from the log in the claim "converges to O(n log(d) sqrt(T))" appears inconsistent with the presence of the root in the log of r_i(T). Is this a typographical error?
- The statement on page 6, "This is the first fast convergence result to CCE using natural, decoupled no-regret dynamics," raises two concerns: firstly, the terms "natural" and "decoupled" are not clearly defined, and it is unclear how the dynamics can be considered "decoupled" given the requirements of Corollary 12, which mandates that all players use OFTRL with specific choices of M_i^t and \eta. Secondly, this claim may be overly broad, as it does not account for previous work such as Hart & Mas-Colell's regret-matching. It would be beneficial to clarify, rephrase, or remove this claim.
Some potential avenues for further exploration include:
- Investigating whether these results can be extended to the partial information setting using sampling, such as Optimistic Exp3, in a straightforward manner.
Several minor points to consider for the camera-ready version, if accepted, include:
- On page 1, the phrase ".. a chink that hints" contains the unclear term "chink."
- On page 3, the space after "PoA" in "(PoA )" should be removed.
- At the end of the proof of Theorem 4 on page 4, it appears that a max{j \in N} may be missing in the right-most inequality, or alternatively, the j subscript could be removed after removing the \sum{j \neq i}.
- The argmax notation should be adjusted so that the set being maximized is entirely under the argmax.
- Several items in the bibliography require correction, including the addition of volume, number, and pages for [4], and the expansion of conference names for [19] and [20]. Overall, this is a well-written paper that presents significant new results in the challenging area of no-regret learning in multiplayer general-sum games, generalizing previous results and demonstrating faster convergence when players use the same algorithm with the RVU property, while also showing that the worst-case O(log(T) sqrt(T)) is still attainable in the adversarial case using a parameterized step-size and the doubling trick.