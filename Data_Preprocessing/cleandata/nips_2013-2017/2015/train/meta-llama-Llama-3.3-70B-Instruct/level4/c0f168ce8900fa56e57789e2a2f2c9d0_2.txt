This paper provides a finite-time examination of the newly introduced Projected LMC algorithm, with its optimization counterpart being stochastic gradient descent (SGD). The primary theoretical finding indicates that, given an appropriately selected step-size and a sufficiently large number of iterations, the sample distribution generated by Projected LMC closely approximates the target log-concave density distribution.
A key contribution of this work is demonstrating that the step-size for Projected LMC is analogous to that of SGD, and the maximum number of iterations for Projected LMC scales polynomially with the dimension of the space, up to a logarithmic factor. Notably, the proposed algorithm operates by evaluating the gradient of the density rather than the density itself, which is particularly advantageous when the normalization constant is intractable.
The paper is well-organized and clearly written, presenting a novel theoretical analysis of the Projected LMC algorithm for sampling from log-concave density distributions. This analysis is based on the evaluation of the first-order oracle, marking a significant development in the field. Specifically, this research introduces the first finite-time analysis of a Monte Carlo algorithm designed to sample from log-concave distributions, requiring only the values of the log-density gradient.