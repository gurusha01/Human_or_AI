This paper explores high-dimensional estimation problems characterized by sub-exponential design and noise, deriving estimation error bounds through the exponential width argument. The authors demonstrate that the estimation error is at most \sqrt{\log p} times worse than in the sub-Gaussian case.
A key finding is the relationship between Gaussian and exponential widths, where the exponential width is shown to be upper bounded by the Gaussian width by a factor of c \sqrt{\log p}.
Another significant result establishes that for sub-exponential design, achieving the restricted eigenvalue condition requires the same sample complexity as in the sub-Gaussian case. The authors outline two proof techniques, revealing that the bound derived from the exponential widths argument is \log p worse than that obtained using VC dimension.
To validate their theoretical sample complexity, the authors conducted simulations. However, the presentation of Figure 1 could be improved; instead of the current format, plotting the normalized sample size (n/s \log^2 p) against the probability of success would more effectively demonstrate whether the sample complexity is O(s \log^2 p) rather than O(s \log p), by showing if curves for different dimensions converge.
In summary, this paper makes substantial contributions to the field of high-dimensional estimation under exponential design and noise, presenting novel and interesting proof techniques. Its publication in NIPS would be warranted, as it provides valuable insights into high-dimensional estimation error bounds with sub-exponential design and noise, along with interesting and useful analyses.