The paper proposes a modified Bayesian Global Optimization approach that utilizes a space partitioning algorithm to generate future samples, leveraging the Upper Confidence Bound (UCB) to circumvent the internal global optimization problem of selecting the next sample. This method is theoretically analyzed, demonstrating exponential regret and thus surpassing previous efforts. The algorithm is well-described, accompanied by an illustrative example, and contributes novel insights to the field. However, a notable concern arises from the experimental results, which appear to be limited to one-dimensional test functions, raising questions about the scalability of the interval partitioning method to higher dimensions due to potential exponential growth in "resolution" requirements. This concern is exacerbated by the similarity to the DIRECT algorithm, known for its poor performance in high-dimensional problems. Although this does not detract from the theoretical contribution, it is a crucial consideration for practical applications. Additionally, minor clarifications are needed, such as the rationale behind choosing UCB over other methods like expected improvement, and further explanation of certain statements, including the simultaneous conduct of global and local searches and the absence of function evaluation at a specific point. Overall, the paper presents a sound and innovative approach but would benefit from addressing these concerns to enhance its practical viability.