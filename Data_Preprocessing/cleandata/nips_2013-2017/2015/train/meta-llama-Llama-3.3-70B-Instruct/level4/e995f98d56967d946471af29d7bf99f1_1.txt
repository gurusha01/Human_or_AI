Recent advances in neural machine translation and text generation have focused on training models to minimize perplexity or negative log-likelihood of observed sequences. However, this approach overlooks the fact that in practice, models condition on generated symbols rather than gold symbols, potentially leading to significantly different contexts than those encountered in the gold data.
This paper addresses this limitation by incorporating generated sequences into the training process, using the generated context instead of the gold context. To mitigate the issue of poor performance at early stages, the authors propose a "scheduled sampling" approach, which alternates between two training methods based on a predefined decay schedule inspired by curriculum learning.
The paper's strength lies in its simplicity and thorough empirical evaluation. The model and inference assumptions are clearly defined, and the internal architecture details are appropriately omitted, making it straightforward to reimplement this approach with LSTMs or other non-Markov models.
The empirical results demonstrate the method's effectiveness, with significant gains across various tasks, and the scheduling component appears to have a substantial impact. Although the parsing results lag behind state-of-the-art, the input representation used is highly reduced, and the speech results show a considerable improvement despite a unique setup.
One aspect that warrants further investigation is how performance varies based on the footnote 1, as token-level and example-level flipping may have distinct effects due to differences in worst-case distance between gold tokens.
A major weakness of the paper is the lack of comparison to other methods that aim to achieve similar goals. For instance, the authors dismiss early-update perceptron with beam search, citing the inability to factor the state sequence. However, this assumption is not inherent to beam search, and its connection to dynamic programming is also noted on line 130. Moreover, a recent paper at ACL demonstrates the applicability of this method to neural network transition-based parsing.
Additionally, the paper fails to adequately distinguish itself from SEARN and reinforcement learning-type algorithms, which are characterized as "batch" approaches. While the original SEARN paper used a batch multi-class classifier, this does not preclude its application in the SGD case. The key idea of SEARN is to interpolate the current prediction with gold-based predictions to produce a sampled trajectory, with the main difference being the learning of the policy versus the use of a schedule.
In conclusion, this paper presents a simple method that yields improvements across several sequence modeling tasks. However, the lack of rigorous baseline comparisons and the dismissal of other methods as inapplicable are concerns that need to be addressed.