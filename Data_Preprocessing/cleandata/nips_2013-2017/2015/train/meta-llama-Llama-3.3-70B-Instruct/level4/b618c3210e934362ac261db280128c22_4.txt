This manuscript proposes a variational inference framework for training Recurrent Neural Networks (RNNs) with stochastic transitions between hidden states, effectively modeling a stochastic nonlinear dynamical system akin to a nonlinear state-space model. Notably, the process noise between hidden states is state-dependent, rather than identically distributed. The parameters of the generative model are learned in tandem with a recognition model through the maximization of the evidence lower bound.
Overall, this is a well-structured paper that successfully connects the dots between RNNs and nonlinear state-space models. The authors prioritize conveying the overarching concepts, resulting in a clear and accessible presentation that eschews excessive technical detail. The model's simplicity and the standard nature of the variational training procedure contribute to the paper's clarity. However, the reported results likely necessitated significant engineering efforts, which are not immediately apparent from the text. Furthermore, details regarding computational resources and training times are noticeably absent.
The notation employing subindices "< t" occasionally obscures the Markovian structure of the model, which could be beneficial for interpreting joint distribution factorizations. A notable omission in the literature review is the lack of discussion on nonlinear state-space models, which are ubiquitous in fields such as engineering, robotics, neuroscience, and others. Explicitly highlighting the connections between the proposed model and nonlinear state-space models could broaden its potential impact beyond the machine learning community.
Several questions arise from the presentation:
1. Are the plots labeled VRNN actually referring to VRNN-GMM, given that VRNN-Gauss was unable to generate well-formed handwriting?
2. Was any regularization applied to the model, and if not, what measures were taken to prevent overfitting?
3. The discrepancy between Equation (1), which expresses ht as a function of xt, and Equation (2), which gives a distribution over xt as a function of ht, appears intentional - could the authors provide clarification on this aspect? 
This paper presents a compelling case for bridging the gap between RNNs and nonlinear state-space models, with convincing experiments and a polished presentation.