The authors propose an algorithm that utilizes the Kullback-Leibler (KL) divergence between the true posterior and the proposed posterior as its objective function. To estimate the KL divergence, the algorithm employs stochastic gradient Langevin dynamics for efficient posterior sampling, and stochastic gradient descent for optimization. 
The presentation is straightforward and easy to follow. The methodology is straightforward yet effective, and the accompanying examples demonstrate its advantages in a compelling manner.
This paper introduces a stochastic gradient-based algorithm for training a parametric model of the posterior predictive distribution in a Bayesian framework.