In my assessment, the paper presents a notable technical contribution by unifying a substantial body of work on isotonic regression (IR), leveraging techniques from fast solvers of linear systems. The underlying concept appears intuitive, and from a technical standpoint, I do not have any major concerns, although my ability to thoroughly verify the proofs is limited by my background.
However, my primary concern is that this paper might be more appropriately suited for an algorithms or theoretical computer science conference or journal, given that the works it builds upon and improves ([16] - [20]) and the techniques it employs ([21] - [29]) were published in such venues. It is unclear whether the results would resonate with the broader NIPS community, as:
- Isotonic regression, while having seen interesting applications in learning problems, is not a fundamental machine learning tool where a faster algorithm, by itself, would garner wide interest. I believe an additional learning-specific insight or extension is necessary for an ML paper. For instance, [12] not only designed a faster algorithm for Lipschitz IR fits but also demonstrated its statistical impact, making it more relevant.
- In applications of IR that I am familiar with, such as probabilistic calibration (related to references [0] and [-1], which could be included) and learning SIMs ([10, 12]), the proposed algorithms may not yield significantly faster runtimes since they operate over very structured DAGs. While faster runtimes for general DAGs are algorithmically interesting, a more direct impact on an ML problem is needed. If there are other learning applications where these algorithms offer a significant advance, this should be clearly articulated.
Additional comments include:
- The work on establishing the optimality of the PAV algorithm for a general class of loss functions ([-2] and references therein) could be cited for further context.
- Upon initial review, it appears [14] works with the standard L2 norm rather than a general Lp norm.
- Consider formatting the four points about Program (5) on page 6 into bullet points for clarity.
Noted typos include:
- Page 1: "IF it is a weakly"
- Page 4: "ACCOMPANYING"
- Page 6: "show that the $D$ factor"
- Page 7: "Regression on a DAG"
References that could be added for context include:
[0] Bianca Zadrozny and Charles Elkan. 2002. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '02). ACM, New York, NY, USA, 694-699.
[-1] Harikrishna Narasimhan and Shivani Agarwal. On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation. In NIPS 2013.
[-2] Niko Brummer and Johan du Preez. The PAV Algorithm optimizes binary proper scoring rules.
The paper introduces new algorithms for weighted isotonic regression under general Lp norms, achieving favourable complexity. However, the implications of this work for machine learning are not entirely clear.