This paper offers a significant theoretical enhancement to the on-the-job learner concept introduced in [22], presenting a more robust formulation. Initially, the learner relies on crowdsourcing to generate outputs but progressively develops its own predictive capabilities, resorting to crowd queries only when necessary. The decision to query the crowd is framed as a decision problem, effectively solved using Monte Carlo Tree Search.
The presentation is generally clear, although it lacks crucial details in some areas. The empirical comparison with a threshold-based heuristic yields somewhat modest results, yet the approach appears reasonable and warrants further exploration.
To strengthen the paper, the following points require clarification or addressing:
1. Equation (2) seems to be incomplete, as the left-hand side should also be conditioned on 's'. Furthermore, the disappearance of 'y' in the probability expression \(p{R}(ri | y, q_i)\) needs explanation.
2. The notation \(N(s)\) in Algorithm 1 is unclear and requires definition.
3. The derivation of the specific values (0.3 and 0.88) used in the threshold baseline is not provided and should be explained.
4. It is essential to establish whether the "online" systems referenced are indeed the best performing machine-learned models for the task. If they are, any observed improvements would be attributable to human annotation, highlighting the need for error analysis to contextualize these gains. Examples of such scenarios would be beneficial in reinforcing the discussion around the potential of this approach, which theoretically formulates an on-the-job learner transitioning from a crowdsourced to a self-sufficient predictive model, with selective crowd querying based on confidence levels.