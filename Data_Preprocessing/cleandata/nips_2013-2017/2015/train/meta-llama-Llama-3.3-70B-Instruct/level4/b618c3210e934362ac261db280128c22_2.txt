This manuscript presents empirical evidence demonstrating the enhanced generative capabilities of a recurrent neural network (RNN) when augmented with latent variables, as evidenced by the production of high-quality samples and improved likelihoods or lower bounds on likelihoods compared to preceding efforts in generating distributions over continuous variables.
The paper is generally well-structured and clear in its exposition, with the exception of a figure that is somewhat perplexing and required careful examination to decipher. As acknowledged by the authors, their proposed model constitutes a modest extension of existing architectures, such as those presented in [1]. A thorough comparison with the model described in [1] was conducted, and the benefits of introducing additional structure to the prior distribution over latent variables $z_t$ (achieved by parameterizing the mean and variance as functions of the preceding hidden state) were effectively demonstrated.
A primary concern regarding this manuscript is its incremental nature. Given that the primary innovation lies in the improved prior distribution over $zt$, a more in-depth analysis of the underlying reasons for its efficacy (beyond empirical evaluation) would be desirable. Furthermore, in contrast to previous studies, the authors appear to utilize the same hidden state $ht$ for both generation and inference, and the motivation behind this design choice is not explicitly stated.
With respect to the speech-related experiments, clarification on the windowing procedure employed for the 200 waveform samples would be appreciated (i.e., whether overlapping windows were utilized). The authors may also consider referencing DRAW as an additional application of a VRNN-like architecture. Although the concept of incorporating latent variables into RNNs has been explored in recent studies, this manuscript presents a well-written and nuanced modification that surpasses previous attempts to integrate the variational autoencoder framework with RNNs.