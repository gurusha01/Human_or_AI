This paper presents a novel information-theoretic concept, approximate max-information, which quantifies the generalization properties of adaptive data analyses. The key insight is that an algorithm with low approximate max-information produces output that is "nearly" independent of the input data, facilitating subsequent analyses that rely on this output. Notably, differentially private algorithms and those with limited cardinality ranges exhibit low approximate max-information, and the composition of such algorithms preserves this property. The authors demonstrate the utility of approximate max-information through several applications, including the generalization of differentially private algorithms, a procedure for managing holdout set reuse in machine learning (Thresholdout), and a method for multiple hypothesis testing (SparseValidate).
In my opinion, this paper deserves acceptance due to its unique perspective on generalization, which, although similar to algorithmic stability concepts, offers a more versatile and useful property, particularly in its composition. The presented applications are also noteworthy. However, the paper could benefit from a comparison with earlier work on generalization in learning theory, such as Freund's self-bounding learning algorithms and Blum and Langford's micro-choice bounds. Additionally, clarifying the distinction between the description length results and previous Occam/MDL-type bounds would strengthen the paper.
The Thresholdout procedure raises an important consideration regarding the interplay between the parameters tau and n. Typically, aiming for tau = O(1/\sqrt{n}) is desirable, as it implies budgets of constant or smaller size. While the results remain interesting due to adaptivity and the "free" evaluation of "good" functions, discussing this quantitative aspect is crucial for contextualizing the findings.
Several minor issues and suggestions for improvement were noted, including corrections to probability statements, definitions, and theorem statements. For instance, Theorem 9's statement could be clarified to address potential issues. Furthermore, explicit qualitative claims, such as the potential for an exponential number of queries in n, would enhance the paper's clarity. The assumption that the training set S_t is an iid sample warrants explanation, as its significance is not immediately apparent.
Upon reevaluation, a critical point of confusion was identified in the text, specifically regarding the simultaneous use of terms like "quadratic" and "exponential" in describing certain parameters. To avoid trivial results in Theorem 9, it is essential to provide precise and explicit statements regarding these terms. Despite this, the paper's contributions and insights into generalization through approximate max-information support its acceptance, offering a valuable and interesting perspective that builds upon and expands existing stability concepts.