The authors propose a sentence representation encoding technique, drawing inspiration from Tomas Mikolov's Skip-gram model. This representation is derived from the final layer of a sentence sequence in a GRU RNN. The model is then trained on a large corpus of one billion words, extracted from books, to predict the preceding and succeeding sentences in a paragraph as a GRU RNN language model conditioned on the encoded representation. Notably, despite being trained on a relatively small vocabulary of 20,000 words, the authors demonstrate a straightforward method to expand this to one million words using word2vec and a linear mapping. An extensive array of experiments is presented across various NLP tasks, utilizing the raw sentence representation as a feature.
I found the paper to be highly engaging and believe it lays a solid foundation for future research into learning sentence representations, with potential extensions to paragraph-level representations. A potential critique is that the method does not achieve a new state of the art, although the community is well aware of the challenges in finding representations that generalize effectively across a broad spectrum of NLP tasks.
One experiment that appears to be missing is fine-tuning the GRU RNN from the sentence representation on several datasets mentioned in the paper to assess how close the resulting improvement would be to the current state of the art. Regarding the bidirectional model, I inquire whether results are available for a single model that integrates both forward and backward passes, rather than concatenating the representations.
Minor suggestions include revising the 2nd and 3rd paragraphs in the Introduction to reference Fig 1 and Table 1 instead of Fig 2 and Table 2, and correcting typographical errors such as 'such an experimental is begin' in the last paragraph of Section 1 and 'learned representation fair against heavily' in the 2nd paragraph of Section 3.2.
For training details, I would appreciate clarification on the model's processing speed in words per second using Adam, as well as the number of passes through the entire dataset completed during the two-week training period. Overall, I commend the paper as a valuable starting point for upcoming research in sentence representation learning, with potential applications in paragraph-level representation learning.