This manuscript presents a novel adaptive learning rate scheme for optimizing nonlinear objective functions in deep neural network training, leveraging recent findings that attribute optimization difficulties to saddle points rather than local minima. The authors propose a method to mitigate the ill-conditioning caused by these saddle points through a linear change of variables, effectively "preconditioning" the objective function by left-multiplying the gradient descent update with a learned preconditioning matrix D, focusing on the case where D is diagonal.
This framework provides a useful structure for considering various adaptive learning rate schemes, including the connection to methods like the Jacobi preconditioner or RMSProp, although it is unclear if this link has been previously established. The authors' primary theoretical contribution is demonstrating the undesirable behavior of the Jacobi preconditioner in the presence of both positive and negative curvature and deriving the optimal "equilibration preconditioner" D^E as |H|^-1. However, due to the computational infeasibility of calculating and storing D^E, which depends on the Hessian and is O(n^2) in the number of network weights, they propose an efficient approximate algorithm, "Equilibrated Stochastic Gradient Descent" (ESGD).
ESGD estimates the optimal D^E using the R-operator, requiring roughly two additional gradient calculations every 20 iterations, and sampling a vector from a Gaussian. The empirical results validate the theoretical and practical contributions, showing that ESGD outperforms standard SGD, Jacobi preconditioning, and RMSProp in training deep auto-encoder networks. The paper is generally well-written, but some notation ambiguities were encountered, particularly in distinguishing between element-wise and matrix operators, and in the definition and indexing of matrices.
For instance, the notation in Eqn 7 and the definition of D^-1 = 1 / ||A{i, .}||2 is unclear, specifically whether the i'th element of D^-1 is defined in this manner, and the meaning of the subscript i in A{i,.}. Similarly, Eqn 10's definition of each element i on the diagonal of D^E as the norm of the i'th row of the Hessian, ||H{i,.}||, requires clarification. The addition of a notation section at the beginning of the paper would be beneficial, specifying which operators act element-wise and how matrices are indexed.
Furthermore, the distinction between notations such as A{i,.} vs Ai, and qi qj vs q{j,i}, should be explicitly stated to avoid confusion. In the supplementary material, Proposition 3's application of (q*^2)^{-1/2} and the resulting squared term on the RHS of the inequality also requires clarification. Overall, this is a well-structured paper that presents a new adaptive learning rate scheme, establishes theoretical connections between methods, and provides empirical evidence of its effectiveness, making it a valuable contribution to the field of deep neural network training.