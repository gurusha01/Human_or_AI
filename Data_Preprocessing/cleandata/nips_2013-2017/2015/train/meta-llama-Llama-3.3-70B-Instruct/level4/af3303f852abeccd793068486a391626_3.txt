This paper proposes an online approximation to Markov Chain Monte Carlo (MCMC) for parameter estimation in Bayesian neural networks, with the predictive distribution subsequently fitted using stochastic approximation. The approach is compared to recent advances in approximate Bayesian inference for the same models and example problems, claiming superiority in certain aspects. However, the paper does not demonstrate the potential of these methods to drive progress in specific applications.
The work represents a logical extension of existing research, presented in a clear and concise manner, making it easily reproducible and potentially impactful. Nevertheless, it would be beneficial to include information on training times, as the abstract highlights the efficiency at test time, implying a potential trade-off.
It is essential to clarify that stochastic gradient Langevin dynamics yields approximate posterior samples, not only due to the usual MCMC burn-in requirements but also because it does not locally respect the posterior when exploring a mode over an extended period. While various developments have been cited, they do not offer the same guarantees as traditional batch MCMC for small-scale problems. Although this method may be the only viable option for large problems, it is crucial not to overstate its capabilities. The discrepancy in the posterior in Figure 2 should be explicitly acknowledged.
The statement equating the priors with L2 regularization should be removed, as this equivalence only holds for Maximum A Posteriori (MAP) estimation, which is not the focus of this work. The use of simple spherical Gaussian priors seems outdated, given the extensive research on Bayesian neural networks in the 1990s. These priors are difficult to take seriously, particularly in the context of large and deep networks.
From an engineering perspective, the overall procedure is useful, although further comparisons are necessary to determine its effectiveness compared to standard regularization techniques, early stopping, dropout, and other methods for preventing overfitting. Additionally, more work is required to establish the trustworthiness of approximate Bayesian inference at this scale, particularly when the goal is to obtain predictive distributions.
Minor suggestions include revising the title to better reflect the contribution, as it does not accurately capture the content of the paper. The fitting algorithm should be clarified, as it appears to be more complex than plain stochastic gradient descent (SGD), requiring a stochastic approximation argument due to the dependence between subsequent theta samples. Furthermore, the typesetting of numerical values, such as $5e-6$, could be improved to $5\!\times\!10^{-6}$, and $1e-5$ could be simplified to $10^{-5}$.
The references should adhere to the NIPS style guide, using the numerical "unsrt" style, and include more complete details. Proper nouns should be capitalized, and author names should be consistently listed in full. This paper builds upon the work of Snelson and Ghahrahmani (ICML 2005) on compact approximations to Bayesian predictive distributions, updating these ideas to incorporate neural networks and online methods for MCMC and gradient-based fitting, and is well-executed in bringing existing concepts up to date.