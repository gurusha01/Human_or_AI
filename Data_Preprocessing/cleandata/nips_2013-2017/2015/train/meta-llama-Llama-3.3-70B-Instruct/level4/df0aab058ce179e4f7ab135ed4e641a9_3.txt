This paper presents a novel generative unsupervised model designed to produce representations that exhibit desirable properties, including sparsity, non-negativity, and high dimensionality. By extending factor analysis through the use of posterior regularization, the model effectively enforces these desired properties.
The proposed model demonstrates strong performance both as an autoencoder for reconstruction tasks and as a pre-training mechanism for classification networks. The authors provide a thorough examination of the model, both analytically and empirically, yielding convincing results that highlight its merits.
Several suggestions for improvement are noted: firstly, the rationale behind the sequential application of five different gradient descent methods in the E-step of the learning process (lines 136-140) warrants clearer explanation, as the current approach appears somewhat arbitrary. Secondly, conducting an ablative analysis of the model, such as removing all constraints (e.g., non-negativity) in addition to normalization, would provide valuable insights. Overall, the paper introduces a compelling method, supported by both analytical and empirical evidence, which deserves consideration by the NIPS audience.