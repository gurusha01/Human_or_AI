The conventional method of training recurrent neural networks relies on predictions based on the current hidden state and the previous correct token from the training set. However, during testing, it is expected that the trained RNN can generate an entire sequence by making predictions based on the previous self-generated token. This paper proposes a novel approach, where the model is forced to gradually generate the entire sequence during training, with the previous token being increasingly generated by the model itself.
Quality:
The technical soundness of the paper is evident, and the usefulness of scheduled sampling is well-supported.
Clarity:
The paper is well-structured and clearly written, making it easy to follow.
Significance:
The main idea is well-motivated and intriguing, with the potential to significantly impact the study of recurrent neural network training.
Minor comments:
It would be beneficial to have a deeper understanding of the differences between the three decay schedules and their representation on the training set. Additionally, there is a concern that the training process may become stuck in a sub-optimal solution. Given the complexity of training recurrent nets, with numerous choices such as momentum, gradient clipping, and rmsprop, more details on the training process would be appreciated to make the experiments reproducible. Furthermore, reporting the cost on the training set would be helpful. It is also worth exploring whether scheduled sampling can aid in optimization. Overall, this is a strong paper that introduces a simple yet effective scheduled sampling strategy to address the discrepancy between training and inference in recurrent neural networks for sequence generation tasks, achieving superior performance to strong baselines on image captioning, constituency parsing, and speech recognition.