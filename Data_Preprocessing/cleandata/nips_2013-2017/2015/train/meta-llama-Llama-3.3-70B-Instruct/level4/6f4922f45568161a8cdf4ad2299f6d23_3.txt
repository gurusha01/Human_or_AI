This manuscript presents the covariance-controlled adaptive Langevin thermostat (CCAdL), a novel Bayesian sampling approach that leverages stochastic gradients (SG) to address the correlated errors arising from the SG approximation of the true gradient. The authors showcase the superior accuracy and robustness of CCAdL compared to existing SG-based methods across various test problems.
Although the paper is generally well-written, its complexity may pose a challenge for readers unfamiliar with these sampling algorithms. The manuscript commences with a review of several SG methods for efficient Bayesian posterior sampling, including SGDL, mSGDL, SGHMC, and SGHNT. To enhance clarity, the authors could consider incorporating a table or figure that provides an overview of the different SG variants, highlighting their similarities and differences.
The CCAdL method combines previously proposed ideas, primarily from SGHMC and SGNHT, including:
- The approximation of the true gradient of the log posterior using a stochastic version obtained through data sub-sampling, which introduces "noise" in the gradient that must be addressed
- An estimate of the covariance matrix of the gradient noise, which is a running average of the Fisher scores, and in high-dimensional problems, is replaced by a diagonal matrix
- The utilization of a thermostat to account for the inefficiency of Metropolis Monte Carlo acceptance/rejection, as used in standard HMC
As a non-expert in the field, it is unclear what the true novelty of CCAdL is, as it appears to combine concepts from SGHMC and SGHNT with a previously proposed estimator for the noise covariance matrix.
The authors demonstrate the efficacy of CCAdL on a logistic regression problem, showcasing its significantly faster convergence to higher log likelihood values. CCAdL also exhibits stability for friction values smaller than those required by SGHMC and SGHNT. By examining the marginal distribution of pairs of parameters, the authors demonstrate that CCAdL produces posterior distributions that closely approximate the "true" distribution obtained by HMC on the full likelihood. In a second large-scale example, the authors train and test discriminative RBMs on three datasets, finding that CCAdL outperforms SGHMC and SGHNT for most step sizes and friction constants.
These results are promising and warrant publication. However, to improve the manuscript's readability for non-experts and address minor errors, the authors could provide explanations for introduced symbols, such as $\mu$ and $dW_A$. Overall, the paper presents a significant contribution to the field, and with revisions to enhance clarity and readability, it has the potential to be a valuable addition to the scientific literature.