This paper proposes an optimization algorithm for the problem 
\min l(x) + r(x)
where l is a smooth function and r(x) = \sum(\rho(|x_i|) for a concave smooth function \rho. The authors introduce a hybrid algorithm combining quasi-Newton and gradient descent steps, drawing inspiration from the OWL-QN algorithm. To address the non-smoothness of the regularizer, the algorithm constrains iterates to remain in the same quadrant, relying on Clarke sub-differential properties due to the non-convex nature of the regularizer.
The problem tackled is indeed challenging, as it involves both non-smoothness and non-convexity. However, several concerns arise. 
(1) Despite criticizing DC programming for non-convex optimization, the proposed algorithm lacks convergence rate guarantees, whereas DC programming offers linear convergence.
(2) The algorithm's convergence relies on a gradient descent step, but finding the optimal balance between the gradient descent and quasi-Newton steps appears to require empirical tuning through repeated runs with different parameters.
(3) The use of non-convex regularizers, as described in this paper, is uncommon in machine learning applications, raising questions about the relevance of this work to the broader community.
The paper presents a well-written and mathematically sound treatment of minimization under a non-convex regularizer that promotes sparsity. Nevertheless, the practical significance and relevance of these results to the machine learning community remain uncertain.