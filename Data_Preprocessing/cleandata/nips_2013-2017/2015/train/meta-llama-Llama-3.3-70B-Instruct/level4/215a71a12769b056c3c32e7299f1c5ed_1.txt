I find the paper to be well-written, with a clearly described idea and experiments that are convincing to some extent. One of the strengths of the paper is the authors' attempt to analyze the learned networks in terms of the pattern of gate outputs.
(1) The gate unit T utilizes a standard sigmoid activation function, which means the response value never reaches exactly 0 or 1, as the authors themselves noted. Consequently, the gradients in equation 5 are inaccurate. The authors should provide a detailed explanation of how backpropagation is performed in these networks.
(2) Analyzing the effect of initialization can be challenging, but the authors' proposal to use constant negative biases based on the number of levels is not entirely satisfactory. A plot illustrating the performance in relation to the initial bias value would be helpful. The initial bias serves as a regularizer, with high negative biases favoring networks with "longer highways" - a comparison I find particularly insightful.
(3) The statement "report by recent studies in similar experimental settings" is unclear. If the methods employ the same experimental setting, a comparison would be feasible; otherwise, a direct comparison would not be possible. The authors should clarify the differences in experimental setups within the paper.
(4) Presenting only "max" accuracies in tables is unconventional and statistically unreasonable. Instead, the authors should consistently report mean accuracy and standard deviation for each method, along with any available additional metrics.
(5) It would be fascinating to examine the proposed approach from a learning theory perspective. The paper introduces a method for using additional gate units in neural networks, enabling layers to function as an identity. Since the gate unit response depends on the input, it allows inputs to propagate through multiple highly selective layers, which warrants further investigation.