This paper presents a modified EM algorithm tailored for sparse and high-dimensional true parameters, accompanied by conditions for convergence to the global solution and the derivation of its convergence rate. Additionally, a statistical testing framework is established to assess the sparsity of the estimator. The paper's organization and summary of related work are commendable. However, the content's density, particularly in sections 3 and 4, hinders readability, and simplification would greatly benefit the audience.
A crucial argument in line 066 posits that sqrt(s*log(d/n)) is minimax-optimal, yet the main manuscript lacks a clear proof of this claim, which was not verified in the supplementary material. A detailed proof of this assertion would be beneficial.
Furthermore, Eq.(3.8) introduces ^s as dependent on s, but since s is unknown in practical scenarios, the choice of ^s and the determination of an appropriate initial value beta^init that satisfies ||beta^init - beta*||_2 < R/2 (as mentioned in line 275) require clarification. 
The truncation step in Algorithm 1 seems pivotal, but its importance and the consequences of omitting it from the EM algorithm are not clearly explained. Elucidating the role of this step would enhance understanding.
Following the rebuttal, the authors' response was found to be satisfactory, prompting an increase in the score from 6 to 7. While the paper is statistically sound, its accessibility to machine learning researchers is limited due to its complexity. Moreover, the practical applicability and real-world usefulness of the proposed method raise some questions.