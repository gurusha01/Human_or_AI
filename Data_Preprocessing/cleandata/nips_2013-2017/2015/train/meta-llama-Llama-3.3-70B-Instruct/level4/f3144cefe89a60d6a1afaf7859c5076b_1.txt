The authors present a hierarchical model that utilizes a multilayer representation of count vectors, where the hidden layers comprise gamma variables with factorized shape parameters. The proposed inference method, based on Gibbs sampling, is capable of learning the widths of the hidden layers, with the first-layer width serving as a limiting factor. The experimental results demonstrate that a deeper network achieves superior classification accuracy and perplexity compared to a single-layer model, which is equivalent to the model presented in [12]. This improved performance is attributed to the model's ability to capture correlations between hidden units and account for overdispersion.
The model and algorithm are well-presented, and the paper effectively highlights the advantages of the multilayer model over its single-layer counterpart, which can be viewed as a nonparametric extension of the PFA model introduced by Zhou et al. (2015). The experimental results are convincing, and the model's novelty lies in its use of nonnegative hidden units and automatic learning of layer widths.
However, the paper falls short in demonstrating the model's superiority over the leading competing method, the over-replicated softmax model [21], a two-layer DBM model. Although the authors acknowledge that their model's classification accuracy is inferior to the over-replicated softmax when the first-layer budget is 512, they attribute this to word preprocessing without further investigation. Furthermore, the observations regarding layer width decay rates on page 8 are based on inference from a single dataset, raising questions about the extent to which these findings are dataset-dependent or influenced by the first-layer budget. The claim on page 2 regarding the revelation of this relationship requires more comprehensive analysis across multiple datasets.
In summary, the proposed multilayer model of count vectors with nonnegative hidden units and Gibbs sampling-based inference is novel, and its advantages over a single-layer model are clearly presented. However, the paper does not establish the model as state-of-the-art in a specific domain, such as topic modeling, nor does it demonstrate significant merits over the leading competing method, the over-replicated softmax model of Srivastava et al. (2013). Additionally, the authors' conclusions are based on results from a single dataset, which may not be representative of the model's performance across different datasets.