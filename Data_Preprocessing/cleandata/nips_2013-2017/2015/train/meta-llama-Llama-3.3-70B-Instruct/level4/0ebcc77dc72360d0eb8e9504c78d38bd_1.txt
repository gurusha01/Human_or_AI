This manuscript introduces a novel algorithm for global optimization, which circumvents delta-cover sampling and attains exponential regret, thereby extending the work of Freitas et al. (2012) that relied on a impractical sampling procedure. 
The paper is well-structured and easy to comprehend, effectively situating the new algorithm within the existing literature on bounded-based search methods. As such, I believe this work warrants attention from the scientific community.
I assign a score of 5, although not higher, due to my reservations regarding the experimental section, which I find to be subpar compared to the standards expected in a Bayesian Optimization (BO) paper. My primary concerns with this section are:
- The authors fail to compare the methods across multiple initial evaluations of the function f, a standard practice in BO methods, thereby precluding meaningful statistical comparisons between the methods.
- Despite proposing a bounded-based search method, the authors omit comparisons with state-of-the-art BO approaches, including information theoretic methods like Entropy Search.
- All experiments are conducted on synthetic functions with a maximum dimension of 6, without any real-world wetlab or parameter tuning experiments to demonstrate the method's performance in practical scenarios.
The manuscript presents a new global optimization algorithm that avoids delta-cover sampling and achieves exponential regret. While the paper is engaging and offers interesting theoretical insights, I believe the experimental section falls short of the standards expected in a NIPS paper.