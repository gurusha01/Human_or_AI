Review after rebuttal:
I find it perplexing that the authors employed the same initialization as prior papers, given that the irrelevance of initialization is a primary contribution of this work, which raises significant concerns about the research.
In my view, this paper is technically sound and presents non-trivial results. Building upon recent research on alternating minimization applied to various statistical problems, the authors have established intriguing results on recovering precision matrices from tensor normal distributions. This can be seen as another demonstration of the effectiveness of alternating minimization in tackling statistical problems. Theorem 3.1, although the simplest result in the paper, encapsulates the core idea and is potentially the most crucial observation for future research. The proofs of the other theorems are more complex but are clear and do not contain any insurmountable issues. However, the proof of the last theorem is missing.
The following are some errors or drawbacks I identified in the paper:
(1) The authors claim that a single sample is sufficient to achieve strong statistical guarantees, but the current proof does not support this claim. In the Supplementary Material, line 616, the Frobenius norm of the error is only bounded for sufficiently large n. This issue can be addressed by considering a fixed n and growing dimensions, allowing the radius in the definition of $\mathbb{A}$ to vanish. For instance, the scaling law in line 294 of the main text meets this requirement, making the conclusion valid. Only minor modifications to parts of the proof are necessary.
(2) In the Supplementary Material, there are several minor errors in the proofs:
2.1) Line 607, the concept of the "boundary" of $\mathbb{A}$ is unclear, as $\mathbb{A}$ is already defined as a sphere-like set.
2.2) Line 661, there is a typo in the second term of the first equality.
2.3) Line 669, the application of the inequality is not apparent. It seems that using the sub-multiplicativity of the Frobenius norm would be sufficient.
2.4) Line 699, the term "primal"-dual witness is used.
2.5) In the proof of Lemma B.1, the constant  is not explicitly defined. It would be beneficial to include the definition of  in the statement of the lemma to highlight the dependence of the bound in Lemma B.1 on || - *||F.
2.6) The authors state that their proof relies on Talagrand's concentration inequality, but its usage is not evident. Lemma C.2 is not Talagrand's concentration inequality; instead, Gaussian concentration can be derived using the Gaussian log-Sobolev inequality.
(3) In line 303 of the main text, the authors claim that their bound is minimax optimal. However, it is unclear what class this minimax optimality refers to.
(4) In line 323, the Hessian is defined as -1 ? -1. This paper investigates the estimation problem for observations following a tensor normal distribution with a separable covariance matrix ($\Sigma^ = \Sigma1^ \otimes \cdots \otimes \SigmaK^* $). The authors consider the classical penalized maximum likelihood estimator, resulting in a non-convex optimization problem. Notably, the main result in this paper shows that this non-convexity can be ignored, and alternating minimization can be used to achieve strong statistical performance.