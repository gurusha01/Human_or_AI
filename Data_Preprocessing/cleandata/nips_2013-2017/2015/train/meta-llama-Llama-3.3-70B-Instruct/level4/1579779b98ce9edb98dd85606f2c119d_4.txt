The submission presents an innovative algorithm for global optimization of decision trees by reframing the problem as latent structured prediction, where latent structured variables are sign vectors representing decisions at each internal node. A non-differentiable and non-convex loss function is introduced, which is approximated by a differentiable yet non-convex upper bound. This loss is minimized using stochastic gradient descent (SGD), involving the solution of a series of loss-augmented inference problems for which efficient algorithms are provided. Experimental comparisons with a standard greedy training method are included.
The key contributions of this work lie in its novel approach to global decision tree optimization as structured prediction and the development of efficient optimization algorithms. This method raises intriguing questions, particularly concerning the potential for optimizing infinitely deep trees by gradually introducing non-zero weights, thus allowing the model's complexity to adapt to the data. This could potentially lead to a method that naturally balances tree depth and complexity, possibly converging to a tree with a limited number of nodes under appropriate regularization.
However, several practical considerations and ambiguities need clarification. For instance, the handling of zero-weight nodes is crucial and currently seems arbitrary, implying a need for a precise definition that aligns with pruning strategies in greedy training. Moreover, the motivation behind using decision trees, which are primarily valued for their interpretability, is questionable if the method yields non-interpretable results due to dense weight vectors from L2 regularization. Switching to L1 regularization might address this but could compromise performance.
The experimental section lacks detail, particularly regarding baselines. The comparison with naive greedy training methods is incomplete, as it does not specify the pruning strategies used. It would be beneficial to include experiments with various pruning strategies and parameters for the naive method, as well as other simple global optimization strategies like training on random subsets of the data.
The paper's clarity is generally good, although sections describing inference, such as Section 6.2 and 6.4, could be more concise and explicit. In summary, while the approach is clever and raises interesting questions, its practical utility is uncertain due to interpretability concerns and the lack of comprehensive experimental comparisons. Further analysis, especially on the initialization and evolution of weights, could uncover more significant advantages of this method. Additionally, the title's mention of forests is not addressed in the paper, suggesting a potential area for future expansion. 
Post-rebuttal, the critical case of zero weights remains a point of interest, potentially offering a direction for making the method more compelling, especially if weights are initially set to zero and then gradually increased, allowing for a dynamic adjustment of the tree's complexity. This aspect deserves further exploration to fully realize the potential of the proposed algorithm.