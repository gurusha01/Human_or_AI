The authors argue that non-negative transforms, such as ReLUs, are crucial for successful models, citing the sparsity and non-negativity of representations learned by ReLUs. However, this claim is weakened by the fact that PReLUs, which are not non-negative, have been shown to outperform ReLUs in some cases. The only significant advantage of non-negative transforms appears to be their non-saturating nature.
The authors outline their goals as constructing input representations that are sparse, non-negative, non-linear, utilize many code units, and model structures in the input data. However, the importance of non-negativity is unclear, and the goal of using many code units is questionable, as effective representations should capture the same structure with fewer hidden units. Notably, the goal of achieving low reconstruction error is missing.
The authors distinguish their approach from current unsupervised deep learning methods, such as autoencoders and restricted Boltzmann machines (RBMs), which they claim do not model specific structures in the data. However, it is unclear whether they intend to imply that RBMs are not generative models, which would be inaccurate.
The results presented in Table 1 are confusing, as the authors claim their method achieves the lowest reconstruction error, yet PCA appears to have lower reconstruction errors in the table. Figure 2 is intended to demonstrate the method's robustness to background noise, but the filters appear less robust than those obtained with denoising autoencoders.
Table 2 is misleading, as it compares the accuracy of the authors' method to state-of-the-art deep learning models that leverage pretraining, while omitting relevant comparisons, such as Schmidhuber's convolutional autoencoders, which achieve better performance on MNIST and CIFAR 10.
To strengthen the experiments, it would have been beneficial to include explicit sparse coding methods, such as l1 regularized regression, in the comparison of RFNs to other unsupervised methods. Additionally, comparisons of sparsity and reconstruction error on real-world examples, rather than just toy datasets, would have been more convincing.
The comparison of RFN pretraining to other methods for pretraining deep nets would have been more compelling if it had included a broader range of methods. Furthermore, it would have been interesting to see the results of classification tasks performed directly on the output of RFNs compared to other unsupervised methods.
While the authors have made significant contributions to the application of posterior regularization to factor analysis, the results presented in this paper are more promising for unsupervised learning than for pretraining strategies in deep learning models. Notably, the classification accuracy on CIFAR10 is significantly worse than that achieved by convolutional neural networks pre-trained with convolutional autoencoders and falls short of the state-of-the-art performance.