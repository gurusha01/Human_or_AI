This manuscript introduces a framework for estimating policy gradients in risk-sensitive scenarios utilizing coherent risk measures and develops corresponding reinforcement learning (RL) algorithms. Notably, it generalizes existing methodologies tailored to specific risk measures, encompassing them within the broader class of coherent risk measures, thereby offering a unified perspective. The proposed algorithm is demonstrated through a simplified financial example, where assets exhibit distinct return distributions.
The underlying motivation is succinctly articulated, with problem formulation and algorithmic design logically stemming from this motivation and preceding research. Although the experimental setup is somewhat basic, confined to a static risk scenario, the outcomes suggest the algorithm's efficacy in exhibiting risk-averse behavior and adaptability across various risk types.
In terms of quality, the manuscript is well-crafted, with technically rigorous equations and comprehensive referencing of pertinent literature.
The clarity of the paper surpasses average standards, facilitating effortless comprehension.
Regarding originality, the manuscript proposes innovative algorithms for risk-sensitive reinforcement learning, building upon related research in risk-sensitive planning and reinforcement learning. The extension of the proposed approach is non-trivial, indicating sufficient originality.
The significance of this work is substantial, as it addresses the intriguing and crucial issue of risk-sensitive reinforcement learning. By presenting explicit algorithms for solving risk-sensitive reinforcement learning problems with coherent risk measures and illustrating the performance of trained policies under different risk settings in a static risk problem, the manuscript makes noteworthy contributions. The approach is clearly motivated and represents a natural evolution of existing algorithms and problems, including policy gradient, actor-critic methods, and coherent risk measure optimization.