This paper tackles the significant challenge of parallel optimization, presenting an algorithm that exhibits improved robustness to staleness, potentially reducing the communication burden compared to current state-of-the-art methods. Additionally, it achieves superior learning outcomes, as evidenced by lower test error rates. The paper is well-structured and enjoyable to read.
Regarding the existing EASGD framework, the center variable update is based on a symmetric elastic force, as described in Equation 4. However, considering the objective function in Equation 2, a more intuitive approach might be to calculate the exact average of all local variables in an online manner. It would be beneficial to know if the authors have explored this alternative and to see a discussion on this topic included in the paper.
An investigation into the impact of the rho parameter would be valuable, particularly in terms of how exploration influences performance. Furthermore, clarification is needed on the line following Equation 4, which mentions the stochastic gradient of "F". The proposed parallel algorithm is designed to facilitate concurrent exploration among computing nodes, aiming to optimize objectives with multiple local optima more effectively. The concept is well-motivated, clearly explained, and substantiated by comprehensive experimental studies.