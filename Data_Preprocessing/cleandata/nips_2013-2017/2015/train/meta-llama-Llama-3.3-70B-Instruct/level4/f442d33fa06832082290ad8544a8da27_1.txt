Upon reevaluation following the author's response, it is clear that the pre-trained sentence vectors will be straightforward to utilize, rendering them highly useful. However, my initial concern was that retraining the model would pose a greater challenge compared to using established methods like word2vec.
The GitHub repository currently lacks the training component. The paper presents an intriguing application of sequence-to-sequence models, adapting the skip-gram concept to predict preceding and succeeding sentences using the current sentence. Although the experiments demonstrate the model's potential, it fails to surpass state-of-the-art performance in any of the tested scenarios. Given the model's lengthy training time, it may be impractical for users to retrain the vectors on their own domain-specific corpora.