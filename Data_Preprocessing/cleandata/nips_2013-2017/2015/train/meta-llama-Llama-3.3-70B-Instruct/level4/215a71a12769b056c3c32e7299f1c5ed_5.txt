I also considered exploring the LSTM approach of controlling gradient flow to construct deeper networks, but hadn't initiated the project. I commend the authors for pioneering this effort.
An alternative viewpoint could be that an extremely deep network can be perceived as a less deep network comprising layers with sub-layers, where each "layer" exhibits more complex non-linear behavior than traditional rectifier or sigmoid layers. This perspective resonates with my understanding of architectures like GoogLeNet's inception and VGG's stacked 3x3 layers, and I speculate whether this interpretation can be applied to highway networks as well.
One potential experiment I propose the authors consider is training a single, thin yet tall network on both MNIST and CIFAR datasets (with a possible fork at the top for domain-specific adjustments). I am intrigued by whether, during inference, the network would exhibit similar patterns to those observed in separately trained networks for each domain, and whether parameters underutilized in MNIST would be leveraged by CIFAR. Essentially, I am interested in examining how sharing occurs in a deeply hierarchical structure and assessing its efficiency.
I have no significant criticisms. I suggest the authors explore applying their network to larger datasets like ImageNet to further demonstrate its capabilities.
This paper presents an LSTM-inspired concept that enables the training of exceptionally deep feedforward networks. The idea is straightforward yet refined, and the accompanying analysis is sound.