This manuscript proposes an algorithm for learning decision trees with linear split functions at the nodes, where the tree parameters, including the weights in the split functions and the output variable distribution at each leaf, are optimized globally. In contrast to the traditional greedy approach, this method formulates an upper bound on the tree's empirical loss, which is then regularized using L2 regularization on the split function weights to facilitate smoother optimization. The algorithm utilizes stochastic gradient descent to identify the optimal tree parameters that minimize this upper bound.
The optimization problem is formulated by establishing a connection with structured prediction involving latent variables, demonstrating elegance in its approach. However, the empirical results presented do not convincingly demonstrate the superiority of global optimization over greedy learning. The globally optimized tree significantly outperforms the greedy tree on only a few datasets, at the cost of increased computational intensity.
The paper's structure is clear, but several points require clarification:
- The description of the stable version of stochastic gradient descent (SGD) lacks clarity, particularly regarding the assignment of data points to leaves. Including pseudo-code could enhance understanding.
- Figure 2's results are unclear regarding whether they were obtained using SGD or stable SGD. Additionally, the term "active" leaf needs a precise definition.
- In Algorithm 1, line 10, the update and projection of \Theta onto a simplex could be better explained, including how the projection problem is solved in practice. It's also worth noting that this projection ensures each line of \Theta sums to one.
Minor comments and typos include:
- Lines 407-408 mention tuning the "number of features evaluated when building an ordinary axis-aligned decision tree," which is confusing since all features are evaluated at each node in standard decision trees.
- Equations (7), (8), and (9) should replace "argmax" with "max."
- Figure 1 is not referenced in the main text.
- Line 141 contains a grammatical error at the end of the sentence.
- Line 238 should be pluralized to "the solutions to..."
- Line 301 contains a grammatical error, missing the indefinite article "A."
- Line 360 introduces "FSGD" without explanation.
- Line 404 likely means "tuning" errors instead of "training" errors.
In the supplementary material:
- Line 29 should change "max{g \in sgn...}" to "max{g = sgn...}"
- Line 30 contains a typo, needing correction to "sgn(Wx)^T Wx + l(\Theta^Tf(sgn(Wx)), y)"
While the tree optimization problem is elegantly formulated, connecting it with structured prediction and latent variables, the results fail to convincingly support the global optimization approach over traditional greedy methods.