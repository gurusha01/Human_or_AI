This paper introduces the CCAdL method, which offers an improvement over the SGNHT method in scenarios where the variance of stochastic gradients exhibits non-constant behavior across the parameter space.
However, this advancement comes at the cost of requiring an explicit estimation of the stochastic gradient variance, which can be reasonably efficiently conducted on a minibatch. Nevertheless, this estimation process introduces additional noise stemming from the estimator itself, as acknowledged by the author in line 193. While thermostats can effectively stabilize the system by neutralizing constant noises, the estimator's noise is parameter-dependent, posing a challenge.
Ultimately, a residual error persists in the system, akin to the SGNHT method, which cannot be entirely eliminated. Despite this, the promising experimental results suggest that the impact of this error may be less severe than the original error associated with stochastic gradients. It would be fascinating for the authors to investigate this error further and characterize it in relation to the stochastic gradient error.
The paper is well-structured, and the experimental results appear convincing, albeit limited to small-scale experiments. Nevertheless, without a more thorough analysis of the newly introduced stochastic noise's error, the paper's overall novelty may be perceived as incremental. Essentially, the paper builds upon the SGNHT by incorporating an estimator for the variance of stochastic gradients, resulting in an incremental algorithm that, while improved, still grapples with an unresolved flaw.