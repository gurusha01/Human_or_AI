A question to clarify in the rebuttal: According to Section 4.1 in [1], the CRM approach is only applicable when delta is within the range of [-1,0], prompting the proposal to rescale a general loss to this interval. I assume that the same principle applies to NORM-POEM, implying that equations (8) and (3) both utilize the rescaled delta within the [-1,0] range. If so, I am unclear about the use of "delta > 0" in the experiment presented in Table 1 and would appreciate clarification.
== Additional Comments ==
- In the setup for "batch learning from logged bandit feedback" outlined in Section 3 (and also in [1]), I believe it is essential to consider the feedback as a random variable rather than a deterministic function, as currently presented on line 110. My reasoning is based on a scenario where partial feedback originates from a classification problem, and the true labels are unknown. Instead, feedback is generated by making a prediction, observing the loss, and receiving feedback based on this loss. In such cases, the labeling process can be noisy, resulting in non-unique labels for each input. Consequently, the same input and prediction can yield different losses due to varying labels, suggesting that delta(x,y) should be treated as a random variable. This perspective may not affect the risk estimator (2), but it highlights the need to incorporate an additional expectation over the randomness of delta in the setup for the true risk (1) to ensure greater generality.
- On lines 170-171, I think the correct notation should be h(y|x) / h_0(y|x). It might also be beneficial to mention that the non-linearity issue was previously noted in [1].
- Line 276 should likely use argmin instead of argmax, given that the risk is being minimized.
- On line 319, to maintain consistency with the preceding notation, it would be better to use hats instead of bars.
=== Update after Rebuttal ===
Thank you for the clarifications. [light reviewer] quality: 6 (out of 10), clarity: 6, originality: 8, significance: 8. I find this work to be an interesting extension of [1], with a compelling analysis of "propensity overfitting" and a novel approach using a multiplicative control variate to mitigate it. The results demonstrate a clear improvement over POEM in the context of batch learning from logged bandit feedback, an area of increasing interest.