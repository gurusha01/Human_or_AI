This manuscript tackles the challenge of learning effective representations in an unsupervised manner. It presents a novel, deep (stacked) factor analysis model that incorporates posterior constraints, optimized using an Expectation-Maximization (EM) algorithm with a projection step in the E-step to ensure constraint satisfaction. The approach is compared to several popular unsupervised methods, yielding consistent improvements.
To my knowledge, this work is the first to apply posterior regularization to a deep factor analysis model and solve it using a simplified, projected version of the EM algorithm.
However, Section 2 is overly complex and could be clarified for better readability.
The authors claim a significant speedup of their algorithm compared to the traditional Newton method, but the calculation of the number of steps at line 145 is unclear. Additionally, comparing the computational timing of different methods would provide further insight.
The first experiment appears to utilize a trivial dataset, as PCA achieves 0 error on reconstruction with 100/150 code units. When comparing RFN and RBM, RFN excels in reconstruction, while RBM performs better in terms of sparsity. It is challenging to conclusively state that RFN is substantially better, and a more detailed explanation of the numerical results would be beneficial to understand the significance of the differences.
The second experiment on pre-training demonstrates an improvement of approximately 0.5%, but this difference may not be substantial on the MNIST dataset. Furthermore, the proposed method does not perform well on CIFAR. Therefore, conducting experiments on larger, more realistic datasets would be valuable. This paper introduces a novel unsupervised learning model for sparse, non-linear, high-dimensional representation, with experimental results showcasing its efficacy.