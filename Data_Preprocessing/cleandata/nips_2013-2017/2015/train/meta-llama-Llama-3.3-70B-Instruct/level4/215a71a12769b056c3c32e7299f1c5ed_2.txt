This manuscript presents a novel architectural design for extremely deep networks, allowing input data to bypass certain layers unaltered through the use of input-dependent gates. 
This design facilitates the training of significantly deeper networks than would be feasible with traditional feed-forward architectures. 
The authors conduct a compelling examination of the trained networks to elucidate how the gates select inputs across various layers, which aligns with my expectations for such a study. 
The paper exhibits high quality, clarity, and originality. 
Although it shares some similarities with recent studies (notably FitNets and LSTMs), the authors clearly articulate the distinctions and provide relevant comparisons where necessary. 
The results obtained are promising. 
I do not have any recommendations for improvement. 
However, I noticed a potential error in the notation "Tables 3 and 3" on page 5. 
Overall, this is a robust paper that introduces an innovative technique, yielding favorable outcomes. The analysis provided is both intriguing and comprehensive.