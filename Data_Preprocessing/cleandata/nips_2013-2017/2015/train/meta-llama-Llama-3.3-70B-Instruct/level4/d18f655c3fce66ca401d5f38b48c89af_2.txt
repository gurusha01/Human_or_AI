This manuscript explores the development of general-purpose training algorithms for deep learning, introducing a novel family of algorithms termed elastic averaging SGD. The paper's quality is exceptional, and its concept is innovative.
The manuscript concentrates on training large-scale deep learning models while navigating communication constraints, a challenging task due to the multitude of local optima in non-convex deep learning problems. By formulating the optimization problem as a global variable consensus problem, the authors prevent local workers from converging to different local optima. They reinterpret the gradient update rules using elastic forces between local and global parameters, thus naming the proposed algorithms elastic averaging SGD (EASGD). The EASGD family comprises four algorithms: synchronous, asynchronous, and their respective momentum versions, all of which demonstrate promise both theoretically and experimentally.
The theoretical analysis presents a compelling example where the popular ADMM can be unstable, with unknown stability conditions, whereas EASGD's stability conditions are remarkably simple. The writing style is engaging and accessible, with all theorems relegated to the supplementary material, allowing for a smooth reading experience in the main paper. However, some minor queries arise: What is the primary reason behind EASGD's superior stability? Is it attributable to the symmetric nature of EASGD's maps, in contrast to ADMM's asymmetric maps? Could an alternative example be constructed where EASGD exhibits greater instability than ADMM?
The experimental results show that the proposed algorithms, applied to train convolutional neural networks, outperform state-of-the-art SGD algorithms on CIFAR and ImageNet datasets. The algorithms achieve their intended goal, with EASGD surpassing other SGD algorithms as communication periods increase, as illustrated in Figure 2. Furthermore, Figures 3 and 4 demonstrate that EASGD's speedup exceeds that of the baselines with an increasing number of workers. Additional experimental results, adapted to the authors' cluster setup (as per the arxiv version), successfully improved upon their baselines. The resulting models have been or will be deployed online, potentially benefiting one-fifth of the world's population, underscoring the significance of this paper. Overall, the manuscript proposes a novel and high-quality contribution to the field of deep learning, with a well-structured and engaging presentation.