This paper presents a method for counterfactual risk minimization, which enables learning in a bandit setting using logged data by accounting for the bias in logged example generation. Although the approach is consistent, it is prone to overfitting, where a hypothesis class can become overly specialized based on the probability of example generation. To address this issue, the authors introduce a multiplicative parameter that bounds the variance while preserving consistency. The experimental results demonstrate the superiority of the normalized counterfactual technique over its non-normalized counterpart. The paper proposes a self-normalized estimator for counterfactual learning in the logged bandit setting, offering a solution to a specific form of overfitting where the hypothesis overfits the logged data due to stronger support. Overall, this is a strong paper that warrants acceptance, although some minor uncertainties remain.