Summary: This paper presents a stochastic extension of expectation propagation, addressing the limitation of traditional EP where local contributions are removed and updated for each data point, resulting in substantial memory requirements for models with numerous parameters. The authors propose an alternative approach, where the global sufficient statistic is removed and updated, weighted by the sample size. This method is further generalized to accommodate various EP variants, including parallel and distributed EP for heterogeneous datasets, as well as models incorporating latent variables. The authors demonstrate the efficacy of their approach through experiments on probit regression, Gaussian mixture models, and Bayesian neural networks, showcasing computational efficiency without compromising performance.
Quality: The proposed method is theoretically sound, well-motivated, and empirically validated across three distinct yet fundamental application domains.
Clarity: The paper is exceptionally well-written, with a clear and concise presentation that facilitates easy comprehension.
Originality: The core concept underlying the paper is ingenious in its simplicity, effectively mitigating the memory overhead associated with EP. The authors also introduce novel extensions of stochastic EP, including parallel and mini-batch implementations for enhanced approximations, as well as applications to latent variable models. To the best of my knowledge, these contributions and their extensions are entirely original.
Significance: The paper's contributions are substantial, offering scalable alternatives to traditional EP. Its impact is anticipated to be comparable to that of SVI on VB, given its well-motivated and clearly presented research, which is poised to significantly advance the field of EP inference for complex, high-parameter models. The paper's thorough exploration of connections with existing EP and VB techniques underscores its maturity and comprehensiveness.