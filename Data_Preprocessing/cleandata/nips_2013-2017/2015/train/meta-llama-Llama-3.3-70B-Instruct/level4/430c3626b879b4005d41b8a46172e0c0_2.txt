The authors present a numerical approach termed pre-conditioning, which employs a linear transformation of variables to facilitate gradient descent in a more favorable parameter space. They introduce the concept of "equilibration" as a pre-conditioner, involving row normalization of a matrix, to improve the conditioning of the loss Hessian. A practical estimation technique for the equilibration pre-conditioner is also proposed, with a negligible increase in computational cost compared to standard gradient-based training. Experimental results demonstrate a reduction in the condition number of random Hessians, while theoretical analysis shows a decrease in an upper bound on the Hessian's condition number, although a more direct proof is needed. Furthermore, the authors demonstrate that their "Equilibrated Gradient Descent" (EGD) algorithm enhances the convergence time of deep neural networks on certain datasets. They also establish a connection between the heuristic RMSProp algorithm and equilibrated EGD, suggesting the former is an approximation of the latter.
Overall, this paper is deemed clear and insightful, offering a comprehensive analysis. Its originality and potential for immediate application in neural network training are notable strengths. The paper's clarity, precise experimental results, and solid theoretical foundation make it a valuable contribution to the field, providing a justification for the success of a common online training algorithm for neural networks and introducing a new, practical online training algorithm.