In summary, this paper introduces a novel extension of neural word embeddings to sentence embeddings, leveraging a "skip-thoughts" model that predicts preceding and subsequent sentences based on the current sentence's encoding. This approach enables training without supervision from raw text, with the resulting embeddings serving as features for downstream tasks. The authors conduct experiments across eight tasks, demonstrating that their method, although not state-of-the-art, outperforms several previous baselines with reduced feature engineering.
The "skip-thoughts" model represents a clever application of the sequence-to-sequence RNN framework, building upon the established concept of learning representations through auxiliary tasks to enhance downstream task performance. While the sentence-level approach is not entirely new, having been explored in paragraph vector work and earlier research, the technical methodology is sound and the vocabulary expansion strategy is commendable.
However, concerns arise regarding the experimental evaluation, primarily revolving around whether the proposed approach offers a superior method for learning from large amounts of unlabeled data compared to alternatives, in terms of downstream performance. Unfortunately, the experiments do not adequately address this question, as there is no baseline that utilizes the same dataset (Book11K corpus) for comparison. This omission leaves uncertainty as to whether the reported results are attributable to the use of a larger dataset or the model's ability to capture meaningful information beyond superficial word co-occurrence statistics.
A comparison to a strong baseline using the same training data would have been beneficial. Alternatively, a weak baseline, such as a bag-of-words model, or training word embeddings on Book11K and incorporating them into more engineered baselines to match data conditions, would have provided valuable insights. Furthermore, discussion of the common issue of RNNs in the encoder-decoder framework, where the encoder must summarize the input in a single vector, and potential heuristics such as token reversal or attention models, would have been appreciated.
Additional details regarding the tuning of the proposed approach, its sensitivity to hyperparameters, exploration of alternate architectures, and performance improvements with increasing amounts of unlabeled data would have strengthened the paper. Following the author response, it is clear that including a baseline with word-level embeddings trained on Book11K would be an improvement. Nonetheless, the paragraph vector model appears to be a more natural baseline for comparison, with matched train and test conditions, and its omission warrants clarification in the paper.
Ultimately, the "skip-thoughts" model is an ingenious application of the sequence-to-sequence RNN framework, but concerns regarding the experimental evaluation, particularly the data mismatch between baselines and the proposed approach, must be addressed to fully appreciate its potential.