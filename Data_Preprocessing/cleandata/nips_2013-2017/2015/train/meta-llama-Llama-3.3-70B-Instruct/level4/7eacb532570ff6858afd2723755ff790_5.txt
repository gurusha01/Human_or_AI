This paper introduces a dueling bandit approach, assuming arms are ranked according to the Plackett-Luce distribution. Unlike typical bandit problems that focus on regret analysis, this work presents sample complexity, specifically the number of samples needed to achieve an \epsilon-optimal arm with a probability of 1 - \delta. However, the paper lacks a discussion on the regret bounds of the proposed algorithms and a comparison with existing dueling bandit algorithms, making it unclear how the proposed method's regret bound stacks up against others.
A significant limitation of this study is its reliance on synthetic data for experiments, which raises questions about the applicability of the Plackett-Luce distribution assumption to real-world problems. The experimental setup appears overly simplistic and does not adequately demonstrate the practical utility of the proposed approach.
Additional observations include:
- Line 202: The term "end" following [M] is unclear.
- Line 7, Algorithm 2: It seems "N" should be denoted as "\hat{N}" for consistency.
- The presentation of an algorithm in the supplementary material, with its analysis in the main paper, seems counterintuitive; ideally, the algorithm should be in the main text and the analysis in the supplement. Overall, while the paper proposes another method for dueling bandits, the experimental support is weak, and it does not convincingly demonstrate how the proposed method outperforms existing approaches.