The authors introduce a variational objective that aims to minimize the Kullback-Leibler divergence to a "population posterior", defined as the expected value of the traditional posterior under an empirical distribution. This formulation is then utilized to develop a streaming variational algorithm, where the objective function is parameterized by an empirical distribution.
The introductory section appears to suggest that online Bayesian posteriors converge to a point mass; however, asymptotically, is this not a correct consequence of consistency? The key point seems to be that this convergence can occur prematurely. To what extent is this premature convergence attributed to the variational posterior approximation, which tends to underestimate uncertainty, rather than the Bayesian inference itself?
In Equation (3), should "min" be replaced with "argmin"? Overall, this is an excellent treatment of streaming Bayesian inference using variational methods, with strong experimental results and an elegant formalism.