This manuscript addresses the challenge of retrieving natural sentences that describe a given sequence of images and generating a coherent sequence of sentences, a significant problem in computer vision and pattern recognition. Notably, this task bears resemblance to describing video content with multiple sentences, as seen in prior work (e.g., [Rohrbach+ 2014]), where video can be decomposed into key frames representing individual shots. However, the authors fail to cite relevant studies in this area.
The paper is well-structured and clear, with a succinct problem statement and effective solution leveraging solid techniques. The proposed CRCN neural network model successfully captures relationships between sequences of natural sentences and images, boasting a technically sound architecture that effectively models discourse relationships using entity-based coherence models.
Although the approach to generating sentence sequences is reasonable, it appears somewhat simplistic, relying on direct associations between sentences and similar training images, followed by straightforward concatenation in the order of corresponding images.
In Section 4.1, the authors mention reusing and collecting new data using a method from [11], but the dataset and data collection details remain undisclosed, mirroring the methodology in [11]. This lack of transparency raises concerns about anonymity, suggesting a potential overlap in authorship between this paper and [11].
Furthermore, Equation (2) may contain an error, implying that st can be derived solely from ot, which would indicate a lack of full connectivity in the network. Despite the model's technical soundness and ability to describe discourse relationships, the sentence generation method for image streams seems ad-hoc. Considering these points, this paper could be suitable for acceptance as a poster presentation in its current form.