The paper addresses two key issues: 
(1) A comparison of the social welfare achieved by players utilizing regret minimization algorithms to the optimal welfare, and 
(2) The potential for improved regret bounds when all players employ a regret minimization algorithm.
Previous research has demonstrated that social welfare converges to the optimal welfare at a rate of 1/T for zero-sum games. However, the convergence rate for general non-zero sum games remained an open question. The authors demonstrate that, under certain smoothness assumptions on the game (as introduced by Roughgarden), the same convergence rate can be achieved when all players use a regret minimization algorithm possessing the RVU property. Notably, they show that the optimistic mirror descent algorithm of Rakhlin and an optimistic variation of FTRL (OFTRL) with a recency bias both exhibit this property.
Regarding the second issue, the authors establish that the regret of each individual player is O(T^{1/4}) when using their variation of FTRL, a finding supported by their simulation results. Furthermore, they enhance their algorithm by incorporating a doubling trick, which ensures that if all players use OFTRL, their regret grows at a rate of T^{1/4}, without exceeding O(T^{1/2}) even when players face adversarial rewards.
The paper is exceptionally well-written and tackles an intriguing problem. The generality of the results, particularly with respect to the RVU property, is a notable strength. The work focuses on bounds for regret minimization algorithms in games, where typical regret bounds are O(\sqrt{T}), assuming a completely adversarial opponent. However, in a game scenario where all players use regret minimization algorithms, the authors investigate whether better rates can be achieved. They demonstrate that a regret of O(T^{1/4}) is attainable for general games, offering a significant improvement.