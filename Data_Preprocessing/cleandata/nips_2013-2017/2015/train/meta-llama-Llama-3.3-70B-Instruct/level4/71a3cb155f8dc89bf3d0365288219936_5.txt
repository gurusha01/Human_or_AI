While the irrepresentability condition (IC) has been extensively utilized in existing literature [29,30], a brief discussion on its application to learning Gaussian MRFs, similar to [30], would be beneficial to include.
The authors should provide clarification on the dependence of Theorem 3.5 on the parameter \alpha in condition (3.4), as well as a brief explanation on how random initialization satisfies this condition. It is possible that the assumption is straightforward, with \alpha being a multiplicative factor in the result of Theorem 3.5, but this is not immediately clear from the paper.
Furthermore, the authors are encouraged to explore and discuss potential extensions of their methodology to their specific problem and other machine learning applications.
There appear to be two statements that suggest learning a simple Gaussian MRF with one sample can achieve consistency. Specifically, Line 060 states that the "statistical rate of convergence in Frobenius norm... is minimax-optimal," and Line 066 claims that the "alternating minimization algorithm can achieve estimation consistency in Frobenius norm even with only one tensor sample." It seems that either these statements require further qualification or one of them may be incorrect, potentially due to assumptions or proof components that rely on sufficiently large sample sizes or dimension-independent constants.
Several minor errors were noted, including: Line 177, which should read "is [a] bi-convex problem"; Line 181, which should be revised to "by alternatively updat{e}[ing] one precision matrix with [the] other matrices fixed"; and Line 190, which should state "corresponds to estimating [a] vector-valued Gaussian graphical model."
The paper presents strong theoretical results on sample complexity and a well-designed experimental setup.