I have one major reservation regarding the proof of Theorem 2, specifically the decision to make lambda and L dependent on d. What consequences arise from this selection, and does it reflect a practical scenario? 
As a potentially relevant reference, the authors may find the following paper useful, which examines the identical loss function presented in equation (4) and proposes an efficient algorithm for its global solution: "Scalable Metric Learning for Co-embedding" by Mirzazadeh et al., published in ECML in 2015. The manuscript is well-structured and offers innovative theoretical perspectives on the sample complexity of metric learning, as well as the benefits of added regularization. The authors provide a thorough theoretical examination of the topic, covering both general and specific cases, and the empirical results are similarly comprehensive yet concise, further substantiating these findings on real-world datasets.