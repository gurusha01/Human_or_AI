This paper presents a novel approach to generating a sequence of consistent sentences that describe a sequence of images from blog posts. The proposed method involves retrieving the top 5 most similar images and associated sentences from the training set for each query image. The key contribution of this work lies in its ability to select the most relevant sentences for the query image sequence, thereby providing a coherent description. To achieve this, sentences are first embedded into vectors, and then a bidirectional LSTM is used to model the sequence of sentences. The output of the bidirectional LSTM is processed through a ReLU and fully connected layer, followed by a compatibility score calculation between the image and sentence. Additionally, a local coherence model is incorporated to ensure compatibility between sentences.
The strengths of this paper include its proposal of a novel and effective architecture for retrieving coherent sentences for an image sequence. The authors also provide a comprehensive evaluation, including quantitative, qualitative, and human assessments, which demonstrate the superiority of their approach over several baselines that do not utilize coherence. Furthermore, they conduct an ablation experiment to evaluate the impact of removing the coherence model. The authors have also committed to releasing the source code and dataset, enhancing the paper's reproducibility and usefulness to the research community.
However, several weaknesses and areas of clarification have been identified. Firstly, the claim that there is no mechanism for coherence between sentences in [5] is disputed, as [5] does predict an intermediate semantic representation of videos that is coherent across sentences by modeling the topic of the multi-sentence description. Secondly, there appears to be a discrepancy between Figure 2b and the description in 3.3/Equation (2), specifically regarding the connections between the fully connected layer and the sentences. Thirdly, it is suggested that the Meteor metric (http://www.cs.cmu.edu/~alavie/METEOR/) would be a more suitable choice for automatically evaluating the generated sentences, especially when there is only a single reference sentence, rather than the currently used BLEU metric. Fourthly, the application of two linear functions in Equation (2) (W{f2}, W{f1}) sequentially is questioned, as the benefit of this approach is unclear, and an ablation study to demonstrate its effectiveness would be beneficial. Fifthly, the use of the same parameters for the fully connected layers in both the BRNN output (o_t) and the local coherence model q (Equation 2) requires clarification. Lastly, it is unclear whether the paragraph vector [16] is fine-tuned or kept fixed during the training process.
Following the rebuttal, the authors have successfully addressed the issues related to formulation, evaluation, and related work, leading to a recommendation for acceptance. To further enhance the paper, the promised changes should be incorporated, and the clarification regarding the paragraph vector [16] should be explicitly addressed in the final version. The proposed model offers a compelling approach to retrieving coherent sentences for an image stream, and with the necessary clarifications, it has the potential to be a convincing and impactful paper.