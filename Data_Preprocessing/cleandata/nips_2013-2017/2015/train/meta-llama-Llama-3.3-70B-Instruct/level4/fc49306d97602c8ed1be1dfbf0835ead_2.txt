Given the modest absolute gains observed in [18] over the two-stage approach, it is unclear whether a combined approach would significantly outperform two-stage methods, thereby rendering experimental validation unnecessary. Considering the widespread adoption of word embeddings in modern NLP systems, comparisons to natural approaches leveraging these embeddings are essential.
In addition to applying CCA to separate word embeddings, a comparison to matching techniques that simply identify the nearest candidate word in an existing multilingual word embedding system is warranted. As the proposed technique can be viewed as an approach to multilingual word embedding, drawing comparisons to the existing literature in this area is crucial, in my opinion.
A revised version of this paper, featuring thorough experimental comparisons to related approaches, would be worthy of publication if the results are promising. Although the novelty of this work over [18] and [19] may not be substantial, its application to a new domain is a sensible contribution. However, without access to these results, it is challenging to enthusiastically endorse the paper. I have increased my score from 4 to 5.
The paper presents a method for cross-domain matching by embedding instances as sets and maximizing the likelihood of a softmax model based on MMD distances between those sets, given limited should-match training pairs.
An alternative algorithm for this problem involves performing word2vec-type word embeddings in each domain and then applying kernel CCA to this representation. This approach is highly natural and addresses the paper's concerns about kernel CCA, yet it is not mentioned.
In general, the paper appears to overlook the vast recent literature on embeddings. A relevant search yielded the following five highly relevant papers in the NLP domain:
- Bollegala, Maehara, and Kawarabayashi. Unsupervised Cross-Domain Word Representation Learning. ACL 2015.
- Shimodaira. A simple coding for cross-domain matching with dimension reduction via spectral graph embedding. arXiv:1412.8380.
- Yang and Eisenstein. Unsupervised Domain Adaptation with Feature Embeddings. ICLR 2015 workshop (arXiv:1412.4385).
- Al-Rfou, Perozzi, and Skiena. Polyglot: Distributed Word Representations for Multilingual NLP. arXiv:1307.1662.
- Nguyen and Grishman. Employing Word Representations and Regularization for Domain Adaptation of Relation Extraction. ACL 2014 short paper.
Furthermore, the numerous deep learning papers from last year on automatic caption generation are also relevant. While some of these techniques rely on more paired training examples than used in this paper, a more thorough evaluation or at least a brief discussion of this distinction is necessary, given the use of some paired examples in this work.
The paper's techniques are intriguing, despite being somewhat incremental over previous similar papers. However, without discussing their relationship to the relevant literature, it is impossible for a reader to assess how the learned embeddings compare to those of actual competing systems, rather than straw-man baselines.
The method also appears to be challenging to scale, similar to the previous two papers in this series. This issue is not addressed, despite the evaluation being conducted on small datasets. The proposed method is interesting, but its advantages over very natural alternatives (not discussed in the paper) are unclear, and the vast literature of related work is barely touched upon.