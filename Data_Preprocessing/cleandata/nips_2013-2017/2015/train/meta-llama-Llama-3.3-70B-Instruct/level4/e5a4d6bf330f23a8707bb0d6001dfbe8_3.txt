Overall, I give this paper a score of 9. The authors extend and generalize the concept of implicit exploration, initially proposed in "Efficient learning by implicit exploration in bandit problems with side observations" (NIPS 2014), which involves utilizing a lower confidence bound to estimate the true loss. Notably, the current analysis provides regret bounds with high probability, whereas previous work focused on expected regret.
The paper's primary contribution is demonstrating the superiority of high-confidence algorithms that leverage implicit exploration. The key result is a general concentration inequality for the sum of losses, presented in Lemma 1. This inequality is then applied to three variants of non-stochastic bandit problems: the multi-armed bandit (MAB) problem with expert advice, tracking the best sequence of arms, and the MAB problem with side information. The analysis reveals an improvement of approximately a factor of 2 in the pre-factors of the regret bounds for these problems. Furthermore, although Exp3.P with explicit exploration is outperformed by standard Exp3 in practice, Exp3-IX outperforms Exp3 in the single experiment conducted, exhibiting less sensitivity to the value of eta and providing more robust regret estimation.
It is worth noting that Exp3 is not suitable for the switching bandit problem, where the best arm changes over time. A comparison between Exp3.S and Exp3.SIX in such scenarios would be interesting. Additionally, there is a minor typo in Algorithm 1, where \hat should be replaced with \sim, as stated in the proof of Lemma 1. Overall, this paper is outstanding, with clear and gradual analysis, relevant theoretical results that can be applied to a broad range of MAB problems, and an experiment that highlights the practical benefits of implicit exploration. The writing is excellent, making the paper a valuable contribution to the machine learning community.