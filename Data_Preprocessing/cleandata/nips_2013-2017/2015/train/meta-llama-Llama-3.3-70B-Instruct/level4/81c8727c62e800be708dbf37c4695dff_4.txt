This article presents a theoretical analysis of non-regularized supervised metric learning formulations, exploring two primary frameworks: distance-based and classifier-based. The authors derive PAC-style bounds for both frameworks, examining the difference between true and empirical risk, as well as the impact of dimensionality on the learned metric. They also investigate regularized formulations using the Frobenius norm and demonstrate the benefits of adding a regularization term through empirical evaluations of ITML and LMNN.
The paper is well-written and provides novel insights into metric learning. However, some improvements can be suggested. The proof of Lemma 2 should be revised to be independent of dimension, and comments should be added to connect Theorems 1 and 2, highlighting their implications on the dependence on dimensionality. Additionally, the authors should mention that the results of Jin et al. (NIPS 2009) can be extended to replace the dependency on dimension with a term based on data concentration, applicable to any distribution and convex regularized formulation.
One of the paper's claims is to generalize previous results to non-convex loss functions, but this comes with the caveat that there is no guarantee of finding the optimal model, making the results less informative for practical applications. The authors focus on the dependence on representation dimension, but it would be interesting to consider the true rank of the matrix instead, allowing for connections to metric learning based on Cholesky decomposition and low-rank regularization.
The bibliography lacks references on learning linear classifiers from good similarity functions, such as Balcan et al. (COLT 2008) and Bellet et al. (ICML 2012). The experiments, although demonstrating the benefits of regularization, are limited in scope, with only three datasets and low-dimensional data. The scaling of the x-axis differs between figures, and it would be interesting to see the results beyond the 120/150 mark for Iris and Ionosphere. A comparison with the accuracy obtained using the rank-1 matrix alone in the distance would also be valuable.
Other comments include correcting minor errors, such as the consideration of binary classification despite the loss function allowing for multiclass settings, and the inconsistent margin values in the true and empirical risk definitions. The authors may also want to explore extending their results to other regularization norms and examining the evolution of the bounds with data and dimension.
Overall, the paper provides interesting and original bounds for metric learning, with a thorough analysis of the hypothesis space and the classifier-based setting. However, the experimental evaluation is limited, and some discussions lack precision. The pros of the paper include its novel contributions, interesting analysis, and potential for future research, while the cons include the limited experimental evaluation and lack of discussion on certain topics.