This manuscript proposes a scene labeling approach utilizing Recurrent Convolutional Neural Networks, where the output of a convolutional layer is fed back as an additional input to the same layer, achieved by replicating the layer multiple times. The network's input consists of the original image and its downscaled versions to better capture contextual information.
The methodology is evaluated on two datasets and compared to existing methods, yielding notable improvements in accuracy and impressive computational efficiency, as the algorithm can be executed entirely on a GPU.
I found the paper to be of interest due to its relevance to current trends, but I have two primary concerns: the text is challenging to comprehend, with ambiguous sentences, such as the use of the passive voice in the introduction's final paragraph, which required careful interpretation to discern the authors' intention, specifically the adoption of a multi-scale RCNN version. Furthermore, although the results are intriguing, the contribution appears limited, as it essentially applies RCNN, previously used in computer vision problems, to image labeling.
Additional minor comments include: in Section 3, explicitly defining RCL as Recurrent Convolutional Layer and clarifying LRN would enhance readability; the notation in Eq (3) should be corrected to g(Zijkz) instead of sigma(zijk); and the discussion of gamma may not be warranted. The results obtained with gamma = 0 are inferior to those with gamma = 1, which is the standard approach, and this outcome is not surprising, as the network could adapt to utilize large values for w_k^rec if a small gamma were beneficial. Overall, the paper presents an interesting method for scene labeling based on Recurrent Convolutional Neural Networks, but the text's clarity and the contribution's significance could be improved.