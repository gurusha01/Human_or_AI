1) It would be beneficial for the authors to acknowledge in section 3.2 that the presented algorithm bears a strong resemblance to those described in references [14] and [15], specifically in its final form, which appears to be a hybrid of the two. This similarity extends to the use of identical approximations in its derivation.
2) The discussion's opening paragraph may be misinterpreted as suggesting that the "constrain-then-train" approach, where a weight distribution is learned and then sampled to generate a fixed network ensemble at test time, is a novel contribution. However, this methodology was previously proposed and evaluated in reference [15].
3) The introduction's last paragraph contains a misleading claim regarding novelty (i). The "expectation backpropagation" algorithm outlined in reference [15] is, in fact, a backpropagation update rule that utilizes expectation propagation, designed for binary neurons with binary weights. Moreover, it can be adapted to accommodate constrained connectivity, which diminishes the novelty of the current work's claims in this regard.
4) The cited state-of-the-art results appear to be incorrect. According to the available information, the best previous results for binary weights and neurons were obtained using the algorithm described in reference [15], which was tested on the MNIST dataset without data augmentation, similar to the approach taken in this paper.
%%% Edited after author's feedback: The paper presents robust and novel experimental results, demonstrating significant advancements in terms of accuracy and power consumption. Nevertheless, upon closer examination, it becomes apparent that both the overarching methodology and the algorithm itself are not as groundbreaking as asserted in the paper.