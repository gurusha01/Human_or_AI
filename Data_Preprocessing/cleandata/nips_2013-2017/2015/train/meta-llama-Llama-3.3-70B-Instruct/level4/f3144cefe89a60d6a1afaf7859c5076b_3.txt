Summary: The authors propose a Poisson gamma hierarchy for modeling counts data, leveraging a gamma distribution hierarchy and novel augmentation techniques to develop a tractable Gibbs sampler despite non-conjugacy, which is then combined with a Poisson likelihood to demonstrate its utility.
The paper is technically sound, addressing the interesting problem of extracting unsupervised multilayered representations of data and providing an alternative approach for counts data.
Detailed comments:
1) The gamma units' induced nonlinearities seem to encompass the linear regime of rectified linear nonlinearity, recovering the linear regime in expectation when the expected rate parameter is 1. This connection to recent work [1] on using rectified linear nonlinearities in RBMs is noteworthy and warrants further discussion in the text.
2) The inference procedure's scalability is a concern, as it appears to involve running 1000 Gibbs sampling iterations after adding each new layer, sweeping over all network variables. This approach may be challenging to scale to deeper architectures. The authors should provide the training time for the 5-layer networks used in multi-class classification and discuss these computational issues.
3) The external classification comparisons to DocNADE and over-replicated softmax are flawed, as the competing algorithms are trained on different vocabularies, making the classification numbers incomparable. The qualitative analysis paragraph is uninformative and should be revised. The authors claim that the discovered topics specialize as one traverses the hierarchy downwards but provide limited evidence to support this claim. It would be beneficial to make the per-layer discovered topics available in a supplement, allowing readers to explore them more closely. Additionally, the authors' claim of generating interpretable synthetic documents from the trained network lacks supporting evidence and could be substantiated by providing examples in an appendix.
[1] Nair, Vinod, and Geoffrey E. Hinton. "Rectified linear units improve restricted boltzmann machines." Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010. Overall, this is an interesting paper, but the sloppy experimental evaluations prevent a higher score.