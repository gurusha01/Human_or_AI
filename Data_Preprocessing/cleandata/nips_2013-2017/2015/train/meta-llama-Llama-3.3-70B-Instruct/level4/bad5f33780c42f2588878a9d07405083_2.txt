Summary
This paper explores a scenario where an analyst utilizes a training set and a hold-out set from the same distribution to construct methods and make crucial decisions by repeatedly checking the hold-out set. The key findings, as presented in Theorems 9 and 10, demonstrate that overfitting to the hold-out set can be avoided if the analyst employs the methods introduced in the paper. This builds upon earlier results that were based on differentially private methods, offering a significant extension.
Positive Points
One of the exciting aspects of these results is their ability to accommodate a wide range of functions without a priori restrictions, such as limiting the class of functions to those with a limited VC dimension. This is noteworthy because standard uniform concentration inequalities for VC classes would otherwise suffice, as highlighted in lines 226-232 of the paper.
Another interesting point is the paper's expansion beyond differentially private methods to include algorithms whose output can be described by a small number of bits. This generalization is facilitated by the concept of (approximate) max-information, which allows for the composition of multiple algorithms in a sequence, as discussed in lines 234-243. Unlike algorithmic stability notions closely related to generalization, max-information provides a more flexible framework.
The high-level discussion in the paper is generally well-written, contributing to its clarity and readability.
Negative Points
Despite the aforementioned positive aspects, significant concerns regarding Theorem 9, one of the main results, lead to the conclusion that the paper may require substantial revisions before acceptance.
Main Concerns
Regarding Theorem 9, several issues arise:
1. The theorem's deterministic statement "for all i such that $ai \neq \perp$" is problematic because $ai \neq \perp$ is random, depending on adaptively chosen functions $\phi_i$, data randomness, and algorithmic randomness.
2. The in-probability statement bounds the probability that $ai$ deviates significantly from the expectation of $\phii$ for a fixed $i$. However, when the analyst makes adaptive decisions based on the algorithm's output, $ai$ must be close to the expectation of $\phii$ for all $i$ simultaneously to avoid incorrect analysis.
3. The claim in lines 119-120 of the introduction, suggesting that the number of queries $m$ can be exponential in $n$ as long as the budget $B$ is at most quadratic in $n$, appears unsubstantiated. Specifically, the result does not seem to deteriorate with $m$, potentially due to the issue mentioned in point 2. For the result to be useful, one would expect $\tau$ to approach 0 with $n$ (e.g., $\tau \sim 1/\sqrt{n}$). However, letting $B$ be quadratic in $n$ and $\tau$ approach 0 would make $\beta$ go to infinity, rendering the result vacuous.
Regarding the experiments, the construction of a classifier using only weights +1 and -1 based on the signs of correlations seems arbitrary. It is unclear if there is a plausible scenario where someone might do this in practice.
Minor Remarks
The authors acknowledge in their proof of Theorem 23 in the additional material that the extension to algorithms whose output can be described using a small number of bits can also be obtained through a union bound argument. This is hinted at in lines 171-179 of the introduction but may not be entirely clear from the discussion.
A Potential Strengthening
The results are technically based on a change of measure from the joint distribution $P$ of a sample and an algorithm's output on that sample to the independent product distribution $Q$ of the two (Theorem 4). If $P$ and $Q$ are sufficiently close in terms of "max-information," the sample can essentially still be treated as fresh even after observing the algorithm's output.
It might be of interest to note that, at least for $\beta = 0$, the bound on max-information can be relaxed. The max-information can be interpreted as the Renyi divergence $D\alpha(P||Q)$ of order $\alpha = \infty$. Since Renyi divergence is increasing in $\alpha$, requiring a bound on $\alpha = \infty$ is the strongest requirement. Theorem 4 states that a change of measure from $P$ to $Q$ is possible if $D\infty(P||Q)$ is sufficiently small. However, it is actually possible to change measures under the weaker requirement that $D_\alpha(P||Q)$ is small for any $\alpha > 1$. For $\alpha = 1$, this recovers regular mutual information, which, as the authors point out, is not strong enough to allow a change of measure.
The proof can be seen as a special case of Lemma 1 of [2], where letting $X$ be the indicator random variable that is 1 if $(S, A(S)) \in \mathcal{O}$ and 0 otherwise, and reading Lemma 1 with $EP[X]$ instead of $LP(h, f)$, $EQ[X]$ instead of $LQ(h, f)$, and $M = \max X = 1$, yields:
\[EP[X] \leq (2^{D\alpha(P||Q)} \cdot E_Q[X])^{(\alpha-1)/\alpha} \cdot M^{1/\alpha}\]
which is equivalent to
\[P[(S, A(S)) \in \mathcal{O}] \leq (2^{D_\alpha(P||Q)} \cdot Q[(S, A(S)) \in \mathcal{O}])^{(\alpha-1)/\alpha}\]
As $\alpha \rightarrow \infty$, this recovers Theorem 4, but the result also holds for smaller $\alpha$.
Minor Issues (Typos, etc.)
- Line 114: "training set" should likely be "hold-out set."
- Theorem 4: Two issues are noted. First, "$k = I\infty^\beta(S;A(S)) = k$" should be "$I\infty^\beta(S;A(S)) \leq k$?"
- Theorem 8: The sentence "Let $Y = ...$ on input $S$" can be removed.
- Lines 314-323: The bounds mentioned actually bound the probability that the empirical estimate is more than $\tau$ away from the true accuracy.
In spite of many positive points, significant concerns about Theorem 9, one of the main results, suggest that the paper may need a serious revision before it could be accepted.