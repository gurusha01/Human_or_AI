This paper presents a distributed stochastic gradient learning approach that accommodates infrequent synchronization, addressing the issue of non-convexity which existing algorithms struggle with due to their sensitivity to distant synchronizations. The proposed method's robustness to parameter value discrepancies across nodes promotes exploration, leading to improved solutions.
The simplicity of the algorithm is notable, and it appears to perform well in the experimental evaluations, which include comparisons with various existing algorithms across multiple tau values. However, it would be beneficial to see experiments involving architectures beyond CNNs, as the discussion of deep learning lacks specificity.
A significant concern with this paper is its limited coverage of existing method implementations, despite revisiting established techniques, such as those discussed in Mark Schmidt's "Notes on Big-n Problems". While these methods are referenced in the introduction, in-depth analysis is primarily confined to ADMM, with scarce examination of other relevant implementations.
Given the potential significance of positive results in distributed stochastic optimization for nonconvex losses, it is crucial for the authors to provide a more comprehensive review of the existing literature, moving beyond mere references. Although the algorithm demonstrates superior performance to existing distributed techniques, its comparison to non-deep learning oriented distribution methods, which have been extensively studied in the optimization literature, is lacking, warranting further investigation.