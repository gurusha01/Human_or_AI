This paper proposes a new algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for parallelizing stochastic gradient descent in deep learning settings. The main claim of the paper is that EASGD provides fast convergent minimization while outperforming existing methods, such as DOWNPOUR, in practice. The authors also propose asynchronous and momentum-based variants of the algorithm.
The paper is well-written and provides a clear explanation of the algorithm and its variants. The authors provide a thorough analysis of the stability of the asynchronous EASGD algorithm in the round-robin scheme and compare it with the ADMM algorithm. The experimental results demonstrate the effectiveness of EASGD and its variants in training deep neural networks on benchmark datasets, such as CIFAR-10 and ImageNet.
The strengths of the paper include:
* The proposal of a new algorithm that addresses the communication constraints in parallel computing environments
* A thorough analysis of the stability of the asynchronous EASGD algorithm
* Experimental results that demonstrate the effectiveness of EASGD and its variants
The weaknesses of the paper include:
* The paper assumes a specific parallel computing environment, which may not be applicable to all scenarios
* The analysis of the stability of the asynchronous EASGD algorithm is limited to the round-robin scheme
* The experimental results are limited to two benchmark datasets and may not be generalizable to other datasets
Arguments for acceptance:
* The paper proposes a new algorithm that addresses a significant problem in deep learning
* The analysis of the stability of the asynchronous EASGD algorithm is thorough and provides valuable insights
* The experimental results demonstrate the effectiveness of EASGD and its variants
Arguments against acceptance:
* The paper assumes a specific parallel computing environment, which may limit its applicability
* The analysis of the stability of the asynchronous EASGD algorithm is limited to the round-robin scheme
* The experimental results are limited to two benchmark datasets and may not be generalizable to other datasets
Overall, I believe that the paper is well-written and provides a significant contribution to the field of deep learning. The proposal of a new algorithm that addresses communication constraints in parallel computing environments is a valuable contribution, and the analysis of the stability of the asynchronous EASGD algorithm provides valuable insights. The experimental results demonstrate the effectiveness of EASGD and its variants, and I believe that the paper is worthy of acceptance. However, I would like to see more experiments on different datasets and scenarios to further demonstrate the generalizability of the algorithm.