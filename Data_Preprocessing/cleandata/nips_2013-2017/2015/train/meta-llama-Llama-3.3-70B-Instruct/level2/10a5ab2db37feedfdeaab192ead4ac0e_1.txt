This paper presents a novel approach to training neural networks that can be deployed on neuromorphic hardware with spiking neurons and low-precision synapses. The authors propose a "constrain-then-train" approach, where they first constrain the network to provide a direct representation of the deployment system and then train within those constraints. This approach allows for the use of backpropagation to train networks that can be mapped to neuromorphic hardware, achieving high performance and energy efficiency.
The paper's main claims are well-supported by theoretical analysis and experimental results on the MNIST dataset. The authors demonstrate that their approach can achieve high accuracy (99.42%) and low energy consumption (108 Î¼J per classification) on a TrueNorth chip. The paper also provides a thorough discussion of the network topology, training methodology, and mapping of the training network to the deployment network.
The paper's strengths include its novelty, clarity, and significance. The approach presented is a significant improvement over existing methods, and the authors provide a clear and well-organized explanation of their methodology. The paper also demonstrates the potential for real-world applicability, particularly in the context of low-power, scalable brain-inspired systems.
However, there are some limitations to the paper. The authors acknowledge that their approach requires a customized training rule, which may limit its applicability to other domains. Additionally, the paper focuses primarily on the MNIST dataset, and it would be beneficial to see results on more complex datasets.
Overall, I would argue in favor of accepting this paper. The paper presents a significant contribution to the field of neuromorphic computing, and its approach has the potential to enable the development of low-power, scalable brain-inspired systems. The paper is well-written, clear, and provides a thorough explanation of the methodology and results.
Arguments pro acceptance:
* Novel approach to training neural networks for neuromorphic hardware
* High performance and energy efficiency demonstrated on MNIST dataset
* Clear and well-organized explanation of methodology and results
* Potential for real-world applicability in low-power, scalable brain-inspired systems
Arguments con acceptance:
* Customized training rule may limit applicability to other domains
* Focus primarily on MNIST dataset, would benefit from results on more complex datasets
* Limited discussion of potential limitations and challenges of the approach.