This paper presents a novel method for training recurrent neural networks to act as near-optimal feedback controllers for physically consistent interactive character control. The authors combine supervised learning with trajectory optimization, using a neural network to learn from the optimizer and generate similar behaviors online. The method is able to generate stable and realistic behaviors for a range of dynamical systems and tasks, including swimming, flying, biped, and quadruped walking with different body morphologies.
The paper's main claims are well-supported by theoretical analysis and experimental results. The authors provide a clear and detailed explanation of their method, including the use of interleaving supervised learning with trajectory optimization, injecting noise during training, and using the trajectory optimizer to obtain optimal feedback gains. The results demonstrate the effectiveness of the method in generating realistic and purposeful behaviors for various characters and tasks.
The paper is well-written and well-organized, making it easy to follow and understand. The authors provide a thorough review of related work and clearly explain the differences between their approach and previous methods. The use of figures and tables helps to illustrate the results and make the paper more engaging.
One of the strengths of the paper is its ability to generate realistic behaviors for a range of characters and tasks without requiring motion capture or task-specific features or state machines. The method is also able to handle unexpected changes in the task specification and can be used for real-time interactive control.
The paper has some limitations, including the need for a large amount of computing resources and the potential for overfitting. However, the authors address these limitations by using a distributed training architecture and injecting noise during training to prevent overfitting.
Overall, the paper presents a significant contribution to the field of character animation and control, and its results have the potential to be used in a variety of applications, including robotics and computer graphics. The paper is well-written, well-organized, and provides a clear and detailed explanation of the method and results.
Arguments pro acceptance:
* The paper presents a novel and effective method for training recurrent neural networks to act as near-optimal feedback controllers.
* The method is able to generate realistic and purposeful behaviors for a range of characters and tasks.
* The paper provides a clear and detailed explanation of the method and results.
* The authors address the limitations of the method and provide solutions to prevent overfitting.
Arguments con acceptance:
* The method requires a large amount of computing resources, which may be a limitation for some researchers.
* The paper could benefit from more comparisons with other methods and more analysis of the results.
* The method may not be suitable for all types of characters and tasks, and more research is needed to fully explore its potential. 
Quality: 9/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results.
Clarity: 9/10
The paper is well-written and well-organized, making it easy to follow and understand.
Originality: 8/10
The paper presents a novel method for training recurrent neural networks to act as near-optimal feedback controllers, but it builds on previous work in the field.
Significance: 9/10
The paper presents a significant contribution to the field of character animation and control, and its results have the potential to be used in a variety of applications.