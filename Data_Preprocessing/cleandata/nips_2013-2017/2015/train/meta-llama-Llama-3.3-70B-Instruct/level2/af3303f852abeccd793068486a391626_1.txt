This paper proposes a novel approach to Bayesian parameter estimation for deep neural networks, which is a crucial problem in settings with limited data or where accurate posterior predictive densities are required. The authors introduce a method called "Bayesian dark knowledge" that combines online Monte Carlo methods, specifically stochastic gradient Langevin dynamics (SGLD), with model distillation to approximate the posterior predictive distribution. The main claim of the paper is that this approach outperforms recent methods based on expectation propagation (EP) and variational Bayes (VB) in terms of log likelihood scores on the test set.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and the proposed method. The technical details are well-explained, and the experimental results are thorough and convincing. The authors demonstrate the effectiveness of their approach on several classification and regression problems, including a toy 2D classification problem, MNIST, and the Boston housing dataset.
The strengths of the paper include:
* The proposed method is simple to implement and scalable to large datasets.
* The authors provide a thorough comparison with recent methods based on EP and VB, demonstrating the superiority of their approach.
* The experimental results are well-presented and easy to follow.
The weaknesses of the paper include:
* The authors could provide more discussion on the limitations of their approach, such as the potential for mode-dropping in the distillation process.
* The paper could benefit from more analysis on the computational cost and memory requirements of the proposed method.
* Some of the experimental results, such as the toy 2D classification problem, may not be representative of real-world scenarios.
Overall, the paper is well-written, and the proposed method is a significant contribution to the field of Bayesian neural networks. The authors demonstrate the effectiveness of their approach on several problems, and the results are convincing. However, the paper could benefit from more discussion on the limitations and potential extensions of the proposed method.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to Bayesian parameter estimation for deep neural networks.
* The authors provide a thorough comparison with recent methods based on EP and VB.
* The experimental results are well-presented and convincing.
Arguments con acceptance:
* The paper could benefit from more discussion on the limitations of the proposed approach.
* Some of the experimental results may not be representative of real-world scenarios.
* The paper could benefit from more analysis on the computational cost and memory requirements of the proposed method.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, providing more discussion on the limitations of their approach and more analysis on the computational cost and memory requirements. Additionally, the authors could consider adding more experimental results on real-world problems to demonstrate the effectiveness of their approach in practice.