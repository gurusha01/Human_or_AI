This paper proposes a novel approach to training recurrent neural networks (RNNs) for sequence prediction tasks, called Scheduled Sampling. The main claim of the paper is that the current approach to training RNNs, which maximizes the likelihood of each token in the sequence given the current state and the previous token, can lead to errors that accumulate quickly along the generated sequence. The authors argue that this is because the training process is different from the inference process, where the previous token is unknown and must be generated by the model itself.
The paper proposes a curriculum learning strategy to bridge the gap between training and inference, where the model is gradually forced to deal with its own mistakes during training. The authors evaluate their approach on several sequence prediction tasks, including image captioning, constituency parsing, and speech recognition, and show that it yields significant improvements in performance.
The strengths of the paper include its clear and well-motivated presentation of the problem, its thorough evaluation of the proposed approach on multiple tasks, and its discussion of related work. The paper also provides a detailed description of the proposed approach, including the scheduled sampling mechanism and the different schedules that can be used to decrease the probability of using the true previous token during training.
The weaknesses of the paper include the fact that the proposed approach is not entirely new, as similar ideas have been proposed in the context of reinforcement learning and parsing tasks. Additionally, the paper could benefit from a more detailed analysis of the results, including a discussion of the limitations of the proposed approach and potential avenues for future work.
Overall, I believe that the paper makes a significant contribution to the field of sequence prediction with RNNs, and that the proposed approach has the potential to improve the performance of RNNs on a wide range of tasks. The paper is well-written and easy to follow, and the authors provide a clear and concise presentation of their ideas.
Arguments for acceptance:
* The paper proposes a novel approach to training RNNs for sequence prediction tasks that addresses a significant problem in the field.
* The paper provides a thorough evaluation of the proposed approach on multiple tasks, including image captioning, constituency parsing, and speech recognition.
* The paper discusses related work and provides a clear and well-motivated presentation of the problem.
Arguments against acceptance:
* The proposed approach is not entirely new, as similar ideas have been proposed in the context of reinforcement learning and parsing tasks.
* The paper could benefit from a more detailed analysis of the results, including a discussion of the limitations of the proposed approach and potential avenues for future work.
Rating: 8/10
Recommendation: Accept with minor revisions. The paper is well-written and makes a significant contribution to the field, but could benefit from a more detailed analysis of the results and a discussion of the limitations of the proposed approach.