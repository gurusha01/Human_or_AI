This paper proposes Rectified Factor Networks (RFNs) as a novel approach to constructing sparse, non-linear, and high-dimensional representations of input data. The authors claim that RFNs can efficiently identify rare and small events in the input, have low interference between code units, and provide a small reconstruction error while explaining the data covariance structure. The RFN learning algorithm is a generalized alternating minimization method derived from the posterior regularization method, which enforces non-negative and normalized posterior means.
The paper provides a clear and well-structured introduction to the problem, related work, and the proposed approach. The authors demonstrate the effectiveness of RFNs through extensive experiments on various benchmark datasets, including comparisons with other unsupervised methods such as autoencoders, RBMs, factor analysis, ICA, and PCA. The results show that RFNs yield sparser codes, capture the data's covariance structure more precisely, and have a significantly smaller reconstruction error.
The paper also explores the use of RFNs as a pretraining technique for deep networks on different vision datasets, where RFNs outperform RBMs and autoencoders. Additionally, the authors apply RFNs to gene expression data from two pharmaceutical drug discovery studies, detecting small and rare gene modules that were missed by other unsupervised methods.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem and the proposed approach
* Extensive experiments demonstrating the effectiveness of RFNs on various benchmark datasets
* A thorough comparison with other unsupervised methods
* The potential of RFNs as a pretraining technique for deep networks and their application to real-world problems in drug discovery
However, there are some limitations and potential areas for improvement:
* The paper could benefit from a more detailed analysis of the computational complexity of the RFN learning algorithm and its scalability to large datasets
* The authors could provide more insights into the interpretability of the learned representations and the identified rare and small events
* The paper could be strengthened by additional experiments on more diverse datasets and applications
Overall, the paper presents a significant contribution to the field of unsupervised deep learning, and the proposed RFN approach shows great promise for constructing sparse and informative representations of input data.
Arguments pro acceptance:
* The paper presents a novel and well-motivated approach to constructing sparse and non-linear representations of input data
* The extensive experiments demonstrate the effectiveness of RFNs on various benchmark datasets
* The potential of RFNs as a pretraining technique for deep networks and their application to real-world problems in drug discovery
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the computational complexity of the RFN learning algorithm and its scalability to large datasets
* The authors could provide more insights into the interpretability of the learned representations and the identified rare and small events
* The paper could be strengthened by additional experiments on more diverse datasets and applications
Recommendation: Accept with minor revisions, addressing the limitations and potential areas for improvement mentioned above.