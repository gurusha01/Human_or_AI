This paper proposes a novel Covariance-Controlled Adaptive Langevin (CCAdL) thermostat for Bayesian posterior inference using Monte Carlo sampling. The main claim of the paper is that CCAdL can effectively dissipate parameter-dependent noise while maintaining a desired target distribution, achieving a substantial speedup over popular alternative schemes for large-scale machine learning applications.
The paper is well-supported by theoretical analysis and numerical experiments. The authors provide a clear explanation of the underlying dynamics of the proposed method and demonstrate its effectiveness in various machine learning tasks, including Bayesian inference for Gaussian distribution, large-scale Bayesian logistic regression, and discriminative restricted Boltzmann machine (DRBM). The results show that CCAdL outperforms existing methods, such as Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) and Stochastic Gradient Nose-Hoover Thermostat (SGNHT), in terms of convergence speed and sample quality.
The paper is well-written and easy to follow, with a clear structure and concise language. The authors provide sufficient background information and references to related work, making it accessible to readers who are not experts in the field. The numerical experiments are well-designed and thoroughly executed, providing strong evidence for the effectiveness of the proposed method.
The paper has several strengths, including:
* The proposed method is novel and addresses a significant problem in Bayesian posterior inference.
* The theoretical analysis is rigorous and well-supported by numerical experiments.
* The paper provides a clear and concise explanation of the underlying dynamics of the proposed method.
* The numerical experiments are well-designed and thoroughly executed.
However, there are some limitations and potential areas for improvement:
* The paper assumes that the noise of the stochastic gradient follows a normal distribution, which may not always be the case in practice.
* The estimation of the covariance matrix associated with the noisy gradient is a critical component of the proposed method, and the authors could provide more discussion on the robustness of this estimation.
* The paper could benefit from more comparisons with other existing methods, such as Stochastic Gradient Langevin Dynamics (SGLD) and its variants.
Overall, this is a well-written and well-researched paper that makes a significant contribution to the field of Bayesian posterior inference. The proposed method has the potential to be widely adopted in large-scale machine learning applications, and the paper provides a solid foundation for future research in this area.
Arguments pro acceptance:
* The paper proposes a novel and effective method for Bayesian posterior inference.
* The theoretical analysis is rigorous and well-supported by numerical experiments.
* The paper provides a clear and concise explanation of the underlying dynamics of the proposed method.
* The numerical experiments are well-designed and thoroughly executed.
Arguments con acceptance:
* The paper assumes that the noise of the stochastic gradient follows a normal distribution, which may not always be the case in practice.
* The estimation of the covariance matrix associated with the noisy gradient is a critical component of the proposed method, and the authors could provide more discussion on the robustness of this estimation.
* The paper could benefit from more comparisons with other existing methods.