This paper proposes a novel approach to learning decision trees, deviating from the traditional greedy methods that optimize split functions one node at a time. The authors formulate a global objective that jointly optimizes the split functions at all levels of the tree, along with the leaf parameters. This non-greedy approach is shown to outperform greedy decision tree baselines on several classification benchmarks.
The paper's main claim is that the proposed algorithm can efficiently learn deep decision trees by optimizing a convex-concave upper bound on the tree's empirical loss. The authors establish a link between decision tree optimization and structured prediction with latent variables, which enables the use of stochastic gradient descent for optimization.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The related work section is comprehensive, and the authors adequately discuss the limitations of previous methods. The experimental results demonstrate the effectiveness of the proposed approach, showing improved performance over greedy baselines on multiple datasets.
The strengths of the paper include:
* A novel and well-motivated approach to decision tree learning
* A clear and concise explanation of the methodology
* Comprehensive related work and discussion of limitations
* Strong experimental results demonstrating improved performance
The weaknesses of the paper include:
* The optimization problem is non-convex, which may lead to local optima
* The choice of regularization parameter ν is crucial, and the authors do not provide a clear guideline for selecting this parameter
* The computational complexity of the algorithm may be high for very deep trees or large datasets
Overall, the paper presents a significant contribution to the field of decision tree learning, and the proposed approach has the potential to improve the performance of decision trees in various applications. However, further work is needed to address the limitations and improve the scalability of the algorithm.
Arguments for acceptance:
* The paper presents a novel and well-motivated approach to decision tree learning
* The experimental results demonstrate improved performance over greedy baselines
* The paper is well-written, and the authors provide a clear and concise explanation of their approach
Arguments for rejection:
* The optimization problem is non-convex, which may lead to local optima
* The choice of regularization parameter ν is crucial, and the authors do not provide a clear guideline for selecting this parameter
* The computational complexity of the algorithm may be high for very deep trees or large datasets
Recommendation: Accept, with minor revisions to address the limitations and provide more guidance on selecting the regularization parameter ν.