This paper proposes a novel adaptive learning rate schedule, called ESGD, based on the equilibration preconditioner for non-convex optimization problems. The authors argue that the equilibration preconditioner is better suited for saddle point problems, which are common in deep neural networks, compared to the Jacobi preconditioner. The paper provides theoretical and empirical evidence to support this claim, including experiments on deep autoencoder benchmarks.
The main claims of the paper are: (1) the equilibration preconditioner is more effective than the Jacobi preconditioner for non-convex problems, and (2) the proposed ESGD method outperforms RMSProp on deep autoencoder benchmarks. The paper supports these claims with theoretical analysis, empirical results, and comparisons to other methods.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of optimizing deep neural networks. The related work section is comprehensive, and the authors provide a good overview of existing methods for adaptive learning rates. The experimental setup is well-described, and the results are presented in a clear and easy-to-understand manner.
The strengths of the paper include: (1) the proposal of a novel adaptive learning rate schedule, (2) the provision of theoretical and empirical evidence to support the claims, and (3) the comparison to other methods, including RMSProp. The weaknesses of the paper include: (1) the lack of a clear explanation of why the equilibration preconditioner is better suited for non-convex problems, and (2) the limited number of experiments and datasets used to evaluate the proposed method.
Overall, the paper is well-written, and the proposed method shows promise for improving the optimization of deep neural networks. However, more research is needed to fully understand the benefits and limitations of the proposed method, and to confirm the results on a wider range of datasets and tasks.
Arguments pro acceptance:
* The paper proposes a novel adaptive learning rate schedule that shows promise for improving the optimization of deep neural networks.
* The authors provide theoretical and empirical evidence to support the claims, including experiments on deep autoencoder benchmarks.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem of optimizing deep neural networks.
Arguments con acceptance:
* The paper lacks a clear explanation of why the equilibration preconditioner is better suited for non-convex problems.
* The number of experiments and datasets used to evaluate the proposed method is limited.
* More research is needed to fully understand the benefits and limitations of the proposed method, and to confirm the results on a wider range of datasets and tasks.