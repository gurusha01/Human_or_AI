This paper presents several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods, a class of stochastic optimization algorithms that achieve linear convergence rates without requiring large memory storage. The main claims of the paper are: (1) SVRG is robust to inexact calculation of the full gradients it requires, (2) using growing-batch strategies can reduce the number of gradient evaluations required in the early iterations, (3) exploiting support vectors can reduce the number of gradient computations in the later iterations, and (4) the commonly-used regularized SVRG iteration is justified and improves the convergence rate.
The paper provides a thorough analysis of the convergence rates of the proposed strategies, including a mixed SG/SVRG method that combines the benefits of stochastic gradient (SG) and SVRG methods. The authors also consider alternative mini-batch selection strategies and provide a theoretical justification for using the regularized SVRG update. The experimental results demonstrate the effectiveness of the proposed strategies on several datasets.
The strengths of the paper include: (1) a clear and well-organized presentation of the ideas, (2) a thorough analysis of the convergence rates of the proposed strategies, and (3) a comprehensive set of experimental results that demonstrate the effectiveness of the proposed strategies. The weaknesses of the paper include: (1) the lack of a clear comparison with other state-of-the-art optimization algorithms, and (2) the limited scope of the experimental results, which only consider logistic regression classification problems.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed strategies and their analysis. The use of notation and terminology is consistent throughout the paper, and the authors provide a comprehensive set of references to related work.
The significance of the paper lies in its contribution to the development of efficient optimization algorithms for large-scale machine learning problems. The proposed strategies have the potential to improve the performance of SVRG methods, which are widely used in many applications. The paper also provides a thorough analysis of the convergence rates of the proposed strategies, which can be useful for understanding the behavior of these algorithms in practice.
Overall, I would recommend accepting this paper for publication. The paper presents a clear and well-organized set of ideas, and the authors provide a thorough analysis of the convergence rates of the proposed strategies. The experimental results demonstrate the effectiveness of the proposed strategies, and the paper has the potential to make a significant contribution to the development of efficient optimization algorithms for large-scale machine learning problems.
Arguments pro acceptance:
* The paper presents a clear and well-organized set of ideas.
* The authors provide a thorough analysis of the convergence rates of the proposed strategies.
* The experimental results demonstrate the effectiveness of the proposed strategies.
* The paper has the potential to make a significant contribution to the development of efficient optimization algorithms for large-scale machine learning problems.
Arguments con acceptance:
* The lack of a clear comparison with other state-of-the-art optimization algorithms.
* The limited scope of the experimental results, which only consider logistic regression classification problems.