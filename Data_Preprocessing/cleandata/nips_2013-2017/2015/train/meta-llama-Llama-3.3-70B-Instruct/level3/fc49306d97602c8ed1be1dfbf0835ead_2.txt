This paper proposes a novel model, the Variational Recurrent Neural Network (VRNN), which combines the elements of Variational Autoencoders (VAEs) with Recurrent Neural Networks (RNNs) to model highly structured sequential data. The authors argue that the introduction of latent random variables into the RNN hidden state can help capture the variability observed in the data, particularly in natural speech and handwriting generation tasks.
The paper is well-written and clearly explains the motivation behind the proposed model. The authors provide a thorough review of the background material, including sequence modeling with RNNs and VAEs. The proposed VRNN model is described in detail, and the authors provide a clear explanation of the generative and inference models.
The experimental results are impressive, with the VRNN model outperforming standard RNN models on several speech and handwriting generation tasks. The authors also provide a qualitative analysis of the latent space, which shows that the VRNN model can capture meaningful patterns in the data.
However, there are some concerns regarding the novelty of the paper. While the authors propose a new model, the idea of combining VAEs with RNNs is not entirely new, and similar models have been proposed in the past. The authors could have done a better job of discussing the relationship between their work and existing literature.
Additionally, the paper could benefit from a more thorough comparison to existing multilingual word embedding systems and matching by looking for the nearest candidate word in such systems. The authors also ignore recent literature on embeddings, including several relevant papers on unsupervised cross-domain word representation learning and domain adaptation.
The scalability of the model is also a concern, as it seems difficult to scale and is evaluated on small datasets, with no discussion of this limitation. The authors propose an alternative algorithm using word2vec-type word embeddings and kernel CCA, which overcomes the paper's complaints about kernel CCA.
Overall, the paper is well-written, and the proposed model is intriguing. However, the novelty of the paper is not enormous, and the authors could have done a better job of discussing the relationship between their work and existing literature. With thorough experimental comparisons and a more detailed discussion of the limitations and potential applications of the model, this paper could be a strong candidate for publication.
Arguments for acceptance:
* The paper proposes a novel model that combines VAEs with RNNs to model highly structured sequential data.
* The experimental results are impressive, with the VRNN model outperforming standard RNN models on several speech and handwriting generation tasks.
* The paper provides a clear explanation of the generative and inference models, and the authors provide a thorough review of the background material.
Arguments against acceptance:
* The novelty of the paper is not enormous, and similar models have been proposed in the past.
* The authors could have done a better job of discussing the relationship between their work and existing literature.
* The scalability of the model is a concern, and the authors ignore recent literature on embeddings.
* The paper could benefit from a more thorough comparison to existing multilingual word embedding systems and matching by looking for the nearest candidate word in such systems.