This paper presents a theoretical analysis of non-regularized supervised metric learning formulations, focusing on two frameworks: distance-based and classifier-based. The authors provide PAC-style bounds on the difference between true and empirical risk, and discuss the necessity of a strong dependency on the dimensionality of the data. They also propose a regularized formulation using the Frobenius norm as a regularizer, which provides better guarantees and shows that adding a regularization term is beneficial through empirical evaluation.
The paper's main contributions include the provision of theoretical guarantees for collaborative ranking and bundled choice modeling, as well as the analysis of the sample complexity of the algorithm. The authors prove an upper bound on the resulting error with finite samples and provide a matching information-theoretic lower bound. The results are interesting, and providing theoretical guarantees on classifiers making use of the result of a known metric is important.
However, there are some areas that require improvement. The proof of Lemma 2 needs to be corrected to make it independent of the dimension. Additionally, comments should be added to link Theorem 1 and Theorem 2. The results of Jin et al. can be extended to replace the dependency on dimension by a term based on data concentration for any distributions and convex regularized formulations.
The paper's claim to generalize previous results to non-convex loss is limited, as there is no guarantee to find the best model minimizing the risk. The authors should consider the true rank of the matrix M rather than the data dimension and discuss its relationship with metric learning based on Cholesky decomposition and low-rank regularization.
The paper lacks references to methods learning a linear classifier from good similarity functions, and the authors should add references such as Balcan et al. and Bellet et al. The experiments are also lacking, with insufficient datasets, methods for comparison, and consideration of classifier-based settings. The scaling for the x-axis is different for the three figures, which makes it difficult to compare the results.
Overall, the paper provides a good foundation for understanding the theoretical aspects of metric learning, but requires improvements in terms of proof corrections, extensions, and experimental evaluations. The authors should address the mentioned weaknesses and provide more comprehensive results to strengthen their claims.
Arguments for acceptance:
* The paper provides a thorough theoretical analysis of non-regularized supervised metric learning formulations.
* The authors propose a regularized formulation using the Frobenius norm as a regularizer, which provides better guarantees.
* The results are interesting, and providing theoretical guarantees on classifiers making use of the result of a known metric is important.
Arguments against acceptance:
* The proof of Lemma 2 needs to be corrected to make it independent of the dimension.
* The paper's claim to generalize previous results to non-convex loss is limited.
* The experiments are lacking, with insufficient datasets, methods for comparison, and consideration of classifier-based settings.
* The paper lacks references to methods learning a linear classifier from good similarity functions.