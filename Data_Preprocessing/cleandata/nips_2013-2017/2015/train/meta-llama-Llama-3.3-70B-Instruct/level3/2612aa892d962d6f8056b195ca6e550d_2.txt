This paper proposes a novel approach to decision tree learning, leveraging stochastic gradient descent to optimize a global objective function that jointly optimizes the split functions and leaf parameters of the tree. The authors establish a link between decision tree optimization and structured prediction with latent variables, allowing for the formulation of a convex-concave upper bound on the empirical loss. This bound is optimized using stochastic gradient descent, enabling efficient learning of deep trees.
The paper is well-written and clearly organized, with a thorough review of related work and a detailed explanation of the proposed approach. The experiments demonstrate the effectiveness of the non-greedy decision tree learning method, showing improved performance over greedy baselines on several benchmark datasets.
However, there are some areas that require clarification and improvement. For instance, the notation around lines 226-240 could be more clearly explained, and the concept of "softer feedback gains" is not well-defined. Additionally, the authors could provide more insight into the computational requirements and robustness to environmental variabilities of their approach, particularly in comparison to recent work by Pieter Abbeel and Sergey Levine.
The significance of this work lies in its potential to improve the state-of-the-art in decision tree learning, enabling more accurate and efficient classification and regression tasks. The proposed approach could be particularly useful in applications where deep trees are required, such as in computer vision and natural language processing.
In terms of quality, the paper is technically sound, with a clear and well-motivated formulation of the problem and a thorough analysis of the optimization framework. The claims are well-supported by experimental results, and the authors are careful to evaluate both the strengths and weaknesses of their approach.
The originality of the work is high, as it proposes a novel approach to decision tree learning that leverages stochastic gradient descent and structured prediction with latent variables. The paper clearly establishes the connection between decision tree optimization and structured prediction, and the proposed approach is distinct from previous work in the field.
Overall, this paper makes a significant contribution to the field of decision tree learning, and its results have the potential to impact a wide range of applications. With some clarifications and improvements, this work could be even more effective in demonstrating the advantages of the proposed approach.
Arguments pro acceptance:
* The paper proposes a novel and well-motivated approach to decision tree learning.
* The experiments demonstrate improved performance over greedy baselines on several benchmark datasets.
* The paper establishes a clear connection between decision tree optimization and structured prediction with latent variables.
* The proposed approach has the potential to improve the state-of-the-art in decision tree learning.
Arguments con acceptance:
* The notation around lines 226-240 could be more clearly explained.
* The concept of "softer feedback gains" is not well-defined.
* The authors could provide more insight into the computational requirements and robustness to environmental variabilities of their approach.
* The paper could benefit from more comparisons to recent work in the field, such as that of Pieter Abbeel and Sergey Levine.