This paper presents a novel algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for parallelizing stochastic gradient descent in deep learning settings. The algorithm introduces an elastic force that links local worker parameters to a center variable, allowing for more exploration and reducing communication overhead. The authors provide a stability analysis of the asynchronous EASGD algorithm and compare it to the Alternating Direction Method of Multipliers (ADMM) algorithm, showing that EASGD is more stable and has a simpler stability condition.
The paper is well-organized and clearly written, making it easy to follow. The introduction provides a good background on the problem of parallelizing stochastic gradient descent and motivates the need for a new algorithm. The experimental evaluation is thorough, comparing EASGD to several baseline methods on two benchmark datasets, CIFAR-10 and ImageNet.
The strengths of the paper include:
* The introduction of a novel algorithm that addresses the problem of parallelizing stochastic gradient descent in deep learning settings.
* A thorough stability analysis of the asynchronous EASGD algorithm and its comparison to ADMM.
* A well-designed experimental evaluation that demonstrates the effectiveness of EASGD in reducing communication overhead and improving test error.
However, there are some weaknesses:
* The paper could benefit from a more detailed analysis of the trade-off between exploration and exploitation in the EASGD algorithm.
* The experimental evaluation could be more comprehensive, including more baseline methods and a larger range of hyperparameters.
* The paper assumes a specific communication model, where the local workers communicate with the master at a fixed interval, which may not be realistic in all scenarios.
Overall, the paper presents a significant contribution to the field of deep learning and parallel computing, and the EASGD algorithm has the potential to be widely adopted in practice.
Arguments pro acceptance:
* The paper introduces a novel algorithm that addresses a significant problem in deep learning.
* The stability analysis is thorough and well-done.
* The experimental evaluation is well-designed and demonstrates the effectiveness of the algorithm.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the trade-off between exploration and exploitation.
* The experimental evaluation could be more comprehensive.
* The paper assumes a specific communication model that may not be realistic in all scenarios.
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 9/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, including providing a more detailed analysis of the trade-off between exploration and exploitation, and considering a more comprehensive experimental evaluation.