This paper presents improved algorithms for computing the Isotonic Regression of a given set of values on a directed acyclic graph (DAG) under various weighted p-norms. The authors provide a unified framework for solving the Isotonic Regression problem for all weighted p-norms, with rigorous performance guarantees. The proposed algorithms are shown to be quite practical and can be implemented to run fast in practice.
The paper makes a good technical contribution by unifying a large body of work on isotonic regression and employing techniques from fast solvers of linear systems. However, the paper may be better suited to an algorithms/theoretical CS conference or journal due to its technical content and lack of direct impact on ML problems. The results in the paper may not be of sufficient interest to the broader NIPS community as isotonic regression is not a core ML tool and lacks additional learning-specific insight or extension.
The proposed algorithms may not bring faster runtimes in certain applications of IR, such as probabilistic calibration and learning SIMs, due to the use of very structured DAGs. The paper lacks clear explanation of the impact of the proposed algorithms on ML problems and needs to spell out the significant advances in other interesting learning applications.
To improve the paper, the authors should consider citing other relevant works, such as the one establishing the optimality of the PAV algorithm for a general class of loss functions. Additionally, the authors should address some minor errors, including typos and a potential mistake in the reference [14] working with the standard L2 norm instead of a general Lp norm.
Arguments for acceptance:
* The paper presents a significant technical contribution to the field of isotonic regression.
* The proposed algorithms are shown to be practical and can be implemented to run fast in practice.
* The paper provides a unified framework for solving the Isotonic Regression problem for all weighted p-norms.
Arguments against acceptance:
* The paper may be better suited to an algorithms/theoretical CS conference or journal.
* The results in the paper may not be of sufficient interest to the broader NIPS community.
* The paper lacks clear explanation of the impact of the proposed algorithms on ML problems.
* The proposed algorithms may not bring faster runtimes in certain applications of IR.