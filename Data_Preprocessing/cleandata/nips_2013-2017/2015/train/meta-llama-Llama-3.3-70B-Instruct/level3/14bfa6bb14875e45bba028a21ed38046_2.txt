This paper presents a novel Bayesian optimization method, called Infinite-Metric GP Optimization (IMGPO), which achieves an exponential convergence rate without the need for auxiliary optimization and δ-cover sampling. The authors propose a smart idea of leveraging the existence of an unknown bound encoded in the continuity at a global optimizer, and simultaneously conducting global and local searches based on all possible candidates of the bounds. The algorithm is shown to outperform other state-of-the-art methods, including SOO, BaMSOO, GP-PI, and GP-EI, in a variety of experiments.
The strengths of the paper include:
* The proposed algorithm achieves an exponential convergence rate, which is a significant improvement over previous methods.
* The authors provide a thorough analysis of the algorithm, including a discussion of the effect of the tightness of the upper confidence bound (UCB) and the use of Gaussian processes (GPs).
* The experimental results demonstrate the effectiveness of the algorithm in a variety of settings, including high-dimensional problems.
However, there are also some weaknesses:
* The algorithm requires the specification of several hyperparameters, including the kernel parameter matrix and the parameter η, which can be difficult to tune in practice.
* The authors assume that the objective function is a non-convex black-box function, which may not always be the case in practice.
* The algorithm may not be suitable for very large-scale problems, as the computational cost can be high.
Overall, the paper presents a significant contribution to the field of Bayesian optimization, and the proposed algorithm has the potential to be widely used in practice. However, further research is needed to address the weaknesses of the algorithm and to improve its scalability and robustness.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of Bayesian optimization.
* The algorithm achieves an exponential convergence rate, which is a significant improvement over previous methods.
* The experimental results demonstrate the effectiveness of the algorithm in a variety of settings.
Arguments con acceptance:
* The algorithm requires the specification of several hyperparameters, which can be difficult to tune in practice.
* The authors assume that the objective function is a non-convex black-box function, which may not always be the case in practice.
* The algorithm may not be suitable for very large-scale problems, as the computational cost can be high.
Quality: 8/10
The paper is well-written and well-organized, and the authors provide a thorough analysis of the algorithm. However, the algorithm requires the specification of several hyperparameters, which can be difficult to tune in practice.
Clarity: 9/10
The paper is clearly written, and the authors provide a good introduction to the background and motivation of the work. The experimental results are also clearly presented.
Originality: 9/10
The paper presents a novel and significant contribution to the field of Bayesian optimization. The proposed algorithm is original and has the potential to be widely used in practice.
Significance: 9/10
The paper presents a significant contribution to the field of Bayesian optimization, and the proposed algorithm has the potential to be widely used in practice. The experimental results demonstrate the effectiveness of the algorithm in a variety of settings.