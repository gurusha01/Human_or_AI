This paper proposes a novel algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for parallelizing stochastic gradient descent in deep learning settings. The algorithm allows local workers to perform more exploration by reducing the amount of communication between local workers and the master, which leads to improved performance in the presence of many local optima. The authors also propose synchronous and asynchronous variants of the algorithm, as well as a momentum-based variant, EAMSGD.
The paper is well-written and clearly explains the motivation and intuition behind the proposed algorithm. The authors provide a thorough analysis of the stability of the asynchronous EASGD algorithm in the round-robin scheme and compare it to the ADMM algorithm, showing that EASGD is more stable and has a simpler stability condition.
The experimental results demonstrate the effectiveness of EASGD and EAMSGD in training deep neural networks on CIFAR and ImageNet datasets, outperforming other baseline approaches such as DOWNPOUR and its variants. The authors also explore the trade-off between exploration and exploitation as a function of the learning rate and communication period.
The strengths of the paper include:
* The proposal of a novel algorithm that addresses the challenge of parallelizing stochastic gradient descent in deep learning settings
* A thorough analysis of the stability of the asynchronous EASGD algorithm
* Experimental results that demonstrate the effectiveness of EASGD and EAMSGD in training deep neural networks
The weaknesses of the paper include:
* The lack of a clear comparison to other state-of-the-art algorithms for parallelizing stochastic gradient descent
* The limited exploration of the hyperparameter space for the proposed algorithm
* The need for more detailed analysis of the trade-off between exploration and exploitation
Overall, the paper is well-written and makes a significant contribution to the field of deep learning. The proposed algorithm has the potential to improve the training of deep neural networks in parallel computing environments, and the experimental results demonstrate its effectiveness.
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses a significant challenge in deep learning
* The analysis of the stability of the asynchronous EASGD algorithm is thorough and well-done
* The experimental results demonstrate the effectiveness of EASGD and EAMSGD in training deep neural networks
Arguments con acceptance:
* The lack of a clear comparison to other state-of-the-art algorithms for parallelizing stochastic gradient descent
* The limited exploration of the hyperparameter space for the proposed algorithm
* The need for more detailed analysis of the trade-off between exploration and exploitation
Recommendation: Accept with minor revisions to address the weaknesses mentioned above.