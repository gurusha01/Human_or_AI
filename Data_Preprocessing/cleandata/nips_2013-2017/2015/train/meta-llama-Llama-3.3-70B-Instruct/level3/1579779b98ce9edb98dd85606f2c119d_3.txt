This paper proposes a novel approach to learning decision trees by framing it as a structured prediction problem. The authors formulate a convex-concave upper bound on the tree's empirical loss and optimize it using stochastic gradient descent (SGD). The proposed method outperforms traditional greedy approaches to decision tree building in terms of test set accuracy, although the improvements are relatively small.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a good overview of the background and motivation for the work, and the related work section provides a thorough review of existing approaches to decision tree learning. The technical sections are detailed and well-explained, with clear descriptions of the proposed algorithm and its components.
One of the strengths of the paper is its ability to establish a link between decision tree learning and structured prediction with latent variables. This connection allows the authors to leverage advances in structured prediction to develop a more efficient and effective algorithm for decision tree learning. The use of a convex-concave upper bound on the empirical loss is also a key contribution, as it enables the authors to optimize the tree's parameters using SGD.
The experimental results demonstrate the effectiveness of the proposed approach, with non-greedy trees outperforming greedy baselines on several benchmark datasets. The authors also provide a detailed analysis of the computational complexity of their algorithm, which is an important consideration for large-scale applications.
However, there are some potential weaknesses to the paper. One concern is that the improvements in test set accuracy are relatively small, which may limit the practical impact of the proposed approach. Additionally, the authors' use of a surrogate objective function may not always provide a tight upper bound on the true empirical loss, which could affect the performance of the algorithm in certain cases.
Overall, I would argue in favor of accepting this paper. The authors have made a significant contribution to the field of decision tree learning, and their approach has the potential to improve the accuracy and efficiency of decision tree-based models. While there are some potential weaknesses to the paper, the strengths of the work outweigh these limitations, and the paper is well-written and clearly organized.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to decision tree learning
* The authors establish a link between decision tree learning and structured prediction with latent variables
* The use of a convex-concave upper bound on the empirical loss enables efficient optimization using SGD
* The experimental results demonstrate the effectiveness of the proposed approach
Arguments con acceptance:
* The improvements in test set accuracy are relatively small
* The use of a surrogate objective function may not always provide a tight upper bound on the true empirical loss
* The computational complexity of the algorithm may be a concern for large-scale applications
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Overall score: 8.2/10
Recommendation: Accept with minor revisions. The authors should consider addressing the potential weaknesses of the paper, such as the relatively small improvements in test set accuracy and the use of a surrogate objective function. Additionally, the authors may want to provide more detailed analysis of the computational complexity of their algorithm and its potential impact on large-scale applications.