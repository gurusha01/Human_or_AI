This paper proposes a novel approach called Principal Differences Analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. The authors also introduce a sparse variant of the method, called SPARDA, which identifies features responsible for the differences.
The technical approach in the paper is sensible, and the authors provide a clear and well-structured presentation of their methodology. The use of the Wasserstein distance as a measure of divergence is well-justified, and the authors provide a thorough discussion of the properties and advantages of this choice.
The experimental evaluation of the method is also thorough, with applications to both synthetic and real-world datasets. The results demonstrate the effectiveness of PDA and SPARDA in identifying differences between distributions, particularly in high-dimensional settings.
However, there are some concerns regarding the comparison to strong baselines using the same training data. The authors compare their method to other approaches, such as sparse PCA and logistic regression, but it is not clear whether these methods are using the same training data. A more thorough comparison to strong baselines, such as a paragraph vector model, would strengthen the paper.
Additionally, the paper lacks discussion on certain issues, such as the problem of summarizing the entire input in a single vector, hyperparameter tuning, and alternate architectures. The authors could provide more insight into how these issues were addressed in their implementation.
Overall, the paper presents a novel and effective approach to analyzing differences between high-dimensional distributions. With some additional discussion and comparison to strong baselines, the paper could be even stronger.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to analyzing differences between high-dimensional distributions.
* The methodology is well-presented and easy to follow.
* The experimental evaluation is thorough and demonstrates the effectiveness of the method.
Arguments con acceptance:
* The comparison to strong baselines is not thorough enough.
* The paper lacks discussion on certain issues, such as summarizing the entire input in a single vector and hyperparameter tuning.
* The authors could provide more insight into how these issues were addressed in their implementation.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should address the concerns regarding the comparison to strong baselines and provide more discussion on the issues mentioned above.