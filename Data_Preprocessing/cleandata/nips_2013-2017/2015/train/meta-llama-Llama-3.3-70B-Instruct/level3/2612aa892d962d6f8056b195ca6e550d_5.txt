This paper presents a novel approach to learning decision trees using a non-greedy method, which optimizes the split functions at all levels of the tree jointly with the leaf parameters based on a global objective. The authors establish a link between decision tree optimization and structured prediction with latent variables, and propose a convex-concave upper bound on the tree's empirical loss. The paper demonstrates promising results, outperforming greedy decision tree baselines on several classification benchmarks.
The strengths of the paper include its well-motivated and clearly presented approach, which addresses the limitations of traditional greedy decision tree induction methods. The authors provide a thorough analysis of the optimization problem and propose an efficient algorithm for minimizing the surrogate objective. The experimental results are convincing, showing that the non-greedy trees achieve better test performance and are less susceptible to overfitting.
However, there are some weaknesses and areas for improvement. The paper could benefit from a clearer discussion of the requirement for full model knowledge from the beginning, specifically referencing Equation 6. Additionally, Sections 4 and 6 appear to be disproportionate in terms of content and discussion, with Section 4 lacking a mention of recurrent units, which were referenced in the abstract. The classification of a neural network as stochastic is also questionable, as the network itself appears to be deterministic.
To improve the paper, the authors could restructure it to better balance the content and discussion in each section. They could also provide more context and motivation for the use of stochastic gradient descent and the introduction of a regularization parameter. Furthermore, the authors could consider discussing the potential applications and extensions of their approach, such as learning sparse split functions or applying the kernel trick to the linear split parameters.
Overall, the paper presents a significant contribution to the field of decision tree learning, and with some revisions to address the mentioned weaknesses, it has the potential to be a strong and impactful publication.
Arguments pro acceptance:
* The paper presents a novel and well-motivated approach to learning decision trees.
* The authors provide a thorough analysis of the optimization problem and propose an efficient algorithm.
* The experimental results are convincing, showing that the non-greedy trees achieve better test performance and are less susceptible to overfitting.
Arguments con acceptance:
* The paper could benefit from a clearer discussion of the requirement for full model knowledge from the beginning.
* Sections 4 and 6 appear to be disproportionate in terms of content and discussion.
* The classification of a neural network as stochastic is questionable.
* The paper could be improved with more context and motivation for the use of stochastic gradient descent and the introduction of a regularization parameter.