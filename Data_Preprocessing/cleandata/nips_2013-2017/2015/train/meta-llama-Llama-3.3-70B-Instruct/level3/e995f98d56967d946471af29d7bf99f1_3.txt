This paper introduces a novel framework for on-the-job learning, which enables a system to learn from noisy crowds and maintain high accuracy while reducing cost over time. The authors model the on-the-job setting as a stochastic game and use Monte Carlo tree search and progressive widening to approximate the optimal policy. The experimental results are impressive, showing significant cost reductions and accuracy improvements over baseline methods.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The use of Bayesian decision theory to trade off latency, cost, and accuracy is a nice touch, and the experimental evaluation is thorough and well-designed. The authors also provide a detailed analysis of the results, highlighting the strengths and weaknesses of their approach.
One of the strengths of the paper is its ability to balance the trade-off between exploration and exploitation. The authors use a combination of Monte Carlo tree search and progressive widening to explore the state space and approximate the optimal policy. This allows the system to adapt to changing conditions and improve its performance over time.
However, there are some areas where the paper could be improved. For example, the authors could provide more analysis on the sensitivity of the hyper-parameters and the tuning process. Additionally, a more rigorous probabilistic analysis of the proposed heuristic would be beneficial to understand why it works and to tease apart the regularization effect.
Furthermore, the authors could provide more comparison to baselines that use different beam widths to mitigate search error. This would help to better understand the effectiveness of the proposed approach and its limitations.
In terms of originality, the paper introduces a new framework for on-the-job learning, which is a significant contribution to the field. The use of stochastic games and Monte Carlo tree search to approximate the optimal policy is also novel and interesting.
Overall, the paper is well-written, and the authors provide a clear and concise explanation of their approach. The experimental results are impressive, and the paper makes a significant contribution to the field. However, there are some areas where the paper could be improved, and the authors could provide more analysis and comparison to baselines to strengthen their claims.
Arguments for acceptance:
* The paper introduces a novel framework for on-the-job learning, which is a significant contribution to the field.
* The experimental results are impressive, showing significant cost reductions and accuracy improvements over baseline methods.
* The authors provide a clear and concise explanation of their approach, and the paper is well-written.
Arguments against acceptance:
* The paper could benefit from more analysis on the sensitivity of the hyper-parameters and the tuning process.
* A more rigorous probabilistic analysis of the proposed heuristic would be beneficial to understand why it works and to tease apart the regularization effect.
* The authors could provide more comparison to baselines that use different beam widths to mitigate search error.