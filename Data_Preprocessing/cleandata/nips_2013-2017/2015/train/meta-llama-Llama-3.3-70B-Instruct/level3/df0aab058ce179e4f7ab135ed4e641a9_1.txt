This paper proposes a new algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for parallelizing stochastic gradient descent in deep learning settings. The algorithm allows local workers to perform more exploration by reducing the amount of communication between local workers and the master, which leads to improved performance in the presence of many local optima. The authors also propose synchronous and asynchronous variants of the algorithm, as well as a momentum-based variant.
The paper is well-written and provides a clear explanation of the algorithm and its variants. The experimental results demonstrate the effectiveness of EASGD in achieving faster convergence and better test performance compared to other baseline approaches, such as DOWNPOUR and its variants. The stability analysis of the asynchronous EASGD in the round-robin scheme is also provided, which shows the theoretical advantage of the method over ADMM.
However, there are some areas that could be improved. For example, the use of five different algorithms for computing a gradient step seems excessive, and it is not clear why all of these algorithms are necessary. Additionally, the proposed technique of using sparsity-enforcing posterior regularization for nonlinear inference in factor analyzers is interesting, but it raises concerns about scalability due to its batch nature.
The results obtained from the algorithm appear reasonable, although the algorithm itself is significantly more expensive than the baseline comparisons. It would be beneficial to include running times for all algorithms to provide a more comprehensive understanding of the trade-offs between different methods.
Overall, the paper presents a novel approach to parallelizing stochastic gradient descent, and the experimental results demonstrate its effectiveness. However, some areas of the paper could be improved, and the authors should consider addressing these concerns in future work.
Arguments pro acceptance:
* The paper proposes a novel algorithm for parallelizing stochastic gradient descent, which is a significant contribution to the field.
* The experimental results demonstrate the effectiveness of the algorithm in achieving faster convergence and better test performance.
* The stability analysis of the asynchronous EASGD in the round-robin scheme provides a theoretical understanding of the method's advantages.
Arguments con acceptance:
* The use of five different algorithms for computing a gradient step seems excessive, and it is not clear why all of these algorithms are necessary.
* The proposed technique of using sparsity-enforcing posterior regularization for nonlinear inference in factor analyzers raises concerns about scalability due to its batch nature.
* The algorithm is significantly more expensive than the baseline comparisons, and running times for all algorithms should be included to provide a more comprehensive understanding of the trade-offs between different methods.