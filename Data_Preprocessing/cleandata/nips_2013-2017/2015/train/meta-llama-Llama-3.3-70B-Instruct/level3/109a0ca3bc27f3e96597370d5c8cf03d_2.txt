This paper proposes a novel approach to batch learning from logged bandit feedback (BLBF) by introducing a self-normalized risk estimator that avoids the propensity overfitting problem inherent in the conventional unbiased risk estimator. The authors demonstrate that the proposed estimator, used in their new learning algorithm Norm-POEM, leads to significantly improved generalization performance compared to the existing POEM algorithm.
The paper is well-written and easy to follow, with clear motivation and good qualitative and quantitative results supporting the proposed method. The authors provide a thorough analysis of the propensity overfitting problem and its consequences, and their proposed solution is well-justified and intuitive. The experimental evaluation is comprehensive and demonstrates the effectiveness of Norm-POEM in various settings.
The strengths of the paper include:
* The identification of the propensity overfitting problem and its consequences in BLBF
* The proposal of a self-normalized risk estimator that avoids this problem
* The development of a new learning algorithm, Norm-POEM, based on the proposed estimator
* The comprehensive experimental evaluation demonstrating the effectiveness of Norm-POEM
The weaknesses of the paper include:
* The computational cost of the self-normalized estimator, which may be higher than the conventional unbiased estimator
* The potential need for careful tuning of hyperparameters in Norm-POEM
Overall, the paper makes a significant contribution to the field of BLBF and provides a valuable solution to the propensity overfitting problem. The proposed Norm-POEM algorithm has the potential to improve the performance of various applications that rely on BLBF, such as recommender systems and online advertising.
Arguments pro acceptance:
* The paper proposes a novel and effective solution to the propensity overfitting problem in BLBF
* The experimental evaluation is comprehensive and demonstrates the effectiveness of Norm-POEM
* The paper is well-written and easy to follow, with clear motivation and good qualitative and quantitative results
Arguments con acceptance:
* The computational cost of the self-normalized estimator may be higher than the conventional unbiased estimator
* The potential need for careful tuning of hyperparameters in Norm-POEM may be a limitation in practice
Quality: 9/10
The paper is technically sound, and the proposed solution is well-justified and intuitive. The experimental evaluation is comprehensive and demonstrates the effectiveness of Norm-POEM.
Clarity: 9/10
The paper is well-written and easy to follow, with clear motivation and good qualitative and quantitative results.
Originality: 8/10
The paper proposes a novel solution to the propensity overfitting problem in BLBF, but the idea of using control variates is not new.
Significance: 9/10
The paper makes a significant contribution to the field of BLBF and provides a valuable solution to the propensity overfitting problem, which has the potential to improve the performance of various applications.