This paper proposes a novel approach to batch learning from logged bandit feedback (BLBF) by introducing a self-normalized risk estimator that avoids the propensity overfitting problem inherent in the conventional unbiased risk estimator. The authors demonstrate the effectiveness of their approach, called Norm-POEM, through extensive experiments on multi-label classification problems. The paper is well-written and easy to follow, with a clear explanation of the problem, the proposed solution, and the experimental results.
The strengths of the paper include:
* A clear and concise introduction to the problem of BLBF and the limitations of the conventional unbiased risk estimator.
* A well-motivated proposal of the self-normalized risk estimator as a solution to the propensity overfitting problem.
* A thorough experimental evaluation of Norm-POEM on several datasets, demonstrating its improved generalization performance compared to the conventional approach.
* A detailed analysis of the results, including the effect of translations of the loss function and the importance of variance regularization.
The weaknesses of the paper include:
* The paper assumes a significant amount of background knowledge in BLBF and importance sampling, which may make it difficult for non-experts to follow.
* The experimental evaluation is limited to multi-label classification problems, and it is unclear how well Norm-POEM will perform on other types of problems.
* The paper could benefit from a more detailed comparison to other related work in the field, such as other approaches to BLBF and importance sampling.
Overall, the paper presents a significant contribution to the field of BLBF, and the proposed approach has the potential to improve the performance of BLBF algorithms in a wide range of applications. The experimental results are convincing, and the analysis is thorough and well-motivated.
Arguments for acceptance:
* The paper presents a novel and well-motivated approach to BLBF that addresses a significant limitation of the conventional unbiased risk estimator.
* The experimental results demonstrate the effectiveness of Norm-POEM on several datasets.
* The paper is well-written and easy to follow, with a clear explanation of the problem, the proposed solution, and the experimental results.
Arguments against acceptance:
* The paper assumes a significant amount of background knowledge in BLBF and importance sampling, which may limit its accessibility to non-experts.
* The experimental evaluation is limited to multi-label classification problems, and it is unclear how well Norm-POEM will perform on other types of problems.
* The paper could benefit from a more detailed comparison to other related work in the field.