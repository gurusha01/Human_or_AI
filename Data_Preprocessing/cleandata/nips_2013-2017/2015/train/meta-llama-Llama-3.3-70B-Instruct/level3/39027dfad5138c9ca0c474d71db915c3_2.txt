This paper presents a significant contribution to the field of batch learning from logged bandit feedback (BLBF), a crucial problem in interactive systems such as search engines, recommender systems, and ad platforms. The authors identify a severe issue with the conventional unbiased risk estimator used in BLBF, known as propensity overfitting, which can lead to degenerate risk estimates. To address this problem, they propose a self-normalized risk estimator that avoids these anomalies and provides a more robust estimate of the risk.
The paper is well-structured, and the authors provide a clear and concise introduction to the problem, related work, and their proposed approach. The technical content is sound, and the authors provide a thorough analysis of the propensity overfitting problem and its solution using the self-normalized estimator. The experimental results demonstrate the effectiveness of the proposed approach, called Norm-POEM, in achieving better generalization performance compared to the conventional estimator.
One of the strengths of the paper is its clarity and organization. The authors provide a clear motivation for the problem, a thorough review of related work, and a well-structured presentation of their approach. The technical content is sound, and the authors provide a detailed analysis of the propensity overfitting problem and its solution.
However, there are some areas that require improvement. The notation and commentary surrounding equation (2) are not clear, and the subsequent comments about its "anomaly" are technically incorrect. Additionally, the estimator in equation (2) is weak due to its denominator, which can be zero if a sample has never been observed in the prior. The modified estimator in equation (7) is a better principle, but its derivation could be more clearly explained.
The experimental results are convincing, and the comparison with the conventional estimator and a full-information CRF provides a clear understanding of the benefits of the proposed approach. However, the runtime of Norm-POEM is surprisingly faster than POEM, which could be further explored to understand the reasons behind this improvement.
In terms of originality, the paper presents a novel combination of familiar techniques, and the self-normalized estimator is a significant contribution to the field. The related work is adequately referenced, and the authors provide a clear understanding of how their approach differs from previous contributions.
Overall, the paper is well-written, and the authors provide a clear and concise presentation of their approach. The technical content is sound, and the experimental results demonstrate the effectiveness of the proposed approach. With some minor improvements in notation and commentary, this paper has the potential to make a significant impact in the field of BLBF.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of BLBF.
* The self-normalized estimator is a novel and effective solution to the propensity overfitting problem.
* The experimental results demonstrate the effectiveness of the proposed approach.
* The paper is well-structured and clearly written.
Arguments con acceptance:
* The notation and commentary surrounding equation (2) require improvement.
* The estimator in equation (2) is weak and requires modification.
* The derivation of the modified estimator in equation (7) could be more clearly explained.
* The runtime of Norm-POEM is surprisingly faster than POEM, which requires further exploration.