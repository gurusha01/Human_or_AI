This paper proposes a novel approach to addressing the problem of overfitting in adaptive data analysis, where a holdout set is reused multiple times to validate the accuracy of hypotheses produced by a learning algorithm. The authors introduce two algorithms, Thresholdout and SparseValidate, which enable the validation of a large number of adaptively chosen hypotheses while provably avoiding overfitting to the holdout set.
The paper is well-motivated, and the problem of overfitting in adaptive data analysis is clearly explained. The proposed algorithms are simple and practical, and the authors provide a thorough analysis of their properties and guarantees. The experimental results demonstrate the effectiveness of the proposed approach in preventing overfitting and providing a valid estimate of classifier accuracy.
However, the paper could benefit from some editorial work to improve clarity and provide more technical details. Some sections, such as the introduction to max-information, could be more clearly explained, and the notation could be more consistently used throughout the paper. Additionally, the comparison of SEP with K=1 to DSEP with K=10 in Figure 3(c) is unclear and requires more explanation.
The use of approximate KL as a metric is also questioned, and the reviewer suggests comparing means and variances of parameter estimates instead. The switch to F-norm metric in the Mixture of Gaussians section is also unclear and requires more justification.
The experimental sections are limited, and more experiments could be conducted to provide a clearer picture of the nature of SEP, its failures, and the investigation of minibatch sizes. Section 5.3 requires a significant overhaul, with a more detailed experimental setup and technical discussion of the results.
Overall, the paper presents an original and interesting idea, but the empirical story is slightly dissatisfactory and requires refreshing and clarification. The paper contains several minor errors, such as typos, incorrect citations, and formatting issues, that need to be addressed.
Arguments for acceptance:
* The paper proposes a novel approach to addressing the problem of overfitting in adaptive data analysis.
* The proposed algorithms are simple and practical, and the authors provide a thorough analysis of their properties and guarantees.
* The experimental results demonstrate the effectiveness of the proposed approach in preventing overfitting and providing a valid estimate of classifier accuracy.
Arguments against acceptance:
* The paper could benefit from some editorial work to improve clarity and provide more technical details.
* The experimental sections are limited, and more experiments could be conducted to provide a clearer picture of the nature of SEP, its failures, and the investigation of minibatch sizes.
* The paper contains several minor errors, such as typos, incorrect citations, and formatting issues, that need to be addressed.