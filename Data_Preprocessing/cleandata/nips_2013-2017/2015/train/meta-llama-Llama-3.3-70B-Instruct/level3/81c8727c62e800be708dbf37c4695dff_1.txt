This paper presents a novel approach to learning the MultiNomial Logit (MNL) model from ordinal data, with applications in collaborative ranking and bundled choice modeling. The authors propose a convex relaxation of nuclear norm minimization to estimate the low-rank matrix representing the underlying preferences. The paper provides a thorough theoretical exploration of the topic, including upper bounds on the error and a matching information-theoretic lower bound, demonstrating the minimax-optimality of the approach.
The paper is well-written, and the authors have made a significant effort to provide a clear and concise presentation of their work. The introduction effectively motivates the problem and provides a clear overview of the contributions. The technical sections are well-organized, and the proofs are thorough and well-explained.
One of the strengths of the paper is its ability to provide a unified framework for collaborative ranking and bundled choice modeling, two scenarios that are commonly encountered in practice. The authors demonstrate the effectiveness of their approach in both settings, providing upper bounds on the error and a matching lower bound.
The paper also provides a thorough analysis of the sample complexity of the algorithm, demonstrating that the number of samples required to achieve a certain level of accuracy scales with the rank of the matrix and the dimensions of the problem. This provides valuable insights into the fundamental limits of the problem and the trade-offs between the different parameters.
However, there are some aspects of the paper that could be improved. For example, the choice of setting lambda and L to depend on d in the proof of Theorem 2 is not fully justified, and it would be helpful to provide more intuition or explanation for this choice. Additionally, the paper could benefit from more discussion on the implications of the results and the potential applications of the approach in practice.
In terms of related work, the paper could benefit from a more detailed discussion of the connections to other work in the area, such as the paper "Scalable Metric Learning for Co-embedding" by Mirzazadeh et al., which provides an efficient algorithm to globally solve the same loss as the one in the paper.
Overall, the paper presents a significant contribution to the field, providing a novel approach to learning the MNL model from ordinal data and demonstrating its effectiveness in two important scenarios. With some minor revisions to address the issues mentioned above, the paper has the potential to make a significant impact in the field.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field.
* The approach is well-motivated and effectively addresses two important scenarios.
* The paper provides a thorough theoretical exploration of the topic, including upper bounds on the error and a matching information-theoretic lower bound.
* The writing is clear and concise, and the proofs are thorough and well-explained.
Arguments con acceptance:
* The choice of setting lambda and L to depend on d in the proof of Theorem 2 is not fully justified.
* The paper could benefit from more discussion on the implications of the results and the potential applications of the approach in practice.
* The paper could benefit from a more detailed discussion of the connections to other work in the area.