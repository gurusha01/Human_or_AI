This paper presents a clean and advantageous work on sample complexity for metric learning, covering both "distance-based" and "classifier-based" frameworks. The authors provide a thorough analysis of the MultiNomial Logit (MNL) model, a popular discrete choice model, and its application to collaborative ranking and bundled choice modeling. The paper is well-written, and the authors demonstrate a clear understanding of the underlying concepts.
The strengths of the paper include its clarity and correctness, making it a worthwhile publication despite a dearth of work in the metric learning sub-community. The authors provide a comprehensive introduction to the MNL model and its applications, as well as a detailed analysis of the convex relaxation approach for learning the model from ordinal data. The theoretical guarantees provided in the paper are impressive, and the authors demonstrate a good understanding of the underlying mathematics.
However, the experiments presented in the paper are underwhelming, which is not a major drawback since the paper is more theoretical in nature. The authors could benefit from discussing existing work on the proposed regularizer, such as the ||M^T M||_F^2 regularizer, which has been explored in other studies. Additionally, the choice of methods to add the proposed regularization, such as ITML and LMNN, may not be ideal as they already have regularizers on the M matrix.
The paper could also benefit from a more detailed discussion on the experimental setup for ITML, which may lead to a low-rank solution due to its initialization with a rank-one metric and the use of the LogDet regularizer. Furthermore, the authors could provide more insights into the practical implications of their work and how it can be applied to real-world problems.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is well-organized, and the writing is clear and concise.
The originality of the paper lies in its application of the MNL model to collaborative ranking and bundled choice modeling, as well as the provision of theoretical guarantees for the convex relaxation approach. The paper adequately references related work and provides a clear understanding of how it differs from previous contributions.
The significance of the paper lies in its potential to advance the state of the art in metric learning and discrete choice modeling. The authors provide a unique theoretical and pragmatic approach to learning the MNL model from ordinal data, which can be applied to various real-world problems, such as recommendation systems and revenue management.
Overall, I would recommend accepting this paper for publication, as it presents a significant contribution to the field of metric learning and discrete choice modeling. The authors demonstrate a clear understanding of the underlying concepts and provide a thorough analysis of the MNL model and its applications.
Arguments pro acceptance:
* The paper presents a clean and advantageous work on sample complexity for metric learning.
* The authors provide a comprehensive introduction to the MNL model and its applications.
* The theoretical guarantees provided in the paper are impressive.
* The paper has the potential to advance the state of the art in metric learning and discrete choice modeling.
Arguments con acceptance:
* The experiments presented in the paper are underwhelming.
* The authors could benefit from discussing existing work on the proposed regularizer.
* The choice of methods to add the proposed regularization may not be ideal.
* The paper could benefit from a more detailed discussion on the experimental setup for ITML.