This paper proposes a novel framework for stochastic optimization in deep learning, called Elastic Averaging Stochastic Gradient Descent (EASGD), which enables parallelization of computations over multiple GPUs. The authors introduce a new algorithm that allows local workers to perform more exploration, reducing the communication overhead between the master and local workers while maintaining high-quality performance.
The paper is well-structured, and the authors provide a clear motivation for their work, discussing the challenges of parallelizing stochastic gradient descent in deep learning. The proposed algorithm is technically sound, and the authors provide a stability analysis of the asynchronous EASGD in the round-robin scheme, comparing it to the ADMM method.
The experimental setup is thorough, and the authors evaluate their algorithm on two benchmark datasets, CIFAR-10 and ImageNet, using a GPU-cluster. The results show that EASGD and its momentum-based variant, EAMSGD, outperform competitor methods, such as DOWNPOUR and its variants, in terms of convergence speed and test error.
However, there are some areas that could be improved. The paper could benefit from more discussion on the trade-off between exploration and exploitation, as well as the choice of hyperparameters, such as the communication period and the learning rate. Additionally, the authors could provide more insights into the differences between EASGD and EAMSGD, and why the latter performs better in certain scenarios.
Overall, the paper is well-written, and the authors demonstrate a good understanding of the problem and the proposed solution. The results are promising, and the algorithm has the potential to be useful in practice.
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses a challenging problem in deep learning.
* The authors provide a thorough stability analysis and experimental evaluation.
* The results show that the proposed algorithm outperforms competitor methods.
Arguments con acceptance:
* The paper could benefit from more discussion on the trade-off between exploration and exploitation.
* The choice of hyperparameters could be more thoroughly explored.
* The differences between EASGD and EAMSGD could be more deeply analyzed.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions. The authors should address the areas mentioned above to improve the paper's clarity and provide more insights into the proposed algorithm.