This paper presents a significant contribution to the field of risk-sensitive reinforcement learning by extending the policy gradient method to the whole class of coherent risk measures. The authors provide a unified approach to risk-sensitive reinforcement learning, generalizing and extending previous results that focused on specific risk measures.
The paper is well-structured, and the authors clearly explain the problem of finding policy parameters that minimize a given risk measure, with a focus on a general class of risk measures defined by coherence properties. The dual representation of coherent risk measures as a convex hull of probability distributions is a key insight, allowing for the derivation of optimal policy parameters and a gradient estimation algorithm.
The authors make a reasonable assumption on the family of coherent risk measures, assuming M-Lipschitz continuity of the convex hull's borders, to analytically derive the optimal policy parameters and gradient estimation algorithm. The paper provides a thorough analysis of the static and dynamic risk measures, including the Markov coherent risk measures, and presents a new formula for the gradient of the Markov coherent dynamic risk measure.
The numerical illustration in Section 6 demonstrates the importance of flexibility in designing risk criteria and shows how the proposed approach can be used to train risk-averse policies. The comparison with other risk measures, such as mean-standard deviation, highlights the benefits of using coherent risk measures.
The strengths of the paper include:
* A unified approach to risk-sensitive reinforcement learning that generalizes previous results
* A clear and well-structured presentation of the problem and the proposed solution
* A thorough analysis of the static and dynamic risk measures
* A numerical illustration that demonstrates the importance of flexibility in designing risk criteria
The weaknesses of the paper include:
* The assumption of M-Lipschitz continuity of the convex hull's borders may not hold in all cases
* The paper could benefit from a more detailed analysis of the convergence rate of the gradient estimates
* The numerical illustration is limited to a simple example, and more complex scenarios should be considered
Overall, the paper is well-written, and the authors provide a significant contribution to the field of risk-sensitive reinforcement learning. The proposed approach has the potential to be used in a wide range of applications, and the authors' suggestions for future work, such as improving the convergence rate of gradient estimates and designing a principled procedure for risk-shaping, are interesting and relevant.
Arguments pro acceptance:
* The paper presents a unified approach to risk-sensitive reinforcement learning that generalizes previous results
* The authors provide a clear and well-structured presentation of the problem and the proposed solution
* The paper includes a thorough analysis of the static and dynamic risk measures
* The numerical illustration demonstrates the importance of flexibility in designing risk criteria
Arguments con acceptance:
* The assumption of M-Lipschitz continuity of the convex hull's borders may not hold in all cases
* The paper could benefit from a more detailed analysis of the convergence rate of the gradient estimates
* The numerical illustration is limited to a simple example, and more complex scenarios should be considered
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Overall, I recommend accepting the paper, as it presents a significant contribution to the field of risk-sensitive reinforcement learning, and the authors provide a clear and well-structured presentation of the problem and the proposed solution. However, the authors should address the weaknesses mentioned above, such as providing a more detailed analysis of the convergence rate of the gradient estimates and considering more complex scenarios in the numerical illustration.