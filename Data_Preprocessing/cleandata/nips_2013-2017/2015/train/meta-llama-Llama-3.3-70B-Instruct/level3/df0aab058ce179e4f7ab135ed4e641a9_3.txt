This paper proposes a novel generative unsupervised model that aims to obtain representations with desirable properties such as sparsity, non-negativity, and high dimensionality. The model demonstrates good performance as an autoencoder for reconstruction and as a pre-training method for classification networks, with convincing results from both analytical and empirical studies. 
The paper is well-structured and easy to follow, with a clear introduction to the problem setting and a detailed explanation of the proposed algorithm, EASGD. The authors provide a thorough analysis of the stability of EASGD and ADMM in the round-robin scheme, which is a significant contribution to the field. The experimental results on CIFAR and ImageNet datasets demonstrate the effectiveness of EASGD and its momentum-based variant, EAMSGD, in achieving faster convergence and better test performance compared to other baseline methods.
One of the strengths of the paper is its ability to provide a simple and intuitive explanation of the proposed algorithm and its variants. The authors also provide a detailed analysis of the trade-off between exploration and exploitation as a function of the learning rate and the communication period, which is helpful in understanding the behavior of the algorithm.
However, there are some areas that require further clarification. For example, the use of five different gradient descent methods in the E-step of the learning process seems arbitrary and requires further justification. Additionally, the authors could provide more insight into the choice of hyperparameters, such as the learning rate and the communication period, and how they affect the performance of the algorithm.
To further improve the paper, the authors could consider conducting an ablative analysis of the model by removing constraints to better understand its performance and merits. This would provide a more comprehensive understanding of the strengths and weaknesses of the proposed algorithm.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a novel contribution to the field. The results are important and have the potential to be used by practitioners and researchers in the field of deep learning.
Arguments pro acceptance:
* The paper proposes a novel and effective algorithm for training deep neural networks in parallel.
* The experimental results demonstrate the superiority of the proposed algorithm over other baseline methods.
* The paper provides a thorough analysis of the stability of the proposed algorithm and its variants.
Arguments con acceptance:
* The use of five different gradient descent methods in the E-step of the learning process seems arbitrary and requires further justification.
* The authors could provide more insight into the choice of hyperparameters and how they affect the performance of the algorithm.
* The paper could benefit from an ablative analysis of the model to better understand its performance and merits.