This paper proposes a novel approach to decision tree learning, deviating from the traditional greedy methods. The authors formulate the decision tree optimization problem as a global objective, jointly optimizing the split functions at all levels of the tree with the leaf parameters. This approach is linked to structured prediction with latent variables, and a convex-concave upper bound on the tree's empirical loss is derived. The paper presents a stochastic gradient descent (SGD) algorithm to optimize this surrogate objective, enabling efficient training of deep trees.
The strengths of this paper include its originality, as it tackles the long-standing problem of greedy decision tree induction and provides a new perspective on the optimization of decision trees. The authors demonstrate the effectiveness of their approach through experiments on several benchmark datasets, showing improved performance over greedy baselines. The paper is well-written, and the ideas are clearly presented, making it easy to follow.
However, there are some weaknesses and areas for improvement. The paper assumes a specific form of the split functions, which might not be suitable for all problems. The optimization of the surrogate objective is non-convex, and the authors do not provide convergence guarantees or rates of convergence. The choice of the regularization parameter ν is crucial, and the authors rely on a grid search to find the optimal value. It would be beneficial to develop a more principled method for selecting this parameter.
The significance of this work lies in its potential to improve the performance of decision trees, which are widely used in machine learning and computer vision. The authors demonstrate that their non-greedy approach can lead to better generalization performance and reduced overfitting. The paper also opens up new avenues for research, such as exploring different forms of the split functions, incorporating sparsity-inducing regularization, and applying the kernel trick to learn higher-order split functions.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful in evaluating the strengths and weaknesses of their approach and provide a clear discussion of the limitations and potential extensions.
Overall, I would recommend accepting this paper, as it presents a novel and significant contribution to the field of decision tree learning. The authors demonstrate the effectiveness of their approach, and the paper is well-written and easy to follow. However, I would suggest that the authors address the areas for improvement mentioned above, such as providing convergence guarantees and developing a more principled method for selecting the regularization parameter.
Arguments pro acceptance:
* The paper presents a novel and original approach to decision tree learning.
* The authors demonstrate the effectiveness of their approach through experiments on several benchmark datasets.
* The paper is well-written, and the ideas are clearly presented.
Arguments con acceptance:
* The optimization of the surrogate objective is non-convex, and the authors do not provide convergence guarantees or rates of convergence.
* The choice of the regularization parameter ν is crucial, and the authors rely on a grid search to find the optimal value.
* The paper assumes a specific form of the split functions, which might not be suitable for all problems.