This paper proposes a novel algorithm for global optimization of decision trees, reformulating the problem as structured prediction with latent variables. The approach is based on a non-differentiable and non-convex loss function, which is approximated by a differentiable upper bound and minimized using stochastic gradient descent (SGD). The method's key contributions include casting global decision tree optimization as structured prediction and devising efficient algorithms to optimize the objective.
The paper is well-written, and the ideas are clearly presented. The authors provide a thorough review of related work and demonstrate the effectiveness of their approach through experiments on several benchmark datasets. The results show that the non-greedy decision trees outperform greedy decision tree baselines, and the method is less susceptible to overfitting.
However, there are some concerns and areas for improvement. The paper lacks detailed analysis of the computational complexity of the algorithm, and the experiments could be more comprehensive, including comparisons with other state-of-the-art methods. Additionally, the authors could provide more insight into the choice of hyperparameters, such as the regularization constant ν, and the learning rate η.
The paper's significance lies in its potential to improve the accuracy and efficiency of decision tree learning, which is a fundamental problem in machine learning. The approach could be applied to various applications, including computer vision and natural language processing. However, further analysis is needed to fully understand the strengths and limitations of the method.
Arguments for acceptance:
* The paper proposes a novel and interesting approach to decision tree optimization.
* The method is based on a well-founded theoretical framework, and the authors provide a clear and concise presentation of the ideas.
* The experiments demonstrate the effectiveness of the approach, and the results are promising.
Arguments against acceptance:
* The paper lacks detailed analysis of the computational complexity of the algorithm.
* The experiments could be more comprehensive, including comparisons with other state-of-the-art methods.
* The authors could provide more insight into the choice of hyperparameters and the learning rate.
Overall, the paper is well-written, and the approach is novel and interesting. With some revisions to address the concerns mentioned above, the paper could be even stronger. I would recommend acceptance, pending minor revisions to address the areas for improvement. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10