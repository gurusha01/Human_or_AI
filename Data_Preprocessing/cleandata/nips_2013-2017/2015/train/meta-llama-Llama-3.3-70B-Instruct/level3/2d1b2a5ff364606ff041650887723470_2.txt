This paper presents a convex relaxation approach to learn the MultiNomial Logit (MNL) model from ordinal data, with applications in collaborative ranking and bundled choice modeling. The authors provide theoretical guarantees for the performance of their approach, including upper bounds on the error and a minimax lower bound. The paper is well-written, and the authors have made a significant effort to provide a clear and detailed explanation of their methodology and results.
The paper extends the work on characterizing the statistical performance of the l2-lasso to single-index models, relaxing the usual Gaussian assumption on the prior. The single-index model is equivalent to a linear model with scaled unknown signal and non-standard noise, which provides a heuristic justification for the approach. The authors focus on the l2-lasso, which has an advantage over the l2^2-lasso as its regularization parameter is independent of the unknown noise variance.
The results of the paper could shed light on the practical question of whether the justification for using l2-lasso over l2^2-lasso is valid due to its oracle value of the regularization parameter. The paper provides a thorough analysis of the performance of the convex relaxation approach, including upper bounds on the error and a minimax lower bound. The authors also provide numerical experiments to illustrate the performance of their approach.
The paper is technically sound, and the authors have provided a clear and detailed explanation of their methodology and results. The paper is well-organized, and the authors have made a significant effort to provide a clear and concise presentation of their work. The paper is a significant contribution to the field, and it provides a thorough analysis of the performance of the convex relaxation approach to learn the MNL model from ordinal data.
Arguments pro acceptance:
* The paper presents a novel approach to learn the MNL model from ordinal data, with applications in collaborative ranking and bundled choice modeling.
* The authors provide theoretical guarantees for the performance of their approach, including upper bounds on the error and a minimax lower bound.
* The paper is well-written, and the authors have made a significant effort to provide a clear and detailed explanation of their methodology and results.
* The paper is technically sound, and the authors have provided a clear and detailed explanation of their methodology and results.
Arguments con acceptance:
* The paper assumes that the underlying matrix is low-rank, which may not always be the case in practice.
* The paper does not provide a comparison with other approaches to learn the MNL model from ordinal data.
* The paper does not provide a thorough analysis of the computational complexity of the approach.
Overall, I recommend accepting the paper, as it presents a novel approach to learn the MNL model from ordinal data, with applications in collaborative ranking and bundled choice modeling, and provides theoretical guarantees for the performance of the approach. However, the authors should address the limitations of the paper, including the assumption of low-rank underlying matrix and the lack of comparison with other approaches.