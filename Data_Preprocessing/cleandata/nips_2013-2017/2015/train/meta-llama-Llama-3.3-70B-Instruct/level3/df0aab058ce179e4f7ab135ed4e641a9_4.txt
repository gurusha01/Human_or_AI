This paper proposes a new algorithm, Elastic Averaging Stochastic Gradient Descent (EASGD), for parallelizing stochastic gradient descent in deep learning settings. The authors argue that their approach allows for more exploration in the model by reducing the amount of communication between local workers and the master, leading to improved performance. The paper presents both synchronous and asynchronous variants of the algorithm, as well as a momentum-based variant.
The authors provide a stability analysis of the asynchronous EASGD algorithm in the round-robin scheme, showing that it is more stable than the Alternating Direction Method of Multipliers (ADMM) algorithm. They also present experimental results on the CIFAR-10 and ImageNet datasets, demonstrating that EASGD and its momentum-based variant outperform other parallel methods, including DOWNPOUR and its variants.
However, there are several weaknesses in the paper. The authors' argument for the importance of non-negative transforms like ReLUs is weak and contradicted by other research. The goals for constructing input representations, including sparsity, non-negativity, and non-linearity, are questionable and unclear. The statement about current unsupervised deep learning approaches not modeling specific structures in the data is unclear and may be misleading.
The results presented in the paper are also limited and would have benefited from comparisons to other unsupervised methods, such as sparse coding methods, and more thorough evaluations of the method's performance on real-world examples. The classification tasks done directly on the output of the EASGD versus other unsupervised methods would have been a useful addition to the paper.
Despite these weaknesses, the paper shows promising applications to unsupervised learning, but unpromising applications to pre-training strategies for deep learning models, with significantly worse classification accuracy on CIFAR10 compared to state-of-the-art methods.
Arguments pro acceptance:
* The paper proposes a new algorithm for parallelizing stochastic gradient descent in deep learning settings.
* The authors provide a stability analysis of the asynchronous EASGD algorithm in the round-robin scheme.
* The experimental results demonstrate that EASGD and its momentum-based variant outperform other parallel methods.
Arguments con acceptance:
* The authors' argument for the importance of non-negative transforms like ReLUs is weak and contradicted by other research.
* The goals for constructing input representations are questionable and unclear.
* The results presented in the paper are limited and would have benefited from comparisons to other unsupervised methods.
* The classification tasks done directly on the output of the EASGD versus other unsupervised methods would have been a useful addition to the paper.
Overall, the paper is well-written and provides a clear explanation of the proposed algorithm and its variants. However, the weaknesses in the paper, including the limited experimental results and unclear goals for constructing input representations, need to be addressed before the paper can be considered for acceptance.