This paper proposes a novel approach to batch learning from logged bandit feedback (BLBF) by introducing a self-normalized risk estimator that avoids the propensity overfitting problem inherent in the conventional unbiased risk estimator. The authors demonstrate the effectiveness of their approach, called Norm-POEM, through extensive experiments on multi-label classification problems. The paper is well-written, and the ideas are clearly presented, making it easy to follow.
The significance of this paper lies in its potential to improve the robustness and generalization performance of BLBF algorithms, which are crucial for training online systems using historical logs. The novelty of the paper is the use of a multiplicative control variate to derive the self-normalized risk estimator, which provably avoids the anomalies of the conventional estimator.
The method itself seems sound, and the theoretical analysis provides a clear understanding of the propensity overfitting problem and how the self-normalized estimator addresses it. The experimental section demonstrates the improved performance of Norm-POEM over the conventional approach, POEM, on several datasets.
However, the experimental section is lacking in some aspects. All experiments are conducted on toy problems and datasets, and there is no comparison to more recent or relevant baselines. Additionally, the authors do not provide an analysis of the computational efficiency of Norm-POEM in comparison to other methods.
To further strengthen the paper, I suggest that the authors conduct more realistic experiments, including comparisons to other state-of-the-art methods and applications to more interesting models and real-world problems. This would provide a more comprehensive understanding of the effectiveness and efficiency of Norm-POEM.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work.
The clarity of the paper is excellent, with clear and concise writing, making it easy to understand the ideas and concepts presented. The organization of the paper is also well-structured, with a clear introduction, related work, and experimental section.
The originality of the paper is high, as it introduces a novel approach to BLBF that addresses a significant problem in the field. The paper provides a clear understanding of how the self-normalized risk estimator differs from previous contributions and how it advances the state of the art in BLBF.
Overall, I believe that this paper has the potential to make a significant contribution to the field of BLBF and machine learning in general. With some additional experiments and analysis, it could be even stronger.
Arguments for acceptance:
* The paper proposes a novel approach to BLBF that addresses a significant problem in the field.
* The self-normalized risk estimator is theoretically sound and provides improved robustness and generalization performance.
* The experimental section demonstrates the effectiveness of Norm-POEM on several datasets.
Arguments against acceptance:
* The experimental section is lacking in terms of realism and comparison to other state-of-the-art methods.
* The computational efficiency of Norm-POEM is not thoroughly analyzed.
* The paper could benefit from more comprehensive experiments and analysis to further demonstrate its effectiveness and efficiency.