This paper proposes an efficient algorithm for sparse and low-rank tensor decomposition, which is a fundamental problem in machine learning and signal processing. The authors provide a comprehensive introduction to the problem, including its motivation, related work, and problem setup. They also present a clear and well-organized algorithm, along with theoretical guarantees and numerical experiments to validate its performance.
The paper's strengths include its ability to handle sparse and low-rank tensor decomposition in a computationally efficient manner, with a time complexity of O(n^3), which is significantly better than existing methods. The authors also provide a detailed analysis of the algorithm's performance, including theoretical guarantees and numerical experiments, which demonstrate its effectiveness in recovering the underlying low-rank and sparse components.
However, there are some weaknesses and areas for improvement. One potential issue is that the algorithm's utility function may not be well-defined in non-structured settings, where the model does not propagate information between adjacent positions. Additionally, the threshold baseline used in the experiments has arbitrary parameters with unknown origins and sensitivity, which may affect the results. Furthermore, the authors could benefit from including an uncertainty sampling baseline similar to active learning for comparison, to further demonstrate the effectiveness of their approach.
Another potential concern is that the distinction between this setting and active online learning is not clear-cut, as active learning could be adapted to work in this domain with some modifications. The authors may need to provide more discussion on how their approach differs from active learning and why it is more suitable for this specific problem.
In terms of the conference guidelines, the paper meets most of the criteria. The authors provide a clear summary of the main ideas and relate them to previous work at NIPS and elsewhere. They also discuss the strengths and weaknesses of the paper, addressing the criteria described in the qualitative evaluation section. However, the authors could provide more detailed and specific comments on the paper's originality, significance, and clarity, to further demonstrate its quality as a scientific contribution to the field.
Overall, the paper presents a significant contribution to the field of sparse and low-rank tensor decomposition, with a well-organized and efficient algorithm, along with theoretical guarantees and numerical experiments. While there are some areas for improvement, the paper demonstrates a good understanding of the problem and its related work, and provides a clear and well-written presentation of the results.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of sparse and low-rank tensor decomposition.
* The algorithm is computationally efficient, with a time complexity of O(n^3), which is significantly better than existing methods.
* The authors provide a detailed analysis of the algorithm's performance, including theoretical guarantees and numerical experiments.
* The paper demonstrates a good understanding of the problem and its related work.
Arguments con acceptance:
* The algorithm's utility function may not be well-defined in non-structured settings.
* The threshold baseline used in the experiments has arbitrary parameters with unknown origins and sensitivity.
* The authors could benefit from including an uncertainty sampling baseline similar to active learning for comparison.
* The distinction between this setting and active online learning is not clear-cut.