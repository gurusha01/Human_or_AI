This paper proposes a novel optimization algorithm, HONOR, for solving non-convex regularized sparse learning problems. The algorithm incorporates second-order information to speed up convergence and uses a hybrid optimization scheme to guarantee convergence. The paper provides a rigorous convergence analysis, showing that every limit point of the sequence generated by HONOR is a Clarke critical point.
The paper is well-written, and the results are original, extending the understanding of local coherence to the case of errors in robust PCA. The experiments demonstrate that HONOR converges significantly faster than state-of-the-art algorithms, such as GIST, on large-scale data sets. The paper also provides a detailed analysis of the effect of the parameter ǫ on the convergence behavior of HONOR.
The strengths of the paper include:
* The proposal of a novel optimization algorithm, HONOR, which incorporates second-order information to speed up convergence.
* A rigorous convergence analysis, showing that every limit point of the sequence generated by HONOR is a Clarke critical point.
* Experimental results demonstrating that HONOR converges significantly faster than state-of-the-art algorithms on large-scale data sets.
The weaknesses of the paper include:
* The paper assumes that the non-convex regularizer is decomposable, which may not be the case in all applications.
* The choice of the parameter ǫ is crucial for the convergence behavior of HONOR, and the paper does not provide a clear guideline for choosing this parameter.
* The paper does not compare HONOR with other optimization algorithms, such as DC programming and DC-PN, which may be relevant for non-convex regularized sparse learning problems.
Arguments for acceptance:
* The paper proposes a novel optimization algorithm, HONOR, which has the potential to improve the state-of-the-art in non-convex regularized sparse learning problems.
* The paper provides a rigorous convergence analysis, which is essential for understanding the behavior of the algorithm.
* The experimental results demonstrate the effectiveness of HONOR on large-scale data sets.
Arguments against acceptance:
* The paper assumes that the non-convex regularizer is decomposable, which may limit the applicability of the algorithm.
* The choice of the parameter ǫ is crucial for the convergence behavior of HONOR, and the paper does not provide a clear guideline for choosing this parameter.
* The paper does not compare HONOR with other optimization algorithms, which may be relevant for non-convex regularized sparse learning problems.
Overall, the paper is well-written, and the results are original and significant. The strengths of the paper outweigh the weaknesses, and I recommend acceptance. However, the authors should be encouraged to address the weaknesses, such as providing a clear guideline for choosing the parameter ǫ and comparing HONOR with other optimization algorithms.