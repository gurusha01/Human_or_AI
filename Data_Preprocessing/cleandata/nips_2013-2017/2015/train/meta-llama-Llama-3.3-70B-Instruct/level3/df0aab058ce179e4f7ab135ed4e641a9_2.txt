This paper presents a novel approach to stochastic optimization for deep learning in a parallel computing environment, introducing the Elastic Averaging SGD (EASGD) method and its variants. The authors propose a new algorithm that enables local workers to perform more exploration, allowing the local variables to fluctuate further from the center variable, while reducing communication overhead between the master and local workers.
The paper is well-organized, and the authors provide a clear explanation of the problem setting, the EASGD algorithm, and its variants. The stability analysis of EASGD and ADMM in the round-robin scheme is thorough, and the authors demonstrate the theoretical advantage of EASGD over ADMM. The experimental results on CIFAR and ImageNet datasets show that EASGD and its momentum-based variant, EAMSGD, outperform other baseline approaches, such as DOWNPOUR and its variants.
However, there are some areas that could be improved. The empirical evaluation, while demonstrating the effectiveness of EASGD, could be more comprehensive. The authors could provide more detailed analysis of the trade-off between exploration and exploitation as a function of the learning rate and communication period. Additionally, the authors could clarify their claims about current unsupervised deep learning approaches and provide more precision on what they mean by "specific structures" in the data.
The paper raises interesting questions about the role of normalizing constraints and the success of representations that eschew sparsity altogether, such as maxout networks. The authors' response to these questions could provide valuable insights into the strengths and limitations of EASGD.
Overall, the paper presents a sound and interesting direction, and the authors demonstrate a good understanding of the problem and the proposed solution. The paper is well-written, and the authors provide sufficient details for the reader to reproduce the results.
Arguments pro acceptance:
* The paper presents a novel approach to stochastic optimization for deep learning in a parallel computing environment.
* The authors provide a thorough stability analysis of EASGD and ADMM in the round-robin scheme.
* The experimental results demonstrate the effectiveness of EASGD and its momentum-based variant, EAMSGD.
Arguments con acceptance:
* The empirical evaluation could be more comprehensive.
* The authors could provide more detailed analysis of the trade-off between exploration and exploitation.
* The paper raises some questions about the role of normalizing constraints and the success of representations that eschew sparsity altogether.
Recommendation: Accept, with minor revisions to address the areas mentioned above.