This paper proposes a novel approach to learning decision trees by optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. The authors establish a link between decision tree optimization and structured prediction with latent variables, and formulate a convex-concave upper bound on the tree's empirical loss. The paper demonstrates the effectiveness of the proposed approach through experiments on several classification benchmarks, showing that the resulting non-greedy decision trees outperform greedy decision tree baselines.
The strengths of the paper include its technical soundness, clarity, and originality. The authors provide a thorough analysis of the problem and propose a well-motivated solution. The paper is well-organized and easy to follow, with clear explanations of the key concepts and techniques. The approach is novel and significant, as it addresses a long-standing problem in decision tree learning and provides a new perspective on the field.
The weaknesses of the paper are relatively minor. One potential limitation is that the approach may not be scalable to very large datasets, due to the computational complexity of the loss-augmented inference step. However, the authors propose a fast variant of the loss-augmented inference algorithm that reduces the computational complexity, making the approach more scalable.
The significance of the paper lies in its potential to improve the accuracy and robustness of decision tree models, which are widely used in many applications. The approach has the potential to be applied to other types of tree-based models, such as random forests and gradient boosting machines, and could lead to significant improvements in the field of machine learning.
Arguments pro acceptance:
* The paper proposes a novel and significant approach to decision tree learning that addresses a long-standing problem in the field.
* The approach is technically sound and well-motivated, with a clear analysis of the problem and a well-designed solution.
* The paper demonstrates the effectiveness of the proposed approach through experiments on several classification benchmarks.
* The approach has the potential to be applied to other types of tree-based models and could lead to significant improvements in the field of machine learning.
Arguments con acceptance:
* The approach may not be scalable to very large datasets, due to the computational complexity of the loss-augmented inference step.
* The paper could benefit from more extensive experiments and comparisons with other state-of-the-art methods.
* The approach may require significant computational resources and expertise to implement and tune, which could limit its adoption in practice.
Overall, I believe that the paper makes a significant contribution to the field of machine learning and should be accepted for publication. The approach is novel, technically sound, and demonstrates significant improvements over existing methods. While there are some potential limitations and areas for improvement, these do not outweigh the strengths of the paper.