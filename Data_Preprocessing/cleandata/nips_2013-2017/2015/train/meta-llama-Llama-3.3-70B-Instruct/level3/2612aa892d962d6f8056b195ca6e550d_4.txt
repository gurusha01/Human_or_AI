This paper proposes a novel approach to decision tree learning, where the split functions at all levels of the tree are optimized jointly with the leaf parameters using a global objective. The authors establish a link between decision tree optimization and structured prediction with latent variables, and formulate a convex-concave upper bound on the tree's empirical loss. The paper is well-written, with a clear structure and appropriate level of detail, making it easy to follow.
The strengths of the paper include the technical sophistication and impressive results demonstrated by the authors. The use of stochastic gradient descent for optimization enables effective training with large datasets, and the proposed surrogate objective provides a natural way to regularize the joint optimization of tree parameters to discourage overfitting. The experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.
However, there are some weaknesses and areas for improvement. The motivations for the extra optimization step in section 7 could be better explained, particularly in relation to the residual trajectory cost or policy regression error. Additionally, the paper's results and proposed optimization scheme appear impressive, but the reviewer is unsure about the novelty and significance of the research due to limited familiarity with the domain.
In terms of quality, the paper is technically sound, with claims well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is a complete piece of work, rather than a position paper, and provides enough information for the expert reader to reproduce the results.
The clarity of the paper is excellent, with a well-organized structure and clear explanations of the key concepts and ideas. The paper adequately informs the reader, providing sufficient background information and context for the research.
The originality of the paper is high, with a novel combination of familiar techniques and a clear explanation of how the work differs from previous contributions. The related work is adequately referenced, and the authors demonstrate a good understanding of the existing literature in the field.
The significance of the paper is also high, with results that are important and likely to be used by other researchers or practitioners. The paper addresses a difficult problem in a better way than previous research, and advances the state of the art in a demonstrable way.
Overall, the paper is well-written, technically sound, and makes a significant contribution to the field. The reviewer recommends acceptance of the paper, with some minor revisions to address the areas for improvement mentioned above.
Arguments for acceptance:
* The paper proposes a novel approach to decision tree learning, with a global objective and joint optimization of split functions and leaf parameters.
* The authors demonstrate impressive results on several classification benchmarks, outperforming greedy decision tree baselines.
* The paper is technically sound, with claims well-supported by theoretical analysis and experimental results.
* The paper is well-written, with a clear structure and appropriate level of detail, making it easy to follow.
Arguments against acceptance:
* The motivations for the extra optimization step in section 7 could be better explained.
* The reviewer is unsure about the novelty and significance of the research due to limited familiarity with the domain.
* Some minor revisions are needed to address the areas for improvement mentioned above.