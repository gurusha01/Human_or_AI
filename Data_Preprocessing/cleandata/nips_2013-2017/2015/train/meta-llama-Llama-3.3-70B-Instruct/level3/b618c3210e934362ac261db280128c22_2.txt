This paper proposes a novel model, the Variational Recurrent Neural Network (VRNN), which incorporates latent random variables into a recurrent neural network (RNN) to improve its generative power. The authors demonstrate that the VRNN outperforms previous RNN-based models on several speech and handwriting datasets, showcasing its ability to model complex sequences. The paper is well-written, and the experiments are thorough, but there are some concerns regarding the novelty and clarity of the model.
The main strength of the paper is its empirical evaluation, which shows significant improvements in modeling highly structured sequences such as natural speech. The authors also provide a clear explanation of the VRNN model and its components, making it easy to follow. However, the paper could benefit from more analysis on why the introduction of latent random variables improves the model's performance. Additionally, the figure provided is confusing, and the model is a slight modification of previous work, such as [1], which makes it difficult to assess the true novelty of the contribution.
Another concern is the use of the same hidden state for generation and inference, which lacks motivation and differs from previous work. The experiments with speech also lack clarity on the windowing process for the 200 waveform samples, specifically whether it was overlapping from sample to sample. Furthermore, the paper could be improved by referencing DRAW as another application of a VRNN-like architecture and providing more context on the novelty of adding latent variables to RNNs despite being explored in previous work.
In terms of the conference guidelines, the paper meets the criteria for quality, as it is technically sound and well-supported by experimental results. The paper is also clear and well-organized, making it easy to follow. However, the originality of the paper is a concern, as it builds upon previous work and the novelty of the contribution is not entirely clear. The significance of the paper is high, as it addresses a difficult problem in sequence modeling and provides a unique solution.
Arguments for acceptance:
* The paper proposes a novel model that improves the generative power of RNNs
* The empirical evaluation is thorough and shows significant improvements on several datasets
* The paper is well-written and easy to follow
Arguments against acceptance:
* The novelty of the contribution is not entirely clear, as it builds upon previous work
* The figure provided is confusing, and the model lacks motivation in some aspects
* The paper could benefit from more analysis on why the introduction of latent random variables improves the model's performance
Overall, the paper is a good contribution to the field, but it requires some revisions to address the concerns mentioned above. With some improvements, the paper has the potential to be a strong contribution to the conference.