This paper proposes a novel algorithm for non-greedy learning of decision trees, which optimizes the split functions at all levels of the tree jointly with the leaf parameters based on a global objective. The authors establish a link between decision tree optimization and structured prediction with latent variables, and formulate a convex-concave upper bound on the tree's empirical loss. The paper is well-written, and the math is clearly explained. The experiments demonstrate that the non-greedy decision trees outperform greedy decision tree baselines on several classification benchmarks.
The strengths of the paper include the novelty of the approach, the clarity of the mathematical formulation, and the effectiveness of the algorithm in practice. The authors provide a thorough analysis of the computational complexity of the algorithm and demonstrate its scalability. The paper also provides a comprehensive review of related work and clearly highlights the contributions of the proposed approach.
However, there are some weaknesses in the paper. The comparative evaluation is limited, and the authors only compare their method with a few greedy baselines. A more thorough comparison with other state-of-the-art methods would strengthen the paper. Additionally, the authors could provide more insights into the choice of hyperparameters, such as the regularization constant ν, and the learning rate η.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is well-organized, and the writing is clear and concise.
In terms of originality, the paper proposes a novel approach to decision tree learning, which is a significant contribution to the field. The authors establish a new link between decision tree optimization and structured prediction with latent variables, which is a new and interesting perspective.
In terms of significance, the paper has the potential to impact the field of machine learning and decision tree learning. The proposed approach can be applied to a wide range of classification and regression tasks, and the authors demonstrate its effectiveness on several benchmarks.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of decision tree learning and has the potential to impact the broader field of machine learning. However, I would suggest that the authors provide more insights into the choice of hyperparameters and consider a more thorough comparison with other state-of-the-art methods.
Arguments pro acceptance:
* The paper proposes a novel and significant approach to decision tree learning.
* The authors provide a thorough analysis of the computational complexity of the algorithm and demonstrate its scalability.
* The paper is well-written, and the math is clearly explained.
* The experiments demonstrate the effectiveness of the algorithm in practice.
Arguments con acceptance:
* The comparative evaluation is limited, and the authors only compare their method with a few greedy baselines.
* The authors could provide more insights into the choice of hyperparameters, such as the regularization constant ν, and the learning rate η.
* The paper could benefit from a more thorough comparison with other state-of-the-art methods.