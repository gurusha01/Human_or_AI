This paper presents a significant contribution to the field of topological data analysis (TDA) by introducing a universal kernel for persistence diagrams, which enables the use of this topological summary representation in the framework of embedding probability measures into reproducing kernel Hilbert spaces. The authors demonstrate the effectiveness of their approach through experiments on synthetic and real-world data, showcasing its potential for two-sample hypothesis testing and statistical analysis of persistence diagrams.
The paper is well-crafted and insightful, providing a clear and concise introduction to the background material and notation. The authors' use of the persistence scale space (PSS) kernel and its universal variant (u-PSS) is well-motivated and thoroughly explained. The experimental results are convincing, and the comparison to previous work is helpful in understanding the advantages of the proposed approach.
One of the strengths of the paper is its ability to bridge the gap between TDA and kernel-based learning techniques, providing a principled way to perform statistical computations with persistence diagrams. The authors' proof of the universality of the u-PSS kernel is a significant contribution, and their discussion of the relation to persistence landscapes is informative.
However, the paper may not have a significant impact on practitioners due to the lack of take-away messages or prescriptions for applying the proposed approach to real-world problems. Additionally, some technical questions arise, such as the application of the data processing inequality and the construction of the Markov chain in the proof of Theorem 1.
To improve the paper, the authors could clarify the distinction between "inference process" and "learning" and provide more discussion on why the distinctions between "learnability", "consistency", "uniform convergence", and "(uniform) generalization" matter. Furthermore, the definition of "learnability" used in the paper seems to differ from the standard definition of PAC learnability, which could be addressed in a future revision.
Overall, the paper is a valuable contribution to the field of TDA, and its strengths outweigh its weaknesses. With some revisions to address the technical questions and provide more context for practitioners, the paper has the potential to make a significant impact on the field.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of TDA
* The authors demonstrate the effectiveness of their approach through experiments on synthetic and real-world data
* The paper provides a clear and concise introduction to the background material and notation
* The authors' use of the PSS kernel and its universal variant is well-motivated and thoroughly explained
Arguments con acceptance:
* The paper may not have a significant impact on practitioners due to the lack of take-away messages or prescriptions
* Some technical questions arise, such as the application of the data processing inequality and the construction of the Markov chain in the proof of Theorem 1
* The definition of "learnability" used in the paper seems to differ from the standard definition of PAC learnability.