This paper presents a novel approach to risk-sensitive reinforcement learning, extending the policy gradient method to the whole class of coherent risk measures. The authors propose a unified framework for both static and dynamic risk measures, providing a new formula for the gradient of the Markov coherent dynamic risk measure. The paper is well-structured, and the authors provide a clear and concise introduction to the problem, related work, and their contributions.
The key strengths of the paper are:
* The authors provide a novel and unified approach to risk-sensitive reinforcement learning, generalizing previous results and extending the policy gradient method to the whole class of coherent risk measures.
* The paper presents a clear and concise introduction to the problem, related work, and the authors' contributions, making it easy to follow and understand.
* The authors provide a detailed analysis of the algorithm, including a proof of convergence and a discussion of the time complexity.
* The numerical illustration in Section 6 demonstrates the importance of flexibility in designing risk criteria and shows the potential of the approach in real-world applications.
However, there are some weaknesses and areas for improvement:
* The paper assumes that the risk envelope U(Pθ) is given in a canonical convex programming formulation, which might not be the case in all applications.
* The algorithm's time complexity is noted to be suitable only for small to medium scale problems, which might limit its applicability in large-scale applications.
* The paper could benefit from more extensive numerical experiments and comparisons with other state-of-the-art methods to demonstrate the effectiveness and efficiency of the proposed approach.
Overall, the paper presents a significant contribution to the field of risk-sensitive reinforcement learning, and the authors' approach has the potential to be applied in a wide range of applications. With some revisions to address the weaknesses and areas for improvement, the paper could be even stronger.
Arguments pro acceptance:
* The paper presents a novel and unified approach to risk-sensitive reinforcement learning.
* The authors provide a clear and concise introduction to the problem, related work, and their contributions.
* The paper includes a detailed analysis of the algorithm, including a proof of convergence and a discussion of the time complexity.
* The numerical illustration demonstrates the importance of flexibility in designing risk criteria and shows the potential of the approach in real-world applications.
Arguments con acceptance:
* The paper assumes that the risk envelope U(Pθ) is given in a canonical convex programming formulation, which might not be the case in all applications.
* The algorithm's time complexity is noted to be suitable only for small to medium scale problems, which might limit its applicability in large-scale applications.
* The paper could benefit from more extensive numerical experiments and comparisons with other state-of-the-art methods to demonstrate the effectiveness and efficiency of the proposed approach.