This paper proposes a novel optimization algorithm, HONOR, for solving non-convex regularized sparse learning problems. The algorithm incorporates second-order information to speed up convergence and uses a hybrid optimization scheme to guarantee convergence. The authors provide a rigorous convergence analysis, showing that every limit point of the sequence generated by HONOR is a Clarke critical point.
The paper is well-written, and the authors provide a clear and detailed explanation of the algorithm and its convergence analysis. The experimental results demonstrate the effectiveness of HONOR in solving large-scale non-convex sparse learning problems, outperforming state-of-the-art algorithms such as GIST.
One of the strengths of the paper is the careful design of the hybrid optimization scheme, which allows HONOR to adapt to different scenarios and converge to a Clarke critical point. The use of second-order information also significantly speeds up the convergence of the algorithm.
However, there are some potential weaknesses and areas for improvement. The choice of initial solution may impact the results, and the authors do not provide a clear explanation of how the initial solution was selected in the numerical experiments. Additionally, the use of second-order information may come at the cost of increased memory usage, which could be a concern for highly non-convex problems.
To improve the paper, the authors could provide more details on the selection of the initial solution and its impact on the results. They could also discuss the potential issues related to memory usage and provide suggestions for mitigating these issues.
Overall, the paper presents a significant contribution to the field of optimization and sparse learning, and the proposed algorithm has the potential to be widely used in practice. The strengths of the paper outweigh its weaknesses, and I would recommend accepting the paper with minor revisions.
Arguments pro acceptance:
* The paper proposes a novel and effective optimization algorithm for non-convex regularized sparse learning problems.
* The algorithm incorporates second-order information to speed up convergence and uses a hybrid optimization scheme to guarantee convergence.
* The experimental results demonstrate the effectiveness of HONOR in solving large-scale non-convex sparse learning problems.
Arguments con acceptance:
* The choice of initial solution may impact the results, and the authors do not provide a clear explanation of how the initial solution was selected.
* The use of second-order information may come at the cost of increased memory usage, which could be a concern for highly non-convex problems.
Recommendation: Accept with minor revisions. The authors should provide more details on the selection of the initial solution and its impact on the results, and discuss the potential issues related to memory usage.