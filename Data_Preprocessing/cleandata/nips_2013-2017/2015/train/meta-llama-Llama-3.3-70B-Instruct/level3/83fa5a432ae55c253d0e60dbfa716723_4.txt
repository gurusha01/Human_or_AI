This paper proposes a novel approach to risk-sensitive reinforcement learning by extending the policy gradient method to the whole class of coherent risk measures. The authors provide a unified framework for both static and dynamic risk measures, which is a significant contribution to the field. The paper is well-written, and the ideas are clearly presented.
The key strength of this paper is its ability to generalize previous results on risk-sensitive reinforcement learning to a broader class of risk measures. The authors demonstrate the effectiveness of their approach through numerical examples, which highlights the importance of flexibility in designing risk criteria. The paper also provides a clear and concise overview of the related work, making it easy to understand the context and significance of the contributions.
One of the weaknesses of the paper is that the importance of the experiments is not fully evaluable, as the authors do not provide a comprehensive comparison with other state-of-the-art methods. Additionally, the paper assumes that the risk envelope is known in an explicit form, which might not always be the case in practice.
Despite these limitations, the paper is of high quality and makes a significant contribution to the field of reinforcement learning. The authors provide a detailed and thorough analysis of the problem, and their approach has the potential to be applied to a wide range of applications.
Here are the arguments for and against acceptance:
Arguments for acceptance:
* The paper proposes a novel and unified framework for risk-sensitive reinforcement learning.
* The approach is able to generalize previous results to a broader class of risk measures.
* The paper provides a clear and concise overview of the related work.
* The numerical examples demonstrate the effectiveness of the approach.
Arguments against acceptance:
* The importance of the experiments is not fully evaluable.
* The paper assumes that the risk envelope is known in an explicit form, which might not always be the case in practice.
Overall, I recommend acceptance of the paper, considering it to be of NIPS quality. The paper makes a significant contribution to the field, and its strengths outweigh its weaknesses. With some minor revisions to address the limitations, the paper has the potential to be a valuable addition to the conference proceedings. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, the importance of the experiments is not fully evaluable, which slightly reduces the quality score.
Clarity: 9/10
The paper is clearly written, and the ideas are well-presented. The authors provide a concise overview of the related work, making it easy to understand the context and significance of the contributions.
Originality: 9/10
The paper proposes a novel and unified framework for risk-sensitive reinforcement learning, which is a significant contribution to the field. The approach is able to generalize previous results to a broader class of risk measures.
Significance: 9/10
The paper has the potential to be applied to a wide range of applications, and the approach can be used to design more effective risk-sensitive reinforcement learning algorithms. The numerical examples demonstrate the effectiveness of the approach, which highlights its significance.