This paper presents a novel approach to risk-sensitive reinforcement learning, extending the policy gradient method to the whole class of coherent risk measures. The authors provide a unified framework for both static and dynamic risk measures, which is a significant technical contribution to the field. The paper is well-written, with sufficient background material on coherent risk measures and policy gradient methods, making it easy to understand for readers.
The strengths of the paper include:
* The authors provide a new formula for the gradient of static coherent risk measures, which is convenient for approximation using sampling.
* The paper presents an actor-critic style algorithm for dynamic Markov coherent risk measures, which is a significant contribution to the field.
* The authors demonstrate the importance of flexibility in designing risk criteria for selecting an appropriate risk-measure, using a numerical example.
* The paper provides a thorough analysis of the convergence of the proposed algorithms and the gradient error incurred from function approximation.
The weaknesses of the paper include:
* The paper assumes that the risk envelope is known in an explicit form, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithms, which is an important consideration in practice.
* The paper could benefit from more numerical examples to demonstrate the effectiveness of the proposed approach in different scenarios.
Overall, the paper is well-written, and the authors provide a significant technical contribution to the field of risk-sensitive reinforcement learning. The proposed approach has the potential to be useful in a wide range of applications, including finance, operations research, and robotics.
Arguments pro acceptance:
* The paper presents a novel and significant technical contribution to the field of risk-sensitive reinforcement learning.
* The authors provide a thorough analysis of the convergence of the proposed algorithms and the gradient error incurred from function approximation.
* The paper demonstrates the importance of flexibility in designing risk criteria for selecting an appropriate risk-measure.
Arguments con acceptance:
* The paper assumes that the risk envelope is known in an explicit form, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithms, which is an important consideration in practice.
* The paper could benefit from more numerical examples to demonstrate the effectiveness of the proposed approach in different scenarios.
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 9/10
Overall, I would recommend accepting this paper, as it presents a significant technical contribution to the field of risk-sensitive reinforcement learning, and the authors provide a thorough analysis of the convergence of the proposed algorithms and the gradient error incurred from function approximation. However, the authors should address the weaknesses mentioned above to improve the paper.