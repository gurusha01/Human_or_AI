This paper proposes a novel distributed stochastic gradient learning scheme, called Elastic Averaging SGD (EASGD), which is designed to be robust to infrequent synchronization and favors exploration, leading to better solutions. The algorithm is simple and effective, performing well in experiments compared to existing algorithms. However, the paper lacks experimentation on architectures other than CNNs, which limits its scope.
The authors' review of existing literature is insufficient, and a more thorough analysis is required to support their positive results on distributed stochastic optimization in nonconvex losses. Additionally, the algorithm outperforms existing distributed techniques, but lacks comparisons with non-deep learning oriented distribution techniques that have been widely studied in the optimization literature.
The paper revisits an old method, but its coverage of existing implementations is limited, with scarce in-depth analysis beyond ADMM and brief references in the introduction. The stability analysis of EASGD and ADMM in the round-robin scheme is a significant contribution, and the authors demonstrate that EASGD is more stable than ADMM.
The experimental results demonstrate the effectiveness of EASGD and its momentum-based variant, EAMSGD, in training deep neural networks on CIFAR and ImageNet datasets. The results show that EAMSGD outperforms comparator methods, including DOWNPOUR and its variants, in terms of convergence speed and test error.
Arguments pro acceptance:
* The paper proposes a novel and effective distributed stochastic gradient learning scheme.
* The algorithm is simple and easy to implement.
* The experimental results demonstrate the effectiveness of EASGD and EAMSGD in training deep neural networks.
* The stability analysis of EASGD and ADMM is a significant contribution.
Arguments con acceptance:
* The paper lacks experimentation on architectures other than CNNs.
* The review of existing literature is insufficient.
* The algorithm lacks comparisons with non-deep learning oriented distribution techniques.
* The paper revisits an old method, but its coverage of existing implementations is limited.
Overall, the paper makes a significant contribution to the field of distributed stochastic optimization, and the proposed algorithm has the potential to be widely adopted. However, the limitations of the paper, including the lack of experimentation on other architectures and the insufficient review of existing literature, need to be addressed in future work. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, the review of existing literature is insufficient, and the paper lacks experimentation on architectures other than CNNs.
Clarity: 9/10
The paper is well-written, and the organization is clear. The authors provide enough information for the expert reader to reproduce the results.
Originality: 8/10
The paper proposes a novel distributed stochastic gradient learning scheme, but it revisits an old method. The stability analysis of EASGD and ADMM is a significant contribution.
Significance: 9/10
The paper addresses a difficult problem in distributed stochastic optimization, and the proposed algorithm has the potential to be widely adopted. The experimental results demonstrate the effectiveness of EASGD and EAMSGD in training deep neural networks.