This paper proposes a novel framework for "on-the-job learning" that leverages crowdsourcing to improve the accuracy of a machine learning model over time, while reducing the cost of querying the crowd. The authors model this setting as a stochastic game, where the system and the crowd interact to achieve a balance between latency, cost, and accuracy. They develop an approximation algorithm based on Monte Carlo Tree Search to find the optimal policy for querying the crowd.
The paper is well-motivated, and the problem formulation is clear and well-defined. The use of Bayesian decision theory to trade off latency, cost, and accuracy is a nice touch, and the authors provide a thorough analysis of the game tree and the utility function. The experimental evaluation is also comprehensive, with three different tasks and several baselines for comparison.
However, there are some areas where the paper could be improved. The presentation lacks important details, such as the definition of N(s) in Algorithm 1, and the derivation of threshold values in the baseline. Additionally, the empirical results are not entirely convincing, with the proposed approach only outperforming the threshold-based heuristic by a small margin. The authors also raise several technical questions, such as the issues with Equation (2), which need to be addressed.
Furthermore, the paper could benefit from more discussion on the tasks used, the comparison to state-of-the-art machine learning systems, and error analysis to understand the gains from human annotation. The authors also mention that the results varied based on the current worker quality, but do not provide a detailed analysis of this effect.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The writing is clear, and the organization is good, although some sections could be improved with more details. The paper proposes a novel approach to on-the-job learning, which is a significant contribution to the field.
Overall, I would recommend accepting this paper, but with some revisions to address the technical questions and provide more details on the experimental evaluation. The strengths of the paper include its novel approach, thorough analysis, and comprehensive experimental evaluation. The weaknesses include the lack of detail in some sections, the need for more discussion on the tasks and comparison to state-of-the-art systems, and the limited margin of improvement over the baseline.
Arguments for acceptance:
* Novel approach to on-the-job learning
* Thorough analysis of the game tree and utility function
* Comprehensive experimental evaluation
* Significant contribution to the field
Arguments against acceptance:
* Lack of detail in some sections
* Limited margin of improvement over the baseline
* Need for more discussion on the tasks and comparison to state-of-the-art systems
* Technical questions raised by the authors need to be addressed.