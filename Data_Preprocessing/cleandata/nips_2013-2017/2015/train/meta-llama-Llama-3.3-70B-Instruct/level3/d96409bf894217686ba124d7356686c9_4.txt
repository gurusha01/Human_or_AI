This paper proposes a novel Gaussian Process (GP) based optimization algorithm, called Infinite-Metric GP Optimization (IMGPO), which achieves an exponential convergence rate without the need for auxiliary optimization and the δ-cover sampling procedure. The algorithm leverages the idea of considering infinitely many possible candidates of bounds, rather than relying on a single bound given by the GP prior. This approach allows the algorithm to adapt to the unknown semi-metric of the objective function, leading to improved performance.
The paper provides a thorough analysis of the algorithm, including a proof of the exponential convergence rate and a discussion of the effects of the tightness of the Upper Confidence Bound (UCB) computed by GP. The authors also provide an abstract version of the regret bound for IMGPO with a family of division procedures, which could be used to design new division procedures.
The experimental results show that IMGPO outperforms other state-of-the-art algorithms, including SOO, BaMSOO, GP-PI, and GP-EI, on a variety of benchmark functions. The authors also demonstrate the scalability of IMGPO by applying it to a 1000-dimensional function using REMBO.
However, there are some limitations and potential areas for improvement. The algorithm's performance can be affected by the choice of hyperparameters, such as Ξmax, and the authors suggest that increasing Ξmax can improve performance at the cost of extra CPU time. Additionally, the algorithm's scalability to higher dimensions may be limited, and the authors suggest that leveraging additional assumptions, such as those in [14, 20], may be necessary to address this challenge.
Overall, the paper presents a significant contribution to the field of Bayesian optimization, providing a novel algorithm that achieves exponential convergence without the need for auxiliary optimization and δ-cover sampling. The thorough analysis and experimental results demonstrate the effectiveness of the algorithm, and the authors provide a clear and well-organized presentation of their work.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of Bayesian optimization.
* The algorithm achieves exponential convergence without the need for auxiliary optimization and δ-cover sampling.
* The paper provides a thorough analysis of the algorithm, including a proof of the exponential convergence rate.
* The experimental results demonstrate the effectiveness of the algorithm on a variety of benchmark functions.
Arguments con acceptance:
* The algorithm's performance can be affected by the choice of hyperparameters, such as Ξmax.
* The algorithm's scalability to higher dimensions may be limited.
* The paper could benefit from additional experimental results and comparisons to other state-of-the-art algorithms.
Rating: 8/10
The paper is well-written, and the authors provide a clear and thorough presentation of their work. The algorithm is novel and significant, and the experimental results demonstrate its effectiveness. However, there are some limitations and potential areas for improvement, such as the choice of hyperparameters and scalability to higher dimensions. Overall, the paper is a strong contribution to the field of Bayesian optimization, and I recommend acceptance with minor revisions.