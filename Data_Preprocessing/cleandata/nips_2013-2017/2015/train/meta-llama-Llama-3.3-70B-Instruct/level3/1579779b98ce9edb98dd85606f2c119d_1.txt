This paper proposes a novel approach to decision tree learning, formulating the problem as a global optimization of the split functions at all levels of the tree jointly with the leaf parameters. The authors establish a link between decision tree optimization and structured prediction with latent variables, and develop a convex-concave upper bound on the tree's empirical loss. The paper is well-written, and the ideas are clearly presented. The authors provide a thorough review of related work and demonstrate the effectiveness of their approach through experiments on several benchmark datasets.
The strengths of the paper include its elegant approach to decision tree learning, which has the potential to lead to more research in this area. The authors' formulation of the problem as a global optimization, rather than a greedy node-by-node optimization, is a significant contribution. The use of stochastic gradient descent to optimize the surrogate objective is also a good choice, as it enables effective training with large datasets.
One of the weaknesses of the paper is the somewhat disappointing experimental results. While the authors demonstrate that their non-greedy decision trees outperform greedy decision tree baselines, the improvements are not dramatic. However, this does not detract from the significance of the paper, as the authors' approach has the potential to lead to more research and improvements in the field.
A potential improvement to the paper could be to combine the global optimization with standard optimization techniques for the last split and leaf node in each tree. This could help to improve the performance of the decision trees, especially in cases where the global optimization does not lead to significant improvements.
The authors should also evaluate the potential of their approach to lead to overfitting, as the global optimization may result in a more complex tree that is prone to overfitting. However, the authors' use of regularization techniques, such as constraining the norm of the weight vectors, may help to mitigate this issue.
Overall, I believe that this paper deserves publication due to its elegant approach and potential to lead to more research in the field. The authors' contributions are significant, and their paper is well-written and easy to follow. With some minor improvements, this paper has the potential to make a significant impact in the field of decision tree learning.
Arguments pro acceptance:
* Elegant approach to decision tree learning
* Potential to lead to more research in the field
* Effective use of stochastic gradient descent
* Significant contributions to the field
Arguments con acceptance:
* Somewhat disappointing experimental results
* Potential for overfitting
* Limited improvements over greedy decision tree baselines
However, the strengths of the paper outweigh its weaknesses, and I believe that it deserves publication.