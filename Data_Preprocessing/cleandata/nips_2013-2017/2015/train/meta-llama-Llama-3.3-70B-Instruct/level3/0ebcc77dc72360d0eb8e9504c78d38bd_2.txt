This paper presents a novel Bayesian optimization method, Infinite-Metric GP Optimization (IMGPO), which achieves an exponential convergence rate without the need for auxiliary optimization and Î´-cover sampling. The algorithm uses a space partitioning approach to generate future samples, leveraging the information encoded in the Gaussian Process (GP) prior to reduce the degree of unknownness of the semi-metric. The authors provide a clear description of the algorithm, along with an example, and demonstrate its effectiveness through experimental results on various test functions.
The paper makes a significant contribution to the field of Bayesian optimization, addressing a long-standing open problem of creating a GP-based algorithm with an exponential convergence rate without impractical sampling procedures. The authors provide a thorough analysis of the algorithm, including a discussion of the infinite-metric exploration loss and the effect of the tightness of the Upper Confidence Bound (UCB) computed by GP.
The experimental results show that IMGPO outperforms other state-of-the-art algorithms, including SOO, BaMSOO, GP-PI, and GP-EI, on various test functions. The authors also provide a comparison of the computational time, showing that IMGPO is much faster than traditional GP optimization methods, although slower than SOO.
However, the paper also has some limitations. The experimental results are limited to 1D test functions, raising concerns about the scalability of the approach to higher dimensions. The authors acknowledge this limitation and suggest that additional assumptions, such as those in [14, 20], may be necessary to address this issue.
In terms of clarity, the paper is well-organized and easy to follow, with clear descriptions of the algorithm and its components. The authors provide sufficient details for the reader to reproduce the results, including the source code of the proposed algorithm.
Overall, the paper presents a significant contribution to the field of Bayesian optimization, providing a novel algorithm with exponential convergence rate and demonstrating its effectiveness through experimental results. While there are some limitations, the paper provides a clear and well-organized presentation of the ideas, making it a valuable contribution to the field.
Arguments pro acceptance:
* The paper presents a novel Bayesian optimization method with exponential convergence rate, addressing a long-standing open problem.
* The algorithm is clearly described, and the authors provide sufficient details for the reader to reproduce the results.
* The experimental results show that IMGPO outperforms other state-of-the-art algorithms on various test functions.
Arguments con acceptance:
* The experimental results are limited to 1D test functions, raising concerns about the scalability of the approach to higher dimensions.
* The paper could benefit from additional discussions on the potential applications and limitations of the algorithm in practice.
* Some minor comments, such as clarifying the use of UCB and explaining certain statements, could improve the clarity of the paper.