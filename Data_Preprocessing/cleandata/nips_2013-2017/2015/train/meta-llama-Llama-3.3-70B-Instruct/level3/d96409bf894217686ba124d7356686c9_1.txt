This paper presents a novel Bayesian optimization method, Infinite-Metric GP Optimization (IMGPO), which achieves an exponential convergence rate without the need for auxiliary optimization and δ-cover sampling. The authors propose a new approach that leverages the existence of an unknown bound encoded in the continuity at a global optimizer, rather than relying on a single bound given by the Gaussian Process (GP) prior.
The paper is well-structured and provides a clear overview of the proposed method, including its technical details and relationship to previous algorithms. The authors also provide a thorough analysis of the method, including a proof of its exponential convergence rate and a discussion of the effects of the tightness of the Upper Confidence Bound (UCB) by GP and the use of GP.
The experimental results demonstrate the effectiveness of IMGPO compared to other state-of-the-art algorithms, including SOO, BaMSOO, GP-PI, and GP-EI. The results show that IMGPO outperforms these algorithms in general, although it may be slower than SOO.
The strengths of the paper include:
* The proposal of a novel Bayesian optimization method that achieves an exponential convergence rate without the need for auxiliary optimization and δ-cover sampling.
* A thorough analysis of the method, including a proof of its exponential convergence rate and a discussion of the effects of the tightness of the UCB by GP and the use of GP.
* Experimental results that demonstrate the effectiveness of IMGPO compared to other state-of-the-art algorithms.
The weaknesses of the paper include:
* The method may be slower than SOO, which could be a limitation in practice.
* The choice of hyperparameters, such as Ξmax, may affect the performance of IMGPO, and it is not clear how to choose these hyperparameters in general.
* The method assumes that the objective function is a non-convex black-box function, which may not be the case in all applications.
Overall, the paper presents a significant contribution to the field of Bayesian optimization, and the proposed method has the potential to be widely applicable in practice. However, further research is needed to address the limitations of the method and to explore its applications in different domains.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of Bayesian optimization.
* The proposed method achieves an exponential convergence rate without the need for auxiliary optimization and δ-cover sampling.
* The experimental results demonstrate the effectiveness of IMGPO compared to other state-of-the-art algorithms.
Arguments against acceptance:
* The method may be slower than SOO, which could be a limitation in practice.
* The choice of hyperparameters, such as Ξmax, may affect the performance of IMGPO, and it is not clear how to choose these hyperparameters in general.
* The method assumes that the objective function is a non-convex black-box function, which may not be the case in all applications.