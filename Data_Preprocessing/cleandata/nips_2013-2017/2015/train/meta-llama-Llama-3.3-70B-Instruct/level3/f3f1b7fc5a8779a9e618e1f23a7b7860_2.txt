This paper presents a novel approach to unsupervised learning of generic sentence representations, called skip-thought vectors. The authors propose an encoder-decoder model that learns to reconstruct surrounding sentences of an encoded passage, allowing sentences with similar semantic and syntactic properties to be mapped to similar vector representations. The paper is well-structured and easy to follow, with clear explanations of the model, experimental setup, and results.
The technical aspects of the paper are strong, with good theoretical results and a well-designed experimental setup. The authors evaluate their model on 8 tasks, including semantic relatedness, paraphrase detection, image-sentence ranking, and sentiment classification, and demonstrate that skip-thought vectors perform well across all tasks. The results are impressive, with skip-thought vectors outperforming previous systems on several tasks and achieving state-of-the-art performance on others.
However, the paper could be improved in terms of motivation, particularly for the transition from sub-Gaussian to sub-exponential designs. The authors could provide more context and explanation for this transition, as it may not be immediately clear to readers outside the theoretical sub-community. Additionally, there are a few typos and missing words or phrases, such as "at least" in lines 321, 330, 344, 358, and 368, which should be corrected.
In terms of originality, the paper presents a novel approach to learning sentence representations, and the results demonstrate the effectiveness of this approach. The authors also provide a clear comparison to previous work, highlighting the strengths and weaknesses of their approach. The significance of the paper is high, as it presents a new and effective method for learning sentence representations that can be used in a variety of applications.
Overall, I would recommend accepting this paper, as it presents a strong technical contribution with good results and a clear explanation of the approach. However, the authors should address the minor issues with typos and motivation to improve the overall quality of the paper.
Arguments pro acceptance:
* Strong technical contribution with good results
* Well-structured and easy to follow paper
* Clear explanation of the model and experimental setup
* Effective method for learning sentence representations with potential for use in a variety of applications
Arguments con acceptance:
* Minor issues with typos and missing words or phrases
* Could benefit from more context and explanation for the transition from sub-Gaussian to sub-exponential designs
* May not be immediately clear to readers outside the theoretical sub-community.