This paper proposes a novel technique called Principal Differences Analysis (PDA) for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. The authors also introduce a sparse variant of the method, called SPARDA, which identifies features responsible for the differences.
The paper is well-written, and the authors provide a clear explanation of the methodology and its applications. The experiments demonstrate the effectiveness of PDA and SPARDA in identifying differences between populations, especially in high-dimensional settings. The results show that SPARDA outperforms other methods, such as sparse PCA and logistic regression, in identifying relevant features.
The strengths of the paper include:
* The introduction of a new methodology for analyzing differences between high-dimensional distributions, which has the potential to be widely applicable in various fields.
* The provision of a clear and detailed explanation of the methodology, including the mathematical formulations and algorithms.
* The demonstration of the effectiveness of PDA and SPARDA through experiments on various datasets, including synthetic and real-world data.
However, there are some weaknesses and potential criticisms:
* The paper does not provide a thorough comparison with other existing methods for analyzing differences between high-dimensional distributions, which would help to establish the novelty and significance of PDA and SPARDA.
* The authors do not provide a detailed analysis of the computational complexity of the algorithms, which is important for large-scale applications.
* The paper could benefit from more theoretical analysis of the properties of PDA and SPARDA, such as their consistency and robustness to noise and outliers.
Some potential suggestions for future work include:
* Investigating the application of PDA and SPARDA to other types of data, such as images and time series.
* Developing more efficient algorithms for computing the Wasserstein distance and its gradient, which would enable the application of PDA and SPARDA to larger datasets.
* Exploring the use of other divergences, such as the Kullback-Leibler divergence, in place of the Wasserstein distance.
Overall, the paper presents a significant contribution to the field of statistical analysis and machine learning, and the methodology has the potential to be widely applicable in various fields. However, further work is needed to fully establish the strengths and limitations of PDA and SPARDA.
Arguments for acceptance:
* The paper introduces a novel methodology for analyzing differences between high-dimensional distributions, which has the potential to be widely applicable.
* The experiments demonstrate the effectiveness of PDA and SPARDA in identifying differences between populations.
* The paper provides a clear and detailed explanation of the methodology, including the mathematical formulations and algorithms.
Arguments against acceptance:
* The paper does not provide a thorough comparison with other existing methods for analyzing differences between high-dimensional distributions.
* The authors do not provide a detailed analysis of the computational complexity of the algorithms.
* The paper could benefit from more theoretical analysis of the properties of PDA and SPARDA.