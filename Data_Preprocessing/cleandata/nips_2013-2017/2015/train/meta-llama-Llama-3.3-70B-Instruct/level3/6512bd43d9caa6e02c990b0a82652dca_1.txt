This paper explores the connection between algorithmic stability and generalization performance, building upon existing notions of stability and learnability. The authors establish a relationship between the two concepts, which is a valuable contribution to the field. However, the utility of the main result is questionable, as it may not provide significant insights into the convergence of training to test errors in cases where uniform convergence does not hold.
The paper's results can be recovered using existing results, such as those from [14], which establishes that learnability is equivalent to the existence of a stable asymptotic empirical risk minimizer. This raises concerns about the originality of the paper's contributions. Furthermore, the difference between training and test error is not a crucial concern, as learnability and test error are the primary interests.
A thorough discussion of the paper's contributions relative to previous results is needed, as well as a link between the paper's notion of stability and existing notions. The paper's discussion of dropout in Section 5.1 is vague and lacks formal results, and the results in Sections 5.2 and 5.3 are not properly instantiated.
The paper's quality is impacted by its failure to contextualize its contributions and motivate its goals, despite being well-written. The paper's originality is limited, as it primarily uses established techniques, although the notion of stability via total variation distance is novel. The paper's significance is uncertain, as its results may not excite the learning theory community, and its connections to existing results are unclear.
Arguments for acceptance:
* The paper establishes a connection between algorithmic stability and generalization performance, which is a valuable contribution to the field.
* The paper's results can be used to recover existing results, which demonstrates their validity.
* The paper's discussion of stability via total variation distance is novel and may lead to new insights.
Arguments against acceptance:
* The utility of the main result is questionable, as it may not provide significant insights into the convergence of training to test errors.
* The paper's results can be recovered using existing results, which raises concerns about the originality of the paper's contributions.
* The paper's discussion of dropout and other topics lacks formal results and proper instantiation.
* The paper's quality is impacted by its failure to contextualize its contributions and motivate its goals.