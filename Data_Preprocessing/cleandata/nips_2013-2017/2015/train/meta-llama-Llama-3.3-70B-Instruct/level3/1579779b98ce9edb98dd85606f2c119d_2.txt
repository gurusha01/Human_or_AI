This paper proposes a novel approach to training decision trees by jointly optimizing the split functions at all levels of the tree with the leaf parameters, based on a global objective. The authors establish a link between decision tree optimization and structured prediction with latent variables, and formulate a convex-concave upper bound on the tree's empirical loss. The paper is well-written, and the proposed method is significant and valuable, as it addresses the limitation of current greedy methods for decision tree induction.
The paper's originality lies in its investigation of joint training for decision trees, which is a challenging problem due to the discrete and sequential nature of the splitting decisions within the tree. The authors' formulation of the decision tree learning problem as a structured prediction problem with latent variables is innovative and allows for the development of an efficient optimization algorithm.
However, I have some concerns regarding the paper's technical soundness. Specifically, I question the validity of the upper bound in Equation 7, as it seems to be based on a flawed assumption. I provide a counterexample to illustrate the potential flaw. Furthermore, I am confused about why some leaves are not assigned any data points, which suggests a potential problem with the objective or initialization.
Additionally, I am not convinced about the importance of the efficient loss-augmented inference, as greedily checking all possible values may still be feasible. I also miss a baseline that trains non-axis aligned split functions of trees in a standard greedy manner for comparison.
The experimental evaluation is limited, and I suggest that additional experiments, such as comparing runtime, would strengthen the paper. Moreover, the depth of the tree needs to be specified ahead of time, which is a limitation that should be discussed more carefully.
Overall, the paper has both strengths and weaknesses. The proposed method is significant and valuable, but the technical soundness and experimental evaluation require further scrutiny. 
Arguments pro acceptance:
- The paper proposes a novel and significant approach to training decision trees.
- The authors establish a link between decision tree optimization and structured prediction with latent variables.
- The paper is well-written and easy to follow.
Arguments con acceptance:
- The technical soundness of the paper is questionable, particularly regarding the upper bound in Equation 7.
- The experimental evaluation is limited and requires additional experiments.
- The depth of the tree needs to be specified ahead of time, which is a limitation that should be discussed more carefully.