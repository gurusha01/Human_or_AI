This paper proposes a novel approach to batch learning from logged bandit feedback (BLBF) by introducing a self-normalized risk estimator to avoid the propensity overfitting problem. The authors provide a thorough analysis of the limitations of the conventional unbiased risk estimator and demonstrate the effectiveness of their proposed method, Norm-POEM, through extensive experiments.
The paper is well-organized, and the authors provide a clear explanation of the background and related work in BLBF. The introduction of the self-normalized risk estimator is well-motivated, and the theoretical analysis is sound. The experimental evaluation is thorough, and the results demonstrate the superiority of Norm-POEM over the conventional approach, POEM.
However, there are some concerns regarding the novelty and originality of the approach. The self-normalized risk estimator is not a new concept, and its application to BLBF is not entirely novel. The authors should acknowledge the similarity of their algorithm in section 3.2 to previously cited references, specifically a hybrid of two existing algorithms. Additionally, the "constrain-then-train" methodology is not new and was already suggested and tested in previous references.
The claim of novelty regarding the "expectation backpropagation" algorithm is also misleading, as it is essentially a backpropagation update rule that can be easily implemented with constrained connectivity. Furthermore, the cited previous state-of-the-art results are incorrect, and the previous best results for binary weights and neurons were actually achieved by a different algorithm.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, the authors should be more careful in evaluating both the strengths and weaknesses of their work. The clarity of the paper is good, and the organization is logical. The originality of the approach is questionable, but the significance of the results is notable, as the proposed method demonstrates improved generalization performance and robustness.
Overall, I would recommend accepting this paper, but the authors should address the concerns regarding novelty and originality, and provide a more accurate representation of the state-of-the-art results. The paper has the potential to make a significant contribution to the field of BLBF, and with some revisions, it can be a strong addition to the conference program.
Arguments pro acceptance:
* The paper proposes a novel approach to BLBF that demonstrates improved generalization performance and robustness.
* The theoretical analysis is sound, and the experimental evaluation is thorough.
* The paper is well-organized, and the authors provide a clear explanation of the background and related work.
Arguments con acceptance:
* The novelty and originality of the approach are questionable.
* The authors should acknowledge the similarity of their algorithm to previously cited references.
* The claim of novelty regarding the "expectation backpropagation" algorithm is misleading.
* The cited previous state-of-the-art results are incorrect.