This paper proposes a novel parallel optimization algorithm that demonstrates better tolerance to staleness, resulting in a reduced communication burden and improved learning outcomes. The algorithm's performance is compared to state-of-the-art methods, showcasing superior test error results. The paper is well-written, and the authors provide a clear motivation for their work, along with extensive experimental support.
The proposed algorithm, HONOR, incorporates second-order information to accelerate convergence while avoiding the need to solve regularized quadratic programming. The authors establish a rigorous convergence analysis, demonstrating that every limit point of the sequence generated by HONOR is a Clarke critical point. This is a significant contribution, as convergence analysis for non-convex problems is typically more challenging than for convex ones.
The paper also suggests a potential variant of the current EASGD method, which involves taking the exact average of all local variables in an online fashion. This is an interesting direction for future research, and the authors are encouraged to investigate and discuss this variant further.
One potential area for improvement is the effect of the parameter rho on the algorithm's performance. The authors are requested to conduct experimental studies to investigate the impact of rho on exploration and overall results. Additionally, the proposed parallel algorithm is designed to encourage simultaneous exploration among computing nodes for optimizing objectives with multiple local optima, which is a valuable contribution to the field.
The strengths of this paper include its clear motivation, extensive experimental support, and rigorous convergence analysis. The weaknesses are limited, but potential areas for improvement include investigating the effect of the parameter rho and exploring the potential variant of the EASGD method.
Arguments for acceptance:
* The paper proposes a novel parallel optimization algorithm with improved tolerance to staleness and reduced communication burden.
* The algorithm's performance is compared to state-of-the-art methods, showcasing superior test error results.
* The paper provides a clear motivation, extensive experimental support, and rigorous convergence analysis.
* The proposed algorithm has the potential to be applied to large-scale data sets and optimize objectives with multiple local optima.
Arguments against acceptance:
* The effect of the parameter rho on the algorithm's performance is not fully investigated.
* The potential variant of the EASGD method is not fully explored.
* The paper could benefit from additional experimental studies to further demonstrate the algorithm's effectiveness.
Overall, this paper makes a significant contribution to the field of parallel optimization, and its strengths outweigh its weaknesses. With some revisions to address the potential areas for improvement, this paper has the potential to be a valuable addition to the conference proceedings.