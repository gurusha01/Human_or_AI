This paper presents a novel approach to stochastic optimization for deep learning in parallel computing environments under communication constraints. The proposed Elastic Averaging SGD (EASGD) method and its variants demonstrate improved performance and stability compared to existing methods such as DOWNPOUR and ADMM. The authors provide a thorough analysis of the algorithm's stability and convergence properties, as well as extensive experimental results on benchmark datasets CIFAR-10 and ImageNet.
The paper's strengths include its clear and well-organized presentation, making it easy to follow and understand the proposed method and its underlying principles. The authors provide a detailed stability analysis of the asynchronous EASGD algorithm and compare it to ADMM, highlighting the advantages of their approach. The experimental results demonstrate the effectiveness of EASGD and its momentum-based variant, EAMSGD, in achieving faster convergence and better test performance.
However, there are some limitations and potential weaknesses. The paper assumes a univariate hyper-parameter and uses an L2 prior on the parameter, which may limit the usability of the results. Additionally, the experiments are focused on deep learning settings, which may not be directly applicable to other domains. The paper could benefit from more discussion on the trade-off between exploration and exploitation as a function of the learning rate and communication period.
Arguments pro acceptance:
* The paper presents a novel and effective approach to stochastic optimization for deep learning in parallel computing environments.
* The authors provide a thorough analysis of the algorithm's stability and convergence properties.
* The experimental results demonstrate the effectiveness of EASGD and EAMSGD in achieving faster convergence and better test performance.
Arguments con acceptance:
* The paper's assumptions and experimental setup may limit the applicability of the results to other domains.
* The trade-off between exploration and exploitation could be further discussed and analyzed.
* The paper could benefit from more comparisons to other existing methods and a more detailed discussion of the advantages and limitations of EASGD and EAMSGD.
Overall, the paper is well-written, and the proposed method shows promise in improving the performance and stability of stochastic optimization for deep learning in parallel computing environments. With some revisions to address the limitations and potential weaknesses, the paper could be even stronger. 
Quality: The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work.
Clarity: The paper is clearly written, well-organized, and adequately informs the reader. The presentation is easy to follow, and the authors provide sufficient information for the expert reader to reproduce the results.
Originality: The paper presents a novel combination of familiar techniques, and it is clear how this work differs from previous contributions. The authors adequately reference related work and provide a unique approach to stochastic optimization for deep learning.
Significance: The results are important, and other people (practitioners or researchers) are likely to use these ideas or build on them. The paper addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.