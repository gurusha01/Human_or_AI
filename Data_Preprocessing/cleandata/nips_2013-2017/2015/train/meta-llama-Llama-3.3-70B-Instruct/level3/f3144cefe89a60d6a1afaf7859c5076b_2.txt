This paper introduces the Principal Differences Analysis (PDA) framework, a novel approach for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. The authors also propose a sparse variant of the method, called SPARDA, which identifies features responsible for the differences between the distributions.
The paper is well-written, and the authors provide a clear explanation of the methodology and its applications. The use of the Wasserstein distance as a measure of divergence is well-justified, and the authors demonstrate its effectiveness in various experiments. The introduction of SPARDA as a sparse variant of PDA is also a significant contribution, as it allows for the identification of the most relevant features contributing to the differences between the distributions.
One of the strengths of the paper is its ability to handle high-dimensional data and identify complex relationships between features. The authors demonstrate the effectiveness of PDA and SPARDA in various experiments, including the analysis of single-cell RNA-seq data. The results show that PDA and SPARDA can identify subtle differences between cell populations and provide insights into the underlying biology.
However, there are some areas that could be improved. The authors could provide more background material on similar neural network implementations and compare their approach to other baseline models. Additionally, the empirical analysis could be strengthened by adding more comparisons to baselines, such as SVM or ridge regression, and typical neural networks with binary hidden units.
The use of a fixed budget to determine layer sizes is also questionable, and the authors could consider using a held-out subset of data to decide whether to grow or shrink the model. Furthermore, the authors could provide more evidence to support the claim that adding layers beyond the second layer improves performance, as the plots show high variance and similar topic descriptions.
Overall, the paper presents a significant contribution to the field of machine learning and data analysis. The PDA and SPARDA frameworks have the potential to be widely applicable in various domains, and the authors provide a solid foundation for future research and development.
Arguments pro acceptance:
* The paper introduces a novel approach for analyzing differences between high-dimensional distributions.
* The methodology is well-justified, and the authors provide a clear explanation of the approach.
* The use of the Wasserstein distance as a measure of divergence is effective in various experiments.
* The introduction of SPARDA as a sparse variant of PDA is a significant contribution.
* The paper demonstrates the effectiveness of PDA and SPARDA in various experiments, including the analysis of single-cell RNA-seq data.
Arguments con acceptance:
* The paper could benefit from more background material on similar neural network implementations.
* The empirical analysis could be strengthened by adding more comparisons to baselines.
* The use of a fixed budget to determine layer sizes is questionable.
* The authors could provide more evidence to support the claim that adding layers beyond the second layer improves performance.