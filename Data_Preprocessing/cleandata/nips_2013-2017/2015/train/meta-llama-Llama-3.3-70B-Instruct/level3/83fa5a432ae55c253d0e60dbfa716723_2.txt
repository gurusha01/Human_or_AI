This paper proposes a novel dimensionality reduction method called Principal Differences Analysis (PDA) that uses the squared Wasserstein distance as a discrepancy measure to find the most different direction between two high-dimensional distributions. The method is well-written and easy to follow, with a technically sound approach that shows promise in comparing favorably with existing methods.
The paper's strengths include its originality, with a new approach that combines the Wasserstein distance with semidefinite programming to find the optimal projection. The formulation is interesting and may have an impact on the machine learning community. The method is also extendable to nonlinear cases, and alternative methods like Transfer Component Analysis can be used to find the most different direction.
However, the paper also has some weaknesses. The problem addressed by PDA is similar to Transfer Component Analysis, which finds a subspace with small discrepancy between two datasets. Additionally, the method can be compared to a simple Lasso-based baseline, which may produce similar results if the input datasets are linearly related.
The paper's quality is good, with a clear and well-organized structure. The claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work.
In terms of originality, the paper proposes a new approach that combines the Wasserstein distance with semidefinite programming. The paper also provides a new perspective on the problem of dimensionality reduction, which is a significant contribution to the field.
The significance of the paper is high, as it addresses a difficult problem in a better way than previous research. The results are important, and other people are likely to use these ideas or build on them. The paper provides unique data, conclusions, and a theoretical or pragmatic approach that advances the state of the art in the field.
Overall, the arguments for acceptance include the paper's originality, technical soundness, and potential impact on the machine learning community. The arguments against acceptance include the similarity to existing methods and the potential for simple baselines to produce similar results. However, the paper's strengths outweigh its weaknesses, and it is a good scientific contribution to the field.
Arguments for acceptance:
* Originality: The paper proposes a new approach that combines the Wasserstein distance with semidefinite programming.
* Technical soundness: The method is well-written and easy to follow, with a clear and well-organized structure.
* Potential impact: The paper may have an impact on the machine learning community, and the results are important and likely to be used by others.
Arguments against acceptance:
* Similarity to existing methods: The problem addressed by PDA is similar to Transfer Component Analysis.
* Potential for simple baselines: The method can be compared to a simple Lasso-based baseline, which may produce similar results if the input datasets are linearly related.