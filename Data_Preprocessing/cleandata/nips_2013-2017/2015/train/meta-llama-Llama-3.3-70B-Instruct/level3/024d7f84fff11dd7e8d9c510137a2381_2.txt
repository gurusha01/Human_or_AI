This paper presents a significant contribution to the field of risk-sensitive reinforcement learning (RL) by extending the policy gradient method to the whole class of coherent risk measures. The authors provide a unified view of existing methods and propose new algorithms for risk-sensitive RL problems, demonstrating sufficient originality and technical soundness.
The paper is well-written, clear, and easy to follow, with all relevant references provided. The authors illustrate their algorithm with a toy example in the finance domain, demonstrating its risk-aversive behavior and flexibility in different risk types. The example highlights the importance of flexibility in designing risk criteria and selecting appropriate problem-specific risk measures for managing cost variability.
The paper's strengths include its ability to generalize and extend previous results, providing a unified approach to risk-sensitive RL. The authors also demonstrate the significance of their contribution by showing how their approach can be used to optimize risk-sensitive objectives in various applications.
The weaknesses of the paper are minimal, but one potential area for improvement is the convergence analysis of the actor-critic algorithm, which is reported in the supplementary material. Additionally, the authors mention that extending their approach to work with a single trajectory roll-out is an interesting direction for future research.
Overall, the paper meets the four criteria for a good scientific paper: quality, clarity, originality, and significance. The authors provide a technically sound and well-written paper that presents a significant contribution to the field of risk-sensitive RL.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of risk-sensitive RL.
* The authors provide a unified view of existing methods and propose new algorithms for risk-sensitive RL problems.
* The paper is well-written, clear, and easy to follow, with all relevant references provided.
* The authors demonstrate the significance of their contribution with a toy example in the finance domain.
Arguments con acceptance:
* The convergence analysis of the actor-critic algorithm is reported in the supplementary material, which may make it difficult for some readers to follow.
* The authors mention that extending their approach to work with a single trajectory roll-out is an interesting direction for future research, which may be seen as a limitation of the current work.
Overall, I recommend accepting this paper for publication, as it presents a significant contribution to the field of risk-sensitive RL and meets the four criteria for a good scientific paper.