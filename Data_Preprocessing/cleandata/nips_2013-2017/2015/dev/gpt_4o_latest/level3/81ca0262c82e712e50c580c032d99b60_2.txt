The paper presents a novel formulation of path integral (PI) control that employs an exponential value function, enabling analytical solutions and backward evaluation of path integrals. This forward-backward optimization approach leverages Gaussian Processes (GP) for forward modeling and backward integration for path costs and derivatives, distinguishing it from existing PI-related methods that rely on forward sampling. The proposed method is technically sound and demonstrates meaningful improvements in sample efficiency, optimality, and generalizability compared to state-of-the-art model-based methods like PILCO and PDDP.
Strengths:  
The paper's primary contribution lies in its novel forward-backward scheme for PI control, which eliminates the need for extensive sampling and provides analytic control laws without policy parameterization. This is a significant advancement over traditional PI methods that are computationally expensive and sample-inefficient. The use of GP-based modeling to incorporate uncertainty in dynamics and adapt control cost weights is another innovative aspect. The experimental results convincingly demonstrate the method's efficiency in learning tasks with fewer samples and its ability to generalize to new tasks using compositionality theory. Furthermore, the paper is well-organized and enjoyable to read, offering valuable insights into the intersection of probabilistic reinforcement learning and optimal control.
Weaknesses:  
Despite its strengths, the paper has some notable issues. First, the baseline evaluation raises concerns about fairness. The proposed method uses significantly fewer samples compared to iterative PI, but the comparison may not be entirely equitable due to differences in sample requirements. Second, the clarity of some key derivations is lacking, particularly in the transitions between equations (e.g., Equation 7 to 8) and the integral transformations on page 5. These steps are non-trivial and require more detailed explanations to ensure reproducibility. Additionally, while the use of GP is not new, the originality of the forward-backward approach could be better contextualized in relation to prior work. Lastly, the lack of equation numbering throughout the paper makes it difficult to reference specific equations during discussions.
Arguments for Acceptance:  
- The paper introduces a novel and impactful approach to PI control that is analytically grounded and computationally efficient.  
- It demonstrates significant improvements in sample efficiency and generalizability, addressing key limitations of existing methods.  
- The experimental results are robust and highlight the method's potential for real-world applications.
Arguments Against Acceptance:  
- Baseline comparisons may not be entirely fair due to differences in sampling strategies.  
- Some derivations and transitions are insufficiently explained, which could hinder reproducibility.  
- The lack of equation numbering and occasional challenges in readability detract from the clarity of the presentation.
Recommendation:  
Overall, the paper makes a strong contribution to the field of stochastic optimal control and probabilistic reinforcement learning. While there are concerns about clarity and baseline evaluation, these issues can likely be addressed in a revision. I recommend acceptance with minor revisions, emphasizing the need for clearer explanations of key derivations and improved presentation (e.g., equation numbering).