The paper introduces a novel approach to data-driven optimal control by extending the path integral (PI) control framework. The proposed method combines Gaussian Process (GP)-learned dynamics with a forward-backward optimization scheme, enabling analytic computation of control laws without requiring policy parameterization. A key contribution is the method's ability to generalize learned controllers to new tasks using compositionality theory, which is a significant departure from existing PI-related methods that rely heavily on forward sampling or parameterized policies. The paper demonstrates the method's sample efficiency and generalizability through experiments on three simulated robotics tasks, comparing it to state-of-the-art (SOTA) methods like PILCO and PDDP.
Strengths:
1. Novelty and Originality: The paper presents a unique combination of PI control theory, probabilistic model-based reinforcement learning, and linearly solvable optimal control. The forward-backward optimization scheme and compositionality-based task generalization are innovative contributions.
2. Significance: The method addresses key challenges in robotics control, such as sample efficiency and task generalization, making it a valuable addition to the field. Its ability to generalize controllers without re-sampling is particularly impactful for multi-task control.
3. Experimental Results: The method demonstrates superior sample efficiency and competitive performance compared to SOTA methods in simulated tasks. The compositionality-based generalization results are promising and highlight the method's practical utility.
4. Clarity in Motivation: The paper clearly identifies the limitations of existing PI-related methods and positions its contributions effectively.
Weaknesses:
1. Clarity Issues: While the paper is generally well-written, certain algorithmic explanations (e.g., derivation of the desirability function and Psi notation) are difficult to follow. The distinction between open-loop and feedback control is also unclear.
2. Sparse Experimental Details: The experiments lack sufficient detail, such as the specific hyperparameters used, the number of trials for convergence, and the computational resources required. Reporting average costs would improve objectivity.
3. Applicability Concerns: The method assumes partial knowledge of the control matrix \( G \), which may limit its applicability to systems with unknown or highly uncertain dynamics. The paper does not address uncertainty in \( G \), which is a significant limitation.
4. Minor Issues: There are several grammar, formatting, and notation inconsistencies. For example, the description of how \(\tilde{G}\) is obtained and actuator details are vague.
Recommendation:
The paper is a strong contribution to the field of optimal control and reinforcement learning, offering a novel and effective approach to sample-efficient control and task generalization. However, the clarity of derivations and experimental details needs improvement. Addressing these issues would significantly enhance the paper's impact and accessibility. I recommend acceptance with minor revisions, contingent on the authors addressing the clarity and experimental reporting concerns.
Arguments for Acceptance:
- Novel combination of techniques with strong theoretical underpinnings.
- Demonstrated improvements in sample efficiency and task generalization.
- Significant potential for real-world applications in robotics and control.
Arguments Against Acceptance:
- Lack of clarity in some derivations and algorithmic explanations.
- Sparse experimental details and limited discussion of computational costs.
- Assumes partial knowledge of the control matrix, which may limit general applicability.
In summary, the paper makes a meaningful scientific contribution but requires refinements in presentation and experimental rigor to fully realize its potential.