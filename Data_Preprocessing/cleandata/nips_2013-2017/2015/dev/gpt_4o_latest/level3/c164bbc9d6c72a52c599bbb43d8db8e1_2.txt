Review of the Paper
This paper introduces a generic acceleration scheme, termed "Catalyst," for first-order optimization methods, with a focus on improving convergence rates for a wide range of algorithms, including gradient descent, block coordinate descent, and incremental methods such as SAG, SAGA, SDCA, SVRG, and Finito/MISO. The proposed approach builds on a novel analysis of the accelerated proximal point algorithm and extends its applicability to both strongly convex and non-strongly convex objectives. Notably, the Catalyst framework provides theoretical speed-ups and practical benefits, particularly for ill-conditioned problems. The authors also address open questions in the field, such as accelerating incremental methods and supporting non-strongly convex objectives without requiring additional regularization.
Strengths:
1. Technical Contributions: The paper makes significant non-trivial contributions by generalizing Nesterov-style acceleration to a broader class of optimization methods. The theoretical analysis is rigorous, and the results demonstrate optimal or near-optimal convergence rates.
2. Clarity and Organization: The paper is well-written and well-structured, with a clear exposition of the problem, methodology, and results. The authors provide sufficient background and place their contributions in the context of prior work, citing relevant literature.
3. Experimental Validation: The experimental results are convincing and demonstrate the practical utility of the Catalyst framework. The authors evaluate their method on large-scale datasets and show substantial improvements, particularly for MISO and other incremental methods.
4. Novelty: The work addresses important gaps in the literature, such as accelerating incremental methods and removing the need for strong convexity assumptions in certain cases. The introduction of a proximal version of MISO and its acceleration is particularly noteworthy.
5. Impact: The proposed framework has broad applicability and potential to influence future research in optimization and machine learning.
Weaknesses:
1. Adaptive Restart Techniques: While the authors achieve significant acceleration, the paper does not explore whether the proposed method could benefit from adaptive restart techniques, as seen in Nesterov-type acceleration methods. This could be a valuable addition to improve practical performance further.
2. Complexity of Implementation: The Catalyst framework introduces additional hyperparameters (e.g., κ, εk) and requires careful tuning for optimal performance. While the authors provide guidelines, the practical implementation may still pose challenges for non-experts.
3. Limited Discussion on Stochastic Methods: The authors explicitly exclude stochastic gradient methods from the scope of their work. While this is understandable, a brief discussion on potential extensions to stochastic settings would enhance the paper's comprehensiveness.
Arguments for Acceptance:
- The paper provides a significant theoretical and practical advancement in the field of optimization.
- The contributions are well-supported by rigorous analysis and extensive experiments.
- The work is highly relevant to the NeurIPS community and has the potential for broad impact.
Arguments Against Acceptance:
- The absence of adaptive restart techniques and limited discussion on stochastic methods may leave some avenues unexplored.
- The complexity of implementation could hinder adoption by practitioners.
Overall Recommendation:
This paper is a strong candidate for acceptance. It introduces a novel and impactful framework for accelerating optimization methods, with both theoretical rigor and practical relevance. The minor weaknesses identified do not detract significantly from the overall quality of the work. I recommend the authors consider exploring adaptive restart techniques in future work to further enhance the Catalyst framework.