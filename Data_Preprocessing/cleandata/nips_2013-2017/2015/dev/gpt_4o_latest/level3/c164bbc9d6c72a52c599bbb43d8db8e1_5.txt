The paper presents a novel scheme for accelerating first-order optimization methods by introducing sharper proximal terms iteratively, achieving a convergence improvement from \( \frac{L}{\mu} \) to \( \sqrt{\frac{L}{\mu}} \). This approach is grounded in a new analysis of the accelerated proximal point algorithm and is applicable to a wide range of methods, including gradient descent, SAG, SAGA, SVRG, and their proximal variants. The authors also address non-strongly convex objectives, removing the need for pre-selecting regularization parameters. This work builds on foundational ideas in optimization, such as Nesterov's acceleration and proximal point algorithms, while extending their applicability and theoretical guarantees.
Strengths:
1. Theoretical Contribution: The proposed scheme is a significant theoretical advancement, offering a generic acceleration framework that applies to a broad class of optimization methods. The convergence analysis is rigorous, addressing both strongly convex and non-strongly convex cases.
2. Novelty: The idea of iteratively sharper proximal terms is innovative and has the potential to inspire future research in optimization and machine learning.
3. Generality: The method is applicable to various optimization algorithms, including incremental methods like SAG, SAGA, and MISO-Prox, which are widely used in machine learning.
4. Practical Implications: The acceleration scheme is particularly useful for ill-conditioned problems and large datasets, where computational efficiency is critical.
Weaknesses:
1. Experimental Validation: The experimental results are insufficiently comprehensive. While the paper demonstrates acceleration for specific methods like SAG, SAGA, and MISO-Prox, it lacks comparisons with state-of-the-art accelerated methods such as Acc-FG, particularly in terms of initial convergence speed.
2. Practical Relevance: The paper does not clearly articulate when the proposed method is practically advantageous, especially in machine learning contexts where data passes and memory constraints are critical.
3. Clarity Issues: Several aspects of the paper could be improved for clarity:
   - Theorem 3.3 would benefit from relocating key content (lines 256-257) into the theorem statement.
   - Parameter defaults (line 390) and tuning details for \( L \), \( \eta \), and learning rate scheduling are unclear.
   - Redundant wording (line 314) and formatting issues, such as Table 1's caption, detract from readability.
4. Cost Analysis: The authors should clarify whether all incremental methods, including ProxSVRG, incur \( O(n) \) costs, as this is crucial for evaluating scalability.
Suggestions for Improvement:
- Expand experimental validation to include comparisons with Acc-FG and other baseline methods, focusing on initial convergence and practical scenarios like non-strong convexity or large datasets.
- Provide clearer guidelines on parameter tuning and default values.
- Address formatting issues and improve the clarity of key theorems and statements.
- Contextualize the contributions better for scenarios where duality cannot be used or datasets are too large to fit in memory.
Recommendation:
Despite its shortcomings, the paper's theoretical contributions and potential to inspire future research make it a valuable addition to the field. I recommend acceptance, provided the authors address the clarity and experimental validation issues in the final version.