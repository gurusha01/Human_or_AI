This paper addresses the challenging problem of online F-measure optimization, presenting a novel algorithm, Online F-measure Optimizer (OFO), that is both theoretically grounded and empirically validated. The authors tackle the non-decomposability of the F-measure, a key metric in imbalanced binary classification tasks, and propose an efficient solution that eliminates the need for a validation set, making it suitable for one-pass learning scenarios.
Strengths:  
The paper is technically sound, with rigorous theoretical analysis and extensive experimental validation. The convergence of the proposed algorithm to the optimal F-score is proven under reasonable assumptions, and the supplemental material provides detailed proofs and additional experiments. The discussion in Section 6 is particularly insightful, highlighting the nuanced relationship between the F-measure as an aggregate metric and regret analysis in online learning. The authors also demonstrate the practical utility of OFO through experiments on benchmark datasets, showing competitive performance compared to the 2-stage F-measure maximization approach (2S). The originality of addressing F-measure optimization in an online setting, a relatively underexplored area, adds significant value to the field. The paper has the potential to inspire further research on optimizing non-decomposable metrics in online learning.
Weaknesses:  
While the paper is well-written and organized, a few areas require clarification. The variance of the estimator $\hat{h}(\tau)$ with respect to the data distribution and its implications for other estimated functions of $h(\tau)$ need more discussion. Additionally, the noisy estimation of $h(.)$ and the role of the decomposition step in mitigating this effect should be elaborated. Assumptions for the online learner (line 286) should be explicitly stated. Minor errors in equations (e.g., sign in Eq.(5), right-hand side of Eq.(1)) and typos (e.g., "positive a negative" â†’ "positive and negative") detract from the clarity. Specific terms and symbols, such as "P" above the arrow in line 155 and $Ft$-measurable in line 251-252, need clearer definitions. Notes in the pseudo-code (e.g., $at$ and $b_t$ calculation in line 7) would improve reproducibility. Lastly, "conference rate" in line 296 should likely be corrected to "convergence rate."
Pro and Con Arguments for Acceptance:  
Pro:  
1. The paper addresses an important and underexplored problem, contributing a novel algorithm with theoretical guarantees.  
2. Extensive experiments demonstrate the practical efficacy of the proposed method.  
3. The work has potential to inspire further research in optimizing non-decomposable metrics.  
Con:  
1. Some theoretical aspects, such as the variance of $\hat{h}(\tau)$ and noisy estimation, require further clarification.  
2. Minor errors and unclear notation slightly hinder readability and reproducibility.  
Recommendation:  
Overall, this paper makes a significant contribution to the field of online learning and F-measure optimization. While minor revisions are needed to address clarity and notation issues, the strengths of the paper outweigh its weaknesses. I recommend acceptance with minor revisions.