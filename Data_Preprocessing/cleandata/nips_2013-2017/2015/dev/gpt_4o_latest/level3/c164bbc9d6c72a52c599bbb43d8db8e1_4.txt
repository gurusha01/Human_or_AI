This paper presents a significant extension of Nesterov's acceleration technique to proximal point algorithms, incorporating inexact solutions of subproblems. The authors introduce a unified framework that accelerates a wide range of first-order optimization methods, including gradient descent, SAG, SAGA, SDCA, SVRG, and their proximal variants. By leveraging a novel analysis of the accelerated proximal point algorithm, the authors derive global convergence rates while allowing controlled inexactness in solving subproblems. The work is both theoretically rigorous and practically relevant, particularly for ill-conditioned problems, where the proposed methods demonstrate substantial improvements.
Strengths:
1. Originality and Contribution: The paper addresses an important gap in optimization by generalizing Nesterov's acceleration to inexact proximal point methods. The unified framework and its application to various first-order methods are novel and impactful.
2. Technical Depth: The authors provide a thorough theoretical analysis, including convergence guarantees for both strongly convex and non-strongly convex objectives. The use of estimate sequences and the adaptation of inexactness criteria are particularly noteworthy.
3. Clarity: The paper is well-written and organized, with clear explanations of the technical ideas. The inclusion of practical guidelines for parameter selection and implementation details enhances its accessibility.
4. Practical Relevance: The experimental results validate the theoretical claims, demonstrating significant speed-ups and stabilization for methods like MISO-Prox. The ability to handle ill-conditioned problems effectively is a strong practical contribution.
Weaknesses:
1. Rapid Decrease in Allowable Inexactness: As noted in Theorem 3.3, the rapid decrease in allowable inexactness for non-strongly convex objectives is a limitation. While this is somewhat expected, it may restrict the practical applicability of the method in certain scenarios.
2. Unverified Proofs: While the theoretical framework appears robust, not all proofs were verified by the reviewer. This leaves room for potential gaps or oversights in the analysis.
3. Limited Experimental Scope: Although the experiments are compelling, they focus primarily on â„“2-regularized logistic regression. It would be beneficial to evaluate the framework on a broader range of optimization problems to further demonstrate its generality.
Arguments for Acceptance:
- The paper makes a substantial theoretical contribution by extending Nesterov's acceleration to a broader class of optimization methods.
- The clarity and rigor of the analysis, combined with practical implementation details, make it a valuable resource for both researchers and practitioners.
- The experimental results provide strong empirical support for the proposed methods.
Arguments Against Acceptance:
- The rapid decrease in allowable inexactness for non-strongly convex objectives is a practical limitation.
- The experimental evaluation could be more diverse to better showcase the generality of the framework.
Recommendation:
I recommend acceptance of this paper. Its contributions to optimization theory and practice are significant, and the framework has the potential to influence future research in accelerated methods. While there are minor limitations, they do not detract from the overall quality and impact of the work.