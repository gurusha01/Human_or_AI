The paper addresses the challenging problem of optimizing the F-measure in an online learning setting, proposing a novel algorithm called Online F-measure Optimizer (OFO). The F-measure, a widely used metric for imbalanced binary classification tasks, is non-decomposable, making its optimization particularly difficult. The authors provide a formal consistency analysis of the proposed algorithm, demonstrating its convergence to the optimal F-measure under certain conditions. Experimental results on large real-world datasets further validate the theoretical findings, showing that OFO achieves performance on par with the widely used two-stage (2S) approach while eliminating the need for a validation set.
Strengths:
1. Novelty and Contribution: The paper presents a simple yet effective algorithm for online F-measure optimization, which is both easy to implement and computationally efficient. The formal consistency analysis is a significant theoretical contribution, as such analyses are rare in this domain.
2. Empirical Validation: The experimental results on multiple real-world datasets convincingly demonstrate the practical utility of the proposed algorithm. The convergence of the online F-measure to the optimal F-measure is particularly noteworthy.
3. Practical Relevance: The algorithm's ability to operate in a purely online setting without requiring a hold-out validation set makes it highly relevant for large-scale and streaming data applications.
4. Clarity of Theoretical Results: The authors provide detailed proofs and supplementary material to support their theoretical claims.
Weaknesses:
1. Comparison with Related Work: The comparison with the framework in [9] is insufficient. While the authors highlight differences in assumptions and objectives, they do not provide experimental comparisons or justify the absence of such comparisons. This omission weakens the paper's positioning relative to prior work.
2. Clarity of Theoretical Guarantees: The guarantees in Theorem 3 and Theorem 5 could be made clearer by explicitly stating the regret bounds. This would make the results more interpretable for readers unfamiliar with the nuances of stochastic approximation.
3. Paper Organization: The organization of the paper is suboptimal. Sections 2 and 3 contain redundant material that could be condensed into a single Background section. Additionally, key contributions like Algorithm 1 and its analysis appear too late in the paper, potentially confusing readers.
4. Synthetic Data Results: Useful results on synthetic data are relegated to the Appendix. Including these in the main paper would provide a more comprehensive view of the algorithm's behavior under controlled conditions.
Arguments for Acceptance:
- The paper addresses an important and challenging problem with a novel and practical solution.
- The theoretical and empirical contributions are significant and well-supported.
- The algorithm has clear advantages over existing methods, particularly in online learning scenarios.
Arguments Against Acceptance:
- Insufficient comparison with related work, particularly [9].
- Suboptimal organization and clarity in presenting key results.
Recommendation:
Based on the satisfactory author responses addressing some concerns and the overall quality of the work, I recommend acceptance. However, the authors should improve the clarity of theoretical guarantees, enhance comparisons with related work, and reorganize the paper for better readability.