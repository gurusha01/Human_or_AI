The paper introduces the Fixed-Length Poisson MRF (LPMRF), a novel distribution that generalizes the Multinomial distribution by enabling dependencies between dimensions, particularly positive dependencies. This is a significant contribution as it addresses the limitations of the Multinomial distribution, widely used in topic modeling, by relaxing the independence assumption between words. The LPMRF is applied as an emission distribution in a topic modeling framework, offering a direct relaxation of LDA's word-independence assumption. The authors also develop methods for estimating the log partition function and likelihood, which were previously intractable for related models like the Poisson MRF (PMRF).
Strengths:
1. Model Contribution: The LPMRF is a meaningful extension of the Poisson MRF, addressing its limitations by restricting the domain to fixed-length vectors. This makes the model both tractable and more suitable for real-world text data.
2. Positive Dependencies: The ability to model positive dependencies between words is intuitive and aligns well with linguistic phenomena, as demonstrated in the qualitative analysis.
3. Perplexity Experiments: The empirical results show that LPMRF outperforms Multinomial-based models in perplexity for small topic sizes, suggesting its potential as a replacement for the Multinomial in probabilistic models.
4. Scalability: The authors provide detailed timing experiments and demonstrate that their algorithms scale well, with practical implementations available online.
5. Generative Process: The generalization of topic models using fixed-length distributions is an interesting theoretical contribution, opening new avenues for research.
Weaknesses:
1. Scalability Concerns: While the authors claim good scalability, the memory and computational requirements for large vocabularies and topic sizes remain unclear. This could limit the model's applicability to larger datasets.
2. Inference Contribution: The paper relies on existing pseudo-likelihood optimization techniques rather than proposing novel inference methods. This limits its originality in the inference domain.
3. Admixture Model Details: The discussion of the Admixture of Poisson MRFs (APM) model is insufficient. A more thorough comparison with linguistic collocation literature and other dependency models would strengthen the paper.
4. Perplexity as a Metric: The reliance on perplexity as the primary evaluation metric is problematic, as it is not always indicative of topic quality. Additional metrics or qualitative evaluations would provide a more comprehensive assessment.
5. Incremental Advance: While the LPMRF is an interesting extension, the statistical advances in topic modeling are incremental, and the results, though promising, are modest compared to existing models like LDA.
Suggestions for Improvement:
- Explore alternative applications of LPMRF beyond topic modeling, such as event count modeling in political science, where dependencies are less understood.
- Clarify the optimization of Dirichlet parameters (\(\beta\) vs \(\alpha\)) to avoid confusion.
- Improve the clarity of figures, font sizes, and notations, and standardize terminology (e.g., CPMRF, dependency matrix sizes).
- Emphasize the LPMRF's role in the generative process with better formatting (e.g., bold font).
- Address the scalability concerns with a more detailed analysis of memory and computational requirements.
Recommendation:
While the paper offers an interesting extension of the Poisson MRF and demonstrates its potential in topic modeling, the contributions are incremental and limited in scope. The modest perplexity gains and reliance on existing inference techniques further temper its impact. I recommend acceptance only if the authors address the scalability concerns, provide a more thorough comparison with related work, and explore alternative evaluation metrics.