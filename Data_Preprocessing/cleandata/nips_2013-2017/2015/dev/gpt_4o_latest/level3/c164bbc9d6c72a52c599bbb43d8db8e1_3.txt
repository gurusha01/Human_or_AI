This paper presents a generic acceleration scheme for first-order optimization methods, leveraging Nesterov's extrapolation and the proximal point operator. The authors extend the accelerated proximal point algorithm to handle non-strongly convex objectives, a significant contribution compared to prior work such as [9]. The proposed method, termed "Catalyst," is applicable to a wide range of algorithms, including gradient descent, SAG, SAGA, SDCA, SVRG, and Finito/MISO, and demonstrates both theoretical speed-ups and practical improvements, particularly for ill-conditioned problems.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous analysis of the Catalyst scheme, demonstrating its ability to accelerate existing methods while supporting non-strongly convex objectives. This addresses a notable gap in the literature, as many existing methods, such as [8], rely on strong convexity assumptions.
2. Generality: The Catalyst framework is highly versatile, applicable to both deterministic and randomized first-order methods. This universality enhances its potential impact on the optimization community.
3. Practical Relevance: The experimental results validate the theoretical claims, showing significant speed-ups for methods like MISO, SAG, and SAGA on large-scale datasets. The stabilization of MISO for ill-conditioned problems is particularly noteworthy.
4. Comparison with Related Work: The paper provides a detailed comparison of results with [8], highlighting its additional contributions, such as the ability to handle non-strongly convex functions.
Weaknesses:
1. Comparison of Methods: While the results are compared with [8], the paper lacks a methodological comparison to clarify the differences in approach. It is unclear whether the method in [8] could also handle non-strongly convex functions with minor modifications.
2. Overlap with SDCA: The modified Finito/MISO algorithm closely resembles a variant of SDCA [24], raising concerns about novelty. The authors should clarify the distinctions and contributions beyond existing SDCA variants.
3. Experimental Gaps: The lack of experimental comparisons with SDCA is a significant omission, especially given the algorithm's similarity to SDCA. Including such comparisons would strengthen the empirical validation.
4. Clarity of Presentation: While the theoretical analysis is thorough, the exposition is dense and could benefit from clearer explanations, particularly for readers less familiar with proximal point methods or Nesterov's acceleration.
Pro and Con Arguments for Acceptance:
Pro:
- Advances the state of the art by extending acceleration techniques to non-strongly convex objectives.
- Provides a unified framework applicable to a wide range of optimization methods.
- Demonstrates significant practical improvements in experiments.
Con:
- Insufficient methodological comparison with [8] and SDCA.
- Limited novelty in the modified Finito/MISO algorithm.
- Dense presentation may hinder accessibility for a broader audience.
Recommendation:
Overall, the paper makes a strong theoretical and practical contribution to the field of optimization. However, addressing the methodological overlap with SDCA and including experimental comparisons with it are crucial to fully establish the novelty and impact of the work. I recommend acceptance with minor revisions, contingent on clarifying these points.