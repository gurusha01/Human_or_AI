The paper introduces a generic acceleration scheme, termed "Catalyst," for first-order optimization methods, building upon a new analysis of the accelerated proximal point algorithm. The authors claim that their approach provides acceleration for a wide range of optimization methods, including gradient descent, SAG, SAGA, SDCA, SVRG, and Finito/MISO, with explicit support for non-strongly convex objectives. The paper also addresses practical challenges, such as ill-conditioned problems, and demonstrates significant improvements in both theoretical convergence rates and empirical performance.
Strengths
1. Novelty and Generality: The Catalyst framework is a significant contribution, offering a universal acceleration mechanism applicable to a broad class of optimization algorithms. This generality addresses an important open question regarding the acceleration of incremental methods like SAG, SAGA, and SVRG.
2. Theoretical Rigor: The paper provides detailed convergence analyses for both strongly convex and non-strongly convex cases, leveraging tools such as Nesterov's estimate sequences. The results are well-supported by theoretical proofs, ensuring correctness and robustness.
3. Practical Relevance: The authors address practical issues, such as the choice of parameters (e.g., κ) and initialization strategies, making the framework applicable to real-world problems. The empirical results on large-scale datasets demonstrate the practical utility of Catalyst, particularly for ill-conditioned problems.
4. Extension of Existing Methods: The paper extends Finito/MISO to handle composite objectives and removes the restrictive "big data condition" (n ≥ βL/µ), broadening its applicability. This is a meaningful improvement over prior work.
5. Clarity of Contributions: The paper clearly delineates its contributions, including the acceleration of existing methods, the introduction of proximal MISO, and the stabilization of algorithms under challenging conditions.
Weaknesses
1. Empirical Evaluation: While the experiments are comprehensive, they focus primarily on ℓ2-regularized logistic regression. It would be beneficial to evaluate Catalyst on a more diverse set of optimization problems to demonstrate its generality.
2. Computational Overhead: The Catalyst framework introduces additional computational complexity due to the auxiliary problems and extrapolation steps. Although the authors discuss this overhead theoretically, a more detailed empirical analysis of runtime trade-offs would strengthen the paper.
3. Limited Discussion of Limitations: The paper does not explicitly discuss potential limitations of the Catalyst framework, such as its dependence on parameter tuning (e.g., κ, εk) or its performance in scenarios where strong convexity is unknown or weak.
4. Comparison with Adaptive Methods: While the authors acknowledge that SAGA adapts to unknown strong convexity parameters, a more detailed comparison with adaptive methods would provide additional insights into Catalyst's relative strengths and weaknesses.
Recommendation
The paper is a strong contribution to the field of optimization and machine learning, addressing both theoretical and practical challenges. Its novelty, rigor, and practical impact make it a valuable addition to the conference. However, addressing the weaknesses outlined above, particularly through broader empirical evaluations and discussions of limitations, would further enhance its impact.
Arguments for Acceptance
- Novel and general acceleration framework applicable to a wide range of methods.
- Rigorous theoretical analysis with well-supported claims.
- Practical relevance demonstrated through empirical results on large-scale datasets.
Arguments against Acceptance
- Limited diversity in empirical evaluations.
- Insufficient discussion of computational overhead and limitations.
Overall, I recommend acceptance with minor revisions to address the identified weaknesses.