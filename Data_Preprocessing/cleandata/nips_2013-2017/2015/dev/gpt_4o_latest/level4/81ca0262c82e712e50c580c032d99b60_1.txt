Review - Summary
This paper introduces a novel approach to path integral control. The proposed method capitalizes on the fact that the system control matrix \( G \) is often known, while the uncontrolled dynamics (or dynamics under a reference controller) can be learned using a Gaussian Process. Unlike previous path integral approaches, the reward function constraint here adopts a more general form, allowing noise and controls to operate in distinct subspaces. The authors also demonstrate how their framework facilitates generalization from known tasks to new ones and validate their method on three simulated robotics problems, where it performs favorably compared to state-of-the-art reinforcement learning and control methods.
Quality
Overall, the derivations appear to be correct and grounded in principle, though I have some reservations about the task generalization aspect (see below). The related work is adequately discussed, with the authors highlighting both similarities and differences compared to prior methods, and benchmarking against state-of-the-art techniques. The experimental results are compelling, though certain details are missing (see below).
The task generalization approach raises some concerns: Equation (13) suggests that the exponents of the reward functions are linearly combined to form the exponent of the new task's reward function. However, it is unclear how this leads to the exponent of the value function being derived as the same type of linear combination (Equation 14). This requires further clarification, and there may be an error here. For example, Equation (14) also states that \( \psi \) represents the exponent of the cost function, which is only valid for the final time step, as indicated in the equation near line 247. Additionally, the task generalization, as defined by the authors, appears to be limited to tasks defined by a target, though it seems feasible to extend this to other task variations if a suitable task similarity kernel is provided.
The experiments address relevant and realistic tasks, with comparisons to state-of-the-art methods. However, the experimental section lacks some technical details necessary for reproducibility, such as the specific dynamics and reward functions used and the length of sampled trajectories. These details could be included in the appendix. One baseline mentioned is iterative path integral control—if this is one of the methods in Table 1, it would be helpful to compare it on qualitative aspects as well. Additionally, Figures 1 and 2 should include error bars for better interpretability. The reported score is the exponent of the value function, \( \psi \). Is this \( \psi \) as computed by the robot? If the robot's calculation of \( \psi \) is overly optimistic, it could skew the results. A more objective metric, such as the average or cumulative (discounted) reward, would be preferable, as this aligns with the algorithm's optimization goal.
Clarity
The paper is generally well-written and effectively explains the proposed methods. The comparison to other methods, such as in Table 1, aids in understanding its relationship to the state of the art. However, there are a few grammatical and formatting issues (see Minor Points below). One confusing aspect is the Gaussian Process (GP) regression targets, which differ between Equation (2) (line 094) and line 208, where the reference controls are omitted. This discrepancy should be resolved or explained. Additionally, the authors should clarify how the GP hyperparameters are determined. The phrase "the posterior distribution can be obtained by constraining the joint distribution" is ambiguous—does this refer to conditioning?
Algorithm 1 is another source of confusion. It appears to optimize an open-loop control sequence, yet the rest of the paper discusses learning a feedback controller. If the algorithm indeed returns an open-loop sequence \( u1, \dots, uT \) in line 13, it is unclear how it accounts for noise. This point should be clarified or corrected.
Originality and Significance
While the main components of the proposed algorithm—such as propagation through a learned stochastic forward model (as in [15]) and path integral control—have been used in prior work, their combination here is novel. To the best of my knowledge, this formulation is original and achieves strong performance. By evaluating the method on realistic problems and comparing it against robust benchmarks, the authors demonstrate that their approach represents a significant improvement over prior work.
Comments on the Rebuttal
Thank you for clarifying Equation (14). However, it seems there is still a typographical error: \( \Psi \) should likely be \( \Phi \), or \( \Psi_{t+\Delta t} \) should be included in the middle term. Unfortunately, this remains unclear to me, so I have adjusted my confidence and score accordingly. I also appreciate the clarification regarding the open-loop control aspect, but I strongly recommend revising the notation to avoid confusion. Additionally, I am still uncertain about the interpretation of \( \psi \) in Figure 1. As I understand it, this represents the exponent of the final costs, but reporting the average cost would be more standard. Perhaps the average cost could be included in the supplementary material.
Minor Points
- While the paper is generally well-written, there are some grammatical issues. I recommend proofreading the following:
  - Line 040: "has been existed" → revise for grammatical correctness.
  - Line 143: "while" → consider replacing with "in contrast."
  - Line 231: Replace the period with a comma or rephrase the sentence.
  - Line 345: "is based on ... model" → "are based on ... models."
- Avoid creating empty sections (e.g., between Section 2 and Section 2.1).
- Address formatting issues, such as:
  - Margins in line 128.
  - Table 1 formatting.
  - Improperly scaled brackets in Equation (8).
- Clarify how \( \tilde{G} \) is derived from \( G \).
- Line 192: "table.1." → "Table 1."
- Notation inconsistency: In Equation (13), \( x_t \) is boldface on the right-hand side but italic on the left-hand side.
- Line 339: "Pilco requires an optimizer for policy evaluation" → Is "policy improvement" intended?
- Line 332: "6 actuators on each joint" → Is "one actuator per joint" intended?
In summary, the paper proposes a novel method for path integral control and evaluates it convincingly in experiments. However, there may be an issue with the derivation for the multi-task setting, and I look forward to the authors' response on this matter.