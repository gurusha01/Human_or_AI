The distribution of light reflected from a surface in a scene is determined by the geometry, scene illuminance, and the surface reflectance (true chromaticity). The color constancy problem involves estimating the surface reflectance from the observed scene, typically without access to either the geometry or the scene illuminance. This problem is significant because humans perform this task relatively well, and it is crucial for artificial vision systems to recognize and identify materials under varying lighting conditions.
The authors propose a method to estimate the scene illuminance of an image, which subsequently enables the recovery of the true chromaticity for all pixels. Their method assumes a single, global illuminance in the scene and employs a function L[x, y] to encode the belief that a pixel with an observed reflectance y has a true chromaticity x. The approach incorporates a prior on scene illuminances and assumes a single illuminance for the entire scene. Using a dataset of calibrated images, the authors can learn L either by treating each pixel independently or by directly minimizing the illuminance estimation error on the training set via gradient descent. Their results demonstrate that the proposed method generally outperforms existing color constancy techniques.
The manuscript is well-written, and the authors provide comparisons with several existing methods. The proposed approach is elegantly simple, based on reasonable assumptions, and performs effectively. Notably, the method does not rely on spatial structure within the scene yet surpasses alternative approaches that do.
The method shares some similarities with the Bayesian estimation approach described in reference 14 (the source of the image dataset). Although the authors compare their results against [14] in Table 1, it would be beneficial to include a discussion of [14] in the context of existing methods on page 2.
For training the end-to-end model, the dataset is augmented by "re-lighting" each image under different illuminants to generate six training images and a seventh validation image. However, it is unclear which hyperparameters were selected using this validation set. Additionally, it is critical to clarify whether the images used for testing accuracy (Table 1) are entirely distinct (not merely re-lit versions) from those used for training or validation. If this is not the case, it would represent a significant flaw, and this point should be briefly addressed in the manuscript.
A limitation of this work is that it primarily appeals to the machine vision community. From a machine learning perspective, the approach is relatively straightforward, and its main contribution is specific to the color constancy problem. The end-to-end method is thus of primary interest to the vision research community.