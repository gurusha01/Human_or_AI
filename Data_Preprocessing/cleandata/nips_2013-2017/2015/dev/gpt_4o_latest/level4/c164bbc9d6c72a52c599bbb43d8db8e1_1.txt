The paper presents certain novel contributions from a technical perspective. However, the primary idea and the theoretical results lack sufficient maturity. Specifically, Algorithm 1 can essentially be interpreted as a straightforward inexact variant of Güler's fast proximal method from [9], with the additional imposition of condition (5). This approach has notable limitations, particularly concerning the choice of \(\kappa\) and the resolution of the subproblem in (5).
The authors provide two convergence theorems addressing both the strongly convex and the non-strongly convex cases. Unfortunately, the selection of the inner accuracy \(\epsilon_k\) relies on the optimal value \(F^\), which is unknown. Furthermore, the method for solving (5) is not clearly articulated, and its convergence guarantee is expressed using the \(\tilde{O}\) notation. This notation does not accurately represent the actual number of iterations required for the inner loop, potentially leading to an overall complexity that is worse than that of existing methods. Additionally, the convergence guarantees in (7) and (11) depend on \(F(x^0) - F^\), which deviates from the guarantees provided by other established methods.
From my observations, the authors fix \(\kappa\) in Algorithm 1. However, this approach is suboptimal because, in accelerated schemes, the error from the inexact oracle accumulates. As the algorithm approaches the true solution, a more precise solution to the subproblem becomes necessary. Therefore, adaptively updating \(\kappa\) is critical.
Extending Algorithm 1 from the non-composite to the composite setting also presents challenges, particularly when the regularizer lacks a low-cost proximal operator. The inner loop requires such an operator at every iteration, which complicates the extension.
In summary, while the paper introduces some new technical contributions, these contributions are not sufficiently robust. Moreover, the convergence analysis lacks rigor. To address these shortcomings, the authors could consider leveraging the sliding gradient or conditional gradient methods proposed by G. Lan to better characterize the convergence of Algorithm 1. The paper introduces a so-called catalyst algorithm for unconstrained convex optimization, which can essentially be viewed as an inexact variant of Güler's fast proximal method from [9]. The authors analyze the convergence of this algorithm for both the strongly convex and non-strongly convex cases. However, several extensions and modifications are mentioned without rigorous justification. A few numerical experiments are provided to demonstrate the advantages of the proposed method.