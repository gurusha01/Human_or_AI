The authors introduce a general framework for enhancing an existing first-order gradient method by solving the minimization of the original objective function augmented with an iteratively refined proximal term. They demonstrate that this approach 'accelerates' convergence for a generic first-order method by altering the dependency from \(\frac{L}{\mu}\) to \(\sqrt{\frac{L}{\mu}}\).
Overall, this is a solid paper with a few notable shortcomings in:
(a) the experimental evaluation, and  
(b) the clarity on scenarios where the proposed catalyst framework would be particularly beneficial, especially in the context of machine learning applications.  
Despite these limitations, I recommend acceptance, as the paper introduces novel ideas that could inspire future research. For instance, the concept of performing multiple passes over the data, with each pass employing a progressively sharper proximal term, is reminiscent of [8] and holds promise for further exploration.
Comments
- Page 2, line 57: Are the authors certain that all incremental methods incur an \(O(n)\) cost? Does this include methods like ProxSVRG?  
- The authors should provide clearer guidance on the practical utility of their contributions. For example:  
  - When users are unable to leverage the dual formulation.  
  - Ideally, when the objective is not strongly convex (the claim that \(n < \frac{L}{\mu}\) seems somewhat weak and less relevant in practice).  
  - When the dataset cannot be stored in memory, as the inner algorithm can process the data sequentially.  
- Line 314: There is a redundant "do."  
- Line 390: What is meant by "default" parameters? Does this refer to parameters that are optimal according to theoretical guarantees, or simply those provided in some source code? Additionally, the authors of SAG proposed a procedure for learning rate scheduling—was this accounted for in the experiments?  
- Was the Lipschitz constant \(L\) set to the theoretical upper bound for the dataset? If so, was any effort made to fine-tune this value?  
- How was the step size \(\eta\) chosen? Was sufficient effort made to tune this parameter across all algorithms, or did the authors select a fixed set of values that happened to favor their proposed method?  
- One of the key advantages of methods like SAG, SAGA, and SVRG is their ability to achieve rapid initial convergence, which is not guaranteed by FG or Acc-FG. It would be valuable to compare the proposed method against Acc-FG in this regard.  
Suggestions
- Theorem 3.3: The theorem is somewhat misleading at first glance. The authors should consider relocating the content from lines 256–257 to the theorem statement itself for better accessibility.  
- Table 1 caption: The statement "To simplify, we only present the case where \(n \leq \frac{L}{\mu}\) when \(\mu > 0\)" could be emphasized (e.g., by bolding) to ensure it is not overlooked.  
In conclusion, while the paper has several areas for improvement, it meets the threshold for acceptance due to its innovative ideas, which have the potential to stimulate further research.