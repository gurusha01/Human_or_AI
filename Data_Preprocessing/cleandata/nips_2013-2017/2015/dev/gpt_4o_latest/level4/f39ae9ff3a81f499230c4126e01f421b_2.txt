To address the challenge of modeling positive correlations in discrete data, which cannot be captured by standard Multinomial distributions, this paper introduces Poisson Markov Random Fields (MRFs) as a method to directly model word correlations in text data. Specifically, since the total number of words in a document is known in textual analysis, the authors demonstrate that the normalizing constant of the Poisson MRF can be efficiently estimated. This efficiency allows the proposed model to be integrated with standard topic models, such as Latent Dirichlet Allocation (LDA).
While the proposed approach is conceptually straightforward and computationally efficient, I have concerns regarding its scalability to real-world text datasets. The experiments in the paper are conducted on a very small dataset comprising only about a thousand documents, resulting in a lexicon of limited size. However, real-world text corpora typically involve lexicons with tens of thousands of words. This poses a significant scalability issue, as the number of parameters in the Poisson MRF grows quadratically, i.e., O(L^2), where L is the size of the lexicon. Furthermore, the assumption about document length distributions is problematic. The authors claim that document lengths follow a Poisson or Normal distribution, but in practice, document lengths are better modeled by distributions such as Gamma. When the Poisson parameter becomes large, its distribution becomes narrow, which does not align well with the observed variability in real-world document lengths.
To strengthen their argument for the utility of this model, the authors should conduct experiments not only on small text datasets but also on other types of discrete data with lower dimensionality. I believe this model may be more suitable for domains outside of natural language processing. Additionally, for capturing both positive and negative word correlations, indirect methods leveraging word embeddings may be more appropriate. Such approaches have been explored in recent work, such as [1] at ACL 2015. Comparisons with these more advanced baselines would make the direct Poisson MRF approach more compelling.
[1] "Gaussian LDA for Topic Model with Word Embeddings," Rajarshi Das, Manzil Zaheer, and Chris Dyer, ACL 2015. Poisson MRF for directly modeling word correlations, with efficient inference in the normalizing constant. Not scalable for real texts beyond the small-scale experiments presented in this paper.