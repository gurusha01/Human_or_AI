Summary: This paper explores the application of tools from the design and analysis of prediction markets to address the challenge of learning an effective hypothesis when training data is distributed across multiple parties.
Additionally, the authors introduce modifications to ensure differential privacy.
Specifically, the approach involves maintaining a current hypothesis, enabling participants to iteratively update this hypothesis (with several "betting languages" considered for these updates), and employing a family of convex cost functions (parameterized by a domain element x) to determine the rewards or penalties for participants after the final hypothesis is evaluated on a test data point.
(The choice of these cost functions is guided by the underlying loss function.)
To incorporate differential privacy, the authors adapt advanced techniques from "continual observation" models, which aim to ensure privacy at each time step without incurring a linear cost in the number of steps.
Quality: The integration of the various models and techniques is executed competently.
However, the paper appears somewhat lacking in terms of motivation.
The results come across as having a "this is what we can do" perspective, rather than following the more conventional structure of "this is a well-motivated problem," followed by "this is our solution and why it improves upon prior or obvious approaches."
Clarity: The writing is reasonably clear and of acceptable quality.
Originality: While none of the individual tools employed are novel, some of the combinations presented in this work appear to be original.
Significance: The results are of moderate significance. The authors explore the adaptation of tools from prediction markets to distributed hypothesis learning and propose modifications to ensure differential privacy.