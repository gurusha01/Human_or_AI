The prediction of a structured output that differs from the input data (e.g., predicting segmentation labels) pushes the boundaries of what is typically understood as an auto-encoder. On the other hand, predicting the missing portion of an image aligns more closely with the conventional conceptual framework of an autoencoding approach.
The term 'conditional prior' seems to diverge somewhat from what many in the NIPS community might regard as standard and appropriate terminology.
Why not refer to P(z|x) as the conditional distribution of the latent variable, the stochastic representation z, or another suitable term? Redefining the notion of a prior as a 'conditional prior' risks clashing with well-established definitions of what constitutes a 'prior.' While I appreciate the motivations behind this choice, I believe the terminology in the paper should be more rigorously aligned with established conventions for clarity and precision.
Additional details about the baseline CNN would be helpful, as understanding whether it represents a strong or weak baseline is important. Furthermore, there are several minor language issues throughout the manuscript that should be addressed. This paper investigates a natural extension of stochastic conditional models, framed here as a conditional variational auto-encoder. The core idea is sound, and the experimental results are comprehensive. However, some of the terminology employed in the paper stretches the conventional interpretations of certain concepts, which warrants closer attention (see detailed comments for specifics).