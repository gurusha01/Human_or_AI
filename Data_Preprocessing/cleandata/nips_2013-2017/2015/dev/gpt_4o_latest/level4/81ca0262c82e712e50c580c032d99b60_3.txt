The manuscript introduces an innovative PI-related approach for stochastic optimal control. The core concept involves leveraging a Gaussian process (GP) to model the trajectories generated by applying a proposal control $u^{\text{old}}$ to the uncontrolled dynamics. The Bellman equation is reformulated to optimize $\delta u$, representing the control update at each iteration. The method capitalizes on analytical techniques for GP inference to efficiently update GP parameters and is validated experimentally across three simulated tasks.
The proposed idea is intriguing, and the authors appear to have developed a functional method that holds potential to compete with existing approaches. Overall, I find the paper promising, but the following issues require attention:
The problem formulation section is somewhat unclear due to the presence of two optimization processes: one for determining the optimal control $u$ and another for optimizing $\delta u$. In Section 2.1, it seems to imply that the optimal $u$ is provided alongside a proposal control $u^{\text{old}}$, which is misleading since identifying the optimal $u$ is the central objective. The authors should clarify this ambiguity.
The paper's organization could be improved significantly. Since the proposed method is introduced prior to Section 3.1, I recommend moving subsection 3.1 to follow subsection 2.1. This restructuring would enhance readability, as Equation (9) relies on the inference step described in subsection 3.1.
Section 3.1 is also somewhat confusing. The GP is defined (line 208) for the uncontrolled process ($u=0$), which seems inconsistent with Equation (2), where the GP depends on the current proposal control. Furthermore, if the GP is initialized using sampled data from the uncontrolled process, the authors should explain how the poor conditioning of the uncontrolled process, in terms of effective sample size, impacts this initialization.
Regarding the GP model update, the formulation appears to disregard the cost of the current control sequence (see Equation (5)). In iterative PI control, this term is included as a Radon-Nikodim derivative [16,17]. Can the authors identify the corresponding term in their formulation? If not, does this method still qualify as a generalization of PI control, or should it be categorized as a policy search, PI-related approach?
The use of belief propagation for forward inference may encounter challenges when the dynamics around the reference trajectory change abruptly, such as in non-Gaussian scenarios (e.g., in the presence of obstacles). Since such cases are not addressed in the experiments, the authors should clarify whether this limitation is a potential concern.
In terms of notation, while the authors describe the method as model-based, it is unclear whether a dynamics model is explicitly used (as the GP provides only a probability distribution over trajectories). I suggest revising the notation for greater clarity.
The authors have done a commendable job relating their approach to existing PI-related methods, which is challenging given the extensive literature on PI control. However, I have the following observations in this regard:
- In PI^2 [8], the constraint is not ignored; rather, PI^2 is an open-loop policy search method, rendering the constraint irrelevant. The same reasoning applies to PI^2-CMA.  
- Feedback PI [17] incorporates a parameterized feedback term (the second-order correction) but does not employ a parameterized policy.  
- PI-REPS [11] is model-based, although the model can be learned, as in guided policy search [23].  
- Iterative PI [16] represents the original PI method with importance sampling, making it model-based, subject to the same structural constraints as PI, and devoid of a parameterized policy.
These observations pertain not only to Table 1 but also to text distributed throughout the paper.
Minor comments:  
- Line 120: "minimize(s)"  
- Lines 144-145: The sentence "while (...)" is unclear.  
- Line 146: "(to) act"  
- Line 229: "based on (a) probabilistic"
In summary, the paper introduces a novel PI-related method for stochastic optimal control that employs a Gaussian process to model trajectories. While the approach is promising, it is unclear whether it constitutes a generalization of PI control or a policy search, PI-related method. The presentation requires improvement, and several points need clarification.