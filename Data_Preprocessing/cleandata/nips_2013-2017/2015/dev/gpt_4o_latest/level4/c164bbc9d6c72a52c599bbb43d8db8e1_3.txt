As stated in the paper, the algorithm presented is essentially the same as that in [9], integrating the extrapolation concept from Nesterov's acceleration framework with the proximal point operator. The algorithm in [9] assumes exact minimization at each iteration, which is impractical in real-world scenarios. The current paper extends the analysis by adopting more realistic assumptions involving approximate minimization. In this regard, paper [8] exhibits significant similarities. While the authors claim that their work is independent of [8], given that [8] has already been published, I believe the current paper should be evaluated primarily based on its contributions beyond the overlapping results with [8]. Although the appendix provides a detailed comparison of the results between this paper and [8], I would have appreciated a comparison of the methods as well. 
One of the key contributions of the current paper is its ability to handle functions that are not strongly convex. However, based on my understanding—though I did not examine the proofs in detail—it appears that the proof techniques for the strongly convex and general convex cases are quite similar. This raises an important question: Can the method in [8] also be extended to handle the case where strong convexity is not required, or does this paper introduce a genuinely novel approach to address this scenario?
Another notable contribution is the acceleration and other enhancements of Finito/MISO. The modified algorithm proposed here bears a strong resemblance to a variant of SDCA [24], albeit with some differences. Given this similarity, the experimental results should have included a comparison with SDCA, and I view this omission as a significant shortcoming. Overall, the paper provides an analysis of a generic acceleration framework for first-order optimization, leveraging Nesterov's extrapolation technique and the proximal point operator.