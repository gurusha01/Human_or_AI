Summary
This paper investigates adaptive stochastic optimization along paths by introducing constraints (specifically, the MLRB and MLB conditions) that define the relationship between the probability of a realization and its utility. While the concept of adaptive submodularity implies that, on average, no item will "surprise" us given a longer observation history, the MLRB and MLB conditions introduced in this work roughly capture the idea that such "surprises" occur only with low probability (i.e., realizations with low probability yield high utility). Utilizing these constraints, the authors present a novel recursive algorithm that achieves a near-optimal guarantee on the expected cost required to achieve a specified coverage. Under the MLB conditions, the paper extends the classical approximation results for Bayesian pool-based active learning, up to a constant factor.
Quality
The paper is well written and logically organized. Regarding the experimental results: based on Theorem 4, it would be highly valuable to see how the proposed RAC algorithm performs in comparison to existing algorithms empirically for the pool-based active learning problem (e.g., [3][5][7], or [6] in the bounded noise setting). This is particularly relevant given that the version space reduction function and the Gibbs error function both satisfy the MLRB condition, the pointwise submodularity condition, and the adaptive submodularity condition. Including such empirical comparisons would significantly strengthen the paper.
Clarity
The majority of the paper is clear. However, it would be beneficial for the authors to provide more detailed intuitive explanations of the two conditions. Specifically, why are these two conditions more natural or appropriate than the adaptive submodularity condition for adaptive optimization problems along paths?
Originality and Significance
To the best of my knowledge, the technical contributions of this work are novel. The paper offers valuable insights into the development of efficient algorithms for adaptive stochastic optimization problems. It proposes an innovative solution for adaptive stochastic optimization along paths (with sets as a special case). The authors identify a class of problems that can be approximated near-optimally using their approach and provide compelling empirical evidence to support their claims. Overall, the presentation is clear and accessible.