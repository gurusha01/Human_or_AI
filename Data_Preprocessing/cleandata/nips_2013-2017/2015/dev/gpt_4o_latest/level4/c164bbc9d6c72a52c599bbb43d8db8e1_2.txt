This work represents a highly interesting and significant contribution, offering several relevant and non-trivial advancements. The paper not only proposes a generic framework to accelerate various optimization methods (specifically several recent incremental approaches) but also broadens the scope of applicability for some of these methods. The manuscript is exceptionally well-written, with its contributions thoughtfully contextualized within the existing state of the art. While the experimental results are naturally constrained by space limitations, they are nonetheless clear and compelling. To the best of this reviewer's knowledge, the findings presented are original.
Recent studies have demonstrated that the convergence rates of Nesterov-type acceleration methods can be substantially enhanced through adaptive restart techniques. For instance, refer to the papers "Adaptive Restart for Accelerated Gradient Schemes" by O'Donoghue and Candes, and "Monotonicity and Restart in Fast Gradient Methods" by Giselsson and Boyd. It would be worthwhile to explore whether the method introduced in this manuscript could similarly benefit from incorporating a restarting scheme, given its reliance on Nesterov-type extrapolation.
Overall, this is a highly interesting and impactful piece of research, offering several significant and non-trivial contributions (as detailed above).