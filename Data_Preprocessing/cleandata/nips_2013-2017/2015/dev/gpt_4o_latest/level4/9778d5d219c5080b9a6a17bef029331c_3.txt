The proposed algorithm is both simple and intriguing. It involves histogramming pixel luminance, chrominance, and illumination chrominance, followed by an evaluation where each pixel's luminance votes on its true chrominance for each of the "memorized" illuminations. The model can either be trained generatively by counting pixels in the training set or trained end-to-end for a modest performance improvement. The algorithm's simplicity and computational efficiency are particularly appealing, and it also shows potential as a foundational component for a more advanced spatially-varying illumination model.
The paper is reasonably easy to follow, though the mathematical presentation could benefit from revision to improve clarity and reduce the unusually high number of symbols introduced for what is ultimately a fairly straightforward algorithm. For instance, the necessity of defining x-hat and g() is unclear. Similarly, is v(n) equivalent to vn? The notation { g(v(n), m-hati}_i seems unnecessarily complex—perhaps even incorrect—given that i is used as a subscript twice. Including figures that directly correspond to the mathematical expressions could greatly aid comprehension, as currently, the only variable depicted in a figure is L[].
The evaluation demonstrates that the proposed method achieves state-of-the-art performance on the standard color checker dataset. However, there is one aspect of the evaluation that raises concerns. When comparing the authors' results to those in [19], the authors slightly outperform [19]. However, the paper introduces a modified set of error metrics, which the supplement explains are derived from the metrics in [7,19] but adjusted to account for what the authors claim is an unfair advantage in [7,19] due to the use of incorrectly black-leveled images. While it may be true that [7,19] were evaluated on improperly processed data, addressing this issue in the manner presented here is problematic. 
As I understand it, the authors' correction assumes that the models in [7,19] were trained on the incorrect dataset and then evaluated on the corrected dataset, which would naturally inflate the reported error metrics. However, this does not accurately reflect how [7,19] would perform if both training and testing were conducted on the corrected dataset. In fact, given that improper black-level correction can severely degrade the performance of a color constancy algorithm, it is plausible that [7,19] might perform better if trained and tested on the corrected data. Thus, the correction presented by the authors is not valid.
While it is unfortunate that [7,19] contain potentially flawed results that may be cited, introducing another set of speculative and unreliable metrics to counteract them is not the appropriate solution. If published, the corrected numbers presented here may be misinterpreted as definitive by future researchers, compounding the issue. Instead, I recommend that the authors present the original error metrics from [7,19] but clearly mark them (e.g., with an asterisk or grayed-out font) as potentially unreliable, with an explanation provided in the supplement. This approach places the responsibility on [7,19] to revise their results while avoiding the dissemination of conflicting and equally questionable metrics. 
This suggestion is not intended to undermine the authors' work; even if the results from [7,19] are accepted as-is, the proposed method still outperforms them. Rather, this is a matter of scientific rigor and etiquette, which should be handled with the utmost care. Overall, the authors present an interesting and effective color constancy algorithm that achieves state-of-the-art results on the standard dataset. While the paper could benefit from improved clarity and the evaluation has some issues, the work appears sound and worthy of consideration.