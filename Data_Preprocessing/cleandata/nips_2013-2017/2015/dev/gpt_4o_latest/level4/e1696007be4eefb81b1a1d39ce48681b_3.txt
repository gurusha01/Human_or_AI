This paper investigates sample complexity and error bounds for Dantzig-type estimators within the framework of structured regression. A key component of these bounds involves geometric properties such as the Gaussian width of the norm ball and other related quantities. For the class of atomic norms that are invariant under sign changes, the authors derive novel bounds for these geometric properties, which are computationally tractable in practice. Additionally, the tightness of these bounds is examined by demonstrating that the derived upper bounds are close to their corresponding lower bounds.
The primary focus of this work addresses a topic that is undoubtedly relevant and engaging for the Machine Learning community. The paper is written in a clear and accessible manner. In my view, the bounds presented hold both theoretical significance and practical utility. That said, I must acknowledge that I did not fully grasp certain technical aspects, such as the proof of Theorem 7.
Overall, this is an interesting theoretical contribution on a highly pertinent topic. Well written.