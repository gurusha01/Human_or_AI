The authors present a novel strategy to incentivize agents to share private and relevant data. This approach leverages intriguing concepts from the machine learning domain, particularly the use of the stochastic gradient descent algorithm to determine payments for data. The proposed mechanism effectively compensates agents while adhering to a constrained budget. Additionally, the authors incorporate privacy-preserving mechanisms and explore various "profit-maximizing" strategies that agents might adopt based on their confidence in their data. The core contribution appears to lie in the development of a mechanism that compensates agents for their data while ensuring a bounded budget.
The practicality of this family of algorithms, however, depends heavily on the availability of timely and accurate information. If "true samples" are only accessible far into the future, then section 2.3 becomes infeasible (see below), and section 2.1 offers little advantage over traditional betting markets, aside from its generalizability to more complex prediction functions.
Section 3 does not seem to address a fundamentally critical issue. Depending on the specific privacy concerns, simpler solutions could suffice to prevent agent \( i \) from learning about agent \( j \). For example: (a) anonymizing participation or (b) anonymizing the relationship between updates and agents (i.e., ensuring that which agent made which trade is not disclosed to others).
The most valuable contribution of this work is the generalization of prediction markets to incorporate sophisticated prediction tools, enabling these markets to extend beyond simple bets on binary or scalar variables. While this is an exciting idea, its practical utility appears limited. Participation in such markets would likely be low unless they operated alongside a steady stream of true samples, allowing for timely evaluation of rewards.
(If agents are unaware of what their data is being used to predict, how much credit it might earn, or when rewards will be distributed, what incentive do they have to participate?)
Section 2.3, which proposes purchasing data, seems contingent on knowing the loss function of predictionsâ€”implying that the correct answers are already known. If this is the case, what is the purpose of bidding on data? Conversely, if the correct answers are unknown, how can the marginal utility of a given data point be determined? This presents a fundamental catch-22.
There are potential workarounds, but they appear vulnerable to exploitation:  
- Using past true samples to assess the value of new training data. However, this could be gamed by submitting large amounts of data that overfit known past samples, thereby generating fake data to maximize rewards.  
- Employing a prediction divergence metric to evaluate the utility of new data (e.g., rewarding data that significantly alters predictions). This, too, is susceptible to manipulation, as agents could generate extreme data to cause predictions to deviate wildly.
Ultimately, it seems that the only viable way to define a cost function is based on future true samples, which poses a significant limitation if those samples are not immediately available. This constraint fundamentally undermines the feasibility of the "selling" scheme described in section 2.3.
The authors propose a family of market algorithms that extend prediction markets by rewarding incremental improvements in prediction accuracy from contributed data. While this is an innovative and exciting concept, its practical applicability appears constrained. The market's reliance on a known loss function means that, without knowledge of the prediction target, it becomes impossible to design accurate incentives.