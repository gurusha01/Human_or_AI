The paper presents a novel generic acceleration scheme, termed "Catalyst," for first-order optimization methods, building upon an analysis of the accelerated proximal point algorithm. The authors propose a framework that accelerates a wide range of optimization algorithms, including gradient descent, SAG, SAGA, SDCA, SVRG, and their proximal variants. The key contributions include providing acceleration for both strongly convex and non-strongly convex objectives, removing the need for pre-selecting regularization parameters, and extending methods like MISO to handle composite objectives. The theoretical results are complemented by empirical evaluations on large-scale datasets, demonstrating significant speed-ups, particularly for ill-conditioned problems.
Strengths
1. Theoretical Contributions: The paper provides rigorous theoretical guarantees, including convergence rates for both strongly convex and non-strongly convex objectives. The use of the Catalyst framework to generalize Nesterov's acceleration is a significant advancement.
2. Generality: The proposed scheme is broadly applicable to a wide range of first-order methods, addressing an important gap in the literature by extending acceleration to methods like SAG, SAGA, and MISO.
3. Practical Relevance: The empirical results demonstrate the practical utility of the Catalyst framework, particularly in stabilizing and accelerating MISO for ill-conditioned problems. The experiments are conducted on large-scale datasets, showcasing real-world applicability.
4. Clarity of Presentation: The paper is well-organized, with clear definitions, theoretical results, and a detailed explanation of the Catalyst algorithm. The inclusion of pseudocode and theoretical analysis makes the methodology accessible.
Weaknesses
1. Empirical Evaluation: While the experiments are comprehensive, the comparison with other state-of-the-art accelerated methods, such as Nesterov's accelerated gradient descent, could have been more detailed. Additionally, the results for SAGA show limited improvement, and the reasons for this are not fully explored.
2. Complexity of Implementation: The Catalyst framework introduces additional parameters (e.g., κ, εk) and requires careful tuning, which may limit its adoption in practice. The paper could benefit from more discussion on how to automate parameter selection.
3. Scope of Acceleration: The authors explicitly exclude stochastic gradient methods from the scope of the Catalyst framework. While this is a reasonable limitation, it would be valuable to discuss potential extensions or the challenges involved in accelerating stochastic methods.
Arguments for Acceptance
- The paper addresses a significant and open problem in optimization by providing a universal acceleration scheme for first-order methods.
- The theoretical contributions are robust and advance the state of the art, particularly in handling non-strongly convex objectives.
- The empirical results demonstrate the practical impact of the proposed framework, particularly for challenging optimization problems.
Arguments Against Acceptance
- The limited empirical improvement for SAGA and the lack of detailed analysis for this case raise questions about the generality of the practical benefits.
- The complexity of implementing the Catalyst framework may hinder its adoption, and the paper does not fully address this concern.
Recommendation
Overall, this paper makes a strong theoretical and practical contribution to the field of optimization. While there are some limitations in empirical evaluation and implementation complexity, these do not outweigh the significance of the contributions. I recommend acceptance with minor revisions to address the empirical results for SAGA and provide more guidance on parameter selection.