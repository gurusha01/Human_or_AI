This paper proposes a Poisson Markov random field (MRF) to model positive correlations in discrete data, particularly for text analysis, where ordinary Multinomial distributions are insufficient. The authors leverage the known number of words in a document to efficiently estimate the normalizing constant of the Poisson MRF, enabling its combination with standard topic models like Latent Dirichlet Allocation (LDA). Although the assumption is straightforward and the associated inference is efficient, a major concern is the model's scalability to typical texts. The experiments conducted on a small dataset with a limited lexicon of around a thousand documents raise questions about the model's applicability to real-world texts, which often feature lexicons with tens of thousands of words. The number of parameters in the Poisson MRF scales quadratically with the lexicon size (O(L^2)), making it impractical for large-scale texts. Furthermore, the distribution of document lengths in actual texts often follows a Gamma distribution, whereas the authors assume a Poisson or Normal distribution, which becomes narrow and unrealistic for large parameters. To demonstrate the model's usefulness, experiments should be performed on larger, more diverse datasets, including other types of discrete data with lower dimensionalities, where the model might be more suitable. For natural language processing, indirect approaches using word embeddings, such as those explored in "Gaussian LDA for Topic Model with Word Embeddings" (Rajarshi Das et al., ACL 2015), may be more effective for modeling word correlations. A comparison with these baselines would be necessary to make the direct Poisson MRF approach more convincing. Overall, while the proposed model shows promise, its scalability and applicability to real-world texts are limited, and further experimentation is needed to fully evaluate its potential.