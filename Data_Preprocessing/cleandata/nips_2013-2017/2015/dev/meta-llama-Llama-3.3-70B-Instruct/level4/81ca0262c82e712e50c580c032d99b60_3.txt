This manuscript introduces a novel approach to stochastic optimal control, leveraging a Gaussian process to model the trajectories resulting from applying a proposed control $u^{old}$ to the uncontrolled dynamics. The proposed Bellman equation is designed to optimize $\delta u$, the control update at each iteration, utilizing analytical methods for GP inference to efficiently update GP parameters. Experimental evaluation is conducted across three simulated tasks.
The concept presented is intriguing, and the authors appear to have developed a functional method that can compete with existing approaches. However, several aspects require clarification and improvement:
1. Problem Formulation: The section is somewhat confusing due to the involvement of two optimizations - one for finding the optimal control $u$ and another for optimizing $\delta u$. It seems that the optimal $u$ is provided alongside a proposal control $u^{old}$, which is misleading since determining the optimal $u$ is the primary objective. Clarification on this point is necessary.
2. Organization: The paper's structure could be significantly enhanced. Given that the proposed approach is introduced before section 3.1, it would be beneficial to rearrange subsection 3.1 to follow subsection 2.1, improving readability, especially considering that Eq (9) involves the inference step explained in subsection 3.1.
3. GP Definition and Update: Section 3.1 is confusing as it defines the GP for the uncontrolled process ($u=0$), which contradicts the definition in Eq(2) that depends on the current proposal control. Additionally, if the GP is initialized with sampled data from the uncontrolled process, the authors should clarify how the poor conditioning of this process affects the initialization.
4. Formulation and Generalization: The formulation seems to ignore the cost of the current control sequence, as seen in Eq(5), which is typically included as a Radon-Nikodim derivative in iterative PI control. The authors should identify the equivalent term in their formulation or explain why it is not necessary, and clarify if this method truly generalizes PI control or is another PI-related policy search method.
5. Belief Propagation and Non-Gaussian Dynamics: The use of belief propagation for forward inference may not be robust to abrupt changes in dynamics, such as those encountered with obstacles, leading to non-Gaussian distributions. Since the experiments do not seem to cover such scenarios, the authors should address whether this poses a potential problem.
6. Notation Clarity: Although the method is presented as model-based, it is unclear if a dynamics model is used, as the approach only provides a probability distribution over trajectories via the GP. Adopting clearer notation could help resolve this ambiguity.
7. Relation to Existing Methods: The authors do a commendable job of relating their approach to existing PI-related methods, a challenging task given the extensive literature. However, there are specific points to consider:
   - PI^2 does not ignore constraints but is an open-loop policy search method, making the constraint moot.
   - Feedback PI uses a parameterized feedback term but not a parameterized policy.
   - PI-REPS is model-based, potentially with a learned model.
   - Iterative PI is the original PI with importance sampling, thus model-based with the same structural constraint and without a parameterized policy.
Minor points include unclear sentences and notation suggestions for improved clarity.
Overall, the paper presents an interesting method but requires improvements in presentation and clarification on several key points to enhance its readability and validity.