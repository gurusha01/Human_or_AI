The reflection of light from a surface in a scene is determined by the interplay of geometry, illuminance, and surface reflectance properties, including true chromaticity. Color constancy, the ability to estimate surface reflectance from observed scenes without prior knowledge of geometry or illuminance, is a complex problem that humans solve relatively effectively, and its resolution is crucial for artificial vision systems to identify materials under varying lighting conditions.
This manuscript presents a novel approach to estimating scene illuminance from an image, thereby enabling the recovery of true chromaticity for all pixels. The method posits a single, global luminance across the scene and employs a function, L[x, y], which encapsulates the likelihood that a pixel with observed reflectance y corresponds to a true chromaticity x. Additionally, it utilizes a prior distribution of scene illuminances and assumes a uniform illuminance throughout the scene. Using a calibrated image dataset, the authors demonstrate that L can be learned either by treating pixels independently or through direct minimization of illuminance estimation error using gradient descent on their training set. Their results show a general superiority over existing color constancy methods.
The manuscript is well-structured, and the authors provide a comprehensive comparison with existing approaches. The proposed method is commendably straightforward, grounded in reasonable assumptions, and demonstrates strong performance. Notably, despite not leveraging spatial scene structure, it outperforms alternative methods that do.
The approach bears similarities to the Bayesian estimation technique described in reference 14, which is also the source of the image database used. While the authors compare their method against [14] in table 1, incorporating [14] into the discussion of existing methods on page 2 would provide additional context.
For end-to-end model training, the dataset is augmented by "re-lighting" each image with various illuminants, generating six training images and a seventh for validation. However, it is not specified which hyperparameters were optimized using this validation set. Crucially, it is presumed that the images used for testing accuracy, as reported in table 1, are distinct from those used in training and validation; clarification on this point is necessary to ensure the validity of the results.
A limitation of this manuscript is its primary appeal to the machine vision community, given the specificity of its contribution to the color constancy problem. From a machine learning perspective, the approach, while effective for color constancy, is relatively straightforward, limiting its broader impact. The end-to-end approach to solving the color constancy problem is mostly of interest to the vision community.