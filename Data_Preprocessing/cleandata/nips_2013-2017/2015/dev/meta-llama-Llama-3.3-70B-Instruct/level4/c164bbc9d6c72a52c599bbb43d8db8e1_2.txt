This manuscript presents a highly intriguing and substantial contribution to the field, offering multiple significant and non-trivial advancements. The paper proposes a universal framework for accelerating various optimization techniques, including several recent incremental methods, while also broadening the applicability scope of these methods. The manuscript is exceptionally well-structured, and the contributions are meticulously contextualized within the current state of the art. Although the experimental results are somewhat constrained by spatial limitations, they are nonetheless lucid and persuasive. To the best of this reviewer's knowledge, the findings appear to be novel.
Recent studies have demonstrated that the convergence rate of Nesterov-type acceleration methods can be substantially enhanced through the utilization of adaptive restart techniques, as exemplified in the works "Adaptive Restart for Accelerated Gradient Schemes" by O'Donoghue and Candes, and "Monotonicity and Restart in Fast Gradient Methods" by Giselsson and Boyd. It would be fascinating to explore whether the proposed method in this manuscript could also reap benefits from incorporating a restarting scheme, given its reliance on Nesterov-type extrapolation.
Overall, this work is deemed highly interesting and pertinent, encompassing several crucial and non-trivial contributions that warrant further examination.