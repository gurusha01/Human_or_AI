Summary
This manuscript explores adaptive stochastic optimization on paths by introducing constraints, specifically the MLRB and MLB conditions, which govern the relationship between the probability of a realization and its associated utility. The concept of adaptive submodularity implies that, on average, no item should have a surprising effect given a longer history. In contrast, the MLRB and MLB conditions in this paper essentially capture the idea that such surprises occur with low probability, where realizations with low probability are associated with high utility. By exploiting these constraints, the authors propose a novel recursive algorithm that offers a near-optimal guarantee on the expected cost required to achieve a certain level of coverage. Notably, under the MLB conditions, the paper reproduces the classical approximation result for Bayesian pool-based active learning, albeit with a constant factor difference.
Quality
The manuscript is well-organized and clearly written. Regarding the experimental results, it would be enlightening to see a comparative analysis of the proposed RAC algorithm with existing algorithms (such as those in [3], [5], [7], or [6] under the bounded noise setting) on the pool-based active learning problem, as indicated by Theorem 4. This is particularly relevant given that both the version space reduction function and the Gibbs error function satisfy the MLRB condition, pointwise submodularity condition, and adaptive submodularity condition. The inclusion of such comparative results would undoubtedly strengthen the manuscript.
Clarity
The majority of the manuscript is well-explained. However, it would be beneficial if the authors provided more intuitive explanations for the MLRB and MLB conditions, specifically why these conditions are more natural than adaptive submodularity for adaptive optimization problems on paths.
Originality and Significance
To the best of my knowledge, the technical contributions presented in this manuscript are novel and offer valuable insights into the development of tractable algorithms for adaptive stochastic optimization problems. The manuscript proposes an innovative solution to the adaptive stochastic optimization problem on paths (with sets as a special case) and identifies a class of problems that can be approximated near-optimally using this solution. The authors also provide robust empirical evidence to support their claims. Overall, the presentation is clear and easy to follow.