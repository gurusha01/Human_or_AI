This manuscript proposes an alternative approach to designing a probabilistic model that explicitly captures word co-occurrences by introducing a variant of the Poisson Markov Random Field (MRF) model. The key innovation lies in conditioning the PMRF density on the document length, which simplifies the computation of the partition function by limiting the summation to documents of the same length. This assumption is reasonable in the context of text modeling, as document lengths are typically observed. The authors demonstrate an efficient method for approximating the log partition function using sampling techniques and apply this model to topic modeling, deriving a method for computing hidden topic assignments and comparing it experimentally to Latent Dirichlet Allocation (LDA).
The paper presents an intriguing idea, and the results for the Length-conditioned Poisson MRF (LPMRF) model indeed demonstrate its capability to capture dependencies between word pairs, making for an enjoyable read. However, the absence of detailed explanations regarding the estimation of the topic model hinders the paper's clarity. Moreover, as with novel count models, a more thorough analysis of the model's fit to actual word counts would be desirable before exploring more complex applications. Notably, an assessment of the model's ability to unsupervisedly recover the original 3-way partition of the Classic3 dataset would have been a valuable addition. While the proposal of a Poisson MRF variant for modeling word co-occurrences in topic models is commendable, the lack of detail and the insufficient rigor in the experimental sections are significant drawbacks.