The authors propose a novel approach to incentivize agents to share private and relevant data by leveraging concepts from machine learning, including the stochastic gradient descent algorithm to determine payment for the data. This strategy enables effective compensation of agents while maintaining a limited budget and incorporates mechanisms to preserve agent privacy. The authors also explore various "profit-maximizing" strategies for agents to select based on their confidence in their data. The primary contribution of this work appears to be the development of a mechanism that compensates agents for their data while adhering to a bounded budget.
The effectiveness of this algorithmic family relies on the availability of timely and accurate information. However, if "true samples" are only available in the distant future, the viability of section 2.3 is compromised, and section 2.1 may not offer significant advantages over traditional betting markets, particularly for complex prediction functions.
Section 3 seems to address a problem that may not be fundamentally serious, as simpler solutions, such as anonymized participation or anonymized relations between updates and agents, could potentially preserve agent privacy.
The most significant contribution of this work is the generalization of prediction markets to incorporate sophisticated prediction tools, enabling the extension of prediction markets beyond binary or scalar variables. Nevertheless, the practical applicability of this concept may be limited, as it may not provide sufficient motivation for participation unless the market is accompanied by a constant stream of true samples, allowing for rapid reward evaluation.
A critical issue arises in section 2.3, which appears to rely on knowledge of the loss function of predictions, implying that the correct answers are already known. This raises questions about the purpose of bidding on data and the evaluation of marginal gain from a given data point. Possible solutions to this problem may expose the system to being gamed, such as using past true samples to evaluate new training data or employing prediction divergence measures.
Ultimately, the setup of a cost function seems to require knowledge of future true samples, which may be a significant limitation, particularly if the future is distant. This limitation may preclude the implementation of the "selling" scheme proposed in section 2.3. The authors' idea of extending prediction markets to reward incremental improvements in prediction from contributed data is intriguing, but its practical applications may be restricted due to the requirement of a known loss function, which may not be feasible if the object of prediction is unknown.