The proposed algorithm is straightforward and intriguing, as it histograms pixel luminance, chrominance, and illumination chrominance, and evaluates each pixel's luminance voting on its true chrominance for each memorized illumination. The model can be trained generatively by counting pixels in the training set or end-to-end for a slight performance improvement. This algorithm's simplicity and speed are notable, and it may also serve as a useful foundation for a more complex spatially-varying illumination model.
I found the paper to be reasonably easy to follow, although the mathematical notation could be revised for clarity and to reduce the excessive number of defined symbols, which seems disproportionate for a relatively simple algorithm. For instance, the definitions of x-hat and g() could be clarified, and it is unclear whether v(n) is equivalent to vn. Additionally, the notation { g(v(n), m-hati}_i appears cumbersome and potentially incorrect due to the repeated use of the subscript i. Incorporating figures to illustrate the mathematical concepts could enhance understanding, as currently, only the variable L[] is depicted in a figure.
The evaluation demonstrates that this technique achieves state-of-the-art results on the standard color checker dataset. However, I take issue with one aspect of the evaluation. When comparing the authors' results to those published in [19], it is shown that the authors slightly outperform [19]. Nevertheless, the authors introduce a modified set of error metrics, which they claim are derived from the metrics in [7,19] with a correction to address an unfair advantage in [7,19] due to incorrectly black-leveled images. While it may be true that [7,19] contain errors, such allegations should be treated with seriousness and not addressed in this manner. The corrected number assumes that the models in [7,19] were trained on the incorrect dataset and evaluated on the correct dataset, which would indeed increase error metrics. However, this does not necessarily reflect how [7,19] would perform if trained and tested on the correct dataset. In fact, given the potential impact of incorrect black-level correction on color constancy algorithms, it is possible that [7,19] might perform better if trained and tested correctly. Therefore, I believe the authors' correction is not valid.
Although it is unfortunate that [7,19] may contain incorrect numbers that could be cited, it is essential to address such inaccuracies without introducing new ones. If published, the modified error numbers presented here may be perceived as truthful by future work, despite being speculative and unreliable. Instead, I suggest that the authors use the same error metrics as in [7,19] but indicate that they are likely untrustworthy, either by using an asterisk or a grayed-out font color, and provide an explanation in the supplement. This approach would place the onus on [7,19] to revise their results, avoiding the circulation of contradictory and equally incorrect numbers for the performance of [7,19] in publications. My intention is not to diminish the authors' work, as their algorithm appears to outperform [7,19] even without the correction. This is a matter of scientific integrity and etiquette that should be handled with care. The authors present an interesting color constancy algorithm that seems to outperform the state of the art on the standard color constancy dataset by a reasonable margin. While the paper could benefit from clarification and the evaluation has some issues, the work appears to be sound overall.