Summary: The authors propose a framework for learning complex structured output representations by extending variational auto-encoders (VAEs) to conditional VAEs, which are conditioned on the input data x.
Quality: The paper is generally well-written, although some improvements could be made to enhance clarity and readability. 
Clarity: The main idea is clearly presented, but some details are lacking, which may hinder a full understanding of the proposed method. 
Originality: While conditional VAEs may be seen as a straightforward extension of standard VAEs, they are still worthy of discussion and exploration. 
Significance: The significance of the work could be strengthened by a more comprehensive evaluation, including results for various modifications and ablation studies.
Comments:
- The use of the term "generative" might be misleading, as the proposed conditional variational auto-encoder technique does not generate data via p(x), which could confuse readers.
- The authors discuss three possible methods for inference, i.e., estimating y given x, but only evaluate two of them using toy data. It would be helpful to know which method the authors recommend and why.
- The authors mention a gap between the proposal q(z|x,y) and the prior p(z|x), measurable by the regularization term during or after learning. However, the magnitude of this gap is not specified, which would be useful to understand the implications.
- The proposed solution to address this gap involves introducing a Gaussian stochastic neural network to model the reconstruction term directly, without using KL divergence regularization. A weighted combination of the conditional VAE and this new network is then proposed as the desirable cost function. However, this approach seems counter-intuitive, as one would expect the regularization gap to decrease with more weight on the regularization term rather than the reconstruction term. The authors should provide more intuition behind this approach and explain how they set or cross-validated the hyperparameter Î± and what the resulting value was.
- For the sake of fairness, the authors should consider citing more recent semantic image segmentation techniques using CNNs, as there is a significant amount of relevant work in this area.
- More details about the noise injection into the data x would be helpful for readers to understand the experimental setup. Specifically, what type of noise was injected, and how was it implemented?
- The authors should provide more quantitative results regarding the modifications described in Sec. 3.3.2, such as the importance of the "latent-to-output" pathway and the performance improvement achieved by using direct output prediction as input for the conditional prior network. Additionally, it would be helpful to clarify whether the conditional prior network was trained using only the direct output prediction or both the data x and prediction \tilde{y}. Although some modifications are investigated in Tab. 3, a more thorough ablation analysis would be beneficial.
- Given that the conditional VAE involves multiple networks, it is unclear whether its improved performance is due to the increased number of trainable parameters. The authors should comment on this and provide insight into whether the performance gain is a result of the proposed methodology or simply the increased model capacity.
- Since the abstract claims efficiency, it would be helpful to know the training and inference times for the proposed method, as this is an important aspect of its practical applicability.
- As pointed out by fellow reviewers, adding some citations for completeness would be beneficial. Extending variational autoencoders to conditional distributions is a valuable contribution, and a more extensive evaluation, along with the inclusion of missing details, could further improve the paper.