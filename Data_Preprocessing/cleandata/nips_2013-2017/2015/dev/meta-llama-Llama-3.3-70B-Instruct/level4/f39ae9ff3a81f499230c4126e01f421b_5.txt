This manuscript introduces a novel family of distributions, referred to as the fixed-Length Poisson Markov Random Field (LPMRF), which addresses the computational intractability of the log partition function inherent in the original Poisson MRF (PMRF). The key concept underlying LPMRF involves constraining the domain of potential realizations to counting vectors with a predetermined, fixed sum of components. As LPMRF is a discrete distribution, this constraint effectively transforms the event space into a finite, bounded set, thereby enabling the estimation of the log partition function through straightforward Monte Carlo sampling. Furthermore, the authors demonstrate that an upper bound of the log partition function exhibits linear behavior with respect to the specified fixed length, with empirical validation of the performance of this linear approximation. The proposed application involves extending topic models to account for pairwise dependencies between word pairs, wherein the LPMRF topic model samples multiple words as vectors per topic for each document, considering both word distributions and pairwise dependencies associated with that topic. Due to exchangeability, the bag-of-words vector for a document can be viewed as a simple sum of indicator vectors, effectively generalizing Latent Dirichlet Allocation (LDA) while preserving its fundamental structure.
Given that real-world data often exhibit fixed, finite lengths, LPMRF presents a tractable generalization capable of capturing arbitrary pairwise word dependencies beyond n-gram topic models or correlated topic models. The primary contributions of this work are twofold: the introduction of the refined LPMRF model and the establishment of a lower bound for the log partition function with its corresponding linear approximation. To further enhance the manuscript, the authors could provide more in-depth analysis of the linear approximation, such as utilizing additional points around the mean in a least squares or regression context. A significant challenge lies in determining the complexity of solving the optimization problem outlined on page 6, which currently employs an EM-fashion algorithm that initially estimates LPMRF parameters using the PMRF algorithm and subsequently solves another optimization problem to find topic assignments given fixed parameters. It remains unclear whether the use of the PMRF algorithm is suitable for the LPMRF setting. Moreover, the second part of the optimization is addressed by identifying optimal portions of words assigned to a topic to reassign to other topics, with unclear scalability and effectiveness for a general number of topics using the dual coordinate ascent approach. More careful handling of these optimizations, potentially in combination with collapsed Gibbs sampling, would further improve the manuscript.
The experimental setup is currently minimal, with LPMRF being tested on an extremely small number of topics, likely due to limited scalability in inference. Given the substantially larger number of parameters compared to LDA, it is not surprising that LPMRF models with only one or three topics achieve comparable perplexities to LDA models with more than 20 topics. A more intriguing aspect would be to compare inter-topic qualities, including how well topics are separated and how effectively inter-topic interactions are captured via word dependencies. In conclusion, the proposed model is both interesting and useful, as it overcomes the intractability of PMRF. However, the inference of solving optimization problems appears insufficiently explored, and the evaluation is too minimal.