This manuscript presents a novel extension of the Poisson MRF model, referred to as the Fixed-Length Poisson MRF (LPMRF), which is capable of capturing positive dependencies. The authors utilize this model as the emission distribution in an admixture model similar to Latent Dirichlet Allocation (LDA), where each topic encapsulates information about pairwise dependencies between word types. The application of this model to text data yields intriguing and competitive results, even with a limited number of topics (only 3).
A key technical inquiry arises from this work: What is the boundary between modeling topics and modeling word dependencies? If a perfect model of word dependencies were available, would a mixture model over topics still be necessary? Alternatively, do the topics merely account for higher-order dependencies (e.g., ternary) that cannot be easily captured by the proposed word dependency model? With a sufficient number of topics, the top words alone could be sufficient to analyze the results, similar to LDA. However, the intuitive nature of word type dependencies may become increasingly questionable as the number of topics increases, particularly since each topic captures specific positive and negative dependencies that may interact across topics in complex ways.
A primary concern with this approach is its scalability, particularly with respect to vocabulary size. As the number of topics increases, the memory requirements may become prohibitively large. It would be beneficial to know the size of the vocabulary in the dataset used and to understand why the authors chose not to test their model with a larger number of topics.
The manuscript makes a significant contribution to modeling, particularly with the development of a fixed-length PMRF with distinct properties from the original PMRF distribution. However, the contribution to inference is relatively minor, as existing techniques that optimize pseudo-likelihood are employed. It would be interesting to know why the authors opted for optimization over sampling parameters.
The Admixture of Poisson MRFs model is mentioned several times but not discussed in detail. A more comprehensive related work discussion, including citations to collocation literature from linguistics and a comparison with the proposed approach, would be beneficial. Unlike traditional collocation approaches that focus on sequential word dependencies, the proposed model captures long-range dependencies without considering word order.
In Section 3, the authors argue that LDA does not capture dependencies between words because only one word is drawn at a time from a generative perspective. However, from an inferential standpoint, allowing for a sufficient number of topics can help capture various types of positive dependencies.
The proposed model demonstrates only marginal improvements over LDA in terms of perplexity. Given that perplexity is a flawed indicator of topic quality and that numerous existing topic models outperform LDA in this regard, this finding is not particularly noteworthy.
The negative dependencies presented in Table 1 are less intuitive than the positive dependencies. The authors should provide further commentary on this aspect.
More compelling applications for this model may lie outside the realm of topic modeling, such as in event count modeling for political science, where relationships are less well understood and the relationships themselves are the primary objects of study.
Minor suggestions include repositioning Figure 1 to page 2, as it is highly effective in conveying the model's concepts. The value of L used to generate these graphs should be specified. Model names and acronyms should be standardized throughout the manuscript. The meaning of CPMRF in Equation 1 should be clarified, and the size of the dependency matrix used to produce Figure 3 should be noted.
For the perplexity results in Figure 4, the caption should indicate that lower values are preferable. The font size in all figures should be increased for better readability. The use of \theta and \Theta as distinct parameters, as well as 'e' both as a mathematical constant and as a vector of ones, should be avoided due to potential confusion.
The section on Monte Carlo sampling should precede the Upper Bound section in 2.1. On Page 5, the "LPMRF" in the generative process should be emphasized in bold font to highlight its significance as the manuscript's primary contribution.
In Section 4, the optimization of the Dirichlet parameter \beta is mentioned, but it seems \alpha might be the intended parameter. Overall, this manuscript presents an interesting statistical contribution and shows incremental advances in document topic modeling, albeit with some limitations and areas for further improvement.