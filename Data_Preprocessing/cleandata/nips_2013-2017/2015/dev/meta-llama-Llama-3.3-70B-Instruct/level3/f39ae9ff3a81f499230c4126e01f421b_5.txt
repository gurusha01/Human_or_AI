This paper proposes a novel approach to online F-measure optimization, which is a crucial problem in machine learning, particularly in applications with imbalanced class distributions. The authors introduce an efficient online algorithm, called Online F-measure Optimizer (OFO), that converges to the optimal F-score when the posterior estimates are provided by a sequence of classifiers with a certain convergence rate.
The paper's main contribution is two-fold. Firstly, it proposes a refined family of distributions, called the fixed-Length Poisson MRF (LPMRF), which overcomes the intractability of computing the log partition function in the original Poisson MRF. Secondly, it provides a lower bound of the log partition function with a linear approximation, allowing for efficient estimation via Monte Carlo sampling.
The authors apply LPMRF to generalize topic models, considering pairwise dependencies between words and sampling multiple words as a vector per topic for each document. The LPMRF topic model generalizes LDA while maintaining its basic structure and can consider arbitrary pairwise word-dependencies beyond n-gram topic models.
The paper's strengths include its novel approach to online F-measure optimization, its ability to handle non-decomposable performance measures, and its promising experimental results. The authors provide a thorough analysis of the algorithm's consistency and convergence properties, and the experimental evaluation demonstrates the effectiveness of OFO in one-pass learning and online learning scenarios.
However, the paper also has some weaknesses. The optimization method used in the paper is unclear and may not be scalable or effective, and could be improved by combining with collapsed Gibbs sampling. The experimental evaluation is minimal, testing OFO with only a small number of topics, and further evaluation is needed to compare inter-topic qualities and capture word-dependencies.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader.
The originality of the paper is high, as it proposes a novel approach to online F-measure optimization and provides a refined family of distributions. The paper is significant, as it addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of machine learning and provides a novel approach to online F-measure optimization. However, I would suggest that the authors address the weaknesses mentioned above, particularly the optimization method and experimental evaluation, to further improve the paper.
Arguments pro acceptance:
* Novel approach to online F-measure optimization
* Refined family of distributions (LPMRF) that overcomes the intractability of computing the log partition function
* Promising experimental results in one-pass learning and online learning scenarios
* Thorough analysis of the algorithm's consistency and convergence properties
Arguments con acceptance:
* Unclear optimization method that may not be scalable or effective
* Minimal experimental evaluation that needs to be further extended
* Lack of comparison with other state-of-the-art methods
* Limited analysis of the algorithm's rate of convergence and robustness to hyperparameter tuning.