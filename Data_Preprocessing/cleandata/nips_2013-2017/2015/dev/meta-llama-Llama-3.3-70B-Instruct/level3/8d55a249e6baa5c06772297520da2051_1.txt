This paper presents a framework for online F-measure optimization, which is a challenging problem due to the non-decomposable nature of the F-measure. The authors propose an efficient online algorithm, called Online F-measure Optimizer (OFO), that converges to the optimal F-score when the posterior estimates are provided by a sequence of classifiers whose L-error converges to zero as fast as t for some > 0.
The paper's quality and clarity are generally good, but some details are missing, and the writing could be improved occasionally. The originality of the paper is questionable, as the proposed algorithm seems to be a straightforward extension of existing stochastic approximation algorithms. However, the significance of the paper could be improved with a more extensive evaluation showing results for various modifications and a clearer explanation of the proposed technique.
One of the strengths of the paper is its ability to handle online learning scenarios without the need for a hold-out validation set, making it a purely online approach. The experimental results show that OFO performs well in practice, with its performance being on par with the 2-stage F-measure maximization approach (2S) in most cases.
However, there are some weaknesses in the paper. The use of the term "generative" in the paper may be confusing, as the proposed technique is not capable of generating data via p(x). Additionally, the paper evaluates two methods for inference using toy data, but it is unclear which method the authors recommend and why.
The paper could benefit from more details regarding the noise injection into data x and the experimental setup. Furthermore, the authors do not provide sufficient quantitative results regarding the modifications described in the paper, making it difficult to evaluate the importance of certain components.
In terms of originality, the paper's contribution is limited, as the proposed algorithm is based on existing stochastic approximation algorithms. However, the paper's significance lies in its ability to handle online F-measure optimization, which is an important problem in machine learning.
Overall, the paper is well-written, and the proposed algorithm is efficient and effective. However, the paper could benefit from more extensive evaluation, additional citations, and clearer explanations of the proposed technique and experimental setup.
Arguments pro acceptance:
* The paper presents a novel online algorithm for F-measure optimization, which is an important problem in machine learning.
* The algorithm is efficient and effective, with its performance being on par with the 2-stage F-measure maximization approach (2S) in most cases.
* The paper provides a clear and concise explanation of the proposed technique and its theoretical foundations.
Arguments con acceptance:
* The paper's originality is limited, as the proposed algorithm is based on existing stochastic approximation algorithms.
* The paper could benefit from more extensive evaluation, additional citations, and clearer explanations of the proposed technique and experimental setup.
* The use of the term "generative" in the paper may be confusing, and the paper evaluates two methods for inference using toy data without recommending one over the other.