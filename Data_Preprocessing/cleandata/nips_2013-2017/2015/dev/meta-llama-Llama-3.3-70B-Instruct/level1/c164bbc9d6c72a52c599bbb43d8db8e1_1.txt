This paper introduces a generic scheme for accelerating first-order optimization methods, which the authors call the "catalyst" acceleration. The approach is based on a new analysis of the accelerated proximal point algorithm and can be applied to a wide range of optimization methods, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.
The paper provides a thorough theoretical analysis of the catalyst acceleration, including convergence rates for both strongly convex and non-strongly convex objective functions. The authors also provide a detailed comparison with existing acceleration methods and demonstrate the effectiveness of the catalyst acceleration in practice through experiments on several datasets.
The strengths of the paper include:
* The introduction of a novel and generic acceleration scheme that can be applied to a wide range of optimization methods.
* A thorough theoretical analysis of the catalyst acceleration, including convergence rates for both strongly convex and non-strongly convex objective functions.
* A detailed comparison with existing acceleration methods, including Nesterov's accelerated gradient descent and accelerated SDCA.
* Experimental results demonstrating the effectiveness of the catalyst acceleration in practice.
The weaknesses of the paper include:
* The paper assumes that the optimization method has a linear convergence rate for strongly convex objectives, which may not always be the case.
* The choice of the parameter κ, which controls the acceleration, is not always clear and may require careful tuning.
* The paper does not provide a detailed analysis of the computational complexity of the catalyst acceleration, which may be important in practice.
Overall, the paper provides a significant contribution to the field of optimization and has the potential to impact a wide range of applications. The catalyst acceleration scheme is novel, generic, and effective, and the paper provides a thorough theoretical analysis and experimental results to support its claims.
Arguments pro acceptance:
* The paper introduces a novel and generic acceleration scheme that can be applied to a wide range of optimization methods.
* The theoretical analysis is thorough and provides convergence rates for both strongly convex and non-strongly convex objective functions.
* The experimental results demonstrate the effectiveness of the catalyst acceleration in practice.
Arguments con acceptance:
* The paper assumes that the optimization method has a linear convergence rate for strongly convex objectives, which may not always be the case.
* The choice of the parameter κ is not always clear and may require careful tuning.
* The paper does not provide a detailed analysis of the computational complexity of the catalyst acceleration.
Rating: 8/10
Recommendation: Accept with minor revisions. The paper is well-written, and the results are significant. However, the authors should provide more details on the choice of the parameter κ and the computational complexity of the catalyst acceleration. Additionally, the authors should consider providing more experimental results to demonstrate the effectiveness of the catalyst acceleration in practice.