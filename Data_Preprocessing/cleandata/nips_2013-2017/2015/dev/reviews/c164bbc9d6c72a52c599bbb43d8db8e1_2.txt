I think this is a very interesting and significant piece of work, with several relevant and non-trivial contributions. Not only does the paper introduce a generic scheme to accelerate several optimization methods (namely several recent incremental methods), but it also extends the domain of applicability of some of those methods. The paper is very well written and the contributions are carefully put in the context of the state of the art. The experimental results, although naturally limited by the scarcity of space, are clear and convincing. As far as this reviewer knows, the results are original.
It has been recently shown that the convergence rate of Nesterov-type acceleration methods can be significantly boosted by using adaptive restart techniques. See for example, the paper "Adaptive Restart for Accelerated Gradient Schemes", by O'Donoghue and Candes, and the paper "Monotonicity and Restart in Fast Gradient Methods", by Giselsson and Boyd. It would be interesting to consider if the method proposed in this manuscript could also benefit from some form of restarting scheme, given that it also uses Nesterov-type extrapolation.
 I think this is a very interesting and relevant piece of work, which contains several important and non-trivial contributions (see details below).