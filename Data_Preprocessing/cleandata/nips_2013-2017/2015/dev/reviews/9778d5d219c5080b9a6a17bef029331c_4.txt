The distribution of light reflected from a surface in a scene a function of the geometry, scene illuminance and the surface reflectance (true chromaticity). The problem of color constancy, is to estimate the surface reflectance from the observed scene, typically without being given either the geometry or scene illuminance information. This is of interest as a task that humans are relatively good at, and of importance for artificial vision systems to recognize and identify materials under variable lighting conditions.
The authors introduce a method for estimating the scene illuminance of an image (which then allows recovering the true chromaticity of all pixels). The method assumes a single, global luminance in the scene and uses a function L[x, y] encoding the belief that a pixel with an observed reflectance y has a true chromaticity x. The method also makes use of a prior on scene illuminances and an assumption of a single illuminance for the whole scene. Using a dataset containing calibrated images, they can learn L either by treating each pixel independently, or by directly minimizing the error in illuminance estimation on their training set with gradient descent. They show that their approach is generally superior to existing color constancy methods.
The manuscript is well-written and the authors compare their approach with a number of existing approaches. The method presented is pleasingly simple, makes reasonable assumptions and performs well. It is notable that this method makes no use of spatial structure in the scene and yet is able to outperform alternative approaches which do.
The approach used bears some similarities to the Bayesian estimation method used in reference 14 (the source of the image database). Although the author's compare against [14] in table 1, it would be helpful to include [14] in the discussion of existing methods on page 2.
For training of the end-to-end model the dataset is augmented by "re-lighting" each image with different illuminants to create 6 training images and a 7th validation image. It is unclear which hyper-parameters were chosen using this validation set. Importantly, I presume the images used for testing accuracy (table 1) are distinct (not just relit) from any used in training or validation (if not, this would be a serious flaw). This should be clarified briefly in the manuscript.
A weakness of this manuscript is that it is primarily of interest only for the machine vision community. The approach used is (from a machine learning point of view) fairly straightforward and the primary contribution is specific to the problem of color constancy. End-to-end approach to the color constancy problem. Mostly of interest to the vision community.