As mentioned in the paper, the algorithm in the paper is basically the same as in [9]. It combines extrapolation idea from Nesterov's acceleration scheme with proximal point operator. The algorithm in [9] used exact minimization in each iteration, which is not possible in practice. The current paper carries out the analysis with more realistic assumptions of approximate minimization. Paper [8] is quite similar in the above respects. It's stated that the current paper is independent of [8], but given that [8] has already been published, in my view the current paper should be considered only on the basis of what it has in addition to the results that overlap with [8]. There is a detailed comparison of the results here and in [8] in the appendix, but I would have also liked to see a comparison of the methods. Thus one of the main new result in the current paper is that it can also handle functions that are not strongly convex. But it seems to me---but I could be wrong as I didn't actually read the proofs---that the methods of proof are similar for the strongly convex and general convex cases. So I guess the question to answer is: Does the method of [8] also easily give the case where the function is not required to be strongly convex or is a new idea needed?
 The other additional result is the acceleration and other improvements of Finito/MISO. The modified algorithm here turns out to be very similar to a variant of SDCA [24], though there are differences. Given this, the experimental results should have included a comparison with SDCA, and I consider this a major omission.  The paper provides an analysis of a generic acceleration method for first-order optimization based on Nesterov's extrapolation idea and proximal point operator.