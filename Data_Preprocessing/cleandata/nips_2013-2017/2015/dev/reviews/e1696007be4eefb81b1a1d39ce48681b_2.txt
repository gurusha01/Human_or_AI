This paper studies atomic norms used in structured sparse regression. For a given atomic set A (satisfying a sign-change invariance assumption), the paper bounds the Gaussian width of the unit norm ball, of the tangent cone, and the L2-compatibility number--quantities which are sufficient to characterize the rate of convergence of the estimation problem under certain settings.
The new technique is general. When applied to the L1 norm, both unweighted and weighted, it is shown to match bounds derived from existing techniques. The authors also apply the new technique on k-support norms and derive novel bounds; the new bounds have the interesting implication that k should be under-specified in practice.
 The unit norm ball analysis builds on a lemma from [14] and does not seem a significantly novel contribution. The tangent cone and the compatibility analyses look nontrivial. They build upon a simple but powerful observation that the tangent cone of an atomic norm can be upper bounded by the tangent cone of a weighted L1 norm (Lemma 3).
My one suggestion is that the authors perform numerical simulations with k-support norms and test whether the bound matches actual behavior. It would be interesting to see whether over-specifying k would hurt recovery.
 I find the paper interesting and the proof relatively easy to follow. The paper is well-organized and the results clearly stated and explained.
 [14] An inequality with application to structured sparsity and multi-task dictionary learning. This paper gives a new method of bounding the Gaussian width of various sets important for deriving the rates of consistency of predictors with structured sparsity penalties. The new method seems innovative and gives a novel convergence rate for regressions with k-support norms.