This paper extends variational auto-encoders to the structured prediction setting, where now it is used to model the probability of the structured output given the input. It adds several innovations to the training process, and shows results on pixel labeling on CUB and LFW. The paper is novel to the best of my knowledge, and using such expressive models for structured prediction is I think an important goal and something that can have a high impact. The aims of this paper and the ideas presented in this paper are therefore very welcome. However the paper falls short in two ways: - The paper is not very clearly written. It is missing several details. First, what is the architecture of all the nets? The authors mention at some point that all networks are MLPs with 2 layers, but that can't be the full story because an MLP is not a generative model. Perhaps the authors mean that the MLP predicts the parameters of a Gaussian distribution as in [24] or a mixture model in [16]? It is also not clear how p(y|x,z) is being modeled by two networks net{z2y} and net{x2y}. Do these two networks produce predictions that are averaged together? Multiplied together? Later researchers trying to reproduce results will have a hard time. - The results are a bit underwhelming. Tables 3 and 4 show that the CNN is remarkably competitive while also being much simpler. Is the 1 point improvement worth it? (I am also not sure of what a 1 point gain in these datasets means). Table 5 does show good results, but the setting is a bit contrived.
In total, I like the ideas described in this paper and believe they deserve exploration. However, the results are not very great, and the paper requires some rewriting to make it clearer. This paper uses recent progress in variational autoencoders to help structured prediction. While the aims are noble, the results are slightly underwhelming, and several details are missing.