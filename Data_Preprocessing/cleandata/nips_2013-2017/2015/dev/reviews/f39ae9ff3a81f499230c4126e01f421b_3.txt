This paper presents yet another attempt at designing a probabilistic model able to explicitely capture word cooccurences. The authors develop a variant of the Poisson MRF model, where the PMRF densitiy is crucially conditionned on the length - this means that the partition function does not need to sum over all word vectors for all possible lengths; instead the summations run over all documents of the same length. In text modeling, this is a reasonable modeling assumption, since the length is observed anyway. For
this model, the authors show how to approximate efficiently the log partition, using sampling techniques. This model is then used for topic modeling, where the author show how to compute the hidden text topic assignments, and experimentally compared the method with LDA.
This is an enjoyable paper, containing a nice idea: the results presented for the LPMRF indeed show that it can model dependency between pairs of words. It is at the same time a bit frustrating, as details regarding the estimation of the topic model are missing, making the paper more difficult to follow than it should. Furthermore, as is the case with new count models, one would like to see more analyses regarding the fit with actual word counts before jumping to more fancy applications. One other result I would have liked to see is the ability of the model to actually recover in an unsupervised manner the original 3-way partition of Classic3.  Yet another attempt to model word coccurrences in topic models based on a variant of the Poisson MRT; the lack of details, and the unsufficient thoroughness of the experimental sections are however problematic.