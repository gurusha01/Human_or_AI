The paper proposes the fixed length PRMs distribution, a distribution that induces dependencies between the dimensions of a multinomial distribution.
 The paper is well written and tackles and interesting problem.
Actually, poisson graphical models are currently receiving a lot of attention and help to develop novel forms of probabilistic topic models.
The main goal is to keep the partition function finite. In the present paper, the idea is to restrict the support of the partition function
to the vectors of the same total length/norm.
I like this idea, although the paper would be much stronger if it were comparing to a truncated Poisson LDA, i.e., using TPMRF as presented in [6]. So far, there is only a theoretical discussion of the differences that are not backed up by any empirical illustration.
Related to this, there should be a comparison to the PMRF topic model as presented in [2]. This comparison should show case the benefits of the presented approach to the more general approach of [2]. While I agree that [2] does not feature easily perplexity evaluations because of the tricky partition function, one could still apply some naive MCMC chain on the local Poisson distribution estimated.
At least a comparison as presented in Table 1 should be presented,
to see any benefit in the discovered dependencies.
 Furthermore, given that the goal is to estimate a "multinomial with dependencies among the states", the authors should comment
on
just using a "factorisation" of the multinomial distribution into k binary random variables with corresponding dependencies, e.g.,
a tree like dependency or a dependency network?
To summarise, an interesting idea that should explore the connection to the related work more for justification. The benefits over existing approaches are not clearly presented.  + novel Poisson MRF with an application to LDA + it is very refreshing to revisit the multinomial approach underlying most of machine learning- baselines should be extended