The paper presents a novel PI-related method for stochastic optimal control. The main idea is to use a Gaussian process to model the trajectories resulting of applying a proposal control $u^old$ on the uncontrolled dynamics. The proposed Bellman equation is defined to optimize for $\delta u$, the control update at each iteration. The method takes advantage of analytical methods for GP inference to update the GP parameters efficiently and it is evaluated experimentally in three simulated tasks.
The idea is interesting and authors seem to have a working method that can compete with existing ones. I like the paper in general, but I think the following points need to be addressed:
The problem formulation section is a bit confusing, since there are two optimizations involved: one is the optimization over $u$ to find the optimal control and the other is the one that optimizes $\delta u$. From 2.1, it seems that the optimal u is given together with a proposal control u^old, which is clearly not the case, since finding the optimal u is exactly the main task. Authors need to clarify this point.
The organization of the paper could be significantly improved: since the proposed approach is not introduced in 3.1 but earlier, I would move subsection 3.1 after subsection 2.1. This would also improve the readability, since Eq (9) involves the inference step explained in subsection 3.1.
I also found confusing section 3.1, in which the GP is defined (line 208) for the uncontrolled process (u=0) which contradicts the definition in Eq(2) which depends on the current proposal control. Additionally, if the GP is initialized using sampled data from the uncontrolled process, authors should also clarify how the poor conditioning of the uncontrolled process in terms of effective sample size affects this initialization.
Also related to how the GP model is updated, it looks like the formulation ignores the cost of the current control sequence, see Eq(5). In iterative PI control this term enters in the equation as a Radon-Nikodim derivative [16,17]. Can the authors point at the equivalent term in their formulation? Can we still talk about a generalization of PI control or is this another policy search, PI-related method?
The use of belief propagation for forward inference may fail when the dynamics around the reference change abruptly, i.e. they are non-Gaussian. This happens in the presence of obstacles, for example, which does not seem to occur in the presented experiments. Authors should clarify if this is a potential problem or not.
About notation, although authors present the method as model-based, it is not clear that a model of the dynamics is used (they only have a probability distribution over trajectories via the GP). I suggest different notation for clarity.
Authors do a good job relating their approach with existing PI-related methods, a difficult task given the (already large) current literature on PI control. I have the following remarks in that respect:
- In PI^2 [8] the constraint is not ignored, but PI^2 is an open-loop policy search method, so the constraint is meaningless and the same argumentation of PI^2-CMA holds. - Feedback PI [17] uses a parameterized feedback term (the second order correction), but not a parameterized policy. - PI-REPS [11] is model-based (although the model can be learned, as in guided policy search [23]) - Iterative PI [16] is original PI with importance sampling and therefore: model-based, with same structural constraint as PI and without parameterized policy.
These remarks do not involve table 1 only, but text spread through the whole paper.
 minor:
line 120: minimize(s) lines 144-145: sentence unclear: "while (...)." line 146: (to) act line 229: based on (a) probabilistic The paper presents a novel PI-related method for stochastic optimal control that uses a Gaussian process to model trajectories. The approach is promising but it is not clear if this is a generalization of PI control or it is a policy search, PI-related method. Presentation should be improved and some other points need to be clarified.