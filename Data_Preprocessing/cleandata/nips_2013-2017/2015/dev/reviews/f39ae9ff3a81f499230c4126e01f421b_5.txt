This paper proposes a refined family of distributions called the fixed-Length Poisson MRF (LPMRF) which overcomes the intractability of computing log partition function in the original Poisson MRF (PMRF). The core idea is to limit the domain of possible realizations only to the counting vectors whose component sum is fixed a priori. Since LPMRF is discrete distribution, this restriction effectively reduces the event spaces into a finite/bounded set, allowing users to estimate the log partition function via a simple Monte Carlo Sampling. The authors also show an upper bound of the log partition function is linear to the given fixed length. The performance of linear approximation is validated empirically. The application is to generalize topic model so that it considers pairwise dependencies between pairs of words. Instead of sampling each word from the underlying topic independently, the LPMRF topic model samples multiple words as a vector per topic for each document considering the word distributions and the pairwise dependencies of that topic. Due to the exchangeability, the bag-of-words vector for the document could be seen as a simple sum of these indicator vectors, generalizing LDA while maintaining the same basic structure.
As real data in many cases have fixed finite length, LPMRF could be a tractable generalization that can consider arbitrary pairwise word-dependencies beyond n-gram topic models or different from the correlated topic models. The main contribution is two-fold. One is the refined model, and the other is the lower bound of log partition function with its linear approximation. The paper could be further improved if the authors include more analysis about linear approximation such as using more points around the mean as a least square or a regression sense. The main difficulty is to figure out how hard to solve the optimization given in the page 6. It seems the current method repeats EM-fashion algorithm that first estimates LPMRF parameters with the PMRF algorithm and then solve another optimization problem finding the topic assignments given the fixed parameters. It is unclear that using PMRF algorithm is the proper choice for LPMRF setting. Moreover, the second part is solved by finding best portions to move words assigned for a topic to another topics. It is also unclear that this dual coordinate ascent manner is scalable and effective enough for the general number of topics. If these optimizations are more carefully handled (possibly combining with collapsed Gibbs sampling), the paper will be further improved.
For the experimental setting, the current evaluation is minimal. LPMRF is tested only with extremely small number of topics (possibly because its limited scalability in inference). Considering the overwhelming number of parameters comparing to LDA, it is not that surprising LPMRFs with only one or three topics achieve comparable perplexities to the LDA with more than 20 topics. The more interesting part is to compare inter topic qualities: how well topics are separated and how well inter-topic interactions are captured via word-dependencies.
 An interesting and useful model which overcomes the intractability of PMRF is proposed. The inference of solving optimization problems seems not sufficiently explored, and the evaluation is too minimal.