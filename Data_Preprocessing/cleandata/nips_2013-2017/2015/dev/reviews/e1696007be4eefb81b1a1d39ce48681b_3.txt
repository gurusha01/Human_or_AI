This paper is about sample complexity and error bounds for Dantzig-type estimators in the context of structured regression. A central ingredient of such bounds are geometric properties like the the Gaussian width of the norm ball and similar related quantities. For the class of atomic norms that are invariant under sign changes, novel bounds of these geometric properties are provided, which can be easily computed in practice. Further, tightness of the bounds is analyzed by showing that these upper bounds are close to the corresponding lower bounds.
The main focus of this work concerns a topic which is definitely relevant and interesting to the Machine Learning community. The papers is written in a clear and transparent way. In my opinion, the bounds obtained are both of theoretical and practical interest. However, I have to admit that I did not fully understood some technical details like the proof of Theorem 7.
  An interesting theory paper about a highly relevant topic. Well written.