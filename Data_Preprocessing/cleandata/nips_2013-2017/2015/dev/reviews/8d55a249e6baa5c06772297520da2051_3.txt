In particular, the prediction of a structured output that is not the input data (ex. predicting segmentation labels) starts to make the use of the terminology of auto-encoders a bit of a stretch. Predicting the other half of an image stays within the normal conceptual framework of an autoencoding scheme.
The use of the term 'conditional prior' deviates a little far from what I think some members of the NIPS community would consider as appropriate and acceptable terminology.
Why not just call P(z|x) the conditional distribution of the hidden variable or the stochastic representation z or something else. Re-defining the concept of a prior as a 'conditional prior' could be quite incompatible with some already very established terminology about what it means to be a 'prior'. I very much understand the motivations here; however, I feel the terminology should be more precise in a publication.
Please give some more details on the baseline CNN, details can be important to understand if this is a strong or straw-man baseline. There are also a number of minor language errors throughout the manuscript that need to be fixed.  This paper explores a natural variation of stochastic conditional models, presented here as a conditional variational auto-encoder. The essence of the idea here is reasonable, and the experimental work is quite extensive. However, some terminology used in the paper stretches the accepted interpretations of certain concepts a little too far (see more detailed comments for specifics).