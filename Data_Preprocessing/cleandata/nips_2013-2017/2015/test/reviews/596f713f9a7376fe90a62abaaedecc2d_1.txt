The basic idea is this: a popular prior for modeling natural image patches is the GMM. A computationally expensive step in (usually iterative) inference algorithms when using such models is computing the posterior mixture assignments---i.e., determining which component a particular patch belongs to. This usually requires computing the relative likelihood of the patch belonging to each component, by the expensive process of computing the norm of the patch with respect to each component's covariance matrix.
Instead, the authors propose learning a smaller gating network that computes these assignments (strictly speaking, the posterior mixing probabilities) using a neural network with a single hidden layer, with a relatively small number of units. The weights of this network are learned to minimize the KL divergence with respect to the true posterior distribution for a training set of patches.
 Essentially, the authors propose training a classifier to predict the component assignments, and show that this discriminatively-trained classifier can replicate the behavior of an explicitly computed posterior from a generative model, but with far fewer computations. Moreover, since this can be seen as a "drop-in" replacement for a computation step that deals only with the prior, it can be trained once and used for inference for different tasks with different likelihood terms. This second property gives the approach an advantage over "end-to-end" discriminative training, which requires re-training for each task (for example, even in the case of de-blurring, such methods would require to be retrained for every blur kernel).
Overall, the paper is based on a novel and interesting idea that is well executed and reasonably evaluated. Indeed, I expect that it has the potential to be used in applications beyond image restoration. I am inclined to recommend acceptance, but would like the authors to comment on the following issues in the rebuttal/final manuscript:
1. The description of the number of computations in terms of dot products is a little confusing. For instance, in the traditional GMM case, the paper says the first layer computes d * K dot-products, and the second computes K dot-products. In this case, the length of the vectors (for which the dot-product is computed) is equal for both layers.
 But in the proposed gating network, it is not. The first layer computes N dot-products of length d, and in the second, it computes K dot-products of length N. When in lines 241-243, the paper says that "we set the number of dot-products to be 100", it is not clear whether this refers to the total number of dot-products, or those only in the first layer (I suspect that this is the latter---but in this case, note that "the second layer" now requires computing K dot-products over much larger vectors).
It would be a lot clearer if the discussion was in terms of the number of multiplies and adds, and the number of neurons in the hidden layer (i.e., the number of v_i's) was explicitly mentioned.
2. I would have liked to see an evaluation (of say just denoising) when using different numbers of mixture components. An interesting question is: to what extent does one need to increase the number of neurons in the hidden layer as the number of mixture components goes up ? Or equivalently, how does the proposed approach change the accuracy-time trade-off for choosing the number of mixture components ?  The paper introduces a computationally-efficient alternative to compute posterior mixture-component assignments during inference in image restoration, when using Gaussian mixture model (GMM) priors for image patches.