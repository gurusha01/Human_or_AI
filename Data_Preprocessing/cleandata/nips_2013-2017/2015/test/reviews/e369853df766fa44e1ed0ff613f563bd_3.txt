This paper proposes a novel online algorithm for constructing a multiclass classifier that enjoys a time complexity logarithmic in the number of classes k. This is done by constructing online a decision tree which locally maximizes an appropriate novel objective function, which measures the quality of a tree according to a combined "balancedness" and "purity" score. A theoretical analysis (of a probably intractable algorithm) is provided via a boosting argument (assuming weak learnability), essentially extending the work of Kearns and Mansour (1996) to the multiclass setup. A concrete algorithm is given to a relaxed problem (but see below) without any guarantees, but quite simple, natural and interesting.
The paper is well written, very interesting and seems to be original. The theoretical analysis seems to be correct. The experimental results suggest a good improvement over other logarithmic time classifiers but fall quite short of obtaining state-of-the-art generalization performance. I see this work as a small step forward toward this goal.
I do have some concerns:
- Obviously, the 1-nearest neighbour classifier has train and test times which are independent of the number of classes k. This is so since with the time scale used in your time complexity analysis, reading log(k) bits is regarded as a O(1) operation. Thus I find your claim that the most efficient possible accurate approach has time complexity log(k) as misleading. I don't find the information theoretic arguments given in the introduction as relevant to your time complexity analysis of the algorithm. Can you please clarify?
- If I understand correctly, the LOMtree algorithm does not quite maximize the relaxed objective given in line 263.
This is due to the fact that your estimates of the expectations mv(y), Ev, etc. are not updated when some retrained hypothesis up the tree changes its decision on some samples along the paths. As the statistics given in algorithm 1 are additive, such changes does not reflect in your estimates. Isn't a more complex mechanism (such as message passing) is needed for accurate bookkeeping? It seems that such "open-loop" inaccurate statistics in the nodes can make the algorithm behave quite erratically. Can you please clarify this point?
- It seems that when one goes down in the tree, the classifiers are doomed to have lower accuracy. This is so since the tree is quite balanced and lower nodes will have much less samples to train a good classifier on. Thus, I suspect that an exponential number of samples will be needed to get good generalization accuracy. This paper proposes an online multiclass classifier with sufficient novelty and interest to accept for publication.