The paper addresses the problem of learning labels on the vertices of a graph. It builds on the work of Ando and Zhang which suggested that embedding the graph on a unit sphere, by means of an orthonormal presentation, results in better generalization error, i.e., error on the test set. Ando and Zhang however, did not present a concrete solution on how to choose an optimal embedding. The current paper's contribution is the proposal of a new optimization formulation which jointly optimize the embedding and the learned labels by introducing a regularization term. The regularization term, the spectral norm of the corresponding graph kernel matrix, is chosen based on the insight obtained from Theorem 1 in the paper which states an upper bound on the generalization error. Further results in the paper states asymptotic upper bound on the generalization error of the proposed method which has favorable scaling compared to previous results. This upper bound can also be translated to an upper bound for labeled sample complexity. While the proposed optimization can be cast an SDP the complexity of solving SDPs are not desirable. Hence, the paper suggest optimizing this problem using inexact proximal method which can scale to graphs with 1000s of vertices. Some experimental results indicate the superiority of the proposed method compared to previous methods.
The reviewer was not able to understand the main result of the paper, i.e., Theorem 4, at the high level. In the paragraph preceding Theorem 4 the paper states "in the presence of unlabeled nodes, without any assumption on the data, it is impossible to learn labels. Following existing literature [1, 12], we assume an edge links similar instances." However, there is no definition of what \emph{similar} means and there is no mention of it in the assumptions of the Theorem. The reviewer could not find any mention or use of such assumption in the proof of the Theorem either. Despite this, the claim of the theorem can be roughly stated, in the case of random graphs, as
 er^{0-1}[\hat{y}]= O( 1/m (n^{3/4} + \log{1/\delta} )^{1/2} ).
This, as far as the reviewer understands, means that if vertices of a large random graph are {0,1} labeled at random and then a diminishing fraction, e.g., \omega{n^{3/4}}, of these labels are observed the algorithm is guaranteed with high probability to find the remaining labels with small error. The reviewer was not able to understand the main result (see below) and can't make any judgment.