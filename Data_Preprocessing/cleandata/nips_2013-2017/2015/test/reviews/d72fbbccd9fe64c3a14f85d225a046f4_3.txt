The paper extends previous work on using deep Poisson Factor Analysis (PFA) for topic modeling by using a Bernoulli-Poisson link, instead of logistic functions. The paper also describes a way to jointly model documents and their associated discrete labels. Experiments show the proposed method outperform related baselines in held-out perplexities and classification accuracy.
The paper extends existing work (Gan et al, ICML'15 in particular) to provide a more flexible way to define the prior on documents' proportions over topics. Even though mostly a combination of existing ideas, I think the paper provides some advances in applying deep models for topic modeling. Here are some of my detailed comments:
- It is interesting to see that both MCMC and variational inference techniques are included. One of the arguments for using VI in the paper is its scalability. It would be interesting to see comparison on running time between the two inference techniques.
- Jointly capturing documents with their associated metadata is a well-studied problem. I am wondering why the performance of traditional supervised topic models such as sLDA (for classification) are not included for comparison in Section 5.
- For readers who are not familiar with conventional notations of deep models, I would suggest including some figure to illustrate the different layers and their input/output in Section 2.  The paper extends and improves previous work on using deep Poisson Factor Analysis for topic modeling. While mostly a combination of existing ideas, I think the paper provides some advances in applying deep models for topic modeling.