The paper considers regression problem with multiple vector valued outputs, that are related either via sharing a common noise vector, or the same coefficient vectors, or some other attributes. This is an interesting theoretical and practical question.
 The paper considers two popular models for such problems. The first is the pooled model where the coefficient vector is shared across the various outcomes. The second is the SUR model which shares the noise vector across the dimensions. In both of these problems it is possible to simply ignore the additional information that can be extracted and perform the ordinary least squares regression (OLS). It can be shown that MLE outperforms this procedure in these settings. However, MLE requires solving a non-convex problem. Therefore, the authors study alternating minimization, and under these models show that its performance is within universal constant factors of MLE.
 1. lines 190-200: Is the repeated use of fresh samples the reason for the additional logarithmic factor in theorem 1? It would be
interesting to see why the logarithmic factor pops up, since the lower bounds do not seem to have it.
2. It would be interesting to apply these results to some real world data.
3. The two models considered are in some sense restrictive. It would be nice if the authors can comment on
what the reason for Alt-Min performing well under these models is. In particular, are there generic conditions under which Alt-Min can be shown to be within universal constant factors of MLE for regression problems?
 The paper shows that under two popular models for regression problems with vector valued outputs, Alternating Minimization can match the performance of MLE estimation up to universal constants. This is an interesting result.