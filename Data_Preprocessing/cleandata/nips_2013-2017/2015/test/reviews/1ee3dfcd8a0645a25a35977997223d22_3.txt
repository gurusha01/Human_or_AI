The authors look of optimizing the orthonormal embedding of a graph and the resulting impact of learning functions on the graph.
The authors start off by reviewing material from prior work using a Laplacian and the corresponding generalization bound (eqn. 2). They then discuss their bound based on the maximum eigenvalue of the kernel matrix (eqn. 4). This bound leads to an alternate optimization criterion--equation (5); \lambda_1(K) is not differentiable, but the optimization criterion is convex (sum of convex functions).
The authors use inexact proximal methods for the solution of SEER.
In the experiments section, the authors apply SEER and MKL-SEER to multiple problems and show significant performance improvements.
Overall, this paper is very solid.
There's good theoretical justification.
Also, there are multiple intuitive leaps as the paper progresses that show cleverness in the approach--the new optimization criterion, the willingness to tackle the difficult optimization problem, and the satisfying application to real problems.
This paper is a nice advancement over the prior approaches.
 The authors look at the problem of finding an optimal embedding for graph transduction.They derivea novel method and demonstrate it works well in practice -- this paper is complete and well-explained.