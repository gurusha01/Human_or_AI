Learning Gaussian Mixture Models is a very well-studied problem in statistics and ML. Expectation Maximization (EM) algorithm from 30 years ago still remains the most popular algorithm. The current paper proposes to use manifold optimization (a reasonably well-established subarea of optimization) for this problem. Very briefly, think of the optimization problem where the variables are the parameters of the mixture model (i.e., the mean, covariance and weight for each component, where the number of components is fixed), and given these one can write down the expression for the (log-) likelihood of the given data. The most important constraint on the variables---and the main source of difficulty---is that the covariance matrices are positive semi-definite. One seeks to find parameters that maximize this. As the paper observes, direct use of standard manifold optimization is not effective as the algorithms tend to be too slow. But, as the paper shows, a simple re-parametrization of the optimization program plus judicious application of standard manifold optimization algorithms with some tweaking improves the performance greatly: In the reported experiments, their algorithm (the paper has several algorithms; I will focus on the best) converges in time that's comparable and often better than EM, and the final value of the objective functions is generally the same for the two algorithms. (Note that these methods may converge to local maxima, and the question of how far the parameters thus estimated can be from the true parameters is not discussed here.) It of course remains to be seen how widely this holds. But given the importance of GMM, these results are interesting.
 Minor: Line 129: "Problem (2.1) in general can require exponential time." This should be phrased more carefully: "Problem (2.1) in general can require number of samples that's exponential in K." I think the paper can be accepted on the strength of experimental results.