Summary: The paper proposes a novel approach to computationally efficient maximum likelihood learning in exponential families. In general, finding the maximum likelihood solution is intractable. From a convex optimization perspective, the sticking point is the need to calculate an integral wrt the currently proposed EF parameter. By assuming that MCMC is fast-mixing for all allowed parameters, the author(s) are able to show that the integrals needed for proximal gradient descent can be calculated with sufficient precision that, when combined with the results of Schmidt et al. (2011), a fully-polynomial randomized approximation scheme for calculating the MLE can be obtained. Both the convex and strongly convex cases are considered, which lead to different types of guarantees: the former on the likelihood error, the latter on the parameter error. A simple experiment demonstrating the theory is also included.
Overall, a high quality piece of work. The approach is well-motivated as an alternative to relying on small-treewidth assumptions. The theoretical analyses appear solid, though I haven't checked the proofs in detail. The proofs are essentially a matter of combining the MCMC mixing assumption with concentration inequalities and the results of Schmidt et al. The paper is thus a combination of existing techniques, though the combination is nontrivial.
 I appreciated the honesty with which the limitations of the approach were discussed. In particular, a thoughtful conclusion analyzed weaknesses of the approach, ways in which the theoretical assumptions are overly pessimistic, and directions for future work. The paper also offers some insight into CD-type algorithms, even though the algorithm that is analyzed is not quite contrastive divergence.
 The introduction was well-written with substantial references to previous work, but there were a few issues with the clarity of technical presentation. One small but important omission was simply the statement that the log-likelihood is in fact convex: the 2nd and 3rd terms in eq. 1 are clearly convex and the convexity of A(theta) follows from the fact that the hessian of A is the covariance of the sufficient statistics, and thus is PSD. While seemingly minor, the whole paper is premised on the the convexity of f(theta), and so discussing it will help guide the reader through the logic of the paper. A clear summary of the assumptions made in the paper. For example, one assumption never mentioned in the main paper is that the parameter set Theta must be convex.
 Overall, I found the paper to be an original and substantial contribution. The results to seem to be of limited practical value due to the fact that proving fast mixing of MCMC is extremely challenging. So in a sense the paper has replaced one hard problem (finding the MLE) with another (finding fast-mixing Markov chain). That said, being able to convert one problem into another is often useful, and thus having the results of the paper available to the community is valuable.
 Other comments: l185: ",w here" => ", where" l192: this paper that => this paper assumes that l207: simple example fast-mixing => simple example of a fast-mixing l281: of of => of l311-12: This sentence is nonsensical An novel approach to developing conditions under which finding the (approximate) MLE is tractable. Well-written, though of seemingly limited practical value.