This work addresses the local convergence properties of the EM algorithm in the high-dimensional setting where the number of dimensions is greater than the sample size. The paper builds on a recent work for the low-dimensional setting. The main novelty here is an iterative regularization scheme that shrinks the regularization parameter as the algorithm gets closer to the optimum. This allows the algorithm to converge at a linear rate in the local neighborhood of the optimum. There are several strong conditions on the objective function that the results rely on. As in most theoretical results on EM-type optimization, the algorithm also requires fresh samples at each step.
As this is a "light" review, I was unable to verify the details of this very technical contribution. If the results are correct, they represent a relevant and solid contribution to the theory of EM. The paper will primarily appeal to experts in this area.
 A solid (albeit very technical) contribution on the theory of EM for high-dimensional estimation.