This paper proves PAC bounds for control policies, and gives a learning algorithm for optimizing policies for these bounds.
This is an interesting and useful problem in robotics, as the authors are correct in stating that many robotics tasks have severe penalties for "worst-case" performance, as in the example given of UAV obstacle avoidance.
 Overall the mathematical formalism and readability of this paper are very good. The experiments are also interesting, as the authors connect largely theoretical work to (simulated) real-world problems.
My main concern with this paper is the lack of comparative evaluation vs. other approaches. For example, the authors cite obstacle avoidance rates improving significantly with a few iterations of their algorithm for the UAV case. It would be interesting to see how that rate compared to a more standard policy learning approach optimizing, for example, for expected reward rather than a probabilistic bound. Comparing mean performance for some metric would also be interesting, as I expect there's a tradeoff. This would significantly strengthen the paper by providing direct evidence for the authors' claims that optimizing PAC bounds helps to avoid cases like collisions.
Overall I think this is a good paper, but it could be much better with some additional evaluation as mentioned above. This paper is theoretically very strong and attacks an interesting problem for robotics and other control applications. More extensive evaluation as compared to existing methods would significantly strengthen it.