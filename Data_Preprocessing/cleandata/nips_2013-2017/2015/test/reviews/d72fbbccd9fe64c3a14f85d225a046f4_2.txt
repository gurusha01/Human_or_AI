The paper presents a multilayer model of count vectors using Poisson Factor Analysis at all layers (providing interpretable topics) and binary units connecting these layers (learning topic correlations). MCMC and SVI inference is straightforward -- all conditional posteriors are in closed form --
and inference scales in the number of non-zero observations and hidden units. The model is an incremental change from Zhou et al. (2012), removing the gobal beta-Bernoulli process and using the Bernoulli-Poisson link to avoid using sigmoid belief networks.
Both the model and inference described in the paper are elegant. The model is only an incremental improvement from prior work (notably Zhou et al. (2012)), but it's likely to be of significant interest to the community.
 1. The experimental analysis has been done with an eye towards comparing to other deep and one-layer models. However, there's hardly any effort in exploring the proposed model itself.
 a. how did you fix the layer widths? have you studied more than 2 layers?
 b. how does the model deal with overdispersion in data?
 c. how should the layer widths decay with depth?
 2. it's surprising that even your one-layer model does significantly better than ORSM and LDA (which is similar to PFA). is this due to your approach to discriminative topic models? there is no explanation provided.
3. why is ORSM not included in Table 1? The paper presents a tweak to existing deep network topic models (combining ideas from Zhou et al. (2012) and Zhou et al. (2015)) and shows how a hierarchy of Poisson Factor Analysis units can be connected using hidden binary units.Although an incremental contribution, both MCMC and variational inference are made much simpler due to local conjugacy and experimental results show superior performance. The paper is lacking in a experimental investigation into the network structure -- instead it seems to be arbitrarily fixed. It is still likely to be of much interest to the deep learning and topic modeling community.