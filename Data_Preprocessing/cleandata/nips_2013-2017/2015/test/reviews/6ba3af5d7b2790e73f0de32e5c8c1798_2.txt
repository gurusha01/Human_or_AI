This paper presents a system for action conditional prediction of video frames using deep convolutional and recurrent neural networks. The authors propose two deep models for encoding the video data using feedforward CNNs or a recurrent neural net, transforming the frame based on the applied action and decoding them back using up-convolution. The system is trained on four video games using the atari emulator and tested for video prediction and efficacy for control using the DQN algorithm.
Quality & Originality: ------------------------ The paper is sound and logical. The proposed deep architectures are similar to previous work, with the added action dependent transformation. The main novelty comes from the new multiplicative formulation of this transformation as compared to an additive fully connected layer. Most other parts including the training has been proposed in the literature. The system does achieve good results, being able to predict multiple frames into the future.
 The experiments compare and contrast the two variants against two other naive baselines which do not consider the actions. A more informative baseline would have been the additive fully connected action transformation layer as this takes into account the action's effect. The authors should try to compare against this.
Other experiments confirm the efficacy of the system for control, based on the DQN algorithm. The informed sampling for the DQN also improves on state-of the art results for the DQN, but that is to be expected given a decent enough generative model of the emulator. A good sanity check there would be to use the emulator as the generative model and compare the scores w.r.t the results from the learned model. Additionally, the actions separate well across the learned representations.
Clarity: --------- The paper is well written and clear. The figures need to be improved (especially Fig 5.a) where it is hard to see any difference between the two methods) and caption text should be added below the figures to make it easier for the reader to gain context.
 Significance: --------------- The paper proposes a new method for learning representations for video prediction conditioned on the actions. The paper builds on the use of deep auto-encoders for learning representations for control and is relevant to the NIPS community. There has been some prior work on action-based image data prediction in robotics that the authors may wish to cite: Boots et al., Learning Predictive Models of a Depth Camera & Manipulator from Raw Execution Traces, ICRA 2014 - This paper also does action-dependent prediction of kinect data (640 x 480), but over much shorter timeframes and simpler environments.
Overall, the paper is concise and the ideas are well presented. I feel that the work is a bit incremental with not a significant theoretical contribution. Adding more experimental results with better baselines should improve the paper as a whole. I vote for a borderline accept.
 This paper presents two deep-architectures for predicting sequences of video frames of the Atari emulator conditioned on the agent's actions and previous frames. The system is tested on a few Atari games and is used with the DQN algorithm to evaluate its efficacy for control. The paper is well written with a few experimental results, but comes across as incremental work. I vote for a borderline accept.