This manuscript presents two post-processing methods for converting the score-valued output of a binary classifier into an estimated class probability. 
While the proposed techniques are straightforward and efficient, and the concept is worthy of publication in theory, the current draft has several significant flaws that need to be addressed. These issues are outlined below.
One major concern is that the distinction between the contribution of the paper and the background information remains unclear until the conclusion.
The content of Section 2 appears to be relatively straightforward, particularly Proposition 2, which can be achieved through simple sorting and binary search. However, this is presented as one of the primary contributions of the paper.
A critical issue is the setting of hyperparameters for the learning machines, which was done by minimizing the training error. This approach can easily lead to overfitting, especially with models like Gaussian kernel SVMs that can achieve zero training error. As a result, all the experimental results become meaningless, which is a significant reason for rejection.
Although the new methods consistently outperform the baseline methods, the effect size is relatively small. It is unclear whether these new methods are relevant, especially considering they require more space and time than Platt's method. A discussion on the practical significance of these methods is lacking, and it is essential to demonstrate that these methods have real-world applications for the paper to be considered for acceptance.
Some minor comments include:
- The paper claims that Platt's method is invariant with respect to the conversion of scores to the unit interval, which is not accurate. It is also unclear whether this conversion is actually performed when using Platt's method.
- The paper mentions that many models output values in the unit interval, which could be interpreted as probabilities. However, this point is not explored further in the experimental evaluation, and it would be beneficial to discuss why this was not pursued.
- In Section 5.2, the paper states that there is no simple ad-hoc regularization for isotonic regression. However, it seems that Platt's regularization technique could be applied by adding virtual scores of positive and negative infinity.
- The title of the paper includes the term "Large-scale," but the content does not reflect this, and there is no apparent connection to the processing of large-scale data.
- The plots in Figure 1 are too small to be legible in a standard black and white printout.
Overall, due to the numerous weaknesses in the paper, it is not suitable for publication in its current form.