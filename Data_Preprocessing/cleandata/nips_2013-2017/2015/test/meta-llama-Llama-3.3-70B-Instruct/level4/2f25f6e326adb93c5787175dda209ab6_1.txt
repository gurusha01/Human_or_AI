Determinantal point processes (DPPs) have garnered substantial attention in the machine learning community due to their capability to capture repulsion between points in a set. This repulsion is defined by a positive definite kernel function, which poses a challenge in learning its parameters from example point realizations. The estimation of these parameters is complicated by a normalization constant that is difficult to compute, as it involves summing over all possible point configurations and cardinalities.
A recent study [7] has proposed a method to address this issue by estimating the spectrum and utilizing it to establish incremental upper and lower bounds, resulting in a provably correct scheme for retrospective Markov chain Monte Carlo (MCMC) inference of kernel parameters. In contrast, the present paper introduces an alternative type of bound that facilitates MCMC inference without requiring eigenvalue estimation. This approach also enables a variational method for learning and draws connections to inducing-point methods for scalable inference in Gaussian process models.
The paper is technically sound and well-written, presenting a novel inequality that provides a lower bound in Proposition 1, which seems to be an excellent fit for this problem. The connection to Gaussian process inducing points and the ability to perform variational inference are notable strengths. However, two technical concerns need to be addressed. Firstly, replacing power iterations with nonlinear optimization problems of increasing dimension may not necessarily result in significant computational gains. Secondly, it is unclear whether simply increasing the cardinality guarantees convergence of the bounds due to the coupled optimization problem, which may be highly non-convex and require achieving a global minimum to ensure the procedure's effectiveness.
Some presentation issues were also noted, including the absence of axis labels in Figure 1 on page 7, and the proof of the proposition being relegated to the appendix instead of being included in the main body of the paper. Additionally, a minor clarification is needed on line 361 of page 7, where the phrase "the variational lower is" could be rephrased for better readability. Overall, the paper presents a clever approach to bounding the partition function of a DPP, potentially leading to faster inference and connections to other models.