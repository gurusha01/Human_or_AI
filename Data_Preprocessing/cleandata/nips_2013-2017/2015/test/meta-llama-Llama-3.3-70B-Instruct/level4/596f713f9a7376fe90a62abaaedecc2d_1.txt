The core concept presented is that Gaussian mixture models (GMMs) are commonly used as priors for modeling natural image patches, but a computationally intensive step in inference algorithms involves calculating posterior mixture assignments, which entails determining the likelihood of a patch belonging to each component. This process typically requires calculating the norm of the patch with respect to each component's covariance matrix, a costly operation.
The authors propose an alternative approach, where a smaller neural network, termed a gating network, with a single hidden layer containing a relatively small number of units, is trained to compute these assignments. The network's weights are optimized to minimize the Kullback-Leibler divergence with respect to the true posterior distribution for a set of training patches.
In essence, the authors suggest training a classifier to predict component assignments, demonstrating that this discriminatively trained classifier can replicate the behavior of an explicitly computed posterior from a generative model but with significantly reduced computations. Furthermore, since this approach can be seen as a "drop-in" replacement for a computation step related to the prior, it can be trained once and applied to different tasks with varying likelihood terms without requiring retraining. This flexibility gives the method an advantage over end-to-end discriminative training, which necessitates retraining for each new task.
The paper presents a novel and intriguing idea that is well-executed and reasonably evaluated, showing potential for applications beyond image restoration. However, to further strengthen the manuscript, the authors should address the following points:
1. The current description of computational complexity in terms of dot products is somewhat confusing. For instance, in the traditional GMM case, the first layer computes d * K dot-products, while the second layer computes K dot-products, with the vector lengths being equal for both layers. In contrast, the proposed gating network computes N dot-products of length d in the first layer and K dot-products of length N in the second. Clarification is needed, particularly when discussing setting the number of dot-products to 100, as it is unclear whether this refers to the total or just those in the first layer. A clearer discussion in terms of multiplies, adds, and the explicit number of neurons in the hidden layer would be beneficial.
2. An evaluation of the method's performance, such as in denoising tasks, using different numbers of mixture components would be valuable. It would be interesting to explore how the number of neurons in the hidden layer needs to increase with the number of mixture components and how this approach affects the accuracy-time trade-off in choosing the number of mixture components. Overall, the paper introduces a promising, computationally efficient alternative for computing posterior mixture-component assignments during inference in image restoration tasks using GMM priors.