This manuscript introduces a framework for predicting video frames conditioned on actions, leveraging deep convolutional and recurrent neural networks. The authors propose two distinct models for encoding video data, utilizing either feedforward CNNs or a recurrent neural network, and then apply an action-dependent transformation to the frames before decoding them via up-convolution. The system is trained on a dataset of four video games using the Atari emulator and evaluated for both video prediction accuracy and control efficacy using the DQN algorithm.
Quality & Originality: 
The manuscript presents a logical and well-structured approach. Although the proposed deep architectures bear similarities to prior work, the novelty lies in the introduction of a multiplicative formulation for the action-dependent transformation, differing from the conventional additive fully connected layer. While the system demonstrates good performance in predicting multiple future frames, most components, including the training methodology, have been previously explored in the literature. 
The experimental section provides a comparative analysis of the two proposed variants against naive baselines that disregard actions. However, a more comprehensive evaluation would involve comparing against a baseline that incorporates an additive fully connected action transformation layer, as this would account for the action's impact. 
Additional experiments validate the system's efficacy for control, leveraging the DQN algorithm, and demonstrate improved state-of-the-art results for informed sampling with the DQN. A suggested sanity check involves using the emulator as a generative model and comparing the scores to those obtained from the learned model. Furthermore, the actions are well-separated across the learned representations.
Clarity: 
The manuscript is well-written and clear. However, the figures require improvement, particularly Figure 5.a, where the differences between the methods are not readily apparent. Adding caption text below the figures would enhance reader comprehension by providing context.
Significance: 
The manuscript proposes a novel approach for learning action-conditioned representations for video prediction. Building upon the use of deep auto-encoders for control representation learning, the work is relevant to the NIPS community. The authors may wish to cite prior research on action-based image data prediction in robotics, such as Boots et al.'s work on learning predictive models of a depth camera and manipulator from raw execution traces (ICRA 2014), which also explores action-dependent prediction, albeit over shorter timeframes and simpler environments.
Overall, the manuscript is concise, and the ideas are well-presented. While the work is somewhat incremental, with limited significant theoretical contributions, the addition of more comprehensive experimental results and robust baselines would enhance the manuscript's quality. Therefore, I recommend a borderline accept.
This manuscript presents a system for action-conditional video frame prediction using deep neural networks and evaluates its efficacy for control using the DQN algorithm. Although well-written, the work appears incremental, with a need for more extensive experimental validation and stronger baselines. Consequently, I vote for a borderline accept.