This study examines the local convergence characteristics of the EM algorithm in high-dimensional scenarios, where the dimensionality exceeds the sample size, extending a recent investigation focused on low-dimensional settings. The key innovation lies in an iterative regularization approach that adaptively adjusts the regularization parameter as the algorithm approaches the optimal solution, facilitating linear convergence rates within the local vicinity of the optimum. However, the findings are contingent upon stringent conditions imposed on the objective function, a common trait in theoretical analyses of EM-type optimizations, which also necessitate the availability of new samples at each iterative step.
Given the brevity of this review, an in-depth verification of the technical aspects of this contribution was not feasible. Nonetheless, if the results are accurate, they constitute a significant and substantial addition to the theoretical framework of EM algorithms. The paper's appeal is likely to be strongest among specialists in this field, given its technical nature. Overall, it presents a robust, albeit highly specialized, contribution to the theory of EM in the context of high-dimensional estimation.