The problem of learning Gaussian Mixture Models (GMMs) is a longstanding and well-examined challenge in the fields of statistics and machine learning, with the Expectation Maximization (EM) algorithm remaining a dominant approach since its introduction three decades ago. This paper proposes an alternative method, leveraging manifold optimization, a reasonably established subfield of optimization, to tackle this problem. The optimization task involves parameters of the mixture model, including mean, covariance, and weight for each component, with a fixed number of components. The log-likelihood of the given data can be expressed using these parameters, with the key constraint being that the covariance matrices must be positive semi-definite. The goal is to maximize this log-likelihood. However, as noted in the paper, direct application of standard manifold optimization techniques is ineffective due to their slow performance. Nevertheless, the authors demonstrate that a simple re-parametrization of the optimization problem, combined with the judicious application of standard manifold optimization algorithms and some adjustments, significantly improves performance. In the experiments reported, their best-performing algorithm converges in a time comparable to, and often better than, EM, with the final objective function value generally matching that of EM. Although these methods may converge to local maxima, and the distance between the estimated parameters and the true parameters is not addressed, the results are noteworthy given the importance of GMMs. The broader applicability of these findings remains to be seen, but they are certainly interesting. A minor suggestion is to rephrase the statement in Line 129 to "Problem (2.1) in general can require a number of samples that's exponential in K" for greater accuracy. Based on the experimental results, the paper is worthy of acceptance.