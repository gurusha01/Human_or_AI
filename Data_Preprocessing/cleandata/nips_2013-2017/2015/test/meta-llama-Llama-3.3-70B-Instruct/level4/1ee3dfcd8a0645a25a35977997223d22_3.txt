The authors investigate the optimization of orthonormal graph embeddings and its subsequent effect on learning functions defined on the graph.
The paper commences with a review of existing research utilizing Laplacian-based methods and the associated generalization bound (equation 2), followed by the introduction of a novel bound based on the maximum eigenvalue of the kernel matrix (equation 4). This bound yields an alternative optimization criterion, as expressed in equation (5), where although \lambda_1(K) is non-differentiable, the overall optimization criterion remains convex due to being a sum of convex functions.
To solve the resulting optimization problem, known as SEER, the authors employ inexact proximal methods.
In the experimental section, the authors evaluate the performance of SEER and its variant, MKL-SEER, on multiple problems, demonstrating notable improvements in performance.
Overall, the paper presents a robust contribution, backed by solid theoretical foundations and exhibiting clever methodological advancements, including the derivation of a new optimization criterion and the successful tackling of a challenging optimization problem, ultimately leading to effective applications in real-world scenarios.
This work offers a significant improvement over preceding approaches, presenting a comprehensive and well-articulated method for optimal graph embedding in the context of graph transduction, supported by both theoretical justification and empirical evidence of its efficacy.