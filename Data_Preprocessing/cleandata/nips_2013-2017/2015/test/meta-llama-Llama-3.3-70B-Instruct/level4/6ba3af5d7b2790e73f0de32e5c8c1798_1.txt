This paper tackles the complex task of learning a predictive model for Atari 2600 games, specifically forecasting future frames based on action inputs, which is a crucial component in developing more sophisticated controllers. The problem's solution has significant implications for reinforcement learning algorithms.
The paper's clarity and organization are commendable, with experiments (including videos) that effectively demonstrate the model's capabilities. The proposed model architecture consists of a convolutional neural network (CNN) with a fully-connected layer, followed by multiplicative interactions with an action vector, and subsequent convolutional decoding layers. A recurrent variant of the model incorporates an additional long short-term memory (LSTM) layer after the CNN.
The authors employ a dual evaluation approach, assessing their models based on both pixel accuracy (a traditional metric) and their usefulness in control tasks (the primary concern). However, it would be beneficial to provide more detailed information regarding the network architecture, particularly the deconvolution aspect, and the training procedure. Making the code available or including additional details in the main text or supplementary materials would be ideal.
Several aspects warrant further consideration: 
- The omission of reward prediction alongside frame prediction is notable, as it would have enabled the use of the model for planning purposes, such as using Upper Confidence bound applied to Trees (UCT), rather than relying on a trained model-free controller to evaluate control usefulness.
- The baseline models used for comparison are somewhat weak, although this is understandable given the lack of obvious candidates for comparison.
- The exploration section raises questions about whether the predictive model is learned online to facilitate exploration or if it is learned using data from a regular Deep Q-Network (DQN) with uninformed exploration and then used to guide the exploration of a new controller. If the latter, the benefits of this approach are unclear, especially since exploration has already been conducted to obtain the model.
- The distinction between controlled and uncontrolled dynamics discussed at the end is intriguing.
- The authors may find it useful to examine the recent work "DeepMPC: Learning Deep Latent Features for Model Predictive Control," which focuses on learning deep predictive models (also utilizing multiplicative interactions with actions) for control, albeit not in the visual domain.
Minor corrections and typos include:
- The statement regarding object appearance in Seaquest being random may not be entirely accurate.
- A typo in line 430 ("predicing").
Upon reviewing the rebuttal and considering other recent papers that learn deep dynamical models from images (though not specifically for Atari games), such as "From Pixels to Torques: Policy Learning with Deep Dynamical Models" and "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images," this paper remains a valuable contribution. It presents a well-written and convincing exploration of learning the dynamics of Atari games from data, making it a notable work in the field.