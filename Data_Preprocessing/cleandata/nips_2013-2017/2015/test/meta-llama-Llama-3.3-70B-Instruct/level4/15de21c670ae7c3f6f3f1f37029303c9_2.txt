This paper presents a novel methodology for efficient maximum likelihood learning in exponential families through a computational approach. Typically, obtaining the maximum likelihood solution is intractable due to the need for integral calculations with respect to the proposed exponential family parameter, a challenge from a convex optimization standpoint. However, by leveraging the assumption that Markov Chain Monte Carlo (MCMC) exhibits fast-mixing behavior for all permissible parameters, the authors demonstrate that the necessary integrals for proximal gradient descent can be computed with sufficient precision. Combining this with the findings of Schmidt et al. (2011), they develop a fully-polynomial randomized approximation scheme for calculating the Maximum Likelihood Estimate (MLE). The analysis encompasses both convex and strongly convex cases, yielding distinct guarantees - likelihood error for the former and parameter error for the latter. A straightforward experimental demonstration of the theoretical framework is also provided.
Overall, this is a high-quality contribution. The approach is well-motivated, particularly as an alternative to relying on assumptions of small treewidth. The theoretical analyses appear robust, although a detailed examination of the proofs was not conducted. Essentially, the proofs involve combining the MCMC mixing assumption with concentration inequalities and the results from Schmidt et al., indicating that the paper effectively combines existing techniques in a non-trivial manner.
The authors' candor in discussing the limitations of their approach is commendable. The conclusion thoughtfully examines the weaknesses of the method, the overly pessimistic nature of the theoretical assumptions, and potential avenues for future research. Furthermore, the paper offers insights into contrastive divergence-type algorithms, even though the analyzed algorithm is not directly contrastive divergence.
The introduction is well-crafted, with substantial references to prior work. However, there were some issues with the clarity of technical presentation. Notably, the convexity of the log-likelihood, a premise of the entire paper, could be more explicitly stated. The convexity of the terms in equation 1 and the positive semi-definiteness (PSD) of the Hessian of A(theta), which follows from it being the covariance of the sufficient statistics, support this convexity. A clear summary of the paper's assumptions, such as the convexity of the parameter set Theta, would also enhance readability.
In conclusion, this paper represents an original and substantial contribution to the field. While the practical applicability of the results may be limited due to the challenge of proving fast mixing of MCMC, the ability to transform one hard problem into another can be valuable. Thus, making these results available to the community is beneficial.
Additional comments include minor corrections: on line 185, ",w here" should be ", where"; on line 192, "this paper that" should be "this paper assumes that"; on line 207, "simple example fast-mixing" should be "simple example of a fast-mixing"; on line 281, "of of" is redundant and should be "of"; and lines 311-12 contain a nonsensical sentence. The paper presents an innovative approach to establishing conditions under which finding the approximate MLE becomes tractable, is well-written, but appears to have limited practical value.