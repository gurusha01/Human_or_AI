In the context of the EM algorithm for high-dimensional settings, the M-step poses significant challenges, necessitating the exploitation of the structural properties of high-dimensional parameters. This paper explores the utilization of decomposable regularizers to address this issue. The primary contribution lies in the development of a methodology for setting the regularizer within the iterative EM process, demonstrating that this approach yields local linear convergence under specific conditions related to population and empirical log-likelihood estimation based on current parameter estimates, a fundamental quantity in EM.
The EM algorithm is widely applied in machine learning, making it crucial to establish conditions that guarantee its performance. This paper provides a foundational approach to regularizing EM, particularly pertinent in high-dimensional settings. It also illustrates the theoretical application to three notable models, resulting in interesting and significant findings.
Regarding the abstract, the authors assert that regularizing the M-step using state-of-the-art methods, as referenced in [19], does not ensure the desired bounds, although further clarification would be beneficial.
Condition 5, as acknowledged by the authors, replaces a term found in previous research. It appears that this condition is pivotal in enabling the algorithm's effectiveness in high-dimensional regions, while the analysis aligns with preceding work.
On a minor note, several corrections are suggested: "Y Z" should be in bold face at line 80, "f" is omitted in Equation (2.2), and the notation "| |_R*" at line 122 is introduced without prior definition, causing slight confusion. This manuscript examines the regularized EM algorithm, proposing a regularizer setting method and identifying conditions for local linear convergence through regularization.