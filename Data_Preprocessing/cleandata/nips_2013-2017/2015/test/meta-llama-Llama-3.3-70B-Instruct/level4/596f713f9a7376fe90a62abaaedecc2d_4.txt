The paper presents a fast approximate inference approach for MAP estimation using the generative EPLL method, leveraging discriminative training to replace the computationally expensive update step of latent variables during alternating optimization. The experimental results demonstrate that this approximate inference procedure achieves a significant speedup of 1-2 orders of magnitude while only causing a modest drop in performance compared to the previously used "exact" EPLL inference.
I strongly agree with the paper's objective of retaining the benefits of generative approaches, as discussed in the introduction. Although the paper does not propose a generic method, instead focusing on a specific problem of inference with the EPLL method, it is an intriguing approach to utilize discriminative methods to approximate expensive steps during generative inference. It would be interesting to explore how this concept can be applied more broadly.
The finding that EPLL yields similar performance with the proposed approximate inference procedure compared to the "exact" one is notable, particularly since EPLL achieves state-of-the-art results and can now be applied to large images with the proposed inference method, as evident in the impressive results shown in Figure 7. The paper could emphasize this aspect more.
One puzzling aspect is why EPLL appears to be the only generative approach competitive with discriminative methods, which warrants further investigation.
However, I believe the paper overstates its contributions. The introduction promises a general method to combine generative and discriminative approaches, but the paper does not deliver a "deep" integration between the two. Instead, it requires standard generative training of the image prior, followed by training a discriminative predictor to approximate expensive steps of generative inference. The paper addresses a specific problem for performing MAP estimation with the EPLL method, which may not be easily applicable beyond this context. Other generative approaches with filter-based image priors do not have the issue of computationally expensive latent variable updates.
Additionally, the discussion of discriminative and generative approaches starting on line 111 lacks nuance, failing to consider aspects such as the ability of generative approaches to use different loss functions at test time or perform marginalization to handle unknown random variables.
In terms of clarity, the explanation of the gating network in Section 3 is difficult to understand and requires revision. Table 1 does not specify the test data used for the denoising results, and it is unclear if these are different test sets for each paper. There are also several typos, including one in Equation 7, where the weight $wi^j$ should be $wi^k$ in the denominator, and another in line 102, where "Hel-Or used" should be "Hel-Or and Shaked used". Figure 2 is unclear due to the lack of labels for the different parts of the figure, and there appears to be a missing connection in the gating network on the right side. The title "return of gating network" is not appropriate, and the title "combining generative models and discriminative training in natural image priors" suggests a generic method that is not actually proposed.
It is worth noting that "modularity" can also be achieved with discriminative approaches by training with a likelihood component that can later be replaced, as demonstrated by Chen et al. in their work on loss-specific training of filter-based MRFs for image restoration. Furthermore, EPLL inference does not actually use half-quadratic splitting, but rather a quadratic penalty method for approximate constrained optimization.
Despite some limitations, the paper makes an important contribution by providing a generative image restoration approach that is competitive with state-of-the-art discriminative methods in terms of both restoration quality and speed of inference.