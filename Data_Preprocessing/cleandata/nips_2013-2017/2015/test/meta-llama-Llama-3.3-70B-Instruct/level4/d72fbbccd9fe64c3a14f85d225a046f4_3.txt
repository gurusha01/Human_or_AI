This paper builds upon prior research on deep Poisson Factor Analysis (PFA) for topic modeling, introducing a Bernoulli-Poisson link as an alternative to logistic functions. Additionally, it presents a method for jointly modeling documents and their associated discrete labels, demonstrating superior performance to related baselines in terms of held-out perplexities and classification accuracy.
The work expands on existing studies, such as Gan et al (ICML'15), by offering a more flexible approach to defining the prior on documents' topic proportions. Although the paper primarily combines existing concepts, it contributes to the advancement of deep models in topic modeling. The following points outline my detailed comments:
- The inclusion of both MCMC and variational inference techniques is noteworthy. Given that scalability is cited as a reason for using variational inference, a comparison of the running times between the two inference methods would be informative.
- The problem of jointly capturing documents and their associated metadata is well-established. It is surprising that the performance of traditional supervised topic models, such as sLDA for classification, is not compared in Section 5.
- To facilitate understanding for readers unfamiliar with conventional deep model notations, I recommend adding a figure to Section 2 to illustrate the different layers and their input/output. Overall, the paper extends and enhances previous work on deep Poisson Factor Analysis for topic modeling, offering some advancements in the application of deep models to this field, despite being largely a combination of existing ideas.