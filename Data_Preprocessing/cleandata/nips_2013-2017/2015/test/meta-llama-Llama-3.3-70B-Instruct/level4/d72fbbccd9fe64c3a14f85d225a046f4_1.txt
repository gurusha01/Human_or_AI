This paper presents a novel probabilistic deep architecture for modeling count data, along with an extension for multi-class classification problems, specifically discriminative modeling. The authors develop two inference techniques, a sampling-based approach and a variational inference method, demonstrating empirical improvements over unsupervised methods on several large-scale datasets.
Questions/Comments: 
- The model is well-explained and motivated, with Section 2.3 being particularly engaging. Adding a figure to illustrate Equations 3 could enhance clarity.
- The implication that [23] does not scale with the number of zeros may be misleading, as using a Poisson likelihood allows all methods to effectively ignore zeros. The computational gains seem to stem more from this aspect than from higher layers, which are typically smaller. Additionally, [23] does support non-linear activation functions.
- The model appears to scale well computationally. Providing insight into how computation scales and the time required to learn from larger datasets would be valuable. A rough comparison to competing models, such as docNADE, LDA, and replicated softmax, would also be beneficial.
- The comparison of classification experiment results to only unsupervised techniques may not be entirely fair. Including at least one simple supervised baseline, such as a small neural network with a softmax output and word frequencies as inputs, would strengthen the comparison.
Other comments: 
- Line 165 notes the sparsity of the Dirichlet relies on the choice of parameters (eta), which should be clarified.
- Overall, this is a strong paper that contributes to our understanding of deep generative models, with a notable discriminative extension. The empirical results are solid, and the model's scalability is a significant advantage.