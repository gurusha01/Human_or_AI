A novel framework is presented for regularized Expectation-Maximization (EM) algorithms, which is thoroughly examined. The primary motivation behind incorporating regularization is to facilitate estimation in high-dimensional environments, where structural assumptions apply, and unconstrained estimation is statistically unfeasible, and standard EM algorithms may not be well-defined. A crucial component of this framework is the regularized EM algorithm, wherein the conventional M-step is modified to maximize the Q-function minus a regularizer applied to the model parameter, scaled by a regularization coefficient.
This study builds upon and refines earlier research in this domain. The key innovations of this paper are twofold. Firstly, whereas previous studies were limited to sparsity as the structural assumption, this work introduces more general regularizers that can be applied to both sparse and low-rank structures. Consequently, the assumptions regarding the statistical properties of the Q-function estimator have been adapted to accommodate this broader regularization framework. Secondly, the authors propose a specific method for selecting the regularization coefficient, which is updated at each iteration and effectively balances the optimization error with the statistical estimation error.
To enhance clarity, it would be beneficial to include a discussion or provide examples illustrating the decomposability condition, the subspace compatibility constant, the norm ||_{R^*} utilized in the pivotal statistical condition 5, and the set C(S,\bar S, R). This contribution significantly expands and refines existing research on EM algorithms in high-dimensional settings under structural assumptions. The new findings have substantial implications for the practical applicability of the theoretical framework.