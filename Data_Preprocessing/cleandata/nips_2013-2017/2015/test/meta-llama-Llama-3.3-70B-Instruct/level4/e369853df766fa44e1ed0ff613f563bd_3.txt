This paper presents a novel online algorithm for constructing a multiclass classifier, achieving a time complexity of logarithmic order with respect to the number of classes, denoted as k. The approach involves the online construction of a decision tree that locally maximizes a newly introduced objective function. This function assesses the quality of the tree based on a combined score of "balancedness" and "purity". The authors provide a theoretical analysis of the algorithm, leveraging a boosting argument under the assumption of weak learnability, effectively extending the work of Kearns and Mansour (1996) to the multiclass setting. Although the theoretical analysis pertains to an algorithm that may be computationally intractable, a concrete, albeit simplified, algorithm is proposed for a relaxed version of the problem. This simplified algorithm, while lacking theoretical guarantees, is noteworthy for its simplicity, naturalness, and interest.
The paper is well-written, engaging, and appears to be original. The theoretical underpinnings seem sound, and the experimental results indicate an improvement over other classifiers with logarithmic time complexity, albeit falling short of achieving state-of-the-art generalization performance. This work can be viewed as a incremental step towards achieving enhanced performance in this area.
However, several concerns warrant attention:
- The claim that the most efficient accurate approach has a time complexity of log(k) may be misleading, as the 1-nearest neighbour classifier operates with train and test times independent of k, given that reading log(k) bits is considered an O(1) operation within the time complexity analysis framework. The information theoretic arguments presented in the introduction do not seem directly relevant to the algorithm's time complexity analysis. Clarification on this point would be beneficial.
- Upon closer examination, it appears that the LOMtree algorithm may not fully maximize the relaxed objective function outlined in line 263. This discrepancy arises because the estimates of expectations (such as mv(y), Ev) are not updated when retrained hypotheses at higher levels of the tree alter their decisions on certain samples. Given that the statistics in Algorithm 1 are additive, such changes are not reflected in the estimates, suggesting the need for a more complex mechanism (like message passing) for accurate tracking. The use of "open-loop" statistics, which do not account for these changes, could lead to erratic behavior in the algorithm. Further clarification on this issue is necessary.
- A potential concern is that as one navigates down the tree, the classifiers may be destined for lower accuracy due to the tree's balanced nature, which results in lower nodes having significantly fewer samples for training. This could imply that an exponentially large number of samples would be required to achieve good generalization accuracy. Despite these concerns, the paper proposes an online multiclass classifier with sufficient novelty and interest to warrant publication.