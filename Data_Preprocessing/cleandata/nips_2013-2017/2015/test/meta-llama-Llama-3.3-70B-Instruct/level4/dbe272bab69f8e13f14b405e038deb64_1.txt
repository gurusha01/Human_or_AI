This manuscript re-examines the use of gradient methods for fitting mixtures of Gaussian distributions. By leveraging manifold optimization, incorporating a specific line search procedure, and utilizing a reformulated objective function, the approach demonstrated empirical superiority over batch Expectation-Maximization (EM) in the reported experiments.
Although I am not an expert in Riemannian manifolds, the methodology presented appears to be sound. The authors have provided a clear and comprehensible explanation of their work, allowing readers to follow the discussion by taking cited references at face value.
Gaussian Mixture Models (GMMs) may seem unremarkable in the current era of Bayesian nonparametrics and deep neural networks. However, they remain a crucial and potent tool in various applications, often serving as a foundation for further extensions, such as incorporating constraints or tying covariances together. In these cases, EM is no longer applicable, whereas gradient-based procedures can be readily generalized, making them a valuable contribution to statistical applications.
In the past, I would have employed an unconstrained parameterization based on the Cholesky decomposition. While the authors provide arguments against this approach, it is unfortunate that it was not included in the empirical comparison. Questions remain regarding the practical implications of spurious stationary points and whether the manifold optimizer offers significant advantages. Furthermore, it is unclear whether the primary benefit stems from the representation in equation (2.3) and whether this representation could also enhance the Cholesky approach. Addressing these questions would strengthen the manuscript.
Minor suggestions include:
* Line 081: Replace "open a new" with "open new"
 Line 138: Change "rely on* geodesics" to "rely on geodesics"
* Presenting the timings in Table 4 as a graph would facilitate a quicker comparison, as precise times are implementation- and machine-dependent.
I recommend revising lines 147-149 to: "Geodesics on the manifold from $\Sigma1$ to $\Sigma2$ are given by [4]:". This rephrasing avoids unnecessary explanation of Riemannian metrics and mitigates potential misinterpretation.
Additionally, I suggest explicitly including the zero-mean constraint ("0,") in the normal density equation (2.3) and the subsequent line. Initially, augmenting the data with a constant appears problematic, but the zero-mean constraint prevents this issue. The compact Gaussian notation should also be defined explicitly, including the mean term, as in equation (2.1).
The transformed problem in equation (2.3) should not be referred to as a "reparameterization," as it represents a distinct model that generates data from a different distribution than the original.
I propose numbering all equations to facilitate referencing in reviews, reading groups, and citations. The novel gradient-based procedure for fitting GMMs presented in this manuscript appears to outperform EM and has the potential to be highly useful in applications. Although some claims require further clarification, I am interested in this work and have increased my score in response to the rebuttal, which promises additional clarification and control experiments to compare this approach to the Cholesky method. I believe this will be an interesting and valuable paper.