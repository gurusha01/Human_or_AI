This paper explores the problem of vertex label learning in graphs, building upon the work of Ando and Zhang, who demonstrated that embedding graphs on a unit sphere via orthonormal representation improves generalization error. However, Ando and Zhang did not provide a concrete method for choosing an optimal embedding. The current paper proposes a novel optimization formulation that jointly optimizes the embedding and learned labels by introducing a regularization term, specifically the spectral norm of the graph kernel matrix, based on the insight from Theorem 1, which establishes an upper bound on generalization error. The paper further derives an asymptotic upper bound on the generalization error of the proposed method, exhibiting favorable scaling compared to previous results, and translates this bound into an upper bound for labeled sample complexity. Although the proposed optimization can be formulated as an SDP, its complexity is undesirable, leading the authors to suggest using an inexact proximal method, which can scale to graphs with thousands of vertices. Experimental results indicate the superiority of the proposed method over previous approaches.
However, the reviewer struggled to comprehend the main result, Theorem 4, at a high level. The paper states that, in the presence of unlabeled nodes and without assumptions on the data, learning labels is impossible, and follows existing literature in assuming that edges link similar instances. Nonetheless, the definition of "similar" is not provided, and this assumption is not mentioned in the theorem's assumptions or utilized in its proof. Despite this, the theorem's claim can be roughly interpreted as stating that, for random graphs, the error rate of the learned labels is bounded by $O(1/m (n^{3/4} + \log{1/\delta})^{1/2})$. This implies that, for a large random graph with randomly assigned {0,1} labels, observing a diminishing fraction of these labels (e.g., $\omega{n^{3/4}}$) allows the algorithm to recover the remaining labels with high probability and small error. Unfortunately, the reviewer was unable to fully understand the main result and therefore cannot make a judgment.