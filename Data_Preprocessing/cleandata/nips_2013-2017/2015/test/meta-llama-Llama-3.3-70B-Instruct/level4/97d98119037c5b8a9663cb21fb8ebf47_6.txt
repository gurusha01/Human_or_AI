This paper, Sample Complexity Bounds for Iterative Stochastic Policy Optimization, introduces a methodology for establishing bounds on the execution cost of a policy based on the execution costs of a preceding policy. The authors compute and compare these bounds to empirical estimates in both a simple robot domain and an aerial robot domain, finding that the bounds for estimated policy costs and crash probabilities closely align with empirical results.
The paper's quality is commendable, with clear explanations that facilitate understanding. However, an exploration of how varying the initial policy affects the convergence of expected costs would have enhanced the study. The work appears to be original, although demonstrating its applicability to more conventional problems, such as grid world or cart pole, would have provided a more comprehensive basis for comparison. Achieving PAC-like bounds directly would significantly bolster the work's impact.
The research focuses on calculating bounds for the robustness of decision-making in the context of trajectory learning, enabling the comparison of policy learning algorithms' performance or the currently learned policy with respect to policy costs. This concept is particularly pertinent in domains where empirical policy testing is costly, highlighting the importance of this line of inquiry.