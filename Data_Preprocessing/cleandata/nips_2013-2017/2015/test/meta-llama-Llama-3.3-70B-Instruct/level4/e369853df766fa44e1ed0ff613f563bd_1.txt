Summary:
This paper addresses multi-class problems with a large number of classes by introducing a novel label tree classifier, which achieves learning and prediction in logarithmic time. The authors provide theoretical guarantees through a boosting-like theorem and demonstrate that both node classifiers and the tree structure can be trained online. Additionally, they propose a subtree swapping procedure to ensure proper tree balancing. Empirical results demonstrate the attractiveness of this approach.
Quality:
The paper presents a strong combination of theoretical and empirical results, with a clearly derived and theoretically justified method. Some minor comments are provided below.
The reformulated objective can be expressed as:
J(h) = 2 \sum_{i=1}^k | \Pr(i) \Pr(h(x) > 0) - \Pr(i, h(x) > 0) |
This objective resembles an independence test between i and h(x) > 0, similar to measures used in rule induction and decision tree learning, such as Carnap's confirmation measure.
Decision tree learning can be viewed as a recurrent procedure of empirical risk minimization guided by a surrogate loss function. It would be interesting to investigate whether a loss function corresponding to J(h) exists. The authors could clarify the difference between LOMTree and standard decision trees, as standard criteria like Shannon or Gini entropy can satisfy the requirements of balanced and pure splits.
A potential trade-off between test-time complexity and function class complexity is suspected, which could explain the empirical results of OAA and LOMTree. The authors are invited to comment on this.
The prediction time results are provided per test example, but it is unclear how this method can be applied when test examples arrive in mini-batches. The authors could explore this scenario and discuss potential optimizations.
Clarity:
The paper is well-written and easy to follow.
Originality:
The paper is highly original in addressing the problem of efficient learning and prediction in large output spaces. Although the introduced algorithm is novel, the authors should acknowledge the extensive research on decision tree induction in the 80s and 90s, which has led to the development of similar algorithms, such as incremental learning of trees and Hoeffding trees.
Significance:
This paper is expected to be highly influential in the field of large-scale machine learning.
After Rebuttal:
The references to confirmation measures were provided as they bear similarities to the proposed objective. It is suggested that the authors explore this topic further in an extended version of the paper, as similar measures have been considered in various settings and may provide interesting links.
The authors are also encouraged to discuss the relationship between their approach and existing incremental learning methods for decision trees, such as Very Fast Decision Trees (Hoeffding trees). A detailed discussion can be postponed to an extended version of the paper.
Overall, this is an inspiring paper that is expected to be highly influential in large-scale machine learning and should be accepted for publication at NIPS.