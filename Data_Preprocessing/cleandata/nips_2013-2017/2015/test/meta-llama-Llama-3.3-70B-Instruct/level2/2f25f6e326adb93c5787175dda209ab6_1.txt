This paper presents a novel approach to learning the parameters of determinantal point processes (DPPs) through likelihood-based inference. The main contribution of the paper is the derivation of bounds on the likelihood of a DPP, both for finite and continuous domains, that do not rely on the spectral decomposition of the operator L. These bounds are cheap to evaluate and can be used for variational inference and Markov chain Monte Carlo (MCMC) methods.
The paper is well-written, and the authors provide a clear and concise introduction to DPPs and their applications. The technical sections are also well-organized, and the proofs of the propositions are provided in the supplementary material. The experiments demonstrate the effectiveness of the proposed approach on both synthetic and real datasets.
The strengths of the paper include:
* The derivation of novel bounds on the likelihood of a DPP that do not rely on the spectral decomposition of the operator L.
* The application of these bounds to variational inference and MCMC methods for learning the parameters of a DPP.
* The demonstration of the effectiveness of the proposed approach on both synthetic and real datasets.
The weaknesses of the paper include:
* The paper assumes that the kernel L is parametrized directly, which may not be the most interpretable choice of parameters.
* The optimization of the pseudo-inputs is crucial for likelihood-based inference, but the paper does not provide a detailed analysis of the optimization process.
* The paper does not provide a comparison with other methods for learning DPPs, such as spectral methods or other variational approaches.
Overall, the paper presents a significant contribution to the field of DPPs and provides a novel approach to learning their parameters. The proposed bounds on the likelihood of a DPP are a valuable tool for inference, and the experiments demonstrate their effectiveness. However, the paper could benefit from a more detailed analysis of the optimization process and a comparison with other methods for learning DPPs.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of DPPs.
* The proposed bounds on the likelihood of a DPP are a valuable tool for inference.
* The experiments demonstrate the effectiveness of the proposed approach on both synthetic and real datasets.
Arguments con acceptance:
* The paper assumes that the kernel L is parametrized directly, which may not be the most interpretable choice of parameters.
* The optimization of the pseudo-inputs is crucial for likelihood-based inference, but the paper does not provide a detailed analysis of the optimization process.
* The paper does not provide a comparison with other methods for learning DPPs, such as spectral methods or other variational approaches.
Recommendation: Accept with minor revisions. The paper is well-written, and the proposed approach is novel and significant. However, the paper could benefit from a more detailed analysis of the optimization process and a comparison with other methods for learning DPPs.