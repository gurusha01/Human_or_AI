This paper proposes novel bounds on the likelihood of determinantal point processes (DPPs), which are point process models that encode diversity between points. The authors derive bounds on the likelihood for both finite and continuous domains, and demonstrate their use for variational inference and Markov chain Monte Carlo (MCMC) methods. The bounds are cheap to evaluate and do not rely on approximating the spectrum of a large matrix or an operator, making them a significant contribution to the field.
The paper is well-written and clearly organized, with a thorough introduction to DPPs and their applications. The authors provide a detailed explanation of the challenges in learning the parameters of a DPP, and how their proposed bounds address these challenges. The experimental results demonstrate the effectiveness of the proposed methods, including a toy example and a real-world dataset of diabetic neuropathy.
The strengths of the paper include:
* The proposal of novel bounds on the likelihood of DPPs, which are cheap to evaluate and do not rely on spectral approximations.
* The demonstration of the use of these bounds for variational inference and MCMC methods, which are important tools for learning the parameters of DPPs.
* The thorough experimental evaluation, including a toy example and a real-world dataset.
* The clear and well-organized writing style, which makes the paper easy to follow.
The weaknesses of the paper include:
* The assumption that the kernel L is parametrized, which may limit the interpretability of the model parameters.
* The need for optimization of the pseudo-inputs, which can be computationally expensive.
* The lack of comparison to other methods, such as spectral approximations, which may be more accurate but more computationally expensive.
Overall, the paper makes a significant contribution to the field of DPPs and point process modeling, and the proposed bounds have the potential to be widely used in practice. The authors have demonstrated the effectiveness of their methods, and the paper is well-written and easy to follow.
Arguments pro acceptance:
* The paper proposes novel bounds on the likelihood of DPPs, which are cheap to evaluate and do not rely on spectral approximations.
* The authors demonstrate the use of these bounds for variational inference and MCMC methods, which are important tools for learning the parameters of DPPs.
* The experimental results demonstrate the effectiveness of the proposed methods.
Arguments con acceptance:
* The assumption that the kernel L is parametrized may limit the interpretability of the model parameters.
* The need for optimization of the pseudo-inputs can be computationally expensive.
* The lack of comparison to other methods, such as spectral approximations, may limit the understanding of the trade-offs between accuracy and computational cost.
Quality: 8/10
The paper is well-written and clearly organized, and the proposed bounds are a significant contribution to the field. However, the assumption that the kernel L is parametrized may limit the interpretability of the model parameters, and the need for optimization of the pseudo-inputs can be computationally expensive.
Clarity: 9/10
The paper is well-written and easy to follow, with a thorough introduction to DPPs and their applications. The authors provide a clear explanation of the challenges in learning the parameters of a DPP, and how their proposed bounds address these challenges.
Originality: 8/10
The proposed bounds are novel and a significant contribution to the field. However, the idea of using bounds on the likelihood for variational inference and MCMC methods is not new, and the authors build on existing work in this area.
Significance: 9/10
The paper makes a significant contribution to the field of DPPs and point process modeling, and the proposed bounds have the potential to be widely used in practice. The authors demonstrate the effectiveness of their methods, and the paper is well-written and easy to follow.