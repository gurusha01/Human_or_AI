This paper proposes a new variational inference method based on a proximal framework that uses the Kullback-Leibler (KL) divergence as the proximal term. The authors make two main contributions: (1) they propose a KL proximal-point algorithm and show its equivalence to variational inference with natural gradients, and (2) they use the proximal framework to derive efficient variational algorithms for non-conjugate models by linearizing the non-conjugate terms.
The paper is well-written and clearly explains the main ideas and contributions. The authors provide a thorough review of related work and establish the connection between their approach and existing methods. The proposed algorithm is applied to various models, including generalized linear models, linear basis function models, and Gaussian processes, and the results show that the method is computationally efficient and provides comparable performance to existing methods.
The strengths of the paper include:
* The authors provide a clear and concise explanation of the proposed algorithm and its connection to existing methods.
* The paper includes a thorough review of related work and establishes the novelty of the proposed approach.
* The authors provide experimental results that demonstrate the effectiveness of the proposed method.
The weaknesses of the paper include:
* The paper assumes a certain level of familiarity with variational inference and proximal methods, which may make it difficult for non-experts to follow.
* Some of the derivations and proofs are relegated to the supplementary material, which may make it difficult for readers to fully understand the details of the proposed method.
* The paper could benefit from more discussion on the potential limitations and future directions of the proposed approach.
Overall, the paper is well-written and provides a significant contribution to the field of variational inference. The proposed algorithm has the potential to be widely applicable and could be of interest to researchers and practitioners working in machine learning and statistics.
Arguments pro acceptance:
* The paper proposes a novel and efficient variational inference method that takes into account the geometry of the posterior distribution.
* The authors provide a thorough review of related work and establish the connection between their approach and existing methods.
* The experimental results demonstrate the effectiveness of the proposed method.
Arguments con acceptance:
* The paper assumes a certain level of familiarity with variational inference and proximal methods, which may limit its accessibility to non-experts.
* Some of the derivations and proofs are relegated to the supplementary material, which may make it difficult for readers to fully understand the details of the proposed method.
* The paper could benefit from more discussion on the potential limitations and future directions of the proposed approach.