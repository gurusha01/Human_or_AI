This paper introduces a novel variational inference method leveraging a proximal framework with the Kullback-Leibler (KL) divergence as the proximal term. The authors make two key contributions: (1) they propose a KL proximal-point algorithm, demonstrating its equivalence to natural-gradient-based variational inference methods, such as stochastic variational inference (SVI), and (2) they extend this framework to non-conjugate models by introducing a splitting procedure that linearizes non-conjugate terms, enabling efficient subproblem solutions. The paper demonstrates the method's applicability across various models and shows competitive performance on real-world datasets compared to existing approaches.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with detailed derivations and proofs, such as the equivalence between the KL proximal-point algorithm and natural-gradient methods. The convergence properties of the proposed algorithms are well-supported by theoretical analysis.
2. Novelty: The use of the KL divergence as a proximal term and the splitting approach for non-conjugate models are innovative. The method builds on and extends prior work in variational inference, such as SVI, by explicitly addressing the geometry and structure of the variational bound.
3. Practical Contributions: The proposed method is computationally efficient, particularly for high-dimensional problems, as it avoids explicit computation of large covariance matrices. The kernel trick further enhances scalability for Gaussian process models.
4. Empirical Validation: The experiments on Bayesian logistic regression and Gaussian process models demonstrate that the proposed method achieves comparable or better performance than existing methods like expectation propagation (EP) and Laplace approximation, with competitive runtime.
Weaknesses:
1. Clarity: While the paper is mathematically thorough, it is dense and difficult to follow for readers unfamiliar with proximal methods or variational inference. The organization could be improved by providing a clearer high-level explanation of the method before diving into technical details.
2. Limited Scope of Experiments: The experiments focus on a few specific models and datasets. While the results are promising, broader validation on more diverse models and larger datasets would strengthen the paper's claims.
3. Comparison with Related Work: Although the paper references prior work, the experimental comparisons are limited. For example, the paper does not compare against more recent advances in variational inference, such as black-box variational inference or amortized inference methods.
4. Hyperparameter Sensitivity: The method relies on fixed step sizes and does not explore adaptive strategies, such as line search or learning rate schedules, which could improve robustness and performance.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by unifying proximal-point methods and natural gradients in variational inference.
- The proposed approach is computationally efficient and has practical utility for non-conjugate models.
- The empirical results demonstrate competitive performance, supporting the method's effectiveness.
Arguments Against Acceptance:
- The paper's clarity and accessibility could be improved, potentially limiting its impact on a broader audience.
- The experimental evaluation is somewhat narrow, and comparisons with more recent methods are missing.
Recommendation:
I recommend acceptance with minor revisions. The paper presents a novel and technically sound contribution to variational inference, with practical implications. However, the authors should improve the clarity of the presentation and expand the experimental evaluation to strengthen the paper's impact.