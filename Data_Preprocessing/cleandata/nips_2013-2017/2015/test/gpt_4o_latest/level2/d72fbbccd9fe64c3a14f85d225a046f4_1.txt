The paper proposes a novel deep architecture for topic modeling based on Poisson Factor Analysis (PFA) modules, introducing a fully nonnegative formulation that is interpretable across all layers. The authors replace traditional logistic links with a Bernoulli-Poisson link, enabling efficient inference and scalability with sparse data. Key contributions include a deep architecture entirely composed of PFA modules, block updates for binary units, and a discriminative extension for classification tasks. The paper demonstrates the model's advantages over existing approaches through experiments on benchmark corpora and a medical records dataset.
Strengths:
1. Novelty and Elegance: The use of PFA modules at all layers is a significant innovation, offering conceptual simplicity and interpretability compared to prior models like DPFA, which mix PFA and Sigmoid Belief Networks (SBNs).
2. Scalability: The model's inference scales with the number of non-zero elements in the data, making it computationally efficient for sparse datasetsâ€”a common scenario in topic modeling.
3. Comprehensive Evaluation: The paper provides extensive experimental results on multiple datasets, demonstrating improved perplexity and classification accuracy compared to state-of-the-art models such as LDA, DPFA, and nHDP.
4. Discriminative Extension: The integration of classification tasks into the generative framework is a valuable addition, achieving superior performance on document classification benchmarks.
5. Real-World Application: The application to medical records highlights the model's practical utility, uncovering clinically meaningful clusters of medications and patient profiles.
Weaknesses:
1. Clarity: While the technical details are thorough, the paper is dense and could benefit from clearer explanations, particularly in the mathematical derivations and the transition from single-layer to deep architectures.
2. Comparative Analysis: Although the paper compares its model to several baselines, it lacks a deeper discussion of why certain models (e.g., DPFA-SBN) perform worse, particularly in terms of interpretability and computational trade-offs.
3. Limited Exploration of Depth: The experiments primarily focus on two-layer models, and the benefits of deeper architectures (beyond two layers) are not fully explored.
4. Scalability Beyond SVI: While stochastic variational inference (SVI) is presented as a scalable alternative to MCMC, the paper does not explore other modern, gradient-based inference methods that could further enhance scalability.
Arguments for Acceptance:
- The paper introduces a significant improvement in deep topic modeling by unifying interpretability, scalability, and performance.
- The results on benchmark datasets and real-world applications demonstrate the model's practical relevance and superiority over existing approaches.
- The discriminative extension provides a valuable contribution to multi-task learning in topic modeling.
Arguments Against Acceptance:
- The dense presentation may hinder accessibility for readers unfamiliar with PFA or deep topic modeling.
- The exploration of deeper architectures and alternative inference methods is limited, leaving questions about the model's scalability and generalizability.
Recommendation:
This paper makes a strong scientific contribution to the field of topic modeling and is well-suited for presentation at NIPS. However, the authors should consider revising the manuscript to improve clarity, particularly in the mathematical exposition, and provide additional insights into the scalability and performance of deeper architectures. Overall, I recommend acceptance with minor revisions.