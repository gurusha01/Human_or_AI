Summary:
The paper addresses multi-class classification problems involving a large number of classes. It proposes a novel label tree classifier capable of learning and making predictions in logarithmic time with respect to the number of classes. Theoretical guarantees are provided through a boosting-like theorem. Furthermore, the paper introduces an online training mechanism for both the node classifiers and the tree structure. A simple subtree swapping procedure is also presented to ensure proper tree balancing. Empirical results highlight the appeal of the proposed approach.
Quality:
The paper provides both theoretical and empirical contributions. The method is rigorously derived and supported by theoretical justifications. Below are some minor comments.
After reformulation, the objective can be expressed as:
J(h) = 2 \sum_{i=1}^k | \Pr(i) \Pr(h(x) > 0) - \Pr(i, h(x) > 0) |
This resembles an independence test between i and h(x) > 0. It appears that similar measures have been employed in rule induction (e.g., for generating association rules) and decision tree learning. This formulation also bears resemblance to Carnap's confirmation measure (see, "Comparison of confirmation measures," Cognition, 2007, and "Logical foundations of probability (2nd ed.)," 1962).
Decision tree learning can be framed as a recursive procedure of empirical risk minimization guided by a surrogate loss function, where both splits and leaf predictions are determined as empirical risk minimizers (thus eliminating the need for a purity measure). For instance, C4.5 uses logistic loss, while CART employs squared error loss. It would be interesting to investigate whether a loss function exists that corresponds to J(h).
The distinction between LOMTree and standard decision trees is not clearly articulated. Footnote 4 mentions that standard criteria like Shannon or Gini entropy satisfy the requirements for balanced and pure splits. Since standard decision trees can be adapted to handle problems with a large number of classes [16,17], the precise difference between these approaches remains unclear. Could J(h) be replaced with Shannon entropy?
There may be a trade-off between test-time complexity and the complexity of the function class. Minimizing the number of prediction steps might necessitate increasing the complexity of the function class. This could potentially explain the empirical results comparing OAA and LOMTree. Could the authors comment on this?
The prediction time results are reported per test example, and the method is designed to optimize test time for individual examples. However, when test examples are processed in mini-batches, various optimization techniques could be employed to further improve test time. How would this method perform in such a scenario?
Clarity:
The paper is well-written and clear.
Originality:
The paper is highly original, as it explicitly addresses the challenge of efficient learning and prediction in large output spaces. The proposed algorithm is novel, though the authors should consider that decision tree induction has been extensively studied in the 1980s and 1990s. Several algorithms with similarities to LOMTree (e.g., incremental tree learning, trees for data streams, Hoeffding trees, and trees with linear splits) have already been introduced.
Significance:
The paper is (or is likely to become) highly influential in the field of large-scale machine learning.
After rebuttal:
Thank you for your detailed responses.
I referenced confirmation measures because they are the closest measures I am familiar with that resemble your objective. It would be valuable to explore this connection further in an extended version of the paper (e.g., on arXiv or in a journal). I find your objective highly interesting, but I believe similar measures have been studied in various contexts, and exploring these links could yield additional insights. That said, I agree that such measures are not widely used or particularly successful in decision tree learning, especially in the multi-class setting.
The same applies to incremental learning of decision trees. There are existing approaches that can be easily found online. The question is how your approach compares to them. A well-known example is the Very Fast Decision Tree (or Hoeffding Tree) introduced by Domingos and Hulten (2000). A detailed discussion of these comparisons could be deferred to the extended version of your paper.
This is an inspiring paper that is (or is poised to become) highly influential in large-scale machine learning. I strongly recommend its acceptance for publication at NIPS.