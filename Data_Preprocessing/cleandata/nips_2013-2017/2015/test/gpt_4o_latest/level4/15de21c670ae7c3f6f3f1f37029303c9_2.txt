Summary: This paper introduces a novel method for computationally efficient maximum likelihood estimation (MLE) in exponential families. Typically, solving for the MLE is computationally intractable. From the perspective of convex optimization, the primary challenge lies in evaluating an integral with respect to the current exponential family (EF) parameter. By assuming that Markov Chain Monte Carlo (MCMC) methods mix rapidly for all permissible parameters, the author(s) demonstrate that the integrals required for proximal gradient descent can be computed with sufficient accuracy. When combined with the results of Schmidt et al. (2011), this leads to a fully polynomial randomized approximation scheme for MLE computation. The paper addresses both the convex and strongly convex cases, offering distinct guarantees: the former pertains to likelihood error, while the latter concerns parameter error. A simple experiment is included to validate the theoretical findings.
Overall, this is a high-quality contribution. The proposed approach is compelling as an alternative to relying on small-treewidth assumptions. While I did not verify the proofs in detail, the theoretical analysis appears robust. The proofs primarily involve combining the MCMC mixing assumption with concentration inequalities and the results of Schmidt et al. The work is therefore a synthesis of existing techniques, but the combination is nontrivial and innovative.
I appreciated the candid discussion of the approach's limitations. The conclusion thoughtfully examines the weaknesses of the method, highlights how some theoretical assumptions may be overly conservative, and outlines potential directions for future research. Additionally, the paper provides insights into contrastive divergence (CD)-type algorithms, even though the analyzed algorithm is not strictly a CD method.
The introduction is well-written and provides extensive references to prior work. However, there are some issues with the clarity of the technical presentation. One notable omission is an explicit statement that the log-likelihood is convex. While the convexity of the second and third terms in Eq. 1 is evident, and the convexity of \( A(\theta) \) follows from its Hessian being the covariance of the sufficient statistics (and thus positive semidefinite), explicitly stating this would help guide readers through the paper's logic. Additionally, a clear summary of the paper's assumptions would be beneficial. For instance, the convexity of the parameter set \( \Theta \), though critical, is not explicitly mentioned in the main text.
In summary, this paper represents an original and significant contribution to the field. However, the practical utility of the results may be limited, as proving fast mixing of MCMC chains is notoriously difficult. In essence, the paper replaces one challenging problem (finding the MLE) with another (ensuring fast-mixing Markov chains). Nonetheless, transforming one problem into another can often be valuable, and the results presented here will likely be of interest to the broader research community.
Other comments:  
- l185: ",w here" → ", where"  
- l192: "this paper that" → "this paper assumes that"  
- l207: "simple example fast-mixing" → "simple example of a fast-mixing"  
- l281: "of of" → "of"  
- l311-12: This sentence is unclear. Rewrite for clarity.  
This is a novel approach to establishing conditions under which finding the (approximate) MLE becomes tractable. The paper is well-written, though its practical applicability may be somewhat limited.