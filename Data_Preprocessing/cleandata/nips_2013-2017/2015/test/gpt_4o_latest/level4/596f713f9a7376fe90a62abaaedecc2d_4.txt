The paper introduces a fast approximate inference method for MAP estimation within the generative EPLL framework [21]. Specifically, it employs discriminative training to replace the computationally intensive update of latent variables during alternating optimization. Experimental results demonstrate that this approximate inference approach achieves a 1-2 orders of magnitude speedup while incurring only a modest reduction in performance compared to the previously-used ("exact") EPLL inference.
Positive  
I strongly support the paper's goal of preserving the advantages of generative methods, as outlined in the introduction. However, the proposed method is not as generic as initially implied, as it specifically targets inference within the EPLL framework. That said, the idea of leveraging discriminative methods to approximate costly steps in generative inference is intriguing. Have you considered how this approach could be generalized to other settings? This would be a fascinating direction to explore.
It is noteworthy that the proposed approximate inference achieves comparable performance to the "exact" EPLL inference. This is significant, as EPLL is known for state-of-the-art results, and the proposed method extends its applicability to larger images (Figure 7 is particularly compelling). I believe the paper could emphasize this contribution more prominently.
One aspect that remains unclear to me is why EPLL appears to be the only generative approach competitive with discriminative methods. Understanding this better would be highly insightful.
Negative  
The paper seems to overstate its contributions. While the introduction sets high expectations by presenting a broad and ambitious agenda, the actual contributions are more narrowly focused. This disconnect left me somewhat disappointed. The introduction suggests a general framework for combining generative and discriminative approaches, but the paper ultimately does not deliver a "deep" integration. Instead, it relies on standard generative training for the image prior, followed by discriminative training to approximate expensive inference steps.
Additionally, the method addresses a very specific problem—MAP estimation with the EPLL framework—which does not appear to generalize easily to other settings. Other generative approaches with filter-based priors, such as [Krishnan and Fergus, "Fast image deconvolution using hyper-Laplacian priors", NIPS 2009], do not suffer from the same computational bottleneck in updating latent variables.
While not central to the paper's main contribution, the discussion of generative versus discriminative methods starting in line 111 lacks nuance. For example, generative methods offer advantages beyond modularity and test-time efficiency, such as the ability to use different loss functions at test time or to handle unknown random variables through principled marginalization.
Clarity  
- The explanation of the gating network in Section 3 is difficult to follow and could be clarified.  
- Table 1 does not specify the test data used for the denoising results. Are the test sets different across the referenced papers?  
- Typo in Equation 7: the weight \( wi^j \) in the denominator should be \( wi^k \).  
- Typo in line 102: "Hel-Or used..." should be "Hel-Or and Shaked used...".  
- Figure 2 is unclear due to the lack of labeled components. Additionally, there appears to be a missing connection in the gating network on the right side: the bottom-left node should connect to the middle-right node.  
- The title "return of gating network" feels inappropriate. Similarly, the title "combining generative models and discriminative training in natural image priors" suggests a generic method, which the paper does not deliver.
Miscellaneous  
- Note that "modularity" can also be achieved in discriminative approaches by training with a likelihood component that can later be replaced. For instance, [Chen et al., "Revisiting loss-specific training of filter-based MRFs for image restoration", GCPR 2013, doi:10.1007/978-3-642-40602-7_30] trained a CRF with loss-specific training for denoising and reused it for deconvolution and super-resolution by swapping the likelihood term without re-training.  
- EPLL inference does not actually use half-quadratic splitting as introduced by Geman and colleagues for filter-based MRFs [cf. Geman and Reynolds, "Constrained restoration and the recovery of discontinuities", PAMI 1992; Geman and Yang, "Nonlinear image recovery with half-quadratic regularization", TIP 1995]. Instead, the method is better described as a quadratic penalty method for approximate constrained optimization [cf. Nocedal and Wright, "Numerical Optimization", Sec. 17.1].  
- Despite its technical simplicity, the paper makes a meaningful contribution by providing a generative image restoration approach that is competitive with state-of-the-art discriminative methods in terms of both restoration quality and inference speed.