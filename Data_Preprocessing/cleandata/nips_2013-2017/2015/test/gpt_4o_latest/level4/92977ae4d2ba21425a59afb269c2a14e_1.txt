This study investigates the local convergence properties of the EM algorithm in high-dimensional scenarios where the dimensionality exceeds the sample size. The paper extends prior work focused on the low-dimensional setting. The key innovation lies in an iterative regularization approach, which progressively reduces the regularization parameter as the algorithm approaches the optimum. This enables the algorithm to achieve linear convergence within the local neighborhood of the optimum. However, the results depend on several stringent assumptions about the objective function. Additionally, as is common in theoretical analyses of EM-like algorithms, fresh samples are required at each iteration.
Given the "light" nature of this review, I was unable to rigorously verify the details of this highly technical contribution. If the findings are indeed correct, they constitute a meaningful and significant advancement in the theoretical understanding of EM. The paper will be of primary interest to experts in this specialized field.
A substantial (albeit highly technical) contribution to the theory of EM in high-dimensional estimation.