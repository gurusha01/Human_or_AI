This paper introduces a novel online algorithm for constructing a multiclass classifier with a time complexity that scales logarithmically with the number of classes \(k\). The approach involves incrementally building a decision tree that locally optimizes a newly proposed objective function, which combines "balancedness" and "purity" to evaluate tree quality. Theoretical analysis is provided for a potentially intractable algorithm using a boosting framework (assuming weak learnability), extending the work of Kearns and Mansour (1996) to the multiclass setting. A practical algorithm is presented for a relaxed version of the problem, which, while lacking guarantees, is simple, intuitive, and intriguing.
The paper is well-written, engaging, and appears to be original. The theoretical analysis seems sound, and the experimental results demonstrate meaningful improvements over other logarithmic-time classifiers, though they fall short of achieving state-of-the-art generalization performance. This work represents a modest but valuable step toward that goal.
However, I have several concerns:
- While the authors claim that their approach achieves the most efficient possible time complexity of \(O(\log(k))\), this assertion is misleading. For instance, a 1-nearest neighbor classifier has training and testing times that are independent of \(k\), as reading \(\log(k)\) bits is treated as an \(O(1)\) operation under the time complexity framework used in the paper. Furthermore, the information-theoretic arguments in the introduction do not seem directly relevant to the time complexity analysis of the proposed algorithm. Could the authors clarify this point?
- If I understand correctly, the LOMtree algorithm does not fully optimize the relaxed objective described in line 263. This appears to stem from the fact that the estimates of expectations such as \(mv(y)\), \(Ev\), etc., are not updated when retrained hypotheses higher up in the tree alter their decisions on certain samples along the paths. Since the statistics in Algorithm 1 are additive, such changes are not reflected in the estimates. Wouldn't a more sophisticated mechanism, such as message passing, be required for accurate bookkeeping? The current "open-loop" approach to maintaining node statistics may lead to erratic algorithmic behavior. Could the authors clarify this issue?
- It seems likely that the accuracy of classifiers degrades as one moves deeper into the tree. This is because the tree is designed to be balanced, and nodes lower in the hierarchy will have significantly fewer samples available for training robust classifiers. As a result, I suspect that achieving good generalization accuracy may require an exponential number of samples. Could the authors address this concern?
Despite these issues, the paper presents a novel and sufficiently interesting approach to online multiclass classification, and I recommend it for publication.