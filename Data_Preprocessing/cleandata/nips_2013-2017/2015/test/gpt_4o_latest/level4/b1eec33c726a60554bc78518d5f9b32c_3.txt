The paper addresses the regression problem with multiple vector-valued outputs that are interconnected either through a shared noise vector, common coefficient vectors, or other attributes. This is both a theoretically and practically significant question.
The authors investigate two well-known models for such problems. The first is the pooled model, where the coefficient vector is shared across the different outcomes. The second is the SUR model, which assumes a shared noise vector across dimensions. In both cases, one could choose to disregard the additional structure and simply perform ordinary least squares regression (OLS). However, it is known that maximum likelihood estimation (MLE) outperforms OLS in these scenarios. The challenge lies in the fact that MLE involves solving a non-convex optimization problem. To address this, the authors analyze the performance of alternating minimization and demonstrate that, under these models, it achieves performance within universal constant factors of MLE.
1. Lines 190â€“200: Is the repeated use of fresh samples responsible for the additional logarithmic factor in Theorem 1? It would be interesting to understand why this logarithmic factor arises, as the lower bounds do not seem to include it.
2. Applying these results to real-world datasets would be an intriguing direction for future work.
3. The two models considered in the paper are somewhat restrictive. It would be helpful if the authors could elaborate on why alternating minimization performs well under these specific models. More broadly, are there general conditions under which alternating minimization can be shown to achieve performance within universal constant factors of MLE for regression problems?
The paper demonstrates that for two widely studied models of regression problems with vector-valued outputs, alternating minimization can achieve performance comparable to MLE up to universal constants. This is a noteworthy and valuable result.