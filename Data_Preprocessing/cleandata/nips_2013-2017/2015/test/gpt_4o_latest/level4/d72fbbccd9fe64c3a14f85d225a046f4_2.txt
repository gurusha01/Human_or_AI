The paper introduces a multilayer model for count vectors, employing Poisson Factor Analysis (PFA) at each layer to provide interpretable topics, while using binary units to connect the layers and learn topic correlations. The inference methods, including MCMC and SVI, are straightforward due to all conditional posteriors being in closed form, and they scale efficiently with the number of non-zero observations and hidden units. The model represents an incremental modification of Zhou et al. (2012), replacing the global beta-Bernoulli process with the Bernoulli-Poisson link to avoid relying on sigmoid belief networks.
Both the proposed model and its inference procedures are elegant. While the contribution is incremental (building on Zhou et al. (2012)), the work is likely to attract significant interest within the community.
1. The experimental evaluation compares the proposed model to other deep and single-layer models, but there is minimal exploration of the model's internal structure and behavior. 
   a. How were the layer widths determined? Have you experimented with architectures beyond two layers?  
   b. How does the model handle overdispersion in the data?  
   c. What is the rationale for how layer widths should decay with depth?  
2. It is surprising that even the single-layer version of the model outperforms ORSM and LDA (which is closely related to PFA). Is this improvement attributable to your approach to discriminative topic modeling? The paper does not provide an explanation.  
3. Why is ORSM excluded from Table 1?  
The paper combines ideas from Zhou et al. (2012) and Zhou et al. (2015) to propose a hierarchical structure of PFA units connected via hidden binary variables. While the contribution is incremental, the use of local conjugacy simplifies both MCMC and variational inference, and the experimental results demonstrate superior performance. However, the paper lacks a thorough investigation into the network structure, which appears to have been arbitrarily fixed. Despite this limitation, the work is likely to be of considerable interest to researchers in deep learning and topic modeling.