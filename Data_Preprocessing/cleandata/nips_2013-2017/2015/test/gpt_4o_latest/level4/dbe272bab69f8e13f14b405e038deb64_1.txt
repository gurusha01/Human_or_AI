This paper revisits the problem of fitting mixtures of Gaussians using gradient-based methods. By combining manifold optimization with a specific linesearch procedure and a "reparameterized" objective, the proposed approach empirically outperforms batch EM in the experiments presented.
Although I am not an expert in Riemannian manifolds, the work appears credible. The presentation is accessible, assuming the reader is willing to trust the results from cited references.
While plain Gaussian mixture models (GMMs) may not seem particularly groundbreaking in the current era of Bayesian nonparametrics and deep neural networks, they remain essential tools in many applications due to their utility and surprising versatility. Extending GMMs by incorporating constraints or tying covariances together is a common need, but such extensions often render EM inapplicable. Gradient-based methods, on the other hand, generalize well and could have a significant impact on statistical applications if they perform effectively on the base model.
Previously, I would have relied on an unconstrained parameterization using the Cholesky decomposition. While the paper provides arguments against this approach, it is unfortunate that it is not included in the empirical comparisons. Are spurious stationary points a practical concern? Is the manifold optimizer genuinely superior? Is the primary advantage derived from the "representation" in equation (2.3), and could this representation also benefit the Cholesky approach? Addressing these questions would strengthen the paper.
Minor comments:
- Line 081: Replace "open a new" with "open new."
- Line 138: Change "rely geodesics" to "rely on geodesics."
- The timings in Table 4 would be more effectively presented as a graph, which would provide a quicker and more intuitive comparison. Precise timings are less relevant, as they depend on implementation and hardware specifics.
- I recommend revising lines 147â€“149 ("At any point... specifically [4]") to: "Geodesics on the manifold from $\Sigma1$ to $\Sigma2$ are given by [4]:". The paper does not explain Riemannian metrics, nor is such an explanation necessary to trust the geodesic result. Additionally, the lack of context and the nearby definition of `$d$' may lead to a misinterpretation of `$d$' as a variable, causing the trace expression to be mistakenly evaluated as `$d^3$'.
- Explicitly include the zero-mean ("0,") in the normal density of equation (2.3) and the line below it. On a quick reading, augmenting the data with a constant in the line above (2.3) might seem problematic, as a Gaussian could fit these new data points with infinite density. The zero-mean constraint prevents this issue. Additionally, the compact Gaussian notation is explicitly defined with a mean term above equation (2.1), so consistency is important.
- I would not describe the transformed problem in equation (2.3) as a "reparameterization." The model being fitted is different and would generate data from a distribution distinct from the original.
- Consider numbering all equations to facilitate referencing, not only in reviews but also in reading groups and citations.
This paper introduces a novel gradient-based procedure for fitting GMMs, which appears to outperform EM. While I remain slightly unclear on some of the claims, I find the paper potentially impactful for applications and have increased my score following the rebuttal. The authors have promised additional clarifications and control experiments to compare their method with the Cholesky approach, which I believe will make this an interesting and valuable contribution.