Learning Gaussian Mixture Models (GMMs) is a well-explored topic in both statistics and machine learning. The Expectation Maximization (EM) algorithm, introduced over three decades ago, remains the most widely used approach for this problem. This paper introduces the use of manifold optimization—a fairly mature subfield of optimization—for tackling GMMs. In essence, the optimization problem involves estimating the parameters of the mixture model (i.e., the mean, covariance, and weight for each component, assuming a fixed number of components) by maximizing the (log-)likelihood of the observed data. The primary challenge lies in the constraint that the covariance matrices must be positive semi-definite, which significantly complicates the optimization process. 
The paper notes that directly applying standard manifold optimization techniques is not practical due to their slow convergence. However, the authors demonstrate that a straightforward re-parameterization of the optimization problem, combined with careful application and fine-tuning of standard manifold optimization algorithms, leads to substantial performance improvements. Specifically, in the experiments presented, their proposed algorithm (among several discussed in the paper, I focus on the best-performing one) achieves convergence times that are comparable to or even better than EM, while yielding similar final values for the objective function. (It is worth noting that both methods may converge to local maxima, and the paper does not address how close the estimated parameters are to the true parameters.) While further investigation is needed to determine the generality of these results, the demonstrated improvements are noteworthy given the significance of GMMs.
Minor comment: On Line 129, the statement "Problem (2.1) in general can require exponential time" should be revised for clarity. A more precise phrasing would be: "Problem (2.1) in general can require a number of samples that grows exponentially with K." Based on the strength of the experimental results, I believe this paper warrants acceptance.