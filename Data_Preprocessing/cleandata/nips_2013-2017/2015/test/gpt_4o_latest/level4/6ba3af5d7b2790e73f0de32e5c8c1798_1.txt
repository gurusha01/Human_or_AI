The paper tackles the challenge of learning a model for Atari 2600 games (a widely-used benchmark for reinforcement learning algorithms), specifically predicting future frames based on action inputs.
This is a difficult problem, and solving it provides a valuable tool for designing improved controllers.
The paper is well-written, logically organized, and supported by compelling experiments (including videos).
The proposed model consists of a CNN (with an additional fully-connected layer) followed by multiplicative interactions with an action vector, and then convolutional decoding layers. The recurrent variant incorporates an LSTM layer after the CNN.
The authors assess their models using both pixel accuracy (a conventional metric for such tasks) and their utility for control (which is the ultimate goal).
However, additional experimental details would strengthen the paper, particularly regarding 1) the network architecture (notably the deconvolution component) and 2) the training procedure. Ideally, the authors could release their code, but providing more specifics in the main text or supplementary material would also suffice.
Specific comments:
- It is somewhat disappointing that the authors did not attempt to predict rewards alongside future frames. Including rewards could have enabled the model to be used for planning (e.g., via UCT), rather than relying on a pre-trained model-free controller to evaluate control performance, which is less straightforward to interpret.
- The baselines used for comparison are relatively weak, though this is understandable given the lack of clear alternatives (to the best of my knowledge).
- Regarding the exploration section, it is unclear whether the predictive model is learned online to aid exploration or if it is trained using data from a standard DQN (uninformed exploration) and then used to guide the exploration of a new controller. If it is the latter, the purpose of this approach is unclear, as exploration has already been performed to train the model. Nonetheless, it is intriguing that this method improves performance in certain games.
- The discussion on controlled versus uncontrolled dynamics in the final section is thought-provoking.
- The authors may wish to review the recent work "DeepMPC: Learning Deep Latent Features for Model Predictive Control," which explores learning deep predictive models (also employing multiplicative interactions with actions) for control, though it is not focused on the visual domain.
Minor issues/typos:
- "In Seaquest, new objects appear from the left side or right side randomly, and these are hard to predict." This statement may not be entirely accurate, though the behavior does appear random.
- Line 430: "predicing" should be corrected to "predicting."
[Updated score after rebuttal. Other recent papers on learning deep dynamical models from images, though not specific to Atari games: - From Pixels to Torques: Policy Learning with Deep Dynamical Models - Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images.]
This is a well-executed paper on learning the dynamics of Atari games from data. It is clearly written and supported by convincing experimental results.