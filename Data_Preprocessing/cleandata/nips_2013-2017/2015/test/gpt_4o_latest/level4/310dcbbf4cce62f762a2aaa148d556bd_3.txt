This paper addresses a contextual bandit problem with a budget constraint \( B \) and a known time horizon \( T \). In each trial, the agent observes a context \( j \) drawn from a fixed probability distribution \( \pi \) over a context set \(\{1, \dots, J\}\) and selects an action \( k \) from \(\{0\} \cup \{1, \dots, K\}\), where \( k = 0 \) corresponds to "skip." If \( k = 0 \), the agent receives no reward and incurs no cost. For \( k > 0 \), the agent receives a reward \( y \) and incurs a cost \( c{j,k} \), where \( y \) is drawn from a fixed probability distribution over \([0,1]\) that depends on \( j \) and \( k \), and \( c{j,k} > 0 \) is a known parameter. The agent's objective is to maximize cumulative reward while ensuring that the cumulative cost does not exceed the budget \( B \).
The paper primarily focuses on a special case where \( \pi \) is known and \( c{j,k} = 1 \) for all \( j \) and \( k \). For this setting, the authors propose an algorithm with a regret bound of \( O(\log T) \) under a mild condition. The paper further generalizes the algorithm to the case where \( \pi \) is unknown and \( c{j,k} \)'s are arbitrary.
While a more general setting is explored in [16], that work provides an algorithm with \( O(\sqrt{T}) \) regret. This paper, therefore, appears to be the first to achieve \( O(\log T) \) regret, albeit under the mild condition. The proposed algorithm is built on an approximation method for computing the optimal action sequence when all statistics are known. The algorithm is straightforward yet highly intriguing.
The paper is well-motivated and written with exceptional clarity. Its significance would be further enhanced if \( O(\log T) \) regret could be shown to hold universally, as opposed to cases where an \( \Omega(\sqrt{T}) \) lower bound emerges in boundary scenarios.
The results hinge on Lemma 1, but I initially had concerns about its validity. Specifically, I questioned whether the optimal action sequence always satisfies (2) for all trials and, consequently, why the average constraint could be considered a relaxation of the hard constraint. However, the authors' rebuttal has clarified these points, and I am now convinced of Lemma 1's correctness. Thank you for the explanation.
Minor comments:  
- Is it NP-hard to compute the exact optimal action sequence?  
- Can the results be extended to scenarios where \( T \) is unknown?  
- For the case of an unknown context distribution, can we assume \( u^1 > u^2 > \dots > u^*_J \) without loss of generality?  
I strongly recommend this paper for acceptance.