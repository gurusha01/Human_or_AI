This paper establishes PAC bounds for control policies and introduces a learning algorithm to optimize policies based on these bounds.
The problem addressed is both interesting and valuable in robotics, as the authors rightly highlight the critical importance of mitigating "worst-case" performance in many robotics tasks, exemplified by the UAV obstacle avoidance scenario.
The mathematical formalism and overall readability of the paper are commendable. Additionally, the experiments are compelling, as they effectively bridge the gap between the theoretical contributions and their applicability to (simulated) real-world problems.
However, my primary concern lies in the absence of comparative evaluations against alternative approaches. For instance, while the authors report significant improvements in obstacle avoidance rates after a few iterations of their algorithm in the UAV case, it would be insightful to compare these results with those achieved using a more conventional policy learning approach that optimizes for expected reward rather than a probabilistic bound. Evaluating mean performance on some relevant metric would also be valuable, as it could highlight potential tradeoffs. Such comparisons would greatly enhance the paper by providing concrete evidence to support the authors' assertion that optimizing PAC bounds effectively mitigates undesirable outcomes like collisions.
In summary, this is a strong paper with solid theoretical contributions and relevance to robotics and other control applications. However, incorporating more extensive evaluations against existing methods would substantially improve its impact and credibility.