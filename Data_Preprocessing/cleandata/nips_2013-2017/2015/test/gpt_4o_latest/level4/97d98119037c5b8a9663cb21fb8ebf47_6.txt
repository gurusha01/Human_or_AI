Sample Complexity Bounds for Iterative Stochastic Policy Optimization introduces a method to derive bounds on the execution cost of a policy based on the execution costs of a prior policy. These bounds are calculated and compared against empirical estimates in two domains: a simple robot domain and an aerial robot domain. The bounds were determined for both the estimated costs of a policy and the probability of a crash, and they were observed to align closely with the empirical results.
The paper is of good quality, with clear explanations throughout. However, it would have been interesting to explore how varying the initial policies impacts the convergence of expected costs. The work appears to be original, though testing on more standard benchmarks, such as grid world or cart pole, would have provided a stronger basis for comparison. The contribution would gain greater significance if PAC-style bounds could be directly established.
This research focuses on deriving robustness bounds for decision-making, particularly in the context of trajectory learning. This enables the evaluation of policy learning algorithms or the currently learned policy in terms of their associated costs. The approach seems particularly valuable for domains where empirical testing of policies is costly.