The authors introduce a novel system for video frame prediction in ATARI games that incorporates game actions into the prediction process. The key innovation lies in a multiplicative transformation layer, which dynamically selects weights based on an action vector. To ensure scalability, the authors employ a factorization of the weight tensor. Image encoding and decoding are handled using a well-established convolutional network architecture.
Two distinct architectures are proposed: a feed-forward network that takes a fixed number of previous frames as input, and a recurrent network that processes a single frame but includes an LSTM layer prior to the transformation layer. (The paper should, at least once, clarify what LSTM stands for and provide a brief explanation.)
The qualitative evaluation of the generated frames, as demonstrated in the supplementary videos, is compelling. Quantitative results, based on mean squared pixel error, are presented clearly, though it is unsurprising that their system outperforms both linear and nonlinear predictors that do not account for game actions.
Another experiment evaluates the system's performance when the generated frames are used as input for a DQN agent instead of real frames. The results align with expectations: performance is worse than when using real frames, better than random play, and superior to predictors that ignore game actions.
An intriguing application of the system is its use during agent training to enhance exploration. The authors demonstrate that when a DQN agent selects actions leading to predicted frames with the least similarity to previously encountered frames (rather than random actions), the agent's final performance improves significantly in certain games.
Additionally, the system is shown to facilitate automatic analysis of game dynamics. For instance, game actions with similar effects can be identified through similarities in the transformation weight matrix. Moreover, the system can estimate which pixels in the image are directly influenced by actions and which represent uncontrollable game dynamics.
The authors claim that "To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs." This assertion may hold, as prior systems of this nature did not incorporate LSTMs. However, it is worth noting that Schmidhuber and Huber published a related work in 1991 (International Journal of Neural Systems), where a neural predictor learned to predict the next visual input frame of a fovea based on prior input and actions. This predictor was integrated into a reinforcement learning system that optimized sequences of saccades to achieve visual goals. Schmidhuber also published earlier works at IJCNN 1990 and NIPS 1991, where both the predictor and action generator were recurrent, though LSTMs were not yet used. It would be beneficial for the authors to clarify how their system differs, particularly in terms of its architecture and reinforcement learning approach.
The results are robust, and the paper is well-organized. I recommend acceptance after minor revisions, as outlined above.
In summary, the paper presents a novel architecture for video frame prediction in ATARI games conditioned on game actions. The results are promising, particularly in improving exploration during agent training and enabling a straightforward analysis of game dynamics.