In the EM algorithm for high-dimensional settings, the M-step poses challenges that must be addressed by leveraging the structure of the high-dimensional parameters. This paper proposes the use of decomposable regularizers. The primary contribution lies in a method to configure the regularizer within the iterative EM process. The authors demonstrate that this approach achieves local linear convergence, contingent on specific conditions related to the population and empirical estimation of the log-likelihood based on the current parameter estimates (a fundamental aspect of the EM algorithm).
The EM algorithm is extensively utilized in machine learning, making it crucial to establish conditions under which it provides provable guarantees. This paper introduces a principled method to regularize the EM algorithm, which is particularly relevant in high-dimensional settings. Additionally, it applies the theoretical framework to three well-known models, yielding results that are both interesting and significant.
--In the abstract: the authors assert that regularizing the M-step using state-of-the-art techniques ([19]) does not necessarily ensure the desired bound. However, this claim would benefit from further elaboration.
--Condition 5: as acknowledged by the authors, this condition replaces a term used in prior work. It appears to be the critical factor enabling the algorithm to function effectively in high-dimensional regimes, while the analysis itself builds on previous research.
Minor comments: --line 80: Variables Y and Z should be in boldface. --Eqn (2.2): The function "f" is missing. --line 122: The notation | |_R* has not yet been defined at this point, making the sentence somewhat unclear.  
This paper investigates the regularized EM algorithm, proposing a method to configure the regularizer and identifying conditions under which the regularization ensures local linear convergence.