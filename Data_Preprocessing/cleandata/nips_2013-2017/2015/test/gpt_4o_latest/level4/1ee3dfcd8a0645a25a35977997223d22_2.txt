The paper investigates the problem of learning labels on the vertices of a graph. It extends the work of Ando and Zhang, who demonstrated that embedding the graph onto a unit sphere using an orthonormal representation improves generalization error, i.e., test set performance. However, Ando and Zhang did not provide a concrete method for selecting an optimal embedding. The current paper contributes by proposing a new optimization framework that jointly optimizes the embedding and the learned labels through the introduction of a regularization term. This regularization term, defined as the spectral norm of the graph kernel matrix, is motivated by insights derived from Theorem 1, which establishes an upper bound on the generalization error. Additional results in the paper provide an asymptotic upper bound on the generalization error of the proposed method, which exhibits more favorable scaling compared to prior approaches. This upper bound can also be interpreted as an upper bound on the labeled sample complexity. Although the proposed optimization problem can be formulated as a semidefinite program (SDP), solving SDPs is computationally expensive. To address this, the paper proposes using an inexact proximal method, which scales to graphs with thousands of vertices. Experimental results suggest that the proposed method outperforms existing approaches.
However, the reviewer was unable to fully comprehend the main result of the paper, specifically Theorem 4, at a high level. In the paragraph preceding Theorem 4, the paper states that "in the presence of unlabeled nodes, without any assumption on the data, it is impossible to learn labels. Following existing literature [1, 12], we assume an edge links similar instances." Yet, the term "similar" is not explicitly defined, nor is it included in the assumptions of the theorem. Furthermore, the reviewer could not identify any reference to or application of this assumption in the proof of the theorem. Despite this, the claim of the theorem can be approximately summarized, in the context of random graphs, as:
\[
er^{0-1}[\hat{y}] = O\left(\frac{1}{m} \sqrt{n^{3/4} + \log{\frac{1}{\delta}}}\right).
\]
As the reviewer interprets it, this implies that if the vertices of a large random graph are labeled with {0,1} at random, and a diminishing fraction of these labels, e.g., \(\omega(n^{3/4})\), is observed, the algorithm is guaranteed with high probability to recover the remaining labels with small error. However, due to the lack of clarity in the main result (see above), the reviewer is unable to provide a definitive assessment.