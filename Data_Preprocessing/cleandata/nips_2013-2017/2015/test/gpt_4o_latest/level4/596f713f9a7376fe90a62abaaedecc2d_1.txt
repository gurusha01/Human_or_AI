The core concept of the paper is as follows: Gaussian Mixture Models (GMMs) are a widely used prior for modeling natural image patches. A computational bottleneck in (typically iterative) inference algorithms that employ such models is the calculation of posterior mixture assignments—i.e., determining the specific component to which a given patch belongs. This step generally involves computing the relative likelihood of the patch for each component, which is computationally expensive due to the need to calculate the norm of the patch with respect to each component's covariance matrix.
To address this, the authors propose an alternative approach: learning a smaller gating network that predicts these assignments (more precisely, the posterior mixing probabilities) using a neural network with a single hidden layer containing a relatively small number of units. The network's weights are trained to minimize the KL divergence between its predictions and the true posterior distribution, using a training set of patches.
In essence, the authors suggest training a classifier to predict the component assignments. They demonstrate that this discriminatively-trained classifier can approximate the behavior of a posterior computed explicitly from a generative model, but with significantly reduced computational cost. Furthermore, since this method serves as a "drop-in" replacement for a computation step that depends solely on the prior, it can be trained once and reused across different tasks with varying likelihood terms. This characteristic offers an advantage over "end-to-end" discriminative training approaches, which require retraining for each specific task (e.g., in deblurring, such methods would need retraining for every blur kernel).
Overall, the paper presents a novel and intriguing idea that is well-implemented and reasonably evaluated. I believe this approach has the potential for applications beyond image restoration. While I am inclined to recommend acceptance, I would like the authors to address the following concerns in their rebuttal or final manuscript:
1. The discussion of computational complexity in terms of dot products is somewhat unclear. For example, in the traditional GMM case, the paper states that the first layer computes \(d \times K\) dot products, and the second layer computes \(K\) dot products. Here, the length of the vectors involved in the dot products is the same for both layers.
However, in the proposed gating network, this is not the case. The first layer computes \(N\) dot products of length \(d\), while the second layer computes \(K\) dot products of length \(N\). When the paper mentions in lines 241-243 that "we set the number of dot-products to be 100," it is unclear whether this refers to the total number of dot products or just those in the first layer (I suspect the latter—but in this case, note that the second layer now involves computing \(K\) dot products over much larger vectors).
The discussion would be much clearer if it were framed in terms of the number of multiplications and additions, and if the number of neurons in the hidden layer (i.e., the number of \(v_i\)'s) were explicitly stated.
2. I would have appreciated an evaluation (e.g., for denoising) using different numbers of mixture components. A key question is: how does the number of neurons in the hidden layer need to scale as the number of mixture components increases? Alternatively, how does the proposed method affect the accuracy-time trade-off when selecting the number of mixture components? 
In summary, the paper introduces a computationally efficient alternative for determining posterior mixture-component assignments during inference in image restoration tasks that use GMM priors for image patches.