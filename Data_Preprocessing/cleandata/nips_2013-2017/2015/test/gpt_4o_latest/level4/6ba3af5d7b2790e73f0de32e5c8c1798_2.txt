This paper introduces a system for action-conditioned video frame prediction, leveraging deep convolutional and recurrent neural networks. The authors propose two deep models for encoding video data: one using feedforward CNNs and the other employing a recurrent neural network. The system transforms frames based on the applied action using a novel multiplicative formulation, then decodes them back using up-convolution. The approach is trained on four Atari video games via the Atari emulator and evaluated for video prediction and control efficacy using the DQN algorithm.
Quality & Originality:  
The paper is technically sound and logically structured. While the proposed architectures are closely related to prior work, the novelty lies in the action-dependent transformation, which uses a multiplicative formulation instead of the commonly used additive fully connected layer. Other components, including the training methodology, are largely borrowed from existing literature. The system demonstrates strong performance, successfully predicting multiple future frames.
The experiments compare the two proposed variants against two naive baselines that do not account for actions. However, a more meaningful baseline would have been an additive fully connected action transformation layer, as it directly incorporates the effect of actions. The authors are encouraged to include this comparison in future work.
Additional experiments validate the system's utility for control using the DQN algorithm. The informed sampling strategy for DQN improves performance over state-of-the-art results, which is expected given a reasonably accurate generative model of the emulator. A useful sanity check would involve using the emulator itself as the generative model and comparing its performance against the learned model. Furthermore, the learned representations effectively separate the actions.
Clarity:  
The paper is well written and easy to follow. However, the figures require improvement, particularly Fig. 5.a, where differences between the methods are difficult to discern. Adding descriptive captions below the figures would provide better context for readers.
Significance:  
This work introduces a novel approach for learning action-conditioned representations for video prediction. It builds on the use of deep auto-encoders for representation learning in control tasks and is relevant to the NIPS community. However, the authors should consider citing related prior work, such as Boots et al., Learning Predictive Models of a Depth Camera & Manipulator from Raw Execution Traces (ICRA 2014), which also addresses action-conditioned prediction, albeit in simpler environments and over shorter timeframes.
Overall, the paper is concise, and the ideas are clearly presented. However, the contribution feels somewhat incremental, with limited theoretical advancements. Including additional experimental results and stronger baselines would enhance the paper's overall quality. I recommend a borderline accept.
Summary:  
This paper proposes two deep architectures for predicting sequences of video frames from the Atari emulator, conditioned on the agent's actions and prior frames. The system is evaluated on several Atari games and tested for control using the DQN algorithm. While the paper is well written and presents some experimental results, it comes across as incremental work. I recommend a borderline accept.