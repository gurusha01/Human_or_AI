The paper builds upon prior work on deep Poisson Factor Analysis (PFA) for topic modeling by introducing a Bernoulli-Poisson link in place of logistic functions. Additionally, it presents a method for jointly modeling documents and their associated discrete labels. Experimental results demonstrate that the proposed approach surpasses related baselines in terms of held-out perplexities and classification accuracy.
This work extends existing research, particularly that of Gan et al. (ICML 2015), by offering a more flexible mechanism to define the prior on documents' topic proportions. While the contributions are primarily a synthesis of existing ideas, the paper makes meaningful progress in leveraging deep models for topic modeling. Below are my detailed comments:
- It is noteworthy that both MCMC and variational inference methods are explored in the paper. Since scalability is cited as a key advantage of variational inference, it would be valuable to include a comparison of the runtime performance between these two inference techniques.
- The task of jointly modeling documents and their associated metadata is well-studied. I am curious why traditional supervised topic models, such as sLDA (for classification tasks), are not included as baselines for comparison in Section 5.
- For readers unfamiliar with standard deep model notations, I recommend adding a figure in Section 2 to visually depict the different layers and their input/output relationships.  
In summary, the paper extends and enhances prior work on deep Poisson Factor Analysis for topic modeling. While it primarily combines existing concepts, it introduces notable advancements in applying deep models to this domain.