This paper introduces novel upper and lower bounds on the determinant of matrices and operators, with applications to determinantal point processes (DPPs). The authors propose a creative use of pseudo-inputs to bound functionals of positive definite kernels, enabling computationally efficient likelihood-based inference methods for DPPs. The work addresses two key challenges in DPP inference: the intractability of spectral decomposition and the computational cost of determinant evaluation. By avoiding spectral methods, the proposed bounds are computationally cheaper and applicable to both finite and continuous domains. The problem is timely and relevant, with potential applications in diverse fields such as machine learning, spatial statistics, and physics.
Strengths:
1. Novelty and Impact: The proposed nonspectral bounds are a significant contribution to the field, offering a practical alternative to existing spectral methods. This innovation has the potential to advance inference techniques for DPPs and inspire further research on related problems.
2. Broad Applicability: The methods are not limited to DPPs but could extend to other areas where determinant evaluation is a bottleneck, making the contribution broadly relevant.
3. Experimental Validation: The experiments are well-designed, demonstrating both the strengths and limitations of the proposed methods. The diabetic neuropathy dataset application is particularly compelling, showcasing the real-world utility of the approach.
4. Clarity of Presentation: The paper is well-organized, with a clear exposition of the problem, methodology, and results. The use of pseudo-inputs is explained effectively, and the bounds are rigorously derived.
5. Future Directions: The discussion section outlines promising avenues for future work, such as exploring connections between kernels \(K\) and \(L\) without spectral knowledge and investigating quadrature-based bounds.
Weaknesses:
1. Limited Discussion on Parameterization of \(K\): While the focus on parameterizing \(L\) is justified for computational reasons, the paper does not sufficiently address the trade-offs in interpretability and potential limitations of this approach. A more detailed discussion would strengthen the paper.
2. Supplementary Material: The paper relies heavily on supplementary material for proofs and additional details. A more concise presentation of key proofs in the main text would improve accessibility.
3. Scalability: While the bounds are computationally cheaper than spectral methods, the scalability of the approach with respect to the number of pseudo-inputs \(m\) could be further analyzed, especially for large datasets.
Arguments for Acceptance:
- The paper addresses a challenging and relevant problem with a novel and impactful solution.
- The proposed methods are rigorously validated and have broad applicability.
- The work is well-aligned with the conference's focus on advancing machine learning and computational methods.
Arguments Against Acceptance:
- The interpretability trade-offs of parameterizing \(L\) instead of \(K\) are not fully explored.
- The reliance on supplementary material may hinder accessibility for some readers.
Suggestions for Improvement:
1. Include a more detailed discussion on the implications of parameterizing \(K\) versus \(L\).
2. Provide a standard supplementary document rather than an extended version of the paper to improve clarity.
3. Explore the scalability of the approach with larger datasets and higher-dimensional problems.
Conclusion:
This paper makes a high-quality scientific contribution by introducing novel bounds for determinant evaluation with significant implications for DPP inference. While minor concerns exist, they do not detract from the overall impact and relevance of the work. I recommend acceptance, with suggestions for addressing the noted weaknesses in a revised version.