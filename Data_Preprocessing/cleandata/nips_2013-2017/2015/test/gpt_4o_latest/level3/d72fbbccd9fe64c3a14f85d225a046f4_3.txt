The paper presents a novel deep architecture for topic modeling that builds upon Poisson Factor Analysis (PFA) modules, introducing a Bernoulli-Poisson link to enhance the model's flexibility and interpretability. The authors extend prior work, particularly Deep Poisson Factor Analysis (DPFA), by employing PFA modules across all layers, which contrasts with DPFA's reliance on Sigmoid Belief Networks (SBNs) in higher layers. This unified approach simplifies interpretation and improves computational efficiency, as the model scales with the number of non-zeros in the data rather than the total size. Additionally, the paper introduces a discriminative extension for classification tasks, demonstrating the versatility of the proposed framework.
Strengths:
1. Novelty and Contribution: The introduction of a Bernoulli-Poisson link and the consistent use of PFA modules across all layers represent a significant advancement over existing models like DPFA. This approach combines the interpretability of Dirichlet Process-based models with the expressive power of deep architectures.
2. Experimental Results: The model achieves state-of-the-art performance in held-out perplexity and classification accuracy across multiple datasets, including 20 Newsgroups, Reuters, and Wikipedia. The discriminative extension also outperforms traditional supervised methods and baselines like sLDA, RSM, and OSM.
3. Inference Techniques: The inclusion of both MCMC and Stochastic Variational Inference (SVI) methods is commendable, as it addresses scalability for large datasets. The focus on computational efficiency, particularly scaling with non-zero elements, is a practical advantage.
4. Real-World Application: The application to medical records demonstrates the model's utility in domains beyond traditional text corpora, showcasing its potential for broader impact.
Weaknesses:
1. Baseline Comparisons: While the model is compared against several deep and shallow baselines, the absence of traditional supervised topic models like sLDA in Section 5 is a notable omission. Including these would provide a more comprehensive evaluation.
2. Clarity and Accessibility: The paper assumes familiarity with advanced notations and deep topic modeling concepts. A figure illustrating the deep architecture, including input/output relationships, would significantly aid readers unfamiliar with these conventions.
3. Running Time Analysis: Although the paper highlights computational efficiency, a direct comparison of running times with baseline models (e.g., DPFA-SBN, DPFA-RBM) is missing. This would strengthen claims about scalability and efficiency.
4. Interpretability: While the model's interpretability is emphasized, more detailed qualitative analyses of learned topics and meta-topics, particularly in the context of real-world datasets like medical records, would enhance the paper's impact.
Recommendation:
The paper is a strong candidate for acceptance due to its technical rigor, novelty, and practical contributions. However, addressing the missing baselines (e.g., sLDA), providing a clearer architectural visualization, and including a detailed runtime comparison would further strengthen the submission.
Arguments for Acceptance:
- Significant improvement over state-of-the-art in perplexity and classification accuracy.
- Novel use of Bernoulli-Poisson links and consistent PFA modules across layers.
- Practical scalability with non-zero elements and applicability to diverse datasets.
Arguments Against Acceptance:
- Missing comparisons with traditional supervised topic models.
- Lack of clarity for readers unfamiliar with deep topic modeling notations.
- Limited qualitative analysis of learned topics and their interpretability.
Overall, the paper makes a substantial contribution to the field of topic modeling and is likely to inspire future research in deep probabilistic models.