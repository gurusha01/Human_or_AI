This paper presents a novel variational inference framework based on proximal algorithms, specifically leveraging the Kullback-Leibler (KL) divergence as the proximal term. The authors make two key contributions: (1) demonstrating the equivalence of their KL proximal-point algorithm to natural gradient-based variational inference methods, and (2) introducing a proximal-gradient algorithm tailored for non-conjugate models by splitting and linearizing difficult terms. The proposed approach is validated on real-world regression and classification datasets, showing comparable performance to existing methods while offering computational efficiency.
Strengths:
1. Technical Novelty: The paper provides a fresh perspective by interpreting natural gradient methods as proximal point algorithms and extending proximal-gradient methods to non-conjugate models. This is a significant contribution to the field of variational inference.
2. Practical Utility: The splitting and linearization approach simplifies inference in non-conjugate models, converting them into subproblems solvable using conjugate models. This is computationally efficient and broadly applicable.
3. Empirical Validation: The proposed methods are tested on real datasets for Bayesian logistic regression and Gaussian process models, demonstrating competitive performance and, in some cases, faster convergence compared to existing methods.
4. Clarity of Implementation: The derivation of efficient updates, particularly for high-dimensional settings, is well-detailed, making the method accessible for practical use.
Weaknesses:
1. Related Work: The paper lacks a discussion of related work by Theis and Hoffman on trust-regions for stochastic variational inference. Incorporating these references would provide a more comprehensive context for the contributions.
2. Transition Explanation: The transition from proximal point methods to proximal gradient methods is not sufficiently clear. A more intuitive explanation or illustrative example would enhance understanding.
3. Theoretical Insights: While the connection between mirror-descent and Bayes theorem-like updates (Eqs. 11-13) is intriguing, it is underexplored. A deeper discussion could strengthen the theoretical contribution.
4. Presentation Issues: The manuscript contains several typos, both grammatical and mathematical, which detract from its overall readability. Careful proofreading is necessary.
5. Limited Scope of Experiments: Although the experiments are well-executed, the datasets and models considered are somewhat limited. Broader benchmarking on diverse tasks would better demonstrate the generalizability of the approach.
Recommendation:
This paper is a strong and well-executed contribution to the field of variational inference, particularly for non-conjugate models. It addresses a relevant and challenging problem with a novel and computationally efficient solution. However, the authors should address the missing references, clarify the transition to proximal gradient methods, and expand the discussion of theoretical connections. Additionally, careful proofreading is required to fix the typos. With these improvements, the paper would be a valuable addition to the NIPS proceedings.
Arguments for Acceptance:
- Novel and theoretically grounded approach to variational inference.
- Demonstrated computational efficiency and practical utility.
- Strong empirical results on real-world datasets.
Arguments Against Acceptance:
- Missing references to related work.
- Insufficient clarity in some theoretical transitions.
- Presentation issues due to typos and limited experimental scope.
Overall, I recommend acceptance with minor revisions.