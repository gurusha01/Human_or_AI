The paper introduces a novel probabilistic deep architecture for modeling count data, leveraging Poisson Factor Analysis (PFA) modules, and extends it to multi-class classification tasks. The proposed Deep Poisson Factor Model (DPFM) is a significant advancement, combining the interpretability of Dirichlet Process (DP)-based models with the representational power of deep hierarchies. Unlike prior approaches such as DPFA, which mix PFA and Sigmoid Belief Networks (SBNs), this model exclusively uses PFA modules across all layers, yielding conceptual simplicity and computational efficiency. The authors also propose two inference techniques: a Markov Chain Monte Carlo (MCMC) method and a scalable Stochastic Variational Inference (SVI) approach. Empirical results demonstrate strong performance on large-scale datasets, including benchmark corpora and medical records, with notable gains in perplexity and classification accuracy.
Strengths:
1. Technical Quality: The paper is technically sound, with well-supported claims through theoretical derivations and empirical evaluations. The closed-form conditional posteriors and scalability of the inference methods are particularly compelling.
2. Clarity and Motivation: The model is well-motivated, with clear connections to prior work in topic modeling and deep generative models. Section 2.3, which interprets the hierarchical structure, is particularly engaging and enhances the reader's understanding.
3. Originality: The use of PFA modules throughout the deep architecture is a novel contribution, addressing limitations of prior models like DPFA and SBN-based approaches. The discriminative extension for multi-class classification is also a unique addition.
4. Significance: The results demonstrate state-of-the-art performance on perplexity and classification tasks, and the scalability of the model makes it applicable to large, sparse datasets. The application to medical records highlights its practical utility.
Weaknesses:
1. Scalability Claims: The paper's claim of scalability relative to reference [23] may be overstated, as the computational gains primarily arise from ignoring zeros via the Poisson likelihood. A more nuanced discussion is needed.
2. Comparative Analysis: While the model is compared to unsupervised methods, the evaluation would benefit from including a simple supervised baseline to contextualize the classification results.
3. Computational Efficiency: Although the model's scalability is promising, a direct comparison of computational efficiency with competing models like docNADE, LDA, and replicated softmax is missing.
4. Sparsity Clarification: The sparsity induced by the Dirichlet distribution depends on hyperparameter choices, which are not adequately discussed.
Pro/Con Arguments for Acceptance:
- Pro: The paper makes a substantial contribution to deep generative modeling, offering a scalable and interpretable architecture with strong empirical results.
- Con: The evaluation could be more comprehensive, particularly in terms of supervised baselines and computational comparisons.
Suggestions for Improvement:
1. Add a figure to clarify Equation 3, as this would enhance the reader's understanding of the hierarchical structure.
2. Provide a more detailed discussion of the scalability claims, particularly in relation to reference [23].
3. Include computational efficiency comparisons with competing models and a simple supervised baseline for classification tasks.
4. Clarify the impact of hyperparameter choices on the sparsity of the Dirichlet distribution.
Conclusion: This paper is a strong candidate for acceptance, as it advances the state of the art in deep generative models with a scalable and interpretable approach. Addressing the identified weaknesses would further strengthen its contribution.