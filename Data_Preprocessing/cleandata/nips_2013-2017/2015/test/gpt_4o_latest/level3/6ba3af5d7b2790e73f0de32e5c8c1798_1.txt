This paper addresses the challenging and impactful problem of learning a predictive model for Atari 2600 games, where future frames are conditioned on both previous frames and action inputs. The authors propose two novel deep neural network architectures—one using feedforward encoding and the other recurrent encoding—incorporating convolutional layers, multiplicative action-conditional transformations, and deconvolutional decoding. The paper is well-written, clearly structured, and supported by extensive experiments and qualitative demonstrations, including videos of predicted frames.
Strengths:  
The paper makes a significant contribution to the field of vision-based reinforcement learning (RL) by presenting architectures capable of generating long-term predictions (up to 100 steps) of high-dimensional video frames. The authors evaluate their models on both pixel accuracy and practical utility for control, demonstrating the relevance of their work to RL. The qualitative results, including videos, convincingly show the models' ability to predict complex object movements, object interactions, and global context. The controlled vs. uncontrolled dynamics analysis is particularly insightful, revealing that the models implicitly disentangle action-dependent and action-independent components of the scene. Additionally, the informed exploration strategy for RL demonstrates a practical application of the predictive models, improving DQN performance in some games. The paper also situates itself well within the existing literature, referencing relevant works such as DQN and prior video prediction methods.
Weaknesses:  
While the paper is strong overall, there are areas for improvement. First, the experimental details, particularly regarding the deconvolution layers and training procedures, are somewhat sparse. Providing more specifics would enhance reproducibility. Second, the baselines used for comparison are relatively weak, though the authors acknowledge the lack of obvious alternatives. Third, the paper does not explore predicting rewards alongside frames, which could enable planning-based applications and further strengthen the practical impact of the work. Additionally, the exploration section raises ambiguity about whether the predictive model is trained online or on pre-existing data. Minor issues include a typo ("predicing") and a questionable claim about randomness in Seaquest.
Pro and Con Arguments for Acceptance:  
Pro:  
- Novel architectures for action-conditional video prediction.  
- Strong experimental results, both qualitative and quantitative.  
- Practical relevance to RL through informed exploration and frame replacement.  
- Insightful analysis of learned representations.  
Con:  
- Limited experimental details on network architecture and training.  
- Weak baselines for comparison.  
- Missed opportunity to explore reward prediction.  
Conclusion:  
This paper is a high-quality contribution to the field of vision-based RL and action-conditional video prediction. Despite minor weaknesses, its strengths in novelty, clarity, and practical relevance outweigh the limitations. I recommend acceptance, with suggestions to improve experimental details and explore reward prediction in future work.