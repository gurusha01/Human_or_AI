The paper introduces a novel deep architecture for topic modeling based on Poisson Factor Analysis (PFA) modules, employing a Bernoulli-Poisson link to connect layers. This approach enables interpretable topic hierarchies and structured topic correlations across layers. The authors propose efficient inference methods using Markov Chain Monte Carlo (MCMC) and Stochastic Variational Inference (SVI), which scale with the number of non-zero elements in the data, making the model computationally efficient for sparse datasets. The work builds incrementally on Zhou et al. (2012) by removing the global beta-Bernoulli process and adopting the Bernoulli-Poisson link, simplifying inference and enhancing interpretability.
Strengths:  
The paper is technically sound and well-executed. The use of PFA modules throughout all layers ensures consistency and interpretability, a notable improvement over prior models like DPFA, which relied on Sigmoid Belief Networks (SBNs). The inference methods are conceptually straightforward and computationally efficient, particularly for large, sparse datasets. Experimental results demonstrate the model's superior performance over baseline methods such as LDA, FTM, and DPFA, particularly in perplexity and classification tasks. The discriminative extension of the model is a valuable contribution, showing significant improvements in document classification accuracy. The application to medical records highlights the model's versatility and potential for real-world impact. Overall, the work is elegant and likely to interest the deep learning and topic modeling communities.
Weaknesses:  
Despite its strengths, the paper has several limitations. The contributions are incremental, primarily building on Zhou et al. (2012) by refining existing ideas rather than introducing fundamentally new concepts. The experimental analysis, while thorough in comparing performance metrics, lacks deeper exploration of the model's structure and behavior. Key aspects such as layer widths, overdispersion handling, and depth-dependent layer width decay are not adequately discussed. The surprising result that the one-layer model outperforms ORSM and LDA is not explained, leaving readers with unanswered questions. Additionally, ORSM, a relevant baseline, is missing from Table 1, which undermines the completeness of the experimental comparisons. The choices for the network structure, such as the number of layers and hidden units, appear arbitrary and are not justified.
Pro and Con Arguments for Acceptance:  
Pros:  
1. Elegant and interpretable deep architecture for topic modeling.  
2. Computationally efficient inference methods suitable for large datasets.  
3. Strong empirical performance across multiple datasets and tasks.  
4. Discriminative extension demonstrates practical utility.  
Cons:  
1. Incremental contributions over prior work.  
2. Insufficient exploration of the model's internal structure and behavior.  
3. Missing baseline (ORSM) in experimental comparisons.  
4. Arbitrary design choices for network structure.  
Recommendation:  
While the contributions are incremental, the paper is well-executed and offers meaningful improvements in interpretability, scalability, and performance. It is likely to be of interest to the community and has potential for further extensions. I recommend acceptance, provided the authors address the missing baseline, clarify the surprising results, and offer more insights into the model's structural choices.