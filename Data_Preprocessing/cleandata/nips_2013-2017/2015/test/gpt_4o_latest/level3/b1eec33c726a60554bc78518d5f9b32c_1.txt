The paper investigates the impact of noise correlation in multi-output regression and provides a comparative analysis of three methods: Ordinary Least Squares (OLS), Maximum Likelihood Estimation (MLE), and the Alternating Minimization (AltMin) algorithm. The authors demonstrate that MLE and AltMin leverage noise correlation to improve performance, whereas OLS does not, leading to significant performance disparities in certain scenarios. The paper also delves into the theoretical underpinnings of these methods, with a focus on the non-convex optimization challenges of MLE and the iterative nature of AltMin. Tight theoretical bounds are provided for OLS and MLE, and the authors show that AltMin's upper bound matches MLE's, which they claim to be tight. Empirical results support the theoretical findings, though experiments are primarily conducted on highly correlated outputs (small ε).
Strengths:
1. Theoretical Rigor: The paper provides strong theoretical contributions, including tight bounds for OLS and MLE, and a detailed analysis of AltMin's performance. These results are valuable for understanding the role of noise correlation in multi-output regression.
2. Empirical Validation: The experiments align well with the theoretical claims, lending credibility to the results. The focus on highly correlated outputs highlights the scenarios where MLE and AltMin excel.
3. Novel Insights: The study offers new insights into the interplay between noise correlation and regression performance, particularly the advantages of MLE and AltMin over OLS in such settings.
4. Clarity of Contributions: The paper is well-organized, with a clear exposition of the problem, methods, and results. The authors also address minor typos and omissions in their update, reflecting attention to detail.
Weaknesses:
1. Limited Experimental Scope: The experiments primarily focus on small ε (highly correlated outputs). It remains unclear whether MLE and AltMin outperform OLS for larger ε values or under different noise structures. Expanding the experimental scope would strengthen the paper.
2. Assumption of Gaussian Noise: While the assumption simplifies analysis, it limits generalizability. Exploring non-Gaussian noise models would enhance the paper's applicability.
3. Non-Convex Optimization: The non-convex nature of MLE and the lack of global optimality guarantees for AltMin are practical limitations. While discussed, these issues could benefit from further exploration or mitigation strategies.
Recommendation:
Overall, the paper is a solid contribution to the field, with strong theoretical and empirical support for its claims. However, the limited experimental scope and restrictive assumptions slightly detract from its broader applicability. I recommend acceptance, provided the authors address the experimental limitations and discuss potential extensions to non-Gaussian noise models.