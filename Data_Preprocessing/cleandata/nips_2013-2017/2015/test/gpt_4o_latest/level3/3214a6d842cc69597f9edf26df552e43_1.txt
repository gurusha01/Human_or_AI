The paper proposes a novel variational inference method leveraging a proximal framework with the Kullback-Leibler (KL) divergence as the proximal term. The authors make two key contributions: (1) demonstrating the equivalence of their KL proximal-point algorithm to natural gradient methods for conjugate exponential-family models, and (2) introducing a proximal-gradient approach for non-conjugate models, where linearization is used to simplify optimization. The method is shown to convert non-conjugate inference problems into subproblems involving conjugate models, with applications to Bayesian logistic regression, Gaussian processes, and other latent variable models.
Strengths:
1. Novelty and Methodological Contribution: The paper introduces a proximal framework that unifies variational inference with natural gradient methods and extends it to non-conjugate models. The splitting and linearization approach is an interesting and potentially impactful idea.
2. Empirical Validation: The method demonstrates slightly improved predictive accuracy compared to standard inference methods on selected datasets. The computational efficiency, particularly for large-scale problems, is a notable advantage.
3. Clarity in Some Sections: The paper is generally well-written, with clear explanations of the proximal framework and its relationship to natural gradients.
Weaknesses:
1. Quality Concerns: A critical component of the method—linearization—has been flagged as unreliable in prior works (e.g., Honkela and Valpola, NIPS 2004). The paper does not adequately address these concerns or verify the accuracy of the linearization procedure. This omission raises questions about the robustness of the proposed approach.
2. Clarity Issues: Some important details are missing or ambiguous. For instance, the definition of \( f_n \) is absent, and Eqs. (2)-(3) require clarification regarding the use of \( \eta \) and the choice of \( \text{arg min} \) vs. \( \text{arg max} \).
3. Incomplete References: The paper does not sufficiently cite prior work on linearization in variational inference, which would provide better context and highlight the limitations of similar approaches.
4. Abstract and Scope: The abstract contains unsubstantiated claims, particularly in the last sentence, which should be revised. Additionally, the connection to stochastic variational inference (SVI) is unclear, as the proposed method appears to be a batch algorithm.
5. Significance: While the method shows promise, its reliability and broader applicability remain uncertain due to the unresolved issues with linearization and the limited empirical evaluation.
Arguments for Acceptance:
- The proximal framework is a novel and theoretically interesting contribution.
- The method shows potential for computational efficiency and scalability.
- The paper is relevant to the conference's focus on advancing variational inference techniques.
Arguments Against Acceptance:
- The reliability of the method is questionable due to the unaddressed weaknesses of linearization.
- Missing details and ambiguities hinder reproducibility and understanding.
- The empirical results, while promising, are limited in scope and do not convincingly demonstrate the method's superiority over existing approaches.
Recommendation:
While the paper presents an innovative approach, the unresolved concerns about linearization, clarity, and empirical robustness make it difficult to recommend acceptance in its current form. A major revision addressing these issues, particularly the reliability of linearization and the inclusion of missing details, would strengthen the paper significantly.