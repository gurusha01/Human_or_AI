This paper addresses a significant challenge in the learning of Determinantal Point Processes (DPPs): the intractability of the normalization constant in the likelihood function. The authors propose novel, nonspectral bounds on the likelihood that avoid the need for eigenvalue estimation, a key limitation of prior work such as [7]. These bounds enable efficient variational inference and exact Markov chain Monte Carlo (MCMC) methods, while also drawing connections to inducing-point methods in Gaussian processes. The proposed approach is particularly innovative in its use of pseudo-inputs to tighten the bounds, offering a computationally efficient alternative to spectral methods.
Strengths
The paper is technically strong and well-written, with a clear exposition of the problem and the proposed solution. The derivation of the nonspectral bounds is mathematically rigorous and represents a clever application of determinant inequalities. The authors demonstrate the utility of these bounds in both finite and continuous DPPs, extending their applicability beyond previous methods. The connection to inducing-point methods in Gaussian processes is insightful and may inspire further cross-pollination between these fields. Experimental results on synthetic and real-world datasets (e.g., the diabetic neuropathy dataset) validate the effectiveness of the proposed methods, showing competitive performance and practical utility. The ability to handle both variational inference and MCMC with the same framework is a notable contribution, as it provides flexibility depending on the computational budget and accuracy requirements.
Weaknesses
While the proposed method avoids the spectral estimation issues of [7], concerns remain about its computational efficiency and convergence guarantees. The optimization problem for the pseudo-inputs is non-convex, raising the risk of getting trapped in local minima, which could impact the reliability of MCMC steps. The variability in results for variational inference in the synthetic experiment highlights potential issues with parameter identifiability, particularly when directly parameterizing the kernel \(L\). Additionally, while the authors emphasize the computational efficiency of their approach, the scalability to very large datasets (e.g., with millions of points) remains unclear. Presentation issues also detract from the paper: Figure 1 lacks axis labels, the proof of the main proposition should be moved to the main body for accessibility, and minor text errors should be corrected.
Pro and Con Arguments for Acceptance
Pros:
- Introduces a novel, computationally efficient method for bounding the DPP likelihood.
- Avoids the eigenvalue estimation bottleneck of prior work.
- Demonstrates strong theoretical contributions and practical relevance.
- Connects to inducing-point methods, broadening the impact of the work.
Cons:
- Computational efficiency and scalability to very large datasets are not fully addressed.
- Convergence guarantees for the optimization problem are unclear.
- Presentation issues detract from the clarity of the paper.
Recommendation
Overall, this paper makes a significant contribution to the field of DPPs by addressing a longstanding computational bottleneck with an elegant and practical solution. While there are some concerns about scalability and optimization, these do not outweigh the strengths of the work. I recommend acceptance, provided the authors address the presentation issues and discuss the scalability and convergence concerns in more detail.