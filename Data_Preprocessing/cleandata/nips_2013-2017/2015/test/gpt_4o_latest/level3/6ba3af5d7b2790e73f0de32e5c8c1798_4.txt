This paper addresses the challenging problem of long-term, action-conditional video prediction in the context of Atari games, a domain characterized by high-dimensional input, complex object interactions, and partial observability. The authors propose two novel deep learning architectures: a feedforward network and a recurrent network with LSTM layers, both incorporating a multiplicative transformation layer conditioned on game actions. These architectures are designed to predict future frames by leveraging convolutional networks for encoding and decoding, with scalability ensured through a factorized weight tensor. The paper claims novelty in its ability to generate long-term, high-dimensional video predictions conditioned on control inputs, a significant step forward compared to prior work.
The paper is technically sound and well-supported by experimental results. Qualitative evaluations demonstrate that the proposed models generate visually plausible predictions over 100 steps in several Atari games, outperforming baseline methods. Quantitative analysis further confirms improvements in mean squared error over baselines. The authors also show that using predicted frames as input for a DQN agent yields reasonable performance, outperforming no-action predictors but underperforming real frames. Additionally, the system enhances exploration during agent training by prioritizing actions that lead to novel predicted frames, improving performance in some games. The analysis of learned representations, such as distinguishing controllable and uncontrollable pixels, adds depth to the study and highlights the interpretability of the models.
Strengths of the paper include its clear structure, rigorous experimentation, and contributions to both video prediction and reinforcement learning (RL). The proposed architectures are domain-independent, suggesting potential applicability to other vision-based RL problems. The use of curriculum learning for stabilizing multi-step predictions is a thoughtful addition, and the qualitative insights into model behavior are compelling.
However, the paper has a few weaknesses. While the authors claim novelty, some aspects of the work, such as action-conditional video prediction, have been explored in prior studies. The related work section could better contextualize these contributions by citing more recent advancements. Additionally, the models struggle with small objects and stochastic elements in games, which could limit their applicability in more complex environments. The quantitative gap between predicted and real frames in DQN performance also highlights room for improvement in prediction accuracy.
In summary, this paper makes a meaningful contribution to the field of action-conditional video prediction and its application in RL. Its strengths outweigh its weaknesses, and the results are likely to inspire further research in this area. I recommend acceptance after minor revisions to better position the work within the existing literature and address the limitations discussed.