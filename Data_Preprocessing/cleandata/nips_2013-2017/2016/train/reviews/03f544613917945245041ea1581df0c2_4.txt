This paper improves existing SG-MCMC by proposing a Richardson-Romberg scheme, such that it improves the rates of convergence in terms of bias and MSE. The scheme is easy to implement, which requires minimal modifications to traditional SG-MCMC. The paper is also supported by solid theoretical results. The paper proposes a Romberg-Richardson scheme that can improve the convergence rates of standard SG-MCMC algorithms, the method looks nice and theory looks solid. The most concerned part is about how the specific convergence rates are obtained from the theorem (as explained in the "explain fatal flaws" section. Except for this, I also have a practical concern about the matrix factorization experiment result. To be fair, if we run the proposed method on a single core computer, it would take 3 time longer than standard SG-MCMC because it has two chains and one of them uses two times more steps. If we take this into account, the plots in Figure 5 would not be fair, the time for SGLD should be multiplied by a factor of 2/3, which would shrink the difference between SGLD and SGRRLD. In Figure 3, the bias plot, it seems the best convergence is achieved by the red cure, which corresponds to \alpha=0.2, not 1/3 stated in the text, anything wrong? Also, the optimal curve looks fluctuating a lot, thus the rate would not match \alpha=0.2, any reasons why this happens? Also, it is stated in line 256 when stepsize is too small, the term 1/K\gamma would dominate. I wonder why this does not happen for SGLD? Some minor comments: Eq.1, why do you use a different symbol than \theta? line 94, RR not defined. line 131, \Gamma_K not defined. line 160, smooth function f, what kinds of smoothness? line 249: Figure 1(b) plots Bias vs. time, not dimension, should it be Figure 1(c)? Also, please use bold letters to represent vectors.