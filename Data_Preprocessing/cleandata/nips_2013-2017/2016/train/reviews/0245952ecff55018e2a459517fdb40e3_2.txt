The authors extend moment matching networks to conditional version. Unlike in models that can do update on single example and where the extension is trivial (feed conditional into the network) here it is non-trivial. That's because we need to match distributions of large number of samples (mini batch) to large number of inputs, but these distributions are not the same anymore (unless same conditional would be used for a given update which is inefficient). The main problem with the paper is that they start with fancy math instead of giving intuition behind this formula and they don't highlight what are the actual computations performed during this algorithm and their efficiency. The authors extend moment matching networks to conditional version. Unlike in models that can do update on single example and where the extension is trivial (feed conditional into the network) here it is non-trivial. That's because we need to match distributions of large number of samples (mini batch) to large number of inputs, but these distributions are not the same anymore (unless same conditional would be used for a given update which is inefficient). The main problem with the paper is that they don't explain the details behind the formula that is being optimized in the end (eq (2)), how efficient it is, what assumptions are made to make it work so that reader sees what is the actual computation performed. Equally they don't explain intuition behind it but instead start with formal math. The non-conditional version of the algorithm can be explained (and is in the original paper) without fancy math (e.g. reproducing kernel Hilbert space) - the formula that is minimize is intuitive. It would be good to come up with equally good intuitive explanation for the conditional case. It would be good to answer with the following questions: - How would you intuitively come up with formula (2). - What are the steps from formula (2) to the practical algorithm. Some of it is in the supplement but hard to follow.