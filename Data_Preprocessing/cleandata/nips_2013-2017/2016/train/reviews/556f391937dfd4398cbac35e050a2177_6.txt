This paper proposes an end-to-end deep feature embedding method by combining similarity metric learning and hard sample mining. The authors use the feature difference vector as well as mean vector to compute the similarity between two feature embeddings. For hard sample mining, they select the hard positive pair with the largest distance from the same class, and then select the sample with the smallest distance from the different classes as the hard negatives for each sample of the selected positive pair. The method achieves promising performance on fine-grained image retrieval, transfer learning and zero-shot learning. This paper is well-written and easy to read. The performance of the proposed method is promising on three vision tasks. My main concerns are as follows. 1. They use the feature mean vector to incorporate absolute position information in metric leaning. The idea is already introduced by Xiong et al. [35], which limits the novelty of the proposed PDDM method. 2. They claim they use a better local Euclidean distance metric for hard sample mining by selecting the most dissimilar positive pair and the most similar negative pairs in each batch. It is not efficient for learning since only 4 samples are used in a batch (e.g., with batch size of 64). More importantly, they do not evaluate the impact of the important parameter of the batch size in this method. 3. There are no discussions on how to choose the other algorithmic parameters. For example, no experimental analysis of lambda makes the effectiveness of the feature-level metric learning unclear. 4. Why will selecting the hardest samples not lead to bad local minima in this method since hard sample mining is performed throughout the learning process not just in early training iterations? It would be interesting to show the performance without hard sample mining in this method. 5. The method treats all the visual classes equal and unrelated without considering the semantic relationship among them. For example, cat and dog should be closer than cat and bicycle in the semantic space. I wonder whether the semantic information helps learn better feature embeddings in this method. 6. Typos: u1 and v1 should be u' and v' in Eq. 2.