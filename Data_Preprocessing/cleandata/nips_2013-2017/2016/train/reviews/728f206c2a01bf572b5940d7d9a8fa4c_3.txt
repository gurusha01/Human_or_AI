This paper proposes to train Boltzmann machines using a Wasserstein distance between data and model, rather than log likelihood. This is a neat idea, and is expressed clearly. Novel unsupervised training objectives are of potentially very high importance. I have concerns that, since the approach is sample based, it may scale particularly poorly with dimensionality and sample size. All experiments were on unusually small data, were heavily regularized with standard KL divergence, and only showed improvement in terms of Wasserstein-like measures of performance. eq 4: sum over "x, x'", rather than "xx'" eq 4: I struggled a lot with this equation. Especially, it seems that it is only defined for gamma > 0, and it's very difficult to understand how the last term induces the correct dependencies. More explanatory text, and a change to stated gamma bound, would be good. 67: alpha* hasn't been defined 92-93: I suspect this approach, of measuring the distance between two distributions using samples from the two distributions, will suffer from exactly the same problems as Parzen window estimates (ie, sec 3.5 in L. Theis, A. van den Oord, and M. Bethge, A note on the evaluation of generative models, 2016 ). Specifically -- you need a number of samples which is exponential in the dimensionality of the space for the estimate to be reasonable, and when there are too few samples there will be a strong bias of the model towards the mean of the data distribution. 115: So this method would not be suitable for minibatch-training? 121-122: Why are local minima more of a problem for the Wasserstein case than the KL case? This didn't make sense to me. 137-139: All of these datasets are unusually small. Can you talk about the scaling properties of this algorithm, both in terms of number of samples and problem dimensionality? 177-178: It seems that the clustering is a result of the entropy regularizer, as opposed to the Wasserstein distance itself. sec 4.3: OK -- but is this shrinkage a desirable property? Will this be much worse for higher dimensional problems? I suspect it will. sec 4.4: All the performance measures are with respect to Wasserstein or Hamming distance. =========================== post rebuttal =========================== Thank you for your response! It clarified a lot. I would encourage you to include discussion of scaling in the final paper. (re "When γ=0, we recover the usual OT dual constraints.", I think this should rather be in the limit as γ goes to 0?)