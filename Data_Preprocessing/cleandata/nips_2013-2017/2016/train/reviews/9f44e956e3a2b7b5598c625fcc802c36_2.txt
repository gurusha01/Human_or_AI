This paper proposes to use an additional recurrent weight matrix with fast synaptic weight dynamics as a non-selective short-term memory. It specifies a simple update rule for the fast weights with exponential decay. Experiments on a key-value paradigm, sequential MNIST and MultiPIE as well as a RL task highlight the efficiency of the proposed memory in sequence tasks that do not require selective forms of memory. UPDATE: Thanks to the authors for addressing my concerns in their rebuttal. With the suggested edits I am very happy to promote this paper for acceptance. --- The paper is very well written, the method is well described and experiments are fairly exhaustive. Still, the current version of the manuscript leaves me with the following questions: 1) This type of short-term memory is argued to be biological plausible. However, implementing the inner loop, eq. (2), would not be easy in a biological setting since Wh(t) would need to be cached during the update of the new hidden state, and because the new state might take many iterations to converge. Can the authors speculate on the biological mechanisms that might underly this mechanisms in real neural networks? Also, how sensitive is the mechanism to the number of iterations? 2) Line 136 refers to the Appendix, but I could not find any? 3) In the simple key-value task, how is the performance of the network evolving with the length of the sequence? 4) Since the memory is non-selective (in what it stores) I'd expect reduced performance in tasks where the network needs to learn to store only certain inputs and not others. This scenario is not really tested here (the RL task looks similar, but selectivity is trivially induced by the task structure). Is there a simple scenario in which LSTMs are clearly better then short-term memory? Such scenarios could be interesting in highlighting the costs and benefits of this new method. 5) Did the authors compare with the key-value memory by Facebook? It looks like the scenarios tested here should be much better covered by this type of memory then by LSTMs. 6) Line 114/115: there is a confusion (at least to the reader) between mini-batches and sequences 7) Is there code online for this network, or are the authors planning to make the code available? 8) Line 209-211: when you state "the results can be integrated with the partial results at the higher level by popping the previously result from the cache" then what do you refer to as "popping"? Is that manually implemented or do you expect the network to implement "popping" itself? If so, can you elaborate how the network can learn the latter (it is non-obvious since the memory is simply decaying, so to actively pop one would need to subtract the right hidden state, which in turn would not only eliminate the memory but also any additional information in the hidden unit activity).