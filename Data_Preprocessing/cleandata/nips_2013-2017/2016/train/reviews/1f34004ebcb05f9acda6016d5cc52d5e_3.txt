In this paper, the authors proposed two optimization methods to solve the supervised PageRank problem. Unlike other existing methods, this paper proposed a gradient-based method with theoretically convergence rate guarantee and can achieve a given accuracy. The proposed gradient-free method has a guaranteed loss function decrease value. These two methods do not require exact value of the objective function and have a estimate of the convergence rate, also the performance is better than state-of-the-art in terms of the ranking quality. The hyper-parameters used in both methods are provided. The data sets are not publicly available but the description is clear so it may not be hard to reproduce their results. The authors proposed two two-level optimization methods, namely gradient-based (GBN) and gradient-free (GFN) to solve the supervised PageRank problem. The lower level and upper level optimization is based on Nesterov and Nemirovski's work (ref:17). The proposed method does not require an exact value of the objective function and has proven estimate of the convergence rate given an accuracy. Therefore the proposed method can avoid computing the large matrix while still have a good result. From Section 2 to Section 5, the authors explained the details of their methods and included all the proofs in the supplementary. However, it may help a lot if the authors can provide a symbol table since there are too many symbols in their equations and it is really easy to lose track of them. For example in Line 139 and equation 3.1 the authors used $N$ and they did not mention the meaning of it (number of steps) until Line 193. Also, in the experiment section, the authors only compared their work with GBP regarding to the loss function instead of the ranking quality. Moreover, since this work is a generalization of ref:17, I think it would be good if the authors can compare with it directly so we can see if the performance is better.