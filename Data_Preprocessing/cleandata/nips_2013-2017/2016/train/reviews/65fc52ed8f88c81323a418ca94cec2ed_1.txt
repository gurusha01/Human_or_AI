The manuscript studies the task of unsupervised construction of features for similarity determination from exemplars. It proposes a new training paradigm and corresponding loss function [a differentiable version of a discrete clustering loss] wherein a CNN architecture is combined with clustering algorithms to allow it to circumvent the problem of highly imbalanced training data when using exemplars with large neural networks. They demonstrate the performance of the approach on three standard datasets: Olympic images, Pose estimation, and the Pascal object recognition dataset. The paper proposes an interesting new way of using CNNs to learn unsupervised features of similarity from exemplars. It develops a novel differentiable loss and uses clustering to break data into mutually distinct cliques that allow balancing of the data (so that the usual problem of a small number of exemplars versus many negative examples might be mitigated). In general the paper is well written and motivated and builds nicely on the trend of combining discrete optimization algorithms with deep networks for compelling effects. The results across several real-world datasets look promising, though a bit more could be done to strengthen these. One worry I did have: Why switch to a different initialization of features for the PASCAL dataset (i.e. from HOG-LDA to Wang et al.)? Do the HOG-LDA features not work well for initialization in this case? Can the proposed approach recover from such a bad initialization? Minor Issues: The phrase "gets corrupted" on line 41 seems an odd wording. Line 130 could perhaps use a citation for complete-linkage clustering. For PASCAL the results are reported only for k=5. It would be nice to know the figures for k=1,...,10. Is the 3% boost contingent on having k=5? Grammar/Spelling issues on lines: 2 -- there's something a bit funny with the tense here. 76 -- "extracting patch for..." 126 -- "but not to another due to..." 225 -- "by solve"