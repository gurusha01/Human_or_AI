The authors propose a method to generate multiple outputs by jointly learning an ensemble of deep networks that minimize the oracle loss, which is useful when interacting with users. The oracle experiments on three tasks demonstrate the effectiveness of the proposed method. This paper integrates deep network to the MCL paradigm and proposes a stochastic block gradient descent optimization method to minimize the oracle loss of MCL. The proposed optimization method can be easily implemented within the back-propagation learning framework and the whole model can be trained in an end-to-end fashion.  My concern is how important the initialization is? What if models are initialized with random values instead of a pre-trained network?  Authors do not explain much about the setting for comparison. Do authors use the same pre-trained network to baseline methods as sMCL for initialization?  Authors use the oracle accuracy as metric for evaluation. What about the diversity of ensemble members? I expect to see more analysis on this.  Besides, there is a work with very similar idea that authors are suggested to compare with. Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J. Crandall, Dhruv Batra, Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks. CoRR abs/1511.06314 (2015)