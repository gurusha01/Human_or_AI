The paper presents an end-to-end deep framework to learn the transferable feature representation for predicting the labels of target domain data-points. The key idea in their proposed method is the iterative optimization strategy, which runs two main components for adaptation and transduction in turn. The authors evaluate their algorithm on several unsupervised domain adaptation tasks and show the good performance on classification and recognition. The paper is well written and easy to follow. The authors contribute two interesting heuristics in their optimization strategy: cyclic consistency for domain shift and structured consistency for prediction. Based on my understanding, the former heuristic focuses on the alignment between source and target manifolds in the feature embedding space, and the latter optimizes to group target examples with the same label together. The optimization strategy looks reasonable and the experimental results outperform all state-of-the-art methods. As the authors state in the paper, their approach has one major issue of the inaccuracy of transduction during the initial stage of the algorithm. Two solutions have been proposed to solve this problem, but I still have the following questions for authors: 1) The structured consistency only considers to pull examples with different class labels apart from each other, but it does not optimize to push target data-points from the same classes to each other. Although the cyclic consistency optimizes to penalize two neighboring points of same or different labels, it just enforces in the similarity metric between source and target domain. Why not optimize the similarity metric of two neighboring points of same labels in same domain? 2) I cannot find how the parameters of $\thetas$ and $\thetat$ are initialized. Are they initialized randomly? Because the $\thetas$ and $\thetat$ are tightly coupled, it will lead to a noisy and even bad start. The authors should provide more details to display how such defective start can convergence to the optimal result, e.g. the accurate curve of the learned metric during the iterations. 3) I think it would be helpful if the authors can provide a convergence curve of the optimization loss. The number of the "maxiter" in Algorithm 1 is also welcome to offer. About spelling: 1) Line 153: the numerator $k'y(xi)$ should be $k{y'}(x_i)$