The paper provides an ensemble training algorithm for deep learners. It is unique in that only one learner of the ensemble is active for each training example unlike previous deep and classical ensembles. During training, the member with lowest loss wrt the true label is identified and updated with the gradient. Overall it is a simple idea that leads to surprisingly good experimental results and fixes some deficiencies of existing methods, thus I recommend for weak acceptance. (Contd. from summary) 1. The testing phase is not clearly explained, I suspect they use the ground truth and find the best member explained in line 159 in page 5. 2. In the figures and text they refer to 'independent ensembles' and 'regular ensembles'. (Denoted as 'Indp.' in figures and line 181 on page 5). I could not find explanation for this baseline. 3. The authors repeatedly point to `specialization` of ensemble members towards different output classes. However, the reassignment in training is on a per example basis. I feel this argument is not justified sufficiently. Can you incorporate specialization in the loss function? 4. Related to above, the authors did not discuss the very related work of diversity regularization in neural networks (e.g. Chen et.al). This technique is more rigorous but yet to be studied in the deep setting.