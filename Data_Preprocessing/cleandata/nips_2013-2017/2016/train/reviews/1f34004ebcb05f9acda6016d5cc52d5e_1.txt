This paper analyzes the convergence of a non-convex loss-minimization problem for learning the parameters of a general graph-based ranking model, that is defined by a random walk conducted by weights of nodes and edges, which are in turn defined by random walks defined by nodes' and edge's features. The optimization problem can not be solved by existing optimization methods which require exact values of the objective function. The proposed approach hence operates in two level. At the first level, a linearly convergent method is used to estimate an approximation to the stationary distribution of Markov random walk. This approach is validated among others and the authors show the value of the loss function can be approximated with any given precision. They also develop a gradient method for general constrained non-convex optimization problems using an inexact oracle, and prove its convergence to the stationary point of the problem. The contribution is in the adaptation of the approach to the case of constrained optimization problems when the value of the function can be calculated with some known precision. They prove the convergence of this method and exploit it on the second level of the proposed algorithm. The paper is well written and proofs seem to be correct though I did not go through all of them. My concern is that the main optimization points made here are adaptations of those proposed in "Yurii. Nesterov and Vladimir Spokoiny, Random Gradient-Free Minimization of Convex Functions, Foundations of Computational Mathematics, 2015, pp. 1â€“40", and the supervised pagerank algorithm has been also proposed previously. On a minor level, a conclusion is expected especially that there is place to write one. I would suggest to present the optimization problem in a more general context, showing the studied algorithm as to be a specific case. This would allow to better place the contribution with respect to the state-of-the-art.