The paper studies the convergence of the EM algorithm (and iterative heuristics for Maximum Likelihood Estimation) for learning mixtures of spherical Gaussians, and presents the following results that show non-convergence to global maxima: 1. Even when the mixture has 3 clusters that are very far apart and with infinite samples, algorithms like EM can get stuck in local optima. 2. For mixtures of k gaussians, random initialization for EM only succeeds with at most exp(-\Omega(k)) probability. 3. Gradient EM will not converge to strict saddle points generically; hence, bad local maxima are typically the issue. The paper presents a coherent set of results that show that EM does not converge to global maxima (even with infinite samples). However, I feel the results are not very surprising, and the techniques also follow expected lines. The main bad example is a mixture with 3 components: two of them are closer to each other, and the third is very far from both. They show that for the log-likelihood objective, there is a local maxima with one center near the first two components, and two centers near the far-off cluster. This is fairly expected behavior, particularly when the centers are separated by >> \sqrt{d}. Under this amount of separation, the clusters do not overlap and learning mixtures of spherical Gaussians becomes very similar in flavor to k-means. Similar examples provide bad instances of Lloyd's algorithm for k-means clustering. This example is extended for k components with a nice recursive construction -- this requires some effort, but again somewhat expected. Similarly, random initialization fails for similar reasons as for k-means clustering with the same failure probability (for k-means this serves as the motivation to pick k logk centers, or distance-squared sampling). To conclude, while the results tell a coherent story about the non-convergence to global optima, I feel this is along somewhat expected lines. Comments: 1. Kumar-Kannan provides convergence guarantees for Lloyd's heuristic for k-means clustering (and hence mixtures of Gaussians under sufficient separations conditions). While EM is different from Lloyd's heuristic when the Gaussians are not sufficiently separated (as in Balakrishnan-Wainwright-Yu), they seem essentially when there is large separation (this is true of the bad examples here). So, I think a comparison with k-means (and Lloyd's heuristic) that also contrasts this result would be very useful.