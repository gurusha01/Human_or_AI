This paper introduces the Phased LSTM, a structure which is meant to handle irregularly sampled data through the introduction of a new time gate. The authors test PLSTMs on 4 datasets, showing improvement over standard LSTMs. In the case of the N-MINST and Lip Reading datasets, an improvement is achieved over the state-of-the-art. Overall, this seems like a great paper. There is a sufficient degree of novelty in the new cell structure and its usefulness has been proven. Given the prevalence of time series data and to learn representations even when this data is not cleanly sampled, I believe this technique could have high impact. The paper is also nicely written and the presentation is clear. Minor questions/comments: How many samples were used to learn the models in the frequency discrimination task and how does accuracy change with added data? Could Phased LSTMs be stacked to further improve performance? In Figure 4, please use the same color for Phased LSTM in the two plots. It would be useful to have an image of the deep learning structures used in the experiments. If this doesn't fit in the main paper, an appendix or extended version should be provided.