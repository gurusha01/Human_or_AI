This paper presents an interesting approximation to make the information bottleneck method practical in high dimensional scenarios, which is illustrated with experimental examples. Moreover, since this technique depends on scalar products, authors propose a nonlinear generalization based on kernels. As opposed to other kernel-based feature extraction methods, the proposed kernel-information-bottleneck method has the advantage of getting an intermediate representation where features may be visualized. I think this successful combination (simplified information bottleneck and kernel generalization) are worth publishing at NIPS! After a convincing exposition of the proposed simplification of the information bottleneck and its kernel-based nonlinear extension, the authors make an interesing point in the discussion: as opposed to other feature extraction techniques, which usually depend only on the statistics of the input signal, "x", their information-bottleneck features also depend on the relevant signal "y". It would be interesting to see examples of the practical effect of such connection: how the sparse features (e.g. Gabor-like edge detectors in natural images) change if the signals to retain information from are a specific class of images (e.g. faces...)?. May this be connected with top-down adaptation of early vision mechanisms?