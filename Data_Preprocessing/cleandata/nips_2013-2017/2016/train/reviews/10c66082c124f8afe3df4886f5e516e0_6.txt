The paper proposes a new method for metric learning for textual documents, building upon previous work known as "Word Mover's Distance". The original is unsupervised, measuring the distance between the embeddings of words. This paper extends this to the supervised setting, where the supervision is of the type 'these documents belong to the same class'. Code (matlab) is promised to be made available. [I appreciated the careful responses of the authors which clarify my questions] The authors propose two things to learn the distance: one is a linear transformation A (the traditional approach), and the other is the word importance weights w. I would have liked to understand better the intuition behind Eq (5) (which defines the usage of w), and understanding which of both parts is responsible for the reported improvements. In the experimental sections, the method is evaluated in a classification task. I understand that evaluating a distance metric is not easy, but - just for comparison - it would be interesting to see how well a simple classifier does on the same datasets. [these are now irrelevant, as clarified by the authors] Questions: I did not understand when Eq 2 is used.The lambda there seems to be different than the lambda in ln170 (?) What are your arguments for the claim "which is particularly forgiving when two documents use syntactically different but conceptually similar words" (ln316)? Other comments: ln257: "on the right we also show the average error across datasets". I couldn't find this, is this supposed to be in Table 3? Table 2: what is the difference between "n" and "ne"? I couldn't find mention on how k (for kNN classification) is defined. Is this data-set or method dependent?