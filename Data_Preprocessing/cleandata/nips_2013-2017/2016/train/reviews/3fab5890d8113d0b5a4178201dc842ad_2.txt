Most proposed neural network models augmented with memory scale poorly in both space and time as the amount of memory grows. The authors present a memory access scheme called Sparse Access Memory, and show that it retains the representational power while training efficiently, being 1000x faster and 3000x memory efficient. I do not think the paper is technically strong enough for acceptance. Efficient backpropagation and approximation nearest neighbour using sparse access memory form the main technical parts but are quite straightforward. Therefore, even though the experiments look interesting, I am not in the favour of acceptance.