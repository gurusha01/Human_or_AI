This work provides an information-theoretic bound on budge-fidelity tradeoff for label crowdsourcing using rate-distortion theory and channel coding theorem. The information-theoretic bound is well derived by this paper. If this is the first attempt that applied rate-distortion theory and channel coding theorem to the crowdsourcing problem, it can be a reasonably important contribution to the field. However, the novelty and practical values of this paper are still questionable, as the derivation of the information-theoretic bound is relatively straightforward and the bound has a risk of being very loose. The main problem is that unlike the channel coding problem, where we have a large amount of freedom in designing a codebook or codewords (we can use repetition code, Hamming code, convolutional code, RS code, low density parity check (LDPC) code), it is not clear whether we have such freedom in designing the sequence of channel input u^n, In equation (3), almost all mappings from B(X)^n to U^(\sum mi) (except degenerate ones) should be valid or implementable codebooks, otherwise the channel capacity may be a loose bound. Obviously for crowdsourcing, something like repetition code is possible where the natural decoding method is majority voting. However, beyond this, it is unclear how we can design u^n to improve the error correcting capability. In particular, there is striking difference between channel coding problem and the crowdsourcing problem in that we cannot assign the same query to the same worker multiple times as this will ruin the iid assumption in equation (1) or (2), as the same worker will probably generate the same answer to the same query, whereas in communication channel we can assume the channel is iid and same query can be used multiple times for the same channel. This is just one example, and in general, it is very likely that the crowdsourcing problem has lots of fundamental constraints in equation (3) unlike channel coding, where an encoding mapping {0,1}^{nR} -> {0,1}^n has little constraint. Therefore, there should be important constraints on the set of possible encoding schemes in equation (3), and due to such constraints the proposed information-theoretic bound has a risk of being very loose. Finally, it would be better to compare the performances of state-of-the-art methods with the derived information-theoretic bound. Minor comments: - In (8) min is over C, C' with E[d(Z,\hat{Z}) ] \leq D^t - Proof for theorem 4 can be more elaborated. - In theorem 4, ln(1-q) < 0. Is \epsilonS larger than \hat{\epsilon} ? - In (17), the reason why we have H_M(E(\epsilon)) should be explained or it needs some citation of the previous work.