This paper presents a new recurrent neural network architecture which contains 'fast weights,' that is, parameters which change on a timescale intermediate between the fast fluctuations of hidden activity patterns and the slow changes of weights due to gradient descent. These fast weights are automatically updated with a Hebbian learning rule, thereby embedding an associative memory automatically into the dynamics of the recurrent neural network. Experiments show that this scheme can rapidly learn a variety of (mostly visual) tasks more quickly than LSTMs or LSTMs augmented with external memories; and often obtains better error as well. Major comments: This paper contains a nice idea, namely, a weight matrix which is architecturally constrained to use a certain learning rule and update itself at various points during processing. This general scheme seems likely to lead to many variants in the future. The performance on the tasks considered is solid, and makes the technique worthy of further consideration. This paper makes a solid contribution to machine learning, but the results in the paper do not support the claim in the conclusion that "the main contribution is to computational neuroscience and cognitive science." The paper makes no contact with experimental data, whether neural or psychological. It engages with only a small subset of the relevant computational neuroscience or cognitive science literature (see, eg, Buonomano D.V. and Maass W. State-dependent Computations: Spatiotemporal Processing in Cortical Networks. Nat. Rev. Neurosci. 10:113-125, 2009; or the extensive literature on neural nets and recursive processing in human linguistic abilities, eg, Prince, A., & Smolensky, P. (1997). Optimality: From Neural Networks to Universal Grammar. Science, 275(1997), 1604–1610). It is plausible that this paper could contribute to understanding how (quasi)recursive computations might be implementable in the brain, and this is an extremely worthy goal, but much more work is required to substantiate this claim. If the claim is that fast weights enable recursive, compositional processing, then a task where this is a key component would be the most convincing grounds for a demonstration. At a minimum, an application to the domain of natural language processing would improve the paper greatly—but to address cognitive concerns, this must demonstrate more than merely good performance. It must reproduce the patterns of errors, for instance, as made by human subjects on complex 'garden path' sentences, etc. As it stands, the paper is squarely in the engineering tradition, with 'success' defined by improved performance on benchmark tasks. From a cognitive perspective, we don't obtain multi scale views of objects and their parts—we cannot 'zoom in' somehow with our eyes, only with our attention. I found it difficult to follow exactly what each model for each experiment was. The figures depicting different architecture variants are subtly different (in Fig 1, 'sustained' transitions are represented by red arrows; in Fig 2, 'integration' transitions are represented by red arrows). The 'integration transition' is not clearly defined. Minor comments: Table 3: classification accuracy? Few details of the RL agent and the MNIST experiments are given. The paper mentions an 'appendix' but none was available in the submission material. This could greatly aid the reproducibility of the results (but I think more details can also be placed in the main text). Cite the 'asynchronous advantage actor-critic method'.