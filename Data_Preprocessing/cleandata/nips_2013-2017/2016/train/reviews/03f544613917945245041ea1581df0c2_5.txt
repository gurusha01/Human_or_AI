To reduce the bias of SG-MCMC, the authors propose a novel method called stochastic gradient Richardson-Romberg MCMC. This method employs Richardson-Romberg extrapolation, an off-the-shelf tool for SDE. In particular, two chains are run embarrassingly parallel in SG-RR-MCMC without communication. The theoretical analysis is provided, showing that SG-RR-MCMC is asymptotically consistent. Empirical results are also satisfactory. The idea is interesting and proved to be effective according to empirical results. It can be seen as a simple combination of Richardson-Romberg extrapolation and SGLD. The efficiency can be validated theoretically via providing a better convergence rate, though this can be reached using high-order integrator (unfortunately, the authors didn't provide a comparison with the integrator in experiment). However, there exists inconsistent statement about the step size as mentioned above. Furthermore, Algorithm 1 (in supplement) is not consistent with Equation (5).