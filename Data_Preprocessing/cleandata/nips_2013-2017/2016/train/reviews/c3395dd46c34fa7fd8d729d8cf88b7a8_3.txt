Proposes a new model called Cooperative Inverse Reinforcement Learning for modeling settings where a human and robot act and both agents try to maximize the human's reward but the robot does not observe the human's reward initially and must infer it, giving incentive to human to take actions that may sacrifice immediate reward in order to be more informative to the robot. They present a novel model that seems like it could potentially have practical impact. There is theoretical and experimental evaluation. The theoretical results did not seem particularly deep, and I think the main value of the contribution rests on how realistic/important the new conceptual model is for modeling realistic scenarios. I would start with a motivating example much earlier, provide more intuition for why it is important, and describe important real-world scenarios that it exemplifies. The first example is not until page 6 line 238. It would be helpful to have more discussion for why this game is interesting (beyond just the description of it and analysis), and how modeling it in just IRL setting would have changed the analysis. I think more discussion and intuition could also be provided for the 313-320 example. It isn't really clear to me what is happening: the human is trying to navigate a grid and the robot is trying to help it? The performance improvement of using the new best-response approach over the prior expert demonstration approach for this example seems quite significant. The discussion of prior and related work is very comprehensive. King Midas analogy could be elaborated on, since not all readers might be familiar. They mention the possibility of multiple optimal policies in footnote 1 and say they defer it to future work, but I'd like to see a little more discussion. Two-player general sum games can have multiple Nash equilibia (each with potentially different values to the players), and I'd like at least some justification for why it is the right solution concept for this setting. In 4.1 I'd avoid using bold H for the human agent and non-bold H for time horizon (T may be better).