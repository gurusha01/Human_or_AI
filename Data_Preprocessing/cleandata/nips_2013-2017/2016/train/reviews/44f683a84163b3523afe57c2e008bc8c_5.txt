This paper tackles the problem of 3D object generation. The method involves building a generative deep network, that is trained using the adversarial signal obtained from a counterpart discriminative deep network. GANs have shown to perform very well on generating 2D images, but this paper goes on to show that adversarial training can help in 3D object synthesis as well. The second extension that the paper proposes is to extend VAE-GANs to 3D, which the authors call VAE-VAN. The VAE-VAN model has an additional encoder network that encodes a 2D image into a latent representation, and the generative network which then reconstructs a 3D object model of the object present in the 2D image from the encoded latent representation. Experiments show that the proposed models are able to synthesize objects in 3D that look better than the state of the art, qualitatively. There are also quantitative experiments that show that the methods perform better than existing approaches. This reviewer feels that the methods in the paper extend well-known techniques from 2D image generation to 3D object generation. Adversarial training and VAE-GANs have been proposed by others in the community. This work uses these two existing techniques, extend them to 3D synthesis, and show that they perform well. So, this work's contribution lies in the extension of the existing techniques to 3D, which is not trivial to do, and at the same time, not significantly novel. However, I do feel that the work is in the right direction and is of interest to the NIPS community. The reviewer does have some questions that should be made clear by the authors. I am a bit confused by the experimental details with regards to 3d object classification and single image 3d image reconstruction. Line 172 says that different VANs were trained for different object categories. If so, which discriminator is used to extract the representations (L193) for classification. Are separate VANs trained for the experiments in Sec 4.2 or is it a single VAN? In either case, what kind of classifiers are used to make the decisions (nearest neighbor, multi-class, etc.)? I have similar questions with the experiments in Sec 4.3? How many VAE-VANs are trained on the IKEA dataset? If there are multiple models trained (one for each object category), how are the models tested on novel test images? Also more analysis is required on understanding what each neuron in the discriminator responds to. From Fig 8. there seem to be multiple neurons that get activated by the same object part. The neurons corresponding to top-right and bottom-left both like left sides of chairs. I would like to see more experimental details to clarify these issues and to enable easy reproducibility.