The authors prove that all local minimum must be global minimum for a noiseless symmetric matrix completion problem. Unlike previous works that requires good initializations, this paper shows that starting from random initialization gradient descent algorithm converges to the global minimum. Extension to noisy setting is also considered in the appendix. The primary theoretical results are strong contributions and technically sound. The technical part is well-written, and all the proof strategies in the main body is easy to read. The result seems to be the first that shows all local minimums are global minimums for matrix completion problems. This opens the direction for other researchers to think about similar geometric properties in other nonconvex machine learning problems. One potential limitation is that the results only applies to the case where the completion matrix is symmetric. This restriction makes the theory less practical. That being said, I still think the results in the paper is a major step toward understanding the geometry of matrix completion problem. The paper is well-written. The organization structure is exceptional, making the paper easy to read. Although not understanding all the details in the proof, I can easily follow the proof strategies for the rank-1 case.