The process of training recurrent neural network models can be quite hungry for memory. With sequences that aren't necessarily of fixed-length, the memory allocation can vary depending on the training sample. Traditionally, RNN/LSTM models have been trained with "full memoization" -- that is, during forward-propagation, all states through time are stored to be reused during backpropagation. If this is too expensive, "Chen's algorithm" uses less memory (but far more computation) by recomputing each hidden state during backpropagation. In this paper, the authors expose a "slider" in resource usage from "high computation / low memory" to "low computation / high memory." This slider works by choosing how many states to memoize (1 of every 3 states, spaced evenly ... or 1 out of 4 states, etc). I have indeed heard complaints from LSTM researchers about memory usage, and this seems like a good solution. So far as I can tell, the authors do not evaluate their results on any dataset, nor do they train a real RNN/LSTM model to convergence. (Here, I use "RNN" as a shorthand for "RNN, LSTM, and related recurrent models.") The authors are solving an important problem. RNN training procedures can be greedy for memory. And, given the sequential nature, it's not trivial to simply to scale the training of each sequence over many machines. As a result, it's important to judiciously use memory and computational resources to train RNNs efficiently. I'm pleased to see the authors not only proposing a new instance of a solution, but to provide a user-selectable tradeoff between the quantity of computation and the memory usage. Real Experiments. I think the authors should really have an experimental result on a real dataset. - The short version: I can't give above a "2" rating for "experimental methods are appropriate" because... there aren't really any experiments! - The long version: RNN models have been applied to a variety of sequence-oriented domains, including automatic classification/understanding of text, audio, and video data. In most of these problem domains, many authors have released code for their RNN-based solutions. My "not-an-RNN-expert" colleague was able to download code, install it, and launch a training run of an RNN for UCF-101 video activity recognition over a two-day period. With all of that in mind.... I really think the authors should use their method to train an actual RNNon an actual dataset and to report results! With the same RNN architecture, the same training protocol, and the same random seed: I think the results (trained model) should be numerically equivalent to "traditional" RNN training with full memoization. And, the authors would be able to state actual numbers for memory savings and increased computational requirements. - If the authors can run such an experiment and deliver sane results, I would give a higher score for Technical Quality. Motivation. The paper would be stronger if you'd say, for example, "In [some application], we've seen that an RNN needs 1000GB of memory, and this just isn't feasible! Here's now we save >100x memory with a modest increase in computational footprint."