CMA-ES is the state-of-the art stochastic/randomized algorithm for (derivative-free) optimization. It combines different learning mechanisms for adapting the parameters of a multivariate normal sampling distribution that can be equivalently seen as adaptation of an underlying metric. This paper proposes a new variant for the update of the covariance matrix based on triangular Cholesky factors that has the advantage compared to previous updates to significantly reduce the complexity of the update as well as the memory needed to store the state parameters of the algorithm. The new method is carefully evaluated numerically. It is shown that the new update does not deteriorate the performance of the standard CMA-ES in terms of number of function evaluations to reach a given target while it significantly reduce the wall-clock time of the algorithm, particularly for large dimensions. I think that this paper is a very significant contribution in the domain of optimization. I expect that this variant of CMA-ES can become a default variant of the method in the context of medium/large-scale as it has several clear advantages over the current default method: namely reduced memory needed, reduced complexity and getting for free the eigenvalues of the covariance matrix. The paper is nicely written and the approach carefully evaluated. It is nice to see this contribution at NIPS given that the CMA-ES is strongly connected to machine learning, information geometry and is also used in reinforcement learning or supervised learning. Minor: explain how you ensure that the different implementations used are comparable in terms of CPU (is it the same implementation language, which one, â€¦) When commenting on Figure 2, it is written "the higher the dimensionality, the more pronounced the differences", it does not seem to be the case for Cigar and Discus in dimension 256. It would be good to provide an explanation for that.