This paper describes an information theoretic feature selection algorithm which approximates a high order mutual information by decomposing categorical variables into binary variables. It further decomposes the high order mutual information into a search over two sets, one which finds the best complementary subset (in terms of increasing the information), and another which finds the subset that contains most of that information. Major comments: Statement 1 and the associated proof is unnecessary. When t & s are greater than the number of selected features then the maximum score that can be assigned to a variable is the mutual information between it combined with all the selected features against the target. Either all variables end up in H or they end up in G depending on how the search is done. This score is then rank equivalent to the CMI score. The behaviour of this criterion is only interesting when it's actually approximating the CMI (i.e. when t & s < i). The interplay between the max and min steps is very interesting. I would expect that most of the time G covers H, as this would minimise the information, but in the case of complementary features then it becomes very dependent on the search procedure. The use of the binary representations blows up the search space and allows the algorithm to tease out interesting interactions. The complexity analysis of the competitor techniques is strictly correct that they increase in computation as i increases, but this is only with a naive implementation. With a memoized implementation (e.g. in the FEAST toolbox used for the experimental study) there are O(d) mutual information calculations in each iteration so the computation does not grow over time, with O(kd) memory required and O(kd) mutual information calculations total. In CMICOT it doesn't appear that these information calculations can be memoized (as the binary features chosen can change) so the computational complexity will grow over time as discussed in the paper. The experimental study lacks a few details (k in k-NN, which multiclass Adaboost is used, what is the base learner for Adaboost and how many ensemble members). The notation in portions of the supplementary material is unclear. For example in the equation above line 21 each of the Gs is a different subset, so they should use different letters (or G' etc). Much of the detail relies on the fact that H and G can have a large intersection (or even H \cup G) which essentially removes H from the mutual information. Minor comments: The proof of proposition 3 doesn't define a simple operation. Calculating a joint entropy requires iterating over all N datapoints and then over all the states of the joint variable, but this gives complexity O(N + |X|) rather than O(Nm) where |X| is the number of states. Separating out the results for poker, ranking and semeion would allow the reader to separate the binary approximation from the max/min criterion. Does the binary approximation improve performance by adding some noise or reduce performance by limiting the amount of information as compared to JMI or RelaxMRMR? With separate results for binary features vs non-binary features it might be possible to answer that question. As part of this it would be interesting to run all the other algorithms on the binary expanded versions of the datasets to see how that affected the performance.