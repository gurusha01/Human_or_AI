The authors study robustness and consistency of the Robust k-means (RKM) objective. Using the framework of the universal breakdown point, the authors show that RKM is not robust! To be precise, the universal breakdown point of RKM for unbiased proximal map is 2/n, and for the biased one it is 1/n. In other words, even 2 outliers are enough to breakdown some centers (that is, move them arbitrarily far). However, they contrast this by showing that RKM is robust on well-clustered data, if we use the notion of (\rho1, \rho2)-balanced data sets by Ben-David and Haghtalab (ICML'14). The authors also show that the consistency property of the usual k-means remains valid even for the more general RKM. The paper is well-written, the problem is well-motivated. The results are not spectacular but should still be of interest to part of the community working on clustering. The experiments are good but I did not pay a great attention to them as the main contribution of this paper is theoretical. Study of robust variants of clustering problems (and k-means in particular) that incorporate outliers has potential for big impact. I would recommend acceptance. Minor typos etc. Line 23 -- when "measured" according