The authors argue that in the n >> p settings GLMs parameters can be approximately estimated by a scaled OLS estimator. The authors present an algorithm that exploits this phenomenon and demonstrate experimentally that it can estimate GLM parameters faster than a number of alternative algorithms and in a number of GLMs/datasets. Additionally, the authors provide theoretical arguments that this phenomenon extends beyond simple Gaussian designs. Finding efficient surrogates to large-scale optimization problems is an important problem and the authors point out an elegant method for estimating GLMs when n >> p. As acknowledged by the authors the fundamental ideas relating to the Gaussian case are not new, but the extension to non-Gaussian designs is valuable and interesting. Overall, I think the paper is well-written and clearly organized. Main feedback: - The results in Figure 1 should be extended with a plot of time vs accuracy for the two methods, as done in Figure 2. It is clear that solving the SLS optimization problem should be faster than the corresponding GLM (e.g., Figure 1a), and that at the same time we encur a cost in accuracy (e.g., Figure 1b). - The inequality of Theorem 1 does not appear to be tight. I.e., it doesn't recover the result of Proposition 1 in the special case when the covariates are Gaussian. It would be helpful if the authors could comment on this gap. - It isn't entirely clear when the authors use |S| = n (i.e., in Figure 1, I presume) and when |S| \leq n. E.g., what was the choice of |S| in the experiments of Fig 2, Table 1? Minor: - Line 159: Hayley -> Halley - Proposition 2 and Theorem 2: The authors didn't define \lambda_min in the main paper.