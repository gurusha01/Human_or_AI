Ordinary recurrent neural networks have 2 types of variables: synapses that are updated at the end of the input sequence and capture regularities among inputs, and neural activities that represent short term memory. In this paper a second type of synapses is considered, called "fast weights". They change slower than neural activities but faster than usual synapses (called "slow weights"). This idea is motivated by physiological evidence that synapses in brains modulate at different time scales. The "fast weights" are used in an associative network to store memories. The update rule for the fast weights is a Hebbian-like learning rule to remember the past hidden activities. This associative network acts like an attention to the recent past, in a way similar to previously studied attention mechanisms that have been used recently to improve sequence-to-sequence RNNs. The difference here is that the strength of the attention to a given past hidden activity is not specified by a new set of parameters, but by the scalar product of that past hidden activity and the current hidden activity. The effectiveness of the proposed algorithm is shown on a variety of tasks. Arguments from math (memory capacity of different models) and neuroscience (different time scales for synaptic plasticity) are given to support the idea of introducing fast weights. The proposed algorithm establishes links with previous work on memory mechanisms (e.g. NTM and Memory Networks) and attention mechanisms, while being clearly much more biologically plausible than all previous models. This looks like an important step towards bridging the gap with computational neuroscience and cognition. From computational point of view, there is a trick to avoid computing the full fast weight matrix A. As explained on lines 100-104, it is sufficient to store the hidden activities. This makes the algorithm computationally much more efficient. Moreover, thanks to this trick, the algorithm also applies to mini-batches (lines 111-115) One downside: As far as I could see, very few details are given regarding how the slow weights W and C are trained (it's only mentioned that the Adam optimizer is used). I think this would deserve a bit more explanations. I think it would be beneficial to add a detailed figure for the computational graph (more detailed than figure 1) to show how exactly automatic differentiation is done. The fact that there is an "inner loop" (for the computation of h_s(t) ) makes it less usual and harder for the reader to visualize, I feel. One question in particular: do we backpropagate the gradients through A(t) (which is a function of the h(tau), and thus a function of W and C), or are these variables considered to be constant when computing the gradients in the computational graph? Other minor remark: typo line 287 : "units"