The paper introduces Sparse Access Memory architecture for neural nets augmented with memory such as Neural Turing Machines and Memory Networks. This allows to achieve remarkable run-time and memory efficiency improvements (three orders of magnitude) compared to the existing neural nets augmented with memory. This architecture also seems to allow remarkable improvements in performance in the setting of curriculum learning and in the ability to generalize to sequences which are an order of magnitude longer than those observed in training (from length of up to 10,000 in training to sequences of length of 200,000 in testing). This is potentially a groundbreaking paper. We have an actively developing field of neural networks augmented with memory starting with Neural Turing Machines and Memory Networks. So far we have observed very interesting research and experimental results in that area, but they did not look like they were at a level where they would be practically applicable. The drastic efficiency and performance improvements reported in the present paper are likely to mean that the memory augmented neural networks are now ready to be used in practice. That is likely to have far-reaching implications. The only thing I am missing in the present paper is a discussion of the reasons for drastic performance improvements in curriculum learning (and also for drastic improvements in the ability to predict sequences with length so far beyond the training range). Certainly, run-time and memory efficiency can't account for that, so something else is going on (my preliminary intuition is that it is likely that Sparse Memory Access architecture plays a similar role here to the role played by Sparse Auto-encoders in other situations; certainly, the readers would like to know the authors' thoughts on this, even if those thoughts are preliminary).