In this paper, the authors proposed a new approach based on deep learning by incorporating an idea of transductive learning for transfer learning. Experiments are conducted on a fully transductive setup. Though the experimental experimental results look promising, there are some major concerns on the proposed model as well as the experimental setup. Please refer to my detailed comments. Regarding the proposed model, below are the major concerns: 1. In Abstract and Introduction, the authors highlighted several times that transfer learning or domain adaptation aims to align the mismatch between the training and testing data distributions, such that good generalization can be obtained across domains or tasks. In the problem setup, the authors further explicitly state that \hat{x}i and xi follow different distributions ps and pt, respectively. However, different from some existing methods, like [19] and [Pan etal., Domain adaptation via transfer component analysis, IEEE TNN, 2011], the proposed model indeed does not explicitly minimize the distance or align the mismatch between the training and testing distributions. There is no guarantee that based on the new representation, the mismatch issue between distributions can be addressed. Therefore, I would suggest the authors the rephrase those claims carefully. The new high-level representation learned by deep learning may be able to reduce the difference between domains, but is not able to align distributions explicitly. 2. Compared to other transfer learning methods, the new idea here is to introduce a tranductive step to make use of predicted labels on the target domain unlabeled data. This looks new to transfer learning. However, this idea is indeed borrowed from the co-training technique in semi-supervised learning (transductive setting). The adaptation step, which learns common and task-specific parameters, is quite standard in deep learning based transfer learning methods. Thus, technically, I would consider the proposed model is a combination of existing techniques. 3. The solution to labeling target domain unlabeled data, i.e., (2), is quite heuristic. The basic idea of the Reject Opinion is to introduce a kind-of confidence on the predicted labels. However, it would be more solid if some theoretical analysis is provided. 4. Discussions on convergence of the alternating optimization procedure are missing. Regarding experiments, 1. I guess the performance of the proposed method is quite sensitive to the value of k in k-nn on different datasets. Sensitivity analysis on k is missing. In addition, in practice, how to tune the value of k is lack of discussions. 2. I guess the performance of the proposed method is also sensitive to the value of \gamma in the proposed Rejection Option. Sensitivity analysis on k is missing. Similarly, in practice, how to tune the value of \gamma is lack of discussions. 3. Experiments are only conducted on a fully transductive setup, which is not practical. Actually, after the labels of the target domain unlabled data are estimated by the proposed model, a k-nn can be applied to make predictions on out-of-sample target domain test data. It would be more interesting to show results on an out-of-sample test dataset.