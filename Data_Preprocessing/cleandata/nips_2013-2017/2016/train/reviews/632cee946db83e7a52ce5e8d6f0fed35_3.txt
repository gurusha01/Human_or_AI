The paper notes that multiclass SVMs rely on a fixed mapping and learning a linear classifier. In contrast, multiclass boosting learns a mapping and applies a fixed classifier. The authors propose to learn both the mapping and the classifier. The paper is very well written, and fairly easy to follow. Experimental results appear sound. I am not an expert in either SVMs or Boosting, so I can't comment on novelty. If the work is novel, then I recommend acceptance. As stated above, the authors note a duality between multiclass SVMs and Boosting and use this to propose a new algorithm that learns both a mapping and a classifier. In order to make this algorithm work, the authors replace the hinge loss of the SVM with an exponential loss (used by Booosting). So, to me, the proposed algorithm feels more like a Boosting algorithm, than a unification of Boosting and SVMs (but that's not a big deal). The algorithm solves a convex optimization problem in each iteration, so it does not solve a convex problem (I have no issues with this). The authors do not really discuss running time of the algorithm. It would be very good to provide numbers on the running time of the proposed algorithm and those of Boosting and SVMs. I think the work is interesting and the paper is well-written. I am, however, not an expert so I cannot judge importance or novelty. A few minor comments:  Sometimes you use \langle and \rangle for dot products, and sometimes you use < and >. Please be consistent.  You seem to use \argmin but not \argmax (rather you seem to use \arg\max) * When you talk of Vapnik's classic work, it would be good with a citation.