This paper proposes to introduce a "reviewer" module into attentive recurrent encoder-decoder networks. The main idea is that the attentive decoder can benefit from access to global summary vectors - referred to as "facts" in the paper - for improved performance in image and source code captioning. After the encoder has processed a sequence, the reviewer module makes T_{r} further passes over the encoded sequence, each time adding a new vector to the accumulated set of facts F. The reviewer can be trained with an auxiliary task such as predicting the presence of vocabulary words, or end-to-end for decoding. The encoder-reviewer-decoder (ERD) variants are shown to outperform the baseline attentive-encoder-decoder, and is competitive with the state-of-the-art models for image captioning. The proposed modification to the architecture of attentive encoder-decoder networks is sensible and appears to confer a performance boost in image captioning. However, it is not clear how much of the benefit comes from simply adding more capacity to the model, and how much is due to the specific architecture of extracting a set of global "fact" vectors before decoding. Overall, this looks like a slightly better way of doing things, but not a groundbreaking result. Detailed comments/questions: - Does discriminative supervision also help the attentive encoder decoder? - How does ERD compare to attentive encoder decoder with more layers? - Currently all of the facts are derived from the image alone and potentially grounded in some discriminative task. Is there any other way to extract a set of factors that might use a knowledge base or other sources other than the image alone, e.g. prior knowledge about objects appearing in the scene and their common-sense relations, etc?