The authors propose a Bayesian nonparametric method for clustering data where the size of clusters grows like o(n) rather than O(n/log(n)), which is assumed in most common models like a Dirichlet process. They propose a generative model where (1) the number of clusters is drawn from a distribution, and then (2) the number of items in each cluster is drawn from a second distribution. Submodels are proposed based on the prior distributions for (1) and (2), including a negative binomial/negative binomial model (NBNB) and a negative binomial/Dirichlet model (NBD). These are tested on survey data. Bayesian clustering models for small cluster sizes is an interesting (and difficult) problem as many real world applications have many small clusters. Technical quality: The biggest issue with the paper is model incoherence, as the authors noted. Incoherence means that marginal distributions generated using the full data set do no necessarily coincide with the distribution using a subset of the data. This model maintains data exchangeability at the cost of incoherence. The authors did not make clear the implications of removing exchangeability vs incoherence for microclustering. For what it is worth, non-exchangeability seems like a more reasonable modeling assumption with microclustering, so please provide motivating counter examples. I am not in the camp that incoherence is always a fatal flaw, but it does need to be approached more carefully. In which situations can this model be used? In which situations can it not be used? What sorts of problems does it create when used as a generative model? Other areas: the paper was fairly clear, methods were well-implemented, and the potential usefulness is moderate to good.