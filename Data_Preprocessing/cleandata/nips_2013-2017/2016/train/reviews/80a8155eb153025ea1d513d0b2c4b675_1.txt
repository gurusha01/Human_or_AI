In this paper the author studied theoretic properties of the robust k-means (RKM) formulation proposed in [5,23]. They first studied the robustness property, showing that if the f\lambda function is convex, the one outlier is sufficient to break down the algorithm; and if f\lambda need not be convex, then two outliers can breakdown the algorithm. On the other hand, under some structural assumptions on the non-outliers, then a non-trivial breakdown point can be established for RKM. The authors then study the consistency issue, generalising consistency results that are known for convex flambda to non convex f\lambda. In this paper the author studied theoretic properties of the robust k-means (RKM) formulation proposed in [5,23]. They first studied the robustness property, showing that if the f\lambda function is convex, the one outlier is sufficient to break down the algorithm; and if f\lambda need not be convex, then two outliers can breakdown the algorithm. On the other hand, under some structural assumptions on the non-outliers, then a non-trivial breakdown point can be established for RKM. The authors then study the consistency issue, generalising consistency results that are known for convex flambda to non convex f\lambda. My main concern of the paper is that the results appear very specific and I am not entirely sure whether they will appeal to a more general audience in machine learning. The results are about the RKM formulation, a formulation IMO has not established itself as a standard method. Also, it does not appear that the technique to develop these results may easily adapt to other methods either. I have a question about section 3.3. From its current exposition, the main result of the subsection appears to be re-capping what is known in literature. Indeed there is no formal theorem presented in section 3.3, and the last paragraph which looks like the main result ends with "see also Theorem 3, section 5 in [2]". Regarding the consistency results, in the abstract the authors mention that they extend non-asymptotic results to the non-convex f_\lambda case. However, the main theorem of this paper - Theorem 2 - reads like an asymptotic convergence result. This might be my misunderstanding, and I would appreciate any clarification from the authors. About the simulation, I am not sure what conclusion can be drawn, in particular about comparison with trimmed k-means.