This paper studies a dimension reduction problem of finding a (weighted) subset of n vectors to approximate the sum of squared distances from those n vectors to any other k-dimensional affine subspace. which can be seen as an "online" PCA problem. Their main contribution is to first prove the existence of such subset of size independent of the number of vectors n and the dimensionality of these vectors d. And they give a computationally efficient algorithm to compute it. They also show the application of their algorithm on the latent semantic analysis of English Wikipedia. This is an interesting paper and the result is novel. They find a corset of size independent of the input dimension, which could significantly reduce the computation time of dimension reduction problem. However, there seems to be an error in the proof. To me, theorem 2 is the key step to establishing the main result. But I do not see why we can replace vi (a vector) by vi vi^T (a matrix) in using Theorem 2. (line 265). Also, it is better to provide more intuitions on how to reduce the running time of Algorithm 1 in proving Theorem 1. I think the result of this paper could be improved by providing a lower bound. Is it possible to construct a special input matrix such that any corset of size o(k^2/\epsilon^2) can not achieve 1+\epsilon approximation? The organization of this paper is very clear and easy to follow, though there are some typos. The authors sometimes use || || to denote the operator norm of a matrix, sometimes use it as the vector l2 norm (Theorem 2, line 250-252). And there seems to be a typo on line 267. Do the authors mean equation (5)? The variance is missing on the line 5 of Algorithm 1.