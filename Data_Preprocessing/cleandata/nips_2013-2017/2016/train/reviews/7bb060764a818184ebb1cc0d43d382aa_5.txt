In this paper, the authors have proposed a distribution-dependent dropout for both shallow learning and deep learning. The authors have theoretically proved that the new dropout achieves a smaller risk and faster convergence. Based on the distribution-dependent dropout, the authors developed an efficient evolutional droupout for training deep neural networks that adapts the sampling probabilities to the evolving distributions of layers' outputs. The authors propose a multinomial dropout and demonstrate that a distribution-dependent dropout leads to a faster convergence and a smaller generalization error through the risk bound analysis for shallow learning. This paper is well written and easy to follow. I have the following concerns. (1) I suspect whether the novelty of this submission meets the requirement of NIPS.The authors may want to emphsize their novelty in the rebuttal. (2) In the experiment part, why do the authors only report the best results on each dataset that yields the fastest convergence? (3) Why do the authors only compare the evolutional dropout with BN on CIFAR-10 dataset? The authors may want to do similar experiments on other datasets to verify the effectiveness of the proposed approach. Minor comments: The citation package is not correctly used. In section 2, there are some "author ?".