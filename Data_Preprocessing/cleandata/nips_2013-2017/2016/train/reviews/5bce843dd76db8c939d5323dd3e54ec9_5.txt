This paper proposes a novel LSTM modulating architecture called "Phased" LSTMs which adds multiple period masks to LSTM networks with different opening frequencies therefore enabling a faster learning of long term dependencies. This strategy, somewhat akin to a Fourier decomposition, presents many advantages besides providing a faster training of LSTM networks with state of the art performance on various tasks. It stand outs by its ability to enable LSTM to work with asynchronous data feeds. This means that the inputs need not arrive with a regular sampling to the network. The authors carefully explain the motivations and design of the architecture. They later on proceed with three very different experiments on different tasks which highlight that Phased LSTMs are able to feed on asynchronous data, offer a better than standard LSTMs in this setting, and train faster in all experiments. This paper is nice to read, well presented, and presents a compelling innovation in the field of recurrent neural nets which has deep ties with the theory of frequency decomposition of time series data. The impact of phased LSTMs, because of their ability to handle asynchronous data feeds, is potentially huge. Most modern sensing problems deal with asynchronous time series data feeds. The phased input gate is an elegant idea, somewhat similar to considering the projection of a time series on a Fourier basis, which clearly improve the speed at which LSTMs can learn by considerably reducing the number of updates necessary and therefore allowing full propagation in time of gradients. The experiments show significant improvement over pre-existing methods and seem to be conducted in a sound and methodical manner on compelling and varied tasks. The writing quality is excellent. I just have two quick remarks: Equation (11) and the corresponding section are nice simplifications that explain why the Phased LSTM may be more likely to capture long range dependencies. However, the model, as it is simplified, is similar to an AR model which is in my opinion a somewhat extreme simplification of a LSTM model. I think the authors should state that this section is over-simplified to serve a pedagogical purpose. In the experiments, I could not find how many different oscillation periods were used in practice. How many are sampled from the exponential distributions? This paper was nice to read, straightforward yet innovative. It is, in my opinion, worthwhile presenting to the community for its impact on neuromorphic computing and time series analysis.