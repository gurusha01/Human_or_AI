This paper presents how we can extend the notion of state-action visitation counts to reinforcement learning domains with large state spaces, where the traditional notion of visitation count becomes uninformatively sparse to be useful. The main idea comes harnessing density estimation to derive pseudo-counts that can be seen as equivalent to counts. Asymptotic analysis is given to show that they are indeed equivalent. The effectiveness of the pseudo-count derived from density estimation (CTS) is demonstrated by extending two RL algorithms (DQN and A3C) to accomodate count-based exploration, and running on a range of games in ALE. This is a very nice paper that could be useful for many (deep) RL algorithms with large state spaces. I have a few comments regarding the pseudo-count: (1) In ICML this year, a number of exploration algorithms were presented that used model uncertainty for exploration. It seems to me that using model uncertainty is simpler and more natural, rather than using an external, density model. This issue is briefly mentioned in future directions section, but how does pseudo-count exploration compare to these model uncertainty exploration methods? When is it advantageous to use pseudo-counts? (2) How does the density-based counting scheme to a really simple counting scheme, e.g. partition the screen into big chunks and use color indicators? (3) In eqn (5), N(x) -> N_a(x) since we want the bonus to be dependent on states and actions?