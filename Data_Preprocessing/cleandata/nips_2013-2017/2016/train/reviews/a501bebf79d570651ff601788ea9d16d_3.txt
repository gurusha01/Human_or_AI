The paper proposes a method for performing back-propagation through time for training recurrent neural networks that can be accommodate computers with much less operating memory than required for the usual method of unfolding in time. The method computes the optimal policy for storing the internal state during network unfolding by means of dynamic programming. Multiple storage schemes are explored, where either the entire internal state is stored, or the activations of the hidden units only, or a mixture of both. Computational analysis demonstrates that the memory requirements can be reduced by 95% with a relatively minimal increase in computations (33% more). The method is likely to be very useful for training RNNs with long sequences on GPUs with limited memory. I think this could potentially be a very useful paper that might expand significantly the length of sequences to which BPTT is applied. Some evidence that would reinforce this impression would be an actual motivating example, where a particular computing architecture (e.g., GPU model) runs out of memory with computing resources to spare. Otherwise, it is not entirely clear that memory would become the bottleneck for long sequences, and not the computing units. A minor typo: On page 7, in the third sentence of Section 4: "orward" -> "forward"