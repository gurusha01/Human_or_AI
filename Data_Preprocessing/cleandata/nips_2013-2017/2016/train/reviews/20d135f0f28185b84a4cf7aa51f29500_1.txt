This paper proposes a new stochastic gradient descent algorithm to train ensemble models. The writers build upon the work of Guzman-Rivera et al [8] where the loss of the ensemble model is the loss of the best output of a single classifier in the ensemble, this yields a diverse set of classifiers. Their contribution is to suggest the use of a stochastic gradient descent algorithm for the training which enables the use of this algorithm for deep neural networks. The paper is interesting and clearly written. My biggest concern is whether the algorithm novelty of the paper is not only incremental to this of [8]. Since the authors convinced me that the small change they add makes this algorithm much more useful I am in favor of acceptance. I believe that an interesting comparison would be to check the MCL algorithm when batches are used (rather than the entire dataset). I would like to see more analysis regarding the "diversity" effect.