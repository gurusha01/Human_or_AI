This paper extends the stochastic variance reduction techniques SVRG and SAGA to deal with strongly convex-concave saddle-point problems where linear convergence is proved. Both theoretical and empirical results demonstrate the effectiveness of the variance reduction method on the saddle-point problems. This paper extends the stochastic variance reduction techniques SVRG and SAGA to deal with strongly convex-concave saddle-point problems. The proposed algorithms can apply to a number of non-separable saddle-point problems, where both the loss (e.g., structured output prediction) and regularizer (e.g., fusion term) can be non-separable. An important contribution is that, different from the previous convergence analysis on convex problems, the analysis on saddle-point problem is much more complex and the authors adopt the monotone operators to prove the convergence. The analysis also shows that the extended SVRG and SAGA algorithms apply to a wider class of problems such as the variational inequality in game theory. Considering a stochastic approach, the authors propose two different splits of the gradient operator, i.e., the element-wise split and factored split, for the SVRG and SAGA respectively. The variance reduction works on the stochastic forward-backward algorithms which are commonly used for saddle-point problem. Non-uniform sampling is considered as well and both theoretical and empirical results show that variance reduction with non-uniform sampling is superior to that with uniform sampling. Experimental results in both the main article and the appendix provide sufficient evidence of the theoretical results and demonstrate the effectiveness of the proposed methods on both non-separable loss and regularizer. Some minor comments: As acceleration in Section 5, the authors follow [8] to incorporate an additional regularization term to push the update similar to an iterate point. It is not clear that how to adaptively update this iterate point, namely (\bar{x}, \bar{y}), to achieve speed-up. A convergence and acceleration proof is provided for SVRG. Will the same theoretical results exist for SAGA? The smooth function K(.) and the design matrix K should use different symbols to avoid confusion.