This paper describes a method to reduce the net-zero win and loss changes (churn) of two models trained successively with increasing features and training size. A stabilization method is proposed to and used in a Markov chain to reduce churn. Both theoretical and empirical analysis is provided. The paper uses a stabilization operator to regularize model B to be consistent with the previous model A. Theoretical and empirical results validate its reasonability. However, I'm a little bit doubting the necessity of such successive training method. The stabilization operator (e.g., RCP) essentially transfers information of model A which is trained on a different dataset TA (e.g., less features) to model B. That is, now model B also "sees" the dataset TA in addition to TB. I am wondering what would happen if you directly train B on TA+TB, or more practically, on TB with random perturbation (e.g., dropping features randomly) like dropout. Will this yield better accuracy and stability? Considering the recent success and popularity of such dropout-style methods, the authors should compare their method to it. The choice of perturbation PT (Line.94) and the claim that classifier A is "pre-trained to be robust to the kind of changes" (Line.104) are not that intuitive. Why A is necessarily robust to the changes encoded in PT (though I know with PT Fk^ tends to generalize better)? How would it impact B's robustness? Please explain how the "True WLR Needed" in Table.1 is calculated. Line.178 re-labels -> re-label Table.3 is hard to read. It'd be nice to put different metrics (e.g., WLR) into separate tables to make comparison easier.