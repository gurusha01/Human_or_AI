This is an interesting paper on providing upper and lower bounds on Markov random field models that include an additional, non-factored but submodular energy term. It uses two existing tricks: (1) upper and lower linear bounds on the submodular function f(.), which convert f(.) to additional low-order factor terms, and (2) standard variational upper and lower bounds on the resulting MRF. In combination, these give upper and lower bounds on the original MRF's partition function. This is an interesting paper on providing upper and lower bounds on Markov random field models that include an additional, non-factored but submodular energy term. It uses two existing tricks: (1) upper and lower linear bounds on the submodular function f(.), which convert f(.) to additional low-order factor terms, and (2) standard variational upper and lower bounds on the resulting MRF. In combination, these give upper and lower bounds on the original MRF's partition function. It's not entirely clear to me how useful these resulting bounds are; in the experiments, the authors mainly report the pseudomarginals of the upper bound, rather than actually doing anything with the partition function. I would expect the upper bounds to be mostly helpful in parameter learning problems. It would be nice if the authors could comment on this. Connections to existing work are very good and clearly explained. On a technical point, I would suggest replacing the marginal polytope form of TRW in (3) with a decomposition form (the dual of the form in (3)), for example a combination of spanning trees (Jancsary & Matz, AISTATS 2011), or dual decomposition (Ping et al., NIPS 2015). This would have the advantage of converting the saddle point, double-loop optimization into a single joint minimization, simplify the weight updates, and likely speed the whole thing up since your computation is dominated by running TRWBP in the inner loop.