This paper proposes a new formulation of two-layer models with latent structure with maintaining a jointly convex training objective. The resulting formulation is nonconvex and tackled by using semi-definite programming (SDP) relaxation. The effectiveness is demonstrated by the superior empirical performance over local training. The paper is technically sound. However, I do not find it reasonable to assume a conditional model based on an exponential family for the first layer and further replace the log-partition function, Omega, by an upper bound. In the end, the resulting formulation is further convexifed by SDP relaxations. I have a few unclarified issues: (a) By solving the SDP, which kind of solution did we get for the original problem including Omega? Is the solution feasible to the original problem or relaxed one? (b) Does Theorem 1 imply that the SDP relaxation gives the exact solution of Problem (15)? If not, finding a relaxed solution of the optimization problem that is obtained as an upper bound seems not reasonable.