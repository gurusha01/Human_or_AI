This paper presents a two-layer model for learning problems with a latent structure. They first provide an objective and then discuss its convex relaxation by enforcing the first-order optimality condition via sublinear constraints. Experiments on two different tasks show the proposed approach is useful. [Technical]; The paper provides nice analysis and makes the assumptions of their model clear. Empirical evaluations are also comprehensive. [novelty]: To my best knowledge the proposed approach in the paper is new. [Impact]: The proposed approach is general and may make a great impact on applications with latent structure. [Presentation]: Overall, the writing is nice. It provides a nice motivation in Section 2 and is not difficult to follow. Please see some comments below. Some comments in an arbitrary order: - Line 41-45: it is not clear what the authors mean in the introduction. Perhaps, provide an example? This statement is clearer after reading the whole paper. - (minor) I appreciate the authors summarize the literature of deep learning with structured prediction in the Introduction. However, it seems to me the proposed approach is not very relevant. Specifically, the model with only one latent layer and the relationship between x and y is encoded through y'Ux and it seems that the connection to the neural network model and auto-encoder are not strong. As a result, the introduction does not motivate the approach well (Section 2, in fact, provides a better introduction to the goal of this paper). - It seems to me the approach is relevant to [a], although [a] doesn't introduce structure into the latent layer. However, the joint likelihood approach and the convexification are relevant. - The authors may want to mention and relates their methods to latent structured prediction approaches including hidden CRF [b] and latent Structured SVM [c]. The key difference is in [b] and [c], the relations between x,y,z are directly encoded by features. [a] Convex Two-Layer Modeling [b] Hidden-state Conditional Random Fields [c] Learning Structural SVMs with Latent Variables ADDITIONAL COMMENTS* Sorry for adding additional comments after the rebuttal period starts. However, it would be helpful if the authors can clarify some additional questions about the experiments: - [12] reported 95.4 in MRR, while the results reported in this paper are much lower. Then, I realize the experiment setting is slightly different. What is the performance of the proposed approach in the setting of [12]? - The authors said "[12] trained it using a local optimization method, and we will refer to it as Local". But [12] actually performed a global learning -- this is actually the main claim of [12]. I wonder if the authors implemented [12] right or if I misunderstand the authors.