The problem of aggregating worker output in crowdsourcing is addressed using the language and tools of information theory, and the fundamental limits of the budget-fidelity tradeoff are given. I did not fully understand the paper (see below) but thought that the insights and model itself were a very interesting, and potentially useful, take on the design of crowdsourcing aggregation techniques. I would have liked to see the high-level intuition/insight stemming from these results (e.g. end of section 3) highlighted more prominently, as well as a conclusion for what one should take away from this (I know space is tight...) While I certainly appreciated the efforts of the authors to make the paper accessible to those not very familiar with information theory, like myself, in the end it was still quite hard to follow. I would recommend restating most/many of the results in terms of the crowdsourcing application itself, rather than retaining the language of channels and codes and distortion. Please mention in the related work the alternate (and slowly merging) line of research on robust economic incentives for workers (usually by the terms "crowdsourcing mechanism" or "mechanism design for crowdsourcing"). Some of this work also discusses varying worker abilities, and recently, how to combine these incentives with more sophisticated aggregation methods. Little things: 84: "is reviewed in" should be "are reviewed in"? 106: I was quite confused here (and earlier, in the initial explanation) about the distinction between the crowdsourcer and taskmaster, perhaps because "crowdsourcing" is a portmanteau of "crowd" and "outsourcing", which one would think would happen before workers got involved 122: type in "A A(X1,X2)" 275-276: "In case..." this was very hard to parse. figure 2: I could not glean much from this figure. Perhaps a more detailed caption would help.