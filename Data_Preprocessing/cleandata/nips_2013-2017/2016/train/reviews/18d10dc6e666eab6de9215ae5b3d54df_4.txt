This paper considers the parallel knowledge gradient method for batch Bayesian Optimization. The authors consider the case when evaluating the function value is time consuming. They propose a new acquisition function called q-KG that can output several new points for parallel evaluation. They also provide an efficient approach to evaluate the q-KG function. Empirical results are shown to verify their theoretical results. This paper considers the parallel knowledge gradient method for batch Bayesian Optimization. The problem they considered is important. Being able to split the work in a distributed way is useful to the performance of data driven applications. Instead of evaluating one point at a time, the algorithm consider q knowledge gradient that propose q point to evaluate at next iteration, which fasten the efficiency of the algorithm a lot. It is an interesting idea to be explored. Yet it is unclear whether Gaussian process is a good assumption to make, especially in neural network architectures where the dimension of the input is very large. It might be helpful if the author could provide some theoretical results of their algorithm on the error bound of their methods and the quantitative analysis of the complexity with regard to dimension. But this is a solid work overall in the field of Bayesian Optimization. The method is solid and the experiment is sound. The paper is well written overall. Some typos and suggestions are l.36 "the set of points to evaluate next that is" l.95 What is the definition of A? l.104 What is the input of mu^n? l.128 It would be helpful to briefly discuss the parallel EI algorithm l.140 It might help to use different notations for A.