The authors prove global convergence of the EM algorithm in a Gaussian mixture model where the covariance and mixing proportions are known a priori. The presentation quality is mostly good but I didn't get a good enough sense of what the authors intended the main selling point of the paper to be. I'm not an expert in this area but the article didn't give a clear enough sense of what the practical importance of its results are. Models 1-2 are easy enough to estimate with large samples using, e.g., method of moments (MOM) or MOM plus a one-step estimator. Presumably MOM, which is sqrt(n)-consistent, is also a good enough pilot estimator for the techniques of Balakrishnan et al. (2014) to apply. The paper might still be acceptable if the proof techniques were interesting or novel enough to possibly generalize to other, more difficult-to-estimate models. But in that case the proof techniques should have been explained in the main body of the paper, at least heuristically. As it is, they are in a very long technical supplement without much in the way of intuitive explanation. Also, it's hard for me to understand why these models were an interesting test case for the EM algorithm. For example, in Model 1 the population likelihood function has only two local maxima (the global maxima at theta and -theta) and one saddle point. Given the general properties of the EM algorithm as a majorization-maximization procedure, convergence to one of the global maxima seems like a foregone conclusion.