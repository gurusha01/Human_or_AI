The authors extend the generative moment-matching networks (GMMNs) by considering matching conditional distributions instead of joint distributions. The key idea is to embed the conditional distributions in a reproducing kernel Hilbert space, a technique proposed by Song et al. in 2009. The proposed algorithm CGMMN is evaluated empirically on several real datasets. In the definition of $\hat{C}{Y|X}$ (Line90), the term related to $\muX$ and $\muY$ (more precisely, their estimators) is missing. It is also unusual to assume that $\muX$ and $\muY$ are zeroes, i.e., the data is already centered in the RKHS. Since $\hat{C}{Y|X}$ is at the heart of CGMMN, it is desirable that the authors provide clarifications for this missing term, as well as how to deal with it in gradient computation and real experiments. The proposed approach is new and neat, but the experimental results are somehow disappointing. In terms of predictive performance, on both datasets, CGMMN gives larger error rates than the state-of-the-art algorithm CMMVA. In terms of generative performance, since no comparison between CGMMN and other conditional generative methods (e.g., the conditional generative adversarial nets) is provided, it is unclear whether the proposed algorithm could generate better samples. The paper contains some typos, e.g., "Frobeniu norm" (Line48), "an universal" (Line65), and "which the" (Line238). -----Update after the author rebuttal----- The authors have addressed all my concerns. I will change my scores.