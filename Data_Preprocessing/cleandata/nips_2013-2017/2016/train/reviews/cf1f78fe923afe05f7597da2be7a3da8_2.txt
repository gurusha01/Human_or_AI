The paper propose a manifold learning technique that seeks a low-dimensional embedded of data, such that it lies on a submanifold of the embedding space. This is achieved by gradient-based optimization of a straight-forward energy measuring the deviation of the intrinsic Riemannian metric from the identity. This is novel, but it is unclear why it is useful. The key novelty of the paper is the idea that the intrinsic manifold dimension can (should?) be lower than the embedding dimension. This is novel, but the authors fail to motivate why it is useful. The only motivation seems to be that this approach can give less distortion. Some comments on this: 1) clearly lower distortions can be achieved when embedding a d-dimensional manifold into R^s (s > d) rather than embedding it into R^d (this is overly trivial); 2) it is not clear why we shouldn't just use Isomap (and its various cousins) to embed into R^s (this is what people do in practice); 3) it is unclear why this distinction is useful. I understand why it is helpful from a mathematical point of view, but which real problems can be solved that could not be solved before?