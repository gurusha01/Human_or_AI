The paper introduces an algorithm to improve the low-dimensional embedding of data sampled from a manifold. The authors proposed distortion Loss functions, which are defined by the push-forward Riemannian metric associated with the given embedding coordinates. This loss directly measures the deviation from isometry. The initial embedding coordinates are then iteratively updated to reduce the loss. The loss function is shown to be non-convex. The new method--Riemannian Relaxation--was shown to have lower distortion than Laplacian Eigenmaps, MUV, Isomap, and HLLE on synthetic datasets. The authors might need to explain more clearly on how the loss functions measure the deviation from isometry. If the loss is zero, does the embedding achieve isometry? Can you provide a theorem and proof on it? How to quantify the deviation from isometry if the loss is not zero? What is the quantitative measure for claiming that an embedding is nearly isometric? In addition, the loss function is non-convex, it might be quite difficult to achieve global optimal. In addition, the algorithm might be sensitive to the initialization (the initial coordinates Y^0). Is it crucial to initialize with the embedding coordinates from Laplacian Eigenmaps, Isomap, MVU, HLLE, or Diffusion maps? It might be useful to include the running times for different embedding methods. Riemannian Relaxation is a computationally more expensive method for doing embedding and the results shown in Figure 2 are not too different. The authors might need to provide more justification why it is useful in practice, for example, improving classification or regression results. The paper is well written overall. The organization is clear. There are a few typos in the paper, for example, Eq (7): what is $\mathcal{L}_k$?