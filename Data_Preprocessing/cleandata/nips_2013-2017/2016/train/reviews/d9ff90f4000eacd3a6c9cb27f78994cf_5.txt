A Long standing issue with recurrent neural networks is the phenomena of vanishing and exploding gradients which prevents recurrent neural networks from learning long sequences. Recently unitary hidden-to-hidden weight matrices have been proposed to alleviate this problem. However, unitary matrices as proposed from previous literature are from their construction limited in their representational power for large dimensions and the current paper proposes the "Givens capacity" used to quantify the representational power of such unitary matrices. Furthermore, the paper proposes a modified version of stochastic gradient descent, which allow for training full-capacity unitary matrixes. These full-capacity unitary matrices allow for more representational power compared to previous proposed matrices, while still preserving the desired properties of being unitary, hence minimizing the effect of vanishing and exploding gradients during training. This paper address an important issue related to recurrent neural networks. The paper is well written, technical and theoretical sound and the authors propose theoretical justified techniques that improve over current state-of-the-art. The use of unitary matrices for recurrent neural networks are still relatively new and its practical applicability and feasibility over traditional architectures has not been rigorously investigated. However, promising theoretical and empirical results are presented in the paper, which indicate that using unitary weight matrices in recurrent neural networks might have potential to be applied in a broad range of applications.