The paper extends Gaussian IB to the case of sparse variables, which makes more sense for data that have sparse or nonlinear manifold like structure. Performance is demonstrated on some toy image patch data, in addition to hand written digits, showing that the algorithm can discover relevant structure in the data. Overall this is an interesting paper. However it would have been nice to see some applications to real data such as natural images rather than the toy image patch data. Also this sentence from the discussion is confusing: 'However, unlike traditional sparse coding models, the encoding (or 'recognition') model p(r|x) is conditioned on a seperate set of inputs, X, distinct from the image patches themselves. Thus, the solutions depend on the relation between X and Y , not just the image statistics. 
' Specifically, it is not clear what is meant by 'a separate set of inputs, X, distinct from the image patches themselves.' My understanding is that Y is just the image to be reconstructed, so this seems very much like sparse coding. Specific comments: Line 39 says that equation 1 is being maximized. I believe it should be minimized. That is at least what is shown in Tishby's paper, eq. 15. The error is also repeated on line 52. Figure 1 example - why not use natural images? would have been more compelling example Occlusion example with kernel - a generative model could also do this, how does this method compare?