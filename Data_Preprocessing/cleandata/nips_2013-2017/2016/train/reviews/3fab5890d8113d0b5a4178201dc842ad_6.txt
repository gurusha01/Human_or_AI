The paper is presenting a Neural Turting Machine (MTN) with sparse read and write access to overcome one of the major problem of MTNs that make them unsuitable for real world applications. The problem is that MTNs need prohibitively large physical memories and operations to do do Backprop, which the architecture presented in this paper hopes to solve while having comparable performance to the state of the art. The paper is well structured and easy to read. If the a brief description of the memory augmeneted neural networks is presented in the background section, or an explanation of the the memory is presented to make the paper more self-contained, that would help the readers who are not experts in the field (e.g even though it is clear to those who have worked in this area, to some readers it might not be clear what the write operator ideally is supposed to do compared to conventional memories, etc.). Also the comparison in Figure 4 might not be fair. By choosing the memories so that they require the same amount of physical memory, you can compare the performance given a limited resource (which is what you have done and definitely a justifiable one!) but I think using the same memory and not the same physical memory is more informative of how the performance is. Nevertheless, it is very interesting to see how well this algortihm is performing while being so close to the theoretical bounds.