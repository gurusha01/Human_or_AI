The authors propose a new method to learn the parameters of the WMD, basically the EMD computed on text documents using word embeddings to compute bin-to-bin dissimilarities. The authors parameterize the EMD cost (i.e. ground metric) matrix using a Mahalanobis distance between vector representations. The authors propose also to learn in addition to this some histogram coefficients reweighting (see Lebanon's work, and comment below). The authors provide the gradients of NCA type costs for this problem, and provide compelling experimental evidence. Overall the paper reads like a nice combination of existing tricks, and provides very convincing experimental results. I would support acceptance. Strengths of the paper are simplicity and a relatively straightforward idea, but not trivial to implement/test. The experimental section is therefore a strong part of this paper. Things to improve: handle better the interplay between regularized/not regularized formulations, be more rigorous with maths (computations/notations are a bit sloppy) and ideally provide an algorithmic box to see more clearly into what the authors propose. A few minor comments: - In Eq.1, the Euclidean distance between word embeddings is used as a cost, in Eq.6, for the purpose of Malahanobis metric learning, that cost becomes the squared euclidean metric (and thus what is usually referred to as 2-Wasserstein). To avoid such inconsistencies, it's probably easier to simply say that WMD is p-Wasserstein between clouds of word embeddings, whatever the p. - The "reweighting" approach proposed in Eq. 5 is not new, it was already proposed by Guy Lebanon in exactly the context of text document classification ("metric learning for text documents"). See also "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations" by Le/Cuturi. - Any experimental insights on the importance of the weight vector w in the learned metric? does that innovation matter? is it relevant in any way? - Related work: it would be interesting to underline more clearly differences with previous work. For instance, this paper builds clearly on the work of Cuturi/Avis, but uses entropy regularization, which clearly speeds up / scales up applications. The cost is also different: while discrete in that work, it's now continuous but parameterized with linear maps. These two arguments are missing. This paper also draws from Wang et al, but that paper is not publishable as it is. The OT optimization is also run once only in their paper, and clearly does not work as it was proposed.