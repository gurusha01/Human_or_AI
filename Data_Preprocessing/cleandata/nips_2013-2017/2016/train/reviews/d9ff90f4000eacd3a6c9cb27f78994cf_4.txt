This work investigates the representation power of the unitary matrix used in the recently proposed unitary evolution RNNs, and then shows how a complete, full capacity unitary matrix can be optimized. A very specific composition of individual unitary matrices (diagonal, Householder reflection, permutation and DFT) was originally proposed, shown in the current paper to be limiting in its ability to represent the entire unitary group once the hidden state dimension exceeds 22. Previous work in optics showed that a unitary matrix can be represented as a product of at most comb(N,2) Givens operators. The current work defines the Givens capacity of a unitary matrix as the minimum number of operators required to construct the unitary matrix, upper bounded by comb(N,2). It is shown how to optimize full capacity unitary matrices using gradient descent using a method discussed in a 2011 technical report. This full capacity optimization is applied to 3 tasks and compared to the restricted capacity unitary matrices proposed in the original work as well as the LSTM. The synthetic data system identification task compares restricted capacity unitary RNN to the full capacity uRNN to the problem of learning the dynamics of a target uRNN given only samples of input and output. MSE objective is used in training. For all hidden dimensions tested the restricted capacity achieves worse performance. The copy memory problem which requires outputting the 10 starting inputs after long delays of 1000 and 2000 steps shows superior performance of the full capacity over restricted and LSTM. Finally a prediction problem of estimating the log magnitude of the short time Fourier transform of a speech sample given all previous STFT. The TIMIT corpus was used. Objective measures of MSE, segmental SNR and perceptual metrics of STOI and PESQ. There is little difference between retricted and full capacity for MSE but both are substantially better than LSTM, for SNR, STOI and PESQ full capacity slightly outperforms the restricted, where again both are substantially better than the LSTM The uRNN is certainly a more elegant solution to the vanishing/exploding gradients of the RNN than the LSTM. With the proposed full capacity optimization an even larger gap is opened up with respect to the LSTM vs. that originally found with the restrictive capacity uRNN. One thing that you can notice between the original and current on the copy memory problem, is that increasing the hidden state size of the LSTM to match the uRNN, the gap between LSTM and restricted seems to go away, whereas both now lag at T=1000, and previously the restricted solved rapidly and perfectly at T=500. Does further increase of the hidden dimension of the restricted and LSTM help performance of both on the increasing T? Does more effort need to be put on the optimization of the LSTM (if you used something like adam, would gradient clipping still be needed). Line 101 missing a word after here?