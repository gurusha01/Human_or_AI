This paper presents a novel method for local similarity-aware embedding using a deep neural net. The novelty of the paper includes the capability to choose hard samples in training to speed up convergence and boost up performance, use of new objective called double-header hinge loss to learn local structures in the data, use of absolute position information for heterogeneous feature distribution, and use of quadruplet in mini-batch for fast sampling procedure. This paper has shown that the presented method outperforms the other competitors on image retrieval and transfer learning. Major comments: 1. The paper claims that the proposed method has advantages both in performance and speed. However, running time is not reported in the paper. 2. The architecture in FIg. 2 should be explained in more detail. For example, CNN in Fig. 2(a) needs to be defined. 3. More explanation about Fig 3. would be nice. What are the differences among contrastive embedding, triplet embedding, and lifted structured embedding, and when they work and when they do not work in comparison to the presented method? 4. Details of objective optimization is missing. E.g., what is gradient? 5. How does the performance change according to the mini-batch sizes? 6. Are there any issues on initialization of parameters, and how were they initialized in the experiments. 7. Standard deviation of the results in Table 1. would be helpful to see the statistical significance of the results. The paper needs more details in the technical parts including optimization, how it was implemented, network architecture to the extent that the results can be reproduced by other researchers. Also, results section should verify each claim made in the paper (e.g., speed).