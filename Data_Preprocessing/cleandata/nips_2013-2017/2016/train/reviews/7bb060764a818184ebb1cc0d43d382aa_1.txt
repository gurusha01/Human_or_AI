The paper addresses one of the recent ubiquitous techniques for improving performance of deep networks, namely dropout. The paper starts by considering a linear shallow network and develops theoretical analyses using risk bounds and update rules for using multinomial sampling for selecting the dropout neurons at each update step (using second order statistics of features of the data). Then they continue to apply the update rules for deep networks where they use second order statistics of a layer for mini-batches of the data. This also shows the connection with the internal covariate shift, well studied in the literature. The technique is showed to work much better in terms of convergence and accuracy for both shallow and deep learning on multiple datasets. The paper is significant in the sense that the implementation seems to be quite simple, while the results clearly show the significant performance gain compared to standard dropout. This is shown to be the new state of the art for dropout and I expect it to be ubiquitous in a very short time. The paper seems technically sound. Claims are supported by theoretical analysis as well as practical experiments. It is a complete piece of work, starting from an initial insightful observation and then going through step by step to the final experiments showing the performance gain of their method. The paper is clearly written and well-organized, it does inform the reader and it is reproducible. The approach is quite new, in the sense that nobody has previously tackled the dropout problem in this way. Especially supported by theoretical bounds, this paves the way for more investigations and variations in this direction, i.e. considering more complex probability distributions for using dropout. Showing that this also addresses the internal covariate shift is very insightful. Minor comments: I think you should specify how costly it is. Is the convergence speed you are referring to in terms of the number of iterations (as it seems to be the case)? If so, how does it perform in terms of wall-time ? What is \mathcal{H} in line 104? Line 242 (iv) please reformulate In the proof of Theorem 1, between lines 15 and 16, second term, you forgot the widehat for \mathcal{L}. Same on line 16. In line 16 you have "the upper bound of in terms of". In proof of Lemma 1, the second equality should be an inequality. In the supplement, after line 35, second math line, first term after the inequality, you forgot the parentheses for the expectation over \mathcal{M}. In the supplementary material you say for Proposition 3 the result is following the KKT conditions, but since this is one of the main results of the paper, you could give more details here.