This paper discusses an approach for examplar learning, based on deep learning techniques. In particular, the problem where we have only a single positive example for a class, and many negative examples. A method for selecting batches for CNN training is proposed. The experiments show competitive results. L71-L73: "However, supervised formulations for learning similarities require that the supervisory information scales quadratically for pairs of images, or cubically for triplets. This results in very large training times." => Since CNNs are trained with minibatches, the training time doesn't necessarily scale linearly with number of tuples/triples. The training time depends on the (conditional) information content in the data, which typically grows sublinearly with the amount of data points. L93: "(ii) The ratio of one exemplar and many negatives is highly imbalanced, so that the softmax loss over SGD batches overfits against the negatives." L110-11: "Since deep learning benefits from large amounts of data and requires more than a single exemplar to avoid biased gradients" => Imbalance/bias can be easily fixed through importance-weighted gradients and/or a reweighted objective. The paper is clearly written, but in my opinion does not give a sufficiently clear introduction into examplar-based learning. At the end of page 3 it's still not entirely clear to me what the exact problem is that this paper is attacking. The experiments show promising results on posture analysis and pose estimation. Due to my unfamiliarity with the problem and related literature, I can not reliably assess the quality of contributions of this paper.