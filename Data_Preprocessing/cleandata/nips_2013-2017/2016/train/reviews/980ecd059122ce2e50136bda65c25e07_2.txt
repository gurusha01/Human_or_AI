The authors propose two metrics for assessing the robustness of neural networks by an approximating algorithm for approximating a robustness criterion that was formerly introduced by Szegedy et al. Some experiments are run on the MNIST and the CIFAR-10 datasets to evaluate the performance of the proposed methods. The authors also explain how their method can be used to improve the robustness of neural networks. The proposed approach is based on the nearest adversarial example (that is, an example with a different label) by Linfinity distance. It defines new metrics based on the fraction of examples within a particular distance and their average distance. These seem like reasonable measures, although the full CDF of adversarial distances is more informative. (They show these CDFs in the experiments, which is helpful.) Assuming that the space is completely linear is a very strong assumption that is not fully justified. Yes, the existence of adversarial examples has been attributed to the linear behavior of the network, but restricting the search to where the network is analytically linear could be problematic. A network may behave approximately linearly in a much larger region than where it is exactly linear. The most interesting result here is that they're able to find many adversarial examples that a previous baseline did not. Thus, while the restriction the linear region of a neural net seems restrictive, it still provides a better measurement and understanding of the vulnerabilities of these networks. Results on CIFAR-10 are informative, though mostly negative. After fine-tuning for accuracy, the robust network makes 10% more errors than before, and can still be fooled on ~60% of the examples by changing an average of 4 pixels (though that's an increase from 3). Minor issues: - Numeric citations would be improved by using author names as the noun instead of the reference number in phrases such as: "[6] takes the approach of [20]" - The paper title and abstract imply an approach that works on any neural network, but the method only applies to ReLUs. A revised title, or at least a revised abstract, seems appropriate. - The introduction needs to be revised. Specifically, it needs to explain more background from the references 5 and 20 in the beginning, such as why Linf norm is the right choice for robustness. Also, rho is not defined before being referred to. The current order of the material makes the paper hard to follow. In the current format, the reader has to go back and forth and has to read those two references first to understand this paper. Typo: "when when" --> "when"