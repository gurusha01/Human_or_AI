The paper studies accelerated first order methods for constrained convex optimization in the continuous and discrete setting. The authors continues the study initiated in by Krichene et al. in [7], where the Nesterov's method is shown to be an appropriate discretization of an ODE system, coupling a primal trajectory with a dual one. In particular, the primal trajectory is obtained performing a specific averaging of the mirror of the dual variable. In the present paper, more general averaging schemes are studied. Theorem 2 then derives convergence rates, under suitable conditions on the averaging coefficients. In the continuous setting, convergence rates that are faster w,r,too 1/t^2 are derived. An adaptive choice of the weights with convergence guarantees is also studied and then used in the experimental section. Such choice is compared with restarting techniques, which are known to improve convergence of accelerated methods in practice. The theoretical analysis of the discrete version of the algorithm is not included. I think this is an interesting and well-written paper. The analysis of the new averaging scheme for accelerated methods is presented in the continuous setting and is rigorous. Even if the main ideas of the analysis of the continuous dynamics are now well-established in a series of papers, there are new contributions and results. In this respect, I think that the work by Attouch, Peypouquet and coauthors should be mentioned. The main theorem, showing that there are averaging schemes with faster convergence rate than 1/t^2 is a relevant one, even if it would be interesting to understand if this improved rate is preserved when passing to the discrete procedure.