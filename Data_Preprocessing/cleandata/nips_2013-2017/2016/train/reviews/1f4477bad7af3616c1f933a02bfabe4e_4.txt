The paper presents nonlinear spectral method which optimally trains a particular class of feedforward neural networks with a linear convergence rate. The condition which guarantees global optimality depends on the parameters of the architecture of the network and boils down to the computation of the spectral radius of a small nonnegative matrix. It seems new and interesting to propose a nonlinear spectral method which optimally trains a particular class of feedforward neural networks. At the same time, I'm worried about the limitation of the approach; it imposes the condition of non-negativity on the weights of the network. In addition, the choice of the activation functions in the neural network are non-standard and very specific ones. The limitation seems very strong, which may hinder the good performance of neural networks. Indeed, numerical experiments show not-so-good performance over SVM. Minor comment: L.90-91: The sentence saying "Note that the nonlinear spectral method has a linear convergence rate and thus converges quickly typically in a few (less than 10) iterations to..." seems exaggerated because the linear convergence rate does not necessarily imply a few iterations.