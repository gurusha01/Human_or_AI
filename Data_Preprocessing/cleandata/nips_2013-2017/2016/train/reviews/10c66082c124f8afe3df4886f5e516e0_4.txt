This paper shows a supervised way of learning the distance between documents on top of word mover's distance (WMD). WMD is proposed in a previous literature motivated from earth mover's distance. More specifically, from one distribution of words to another distribution, optimal transport is calculated to obtain the document distance. For the transport distance between words, word2vec embedding is initially used. In this paper, the authors propose learning (1) a linear transformation of embedded words and (2) a reweighted word distribution, so the documents of different labels are more separated. This paper is clearly written and easy to follow. The paper contains a thorough review of previously related works and the authors have a good understanding of those literatures. The authors provide experimental results of their algorithm for many novel datasets. However, the motivation of the formulation needs more explanation, and the significance is low. 1) Why the authors use reweighting of the word distribution using w? This is not well-motivated, and it seems this reweighting can change the document into a completely different document. If the authors have shown why this reweighting helps without changing the document into a different document, then this reweighting could be justified. 2) The contribution of this work is not significant. Adopting linear transformation `A' and reweighting `w' with NCA objective function is one possible extension for supervised setting of WMD, but this formulation is not groundbreaking nor fancy. 3) How this algorithm can find a distance for a new document? For a completely new document, how one can find a reweight w? 4) It seems `the semantic difference' in the abstract indicates the label of document. Authorship difference or topic difference does not seem a semantic difference. Does the semantic difference mean something else? In general, the paper is nicely written, and I do not object the acceptance of this paper once the questions are appropriately explained.