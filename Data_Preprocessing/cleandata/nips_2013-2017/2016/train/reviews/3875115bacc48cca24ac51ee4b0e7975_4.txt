This paper studies the structure of local maxima of the log-likelihood function of Gaussian Mixture Models (with equal mixing weights and identity as covariance). It presented three related results regarding this vein: 1. It provided a counter-example showing that there exist local maxima (when k = 3 and d=1), whose likelihood is arbitrarily worse than the global maximum. 2. They showed with random initialization, the probability of not converging to a bad local maximum could be exponentially small in k on when the means of Gaussians are configured in a special way. 3. They also showed gradient EM does not converge to a saddle point of the log-likelihood almost surely. Summary: This paper studies the structure of local maxima of the log-likelihood function of Gaussian Mixture Models (with equal mixing weights and identity as covariance). It presented three related results regarding this vein: 1. It provided a counter-example showing that there exist local maxima (when k = 3 and d=1), whose likelihood is arbitrarily worse than the global maximum. 2. They showed with random initialization, the probability of not converging to a bad local maximum could be exponentially small in k on when the means of Gaussians are configured in a special way. 3. They also showed gradient EM does not converge to a saddle point of the log-likelihood almost surely. Technical quality I only checked the proofs in the main paper (Theorem 1), and I think the proofs are sound. Having read the analysis of Theorem 1, I think the proof idea for Theorem 2 also seems reasonable, although I didn't check the details in the Appendix. All of the three theorems are well interpreted after their statements. Novelty & Originality The paper ties the mean configuration of GMM to the local maxima of its log-likelihood. I think this observation, although intuitive, is very original. And I think the key idea of Theorem 2 based on Theorem 1, that the hierarchical grouping structure of the true mean determines the local maxima for k > 3 is also very clever. Potential impact & Usefulness I think the results could have good impact on both theory and practice: in theory, it could inspire the study of the local optima structure of related clustering tasks, such as k-means problems; it could be also interesting to examine whether their result in Lemma 7 also holds for d > 1; in practice, it shows the importance of seeding enough points in each true cluster, and maybe will provide some insights in designing new algorithms. Clarity and presentation The paper does a very good job in presenting their results, and explaining the intuition of their analysis. It clearly introduces the problem setup, and provided intuition and interpretation of all their analysis.