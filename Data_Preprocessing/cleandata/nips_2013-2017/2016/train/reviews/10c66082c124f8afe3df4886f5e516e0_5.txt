The authors analyze the problem of text classification into categories. They expand a previous work that suggested Word Mover's Distance (Wasserstein) to make it supervised by learning word specific weights. This contribution suggests to use the Sinkhorn distance as proxy for the Wasserstein distance to improve its speed, and evaluates on 8 datasets against 26 baselines showing the best average performance. This is a very well written paper that presents well the topic and analyzes the math behind the problem in depth but also with great clarity. The only weaknesses are, maybe, the limited novelty compared to the previous Word Mover's Distance paper, and I don't find the references against the World Centroid Distance relevant, but confusing. I have the feeling that lambda (the regularizing factor) is poorly discussed. In my experience the value of lambda has a very large impact, sometimes being the dominant factor of the eq(2). Theoretically, results should be similar to pure Wasserstein distance given a large enough lambda at the cost of slower convergence, but most people using Sinkhorn use a very small lambda to improve converge rate, achieving a behavior plainly different from Wasserstein. So I am curious if, in this contribution, lambda dictates a pure Wasserstein behavior or a Sinhorn (entropic) behavior.