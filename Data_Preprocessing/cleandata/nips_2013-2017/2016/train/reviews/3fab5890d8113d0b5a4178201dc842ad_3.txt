This paper introduced a novel memory access scheme called Sparse Access Memory (SAM) in memory augmented neural network architecture. This research area is fairly recent with the invention of Neural Turing Machines. This augment allows neural networks to learn beyond the limits of traditional LSTM model. The contribution of this paper is all writes to and reads from external memory are constrained to a fixed size, thus subset of the memory words instead of unlimited size. The authors prove the methodology and its optimality. Conceivably it will achieve good read and write performance due to reduced memory size as shown in section 4.1. But the authors also incorporate the learning costs of introducing sparsity in section 4.2 using standard NTM tasks: 1. copy. 2. Associated Call and 3. Priority sort. The combined cost are lower than NTM for some tasks (i.e. 2 and 3). I also like the fact that the authors include real-world example in section 4.4. The benchmark is using Torch7. The idea is straight-forward and novel. The speedup is significant as opposed to NTM which is impressive. The fact that this sparse reads and writes by a linear or an ANN can actually benefit early-stage learning in some cases are very interesting (section 4.2). If the author could elaborate and think about how to generalize to other cases (under which cases the learning could be improved), or by accessing different ANN approaches with SAM would be very helpful for the audience. Overall it is a well written paper with all assumptions laid out clearly and architecture in the appendix. It is interesting to read.