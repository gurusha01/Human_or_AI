The paper shows lower bounds for the problem of learning with missing attributes. In this problem, one is given examples (x,y) sampled from some distribution on R^d \times R. The goal is to find a linear classifier that minimizes a certain lose (e.g., the square loss or the hinge loss). The twist in this framework is that the learner cannot see all features, but rather, for each example, it can choose to see at most k of the features. This setting has been suggested by Ben David and Dichterman in 1998, and since then algorithms were found for the square loss, even in the case that k=2. On the other hand, for other loss functions, there are no known algorithms for fixed k. This paper shows that indeed such algorithms do not exist. Namely, it is shown that for the square loss, learning is impossible for k=1 (so that k-2 is tight). For the absolute and the hinge loss, it is shown that learning is impossible for every fixed k. The proof goes by exhibiting two distributions that are indistinguishable given this limited access to examples. The proof is rather simple in the case of the square loss, but more delicate in the case of the absolute and hinge losses. The paper makes a solid contribution to the problem at hand. Hence, I'd recommend acceptance.