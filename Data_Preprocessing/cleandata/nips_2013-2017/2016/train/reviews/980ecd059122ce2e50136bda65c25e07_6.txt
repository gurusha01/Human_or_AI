The paper suggests a new way of constructing adversarial examples for NN that are not necessarily at the direction of the signed gradient (of wrong labels). The idea is to encode the NN as a set of constraints and solve a LP problem whose (feasible) solution provides an adversarial example as well as the robustness measure. The authors also formalize the notion of robustness and propose 2 new statistics for it. 1) equation between lines 212-123: i think the conditions are switched. 2) The authors argue that current robustness estimations are biased because they are measured on data produced by the inspected algorithm itself. However what they propose is not much different. They still use their algorithm to measure rho. 3) They claim is that they are less biased just because they find more bad examples. Please support this claim. 4) I believe the proposed algorithm is indeed biased because in the process of removing disjunction constraints the algorithm always chooses to remove the one not satisfied by the seed. Can you say anything about a stochastic version to choose between disjunctions? 5) Overall the paper is hard to follow. Symbols and equations are not formulated enough. For example line 147: what's the dimensionality of x? what does (.)_i relate to? rows? columns? 6) line 64: i believe it should be "x with true label l*"