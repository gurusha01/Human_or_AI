Machine learning algorithms involve many training iterations and constantly refining feature set, with an input stream of training examples. In such cases, we should be able to observe statistically significant improvements over the model in the previous iteration. The authors define a new term called "churn" which helps to determine whether a refined model (through more training) shows a statistically significant improvement over its previous version. The authors also provide a new training scheme which could be applied to most classifiers so as to reduce the churn (and thus we can be sure that the model has really improved). Through experiments, the authors show that they significantly reduce the value of the "churn" metric by using three different classifiers on three different datasets. Pros: The formulation of the "churn" metric is useful especially when the classifier is trained on a constant stream of input data. Reducing the churn will ensure that additional resources spent contributed to significant improvement. In Table 3, what is V1 and V2. I assumed that they are the accuracies of the classifiers A and B. It would be helpful to discuss some intuitions behind why Eqn 2 and Eqn for Diplopia operator would reduce churn.