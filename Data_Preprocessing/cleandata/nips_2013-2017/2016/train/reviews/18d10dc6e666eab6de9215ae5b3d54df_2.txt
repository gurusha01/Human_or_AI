Bayesian optimization has become very popular for machine learning tasks like hyperparameter optimization, but most existing approaches are inherently sequential. The paper proposes a parallel Bayesian optimization algorithm that computes Bayes-optimal batches. The authors show that the proposed method outperforms existing Bayesian optimization approaches. The paper is well written and easy to follow. Parallelization of BO is an important subject for practical hyperparameter optimization and the proposed approach is interesting and more elegant than most existing approaches I am aware of. The fact a Bayes-optimal batch is determined is very promising. The authors assume independent normally distributed errors, which is common in most BO methods based on Gaussian processes. However, in hyperparameter optimization this assumption is problematic, since measurements errors represent the difference between generalization performance and empirical estimates (e.g., through cross-validation). Structural bias is common in certain regions of hyperparameter-space, especially given small sample sizes. I think it is worth mentioning that assuming independence is not always realistic. The theoretical aspect of the paper is strong, but the experiments are somewhat disappointing, for three main reasons: 1. The use of test set error as an optimization criterion is problematic, and it is well known that such score-functions are suboptimal for hyperparameter tuning (e.g., [1]). Threshold-based metrics like test set error introduce a number of problems for model selection and hyperparameter optimization, so it would be better to use metrics like area under the ROC curve or log loss. 2. The authors used the MNIST and CIFAR10 data sets as a basis of their experiments, which are indeed commonly used in benchmarks. I would have preferred to see benchmarks in HPOlib [2], which is a widely-used library specifically designed to benchmark hyperparameter optimization algorithms. 3. The amount of data sets (2) used in the benchmark is very low, and the entire benchmark is based on a single machine learning task (classification) with only two learning algorithms (logistic regression and CNN). I would have liked to see more variety in the bechmark, since it is known that some optimizers perform well for some types of learning algorithms but poorly for others. Overall, I like the paper and the theoretical contribution is definitely valuable. However, the current benchmark is too limited to be truly convincing. [1] Provost, Foster J., Tom Fawcett, and Ron Kohavi. "The case against accuracy estimation for comparing induction algorithms." ICML. Vol. 98. 1998. [2] Eggensperger, Katharina, et al. "Towards an empirical foundation for assessing bayesian optimization of hyperparameters." NIPS workshop on Bayesian Optimization in Theory and Practice. 2013.