This paper proposes to use a numerical sequence acceleration method (higher-order numerical integrator), the Richardson-Romberg extrapolation, to improve the convergence of SG-MCMC algorithm. It requires running two SG-MCMC chains in parallel, with different step sizes and correlated injecting Gaussian noise. The convergence of both asymptotic and non-asymptotic properties are provided. The experiments on synthetic Gaussian model verify the bias and MSE bounds of theorems, and the experiments of large-scale matrix factorization on real data demonstrate of the practical use of proposed algorithms over standard SGLD. This is a nice paper overall in terms of proposed technique and presentation. I feel that this paper proposes a valid contribution to the area of SGMCMC methods, and does a good job putting this method in context with similar previous methods. I have two main concerns: (1) The novelty of the paper may be incremental, given that the effect of numerical integrator on SGMCMC has been analyzed in [10], and proposed RR extrapolation has been used for approximation of invariant distributions in [23]. (2) The pracitcal use of algorithm can be limited, and there may be a lack of fair comparison in experiments. This is parallel implementation of SG-MCMC algorithm, and the fact of using correlated Brownian motion requires the two chains to communicate at every step. However, it seems the authors did not consider the communication cost, assume there is no cost. It is better to mention this. It is not clear how the computing resource is assigned for the SGRRLD and SGLD in experiment. For example, is the single chain of SGLD implemented using the resouce that the two chains of SGRRLD use? If not, this may be not a fair comparison for pratical consideration. If yes, then question is how to split the resource for the two chains of SGRRLD in the algorithm procedure. The computation cost of SGMCMC mostly comes from computing stochastic gradient, I would wonder how could partial resource used in SGRRLD for computing gradient at each chain lead to less wall-clock time, than full resource used in SGLD? Please add more details on the setup of implementation. Also, the alternative is to run two chains of SGLD embarrassingly (without communication), is it possible to compare with this? Minor comments: (1) "Unadjusted Langevin Algorithm (ULA)" appears twice, line 25 and 70.