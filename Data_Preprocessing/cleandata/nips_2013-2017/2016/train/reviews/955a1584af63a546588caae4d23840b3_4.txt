This paper derived the lower bound of attributes needed to achieve some precision levels, when the regression/classification models with different loss functions can only access limited attributes. The paper is well-written, and the theoretical proofs in the paper and appendix look solid to me. However, it would be more convincing if the authors can provide supportive experimental results. And I feel some description could be added in the appendix about the general ideas of proofs, e.g. why the theorems can be proved from fact that sets S1 and S2 are disjoint. In the proofs, the elements in vector x are either 0 or 1, I am curious about whether the results change under assumption that x belongs to [0, 1]^d.