This paper proposed a variation of LSTM model by introducing a time moderator gate, so that the proposed model can handle longer sequence input and keep memories for longer time. The newly designed time gate is a cyclic function of time and controls the degree to which the cells and hidden states are updated based on the current input or kept from the previous time step. Empirical results demonstrated the superiority of the new model compared with standard LSTM by capturing longer time memories. Improving RNN's capacity of capturing longer memories and thus handling longer sequences is a quite important problem. This paper proposed an interesting and effective idea on new gates whose value is computed by the input time. With different each cell in LSTM has the ability of capturing memories in different lengths, as illustrated in figure 2(b), thus it is suitable for longer sequence of over-sampled sequence input. Also the same idea can be generalized to other gated RNN such as GRU. In the meanwhile, I have a few concerns as follows: I am agreed that the proposed model can handle long sequences, but I'm not convinced that the designed time gate explicitly handles the event-based or irregularly sampled data. The purpose of introducing a time gate, from my perspective, is mainly about to limit the hidden states to be updated only at a small ratio of time. To be convinced that the proposed time gate is good for irregular or event-based sequence, I'd like to see the comparison of the proposed one with an almost same but random time gate: That is, the time gate kt is just a random number with the same distribution as it is (e.g., with prob. ron to be 1 and other to be 0, or other 'leaky' modifications) but independent of input time t. This random model is probably able to handle long sequence as well since the open ratio is also small. I'd like to see whether the propose model can model event-based sequences better. At this time point, the input time to the gate kt is more like just a random seed to generate the gate value with mean (ron) and does nothing related to the time step itself. In experiments, Figure 3(d) showed the proposed LSTM outperformed baselines in both high resolution sampling and async. sampling settings. But what is the average sampling rate in the async setting? It seems that even async setting has more input steps than standard condition. Then how can we tell if the baselines fail in this condition just because of the longer sequences than standard condition? The authors claimed that the new LSTM has 'an order-of-magnitude fewer computes'. I'm not sure the computes that they referred to are the numbers of training epochs before convergence, or the computing time for applying the model on one input sequence. From the descriptions in introduction section, the authors aim to handle an unnecessarily higher computational load and power consumption so that all units in the network can be updated with one time step. That is, the new model is desired to have efficiency by only partially updating its cells at each time step. However, this might not be true for the proposed model. First, during training part due to the 'leaky' behavior, all nodes are needed to be updated. Second, even only a small portion of states need to be updated, I'm not sure this can lead to more efficiency, since commonly neural network operations is done by matrix operations (especially on GPU), and whether one element is skipped or not doesn't make much sense. At the end of Section 3.3 the authors mentioned that the proposed LSTM needs 5% of updates per neuron, but this may not lead to 5% time. Or the authors should point out if their models are all implemented in other efficiently way.