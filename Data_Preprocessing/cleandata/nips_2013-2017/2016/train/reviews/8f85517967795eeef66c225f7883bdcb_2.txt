Authors adopt the "cooperative graph cut" approach of Jegelka and Bilmes and present an approach for approximate probabilistic inference, which is more involved than with basic pairwise graphical models due to the nonlocal "cooperative term" f in (1). The basic idea is to linearize this problematic term and to adapt it through the outer optimization task of (3), based on outer convex relaxations for lower bounding the log-partition function and on non-convex inner relaxations (mean-field) for the upper bound. Since probabilistic inference (as opposed to MAP) entails the corresponding surrogate of the entropy, the dependency of (3) on g is smooth (i.e. C^{1,L}) and hence there is no need to resort to inefficient subgradient-based schemes. The novelty of the paper concerns the knowledgeable assembly of established components. The resulting overall approach should find many applications. The title sounds too general to me in view of related work, see e.g. http://dx.doi.org/10.1007/s10107-016-1038-y and related prior work. More appropriate would be something like: Efficient probabilistic inference with cooperative graph cuts.

 The presentation lacks precision here and there, to some extent to lack of space, of course. I pick out three points and ask authors to clarify and to improve the presentation: (i) while the interplay between convexity and smoothness through duality is clear, a precise reference to a quantitative statement in the literature would be appropriate, in connection with theorem 1. (ii) Strong convexity of a functions means that subtraction of the Euclidean squared norm (multiplied with some fixed constant) does preserve convexity. Since this is not the case e.h. for the basic entropy function \sumi xi log(x_i), it is not immediate and obvious that global strong convexity holds for -\bar{H} (theorem 1). The claimed applicability of FISTA requires a global Lipschitz constant too. Please specify this point in view of the bounded feasible sets. (iii) If the quantitative assertion of Thm. 1 holds, then the Lipschitz constant depends on the number of labels $k$. In realistic application, this number can be much larger than in your toy experiments and hence should slow down optimization. Please comment.