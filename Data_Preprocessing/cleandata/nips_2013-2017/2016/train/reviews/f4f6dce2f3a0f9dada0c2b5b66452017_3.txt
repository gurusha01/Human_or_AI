The paper proposes a way to speed up tensor CP decomposition via ALS by approximately solving each least squares problem via leverage score sampling. The authors exploit the structure of the design matrix in each least squares problem (which is a Khatri Rao product of the other modes of the tensor) to efficiently sample rows according to a (bound on) their leverage scores and solve each least squares problem without having to form the KR product. The paper provides theoretical results on the number of samples necessary to solve the least squares problem with an additive approximation guarantee. Since errors do not accumulate over the ALS iterations the approximation requirements are mild and in the experimental results the technique is shown to provide speedups up to 30x with little or no loss of accuracy. The ideas and contributions of this paper are very nice and I hope to see more such speedups in other applications since ALS is such a general workhorse. Sampling proportionally to the leverage scores is not terribly new but I think this paper innovates well on the application side to meet the NIPS bar. The presentation could be improved substantially by making sure symbols are always defined before being used (such as R, n, etc.) and fixing the typos listed below. The assertion in line 160 is not immediately obvious and needs a proof or a citation. Typos: Line 5: the the Khatri-Rao -> the Khatri-Rao Line 13 significantly speedups -> significant speedups Line 22: from the tensor data -> from tensor data Line 25: has became -> has become Line 26: tensor analytic -> tensor analytics Line 52: SPASL -> SPALS Line 62: approximates -> approximating Line 149: \tau{i,j}^{A\otimes B} -> \tau{i,j}^{A \bigodot B} Line 163: matrix -> matrices Line 202: spare -> sparse Line 202: the the -> the