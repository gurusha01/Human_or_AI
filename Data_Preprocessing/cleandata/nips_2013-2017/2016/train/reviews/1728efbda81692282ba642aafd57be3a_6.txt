In the present paper the authors derive a spectral analysis of the Koopman operator in reproducing kernel Hilbert spaces and develop an algorithm to perform this analysis based on measurement data from a dynamical system. The Koopman operator is a concept from fluid dynamics, and the authors seem to imply it has relevance to machine learning. The authors demonstrate the algorithm and the analysis using two simple dynamical systems a damped linear 1D dynamics and a chaotic map. They also apply their method to real-world locomotion data and compare the performance with standard dynamic mode decomposition, PCA and kernel PCA, obtaining somewhat unclear results. The paper derives an interesting extension of dynamic mode decomposition, somewhat along the lines of older work on "kernelizing" techniques phrased in terms of linear operators (such as kernel PCA, kernel ICA, kernel independence tests, GP bandits, etc.). The authors provide some experimental results on largely artificial problems. The paper is overall clearly written, and has educational value for people foreign to concepts used in fluid dynamics. I am in general supportive of work which introduces machine learners to techniques in different disciplines. Here, this seems a decomposition technique for functions from M to M, that are used to construct a dynamical system (without any stochastic components). I am personally not aware of such setups being used in machine learning (typically, people use dynamical systems where random effects play a role, like innovations and noise), but I may miss something. In the current case, I think the authors need to do more work to convince ML readers of the practical usefulness of these techniques. What are potential use cases? Can you compete, at least to some extent, with the state of the art there? An ML discipline where inference and learning in dynamical systems is important, and where a lot of work has been done, is robotics. What would Koopman analysis buy somebody working in robotics? Or please pick any other domain. And while the theoretical derivations are well executed, I am not terribly surprised about any of them, given I know about kernel PCA or kernel ICA, or follow-up work on kernel independence tests or even "kernel Bayes rule". You have a linear operator, and you can do nice things if functions live in an RKHS. You end up with algorithms that do some sort of eigendecomposition on finite-D matrices. Some details: It is not quite clear to me, whether in the modal decomposition algorithm one needs the explicit representation in feature space. If so, this is a limitation compared to kernel PCA and the authors should make this more explicit. It would also be good if the authors would show the performance of standard dynamic mode decomposition on the toy example and the Henon map to demonstrate the differences in this simple setup.