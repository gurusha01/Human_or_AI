This paper presents an adaptive averaging heuristic for accelerating constrained convex optimization. The presented method provides the guarantee that it preserves the original convergence rate when the objective function is not strongly convex. This is an advantage over existing restarting heuristics and the paper shows this both theoretically and experimentally. I had a hard time to follow the content of the paper as it is out of my area of expertise. I assume that the used abbreviations and terms are known to those readers that are addressed by the paper, but I would suggest to include background information. The equations between lines 33,34 and 46,47 could benefit from a more intuitive explanation. Furthermore, I would like to know the relation to adaptive averaging methods used in non-convex optimization such as Adagrad [1] or Adam [2]. Finally, I am wondering about the practical significance of this work. The experiments were carried out in R^3 which is obviously a toyish setup. How does your method (and related work) scale to much higher dimensions? Minor comment: Reference to solid line in Figure 1 is ambiguous. Maybe use a dotted line or colors to show the trajectory of the primal variable? [1] Duchi, John, Elad Hazan, and Yoram Singer. "Adaptive subgradient methods for online learning and stochastic optimization." Journal of Machine Learning Research 12.Jul (2011): 2121-2159. [2] Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic optimization." International Conference on Learning Representations (ICLR). (2014).