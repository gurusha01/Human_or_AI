The authors study the robust k-means algorithm, which is the traditional k-means algorithm with gives the points error terms, and penalizes the errors to encourage most errors to be zero. They show that adversarially moving two points is enough to break down the RKM algorithm, in the sense that the estimates for the cluster centers become arbitrarily bad. Then they show a condition that the dataset is well-structured allows the algorithm to be robust to noise. The well-structured condition is that at least half of the points can be partitioned into k clusters with sizes upper and lower bounded, and the distance between pairs of centers are lower bounded. They also show that robust k-means has nearly all of the same consistency results as the traditional k-means. They also show optimality conditions for the RKM minimization problem. Finally, they show experiments in which robust k-means performs slightly better than a variant, trimmed k-means, for several datasets. This paper studies an important question, as the k-means problem is used often in practice. The robust k-means algorithm is a candidate for outperforming traditional k-means in certain settings, as it has nearly the same computational simplicity. Therefore, a theoretical analysis of robust k-means is worthwhile. The authors give two results on the robustness of this variant. They show a worst-case lower bound, and an upper bound under well-structured conditions. The conditions are a bit high, but these conditions have been studied in a previous ICML paper. The consistency result for robust k-means is interesting, and provides further justification for this variant.