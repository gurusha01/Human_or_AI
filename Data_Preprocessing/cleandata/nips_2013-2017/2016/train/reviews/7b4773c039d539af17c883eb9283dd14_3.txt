The paper provides a more refined algorithm, with analysis, for Monte-Carlo planning. The algorithm cleverly tries to merge Monte Carlo sampling (without action selection) with planning by ignoring other action branches when there is enough evidence to unambiguously decide the best action at a node. This allows the algorithm to explore only a subset of states reachable by following near-optimal policies. Analysis shows that the sample complexity depends on a measure of the quantity of near-optimal states. The paper also shows that the algorithm/analysis improves over best previous worst-case bounds under various conditions. The algorithm appears to be easy to implement. With a single relatively simple algorithm, the authors achieve a number of things: improved worst case bound when the number of states N is finite, bounds that depends on the size of the space explored by near-optimal policy, conditions that allows polynomial sample complexity when N is infinite, and behaviour that is similar to Monte-Carlo sampling when there is a gap between the value of the best action compared to other actions. This is nice theoretical progress. Although the algorithm is relatively simple to implement, it appears to me that the progress is still mainly theoretical -- a very large tree needs to be explored before useful results can be obtained at the root, unlike practical algorithms that are mainly anytime algorithms. Can an anytime version be developed? Otherwise, the impact may be limited to theory. After author feedback: Thanks for the answer. Since the algorithm can be made anytime, it would be interesting to see experiments with it in the future.