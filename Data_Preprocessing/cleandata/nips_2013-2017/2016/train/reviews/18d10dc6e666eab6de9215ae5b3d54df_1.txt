This paper is about batch-sequential Bayesian Optimization. A batch-sequential version of the Knowldege Gradient criterion is introduced. After some background on related work and Gaussian processes, the parallel (q-KG) criterion is defined. Its computation is detailed, following the route of the standard KG criterion, and a number of numerical experiments are presented where q-KG appears to outperform some state-of-the-art batch-sequential Bayesian Optimization algorithms. This is a very good paper and I think that it possesses important qualities that would justify its publication in NIPS. The potential impact on society of parallelizing Bayesian optimization algorithms is paramount. A minor criticism is that speed-ups (from sequential to batch-sequential) are not studied, and also a few statements are slightly imprecise in the literature review; for instance the idea of integrating EI with respect to posterior distributions was already present in "Kriging is well-suited to parallelize optimization" (along with Constant Liar), while Chevalier et al. presented notably CL-mix in [2]. Also q-EI maximization using natural gradient is studied in "Differentiating the multipoint Expected Improvement for optimal batch design". Finally I have the following remarks and questions:  Is A compact, f continuous?  Why restrict A to be an LHS at the initial stage ?  In Algorithm 1: what about hyperparameter re-estimation (or Bayesian updating)?  In 5.2: is it a max or a min in the definition of g?  About 5.2 again: the smoothness of g doesn't seem to go without saying.  Similarly, the derivative of the Cholesky factor might be non obvious.  Mat√©rn is 5/2 and not 2/5 I guess  About Figure 1 and the corresponding experiments: was it possible to fit the same GPs using different softwares?