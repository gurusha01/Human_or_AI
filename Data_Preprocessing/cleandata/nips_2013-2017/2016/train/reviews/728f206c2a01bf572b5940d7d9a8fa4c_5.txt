This paper proposes to use Wasserstein distance instead of the commonly-used KL divergence as the cost function of RBMs. It presents advantages of Wasserstein distance through toy examples and promising experimental results. I am convinced on the good direction this paper perused to apply Wasserstein distance to machine learning, and on the advantages of Wasserstein RBM which considers the metric structure of {0,1}^d while classical RBM does not. My main critism is regarding the writing. It is not self-closed with many outgoing links and requires one without prior knowledge of Wasserstein distance to read the refernce [4][5]. It seems that the author(s) didn't put much efforts on making it easier to read and broadening the potential audience. For example, it is useful to introduce, in the RBM setting, the optimal transport problem after equation 3, and then the dual problem. then the optimal dual alpha^\star. In its current form, it is not clear how alpha^\star is obtained. Similarly, in equation 6, it is not useful to point the reader to [5] without explaining the expression of alpha^\star(\tilde{x}_n) and its meaning. The author(s) are suggested to rewrite section 2 to be more self-closed and to be more intuitive. To save space the author(s) can put the proofs into supplementary and streamline section 3. As an unsupervised learning method, the proposed Wasserstein RBM has three additional hyper-parameters: gamma in equation 3, and lambda and eta in the end of page 4. The requirement of tuning these hyper-parameters puts a limit on the usefulness of the proposed method. As the author(s) said, "the proposed procedures can be seen as fine-tuning a standard RBM". Is lambda and eta necessary because they can give an improved energy surface that is easier to optimize, or because that without them the minima of the cost function is essentially different and gives trivial solutions? The Wasserstein distance is well defined on the parameter manifold including its boundaries, while KL divergence is not. This has a meaning in de-noising problems. If certain input dimension is mostly 0's with only a few 1's , a Wasserstein RBM can remove the noise and make the dimension deterministic, while KL-based RBM cannot. Do similar observations appear in the experiments? This might be a potential advantage of the proposed method. Minor comments: The title could be misunderstood as improving the training of the classical RBM. However the method proposes a different cost function and essentially is a different learning method. Equation 4. It is easier to read if the sum is over (x, x') instead of (xx'). Line 91: why the dual potential is centered? This is not straightforward and need more words. Section 3: the beginning has some overlapping contents with section 1 and can be streamlined.