This paper addresses the expectation-maximization (EM) algorithm for the estimation of the set of mean parameters for a model of mixture of Gaussian variables. The number of Gaussian variables, the weights of the mixture, and the covariance matrices of the Gaussian variables are known and fixed. The paper is theoretical and prove three negative results for the EM algorithm. 1) There can be, even for large sample size, local maxima for the likelihood function, with likelihood values arbitrary smaller than the maximal likelihood value. 2) The EM algorithm, randomly initiated, converges with high probability to a critical point with likelihood value arbitrarily smaller than the maximal likelihood value. 3) The same negative result holds for the gradient EM algorithm. I think that the question addressed by the paper is interesting and worthy of attention. In order to obtain rigorous proofs, the paper is restricted to a simplified mixture of Gaussian variable model: the number of components is known, the weights are known and uniform and the covariance matrices are equal to identity. I do not think that these simplifications are a problem, given that the proofs are already technical, and that the obtained results are already informative. These results are very interesting in my opinion, and can be useful to understand better the EM algorithm in practice. It should be noted that the paper is written as addressing a case of general dimension d, while the proofs address the case d=1. Perhaps the authors could write if similar proofs would be possible for arbitrary dimension d, and if not, mention briefly the additional difficulties of the case d larger than 1. My only concern about the paper is the proof of Theorem 1 (unfortunately, I did not read the proofs in the supplementary material). I think that this proof is too short, and I would have preferred it to be longer, at the expense of shortening some of the discussions in the paper. The part of the proof where I think more details should be given is from line 288 to 306. The authors provide limits (as gamma or R goes to infinity) of supremums of likelihood functions over some domains. I do not have a problem with the computation of limits of likelihood functions evaluated a fixed parameters, so that, for instance, I conceive that the three equations of line 288 should be correct. Nevertheless, obtaining the limit of a maximum is not entirely obvious to me, when the domain is unbounded or depend of the parameters that go to infinity. In the same way, the authors mention at some places that the likelihood function is continuous, but this does not imply directly the types of convergences stated in the paper, in my opinion. I think that more details, and so a longer proof, are needed so that the reader can be completely convinced of the validity of the proof of Theorem 1. Finally, I have an other minor concern with the part of the paper addressing the Gradient EM, from line 162 to line 172. I found this part more difficult to follow than the rest of the paper. In particular, perhaps the authors could give more explanation on how Equation (5) is obtained.