The paper proposes a version of CMA-ES with "Optimal Covariance Update and Storage Complexity". The authors suggest that the proposed modification does not change the behavior of the algorithm but makes it cheaper to run. The proposed method is called Cholesky-CMA-ES and this is misleading because this name was first introduced by T. Suttorp et al. 2009 for a similar algorithm and is well known. The authors do not mention this issue, instead they introduce their variant as "Our CMA-ES variant, referred to as Cholesky-CMA-ES". The proposed method is an incremental modification to the work of Krause et al. 2015. More specifically, the only algorithmic contribution is to call rankOneUpdate (it is also present in the work of Krause et al.) not once as in Plus strategies but in a loop for all individuals. Empirically, there is a difference on Cigar function in Figure b): the observed plateau has different objective function values for CMA-ES-Ref and Cholesky-CMA-ES. It might because of the way the authors plot median results. The paper should compare the proposed approach to Cholesky-CMA-ES by Suttorp (empirically) and Cholesky-CMA-ES by Krause (algorithmically). The claim about "Optimal Covariance Update and Storage Complexity" is a bit misleading. It is not optimal when we consider Sphere function where we don't need the covariance matrix at all, on separable functions we may deal with the diagonal only. The proposed approach is only two times cheaper than the omitted Cholesky CMA-ES by Suttorp. The latter is claimed in this paper to be prone to numerical instabilities but the original paper suggested the opposite. Again, its omission in Figure 3 can mislead the reader regarding the actual contribution of the paper. I don't think that the proposed approach changes something for CMA-ES regarding large-scale optimization since the complexity is still quadratic. However, I admit that the proposed modification might be the best thing one can do to save time and memory when the full covariance matrix is considered. Overall, I think that the work is too incremental (algorithmically, it is only one "for" loop). I suggest the following modifications: a) Resolve the naming issue; b) Perform experiments with variants of Suttorp et al. and Krause et al. and update all 3 figures. If they will become unreadable, then shown only the best performing variant in the main text and show the full figures in Supplementary Material; c) Better explain that the main algorithmic contribution is the "for" loop in Algorithm 1 or explain why it is wrong to think so. Minor notes: "Typically, an eigenvalue decomposition of the covariance matrix is performed in each update step." This is atypical, i.e., in all proper implementations it is done periodically.