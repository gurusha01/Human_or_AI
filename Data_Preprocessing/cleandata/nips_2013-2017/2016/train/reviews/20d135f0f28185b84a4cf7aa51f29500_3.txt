This paper proposes a simple solution to the multiple choice learning problem where the learners are deep networks. Current solutions to MCL do not directly extend to deep networks. This paper shows that a simple layer of winner-takes-the-gradient in back propagation allow them to combine the training of the learners with the assignment problem in an ensemble setting. Experimental results show that this method performs well across wide range of problems: Image classification, image segmentation, and captioning. Strengths: - In general, the problem of MCL for deep learning is of enough importance to the NIPS community. - I like the simplicity of the proposed method. - Experiments show results on 3 different tasks. Weaknesses: - My biggest concern with this paper is the fact that it motivates "diversity" extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity. - The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions: - The first sentence of the abstract needs to be re-written. - Diversity should be toned down. - line 108, the first "f" should be "g" in "we fixed the form of .." - extra "." in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.