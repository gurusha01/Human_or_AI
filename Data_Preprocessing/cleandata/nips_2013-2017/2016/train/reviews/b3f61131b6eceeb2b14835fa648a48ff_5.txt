The paper focus on a relaxation of the standard no-regret property in online algorithms which they define as a low approximate regret property and which allows a small a multiplicative approximation factor of regret. This property is satisfied by many learning algorithms and the flexibility of the multiplicative term in the definition allows for a smaller error term, since the standard error term can be "rolled" in the multiplicative factor. This approximate low regret outcomes still provide price of anarchy guarantees in (\lamba-\mu) smooth games and the small error term after a smaller number of period of play. The results require only realized feedback, or even just bandit feedback in some cases, not the expectation over actions of other players, I think the trick of additive error term to the multiplicative error term is interesting and is exploited well in the paper. There is a lot of technical overlap with both SAL15, LST16 which makes me less enthusiastic about an oral level presentation. Also, I think the paper language to some extent is overselling the result to the point where it becomes a distraction. Does learning really converge fast (to equilibria) in games? The main point here is that if we relax the notion of convergence (from convergence of the actual behavior to convergence of time-averages) and if we relax the notion of target states (to be states where agents have low approximate regret) the (\lambda-\mu) smoothness results still carry over. The technical points are interesting in themselves. Stretching the interpretation of convergence, equilibrium to become increasingly inclusive so as to get faster "convergence" detracts from these points. Also, if this is a result for general games (as the title suggests) what are the practical implications for the class of two player games? Or even for 2x2 games?