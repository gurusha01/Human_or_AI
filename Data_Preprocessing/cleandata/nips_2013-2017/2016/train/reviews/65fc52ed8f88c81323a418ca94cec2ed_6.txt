This paper presents an unsupervised algorithm for training deep neutral networks. It is based on exemplar-based methods for similarity learning, but tries to tackle a few intrinsic limitations of this type of approach (line 91-101). The proposed algorithm alternates between (1) partitioning dataset into batches of compact cliques; and (2) optimizing the neutral network with a softmax loss on pseudo clique labels. Experiments on posture analysis and pose estimation problems demonstrate great performance, which is even competitive to fully supervised approach in some cases. The proposed method is technically sound. With "offline" batch / clique data partitioning, it seems to well solve the problems of traditional exemplar-based approaches. This advantage can be seen from the compelling results in experiments section. The only concern of mine is that whether such an alternating approach would make the learning process less efficient, or less automatic (if extra hyper-parameter tuning is required across iterations / epochs). I'd love to see some discussion on these matters, as well as how the network performance improves over iterations / epochs. The presentation can be improved, e.g. - What are f() and r() in Eq. (7)? - (line 93) The ratio of one exemplar and many negatives is highly imbalanced ... - (line 88) ... learning similarities between all pairs of a large number of exemplars