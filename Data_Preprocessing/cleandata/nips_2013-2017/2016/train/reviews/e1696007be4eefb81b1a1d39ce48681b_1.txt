The authors investigate estimating GLMs when n is much larger than p. They exploit the fact that the GLM estimator is sometimes proportional to the OLS estimator and show this holds more generally for random design problems. They propose a two step estimator: firstly a fast OLS estimator then using a fast root finder to estimate the constant of proportionality. Finally on benchmark datasets they show their method reaches a desired test error much quicker than a large variety of standard first and second order optimisation algorithms. Overall I think the contribution is nice. The paper is well written and presented. The main idea is a good one and appears novel. That said, I have the following concerns: The experiments seem extensive but there is a glaring omission. A stochastic optimisation method should have been used for comparisons. For example I have a strong feeling that SVRG [Johnson&Zhang 2013] would have performed better than most of the other competing methods. The theorems and in particular prop 2 only hold for sub-gaussian random variables. The results are nice but the naive subsampling estimator will certainly perform badly when the data has heavier tails. This is why Dhillon et al and many others have proposed random-projection based estimators. Although using these will immediately add an extra factor of O(np log p) to the computation time. It would have been nice to see experiments with heavier tailed data to demonstrate the robustness of the algorithm. Therefore, it is not clear exactly how useful the algorithm is as it is presented. A practitioner would have to exercise care when dealing with potentially heavy-tailed data but might anyway get the same speedups (without having to be as vigilant) by using a stochastic variance reduced optimisation method.