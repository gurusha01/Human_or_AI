This paper introduces so called cooperative inverse reinforcement learning (CIRL) model for learning from demonstration problems. It assumes the learning process is a two-player Markov game with identical payoffs between a demonstrator and a learner. Authors reduce the computation of solving the Markov game as solving a POMDP problem making the model computation applicable to small problems in practice. The authors formulate the apprenticeship learning problem as a turn-based CIRL with a learning phase and deployment phase. The key contribution of this work is to show the claim that the learner's maximizing reward response (learner's policy in deployment phase) to the demonstration of CIRL model (learning phase) outperforms the expert demonstration assumed in the earlier work of inverse reinforcement learning. This claim is verified by a coffee supplier counter example and also a 2D navigation grid experimental setting. This work seems novel and can be useful in the case where the learner can interact with the demonstrator. However, the experiment seems not sufficient to convince us that the model is useful in practice especially since there is no real data experimental setting. 1.The counter example assumes different action domains of demonstrator and learner. Is this the reason that the best-response for H to \pi^R violates the expert demonstrator assumption? What if the embodiments setting of demonstrator and learner are the same? 2.It is important to have real experimental result to convince us that the CIRL works in practice. 3.Corollary 1 doesn't interpret the symbol \Delta^\theta which I assume is the domain of R's belief.