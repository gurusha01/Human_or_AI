This paper presents methods aimed at reducing the prediction churn (changes in the predictor) without sacrificing the potential gain in accuracy that a change in the predictor could introduce. After well motivating the problem, the paper proposes two stabilization operators that can be used in conjonction with a MCMC chain that iteratively perturb the training sets; hopefully mimicking the changes that could occur in practice as more data is gathered and new features are added. Theoretical results (in a restricted setting) are presented that support the proposed method along with experiments on real data sets. The paper is well presented and the problem well motivated. The proposed stabilization operators make sens but introduce two hyperparameters to tune. To be effective, the proposed MCMC chain should mimick the changes that are likely to occur in practice: ie, the number of available training examples is (almost always) increasing with time and the number of features is usually increasing with time. Unfortunately, the proposed MCMC chain keeps both of these numbers constant (or approximately constant) over time and this affects the potential impact score pretty much...