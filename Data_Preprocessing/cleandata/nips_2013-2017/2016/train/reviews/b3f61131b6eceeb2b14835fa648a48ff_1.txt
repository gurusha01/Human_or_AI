The paper presents an analysis of a wide class of learning algorithms in smooth games, demonstrating fast convergence. This is an interesting result within game-theoretical control - traditional results prove convergence, but the rate is somewhat new. The authors have brought together techniques from learning theory, with results based on price of anarchy in smooth games. The results are interesting to the game-theoretical control community. I enjoyed the paper, as somebody who knows both game theory and learning theory. The results are novel and interesting, and I think important for the area of game-theoretical control. There is a lot of material here - fast learning for both full information feedback and bandit feedback, in expectation and in high probability, and in addition similar results in dynamic population games. However this is, in essence, the major failing of the paper. So much has been crammed in to such a short space that the work is poorly motivated, terms are not well defined, and I struggle to reconcile the specialist knowledge needed to appreciate the work with a general conference such as NIPS. Post-rebuttal: Given my complaints of too much material, and your (fair) rebuttal, perhaps you could try to amend the intro slightly to emphasise more that you feel your major contribution is demonstrating the benefit of considering low approximate regret, and to provide examples of the power it gives - I (incorrectly it turns out) read the paper as if you were most pleased about being able to show Fast Convergence of Common Learning Algorithms in Games. (Perhaps a title change should be considered?)