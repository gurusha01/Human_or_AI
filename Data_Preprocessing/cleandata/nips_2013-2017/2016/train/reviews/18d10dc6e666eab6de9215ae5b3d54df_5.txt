This paper derives a batch-version of Knowledge Gradient (KG). To find an optimal batch of design points, a gradient estimation method based on Monte-Carlo simulation is proposed. The experiments show that the proposed method outperforms other batch Bayesian optimization (BO) methods. 1. As I know, the batch KG is not newly proposed method by this paper. Please clarify the contribution of this paper with respect to previous batch KG papers such as [1] and [2]. In my opinion, the contribution is estimating the gradient for finding the optimal batch. If so, the efficiency/effectiveness of the proposed method should be compared with the previous batch KG papers by experiments or possibly mathematical analysis. 2. Section 5.2 should be elaborated more, since I think the section is the key point of this paper. 3. Basically, EI assumes noiseless observations. Hence, the comparison in the noisy setting is unfair. Does a variant of EI regarding noise is used? If the plain EI is used, the results in Section 6.2 should be discussed with the reason that EI does not consider the noise. 4. In line 162, the abbreviation "IPA" is redefined. [1] Yingfei Wang, Kristofer G. Reyes, Keith A. Brown, Chad A. Mirkin, and Warren B. Powell. 2015. Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science. SIAM J. Sci. Comput. 37, 3 (January 2015), B361â€“B381. DOI:http://dx.doi.org/10.1137/140971117 [2] http://castlelab.princeton.edu/theses/Peng%20-%20Senior%20Thesis%20-%20May032010.pdf