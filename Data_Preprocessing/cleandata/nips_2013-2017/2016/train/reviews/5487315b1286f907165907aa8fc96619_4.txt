The paper presents a technique for bi level modeling input -> latent -> output where the latent layer is generally a discrete (possibly combinatorial) structure. The set up is applicable to some problem like transliteration in NLP where the latent representation is the bipartite matching. The goal of the paper is to reduce the learning problem to a convex problem which can be solved efficiently provided any linear maximization subject to the latent space constraints (the polar operator) can be solved efficiently. The authors correctly point out that usually this problem is modeled as a bi-level optimization: first the latent layer is selected conditioned on the input and then the output is predicted conditioned on the latent layer. This paper massages the problem by relaxing the discrete latent variables into continuous variables. After using the properties of duality, the paper finally obtained a SDP formulation. With appropriate relaxations, the SDP can be convexified and solved efficiently. The paper attacks a reasonably solid problem and the approach generally seems reasonable. I think the paper can benefit from better exposition of technical derivations. In particular, the derivations in page (4) can benefit greatly if the authors indicate what is the roadmap i.e. why are the derivations being made. Also I am not completely certain about some of the claims and would like clarifications for those: 1) Equation 19 contains maximization w.r.t \pi \in S. However the function being maximized is quadratic w.r.t. \pi -- how is that handled (since the assumption is only that linear functions over \s are tractable). 2) Also I think the authors erroneously claim that the rank relaxation is the only relaxation they introduce -- they also relax the latent variable 'y' from discrete space into continuous with the perturbed term, which is the key relaxation assumption. Please clarify.