This paper focuses on unsupervised domain adaptation and on the existence of a source-target classifier mismatch besides the marginal distribution difference of source and target. The difference among the classifiers is expressed as a perturbation function (as in [25,26]): this is learned with a new deep residual network which also integrates feature learning and feature adaptation to reduce the marginal distribution shift. This paper smartly builds over the deep network that won the ILSVRC challenge in 2015 proposing to use it for domain adaptation. The explanation about how the residual block is added to the CNN architecture and used to estimate the perturbation function to the target classifier is very clear and constitutes an interesting novel contribution. Moreover, as far as I'm concerned, the entropy minimization principle [28] was not integrated before in a domain adaptation network and the MK-MMD is also a new variant of what used in [5]. On the experimental side, the obtained results are quite convincing but there might be the need of tests on more challenging testbeds. It might be also useful to clarify on the basis of which measure figures 2-c,d appear better than 2-a,d: maybe indicating the average class-to-class distance could help to clarify the claimed improvement -- the figure is not fully self-explanatory.