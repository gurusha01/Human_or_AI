Authors address an online learning problem where at each iteration the objective is to choose a list of items given the contextual information and based on which the learner receives some scalar feedback values for each individual selected items and a reward value: authors refer this problem as contextual semi-bandit problem in partial feedback setting, and proposed efficient algorithms (with sublinear regret guarantees) for this problem on different settings. 1. The problem formulation and contribution looks notable although the proposed algorithms are not intuitively well motivated. 2. The paper is difficult to follow, its not well written, there are various notational inconsistencies/ill defined quantities, e.g. definition of p{min} is vague; in Algorithm 1 (VCEE), Qt and \tilde{Q}_t are not properly defined. 3. Its not clear how the regret guarantee depends on the size of the policy set, authors mentioned that the dependence is O(log|\pi|) in the introduction but its not clear from Theorem 1 and 2. 4. It might be worth giving some insights on how this algorithm can be generalized for more general reward functions other than the linear structure which is currently addressed in the paper.