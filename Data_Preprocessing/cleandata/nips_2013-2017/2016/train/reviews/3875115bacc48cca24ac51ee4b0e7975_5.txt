The paper presents an answer to the open question asked by Srebro in his 2007 paper "Are there local maxima in the infinite-sample likelihood of Gaussian mixture estimation?". This paper answers the question in the negative through the construction of a general class of counterexamples. The authors begin be outlining the schemes by which the likelihood is maximized in the setting of Gaussian Mixture Models. These include the EM and gradient EM algorithms. The authors also address the issue of initialization and give a common choice. The authors then state their primary theorems, which give the counterexample to the question by Srebro [2007]. The authors further show even with the common choice of random initialization, the EM and gradient EM algorithms converge to the non-optimal maxima with exponentially high probability. The authors then give some intuition for their proofs by proving a simple case (k=3, d=1). This paper is very clear and well written, and balances well the space requirements of the paper with the need to give intuition for their theoretical result. The question answered by the authors is of great importance for further understanding the issues that arise with non-convex optimization, even in the asymptotic limit for simple distributions. I did not carefully check the supplementary material, but some estimates of the constants in the probability term that they give would be nice. This is important because it seems to me that one would never consider k to be too large. Estimates of the constants would be nice to see when we could start to see the convergence to bad solutions in the asymptotic limit (i.e. where is the phase transition?). If the constants are too complicated or do not make sense, supporting simulations would be nice to at least this is a phenomena that we can observe. However, this issue does not detract from the novelty or impact of this paper.