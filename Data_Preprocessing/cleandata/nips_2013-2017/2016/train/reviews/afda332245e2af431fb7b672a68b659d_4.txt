This paper proposes an exploration strategy for deep RL. In particular, the authors derive a 'pseudo-count' from a sequential density model of the state space in ALE and claim to make explicit a relationship between information gain, prediction gain, and their proposed pseudo-count quantity. The authors present breakthroughs on ALE, making significant progress in Montezuma's Revenge. Exploration is a key problem in RL; the authors revisit old ideas and derive new insights to improve on the state of the art. This is a good paper, with strong experimental results, but I still would like some additional explanations: - The pseudo-count quantity is derived from the so-called recoding probability of a density model, which is shown to be consistent with the empirical count. Say you have a specific state element x, which corresponds to some pseudocount N(x). Updating the density model with this element would lead to an updated pseudocount N'(x), which is consistent with the empirical count. But what happens to N(x) if you update the density model with a different element y? I would guess that N(x) can go down (which is obviously not consistent with the empirical count). This seems to relate to Fig. 1 (right),in which the pseudo-count has up/downward spikes. - Somehow I have the feeling that the notion of pseudo-count is somewhat redundant to the whole story. It seems that the authors can derive prediction gain (PG) and immediately derive information gain (IG), with no additional data needed. IG is well-established as a notion of intrinsic motivation. My question is, why work with pseudo-count? It seems to be quite a loose bound on PG/IG, which makes me think of it to be inherently less stable. If the whole pseudo-count measure would be removed from this paper, we basically have: measure the size in update to the density model and use this as an intrinsic reward. - It seems to me that the authors' definition of IG is different from what is used in literature (e.g., Planning to Be Surprised by Sun2011). In particular, it seems that an agent can gain infinite information over time by following a predefined policy, without actually exploring. This seems invalid. If the authors' IG quantity is different from what is generally established in literature, how could the authors relate it to it? If the authors' IG is a new quantity, what is the added value of of the authors' contribution 'a relation between IG and pseudo-counts'? Minor comments: - Corollary 1 reads quite dense. - In Fig 1 (left) the pseudo-count goes up in periods without salient events, is this due to generalization across states?