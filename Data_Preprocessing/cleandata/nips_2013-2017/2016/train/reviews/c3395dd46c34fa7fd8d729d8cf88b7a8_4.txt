The paper proposes a game model for cooperative learning in Human-Robot scenario. It formalises an inverse RL problem within the framework of cooperative partial information two-player game. The paper deals with interesting problem cooperative learning in Human-Robot setup. The author has shown how their approach relates to the current SoA. The paper is well-written; however the overall contribution (both technical quality and novelty) is doubtful. The main arguments supporting this statement are: • The DBE assumption is strong enough and I doubt whether it can be fully applied to practical examples. Reason: strict dependence of H's reward on success of R's learning (H's internal reward function) has a significant influence on R's learning (a kind of positive feedback). • It is unclear what cooperation is supposed in the described H-R scenario when H and R collectively solve a single decision problem and how the considered type of cooperation influences the reward function. This should be commented. • Proof of Theorem 1 provided in the supplementary material is either wrong or not clear enough. In particular statement "R can also simulate C" contradicts Definition 2, in particular definition of C's actions, which include a decision rule for H. • Proof of Theorem 3 provided in the supplementary material is does not sound mathematically. • Feature function and its role is not clearly described. • Minor: Some symbols and notions are not introduced: for instance \delta_{\Theta} is explained neither in the main text (p.2 line 217), nor in supplementary material (p.2, Corollary 1); \eta (page 7, line 299) is not introduced too.