The submission studies matrix completion. A number of non-convex optimization methods for this problem are popular and perform well in practice, and there has been recent progress on showing that various methods in this class converge to a global optimum if initialization is performed carefully. The goal of the submission is to show that for a regularized version of the problem, all local minima are global minima. Hence, any optimization method that converges to a local minima will converge to a global one. Note that a number of popular algorithms, such as SGD, satisfy this property when initialized with a random or arbitrary initialization point. The results in the submission are also robust to noise. Results in this vein are known for a handful of other problems, such as dictionary learning, but these problems differ qualitatively from matrix completion. A quick summary of the (mild) limitations of the result: the proof considers specifically symmetric matrices, leaving the asymmetric case as an open question. It is also specific to the case that the objective function to be minimized is Frobenius distance from the observed matrix entries. The result makes the (apparently standard) assumptions that the condition number of the matrix is bounded, and no row makes up too big a fraction of its Frobenius norm. The analysis is interesting. It uses first- and second-order optimality conditions that any local minimum x must satisfy, to conclude that x has to equal a global minimum. In more detail, the second-order condition ensures that x has large L2 norm. The first order condition ensures that x has small L{infty} norm. And if x has small L{infty} norm and large L2 norm, then the first-order condition also ensures that x is (close to) a global minimum This is a nice contribution to an important and well-studied problem. The result gives some theoretical justification for the empirical fact that popular non-convex methods for matrix completion seem to do well in practice, even when not much effort is put into initialization. The analysis is interesting and addresses specific properties of matrix completion. I think the submission is clearly above the bar for NIPS. One issue that could benefit from additional discussion: exactly what properties of the regularizer does the analysis require/exploit? And how does the proposed regularizer compare to those actually used in practice?