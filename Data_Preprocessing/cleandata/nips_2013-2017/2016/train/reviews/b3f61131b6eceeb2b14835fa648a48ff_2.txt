The paper presents a number of new results in the decentralized no-regret dynamics on smooth games. The authors propose a new type of property called "Low Approximate Regret", generalizing previous classical algorithms. New results also include different feedback machanisms, the bandit case, and population games. This paper presents a number of new results that follow-up on SALS15 (and LST16). The results are significant and interesting to this community. The main contributions (improved convergence bounds and in the different feedback settings) are possible due to a reformulation of no-regret called The Low Approximate Regret property, which bounds the difference between the approximate (1-epsilon) cost paid and the comparator. The authors show that this property holds for a number of well-known learning algorithms and shows regret bounds based on this property. If there is any problem with the paper, it's that it contains too much content. As a result, the bulk of the derivations are in the appendix and only the main points are summarized in the paper. This makes the paper a nice read and results easier to contextualize with related work in the area. However, it does feel that there are frequent jumps to different settings and topics throughout the paper. Also, though it is not a requirement, it is always nice to have experiments to complement/validate theoretical results and this paper has none, whereas SALS15 also showed the practical application. The paper ends also ends abruptly with a conclusion. One point of clarification: the authors use the term "realized feedback" and then later use the term "full information feedback" to mean the same. Consistent terminology should be used throughout the paper; if these are not the same, please explain the difference. In the bandit case, the authors should distinguish between the 'expected Low Approximate Regret' in Lemma 4 and Low Approximate Regret in Equation (1).