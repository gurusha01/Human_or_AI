This paper addresses the problem of learning deep feature embeddings. A key challenge is the non-uniform density of the feature space, which makes effective hard negative mining difficult. The paper proposes a unit that takes into account the mean position of a pair of features (inspired by [35]), along with their difference, in computing a similarity between the feature pair. The positional information allows the unit to account for the non-uniform density in feature space. The unit is differentiable, and can be plugged into existing CNNs. A double-header hinge loss is formulated over pairwise differences/similarities before and after the unit. For each minibatch of SGD, hard quadruplets are mined, where a pair of positive examples with low similarity are found, and corresponding negative pairs with high similarity are found. The approach is evaluated on the task of image retrieval over the CUB-200-2011 and CARS196 datasets, and on transfer/zero-shot learning on ImageNet-10K and ImageNet 2010 datasets with improvements shown for all tasks, particularly with respect to training time. The paper addresses a known important problem in computer vision, and the formulation is novel as far as I'm aware. I found the paper to be well written and references good. The results look interesting, and the combination of the PDDM unit and quadruple hard mining appear to offer benefit. I'm a bit confused by the "PDDM score" versus the "Quadruplet+PDDM" rows in Table 1. Just to clarify, do they correspond to retrieval using the output of the PDDM module (former) versus the learned embedding before the PDDM module using Euclidean distance (latter)? I'm curious how important is having the embedding loss E_e in Equation (4) since it appears that gradients can be backpropped through the PDDM modules. Are the features for hard negative mining computed every minibatch round, or are they cached and refreshed from time to time for efficiency?