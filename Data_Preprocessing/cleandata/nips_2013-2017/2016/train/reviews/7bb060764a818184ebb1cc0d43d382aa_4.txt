This paper presents an improved dropout approach for shallow and deep learning. Instead of uniform sampling as done in the conventional dropout, the proposed approach drops the neurons according to a multi-normal distribution. It is claimed that the improved dropout performs similarly with batch normalization. A few experiments are conducted to justify the powerfulness of the proposed approach. Overall, I like the ideas of data-dependent dropout and evolutionary dropout. I also think it is interesting that it performs similarly to batch normalization. What is not satisfactory is that experiments are not strong enough. It is suggested to add more results, e.g., in a table form, to compare with state-of-the-arts and results in ImageNet.