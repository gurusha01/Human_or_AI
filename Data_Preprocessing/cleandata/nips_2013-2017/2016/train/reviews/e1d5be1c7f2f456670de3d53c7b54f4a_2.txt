The authors study the problem of contextual semi-bandits and efficient algorithms for this problem when a supervised learning oracle is provided. The authors consider the case when the reward is a known linear function of each individual feedbacks and also the case when the linear transformation is unknown. In the case when the linear transformation is known the algorithm proposed is similar to the one provided by Agarwal et al [1] but with some differences. In the case of unknown transformation the algorithm is a phased explore-exploit problem where the reward vector is estimated first and then used with an optimal policy. The paper is fairly well written. Comments: 1) the OP problem is stated as a feasibility problem. Why not consider the optimization version where we try to find the policy with the smallest possible empirical regret subject to variance constraints (i.e optimize LHS in equation 4 subject to constraint 5). My guess is that the AMO is more suitable for solving the feasibility problem but it would be good to see it explicitly stated. 2) It would have been nice to see more discussion about how the AMO is used in the main body of the paper. For example let us say my policy class is linear classifiers then what does the AMO do?