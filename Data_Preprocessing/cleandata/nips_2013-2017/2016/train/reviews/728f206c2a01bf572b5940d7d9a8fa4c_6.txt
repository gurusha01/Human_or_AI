The paper introduces the idea of augmenting the standard MLE objective for training an RBM with a smooth Wasserstein distance. The Wassterstein distance is a function of a chosen metric on the data space, and as such encourages the model to put mass in areas not far from the data according to the metric. This results in more plausible samples from the model at the possible expense of coverage. The paper is clear and well-written. The clustering or shrinkage property observed in the experiments is compelling and could be very useful if the method can be scaled to problems of practical interest. On this note I think it would be useful to include details on how long experiments took to run, and more generally some information about complexity of the steps involved would be nice. Objective functions that induce more realistic samples is currently a hot topic in the variational autoencoder / generative adversarial network community, and I think that this work makes a useful contribution along related lines for the RBM. If there were possible applications to those methods as well it would be well-worth exploring.