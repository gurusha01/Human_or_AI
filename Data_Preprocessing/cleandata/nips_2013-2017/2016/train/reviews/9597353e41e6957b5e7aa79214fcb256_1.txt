This paper studies an interactive semi-supervised clustering setting where the learning algorithm is trying to minimize the k-means cost given access to an oracle that can answer pairwise same-cluster queries (i.e. given two points it reveals whether they are in the same cluster in the optimal k-means solution). The paper proves three results. (1) under a margin assumption a randomized polynomial-time algorithm can find the optimum using k^2log(n) queries. (2) even with the margin assumption, without queries minimizing k-means cost is NP hard. (3) Minimizing k-means cost is NP hard even with weaker margin condition and with few queries O(\log(k) + \log(n)). I'm curious about the tradeoff between the three knobs in the problem, the margin assumption, the running time, and the number of queries. I wonder if something more general can be proved about this. For example, on one hand, no matter the margin, we can always use O(n^2) queries to solve the problem in polynomial time, while on the other hand, Thm 10 shows that even if the margin is moderately large, if we use no queries the problem is NP-hard. I'm not saying this is a shortcoming of the paper at all, but I wonder whether one could prove something like: for margin gamma, the problem is NP-hard if the number of queries is < f(n,k,gamma) and it is solvable otherwise. I am also curious about what happens if the oracle either makes mistakes (seems natural in many applications) or is misaligned with the k-means optimum. For the former, suppose that the oracle, for each pair of points and with some probability q makes a persistent mistake about whether the pair is in the same cluster or not (by persistent I mean that you cannot resample to denoise the query).For small q, can we design a clustering algorithm in this case? For the latter, suppose the oracle has in mind a clustering with k-means cost that is slightly worse than the optimal. Can we use this oracle to find the optimal k-means solution efficiently? This also seems like it could be possible under the margin assumption, because if the oracle is close to the optimal cost, its clustering must be well aligned with the optimal clustering. It would be cool to explore these questions.