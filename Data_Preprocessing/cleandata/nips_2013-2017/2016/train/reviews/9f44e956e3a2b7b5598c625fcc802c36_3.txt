The authors introduce an intermediate timescale to neural networks, and demonstrate that it improves computation. The new addition effectively adds an input corresponding to the correlation of the current network state with past states. This is a nice and timely work. The ideas are well presented and motivated, and the simulations are appropriate. Novelty – intermediate timescales were introduced in the context of reservoir computing, but not in more standard machine learning tasks as done here. Furthermore, the specific form of addition here is novel. The paper claims that the main contribution is to computational neuroscience. From this perspective, there are a few puzzles. How plausible is the proposed normalization procedure? Why are the matrices W and A in equation 2 applied to different activity vectors (what is the Biophysical equivalent of the inner loop). Specific comments: Size of mini batches – this seems to limit memory, but the issue is not discussed. Table 3 – should be percent correct and not error Line 277: reference missing It seems that the advantage of fast weights is mostly for small networks – this could be an indication of some larger problem, but it is hard to judge from available results.