The paper leverages the ODE interpretation of Nesterov's acceleration of Krichene et al. in NIPS 2015 to design and evaluate a natural heuristic way to adaptively reweigh the iterates of the algorithm to speed up convergence. In contrast with previous adapative averaging heuristics, this method is guaranteed to dominate the standard fixed-schedule averaging. The empirical evaluation shows that the method yields speed-ups on a general class of functions. I enjoyed reading this paper. The mathematical argument is simple and easy to follow, especially given knowledge of the work of Krichene et al. At the same time, the empirical results show very promising speed-ups. While the technical development is a close extension of Krichene et al, I believe that this paper has a large potential impact, because Nesterov's acceleration is so pervasive in application. Too many times, even within empirical evaluations, a new, possibly highly adaptive method is compared to Nesterov's acceleration with a fixed weight sequence. However, as this paper confirms, rendering Nesterov's method adaptive only requires a small modification and can lead to dramatic improvements. To improve the paper from a technical point of view, I would recommend providing an analysis of the discretized process showing that the rate of convergence is preserved. If I am not mistaken, this should be relatively straightforward. I would also be interested in possible connections between the adaptive averaging of Nesterov's method and the Conjugate Gradient method for strongly convex quadratic optimization.