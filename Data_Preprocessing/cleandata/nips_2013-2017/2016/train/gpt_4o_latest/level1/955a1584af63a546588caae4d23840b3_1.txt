This paper addresses the problem of linear regression and classification in the Limited Attribute Observation (LAO) setting, where the learner can observe only a fixed number of attributes per example. The authors provide novel information-theoretic lower bounds for regression with absolute and squared loss, as well as classification with hinge loss, establishing fundamental limits on the precision achievable by any algorithm in this setting. Complementing these lower bounds, the paper also introduces a general-purpose algorithm for learning with missing data, achieving error bounds that are close to the theoretical limits.
The paper builds on prior work in regression with missing data, such as the efficient algorithms and sample complexity bounds for squared loss regression by Cesa-Bianchi et al. (2004) and Hazan and Koren (2012). However, the authors extend these results by proving impossibility results for achieving arbitrarily low error in regression with absolute loss and classification with hinge loss, even when observing all but one attribute. These results are significant as they close gaps in the literature and provide a more complete understanding of the LAO setting. The proposed algorithm further generalizes existing approaches, such as AERR, by accommodating a broader range of loss functions.
Strengths:
1. Novel Theoretical Contributions: The paper provides the first lower bounds for regression with absolute loss and hinge loss classification in the LAO setting, advancing the theoretical understanding of learning with limited observations.
2. Complementary Algorithm: The proposed algorithm is general-purpose and achieves error bounds close to the theoretical limits, making it a practical contribution alongside the theoretical results.
3. Clarity and Rigor: The proofs are detailed and rigorous, with clear connections to prior work. The authors also provide explicit constructions of hard distributions, which strengthen the theoretical claims.
4. Significance: The results address a fundamental problem in machine learning and have implications for real-world applications, such as medical diagnosis, where data collection is often constrained.
Weaknesses:
1. Exponential Gap for Classification: While the paper establishes lower bounds for hinge loss classification, the gap between the lower and upper bounds is exponential, leaving room for further refinement.
2. Limited Empirical Validation: Although the theoretical results are strong, the paper lacks empirical evaluation of the proposed algorithm, which could demonstrate its practical utility in real-world scenarios.
3. Scope of Loss Functions: The focus is primarily on squared, absolute, and hinge loss. Extending the results to other commonly used loss functions, such as logistic loss, would enhance the paper's generality.
Arguments for Acceptance:
- The paper makes significant theoretical contributions, filling important gaps in the literature on learning with limited observations.
- The proposed algorithm is a valuable addition, providing a practical approach that complements the theoretical findings.
- The work is well-motivated, addressing a challenging and impactful problem with clear implications for both theory and practice.
Arguments Against Acceptance:
- The exponential gap for hinge loss classification raises questions about the tightness of the results in this case.
- The lack of empirical evaluation limits the demonstration of the algorithm's practical effectiveness.
Recommendation:
I recommend acceptance of this paper, as its theoretical contributions are substantial and address an important problem in the field. However, I encourage the authors to address the exponential gap for hinge loss classification in future work and to include empirical validation of their algorithm to strengthen its practical impact.