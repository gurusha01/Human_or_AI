This paper addresses the critical issue of robustness in neural networks by proposing novel metrics for evaluating robustness and introducing an efficient algorithm to approximate these metrics. The authors formalize the notion of pointwise robustness and define two statistics: adversarial frequency and adversarial severity, which measure the occurrence and impact of adversarial examples, respectively. They encode the robustness computation as a linear program (LP) and develop an iterative optimization approach that significantly speeds up the process. The paper demonstrates the efficacy of these metrics and the proposed algorithm through experiments on MNIST and CIFAR-10 datasets, showing that the algorithm provides more accurate robustness estimates compared to existing methods. Additionally, the authors highlight the problem of overfitting to adversarial examples generated by specific algorithms and show that their approach mitigates this issue.
Strengths:
1. Novelty and Originality: The paper introduces new metrics for robustness evaluation and a novel algorithm for approximating these metrics. The focus on adversarial frequency and severity provides a more nuanced understanding of robustness compared to prior work.
2. Technical Soundness: The formulation of robustness as a constraint system and its approximation via LP is well-grounded. The iterative optimization approach is a notable contribution, offering significant computational efficiency.
3. Experimental Validation: The experiments are thorough, comparing the proposed algorithm against established baselines like L-BFGS-B. The results convincingly demonstrate the superiority of the proposed method in estimating robustness and mitigating overfitting.
4. Significance: The work addresses a pressing challenge in deploying neural networks in real-world, security-critical applications. The proposed metrics and algorithm have the potential to become standard tools for robustness evaluation.
Weaknesses:
1. Scalability: While the algorithm performs well on MNIST, its application to the larger CIFAR-10 dataset reveals limitations in improving robustness for complex models like NiN. This suggests that the approach may not scale effectively to deeper or more complex architectures.
2. Clarity: The paper is dense and highly technical, which may hinder accessibility for readers less familiar with constraint systems or LP formulations. Simplifying the presentation of key ideas could improve clarity.
3. Limited Exploration of Alternatives: While the paper critiques existing methods, it does not explore hybrid approaches or combinations of its algorithm with other robustness-improving techniques, which could provide additional insights.
4. Evaluation Scope: The experiments focus primarily on adversarial robustness. It would be valuable to assess whether the proposed methods impact other performance metrics, such as generalization or accuracy on clean data.
Arguments for Acceptance:
- The paper makes a significant contribution to the field by introducing novel metrics and an efficient algorithm for robustness evaluation.
- The experimental results are compelling and demonstrate the practical utility of the proposed methods.
- The work addresses an important and timely problem, advancing the state of the art in adversarial robustness.
Arguments Against Acceptance:
- The scalability limitations on CIFAR-10 suggest that the approach may not generalize well to more complex models.
- The dense and technical presentation could limit the paper's accessibility to a broader audience.
- The lack of exploration of alternative or complementary methods leaves some questions unanswered.
Recommendation:
I recommend acceptance of this paper, as its contributions to robustness evaluation are both novel and impactful. However, the authors should consider addressing the scalability challenges and improving the clarity of the manuscript in the final version.