This paper presents SPALS, a novel sparse alternating least squares algorithm for efficient computation of tensor CANDECOMP/PARAFAC (CP) decomposition. The authors focus on addressing the computational bottleneck posed by the Khatri-Rao product in alternating least squares (ALS) by leveraging randomized numerical linear algebra techniques. Specifically, they propose a sampling-based approach that estimates statistical leverage scores of the Khatri-Rao product to enable sublinear time per iteration. The paper demonstrates significant theoretical contributions, including efficient leverage score estimation and provable approximation guarantees for the proposed algorithm. Empirical results on both synthetic and real-world datasets, such as a large Amazon review tensor, show substantial speedups over existing deterministic and randomized ALS methods.
Strengths:
1. Technical Novelty: The paper introduces a novel approach to tensor decomposition by integrating statistical leverage score sampling into ALS. This is a significant contribution to the field, as it enables sublinear time complexity per iteration, a notable improvement over existing methods.
2. Theoretical Rigor: The authors provide detailed theoretical analysis, including guarantees for leverage score estimation and approximation bounds for SPALS. The results are well-supported by proofs and align with the state-of-the-art in randomized numerical linear algebra.
3. Empirical Validation: The experiments are thorough, comparing SPALS against both deterministic ALS and recent randomized methods. The results convincingly demonstrate the algorithm's efficiency and scalability, particularly on large-scale sparse tensors.
4. Relevance and Impact: Tensor decomposition is a critical tool in modern data analytics, and the proposed method addresses a pressing need for scalable algorithms in the era of big data. The work has potential applications in areas such as recommendation systems, signal processing, and deep learning.
Weaknesses:
1. Clarity: While the paper is technically sound, its presentation is dense and occasionally difficult to follow. For instance, the derivation of leverage score bounds and the sampling algorithm could benefit from clearer explanations and visual aids.
2. Limited Baseline Comparisons: The experimental section primarily compares SPALS to one recent randomized method and deterministic ALS. Including additional baselines, such as other tensor decomposition techniques (e.g., Tucker decomposition), would strengthen the empirical evaluation.
3. Generality: The paper focuses on third-order tensors, with limited discussion on extending SPALS to higher-order tensors. While the authors claim generalizability, explicit experimental validation on higher-order tensors would enhance the paper's impact.
Arguments for Acceptance:
- The paper addresses a significant challenge in tensor decomposition with a novel and theoretically grounded approach.
- The empirical results demonstrate clear advantages in terms of scalability and efficiency, particularly on large-scale datasets.
- The work is relevant to the NeurIPS community, given its focus on scalable algorithms for big data analytics.
Arguments Against Acceptance:
- The paper's clarity could be improved, particularly in the theoretical sections.
- The experimental evaluation could be more comprehensive, with additional baselines and higher-order tensor experiments.
Recommendation:
Overall, this paper makes a strong contribution to the field of tensor decomposition and randomized numerical linear algebra. Despite minor issues with clarity and experimental breadth, the novelty, theoretical rigor, and practical impact of SPALS warrant acceptance. I recommend acceptance with minor revisions to improve clarity and expand the experimental evaluation.