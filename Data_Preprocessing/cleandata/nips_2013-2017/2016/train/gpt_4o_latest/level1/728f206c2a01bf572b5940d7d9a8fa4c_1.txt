This paper introduces a novel training objective for Restricted Boltzmann Machines (RBMs) based on the Wasserstein distance, as opposed to the traditional Kullback-Leibler (KL) divergence. By leveraging a known metric between observations, the authors propose a minimum Wasserstein distance estimator for RBMs, deriving the gradient of the Wasserstein distance from its dual formulation. The paper demonstrates that this approach leads to generative models with distinct statistical properties, particularly in tasks such as data completion and denoising. The authors validate their method on several datasets, including MNIST and UCI PLANTS, and show that Wasserstein-trained RBMs (RBM-W) produce distributions that are more compact and metric-aware compared to standard RBMs.
Strengths:
1. Novelty and Originality: The use of the Wasserstein distance as a training objective for RBMs is a novel contribution. While Wasserstein distances have been explored in other contexts, their application to Boltzmann machines is innovative and well-motivated.
2. Theoretical Rigor: The paper provides a solid theoretical foundation, deriving the Wasserstein gradient and comparing it to the KL gradient. The inclusion of sensitivity analysis and the use of the Sinkhorn algorithm for computational efficiency are commendable.
3. Practical Relevance: The authors demonstrate the practical utility of their approach in data completion and denoising tasks, where the metric between observations is critical. The experiments show that RBM-W outperforms standard RBMs in these tasks, particularly in terms of bias-variance trade-offs.
4. Comprehensive Experiments: The paper includes extensive experiments across multiple datasets and scenarios, providing insights into the behavior of Wasserstein-trained RBMs. The analysis of hyperparameters and the shrinkage effect further strengthens the empirical contributions.
Weaknesses:
1. Clarity: While the theoretical sections are rigorous, the paper is dense and could benefit from clearer explanations, particularly for readers unfamiliar with Wasserstein distances or Boltzmann machines. For instance, the derivation of the Wasserstein gradient and its implications could be more intuitively explained.
2. Limited Scope of Applications: The experiments focus primarily on binary data and relatively small datasets. It remains unclear how well the proposed method scales to larger, more complex datasets or continuous data distributions.
3. Bias in Learned Distributions: The Wasserstein-trained RBMs tend to produce compact, cluster-like distributions, which may not always capture the diversity of the data. While this is acknowledged as a trade-off, it could limit the applicability of the method in scenarios requiring high diversity.
4. Computational Complexity: Although the authors employ the Sinkhorn algorithm to approximate Wasserstein distances, the computational cost of training RBM-Ws is still higher than standard RBMs. This could be a barrier to adoption in large-scale applications.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound approach to training RBMs, advancing the state of the art in generative modeling.
- The empirical results demonstrate the practical utility of the method in tasks where metrics between observations are critical.
- The work opens up new avenues for integrating metric-aware objectives into generative modeling.
Arguments Against Acceptance:
- The paper's clarity could be improved, particularly in explaining the theoretical contributions to a broader audience.
- The method's scalability and applicability to more diverse datasets remain underexplored.
- The tendency of RBM-Ws to produce biased, cluster-like distributions may limit their generalizability.
Recommendation:
Overall, this paper makes a significant contribution to the field of generative modeling and is well-suited for NIPS. While there are areas for improvement, particularly in clarity and scalability, the novelty and practical relevance of the approach outweigh the limitations. I recommend acceptance with minor revisions to address clarity and scalability concerns.