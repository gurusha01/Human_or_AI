This paper introduces a novel algorithm, the Scaled Least Squares (SLS) estimator, for efficiently estimating the coefficients of generalized linear models (GLMs) in large-scale settings where the number of observations \( n \) greatly exceeds the number of predictors \( p \). The authors build on the observation that GLM coefficients are approximately proportional to ordinary least squares (OLS) coefficients under random design settings, regardless of the predictor distribution. By leveraging this proportionality, the proposed algorithm achieves computational efficiency and accuracy comparable to the maximum likelihood estimator (MLE) with a significantly reduced per-iteration cost of \( O(n) \), outperforming traditional batch optimization methods by at least a factor of \( O(p) \). The paper provides theoretical guarantees for the algorithm, analyzes its convergence behavior, and demonstrates its effectiveness through extensive experiments on synthetic and real-world datasets.
Strengths
1. Novelty and Originality: The paper presents a novel approach by exploiting the proportionality between GLM and OLS coefficients, which has been underexplored in the context of computational efficiency for large-scale problems. This is a significant contribution to the field.
2. Theoretical Rigor: The authors provide detailed theoretical analysis, including convergence guarantees and error bounds, which are well-supported by mathematical derivations. The use of zero-bias transformations to generalize results to non-Gaussian predictors is particularly noteworthy.
3. Computational Efficiency: The proposed SLS algorithm is computationally efficient, with a per-iteration cost of \( O(n) \), making it highly scalable for large datasets. The cubic convergence rate of the root-finding step is a strong advantage over existing methods.
4. Empirical Validation: The extensive experiments on both synthetic and real datasets demonstrate the practical utility of the SLS algorithm. The results show that SLS achieves comparable accuracy to MLE while being significantly faster, especially in the \( n \gg p \) regime.
Weaknesses
1. Clarity: While the paper is mathematically rigorous, the presentation is dense and may be challenging for readers unfamiliar with GLMs or optimization methods. Simplifying some explanations and providing more intuitive insights would improve accessibility.
2. Limited Scope of Experiments: Although the experiments are thorough, they focus primarily on logistic and Poisson regression. It would be beneficial to evaluate the algorithm on a broader range of GLM applications, such as multinomial regression or graphical models.
3. Regularization: The discussion on regularization (e.g., ridge regression) is brief and lacks empirical validation. Since regularization is crucial in high-dimensional settings, a more detailed exploration would strengthen the paper.
4. Dependence on OLS Initialization: The algorithm relies on the OLS estimator as a starting point, which may not always be computationally trivial for very large datasets. While subsampling is suggested, its practical implications (e.g., trade-offs in accuracy) are not fully explored.
Arguments for Acceptance
- The paper addresses an important problem in large-scale machine learning and proposes a novel, theoretically sound, and computationally efficient solution.
- Theoretical contributions, such as the generalization of the proportionality relationship to non-Gaussian predictors, are significant and advance the state of the art.
- The empirical results convincingly demonstrate the algorithm's effectiveness and scalability.
Arguments Against Acceptance
- The paper's clarity and accessibility could be improved, particularly for a broader audience at NeurIPS.
- The experimental evaluation could be expanded to include more diverse GLM applications and regularization scenarios.
Recommendation
I recommend acceptance of this paper, as it makes a strong scientific contribution to the field of scalable optimization for GLMs. Addressing the clarity issues and expanding the experimental scope in a future revision would further enhance its impact.