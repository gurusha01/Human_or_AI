Review
This paper addresses two fundamental questions about the population likelihood function of Gaussian Mixture Models (GMMs) with \( M \geq 3 \) components, focusing on the behavior of the Expectation-Maximization (EM) algorithm. The authors resolve an open problem posed by Srebro [2007] by demonstrating that even in the favorable setting of equally weighted, well-separated, spherical Gaussians, the population likelihood function can have bad local maxima. They further show that the EM algorithm (and its first-order variant) with random initialization converges to suboptimal critical points with high probability, emphasizing the necessity of careful initialization. These results challenge prior assumptions about the benign behavior of GMM likelihood surfaces and provide new insights into the limitations of local search methods for non-convex optimization.
The paper builds on prior work in both statistical theory and algorithmic analysis. Early studies on GMMs, such as Teicher [1963] and Chen [1995], focused on identifiability and convergence rates, while more recent efforts (e.g., Balakrishnan et al. [2015]) analyzed the local convergence of EM under specific conditions. The authors extend this line of research by proving the existence of arbitrarily bad local maxima for \( M \geq 3 \), a significant departure from the conjecture that local maxima are global in the population setting. Additionally, their probabilistic analysis of random initialization complements prior work on initialization schemes (e.g., Hsu and Kakade [2013]) and highlights the exponential difficulty of achieving global convergence.
Strengths:
1. Theoretical Contribution: The paper resolves a long-standing open question and introduces novel techniques to analyze the structure of the population likelihood, particularly in proving the existence of bad local maxima.
2. Algorithmic Insights: The results provide a rigorous explanation for the failure of EM and its variants under random initialization, offering practical implications for designing better initialization schemes.
3. Clarity of Results: Theorems are clearly stated, and the implications for both theory and practice are well-articulated.
4. Broader Impact: The findings have relevance beyond GMMs, as they contribute to the understanding of non-convex optimization landscapes in general.
Weaknesses:
1. Empirical Validation: While the theoretical results are strong, the paper lacks empirical experiments to illustrate the practical impact of bad local maxima and the failure probabilities of EM in finite-sample settings.
2. Scope of Analysis: The focus is restricted to uniformly weighted, spherical Gaussians. It would be valuable to discuss whether the results generalize to more complex GMMs with non-uniform weights or anisotropic components.
3. Initialization Alternatives: Although the authors emphasize the need for careful initialization, they do not propose or evaluate alternative schemes, leaving a gap in actionable recommendations.
4. Complexity of Proofs: Some proofs, particularly those involving recursive structures in Theorem 2, are highly technical and may be inaccessible to a broader audience.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by resolving an open problem and advancing the understanding of GMM likelihood surfaces.
- The results are relevant to both the statistical and machine learning communities, particularly for researchers working on non-convex optimization and clustering algorithms.
- The insights into the limitations of EM and the importance of initialization are practically relevant.
Arguments Against Acceptance:
- The lack of empirical validation limits the paper's accessibility and practical impact.
- The focus on a specific GMM setting may restrict the generalizability of the results.
- The paper does not propose concrete solutions to mitigate the identified issues, such as improved initialization strategies.
Recommendation:
I recommend acceptance of this paper, as its theoretical contributions are substantial and address a fundamental problem in the analysis of GMMs and the EM algorithm. However, the authors are encouraged to include empirical results and discuss potential extensions to broader GMM settings in future work.