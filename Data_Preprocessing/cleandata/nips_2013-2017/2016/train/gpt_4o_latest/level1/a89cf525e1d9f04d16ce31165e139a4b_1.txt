This paper presents a novel extension of the Information Bottleneck (IB) framework, addressing its computational challenges for high-dimensional and non-Gaussian data through a variational approximation. The authors propose a Sparse IB algorithm, which incorporates a Student-t distribution to model sparse latent features, and demonstrate its ability to recover sparse, relevant features in both linear and kernelized settings. The kernelized version, termed Kernel IB (kIB), enables the model to handle non-linear relationships between input (X) and relevance variables (Y). The paper provides extensive theoretical derivations and empirical results, showcasing the algorithm's performance on tasks such as denoising, occlusion, and handwritten digit reconstruction.
Strengths
1. Technical Contribution: The paper makes a significant contribution by extending the IB framework with a variational approximation, enabling its application to sparse and non-linear data. The use of a Student-t distribution for sparse coding is novel and well-motivated.
2. Empirical Validation: The experiments are thorough, with diverse tasks demonstrating the algorithm's effectiveness. For instance, the Sparse IB model successfully recovers sparsely occurring features, outperforming Gaussian IB in terms of relevant information encoded.
3. Kernelization: The kernelized extension is a notable advancement, allowing the framework to handle non-linear relationships. The dual-space formulation and the connection to kernel ridge regression (KRR) are well-articulated.
4. Relation to Prior Work: The paper situates its contributions within the context of previous IB methods, sparse coding models, and related techniques like canonical correlation analysis (CCA) and KRR. This provides clarity on how the proposed method advances the state of the art.
Weaknesses
1. Clarity: While the theoretical derivations are rigorous, the paper is dense and may be difficult for non-experts to follow. Simplifying some explanations or including more intuitive diagrams could enhance accessibility.
2. Comparative Analysis: The paper compares Sparse IB primarily to Gaussian IB and related methods like CCA. However, it would benefit from a broader comparison to other modern feature extraction or representation learning methods, such as deep learning approaches.
3. Scalability: The computational complexity of the kernelized IB algorithm, particularly for large datasets, is not thoroughly discussed. While the authors mention using a subset of training data for efficiency, more details on scalability are needed.
4. Real-world Applications: The experiments focus on synthetic and benchmark datasets. Demonstrating the algorithm's utility on real-world, high-dimensional datasets (e.g., in vision or natural language processing) would strengthen its significance.
Arguments for Acceptance
- The paper provides a principled and novel extension to the IB framework, addressing key limitations of existing methods.
- The Sparse IB and kernelized IB algorithms are well-justified and supported by strong empirical results.
- The work has the potential to impact both theoretical research in information theory and practical applications in feature extraction and representation learning.
Arguments Against Acceptance
- The paper's clarity and accessibility could be improved, particularly for readers less familiar with the IB framework.
- Limited discussion of scalability and real-world applicability may hinder its practical adoption.
Recommendation
Overall, this paper represents a valuable contribution to the field of information processing and representation learning. While there are some concerns regarding clarity and scalability, the technical novelty and empirical results justify its acceptance. I recommend acceptance with minor revisions to address the identified weaknesses.