The paper addresses the critical problem of computing safe policies in sequential decision-making under uncertainty, particularly in the context of infinite-horizon discounted Markov Decision Processes (MDPs). The authors propose a novel robust optimization framework that minimizes regret with respect to a baseline policy, enabling the learned policy to improve upon the baseline in states with accurate dynamics while reverting to the baseline in uncertain states. This approach contrasts with existing methods that either adopt a new policy with provable improvements or default entirely to the baseline policy, which can be overly conservative. The paper demonstrates that the proposed problem formulation is NP-hard, introduces an approximate algorithm, and provides empirical evidence of its effectiveness in outperforming standard approaches in uncertain environments.
Strengths:
1. Novelty and Originality: The paper introduces a new perspective on safe policy improvement by minimizing robust regret relative to a baseline policy. This approach unifies uncertainties in learned and baseline policies and provides a more systematic performance comparison. The idea of interleaving the baseline and learned policies on a state-by-state basis is innovative and addresses limitations of existing methods.
   
2. Theoretical Contributions: The authors rigorously analyze the proposed optimization problem, proving its NP-hardness and deriving performance bounds for the approximate solutions. The theoretical insights, such as the necessity of randomized policies and the performance loss bounds, are valuable contributions to the field.
3. Practical Relevance: The proposed approach is highly relevant for real-world applications where deploying untested policies is risky. The empirical evaluation on domains like grid problems and energy arbitrage demonstrates the practical utility of the method, especially in scenarios with significant model uncertainty.
4. Clarity of Experimental Results: The experiments are well-designed, comparing the proposed method with standard approaches under varying levels of model uncertainty. The results clearly highlight the advantages of the proposed method in terms of safety and performance improvement.
Weaknesses:
1. Computational Complexity: While the paper acknowledges the NP-hardness of the proposed optimization problem, the practical implications of this complexity are not fully explored. The scalability of the approximate algorithm to large-scale problems remains unclear.
2. Limited Baseline Comparisons: Although the paper compares its approach to standard robust MDP methods and simulator-based policies, it does not benchmark against other recent advancements in safe policy learning, such as constrained reinforcement learning or Bayesian approaches.
3. Clarity of Presentation: The paper is dense with technical details, and some sections, such as the proofs and algorithm descriptions, could benefit from additional explanations or visual aids to improve accessibility for a broader audience.
4. Assumptions on Baseline Policy: The assumption that the baseline policy's transition probabilities are well-estimated (or error-free in some cases) may not hold in all real-world scenarios. The implications of this assumption on the robustness of the method could be further discussed.
Arguments for Acceptance:
- The paper addresses an important and underexplored problem in safe policy learning, offering a novel and theoretically grounded solution.
- The empirical results demonstrate significant improvements over standard methods, particularly in uncertain environments, making the approach practically impactful.
- The theoretical contributions, including the analysis of randomized policies and performance bounds, advance the understanding of robust decision-making under uncertainty.
Arguments Against Acceptance:
- The computational complexity of the proposed method may limit its applicability to large-scale problems, and this issue is not sufficiently addressed.
- The paper could benefit from broader comparisons with alternative approaches and a more accessible presentation of its technical content.
Recommendation:
Overall, the paper makes a strong contribution to the field of safe policy learning, particularly in its novel formulation of robust regret minimization and its practical implications. While there are some areas for improvement, the strengths outweigh the weaknesses. I recommend acceptance, with minor revisions to address clarity and scalability concerns.