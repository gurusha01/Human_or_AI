Review of "Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD)"
Summary:
This paper introduces a novel sampling algorithm, Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD), which applies Richardson-Romberg (RR) extrapolation to Stochastic Gradient Langevin Dynamics (SGLD) to reduce bias while maintaining reasonable variance. The authors provide a comprehensive theoretical analysis, demonstrating that SGRRLD achieves higher rates of convergence for bias and mean squared error (MSE) compared to SGLD. The proposed method is shown to achieve the theoretical accuracy of higher-order integrators while retaining the simplicity of first-order methods. The paper supports its theoretical claims with experiments on synthetic data and a large-scale matrix factorization task, showing that SGRRLD outperforms SGLD in terms of both accuracy and computational efficiency.
Strengths:
1. Technical Soundness: The paper provides a rigorous theoretical analysis, including asymptotic and non-asymptotic bounds for bias and MSE, as well as a central limit theorem for the proposed estimator. These results are well-supported and align with the experimental findings.
2. Novelty: The application of RR extrapolation to SG-MCMC methods, particularly SGLD, is novel and addresses a critical limitation of these methods—bias reduction—without requiring higher-order integrators.
3. Practical Relevance: The proposed algorithm is computationally efficient and well-suited for parallel and distributed architectures, making it attractive for large-scale Bayesian inference tasks.
4. Experimental Validation: The experiments are thorough and demonstrate clear improvements in bias, MSE, and convergence rates over SGLD. The large-scale matrix factorization task highlights the practical utility of SGRRLD in real-world applications.
5. Clarity: The paper is well-written and organized, with clear explanations of the algorithm, theoretical results, and experimental setup.
Weaknesses:
1. Limited Scope of Comparison: While the paper compares SGRRLD to SGLD and briefly to SGHMC, it does not provide a comprehensive comparison to other advanced SG-MCMC methods, such as those using adaptive step sizes or preconditioning techniques.
2. Computational Overhead: Although the authors emphasize the parallelizability of SGRRLD, running two chains in parallel may still introduce additional computational overhead compared to single-chain methods, which is not fully addressed in the discussion.
3. Assumptions: The theoretical results rely on several technical assumptions (e.g., smoothness of the potential energy function and ergodicity conditions), which may limit the applicability of SGRRLD in certain scenarios. This is not thoroughly discussed.
4. Supplementary Material Dependence: Many key details, including proofs and experimental configurations, are relegated to the supplementary material, which may hinder accessibility for readers.
Arguments for Acceptance:
- The paper addresses a significant limitation of SG-MCMC methods (bias) with a novel and theoretically sound approach.
- The results are well-supported by both theoretical analysis and empirical validation.
- The proposed method is practical and scalable, making it relevant for large-scale applications.
Arguments Against Acceptance:
- The comparison to other SG-MCMC methods is somewhat limited, leaving open questions about the relative performance of SGRRLD in a broader context.
- The computational trade-offs of running two chains in parallel are not fully explored.
Recommendation:
I recommend acceptance of this paper. While there are minor shortcomings, the contributions are significant, and the proposed method has the potential to advance the state of the art in scalable Bayesian inference. The combination of theoretical rigor and practical relevance makes this paper a strong candidate for inclusion in the conference. 
Score: 8/10