The paper introduces a novel method, Bounding Divergences with Reverse Annealing (BREAD), to evaluate the quality of Markov chain Monte Carlo (MCMC)-based posterior inference algorithms. Building on the bidirectional Monte Carlo (BDMC) framework, the authors propose a technique to upper bound the symmetrized KL divergence (Jeffreys divergence) between approximate and true posterior distributions. The method leverages annealed importance sampling (AIS) chains run in both forward and reverse directions and is validated on simulated data. BREAD is integrated into probabilistic programming languages WebPPL and Stan, and its utility is demonstrated through experiments on various models and datasets. The authors also propose a protocol for applying BREAD to real-world data by simulating datasets from fitted hyperparameters. The paper contributes to the broader goal of rigorously diagnosing inference quality in probabilistic programming and MCMC methods.
Strengths:
1. Technical Contribution: The extension of BDMC to bound Jeffreys divergence is a significant theoretical advancement. The method provides rigorous guarantees for simulated data, addressing a critical gap in evaluating MCMC-based inference algorithms.
2. Practical Relevance: By integrating BREAD into WebPPL and Stan, the authors make the method accessible to practitioners. The experiments demonstrate its utility in guiding model representation choices and debugging probabilistic programming implementations.
3. Validation: The paper provides thorough validation of BREAD on both simulated and real-world data. The consistency between simulated and real data results strengthens the method's credibility.
4. Novel Applications: The use of BREAD to compare collapsed and uncollapsed model representations is insightful, offering practical guidance for users of probabilistic programming languages.
5. Clarity of Experiments: The experiments are well-designed, and the results are clearly presented, highlighting the practical implications of the method.
Weaknesses:
1. Scope of Applicability: The method requires exact posterior samples, limiting its direct applicability to simulated data. While the authors propose a heuristic for real-world data, its reliance on approximate posterior samples introduces potential inaccuracies.
2. Computational Overhead: Running both forward and reverse AIS chains can be computationally expensive, particularly for large models or datasets. The paper does not discuss the scalability of BREAD in detail.
3. Comparison to Related Work: While the authors mention related work, such as [CTM16], the comparison lacks depth. It would be helpful to more explicitly contrast BREAD's advantages and limitations relative to other divergence estimation methods.
4. Clarity of Presentation: The paper is dense, with some sections (e.g., the derivation of bounds) being difficult to follow without significant prior knowledge. A more intuitive explanation of the key ideas could improve accessibility.
Arguments for Acceptance:
- The paper addresses an important problem in probabilistic inference and provides a rigorous, theoretically grounded solution.
- It demonstrates practical utility through integration into widely used probabilistic programming languages and real-world experiments.
- The method has the potential to advance the state of the art in evaluating and improving MCMC-based inference algorithms.
Arguments Against Acceptance:
- The reliance on exact posterior samples limits the method's applicability to real-world datasets.
- The computational cost of the method may hinder its adoption for large-scale problems.
- The paper could benefit from clearer exposition and a more detailed comparison to related work.
Recommendation:
Overall, the paper represents a significant contribution to the field of probabilistic inference and probabilistic programming. While there are limitations in scope and scalability, the strengths outweigh the weaknesses. I recommend acceptance, with minor revisions to improve clarity and address the computational concerns.