The paper presents a novel approach to exploration in non-tabular reinforcement learning (RL) by introducing the concept of pseudo-counts derived from density models. The authors bridge the gap between count-based exploration methods, traditionally limited to tabular settings, and intrinsic motivation approaches, which generalize better but lack strong theoretical foundations. By leveraging density models to compute pseudo-counts, the paper extends count-based exploration to high-dimensional state spaces, such as those encountered in Atari 2600 games. The proposed method demonstrates significant improvements in exploration, particularly in challenging environments like Montezuma's Revenge, where traditional methods fail. The authors also provide theoretical insights connecting pseudo-counts to information gain and prediction gain, offering a unified framework for exploration bonuses.
Strengths:
1. Novelty and Originality: The paper introduces pseudo-counts as a generalization of visit counts, a novel contribution that connects intrinsic motivation and count-based exploration. This is a meaningful advancement over prior work, such as MBIE-EB (Strehl and Littman, 2008) and intrinsic motivation methods (Schmidhuber, 1991; Oudeyer et al., 2007).
2. Theoretical Rigor: The authors provide a solid theoretical foundation, including proofs that relate pseudo-counts to information gain and prediction gain. This connection is a significant contribution to the literature on exploration in RL.
3. Empirical Results: The method achieves state-of-the-art performance in challenging Atari games, particularly Montezuma's Revenge. The results convincingly demonstrate the utility of pseudo-counts in guiding exploration in sparse-reward environments.
4. Clarity and Organization: The paper is well-structured, with clear explanations of the derivations and experimental setup. The inclusion of illustrative examples, such as the Freeway experiment, helps clarify the properties of pseudo-counts.
Weaknesses:
1. Density Model Limitations: The paper relies on a simplified CTS density model, which may not generalize well to more complex environments or continuous state spaces. While the authors acknowledge this limitation, the empirical evaluation could have included comparisons with more advanced density models.
2. Scalability Concerns: The computational cost of maintaining and updating density models in high-dimensional spaces is not thoroughly discussed. This could be a bottleneck in scaling the approach to more complex RL tasks.
3. Limited Benchmarking: While the Atari experiments are compelling, the evaluation is restricted to a small subset of games. A broader set of benchmarks, including continuous control tasks, would strengthen the paper's claims.
4. Practicality of Theoretical Guarantees: The theoretical results, while rigorous, are primarily asymptotic. It remains unclear how well these guarantees translate to practical settings with finite data.
Arguments for Acceptance:
- The paper addresses a critical challenge in RL—exploration in high-dimensional, sparse-reward environments—and provides a novel, theoretically grounded solution.
- The empirical results are compelling, showcasing significant improvements over baseline methods in challenging tasks.
- The theoretical contributions are substantial, offering a unified framework that connects intrinsic motivation and count-based exploration.
Arguments Against Acceptance:
- The reliance on a simplified density model raises questions about the generalizability of the approach to more complex tasks.
- The evaluation is somewhat narrow, focusing primarily on Atari games without exploring other RL domains or continuous state spaces.
Recommendation:
Overall, this paper makes a strong contribution to the field of reinforcement learning by introducing pseudo-counts as a generalization of visit counts for exploration. While there are some limitations in the scope of evaluation and scalability, the novelty, theoretical insights, and empirical results outweigh these concerns. I recommend acceptance, with suggestions to expand the evaluation in future work.