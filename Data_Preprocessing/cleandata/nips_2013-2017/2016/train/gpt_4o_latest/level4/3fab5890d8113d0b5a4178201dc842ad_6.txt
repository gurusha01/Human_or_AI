The paper introduces a Neural Turing Machine (NTM) with sparse read and write access to address a significant limitation of NTMs that renders them impractical for real-world applications. Specifically, traditional NTMs require exceedingly large physical memory and computational resources to perform backpropagation, a challenge that the proposed architecture aims to resolve while maintaining performance comparable to the state of the art. The paper is well-organized and easy to follow. However, including a brief overview of memory-augmented neural networks in the background section, or providing an explanation of the memory mechanism, would make the paper more self-contained and accessible to readers who are not experts in the field. For instance, while it may be clear to researchers familiar with this domain, some readers might not fully understand the intended role of the write operator compared to conventional memory systems. Additionally, the comparison in Figure 4 could be reconsidered. While the current approach of matching physical memory usage to compare performance under constrained resources is valid and defensible, comparing performance using the same memory (rather than the same physical memory) might offer a more informative perspective. That said, it is fascinating to observe how closely this algorithm performs relative to theoretical bounds while addressing the aforementioned challenges.