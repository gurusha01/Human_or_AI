The authors present a theoretical framework for assessing the capacity of a specific family of unitary matrices. They demonstrate that prior work, which utilized a restricted family of unitary matrices to parameterize a unitary recurrent neural network, exhibits lower capacity compared to the full unitary group. By leveraging an established method for gradient descent on the Stiefel manifold, the authors optimize directly over the full unitary group. Through both synthetic and real-world data experiments, they validate the superiority of their approach. Interestingly, they also observe that it offers improved computational efficiency. Overall, this is a strong and well-written paper. A key contribution is the introduction of a novel method for evaluating the capacity of unitary matrix families. Using this method, the authors show that the restricted family of unitary matrices previously used constrained the representational capacity of the neural network. However, could the authors clarify why a straightforward counting argument does not suffice in this context? As noted, the prior family of unitary matrices was parameterized by 7n real parameters, while U(N) is parameterized by nÂ² real parameters. This seems to imply an even stricter lower bound on when the capacity of the restricted family falls short of covering the entire unitary group. To further enhance the paper, the authors might consider conducting additional experiments, such as replicating some of the benchmarks from the original unitary evolution recurrent neural network paper, like the permuted MNIST experiments.