The paper introduces a manifold learning technique aimed at finding a low-dimensional embedding of data such that it resides on a submanifold within the embedding space. This is accomplished through gradient-based optimization of a straightforward energy function that quantifies the deviation of the intrinsic Riemannian metric from the identity. While the approach is novel, its practical utility remains unclear. The primary contribution of the paper lies in the idea that the intrinsic manifold dimension can (or perhaps should) be lower than the embedding dimension. Although this is an interesting concept, the authors do not adequately justify its usefulness. The only apparent motivation provided is that this method can result in reduced distortion. A few comments on this: 1) it is evident that lower distortions can be achieved when embedding a d-dimensional manifold into R^s (where s > d) rather than into R^d, but this observation is rather trivial; 2) it is unclear why existing methods like Isomap (or its variants) cannot be used to embed into R^s, as this is already common practice; 3) the practical significance of this distinction is not well-articulated. While the mathematical perspective is clear, it remains uncertain which real-world problems this approach addresses that existing methods cannot.