In this paper, the authors propose an alternative methodology for Approximate Bayesian Computation, specifically tailored for models with intractable likelihoods but for which mock datasets can be efficiently simulated under arbitrary parameter settings (within the prior support, etc.). Their approach leverages a versatile parametric modeling tool—the Mixture Density Network—to approximate the Bayesian conditional density over the parameter space given the (mock) data. This represents a promising synthesis of concepts from machine learning and statistical theory. I find this to be a potentially exceptional paper, as the proposed approach is both well-motivated and clearly articulated. Based on the numerous worked examples provided, it appears likely that this method will significantly influence the application of likelihood-free techniques, particularly in scenarios where generating mock data is computationally expensive, making the efficiency of the sampler or posterior approximation method critical (e.g., weather simulations, cosmological simulations, individual-based epidemiological models). 
It is worth mentioning the parallel developments in the statistics community, such as the use of random forest methods for epsilon-free ABC inference targeting conditional density models (Marin et al., 1605.05537), which underscores the growing interest in innovations along these lines. However, I have a concern regarding the proof of Proposition 1, specifically the use of the term "sufficiently flexible," which is not explicitly defined. This should be clarified, as it would allow for the identification of sufficient conditions on the posterior that justify the use of the MVN model. Naturally, these conditions are likely to be restrictive, so it becomes important to explore and delineate cases where the approximation is adequate or inadequate, as well as to provide empirical metrics to guide users in making this determination.
Minor comments:  
- The comparison to existing work in Section 4 is well-executed (e.g., the identification of regression-adjustment as a related development). However, it might be worth noting that the "earliest ABC work" by Diggle & Gratton (1984) introduced a kernel-based estimator of the likelihood.  
- In the introduction, the authors state that "it is not obvious how to perform some other computations using samples, such as combining posteriors from two separate analyses." A number of recent studies in scalable Bayesian methods have addressed this issue (e.g., Zheng, Kim, Ho & Xing 2014; Scott et al. 2013; Minsker et al. 2014).