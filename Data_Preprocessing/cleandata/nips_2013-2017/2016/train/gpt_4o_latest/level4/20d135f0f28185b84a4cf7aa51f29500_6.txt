The paper introduces an ensemble training algorithm for deep learners, which is distinctive in that only one learner from the ensemble is active for each training example, unlike prior deep and classical ensemble methods. During training, the ensemble member with the lowest loss relative to the true label is selected and updated using the gradient. Overall, the approach is straightforward yet yields surprisingly strong experimental results, addressing some shortcomings of existing methods. Therefore, I recommend it for weak acceptance. 
1. The testing phase is not described in sufficient detail. Based on my interpretation, it appears that the ground truth is used to identify the best-performing member, as suggested in line 159 on page 5. Clarification on this point is necessary.  
2. The paper refers to "independent ensembles" and "regular ensembles" (denoted as "Indp." in the figures and mentioned in line 181 on page 5). However, I could not find a clear explanation of this baseline, which is essential for understanding the comparisons.  
3. The authors frequently emphasize the `specialization` of ensemble members toward different output classes. However, since the reassignment during training is performed on a per-example basis, this claim seems insufficiently justified. Could the authors consider incorporating specialization explicitly into the loss function to strengthen this argument?  
4. Related to the above point, the paper does not discuss the closely related work on diversity regularization in neural networks (e.g., Chen et al.). This technique offers a more formal approach to encouraging diversity but has not yet been extensively explored in the deep learning context. Including a discussion of this work would provide valuable context and highlight connections to existing literature.