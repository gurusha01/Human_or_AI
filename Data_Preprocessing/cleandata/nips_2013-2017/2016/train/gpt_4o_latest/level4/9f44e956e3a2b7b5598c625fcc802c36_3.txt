The authors propose the introduction of an intermediate timescale to neural networks and demonstrate its potential to enhance computational performance. This addition effectively incorporates an input that reflects the correlation between the current network state and its past states. The work is both timely and well-executed. The ideas are clearly presented and well-motivated, with appropriate simulations supporting the claims. Regarding novelty, while intermediate timescales have been explored in the context of reservoir computing, their application to more conventional machine learning tasks, as done here, is new. Additionally, the specific implementation of the addition in this work is novel. The paper positions its primary contribution within the realm of computational neuroscience. However, a few questions arise from this perspective. For instance, how plausible is the proposed normalization procedure? Additionally, why are the matrices W and A in equation 2 applied to different activity vectors, and what would be the biophysical equivalent of the inner loop?  
Specific comments:  
- Mini-batch size: While this appears to limit memory, the issue is not addressed in the paper.  
- Table 3: The metric should be percent correct rather than error.  
- Line 277: A reference is missing.  
- The advantage of fast weights seems most pronounced in small networks. This could point to a broader underlying issue, but the current results make it difficult to draw definitive conclusions.