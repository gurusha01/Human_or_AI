This paper presents a method aimed at reducing the net-zero win and loss changes (churn) between two models trained sequentially with increasing feature sets and training data sizes. The authors propose a stabilization method, implemented within a Markov chain framework, to mitigate churn. Both theoretical and empirical analyses are provided to support the approach. Specifically, the paper introduces a stabilization operator to regularize model B, ensuring its consistency with the earlier model A. Theoretical and experimental results are offered to validate the method's soundness. 
However, I question the necessity of this successive training approach. The stabilization operator (e.g., RCP) essentially transfers information from model A, which is trained on a different dataset TA (e.g., with fewer features), to model B. Consequently, model B effectively "sees" both datasets TA and TB. This raises the question of whether directly training model B on the combined dataset TA+TB, or more practically, training on TB with random feature perturbations (e.g., via dropout), might yield better accuracy and stability. Given the recent success and widespread adoption of dropout-style methods, the authors should include a comparison of their approach with such techniques.
Additionally, the choice of perturbation PT (Line 94) and the claim that classifier A is "pre-trained to be robust to the kind of changes" (Line 104) are not entirely intuitive. Why is A necessarily robust to the changes encoded in PT? While I understand that PT may help Fk^ generalize better, it is unclear how this robustness translates to A. Furthermore, how does this robustness affect B*? The authors should clarify these points. 
Please also explain how the "True WLR Needed" in Table 1 is calculated. Minor issues include the phrasing in Line 178, where "re-labels" should be revised to "re-label." Additionally, Table 3 is difficult to interpret. It would be helpful to separate different metrics (e.g., WLR) into distinct tables to facilitate easier comparisons.