This paper builds upon the stochastic variance reduction techniques SVRG and SAGA, extending them to address strongly convex-concave saddle-point problems with proven linear convergence. Both the theoretical analysis and empirical evaluations substantiate the effectiveness of the proposed variance reduction methods for saddle-point problems. The paper broadens the applicability of SVRG and SAGA to a variety of non-separable saddle-point problems, where both the loss function (e.g., structured output prediction) and the regularizer (e.g., fusion term) may be non-separable. A notable contribution is the authors' approach to tackling the more intricate convergence analysis required for saddle-point problems, which differs significantly from prior analyses on convex problems. By leveraging monotone operator theory, the authors establish convergence results. Furthermore, their analysis demonstrates that the extended SVRG and SAGA algorithms are applicable to a broader class of problems, including variational inequalities in game theory. 
The authors propose two distinct gradient operator splitting strategies—element-wise splitting for SVRG and factored splitting for SAGA—within a stochastic framework. These variance reduction techniques are integrated into stochastic forward-backward algorithms, which are widely used for solving saddle-point problems. The paper also explores the use of non-uniform sampling, showing through both theoretical insights and empirical results that variance reduction with non-uniform sampling outperforms uniform sampling. Experimental evidence, presented in both the main text and the appendix, strongly supports the theoretical findings and highlights the effectiveness of the proposed methods for problems involving non-separable losses and regularizers.
Some minor comments: In Section 5, the authors incorporate an additional regularization term, following [8], to accelerate convergence by pushing the update toward an iterate point. However, it is unclear how to adaptively update this iterate point, denoted as (\bar{x}, \bar{y}), to achieve acceleration. While a convergence and acceleration proof is provided for SVRG, it is not specified whether similar theoretical guarantees hold for SAGA. Additionally, the notation for the smooth function K(.) and the design matrix K should be revised to use distinct symbols to avoid potential confusion.