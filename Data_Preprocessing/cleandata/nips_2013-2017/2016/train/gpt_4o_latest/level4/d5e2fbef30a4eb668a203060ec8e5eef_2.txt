This paper introduces an information-theoretic feature selection algorithm that approximates high-order mutual information by transforming categorical variables into binary variables. The algorithm further breaks down the high-order mutual information into a search over two sets: one identifies the optimal complementary subset (maximizing information gain), while the other identifies the subset containing the majority of that information. 
Major Comments:  
Statement 1 and its corresponding proof appear unnecessary. When `t` and `s` exceed the number of selected features, the maximum score assignable to a variable equals the mutual information between that variable (combined with all selected features) and the target. Depending on the search method, all variables either end up in `H` or in `G`. This score becomes rank-equivalent to the Conditional Mutual Information (CMI) score. The criterion's behavior is only meaningful when it approximates the CMI (i.e., when `t` and `s` are less than `i`). The interaction between the max and min steps is particularly intriguing. It seems likely that `G` often encompasses `H` to minimize information, but in cases of complementary features, the outcome heavily depends on the search strategy. The binary representation significantly expands the search space, enabling the algorithm to uncover interesting interactions.  
The complexity analysis of competing techniques is technically accurate in stating that their computational cost increases with `i`, but this assumes a naive implementation. In a memoized implementation (e.g., the FEAST toolbox used in the experimental study), there are `O(d)` mutual information calculations per iteration, resulting in constant computation over time, with `O(kd)` memory usage and `O(kd)` total mutual information calculations. For CMICOT, memoization does not seem feasible (due to the dynamic nature of binary feature selection), so its computational complexity grows over time, as noted in the paper.  
The experimental study omits some critical details, such as the value of `k` in k-NN, the specific multiclass Adaboost variant used, the base learner for Adaboost, and the number of ensemble members. Additionally, portions of the supplementary material suffer from unclear notation. For instance, in the equation above line 21, each `G` represents a different subset, so distinct symbols (e.g., `G'`) should be used. Much of the discussion hinges on the fact that `H` and `G` can have a substantial overlap (or even form `H âˆª G`), effectively removing `H` from the mutual information calculation.  
Minor Comments:  
The proof of Proposition 3 does not clearly define a straightforward operation. Calculating joint entropy involves iterating over all `N` data points and all states of the joint variable, resulting in complexity `O(N + |X|)` rather than `O(Nm)`, where `|X|` denotes the number of states. Separating the results for poker, ranking, and semeion datasets would help readers distinguish the effects of the binary approximation from those of the max/min criterion. It remains unclear whether the binary approximation enhances performance by introducing noise or reduces it by limiting the information compared to JMI or RelaxMRMR. Presenting separate results for binary and non-binary features could clarify this. Furthermore, it would be valuable to evaluate all competing algorithms on binary-expanded datasets to assess how this transformation impacts their performance.