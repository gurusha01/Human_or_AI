The authors introduce a novel approach for learning the parameters of the Word Mover's Distance (WMD), which is essentially the Earth Mover's Distance (EMD) applied to text documents, with word embeddings used to compute bin-to-bin dissimilarities. Their method parameterizes the EMD cost matrix (i.e., the ground metric) using a Mahalanobis distance defined over vector representations. Additionally, the authors propose learning histogram coefficient reweighting alongside this parameterization, drawing inspiration from Lebanon's earlier work (see comments below for further discussion). Gradients for NCA-type costs are derived for this problem, and the paper presents compelling experimental results to support the proposed method. Overall, the paper combines existing techniques in a thoughtful manner and delivers convincing empirical evidence. I am inclined to recommend acceptance. The paper's strengths lie in its simplicity and the relatively straightforward nature of the idea, which, while not trivial to implement or test, is executed effectively. The experimental section is a particularly strong aspect of the work.
Areas for improvement include better handling of the interplay between regularized and non-regularized formulations, ensuring greater mathematical rigor (as some computations and notations are somewhat sloppy), and ideally providing an algorithmic box to clarify the proposed method. 
Minor comments:  
- In Eq. 1, the Euclidean distance between word embeddings is used as the cost, whereas in Eq. 6, for the Mahalanobis metric learning, the cost becomes the squared Euclidean metric (commonly referred to as the 2-Wasserstein distance). To avoid such inconsistencies, it might be clearer to describe WMD as the p-Wasserstein distance between clouds of word embeddings, regardless of the specific value of p.  
- The "reweighting" approach presented in Eq. 5 is not novel; it was previously introduced by Guy Lebanon in the context of text document classification ("Metric Learning for Text Documents"). See also the work "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations" by Le and Cuturi.  
- Are there any experimental insights into the significance of the weight vector \( w \) in the learned metric? Does this innovation have a meaningful impact, and is it relevant in practice?  
- Related work: The differences from prior work could be more clearly articulated. For example, this paper builds on the work of Cuturi and Avis but incorporates entropy regularization, which significantly improves scalability and computational efficiency. Additionally, the cost function differs: while discrete in their work, it is now continuous and parameterized by linear maps. These distinctions are not sufficiently emphasized. The paper also draws from Wang et al., though that work is not publishable in its current form. Unlike this paper, their approach runs the OT optimization only once, which is insufficient and ineffective as proposed.