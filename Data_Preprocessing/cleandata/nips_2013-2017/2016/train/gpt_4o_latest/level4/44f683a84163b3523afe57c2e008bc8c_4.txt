This paper introduces a novel approach to 3D shape generation by integrating a Generative Adversarial Network (GAN) with a Volumetric Convolutional Network, resulting in a combined architecture named VAN. The network is designed to learn a latent representation of objects, which is subsequently utilized for shape generation. The experimental results demonstrate that the proposed pipeline performs reasonably well, producing highly convincing shapes. Additionally, the experiments reveal that the latent representation \( z \) is semantically meaningful. Overall, I find the paper to present a straightforward and effective method for generating 3D objects. The concept of combining a GAN with a Volumetric Network for 3D shape generation is undoubtedly innovative. Moreover, the integration of an image encoder into VAN to extract the latent representation vector \( z \) from an image is particularly intriguing. The experiments are thorough, well-structured, and convincingly validate the proposed method. However, the approach appears to be a combination of existing techniques applied to a novel problem, which somewhat limits its originality. Below are my specific concerns:
1. The paper claims that the shapes generated by VAN are similar but not identical to the nearest neighbors retrieved from the training set. However, judging the similarity solely based on Figure 2 is challenging and not entirely convincing, as the shapes appear quite similar to me. Could you provide a quantitative evaluation to support this claim?
2. In Section 4.3, it is unclear how the output features from the second-to-last layer are utilized. Are these features used to retrain a classifier for classification tasks? Please clarify this point.
3. What is the underlying intuition behind the loss function of VAE-VAN presented in Equation (2)? Additionally, how is the VAE-VAN trained? Is the training process identical to that of VAN, or are there differences?
4. Section 4.3 presents an experiment on reconstructing 3D shapes from single images using VAE-VAN. Figure 4 showcases a diverse set of furniture examples captured from various viewing angles. Does this imply that the method is invariant to viewing angles? Specifically, do images of the same object taken from different angles yield the same latent representation \( z \) and the same final shape?
5. The paper uses the output of the second-to-last layer of the discriminator network as a feature for classification, achieving promising results. However, given the demonstrated semantic richness of the latent representation \( z \) in Section 5, have you attempted to use \( z \) as a feature for classification? If so, what were the results?
6. The generated output mesh grid appears coarse, likely due to the low resolution employed. This suggests that the network is learning general mid-level structures of shapes, such as chair legs or armrests, as evidenced by the experiments. Have you experimented with higher resolutions? If so, does the method perform well at higher resolutions, where finer details are present? Can it still learn effectively under such conditions?
In conclusion, I find this paper to be an interesting and valuable contribution. However, I encourage the authors to address the concerns raised above to further strengthen the work.