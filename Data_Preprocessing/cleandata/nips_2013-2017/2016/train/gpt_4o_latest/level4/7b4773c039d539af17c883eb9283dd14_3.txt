The paper introduces a more advanced algorithm, accompanied by analysis, for Monte Carlo planning. The proposed algorithm innovatively integrates Monte Carlo sampling (without action selection) with planning by disregarding alternative action branches once sufficient evidence identifies the optimal action at a node. This approach enables the algorithm to focus on exploring a subset of states reachable through near-optimal policies. The analysis demonstrates that the sample complexity is influenced by a measure of the proportion of near-optimal states. Additionally, the paper establishes that the algorithm and its analysis outperform prior worst-case bounds under various conditions. The algorithm itself seems straightforward to implement. With a single, relatively simple framework, the authors achieve several significant results: improved worst-case bounds when the number of states \( N \) is finite, bounds that depend on the size of the space explored by near-optimal policies, conditions enabling polynomial sample complexity when \( N \) is infinite, and behavior akin to Monte Carlo sampling when there is a clear gap between the value of the best action and other actions. This represents a commendable theoretical advancement. However, despite the algorithm's simplicity, its contributions appear primarily theoreticalâ€”exploring a very large tree is necessary to produce meaningful results at the root, unlike practical algorithms that are typically designed as anytime algorithms. Could an anytime version of the algorithm be developed? Without such a version, the impact may remain confined to theoretical insights.  
After author feedback: Thank you for the clarification. Since the algorithm can indeed be adapted into an anytime framework, it would be valuable to see experimental evaluations of this adaptation in future work.