The manuscript addresses likelihood-free inference, specifically parametric inference for models where evaluating the likelihood function is computationally prohibitive. The proposed method approximates the posterior distribution of parameters by modeling the conditional distribution of data given parameters using a Gaussian mixture model (implemented as a mixture density network). The authors highlight two primary advantages over standard approximate Bayesian computation (ABC): (1) their method provides a "parametric approximation to the exact posterior" rather than samples from an approximate posterior (line 49), and (2) it is computationally more efficient (line 55). The paper includes a brief theoretical analysis, demonstrating that the proposed approach asymptotically recovers the correct posterior under the assumption that the mixture model can represent any density. Empirical validation is conducted using two toy models with known posteriors and two more complex models with intractable likelihoods.
Technical Quality
---
My evaluation is based on the following points:
1. The claim that the method approximates or targets the exact posterior (e.g., lines 50, 291) is misleading. The paper focuses on approximating the posterior conditioned on summary statistics, which are often not sufficient. Consequently, critical information may be lost, making it impossible to recover the "exact posterior."
2. The assertion that the approximation can be made arbitrarily accurate (line 51) is problematic. While increasing the number of components and hidden layers in the network may improve accuracy, the paper does not discuss model selection. Specifically, guidance on choosing the number of layers and components, while accounting for the increased computational cost of simulating additional data, is absent.
3. Although theoretical justifications are provided, Proposition 1 offers limited insight. Rejection ABC also asymptotically recovers the posterior under weaker conditions, as the threshold (bandwidth) approaches zero with increasing simulations (e.g., Section 3.1 of Blum 2010). Thus, rejection ABC can also achieve arbitrarily accurate approximations.
4. Regression adjustment, a classical ABC method, addresses the computational challenges associated with decreasing the bandwidth epsilon (see Section 4.1 of Beaumont 2010). While the paper briefly mentions such methods, it does not include experimental comparisons. Such comparisons are crucial because:
   - Regression adjustment is standard in ABC.
   - It also aims to approximate posteriors as epsilon approaches zero, similar to the proposed method.
   - There are clear theoretical connections between regression adjustment and the proposed approach.
Additional Comments:
1. In Fig. 1 (left), Algorithm 2 (MDN with proposal) appears to produce a less accurate posterior approximation than Algorithm 1 (MDN with prior). This is concerning, as Algorithm 2 is more computationally expensive and builds on Algorithm 1. Could the authors explain this discrepancy?
2. In Sections 3.3 and 3.4, the log probability (density?) assigned by the learned posterior to the true parameter value is used to evaluate accuracy. This metric may be misleading, especially for weakly informative data, where the true posterior is not centered on the data-generating parameter (e.g., the posterior for the mean of a Gaussian). Additionally, this metric does not assess the accuracy of the posterior's spread, which is critical (as noted by the authors in line 22).
3. The use of uniform priors on bounded intervals (e.g., [0,1]) is common. If the estimated posterior \( \hat{p} \) in Eq. (3) is a truncated mixture of Gaussians, this raises concerns:
   - Exact normalization is infeasible and computationally challenging in higher dimensions.
   - Marginals of truncated (mixtures of) Gaussians are generally not truncated (mixtures of) Gaussians (see Horrace 2005).
   Could this pose challenges for the proposed approach?
4. Eq. (7) in the supplementary material implies that \( q\phi(\theta|x0)/\tilde{p}(\theta) \) is proportional to the likelihood function. Since \( \tilde{p} \) is fixed in each iteration, does learning \( q_\phi \) effectively correspond to learning the likelihood function?
Novelty/Originality
---
The paper's novelty claims are overstated. While related work is mentioned in Section 4, the discussion is superficial, and the proposed method is not adequately contextualized within the existing literature. The method is described as a "new approach to likelihood-free inference based on Bayesian conditional density estimation," but it appears to conceptually align with existing approaches. 
1. Conditional density estimation has been applied to ABC for over a decade (e.g., Section 4.1 of Beaumont 2010). This connection underpins regression adjustment methods, which also target the approximate posterior as epsilon approaches zero.
2. For linear models, a pilot ABC run is required to constrain the parameter space, discarding some simulations. For nonlinear models, all simulations can be utilized (see Fig. 1 of Blum 2010). This corresponds to "epsilon-free inference." Neural networks have also been used to model the relationship between data (summary statistics) and parameters (Blum 2010).
3. The 2012 paper by Fan, Nott, and Sisson (reference 8) describes an epsilon-free inference method based on conditional density estimation, using mixture models. While Fan et al. focus on approximating the likelihood, Bonassi et al. (2011) emphasize posterior approximation. Both works are closely related to the proposed approach.
4. Likelihood-free inference methods can be categorized into parametric and nonparametric approaches (e.g., Section 3 of reference 11). Classical ABC methods correspond to nonparametric approximations, where epsilon functions as a bandwidth. Parametric methods, by design, do not require a bandwidth (e.g., reference 12). The proposed method is a parametric approach, so its "epsilon-free" nature is unsurprising.
While the authors may employ a specific neural network architecture not previously used, the paper does not convincingly justify its presentation as a "new approach to likelihood-free inference." Instead, the method appears to represent a technical variation of existing approaches, and its advantages over recent efficient likelihood-free inference methods are neither clearly articulated nor experimentally demonstrated.
References
---
- Blum 2010: Approximate Bayesian Computation: A Nonparametric Perspective, Journal of the American Statistical Association, 2010, 105, 1178-1187.
- Beaumont 2010: Approximate Bayesian Computation in Evolution and Ecology, Annual Review of Ecology, Evolution, and Systematics, 2010, 41, 379-406.
- Horrace 2005: Some Results on the Multivariate Truncated Normal Distribution, Journal of Multivariate Analysis, 2005, 94, 209-221.
- Bonassi 2011: Bayesian Learning from Marginal Data in Bionetwork Models, Statistical Applications in Genetics and Molecular Biology, 2011, 10.
Update
---
Following the authors' responses, I have revised my score from 2 to 3. I recommend addressing the following points in the revision:
- Clarify that the "exact posterior" refers to the posterior conditioned on summary statistics, not the true posterior.
- Differentiate the proposed method from existing likelihood-free inference approaches based on parametric conditional density estimation, and adjust novelty claims accordingly.
- Acknowledge that prior work on likelihood-free inference has also significantly reduced computational costs (e.g., references 11, 15).
- Clarify why the method approximates the posterior rather than the likelihood function. Eq. (3) suggests that \( q\phi(\theta|x0)/\tilde{p}(\theta) \) is proportional to the likelihood function, implying that learning \( \phi \) corresponds to learning the likelihood.