This paper addresses the challenge of 3D object generation. The proposed approach involves constructing a generative deep network, which is trained using an adversarial signal derived from a corresponding discriminative deep network. While GANs have been demonstrated to perform effectively in generating 2D images, this work extends the concept to demonstrate that adversarial training can also facilitate 3D object synthesis. Additionally, the paper introduces a novel extension by adapting VAE-GANs to 3D, naming the resulting model VAE-VAN. The VAE-VAN framework incorporates an encoder network that maps a 2D image into a latent representation, and a generative network that reconstructs a 3D object model of the depicted object from the encoded latent space. Experimental results indicate that the proposed models qualitatively outperform the state of the art in synthesizing 3D objects. Quantitative experiments further validate that the methods achieve superior performance compared to existing approaches.
This reviewer observes that the methods presented in the paper effectively extend established techniques from 2D image generation to 3D object generation. While adversarial training and VAE-GANs are well-known in the community, this work applies these techniques to the domain of 3D synthesis, demonstrating their efficacy. The primary contribution of this paper lies in extending these existing methods to 3D, which is a non-trivial task, though the novelty of the work is somewhat limited. Nonetheless, the reviewer believes that this research represents a step in the right direction and is likely to be of interest to the NIPS community.
The reviewer, however, has several questions that the authors should address for greater clarity. There is some confusion regarding the experimental details related to 3D object classification and single-image 3D reconstruction. For instance, Line 172 mentions that different VANs were trained for different object categories. If this is the case, which discriminator is used to extract the representations (Line 193) for classification? Are separate VANs employed for the experiments in Section 4.2, or is a single VAN used? Additionally, what type of classifiers (e.g., nearest neighbor, multi-class, etc.) are utilized for decision-making? Similar questions arise regarding the experiments in Section 4.3. Specifically, how many VAE-VAN models are trained on the IKEA dataset? If multiple models are trained (one per object category), how are these models evaluated on novel test images?
Furthermore, the reviewer would like to see more analysis to better understand the behavior of individual neurons in the discriminator. For example, Figure 8 suggests that multiple neurons are activated by the same object part, such as the top-right and bottom-left neurons both responding to the left sides of chairs. Additional experimental details and analysis are needed to clarify these points and to ensure the reproducibility of the results.