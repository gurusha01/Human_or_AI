The authors investigate a "semi-bandit" learning framework, where a learner selects a subset of L actions from a base set of K actions over a sequence of rounds. In each round, nature assigns rewards to all base actions, and the learner observes the rewards for the L base actions it selected (semi-bandit feedback). The learner's reward for the composite action is a weighted linear combination of the L base rewards, with the weighting potentially unknown and dependent on the ordering of the base actions. The primary contribution of this work is extending this problem to a contextual setting, where the objective is to compete with the best policy within a given class. The authors assume access to a supervised learning oracle capable of identifying the optimal policy for a finite dataset of contexts, base rewards, and weightings. They propose an algorithm that achieves a regret of âˆšT while making a polynomial number of calls to the oracle.
The closest related work is by Kale et al., which addresses a similar problem and achieves comparable regret rates. However, their approach modifies the EXP4 algorithm and requires maintaining a distribution over all policies in the policy class. In contrast, the authors adopt a technique akin to that of Agarwal et al. (https://arxiv.org/pdf/1402.0555v2.pdf), solving the associated optimization problem using coordinate descent and oracle calls while maintaining a distribution over a sparse set of policies. The authors also argue that the Kale et al. approach imposes the restriction that the weight vector must be w = 1. However, this claim appears questionable in the known-weights setting; unless I am mistaken, it seems feasible to construct -wl yt(a_{t,l}) as the base-action loss for the Kale et al. algorithm.
In summary, while the core ideas of this paper are adaptations of those in Agarwal et al., the analysis is intricate and non-trivial. The problem addressed is significant, particularly due to its relevance in applications such as online content recommendation.