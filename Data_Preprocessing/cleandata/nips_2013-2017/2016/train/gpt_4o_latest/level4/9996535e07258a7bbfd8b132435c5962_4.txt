Building upon the traditional encoder-decoder framework for end-to-end learning, this paper introduces a reviewer module positioned in the middle to enhance visual caption generation. The experimental findings demonstrate that the reviewer module improves the performance of both image captioning and source code captioning compared to the conventional attentive encoder-decoder framework. By incorporating multi-step attention over hidden units, the reviewer module effectively captures global information across multiple attention mechanisms and employs a discriminative loss to guide the overall learning process. The concept of utilizing a multi-step reviewer module is compelling. Overall, the paper is well-written. The attentive mechanism is presented in two variants: input reviewer and output reviewer. The authors should provide a detailed explanation of their differences and appropriate use cases under various settings in the method section. Additionally, the multi-step attention mechanism closely resembles the multi-hop attention used in memory networks, where it is well-established that performance is highly sensitive to the number of steps. The authors should clarify how varying the number of reviewing steps impacts the results.