This paper investigates a specific category of feedforward neural networks that can achieve global optimality with a linear convergence rate by employing a nonlinear spectral method. The proposed approach is applied to deep networks with one- and two-hidden layers, and experiments are conducted on several real-world datasets. As noted by the authors, the class of feedforward neural networks under study is restrictive and somewhat counterintuitive, as it enforces non-negativity on the network weights and maximizes their regularization. Additionally, the use of the less common generalized polynomial activation function is required to satisfy the optimality condition. These assumptions appear to be quite limiting. Providing detailed and thorough explanations from the perspective of practical applications or real-world datasets would be more beneficial than introducing further restrictive assumptions to achieve global optimality.
The authors argue that the studied class of neural networks retains sufficient expressive power to model complex decision boundaries and delivers strong performance. However, the empirical results presented do not convincingly support this claim. It appears that linear SVM outperforms NLSM1 and NLSM2 in scenarios where the linear SVM is not optimally tuned, while NLSM benefits from tuning its network structures. Furthermore, given that NLSM is a nonlinear model, it is puzzling why a linear SVM is used as a baseline rather than a nonlinear SVM, such as one with a polynomial or Gaussian kernel. This raises concerns about the fairness of the comparisons and undermines the credibility of the conclusions drawn from these results. Consequently, the experimental findings fail to substantiate the claim that the studied class of neural networks possesses sufficient expressive power.
In the experiments, the datasets from the UCI repository are relatively small in terms of both sample size and feature dimensionality. According to the analysis, the spectral radius bounds of the matrix A increase with the number of hidden units, which poses challenges for high-dimensional data. Is there a way to address this issue? This limitation significantly hinders the applicability of the proposed method to a wide range of real-world problems. Additionally, it seems that the optimality condition is not guaranteed under all parameter settings that might be used in the studied neural networks. While the condition can be verified without running the networks, it becomes problematic when no prior knowledge is available, or when the parameter space is exceedingly large. Is there a faster approach to identify a smaller set of valid candidate parameters?
Finally, does the optimality of the proposed model stem from the specific objective function? It would be interesting to explore whether stochastic gradient descent can empirically converge to the global optimum on some of the tested datasets.