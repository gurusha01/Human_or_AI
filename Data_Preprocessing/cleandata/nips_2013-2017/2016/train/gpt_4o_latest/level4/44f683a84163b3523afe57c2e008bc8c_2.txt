The paper introduces a framework for 3D object generation based on generative adversarial networks (GAN), referred to as Volumetric Adversarial Networks (VAN). To tailor the GAN framework for 3D object generation, the authors employ a 3D volumetric convolutional architecture for both the generator and discriminator. Additionally, the paper integrates the VAN framework with a variational autoencoder to enable the synthesis of 3D shapes from 2D query images, naming this combined approach VAE-VAN. The proposed method is evaluated through a series of experiments, which include: 1) visual comparisons of the generated 3D shapes with prior work, 2) shape classification performance using unsupervised features extracted from the discriminator network compared to several prior methods, 3) visualization of 3D objects generated from color images using the VAE-VAN framework, and 4) quantitative comparisons of 3D objects generated from color images using VAE-VAN against other approaches. The paper also demonstrates shape arithmetic operations facilitated by VAN and provides visualizations of neurons in the discriminator.
Regarding novelty, the work can be seen as a relatively straightforward extension of GAN and DCGAN methodologies to the 3D object generation domain, achieved by substituting 2D image generation and discrimination networks with their 3D volumetric counterparts. The primary contribution lies in empirically demonstrating that several desirable properties of GANs and DCGANs for 2D image generation also hold for 3D object generation. These properties include the ability to generate novel objects and the utility of the discriminator network as a feature extractor for classification tasks. 
In terms of potential impact, the paper shows that the unsupervised features extracted from the discriminator network are more discriminative than other unsupervised feature learning methods but are only on par with state-of-the-art supervised algorithms. However, training a classification layer on these unsupervised features still requires labeled samples, which does not reduce the number of labeled examples needed to achieve a target performance. This raises questions about the practical advantage of the proposed unsupervised feature learning approach. The results would be more compelling if the method could achieve comparable or superior performance with fewer labeled examples using the proposed unsupervised features.
The paper is well-written and easy to follow, and I did not encounter any difficulties in understanding its content.