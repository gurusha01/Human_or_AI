This paper introduces a batch-version of the Knowledge Gradient (KG). To identify an optimal batch of design points, the authors propose a gradient estimation approach grounded in Monte Carlo simulation. Experimental results demonstrate that the proposed method outperforms other batch Bayesian optimization (BO) techniques. 
1. To my knowledge, the concept of batch KG is not novel and has been explored in prior works. Please clarify the specific contributions of this paper in comparison to earlier batch KG studies, such as [1] and [2]. My understanding is that the primary contribution lies in the proposed gradient estimation method for determining the optimal batch. If this is the case, the efficiency and effectiveness of the proposed approach should be rigorously evaluated against previous batch KG methods, either through experimental comparisons or mathematical analysis.
2. Section 5.2 appears to be central to the paper's contributions and should be elaborated further to provide greater clarity and depth.
3. Expected Improvement (EI) typically assumes noiseless observations. Therefore, the comparison in the noisy setting may not be entirely fair. Was a noise-aware variant of EI employed in the experiments? If the standard EI was used, the results in Section 6.2 should be discussed with an explanation of how EI's inability to account for noise impacts the comparison.
4. In line 162, the abbreviation "IPA" is redefined, which could cause confusion.
[1] Yingfei Wang, Kristofer G. Reyes, Keith A. Brown, Chad A. Mirkin, and Warren B. Powell. 2015. Nested-Batch-Mode Learning and Stochastic Optimization with An Application to Sequential MultiStage Testing in Materials Science. SIAM J. Sci. Comput. 37, 3 (January 2015), B361â€“B381. DOI:http://dx.doi.org/10.1137/140971117  
[2] http://castlelab.princeton.edu/theses/Peng%20-%20Senior%20Thesis%20-%20May032010.pdf