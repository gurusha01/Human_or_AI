The paper introduces a novel method for training ensembles of Deep Neural Networks (DNNs) (or other gradient-based models) by leveraging an oracle that identifies the optimal ensemble member for each training instance. Backpropagation is then applied exclusively to the network selected by the oracle. This approach enables the networks to specialize, resulting in an ensemble that effectively covers most (if not all) desired outputs. The method is evaluated across multiple domains, demonstrating strong overall performance. 
I find this to be a solid contribution. The paper effectively presents an intriguing idea for training DNN ensembles with an oracle that guides backpropagation updates to the network producing the best output (based on the true label). The method appears to promote specialization effectively in classification tasks. The presentation is clear, and the experiments span several domains, showcasing the robustness of the approach. Overall, this is a strong piece of work that merits publication.
That said, the paper would have been exceptional if it had addressed the practical exploitation of DNN ensembles trained using the proposed method. While the training process with the oracle is well-detailed, the utilization of the resulting ensemble is not thoroughly explored. Beyond mentioning that the approach aligns with the top-k metric used in certain problems (e.g., ImageNet) and provides a diverse pool of outputs, the paper does not delve into how these ensembles can be applied in practice. This omission is significant for making the approach fully actionable, though it is somewhat problem-specific and does not detract from the core contribution.
Additionally, Fig. 2 presents pseudo-code as raster images embedded in the paper. It would be preferable to include the pseudo-code directly in the text or, at the very least, embed it as a vector graphic for better readability and scalability.