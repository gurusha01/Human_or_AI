This paper presents a property termed low approximate regret for the expert algorithm, which facilitates faster convergence in games compared to prior work. The authors demonstrate that this property is achievable by several standard algorithms, including the vanilla Hedge algorithm. The feedback model considered in this paper is also more natural and less restrictive: the player receives either a realized loss vector based on the actions of other players or just the chosen coordinate of this loss vector (i.e., bandit feedback). The authors derive high-probability bounds and extend their results to dynamic population games, improving upon earlier work by [LST16]. Overall, the results in this paper are generally interesting. However, from a technical perspective, the main contributions appear to be a straightforward generalization of the insights in Theorem 23 of [SALS15], which discusses how a small loss bound leads to a faster convergence rate. Specifically, a small loss bound implies low approximate regret, as detailed in Section 3 of this paper. Furthermore, while the primary results in [SALS15] focus on the expectation feedback model, Theorem 23 in that work is applicable to the same realized feedback model considered here. Therefore, the claim that this paper improves upon [SALS15] in terms of the feedback model seems overstated. Additionally, the claim regarding improved convergence speed is somewhat misleading, particularly in the earlier sections of the paper. While the convergence is faster, it is only to an approximation of the Price of Anarchy (PoA), making these rates not entirely comparable. In the bandit setting, there exists a known algorithm that achieves a small loss bound against a non-oblivious adversary. See Allenberg, Chamy, Peter Auer, László Györfi, and György Ottucsák. "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring." 2006. It seems this algorithm could be directly applied here. Below are some detailed comments:
1. I recommend clarifying why most prior work relies on the expectation feedback model, whereas it is unnecessary in this paper.  
2. As noted earlier, Line 49 (the second bullet point for improvements) appears to be an overstatement. Similarly, in Line 80, the phrase "without a constant factor loss" seems inaccurate since the result is still not exactly the PoA, correct?  
3. In Definition 1, it might be helpful to explicitly require that \(\epsilon\) falls within (0,1).  
4. In Definition 2, the right-hand side of the inequality is missing the summation over \(i\).  
5. At the end of Proposition 2, it might be more intuitive to express \(\gamma\) in terms of \(\epsilon\) rather than the reverse.  
6. I suggest relocating Examples 1-3 (and the associated discussion) to Section 2, immediately after introducing the definition of low approximate regret.  
7. I am unclear on how the doubling trick transforms any algorithm with weak low approximate regret into one with strong low approximate regret.  
8. For Theorem 3, it is unconventional to use informal language such as "regret is bounded by \(\sqrt{\text{Total loss of best action}}\)" in a theorem. I recommend explicitly stating the precise expression.  
9. In Line 222, why is it stated as "within a (1+\(\epsilon\)) favor"? Shouldn't it be \(\lambda/(1-\mu-\epsilon)\) compared to \(\lambda/(1-\mu)\)?  
10. The proposed noisy Hedge algorithm bears some resemblance to Fixed Share; see, for instance, Cesa-Bianchi et al., 2012, "Mirror descent meets fixed share (and feels no regret)."  
11. Minor typos and errors:  
    1) Line 3: "a small a multiplicative"  
    2) Line 34: Remove "in"?  
    3) Line 42: "…more information than is…"?  
    4) Line 155: "required that is s is…"?  
    5) Line 207: "…For instance [FS97, YEYS04]…", consider placing this in parentheses and correcting the punctuation.