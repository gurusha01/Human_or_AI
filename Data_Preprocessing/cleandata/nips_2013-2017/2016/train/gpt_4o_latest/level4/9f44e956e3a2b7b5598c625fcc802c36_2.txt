This paper introduces an additional recurrent weight matrix with fast synaptic weight dynamics, functioning as a non-selective short-term memory. The authors propose a straightforward update rule for the fast weights, incorporating exponential decay. Through experiments on tasks such as the key-value paradigm, sequential MNIST, MultiPIE, and a reinforcement learning (RL) task, the paper demonstrates the effectiveness of the proposed memory in sequence-based tasks that do not necessitate selective memory mechanisms. UPDATE: I appreciate the authors' rebuttal and their efforts to address my concerns. With the suggested revisions, I am pleased to recommend this paper for acceptance. 
The manuscript is well-written, the proposed method is clearly described, and the experiments are comprehensive. However, I am left with the following questions regarding the current version: 
1) The authors claim that this form of short-term memory is biologically plausible. However, implementing the inner loop (Eq. 2) in a biological context seems challenging, as it would require caching \( W_h(t) \) during the update of the new hidden state. Additionally, the new state might require multiple iterations to converge. Could the authors speculate on potential biological mechanisms that might support this process in real neural networks? Furthermore, how sensitive is the mechanism to the number of iterations?
2) Line 136 references the Appendix, but I was unable to locate it. Could the authors clarify this?
3) In the simple key-value task, how does the network's performance evolve as the sequence length increases?
4) Given that the memory is non-selective in what it stores, I would expect reduced performance in tasks where the network must learn to store specific inputs while ignoring others. This scenario is not explicitly tested in the paper (the RL task appears similar but induces selectivity trivially through task structure). Could the authors explore a simple scenario where LSTMs outperform the proposed short-term memory? Such examples could effectively highlight the trade-offs of the proposed method.
5) Did the authors compare their approach with Facebook's key-value memory? The tasks evaluated in this paper seem better suited for comparison with such memory mechanisms than with LSTMs.
6) Lines 114–115 appear to conflate mini-batches with sequences, which may cause confusion for readers. Could the authors clarify this distinction?
7) Is the code for this network available online, or do the authors plan to release it?
8) Lines 209–211 mention that "the results can be integrated with the partial results at the higher level by popping the previously result from the cache." Could the authors clarify what they mean by "popping"? Is this process manually implemented, or do they expect the network to learn this behavior autonomously? If the latter, could the authors elaborate on how the network might learn this mechanism? This is non-trivial, as the memory decays passively, and actively "popping" would require subtracting the appropriate hidden state, which could inadvertently erase both the memory and other information encoded in the hidden unit activity.