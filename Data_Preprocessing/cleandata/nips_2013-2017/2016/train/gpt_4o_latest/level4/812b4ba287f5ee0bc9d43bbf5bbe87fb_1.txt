This paper introduces a method for generating object proposals using a sequential search strategy trained via reinforcement learning, specifically Q-learning. The work makes two key contributions: first, at test time, the method selects two actions per step (one optimal action from each of two action classes) and bifurcates. This enables the model to better handle multimodality by predicting multiple objects simultaneously. Second, the reward signal is carefully crafted to provide feedback both when object localization improves and when the model identifies a new object, without enforcing any specific order. The method achieves impressive recall and, when integrated with Fast R-CNN, outperforms RPN by approximately 2 points. 
I found the ideas in this paper compelling. This is the first sequential search strategy I have encountered that aims to output all objects in an image without imposing an arbitrary or hard-to-justify order on bounding boxes during training. However, I have a minor clarification request and some suggestions to strengthen the experimental section, which I believe would elevate the paper from a poster to an oral presentation.
Clarification: The ranking of the proposals appears to be based solely on the depth of the tree at which they are discovered. Why is this ranking effective? Is there any aspect of the training process that encourages the model to identify objects quickly? While I can think of intuitive justifications, it would be helpful for the authors to explicitly address this in the paper.
Suggestions for improvement: Object proposal generation is a rapidly evolving subfield of computer vision, and the paper would benefit from additional experiments:  
1. Comparison to more proposal methods: Specifically, comparisons to MCG [1] and DeepMask [2] would be valuable, as both methods significantly outperform simpler approaches like EdgeBoxes. MCG proposals are readily available online.  
2. Evaluation on COCO: The COCO dataset encompasses a broader range of scale variations, particularly for small objects, where methods like RPN often struggle. It would be insightful to evaluate the proposed method on COCO.  
3. Generalization to unseen object categories: If the authors envision this as a general-purpose proposal method, it should generalize to object categories not encountered during training. Testing on COCO categories absent from PASCAL or ImageNet would provide insight into the method's generalization capabilities.  
4. Detection AP for higher overlap thresholds: The method appears to improve recall at higher Intersection over Union (IoU) thresholds, so it would be interesting to see detection AP under more stringent evaluation metrics. This could highlight the method's advantages in scenarios requiring precise localization.
In summary, the paper presents a novel and promising approach, but addressing these points would further solidify its contributions and impact.