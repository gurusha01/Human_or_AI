The paper introduces a method to replace standard Monte Carlo techniques for ABC with an approach grounded in Bayesian density estimation. While density estimation has previously been applied in the context of ABC, this method enables a direct substitution of sample-based posterior approximations with an analytic one. The novelty is comparable in scope to that of ABC with variational inference [1], though the methodology here is distinctly different. [1] Tran, Nott, and Kohn, 2015, "Variational Bayes with Intractable Likelihood."
The paper is well-written and highly accessible. The technical exposition is appropriate for an expert audience, avoiding unnecessary details while maintaining clarity. Although the work builds heavily on prior research, the central idea of Proposition 1 is skillfully applied throughout, both in selecting the proposal prior and in approximating the posterior. Additionally, the paper includes other moderately novel but practical contributions, such as the extension of MDN to SVI. That said, the authors should also address related work on the use of SVI in ABC, such as [1].
The paper lacks a rigorous theoretical foundation beyond the asymptotic justification provided by Proposition 1 for the proposed algorithm. However, I consider this acceptable for a paper of this nature, particularly given the constraints of the NIPS format [where it is unlikely that detailed theoretical explanations, experiments, and model descriptions could all fit within the eight-page limit].
The experimental results strike a good balance between simple illustrative examples and more complex datasets, and they are presented clearly. I also appreciate how the authors disentangle the impact of the proposal distribution choice from the posterior approximation. While the plots attempt to account for effective sample size, I am uncertain whether this is the most appropriate metric. Since samples in this context are purely computational constructs, it might be more meaningful to evaluate actual CPU time instead.