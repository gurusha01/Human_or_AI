This study introduces a method for parallel Batch Bayesian Optimization by determining a Bayes-optimal batch of configurations for the next evaluation. The proposed approach quantifies the utility of evaluating a batch of configurations using the knowledge gradient (q-KG). To efficiently optimize the acquisition function, the authors present a technique based on infinitesimal perturbation analysis (IPA) to estimate the gradient of q-KG. Through three experiments, the authors demonstrate that q-KG performs competitively with existing parallel Bayesian optimization methods on problems without observation noise and surpasses these methods on problems with observation noise. While I appreciate the concept of parallel hyperparameter optimization to leverage multi-core environments effectively, certain aspects remain unclear, and I find the experimental results unconvincing. Below, I provide feedback on the aforementioned points and pose several questions for clarification:
---
Technical Quality  
The paper evaluates the proposed method using standard artificial benchmark functions to establish its superiority. Additionally, it includes results for optimizing a CNN and logistic regression on CIFAR-10 and MNIST datasets. However, I would like to see an experiment that reports wall-clock time or an analysis quantifying the gains from using this parallel method compared to simpler approaches like random search or sequential optimization. Furthermore, it is unclear why the batch size (q) is fixed at 4 across all experiments.
---
Novelty/Originality  
The paper proposes using the knowledge gradient (a generalization of expected improvement) as the acquisition function and employs infinitesimal perturbation analysis (IPA) to optimize q-KG. While these methods are well-established, the paper references "Parallel Bayesian global optimization of expensive functions," which already demonstrates how to use IPA for selecting a batch of configurations with expected improvement. Therefore, the novelty of the proposed approach appears limited.
---
Impact/Usefulness  
Parallel hyperparameter tuning methods are highly relevant for real-world applications across various domains, including machine learning, computer vision, biology, and robotics. The proposed method has the potential to contribute to these fields by enabling efficient parallel optimization.
---
Clarity/Presentation  
The technical details and background are well-articulated, and the experimental section includes sufficient information to understand the comparisons conducted. However, certain aspects could benefit from additional clarification, as outlined in the questions below.
---
Questions to the Authors  
1. Wang et al. (2015) appears to be highly relevant to this work, and their code is reportedly available online. Have you compared your method to theirs? If this corresponds to "parallel EI in MOE," it would be helpful to explicitly refer to it as q-EI to distinguish it from Spearmint's parallel method. Clarifying these distinctions in the paper would reduce potential confusion.  
2. Modern multi-core systems can support significantly more than 4 parallel runs. For example:  
   - "Scalable Bayesian Optimization Using Deep Neural Networks" (ICML 2015) utilized up to 800 parallel runs of DNGO.  
   - "Parallel Algorithm Configuration" (LION 2012) ran 25 SMAC instances in parallel.  
   - "Practical Bayesian Optimization of Machine Learning Algorithms" (NIPS 2012) employed up to 10 parallel runs of Spearmint.  
   Do you have insights into how well your method scales with larger batch sizes and the potential benefits of increased parallelism?  
3. Your method currently operates synchronously. Could it be extended to run asynchronously, similar to Spearmint?  
4. Since wall-clock time is critical in practice, have you evaluated how your method compares to random search or sequential Bayesian optimization in terms of wall-clock time? Additionally, what is the computational overhead of generating a new batch?  
5. In your plots, what does "iterations" on the x-axis represent? Does it correspond to the number of function evaluations or the number of batches? Does the x-axis account for the initial design? If so, why do the methods not start from the same value?  
---
Minor Comments  
- Is there a missing \( z^{(1:n)} \) term in Equation (4.1)?  
- "This architecture has been carefully tuned by the human expert, and achieve a test error" â†’ "achieves."  
- Parenthetical references should not be treated as nouns. For example, "[16] suggests parallelizing" should be rephrased as "Snoek et al. [16] suggest parallelizing."  
- Wang et al. (2015) does not appear to be the correct reference for IPA, as they did not introduce the method but rather applied it.