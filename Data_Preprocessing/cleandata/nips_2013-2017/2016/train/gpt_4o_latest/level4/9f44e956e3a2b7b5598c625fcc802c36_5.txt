Revised Review:
Standard recurrent neural networks (RNNs) involve two types of variables: synapses, which are updated at the end of the input sequence to capture patterns across inputs, and neural activities, which serve as short-term memory. This paper introduces a second category of synapses, referred to as "fast weights." These fast weights evolve at a rate slower than neural activities but faster than conventional synapses (termed "slow weights"). The concept is inspired by physiological evidence suggesting that synapses in the brain operate on multiple timescales. The fast weights are employed in an associative network to store memories, with their update governed by a Hebbian-like learning rule designed to retain past hidden activities. This associative network functions as a mechanism for attending to recent history, akin to previously explored attention mechanisms that have enhanced sequence-to-sequence RNNs. The key distinction here is that the attention strength to a particular past hidden activity is determined not by a separate set of parameters but by the scalar product between the current and past hidden activities.
The proposed algorithm's effectiveness is demonstrated across various tasks, and the authors provide both mathematical arguments (e.g., memory capacity comparisons) and neuroscientific evidence (e.g., synaptic plasticity on different timescales) to justify the introduction of fast weights. The algorithm builds connections to prior work on memory mechanisms, such as Neural Turing Machines (NTM) and Memory Networks, as well as attention mechanisms, while being significantly more biologically plausible than earlier models. This represents a meaningful step toward narrowing the gap between computational neuroscience and cognitive science. From a computational standpoint, the authors employ a clever optimization to avoid explicitly computing the full fast weight matrix \( A \). As outlined in lines 100–104, it suffices to store the hidden activities, which greatly enhances the algorithm's computational efficiency. Additionally, this optimization enables the algorithm to handle mini-batches effectively, as described in lines 111–115.
One limitation I noticed is that the paper provides limited details on how the slow weights \( W \) and \( C \) are trained, only mentioning the use of the Adam optimizer. A more thorough explanation would be valuable. Furthermore, I recommend adding a detailed computational graph (more elaborate than Figure 1) to clarify how automatic differentiation is implemented. The presence of an "inner loop" for computing \( h_s(t) \) makes the process less intuitive and potentially harder for readers to grasp. A specific question arises: are gradients backpropagated through \( A(t) \) (which depends on \( h(\tau) \), and consequently on \( W \) and \( C \)), or are these variables treated as constants during gradient computation in the computational graph?
Lastly, a minor remark: there is a typo on line 287 where "units" is misspelled.