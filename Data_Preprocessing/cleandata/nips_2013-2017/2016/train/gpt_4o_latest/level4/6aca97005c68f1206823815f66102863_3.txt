The authors present a method to approximate the posterior distribution of intractable models using a neural network-based density estimator. A key advantage of this approach, compared to ABC methods, is the elimination of the need to select a tolerance parameter. The novel aspect of the work lies in directly modeling the posterior distribution, as opposed to the more conventional strategy of approximating or estimating the intractable likelihood. Consequently, Proposition 1 emerges as the central result of the paper. Building on Proposition 1, the authors employ a Mixture Density Network (MDN) as a conditional density estimator, though other estimators could also be used. They detail the estimation of the proposal prior and posterior density through Algorithm 1 and Algorithm 2, respectively. The method is demonstrated on several simple examples, including two with intractable likelihoods. The most innovative contribution is Proposition 1, which is intriguing. However, I have reservations about the assumptions underlying formula (2). As clarified in the appendix, this formula holds under the condition that \( q\theta \) is sufficiently expressive to ensure a KL divergence of zero. In practical scenarios with finite sample sizes, \( q\theta \) cannot be overly complex, as this would lead to overfitting. Thus, formula (2) is only approximately valid. 
The examples provided are somewhat underwhelming. Since tolerance-based ABC methods struggle in high-dimensional settings, I expected at least one example in a relatively high-dimensional space (e.g., 20 dimensions). It is unclear whether the MDN estimator would scale effectively with an increasing number of model parameters or summary statistics. The practical utility of the proposed method heavily depends on its scalability, which is not demonstrated. My understanding is that the complexity of fitting the MDN depends on the hyperparameter \(\lambda\) and the number of components. While the number of components was selected manually, the value of \(\lambda\) is not reported. How was this parameter chosen?
Additional comments, organized by section:
- Section 2.3  
  1. Is a proper prior required?  
  2. In Algorithm 1, how is convergence assessed, given the stochastic nature of the algorithm?  
- Section 2.4  
  1. The authors state: "If we take \(\tilde{p}(\theta)\) to be the actual prior, then \(q\phi(\theta | x)\) will learn the posterior for all \(x\)." Is this claim accurate? Depending on the prior, the model might learn the posterior for values of \(x\) that are very different from \(x0\), but likely not "for all \(x\)." It may also be worth emphasizing that \(q\phi(\theta | x)\) needs to be modeled near \(x0\) because the posterior is being modeled non-parametrically. For example, if a linear regression model were used, the variance of the estimator would decrease by selecting points \(x\) far from \(x_0\).  
  2. Why do the authors use a single Gaussian component for the proposal prior but multiple components for the posterior? Is sampling from an MDN with multiple components computationally expensive? If the same number of components were used, it might be possible to unify Algorithms 1 and 2 by iteratively applying Algorithm 2, using the estimated posterior from the \(i\)-th iteration as the proposal prior for the next iteration.  
  3. It is unclear how the MDN is initialized at each iteration in Algorithm 1. The authors mention that initializing the prior using the previous iteration allows them to keep \(N\) small. This suggests that initialization involves more than simply providing a good starting point for the optimizer, potentially involving the reuse of simulations from earlier iterations. However, this process is not clearly explained.  
- Section 2.5  
  1. The rationale for why MVN-SVI avoids overfitting is unclear. Whether overfitting occurs likely depends on the hyperparameter \(\lambda\). How is \(\lambda\) currently chosen? Presumably not via cross-validation, as the authors state that no validation set is required.  
- Section 3.1  
  1. The differences between the densities in the left plot of Figure 1 are barely discernible. Consider plotting log-densities instead.  
  2. What value of \(\lambda\) was used to produce these results? This information is not provided here or in the other examples.  
- Section 3.2  
  1. Is formula (5) correct? \(x\) is a vector, yet its mean and variance are scalars.  
  2. In Figure 2, it may be helpful to explain why, in ABC, the largest number of simulations does not correspond to the smallest KL distance. Presumably, this is because \(\epsilon\) is too small, leading to a high rejection rate.  
- Section 3.3  
  1. The authors note that "in all cases the MDNs chose to use only one component and switch the rest off, which is consistent with our observation about the near-Gaussianity of the posterior." Does this behavior occur for any value of \(\lambda\)?