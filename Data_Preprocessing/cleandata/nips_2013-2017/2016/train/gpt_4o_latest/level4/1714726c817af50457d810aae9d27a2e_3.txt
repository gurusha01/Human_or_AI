The paper investigates accelerated first-order methods for constrained convex optimization in both continuous and discrete settings. The authors build upon the work initiated by Krichene et al. in [7], where Nesterov's method was shown to correspond to a discretization of an ODE system coupling a primal trajectory with a dual one. Specifically, the primal trajectory is derived by performing a particular averaging of the mirror of the dual variable. In this work, the authors explore more general averaging schemes. Theorem 2 establishes convergence rates under appropriate conditions on the averaging coefficients. In the continuous setting, the authors derive convergence rates that surpass 1/t^2. They also investigate an adaptive selection of weights with convergence guarantees, which is subsequently employed in the experimental section. This adaptive approach is compared with restarting techniques, which are known to enhance the practical performance of accelerated methods. However, the theoretical analysis of the algorithm's discrete version is not included. Overall, I find this to be an interesting and well-written paper. The analysis of the new averaging schemes for accelerated methods is rigorous and presented within the continuous framework. While the main ideas underlying the analysis of continuous dynamics have been well-established in prior works, this paper offers new contributions and results. In this context, I believe the work of Attouch, Peypouquet, and their collaborators should be cited. The main theorem, which demonstrates the existence of averaging schemes achieving faster convergence rates than 1/t^2, is significant. However, it would be valuable to investigate whether this improved rate is retained when transitioning to the discrete setting.