This paper introduces a novel LSTM-based architecture called "Phased LSTMs," which incorporates multiple periodic masks into LSTM networks, each with distinct opening frequencies. This design facilitates faster learning of long-term dependencies. The proposed approach, reminiscent of a Fourier decomposition, offers several advantages, including accelerated training of LSTM networks while achieving state-of-the-art performance across various tasks. A key strength of this work lies in its ability to enable LSTMs to process asynchronous data feeds, meaning that inputs do not need to arrive at regular intervals. The authors provide a thorough explanation of the motivations behind and the design of the architecture. They follow this with three diverse experiments on distinct tasks, demonstrating that Phased LSTMs can effectively handle asynchronous data, outperform standard LSTMs in such scenarios, and achieve faster training in all cases.
The paper is well-written, clearly presented, and introduces a compelling innovation in the domain of recurrent neural networks, with strong connections to the theory of frequency decomposition in time series analysis. The potential impact of Phased LSTMs is significant, particularly due to their ability to process asynchronous data feedsâ€”a characteristic relevant to many modern sensing applications involving irregular time series data. The phased input gate is an elegant concept, akin to projecting a time series onto a Fourier basis, which significantly enhances the learning speed of LSTMs by reducing the number of updates required and enabling more effective gradient propagation over time. The experiments are methodically conducted on diverse and relevant tasks, showing substantial improvements over existing methods. The quality of writing is excellent throughout.
I have two minor comments: First, Equation (11) and the corresponding section provide a useful simplification that helps explain why Phased LSTMs are more likely to capture long-range dependencies. However, the simplified model closely resembles an AR model, which I believe is an overly simplified representation of an LSTM. The authors should clarify that this section is intended as a pedagogical simplification. Second, in the experiments, it is unclear how many different oscillation periods were used in practice. Specifically, how many periods were sampled from the exponential distributions? 
Overall, this paper is innovative, straightforward, and enjoyable to read. It has significant potential for advancing neuromorphic computing and time series analysis, and I believe it is well worth presenting to the community.