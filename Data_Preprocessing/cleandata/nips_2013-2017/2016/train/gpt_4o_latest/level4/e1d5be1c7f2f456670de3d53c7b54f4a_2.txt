The authors investigate the problem of contextual semi-bandits and propose efficient algorithms for addressing this problem when a supervised learning oracle is available. They examine two scenarios: one where the reward is a known linear function of individual feedbacks, and another where the linear transformation is unknown. For the case of a known linear transformation, the proposed algorithm bears similarities to the approach by Agarwal et al. [1], albeit with some differences. In the scenario with an unknown transformation, the algorithm adopts a phased explore-exploit framework, where the reward vector is first estimated and subsequently used in conjunction with an optimal policy. The paper is generally well-written. 
Comments:  
1) The OP problem is formulated as a feasibility problem. Why not consider the optimization version, where the goal is to identify the policy with the smallest possible empirical regret, subject to variance constraints (i.e., optimizing the LHS in Equation 4 under the constraints in Equation 5)? My assumption is that the AMO is better suited for addressing the feasibility problem, but it would be helpful to have this explicitly clarified in the paper.  
2) It would be beneficial to include more discussion in the main body of the paper regarding how the AMO is utilized. For instance, if the policy class consists of linear classifiers, what specific role does the AMO play in this context?