A convex training framework for a two-layer probabilistic model \( p(z|y)p(y|x) \), representing the structure \( Z \leftarrow Y \leftarrow X \) (where \( X \) is the input, \( Y \) is the latent space, and \( Z \) is the output), is introduced and analyzed. Unlike prior work, this approach only necessitates tractable MAP inference on \( p(y|x) \), which is feasible for graph-matching models or any integer linear program with a totally unimodular constraint set. Traditional methods for training two-layer models often rely on bi-level optimization, where \( p(z|\arg\max_y p(y|x)) \) is solved by first optimizing the encoder model over \( y \) in an "inner" step, followed by solving the decoder in an "outer" step. However, training such models is challenging because small perturbations in the encoder model typically yield the same discrete latent representation \( y \), leading to gradients that are zero almost everywhere. 
The proposed method addresses this issue by incorporating the first-order optimality conditions of the "inner" optimization into the "outer" optimization using Lagrange multipliers and a saddle-point formulation. While the resulting problem is non-convex in the training parameters, the authors propose a semi-definite programming (SDP) relaxation by dropping the rank constraint, resulting in a convex formulation. Consequently, the authors claim that learning only depends on efficient MAP inference over the latent space. The approach is demonstrated on transliteration and image inpainting tasks, where it outperforms the provided baselines.
Review Summary: The paper presents an elegant method to address challenges in bi-level optimization for probabilistic modeling. However, several critical details are missing, making it difficult to fully assess the quality of the solution. For instance, additional assumptions may be required (see Point 1 below), key aspects lack clarity (see Point 3 below), and the experimental evaluation is somewhat limited. Detailed comments follow.
- Technical Quality: The derivations appear sound, but additional assumptions may be necessary to ensure efficiency.  
- Novelty: The paper introduces a novel approach to two-layer probabilistic modeling.  
- Potential Impact: The impact is difficult to evaluate due to missing details about the method's efficiency.  
- Clarity: Certain important aspects are insufficiently explained.  
I will adjust my score based on the authors' responses.
---
Review Comments:
1. Tractability of Decoder Normalization:  
   Assuming \( z \) is discrete, the decoder probability in Eq. (4) depends on the function \( G \), which should correspond to the log-partition function to ensure proper normalization. In the proposed formulation (e.g., Eq. (15)), the maximization over the dual (entropy restricted to the probability simplex) is required. Does this not assume tractability of the maximization task? Specifically, the resulting program appears to involve a quadratic term, entropy terms, and simplex constraints. Could the authors clarify how this task is solved efficiently? Section 5 does not provide sufficient details.
2. Upper Bound in Line 113:  
   Although not part of the proposed approach, the authors should verify the correctness of their claim in line 113 that the partition function \( \Omega(Ux) \) is replaced by an upper bound \( \max -y'Ux \). Based on my understanding, \( \Omega(Ux) = \log\sumy\exp(-y'Ux) = -y^\ast'Ux + \log\sumy\exp(-y'Ux + y^\ast'Ux) \), where \( y^\ast = \arg\max_y -y'Ux \). The log term can be either positive or negative, so the maximization does not necessarily represent an upper bound. Did I overlook something?
3. Bounded Model Parameters:  
   The introduction of bounded model parameters is unclear. The original formulation in Eq. (7) does not include constraints, yet constraints appear in Eq. (10) and subsequent equations. The explanation provided ("to simplify the presentation") is insufficient. Could the authors elaborate on the rationale for these constraints? Is feasibility an issue?
4. Baseline Comparisons:  
   The method is compared to a baseline from 2010 for the transliteration task. Including more recent baselines would strengthen the evaluation. Could the authors comment on why more contemporary methods were not included?
5. Computational Complexity:  
   The use of an SDP formulation raises concerns about computational scalability. Could the authors discuss the complexity of their approach and its applicability to larger problem sizes?
6. Gradient Descent Comparison:  
   Have the authors compared their SDP-based approach to gradient descent on the objective in Eq. (15)? Such a comparison would help assess the benefits of the proposed relaxation.
---
Minor Comments:
- The authors should clarify that the \( ^\prime \) operator denotes the transpose.