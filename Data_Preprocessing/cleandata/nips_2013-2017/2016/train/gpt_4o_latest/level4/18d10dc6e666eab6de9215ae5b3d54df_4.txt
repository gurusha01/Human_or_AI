This paper investigates the parallel knowledge gradient method for batch Bayesian Optimization, focusing on scenarios where function evaluations are time-intensive. The authors introduce a novel acquisition function, q-KG, designed to generate multiple points for parallel evaluation. Additionally, they present an efficient approach for evaluating the q-KG function. Empirical results are provided to support their theoretical findings. The study addresses an important problem, as distributing computational tasks can significantly enhance the performance of data-driven applications. By proposing q points for evaluation in each iteration, rather than a single point, the q-KG algorithm substantially improves efficiency. This is an intriguing concept worthy of further exploration. However, the assumption of a Gaussian process may not always hold, particularly in neural network architectures with high-dimensional inputs. It would be beneficial for the authors to include theoretical results on the error bounds of their method and a quantitative analysis of its complexity with respect to dimensionality. Despite these concerns, the work is a strong contribution to the field of Bayesian Optimization. The proposed method is robust, and the experimental results are convincing. The paper is generally well-written, though there are a few typos and areas for improvement: l.36: "the set of points to evaluate next that is" should be revised for clarity. l.95: The definition of A is unclear. l.104: Clarify the input to Î¼^n. l.128: A brief discussion of the parallel EI algorithm would be helpful. l.140: Consider using distinct notations for A to avoid confusion.