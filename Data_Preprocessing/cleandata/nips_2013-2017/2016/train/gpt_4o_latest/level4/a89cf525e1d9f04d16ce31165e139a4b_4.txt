This paper proposes a method to incorporate a sparse prior into the learning of encoders and decoders within the information bottleneck (IB) framework. The authors demonstrate how variational approximations can be employed to address the associated optimization problem. The approach is shown to learn Gabor-like filters from image patches, akin to those obtained through traditional sparse coding techniques. Additionally, the paper introduces a kernel-based extension that facilitates the learning of non-linear encoders. The utility of the proposed method is illustrated through an image inpainting task involving occluded handwritten digits. The paper is well-written, and the methodology appears to be technically sound. It is intriguing to see how sparsity can be integrated into the IB framework to produce results visually comparable to conventional sparse coding. 
However, the utility of sparse priors is well-established, and there are already numerous approaches that leverage sparsity (e.g., LASSO, sparse autoencoders, etc.). The paper would be significantly strengthened if the authors provided deeper insights and quantitative comparisons to highlight the advantages of the IB-based approach. The proposed method seems particularly related to sparse autoencoders, as both can produce linear encoders and decoders. The experimental evaluation, however, appears somewhat limited, focusing primarily on 9x9 image patches without extensive analysis of the results. As a result, it remains unclear under what specific conditions the IB method would be preferred. The example applications presented, such as image denoising and inpainting, are tasks that can also be addressed using conventional sparse coding. It would be valuable to clarify whether the IB method offers any tangible advantages in these scenarios. Overall, while the paper is technically sound, additional comparisons with existing methods are necessary to better establish its contributions.
Detailed comments:  
1. Line 32 discusses varying gamma to balance encoding quality and compression. This trade-off can similarly be achieved in other sparse coding methods, such as by adjusting the strength of the l1-norm regularization in LASSO. This does not appear to be a unique feature of sparse IB.  
2. Another distinction from conventional sparse coding is that the IB method learns a specific input-output relationship. How should one select an appropriate output in practice?  
3. Line 73: Are you referring to q(y|r) = N(y|Ur)?