This paper introduces a variation of the LSTM model by incorporating a time moderator gate, enabling the model to process longer sequence inputs and retain memories over extended time periods. The proposed time gate is a cyclic function of time, controlling the extent to which the cell and hidden states are updated based on the current input or preserved from the previous time step. Experimental results demonstrate that the proposed model outperforms standard LSTM in capturing long-term dependencies. Addressing the challenge of enhancing RNNs' ability to handle longer sequences is a significant contribution. This paper presents an innovative and effective approach by designing a new gate whose value depends on the input time. As illustrated in Figure 2(b), each LSTM cell can capture memories of varying lengths, making the model well-suited for handling long or over-sampled sequence inputs. Furthermore, the proposed concept can potentially be extended to other gated RNNs, such as GRUs. 
However, I have several concerns: While I agree that the proposed model is capable of handling long sequences, I am not fully convinced that the designed time gate explicitly addresses event-based or irregularly sampled data. From my perspective, the primary purpose of introducing a time gate is to restrict updates to the hidden states to a small fraction of time steps. To validate the claim that the proposed time gate is effective for irregular or event-based sequences, I recommend comparing the proposed model with a similar model that uses a random time gate. Specifically, the time gate \( kt \) could be replaced with a random value drawn from the same distribution (e.g., with a probability \( r{on} \) of being 1 and otherwise 0, or other "leaky" variations) but independent of the input time \( t \). Such a random model might also handle long sequences effectively due to the small update ratio. This comparison would help determine whether the proposed model is genuinely better at modeling event-based sequences. At present, the input time to the gate \( kt \) appears to function more as a random seed for generating gate values with a mean of \( r{on} \), without directly leveraging the time step itself.
Regarding the experiments, Figure 3(d) shows that the proposed LSTM outperforms baselines in both high-resolution sampling and asynchronous sampling settings. However, what is the average sampling rate in the asynchronous setting? It appears that even the asynchronous setting involves more input steps than the standard condition. If this is the case, how can we determine whether the baselines underperform simply due to the longer sequences in this condition? 
Additionally, the authors claim that the new LSTM achieves "an order-of-magnitude fewer computes." However, it is unclear whether "computes" refers to the number of training epochs required for convergence or the computational time needed to process a single input sequence. According to the introduction, the authors aim to address unnecessarily high computational load and power consumption by updating only a subset of units in the network at each time step. While this goal is commendable, it may not hold true for the proposed model. First, during training, the "leaky" behavior necessitates updating all nodes. Second, even if only a small fraction of states are updated, this may not translate to improved efficiency, as neural network operations are typically performed via matrix operations (especially on GPUs), where skipping individual elements may not significantly impact computational cost. At the end of Section 3.3, the authors state that the proposed LSTM requires updates for only 5% of neurons, but this does not necessarily imply a 5% reduction in computational time. Alternatively, the authors should clarify whether their implementation employs a more efficient approach to achieve this reduction.