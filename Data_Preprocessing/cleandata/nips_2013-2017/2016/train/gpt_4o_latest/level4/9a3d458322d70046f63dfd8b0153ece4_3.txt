This paper addresses the challenge of developing safe reinforcement learning algorithms, where safety is defined relative to a given baseline policy. The authors introduce a novel safety concept, focusing on maximizing the minimum improvement over the baseline. The paper provides theoretical insights into this new formulation and proposes an approximate algorithm to operationalize it. This work offers a fresh perspective on the design of safe RL algorithms. However, the exposition can be difficult to follow at times, and a review of existing safety definitions prior to introducing the new concept would have been helpful. Additionally, placing entire algorithms in the appendix detracts from the paper's clarity and accessibility. 
Issues/questions:  
- Line 104: The definition of safety is unclear. Is it original to this work, or does it build on prior definitions? For instance, Thomas et al.'s "High Confidence Policy Improvement" offers a high-probability, value-based definition of safety. The paper should clarify whether multiple definitions exist and justify the choice of the proposed one.  
- Theorem 3: Why is a multiplicative bound the appropriate choice here? Would an additive bound lead to different conclusions, and if so, why is the multiplicative approach preferred?  
- Proposition 7: Can the approximation error be bounded when the stated assumption fails? Specifically, in Remark 1, could Algorithm 1 perform arbitrarily worse than the exact solution under certain conditions?  
- Proof of Theorem 5: The transition on line 392 from ρ(π_S, ξ) to ρ(π, ξ) is unclear. Since π is the optimal policy for P*, not for any ξ, this step requires further explanation.  
- Proof of NP-hardness: The proof is challenging to follow. For example, why are certain states (e.g., l_13) missing at the bottom of Figure 6? Additionally, the statement of the result needs refinement, as it appears incorrect in its current form. The uncertainty set should be described in terms of its independent factors. If the algorithm is provided with the entire set Ξ as a single input, the minimization becomes trivially polynomial in the size of Ξ, which is exponential.  
Minor issues:  
- P: The introduction of P seems unnecessary, as the MDP is already defined using P. Would it not be simpler to use P throughout?  
- Definition 2: Are we optimizing for a maximal ζ or for a policy π that achieves the maximum? This distinction should be clarified.  
- Line 118: The phrase "standard approach" feels overly definitive, especially since the paper has not yet provided a thorough review of existing safe RL methods.  
- Line 160: The state occupancy measures are defined with respect to p_0 and are discounted. This should be explicitly stated for clarity.  
- Theorem 8 and Lemma 11: The use of the vectors e and e_π is somewhat imprecise and requires guesswork. These vectors should be rigorously defined to avoid ambiguity.