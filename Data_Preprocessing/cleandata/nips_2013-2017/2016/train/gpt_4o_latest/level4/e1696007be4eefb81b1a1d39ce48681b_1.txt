The authors explore the problem of estimating GLMs in scenarios where \( n \) is significantly larger than \( p \). They leverage the observation that the GLM estimator is sometimes proportional to the OLS estimator and demonstrate that this relationship holds more broadly in random design settings. To capitalize on this, they propose a two-step estimator: first, a fast OLS computation, followed by a quick root-finding step to estimate the proportionality constant. They validate their approach on benchmark datasets, showing that their method achieves a desired test error much faster than a wide range of standard first- and second-order optimization algorithms. Overall, I find the contribution to be valuable. The paper is well-written and clearly presented. The central idea is compelling and appears to be novel. However, I have a few concerns: 
While the experiments are extensive, there is a notable omission. The authors did not include comparisons with a stochastic optimization method. For instance, I strongly suspect that SVRG [Johnson & Zhang, 2013] would outperform many of the competing methods considered. Additionally, the theoretical results, particularly Proposition 2, are limited to sub-Gaussian random variables. While the results are interesting, the naive subsampling estimator is likely to perform poorly in the presence of heavy-tailed data. This limitation has been addressed in prior work, such as Dhillon et al., and others, who proposed random-projection-based estimators. Although incorporating such methods would introduce an additional computational cost of \( O(np \log p) \), it would have been beneficial to include experiments with heavy-tailed data to assess the robustness of the proposed algorithm. As it stands, the practical utility of the algorithm is somewhat unclear. A practitioner working with heavy-tailed data would need to exercise caution, but they might achieve similar speedups—without such vigilance—by using a stochastic variance-reduced optimization method.