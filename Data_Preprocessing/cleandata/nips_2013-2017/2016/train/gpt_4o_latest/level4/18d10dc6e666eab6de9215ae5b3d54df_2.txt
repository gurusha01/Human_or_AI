Bayesian optimization has gained significant traction in machine learning applications such as hyperparameter optimization, though most existing methods are fundamentally sequential in nature. This paper introduces a parallel Bayesian optimization algorithm that computes Bayes-optimal batches. The authors demonstrate that their proposed method achieves superior performance compared to existing Bayesian optimization techniques. The manuscript is well-written and straightforward to understand. Parallelizing Bayesian optimization is a crucial topic for practical hyperparameter tuning, and the proposed approach is both intriguing and more refined than many existing methods I am familiar with. The ability to determine a Bayes-optimal batch is particularly promising. The authors assume independent, normally distributed errors, which is a standard assumption in most Bayesian optimization methods leveraging Gaussian processes. However, this assumption can be problematic in the context of hyperparameter optimization, as measurement errors often reflect the discrepancy between generalization performance and empirical estimates (e.g., via cross-validation). Structural biases are prevalent in certain regions of the hyperparameter space, especially when sample sizes are small. It would be worthwhile to note that assuming independence may not always be realistic. 
While the theoretical contributions of the paper are strong, the experimental evaluation is somewhat underwhelming for three primary reasons:  
1. The use of test set error as the optimization criterion is problematic, as it is well-documented that such score functions are suboptimal for hyperparameter tuning (e.g., [1]). Metrics like test set error, which are threshold-based, introduce several challenges for model selection and hyperparameter optimization. Alternative metrics such as area under the ROC curve or log loss would be more appropriate.  
2. The experimental benchmarks rely on the MNIST and CIFAR10 datasets, which, while commonly used, are not ideal. Benchmarks from HPOlib [2], a widely-used library specifically designed for evaluating hyperparameter optimization algorithms, would have been more suitable.  
3. The benchmark is limited in scope, utilizing only two datasets and focusing exclusively on a single machine learning task (classification) with just two learning algorithms (logistic regression and CNN). A broader variety of datasets and tasks would have been preferable, as it is well-known that certain optimization methods perform well for specific types of learning algorithms but poorly for others.  
In summary, I find the paper to be compelling, and the theoretical contribution is undoubtedly valuable. However, the current experimental evaluation is too narrow to be fully convincing.  
[1] Provost, Foster J., Tom Fawcett, and Ron Kohavi. "The case against accuracy estimation for comparing induction algorithms." ICML. Vol. 98. 1998.  
[2] Eggensperger, Katharina, et al. "Towards an empirical foundation for assessing bayesian optimization of hyperparameters." NIPS workshop on Bayesian Optimization in Theory and Practice. 2013.