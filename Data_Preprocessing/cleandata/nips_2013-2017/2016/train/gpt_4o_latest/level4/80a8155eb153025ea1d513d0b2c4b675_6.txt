The authors investigate the robust k-means (RKM) algorithm, which extends the traditional k-means algorithm by assigning error terms to points and penalizing these errors to encourage most of them to be zero. They demonstrate that adversarially perturbing just two points can cause the RKM algorithm to fail, leading to arbitrarily poor estimates of the cluster centers. Subsequently, they identify a condition under which the algorithm becomes robust to noise, provided the dataset is well-structured. This well-structured condition requires that at least half of the points can be partitioned into k clusters with sizes bounded both above and below, and that the distances between cluster centers are lower bounded. Additionally, the authors establish that robust k-means retains nearly all of the consistency properties of traditional k-means. They also derive optimality conditions for solving the RKM minimization problem. Finally, their experiments demonstrate that robust k-means slightly outperforms a variant, trimmed k-means, across several datasets. 
This paper addresses an important problem, as the k-means algorithm is widely used in practice. The robust k-means algorithm is a promising alternative to traditional k-means in certain scenarios, given its similar computational simplicity. Thus, a theoretical analysis of robust k-means is both relevant and valuable. The authors contribute two key results regarding the robustness of this variant: a worst-case lower bound and an upper bound under well-structured conditions. While the well-structured conditions are somewhat stringent, they have been explored in prior work, including a previous ICML paper. The consistency result for robust k-means is particularly noteworthy, as it provides additional theoretical support for this variant.