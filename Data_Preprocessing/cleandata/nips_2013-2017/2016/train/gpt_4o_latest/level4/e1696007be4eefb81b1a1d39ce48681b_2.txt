The authors propose that in high-dimensional settings where \( n \gg p \), the parameters of generalized linear models (GLMs) can be approximately estimated using a scaled ordinary least squares (OLS) estimator. They introduce an algorithm leveraging this property and demonstrate experimentally that it estimates GLM parameters more efficiently than several alternative methods across various GLMs and datasets. Furthermore, the authors provide theoretical insights suggesting that this phenomenon extends beyond simple Gaussian designs. Identifying efficient surrogates for large-scale optimization problems is a significant challenge, and the authors present an elegant approach for estimating GLMs in the \( n \gg p \) regime. While the foundational ideas for the Gaussian case are acknowledged to be pre-existing, the extension to non-Gaussian designs is both valuable and intriguing. Overall, the paper is well-written and organized effectively.  
Main feedback:  
- The results in Figure 1 should be expanded to include a plot of time versus accuracy for the two methods, similar to what is presented in Figure 2. It is evident that solving the scaled least squares (SLS) optimization problem is faster than solving the corresponding GLM (e.g., Figure 1a), but this comes at the expense of some loss in accuracy (e.g., Figure 1b).  
- The inequality in Theorem 1 does not appear to be tight. Specifically, it does not recover the result of Proposition 1 in the special case where the covariates are Gaussian. It would be beneficial for the authors to address this gap and provide further clarification.  
- It is not entirely clear when the authors use \( |S| = n \) (e.g., presumably in Figure 1) versus when \( |S| \leq n \). For instance, what was the choice of \( |S| \) in the experiments shown in Figure 2 and Table 1?  
Minor comments:  
- Line 159: "Hayley" should be corrected to "Halley."  
- In Proposition 2 and Theorem 2, the authors did not define \( \lambda_{\text{min}} \) in the main paper.