The authors establish precise criteria for \( l1 \)-sparse support recovery under small noise within the framework of non-smooth regression losses \( l\infty \) and \( l1 \). Historically, support recovery problems have primarily fixed the loss function as the Mean Squared Error (MSE, \( l2^2 \)) and investigated various non-smooth penalties, starting with Fuchs' work on the \( l1 \) penalty and culminating in the more general partly smooth penalties of Vaiter et al. In contrast, this paper retains the simple \( l1 \) penalty but shifts focus to the impact of altering the loss function. Specifically, the authors analyze two polyhedral loss functions: \( l\infty \) and \( l1 \). Rigorous proofs for both cases are provided, supplemented by experimental validation. This work offers a novel perspective on generalizing \( l1 \)-based support recovery criteria to alternative loss functions that are relevant in regression contexts: \( l\infty \) aligns with a uniform noise model, while \( l1 \) corresponds to a sparse noise model. In doing so, the paper highlights the utility of certain quantities that were previously derived in the classic MSE + \( l1 \) setting but lacked general recognition or nomenclature. This represents a meaningful advancement in understanding the underlying mechanisms of support recovery. 
The proofs presented in the main paper are mathematically sound. However, my primary concerns revolve around the paper's readability. This work was particularly challenging to review, as the authors, while rigorous, appear to a) underestimate the complexity of their contribution, which involves non-trivial convex analysis, b) refrain from offering intuitive explanations or visual aids, and c) risk alienating readers who may miss the key insights of the paper. My specific suggestions for improvement are as follows:
- Figures and Diagrams: Including a simple diagram illustrating the regression problem in 1D or 2D would be immensely helpful, especially since \( l1 \) and \( l\infty \) losses are either equal or equivalent in these settings. There is sufficient space to incorporate such a figure without removing existing content.
  
- Transitions Between General and Specific Cases: The paper initially works with general dual pairs \( \alpha, \beta \) whose harmonic mean is 1, then specializes to polyhedral losses, and later focuses on \( l\infty \) (with \( l1 \) discussed in the supplementary material). These transitions should be made more explicit. Additionally, it would be helpful to briefly outline the intuition behind maintaining general \( (\alpha, \beta) \) pairs (e.g., \( \alpha \in (1, \infty) \) is not polyhedral, so the proof differs, but here's how one might approach it and why this is beyond the scope of the current paper).
- Notation and Definitions: The first reading of the paper is challenging due to the introduction of many notations a posteriori. For instance, Table 1 introduces \( \tilde{\Phi} \) and \( S \), but their definitions appear only in subsequent lines. Similarly, the assumption \( |S| = |J| \) seems surprising at first and is deferred to Lemma 2 for justification. A brief explanation that this assumption "almost surely holds" in a probabilistic setting would provide immediate clarity before referring to the detailed discussion in Lemma 2. Additionally, formulae often introduce symbols and clarify their meanings in the following lines (e.g., "where [symbol] means [meaning]"). Reversing this order would improve readability.
- Prior Work and Context: In discussing the case where \( I \neq J \) (line 120), it would be valuable to explicitly reference Tibshirani's work on "The Lasso problem and uniqueness" and clarify how the current results relate to prior contributions. While the paper addresses something implicit in earlier works, making these connections explicit would strengthen the context.
- Convex Duality and First-Order Conditions: Line 139 introduces the bold \( r \) in regularization, linking it to \( FOr \), but this connection is not immediately clear. Explicitly introducing \( FOr \) and \( FO_\beta \) would help readers follow the argument. Furthermore, adding an intermediate step to clarify what "corresponds" means in this context would be beneficial. Since there is still unused space in the paper, the authors could also include a brief explanation of how the Fenchel dual is derived (e.g., by introducing the Lagrangian, splitting primal variables, and minimizing with respect to both). This would provide additional clarity for readers less familiar with convex duality.
- Lagrange Multipliers and Geometric Intuition: More intuition on the Lagrange multipliers \( v\beta \) (and, to a lesser extent, \( p\beta \)) would be helpful. For instance, in which space do these multipliers reside (e.g., the same as \( x \) or \( y \))? How can they be visualized geometrically? The polytope defined by the regression constraint is challenging to interpret in \( x \)-space, as it depends on which constraints are active. Since this polytope is an intersection of seminorm balls, a 2D illustration could greatly aid comprehension. While such diagrams may be difficult to construct, they would significantly enhance the paper's accessibility.
- Specific Suggestions: 
  - Line 192: Replace "So now," with "Now".
  - Line 194: Replace "hypotheses" with "hypothesis".
  - Between lines 200 and 201: Add a phrase such as "or the second line, which implies the first by operator inequality".
  - Line 202: Replace "get" with "obtain".
  - Line 207: Replace "get" with "obtain".
  - Line 248: Replace "exceeds" with "exceed".
  - Line 255: Correct "theoertical" to "theoretical".
  - Between lines 189 and 190: Should it be \( p_{1, S} \)?
In summary, while the paper makes a significant contribution to the field, its presentation could be improved to enhance readability and accessibility. Addressing the above points would make the work more approachable for a broader audience without compromising its technical rigor.