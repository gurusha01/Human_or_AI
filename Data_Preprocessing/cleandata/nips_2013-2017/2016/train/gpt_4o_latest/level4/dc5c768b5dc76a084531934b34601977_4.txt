The paper addresses a significant issue—minimizing unnecessary variations in the predictions of machine learning models—by explaining its relevance (such variations reduce usability and complicate statistical validation of model improvements) and introducing the term "prediction churn" along with a broad formal definition. It proposes a conceptually intriguing and broadly applicable fixed-point approach to mitigating prediction churn, which is loosely described as an MCMC-based technique. The method integrates regularization of new classifiers relative to prior ones with training on perturbed versions of the dataset. Iteratively applying these techniques is expected to yield a final classifier that exhibits minimal changes when retrained on a perturbed dataset, particularly under real-world drift. The paper also presents two general stability operators with hyperparameters to balance churn reduction and accuracy. While theoretical bounds on churn are provided for specific cases, the paper does not offer a comprehensive theoretical analysis of the iterative Markov chain process, such as conditions for the existence of a stationary distribution. 
The experimental results, conducted on real-world datasets and various learning algorithms, demonstrate that both the one-step and iterative Markov chain de-churning methods effectively reduce churn while maintaining or improving accuracy. However, the experiments do not highlight the significance of the fixed-point property of the Markov chain. The problem tackled by the paper is important, the proposed techniques are broadly applicable, and the experiments validate their utility. The fixed-point MCMC approach is conceptually appealing, but the lack of theoretical rigor and analysis diminishes its impact, especially since the results suggest that a single iteration may suffice. The theoretical section on the Markov chain feels underwhelming given its potential, and a more thorough analysis would have strengthened the paper's technical depth. Alternatively, focusing on the single-step stabilization operator and deriving more general theoretical results for this simpler case could have made the paper more cohesive and self-contained.
The paper's presentation lacks polish, which detracts from its clarity. For instance, in Section 5.1, the authors refer to "the RCP operator comparison" and "the MCMC operator comparison" without clarifying that the latter encompasses multiple RCP operators. Additionally, the order in which new notation is introduced is occasionally confusing (e.g., TA is defined on line 87 but is not used until later; the undefined notation PT appears on line 91). These issues in wording and structure make the paper harder to follow than necessary. Furthermore, it would have been valuable to explore connections between the proposed technique and online Bayesian learning. The Markov chain, stabilization operator, and data perturbation approach seem conceptually linked to Bayesian methods, and the omitted Markov chain analysis could have benefited from this perspective. Finally, the paper would have been stronger with a more detailed discussion or conclusion elaborating on the broader implications and potential impact of this work.