The paper investigates the existence of local modes in the log-likelihood function of Gaussian mixture models. The authors focus on the simplest possible case: isotropic Gaussian components with a known scale, under the assumption that the model is well-specified. They demonstrate that local modes can exist even with infinite data and that these modes can be arbitrarily suboptimal in terms of the log-likelihood difference between the local and global optima. The study emphasizes specific configurations, particularly those where the mixture centers are divided among well-separated clusters. Under these configurations, the authors show that random initialization almost certainly causes the EM algorithm to miss the global optimum as the number of components increases. The paper is well-written, though it contains occasional typos. I have two primary concerns regarding its practical relevance.
1. Theorem 1 establishes that the log-likelihood function can have local modes, even with infinite data. However, this result depends on a specific configuration of the centers. My question is: how likely is such a configuration to arise in practice? The paper would be more compelling if the authors made assumptions about the distribution of the centers and demonstrated that "bad" configurations become increasingly probable as the number of components (k) or the dimensionality of the problem grows. While lines 210–218 suggest that Theorem 2 addresses this issue, I am not convinced it does. A clearer statement quantifying the likelihood of "bad" configurations under certain assumptions would strengthen the paper. To clarify, I am referring to "bad" configurations of the true centers, not "bad" initializations.
2. Theorem 2 examines the role of initialization in the EM algorithm. The authors consider initializing the component centers by randomly sampling from the mixture. They show that, under specific configurations of the centers, this initialization often causes EM to fail to converge to the global optimum. However, consider a scenario where the data is generated from three Gaussians, two of which are close together and one far apart. If the same number of data points (N) is drawn from each Gaussian, then 2N points will cluster near the first two Gaussians, while N points will cluster near the third. It seems implausible that someone would initialize EM by placing two centers in the region with only N points and one center in the region with 2N points, especially when the clusters are well-separated. Is random initialization practically relevant in cases where the data exhibits clear clustering? Additionally, I am unclear about the purpose of the example on lines 74–81. It does not convincingly support Srebro's conjecture for k = 2. Could the authors clarify its intent? 
L233–235: I understand the rationale behind requirement (2), but the necessity of requirement (1) is unclear. Are the authors suggesting that if a center is initialized between two clusters, it cannot be guaranteed to move toward the correct cluster?  
L267–273: The authors prove that EM is unlikely to converge to a saddle point. From a practical perspective, why is this result significant? Is convergence to a saddle point more or less problematic than convergence to a local optimum?
Minor Points:  
- L18: "We further show gradient EM algorithm" → add "that the" before "gradient."  
- L38: "quality of the estimate" → consider changing to "estimates."  
- L60: "to characterize when it is that efficient algorithms..." → perhaps rephrase as "to characterize under which conditions efficient algorithms..."  
- L138: To clarify, does O(d^1/2)-separated mean that all densities are c-separated, with c being O(d^1/2)?  
- L172: This sentence seems redundant. Please consider rephrasing or removing it.  
- L177: "this is equivalent of sampling" → change to "equivalent to sampling."  
- L218: The term "well-separated constant" is unclear. Could the authors elaborate?  
- L220: "initialized by the the random initialization" → remove the repeated "the."  
- L225: The notation \Omega(k) is not explained. Please clarify.  
- L245: "unfortunate coincident for one single algorithm" → change "coincident" to "coincidence."  
- L249: "Recall for uniform weighted" → add "that" after "recall."  
- L308: "And claim when" → add "that" after "claim."