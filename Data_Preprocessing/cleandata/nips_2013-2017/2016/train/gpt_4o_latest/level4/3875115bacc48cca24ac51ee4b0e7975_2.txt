This paper investigates the expectation-maximization (EM) algorithm for estimating the set of mean parameters in a mixture model of Gaussian variables. The number of Gaussian components, their mixture weights, and their covariance matrices are assumed to be known and fixed. The work is theoretical in nature and establishes three negative results for the EM algorithm: 1) Even with large sample sizes, the likelihood function can exhibit local maxima with likelihood values arbitrarily smaller than the global maximum. 2) When randomly initialized, the EM algorithm converges with high probability to a critical point whose likelihood value is arbitrarily smaller than the global maximum. 3) The same negative result applies to the gradient EM algorithm. The research question tackled by the paper is both interesting and significant. To ensure rigorous proofs, the study focuses on a simplified mixture of Gaussian model where the number of components is fixed, the weights are uniform, and the covariance matrices are set to the identity. I believe these simplifications are reasonable, given the technical complexity of the proofs and the informative nature of the results. The findings are, in my view, highly valuable and contribute to a deeper understanding of the EM algorithm in practical scenarios. 
However, it should be noted that while the paper is framed as addressing a general dimension \(d\), the proofs are restricted to the case \(d = 1\). It would be helpful if the authors could clarify whether similar proofs could extend to arbitrary dimensions \(d\), and if not, briefly outline the additional challenges associated with higher dimensions. My primary concern with the paper lies in the proof of Theorem 1 (I did not consult the supplementary material for further details). I find this proof to be overly concise and would have preferred a more detailed exposition, even if it required condensing other parts of the paper. Specifically, the section of the proof from lines 288 to 306 warrants further elaboration. While I do not question the correctness of the three equations on line 288, which involve computing limits of likelihood functions at fixed parameters, I find the derivation of limits for supremums of likelihood functions over unbounded or parameter-dependent domains less straightforward. Similarly, although the authors note that the likelihood function is continuous, this does not, in my opinion, directly establish the types of convergence they claim. Additional details would enhance the reader's confidence in the validity of Theorem 1's proof.
A minor concern pertains to the discussion of the Gradient EM algorithm between lines 162 and 172. I found this section less clear than the rest of the paper. In particular, the derivation of Equation (5) could benefit from further explanation to improve its accessibility to readers.