This paper presents an end-to-end differentiable memory access mechanism that retains the representational capabilities of the original NTM while significantly improving training efficiency with very large memory sizes. It proposes numerous original ideas that demonstrate considerable technical ingenuity. The problem addressed is both well-motivated and effectively tackled. Scaling memory systems for neural networks is a highly impactful challenge, and this work introduces both the problem and its solution in a manner that is clear, concise, and thoroughly explained.