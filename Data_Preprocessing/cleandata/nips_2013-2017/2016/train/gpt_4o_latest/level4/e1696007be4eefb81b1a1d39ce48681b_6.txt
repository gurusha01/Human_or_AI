This paper introduces an efficient algorithm for solving GLMs in large-scale problems under specific random design assumptions. The algorithm achieves cubic convergence once an approximate OLS estimator is obtained, with a per-iteration computational cost of only O(n). Theoretical analysis establishes the convergence rate of the estimation error, and numerical experiments validate the findings. The approach leverages a classical idea of linking the GLM coefficient to the OLS coefficient, adapting it to modern large-scale scenarios under the assumption that the OLS estimator can be computed accurately with significantly reduced effort. The method is innovative and extends the theoretical results to non-Gaussian designs using a zero-bias transformation. Typo: cubic convergence â€“ Are you referring to Halley's method? I have a question about the subsampling step used to obtain the OLS estimator. Would it be possible to perform this step at the outset and then either compute the MLE directly or follow a similar procedure to the one proposed here? How would the results compare to the current approach? Overall, this paper is exceptionally well-written and offers numerous intriguing results and implications.