The paper introduces a novel sampling algorithm designed for planning in Markov Decision Processes (MDPs). This planning algorithm estimates the value function at the starting node of a loop-free MDP, assuming the planner has access to a generative model (i.e., the ability to sample a random next state from any given state). The authors provide a theoretical analysis of the algorithm's sample complexity, demonstrating that it matches or improves upon prior results for MDPs with finite branching factors and extends to cases where the number of possible next states is countably infinite. The paper addresses an important and engaging problem, and the results can be seen as a natural synthesis of the planning algorithm by Busoniou and Munos (2012) and the sampling method of Kearns et al. (1999). Additionally, the paper incorporates several novel techniques to make this combination effective, such as balancing confidence intervals and uncertainties across different parts of the planning tree. 
The presentation is generally clear, and the authors make an effort to provide intuition behind their algorithmic design choices. However, clarity could be further improved by explicitly noting that the MAX component of the algorithm functions as action elimination for best arm identification. Could the authors leverage existing results in this area instead of reproving everything from scratch? Furthermore, a more detailed discussion of parameter choices, such as \( kl/(1-\eta)^2 \) and \( \eta \max(Ul\epsilon) \), would be beneficial. It would also help to mention in the introduction of the algorithm that the semantics will be explained later, or alternatively, adjust the order of discussion for better flow.
On the downside, the complexity measures remain difficult to interpret, as is the case with prior work, though this may be unavoidable. Additionally, in line 45, the authors state that the algorithm can be extended to general MDPs with loops and merging states, but it is unclear how sampling results would be propagated in such cases. Previous algorithms encounter significant computational challenges when computing the optimal policy in such scenarios, and the paper does not adequately address why these challenges would not arise here. The authors should either provide a detailed explanation or remove this claim. 
Regarding the case of infinite \( N \), it is worth clarifying that this refers to countably infinite states. Could the authors explore whether the infinite \( N \) case could be reduced to the finite case by neglecting successor states with sufficiently low probabilities (relative to \( \epsilon \)), as these states are unlikely to be sampled and contribute minimally to the value function? Additionally, is the complexity measure \( d_H \) superior to what would result from this reduction?
Minor comments:  
- Line 59: Explicitly state that the goal is to obtain an \( \epsilon \)-optimal policy.  
- There are several typos throughout the paper (e.g., incorrect use of articles).  
- Page 3, footnote: Define \( \delta \).  
- Line 423: "output" should be "outputs."