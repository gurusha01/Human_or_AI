The authors present a method for generating multiple outputs by jointly training an ensemble of deep networks to minimize the oracle loss, which is particularly beneficial for user-interactive scenarios. Experiments conducted on three tasks using the oracle demonstrate the proposed method's effectiveness. This work integrates deep networks into the MCL paradigm and introduces a stochastic block gradient descent optimization technique to minimize the oracle loss in MCL. The proposed optimization approach is straightforward to implement within the back-propagation framework, enabling end-to-end training of the entire model.  
* One concern is the significance of initialization. What happens if models are initialized with random values instead of using a pre-trained network?  
* The paper does not provide sufficient details about the comparison setup. Do the baseline methods, such as sMCL, also use the same pre-trained network for initialization?  
* The authors evaluate performance using oracle accuracy, but what about the diversity among ensemble members? A more detailed analysis on this aspect would be valuable.  
* Additionally, there is prior work with a closely related idea that the authors should consider comparing against: Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David J. Crandall, Dhruv Batra, "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks," CoRR abs/1511.06314 (2015).