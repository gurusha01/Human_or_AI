The paper introduces a novel deep learning architecture tailored for the unsupervised domain adaptation problem, where labeled training data is available for the source domain but only unlabeled data exists for the target domain. The proposed approach integrates concepts from deep adaptation networks (ref. 5), residual networks (ref. 8), and entropy minimization for unsupervised learning (ref. 28) to deliver a robust and principled solution. Specifically, the architecture cleverly combines previously established ideas: leveraging MK-MMD from deep adaptation networks (ref. 5), incorporating residual network blocks (ref. 8) to connect source and target domain classifiers, and employing entropy minimization (ref. 28) to guide learning in the target domain in the absence of labeled data. This combination enables the proposed method to outperform state-of-the-art techniques on standard benchmarks. 
The integration of residual learning with an entropy minimization criterion to jointly train source and target classifiers is innovative and appears to perform well in practice. However, while the residual learning framework complements the MK-MMD approach from (ref. 5), as demonstrated in the experiments, the work feels more incremental relative to (ref. 5) than a fundamentally novel model, contrary to the authors' claims. The distinction between the use of MK-MMD in this work and its application in (ref. 5) is not entirely clear. What exactly differentiates the two approaches? Why is the proposed variant superior, and what is the underlying motivation? This aspect is not fully convincing, especially given the absence of an experimental comparison between the method in (ref. 5) and the variant proposed here.
The authors repeatedly state that the residual learning framework ensures the residual component remains small, but it is unclear whether such guarantees exist in this context. This claim seems more like an empirical observation in a different setting. Why should it hold true in the current scenario? 
Experimental results are presented for the two benchmarks used in (ref. 5) and demonstrate notable improvements over state-of-the-art methods. These results also highlight the complementarity of the MMD criterion and the residual learning approach. However, as mentioned earlier, a direct comparison with the original MK-MMD implementation in (ref. 5) would better illustrate the efficiency of the proposed variant. Furthermore, some discrepancies in reported results warrant clarification. For instance, the DAN method from (ref. 5) achieves an average accuracy of 72.9% on the Office-31 dataset, but Table 1 in this paper reports it as 70.0%, despite the experimental protocol described in Section 4.1 being identical. Similarly, benchmark methods like TCA and GFK perform significantly better here (~66%) compared to their results in (ref. 5) (~27%). This improvement might stem from the use of deep features as input in this paper, whereas (ref. 5) used raw features. Additional details on these differences would be helpful.
Overall, the paper is well-written and easy to follow. It introduces a meaningful extension to existing deep architectures for unsupervised domain adaptation. While the work is somewhat incremental, it represents a thoughtful application of the residual learning concept.