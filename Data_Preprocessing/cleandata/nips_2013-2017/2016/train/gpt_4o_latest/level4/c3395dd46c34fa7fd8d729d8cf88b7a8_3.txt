The paper introduces a novel framework called Cooperative Inverse Reinforcement Learning (CIRL) for scenarios where a human and a robot collaborate, with both agents aiming to maximize the human's reward. However, the robot initially lacks access to the human's reward function and must infer it, creating an incentive for the human to take actions that may forgo immediate rewards to provide informative signals to the robot. The proposed model is innovative and has the potential for practical applications. The work includes both theoretical and experimental evaluations, though the theoretical contributions do not appear particularly deep. The primary value of the paper lies in the realism and relevance of the conceptual model for capturing practical scenarios.
The paper would benefit from introducing a motivating example much earlier to provide better context and intuition for the importance of the problem. Currently, the first example does not appear until page 6, line 238. Including a discussion of why the example is compelling, beyond its description and analysis, would strengthen the narrative. Additionally, the authors should clarify how analyzing the example in a standard IRL framework would differ from their approach. For the example on lines 313-320, more explanation and intuition would be helpful. It is somewhat unclear what is happening in this scenarioâ€”does the human navigate a grid while the robot assists? The significant performance improvement observed when using the new best-response approach compared to the prior expert demonstration method is noteworthy and deserves further elaboration.
The discussion of prior and related work is thorough and well-done. However, the King Midas analogy could be expanded upon, as not all readers may be familiar with it. Regarding footnote 1, which mentions the possibility of multiple optimal policies, I would appreciate additional discussion on this point. In two-player general-sum games, multiple Nash equilibria can exist, each with potentially different payoffs for the players. Some justification for why Nash equilibrium is the appropriate solution concept for this setting would be valuable. Finally, in Section 4.1, I recommend avoiding the use of bold H for the human agent while using non-bold H for the time horizon, as this could be confusing. Using T for the time horizon might be a clearer alternative.