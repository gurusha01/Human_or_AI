This paper investigates the structure of local maxima in the log-likelihood function of Gaussian Mixture Models (GMMs) with equal mixing weights and identity covariance matrices. It presents three key findings in this context: 1. A counter-example is provided, demonstrating the existence of local maxima (for k = 3 and d = 1) whose likelihood can be arbitrarily worse than the global maximum. 2. The authors show that, under random initialization, the probability of avoiding convergence to a poor local maximum can be exponentially small in k when the Gaussian means are arranged in a specific configuration. 3. They also establish that the gradient EM algorithm almost surely does not converge to saddle points of the log-likelihood function.
Summary: This paper explores the structure of local maxima in the log-likelihood function of GMMs (with equal mixing weights and identity covariance). It presents three related results: 1. A counter-example is given to show the existence of local maxima (for k = 3 and d = 1) with likelihoods arbitrarily worse than the global maximum. 2. The authors demonstrate that, with random initialization, the probability of avoiding convergence to a poor local maximum can be exponentially small in k when the Gaussian means are configured in a specific way. 3. They prove that gradient EM almost surely avoids convergence to saddle points of the log-likelihood.
Technical quality: I reviewed the proofs in the main paper, particularly Theorem 1, and found them to be sound. Based on my reading of the analysis for Theorem 1, I believe the proof approach for Theorem 2 is also reasonable, though I did not verify the detailed arguments in the Appendix. All three theorems are clearly stated and well interpreted following their presentation.
Novelty & Originality: The paper connects the configuration of GMM means to the structure of local maxima in its log-likelihood function. While this observation is intuitive, I find it to be highly original. Additionally, the key insight behind Theorem 2, which builds on Theorem 1, is particularly clever: the hierarchical grouping structure of the true means governs the local maxima for k > 3.
Potential impact & Usefulness: The results have significant potential for both theoretical and practical impact. Theoretically, they could inspire further exploration of the local optima structure in related clustering problems, such as k-means. It would also be interesting to investigate whether the result in Lemma 7 extends to cases where d > 1. Practically, the findings emphasize the importance of initializing enough points in each true cluster, potentially offering insights for designing improved algorithms.
Clarity and presentation: The paper is well-written and effectively communicates its results and the intuition behind its analysis. The problem setup is clearly introduced, and the authors provide clear explanations and interpretations of their findings throughout the paper.