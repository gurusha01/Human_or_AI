The manuscript presents an algorithm designed to enhance the low-dimensional embedding of data sampled from a manifold. The authors introduce distortion loss functions, which are formulated using the push-forward Riemannian metric derived from the specified embedding coordinates. These loss functions explicitly quantify deviations from isometry. The embedding coordinates are iteratively refined to minimize the loss. It is demonstrated that the loss function is non-convex. The proposed approach, termed Riemannian Relaxation, is shown to produce embeddings with lower distortion compared to Laplacian Eigenmaps, MVU, Isomap, and HLLE on synthetic datasets. However, the authors should clarify how the loss functions quantify deviations from isometry. If the loss is zero, does this imply that the embedding achieves perfect isometry? Can a theorem and proof be provided to support this? Additionally, how can the deviation from isometry be quantified when the loss is nonzero? What specific quantitative criteria can be used to assert that an embedding is nearly isometric? Furthermore, since the loss function is non-convex, achieving a global optimum may be challenging. The algorithm may also be sensitive to the choice of initialization (the initial coordinates \( Y^0 \)). Is it essential to initialize using embedding coordinates derived from methods such as Laplacian Eigenmaps, Isomap, MVU, HLLE, or Diffusion Maps? Including a comparison of running times across different embedding methods would also be beneficial. Riemannian Relaxation appears to be computationally more intensive, and the results in Figure 2 do not show significant differences. The authors should provide stronger justification for the practical utility of this method, such as demonstrating improvements in downstream tasks like classification or regression. Overall, the paper is well-written and clearly organized. However, there are minor typographical errors, such as in Eq. (7), where the notation \( \mathcal{L}_k \) is not defined.