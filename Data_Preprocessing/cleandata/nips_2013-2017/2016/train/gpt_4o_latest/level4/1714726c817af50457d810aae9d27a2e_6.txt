This paper introduces an adaptive averaging heuristic aimed at accelerating constrained convex optimization. The proposed approach ensures that the original convergence rate is maintained even when the objective function lacks strong convexity, which represents a clear advantage over existing restarting heuristics. The authors substantiate this claim through both theoretical analysis and experimental validation. However, I found the paper challenging to follow as it falls outside my area of expertise. While I assume that the abbreviations and terminology used are familiar to the target audience, I recommend including additional background information for clarity. The equations between lines 33–34 and 46–47 would benefit from more intuitive explanations to enhance accessibility. Additionally, I am curious about the connection between the proposed method and adaptive averaging techniques employed in non-convex optimization, such as Adagrad [1] and Adam [2]. Another concern is the practical relevance of the work. The experiments were conducted in R³, which is a highly simplified setting. How well does the proposed method (and related approaches) scale to significantly higher-dimensional problems? Minor comment: The reference to the solid line in Figure 1 is unclear. Consider using a dotted line or employing color to better illustrate the trajectory of the primal variable. [1] Duchi, John, Elad Hazan, and Yoram Singer. "Adaptive subgradient methods for online learning and stochastic optimization." Journal of Machine Learning Research 12.Jul (2011): 2121-2159. [2] Kingma, Diederik, and Jimmy Ba. "Adam: A method for stochastic optimization." International Conference on Learning Representations (ICLR). (2014).