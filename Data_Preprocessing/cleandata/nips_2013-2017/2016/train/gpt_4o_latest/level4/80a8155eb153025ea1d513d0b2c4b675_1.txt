In this paper, the authors investigate the theoretical properties of the robust k-means (RKM) formulation introduced in [5,23]. They begin by analyzing the robustness property, demonstrating that when the function \( f\lambda \) is convex, a single outlier is sufficient to cause the algorithm to break down. Conversely, if \( f\lambda \) is not required to be convex, the presence of two outliers can lead to a breakdown. However, under certain structural assumptions on the non-outliers, the authors establish a non-trivial breakdown point for RKM. Subsequently, they address the consistency issue, extending known consistency results for convex \( f_\lambda \) to the non-convex case.
My primary concern with this paper is that the results seem highly specific, and I am uncertain whether they will resonate with a broader machine learning audience. The findings pertain exclusively to the RKM formulation, which, in my opinion, has not yet gained recognition as a standard method. Furthermore, the techniques employed to derive these results do not appear to be easily transferable to other methods.
I have a specific question regarding Section 3.3. As currently presented, the main result of this subsection seems to reiterate existing findings from the literature. Notably, there is no formal theorem provided in Section 3.3, and the concluding paragraph, which appears to summarize the main result, references "see also Theorem 3, Section 5 in [2]."
Regarding the consistency results, the abstract claims that the authors extend non-asymptotic results to the non-convex \( f_\lambda \) case. However, the central theorem of the paper—Theorem 2—reads more like an asymptotic convergence result. This may be a misunderstanding on my part, and I would appreciate clarification from the authors.
Lastly, concerning the simulations, I am unsure what concrete conclusions can be drawn, particularly regarding the comparison with trimmed k-means.