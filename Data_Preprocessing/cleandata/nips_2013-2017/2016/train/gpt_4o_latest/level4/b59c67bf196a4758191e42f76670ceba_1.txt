In this paper, the authors present a novel approach leveraging deep learning by integrating the concept of transductive learning into transfer learning. The experiments are conducted in a fully transductive setting. While the experimental results appear promising, there are significant concerns regarding both the proposed model and the experimental setup. Detailed comments are provided below. 
Concerns Regarding the Proposed Model:
1. In the Abstract and Introduction, the authors repeatedly emphasize that transfer learning or domain adaptation seeks to address the mismatch between training and testing data distributions to achieve robust generalization across domains or tasks. In the problem formulation, the authors explicitly state that \(\hat{x}i\) and \(xi\) follow different distributions \(ps\) and \(pt\), respectively. However, unlike existing methods such as [19] and [Pan et al., Domain adaptation via transfer component analysis, IEEE TNN, 2011], the proposed model does not explicitly minimize or align the mismatch between the training and testing distributions. There is no assurance that the learned representation will resolve the distribution mismatch. I recommend the authors carefully rephrase these claims. While the deep learning-based representation may reduce domain differences, it does not explicitly align distributions.
2. The novel aspect of the proposed approach lies in introducing a transductive step to utilize predicted labels for unlabeled target domain data, which is relatively new in the context of transfer learning. However, this idea is borrowed from the co-training technique commonly used in semi-supervised learning (transductive settings). The adaptation step, which involves learning shared and task-specific parameters, is a standard practice in deep learning-based transfer learning methods. Consequently, the proposed model can be viewed as a combination of existing techniques.
3. The method for labeling unlabeled target domain data, as described in Equation (2), is heuristic. The core idea of the Rejection Option is to incorporate a form of confidence in the predicted labels. However, the approach would be more robust if supported by theoretical analysis.
4. The paper lacks discussions on the convergence of the alternating optimization procedure.
Concerns Regarding the Experiments:
1. The performance of the proposed method is likely sensitive to the value of \(k\) in \(k\)-NN across different datasets. However, sensitivity analysis for \(k\) is missing. Furthermore, practical guidance on how to tune \(k\) is not provided.
2. Similarly, the performance of the proposed method is expected to be sensitive to the value of \(\gamma\) in the Rejection Option. Sensitivity analysis for \(\gamma\) is also missing, and the paper does not discuss practical strategies for tuning this parameter.
3. The experiments are conducted exclusively in a fully transductive setting, which is not practical in real-world scenarios. After estimating the labels of the unlabeled target domain data using the proposed model, a \(k\)-NN classifier could be applied to make predictions on out-of-sample target domain test data. It would be more compelling to include results on an out-of-sample test dataset.