This paper establishes an information-theoretic bound on the budget-fidelity tradeoff in label crowdsourcing by leveraging rate-distortion theory and the channel coding theorem. The derivation of the information-theoretic bound is rigorous and well-presented. If this is indeed the first application of rate-distortion theory and channel coding theorem to the crowdsourcing problem, it could represent a meaningful contribution to the field. However, the paper's novelty and practical significance remain debatable, as the derivation appears relatively straightforward, and the resulting bound might be overly loose. 
A key concern is that, unlike the channel coding problem—where there is significant flexibility in designing codebooks or codewords (e.g., repetition codes, Hamming codes, convolutional codes, RS codes, LDPC codes)—it is unclear whether such flexibility exists for designing the sequence of channel inputs \( u^n \). In equation (3), nearly all mappings from \( B(X)^n \) to \( U^{\sum m_i} \) (excluding degenerate ones) would need to be valid or implementable codebooks; otherwise, the channel capacity could become a loose bound. For crowdsourcing, repetition codes are feasible, with majority voting serving as a natural decoding method. However, beyond this, it is unclear how \( u^n \) can be designed to enhance error-correcting capabilities. 
A particularly notable distinction between the channel coding and crowdsourcing problems is that, in the latter, assigning the same query to the same worker multiple times violates the i.i.d. assumption in equations (1) and (2), as the same worker is likely to provide the same response to the same query. In contrast, in communication channels, the i.i.d. assumption holds, allowing the same query to be used repeatedly. This is just one example, but it highlights the likelihood of significant constraints in equation (3) for the crowdsourcing problem, unlike the channel coding scenario where the encoding mapping \( \{0,1\}^{nR} \to \{0,1\}^n \) is relatively unconstrained. Consequently, these constraints could render the proposed information-theoretic bound excessively loose. 
Additionally, it would strengthen the paper to include a comparison of the derived information-theoretic bound with the performance of state-of-the-art methods.
Minor Comments:
- In equation (8), the minimization is over \( C, C' \) with \( E[d(Z,\hat{Z})] \leq D^t \).
- The proof for Theorem 4 could be more detailed.
- In Theorem 4, since \( \ln(1-q) < 0 \), is \( \epsilon_S \) larger than \( \hat{\epsilon} \)?
- In equation (17), the rationale for \( H_M(E(\epsilon)) \) should be clarified or supported with a citation from prior work.