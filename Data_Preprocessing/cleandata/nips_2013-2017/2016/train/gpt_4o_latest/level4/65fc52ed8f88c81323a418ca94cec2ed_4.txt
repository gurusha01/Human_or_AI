This paper presents an approach to exemplar learning using deep learning techniques. Specifically, it addresses the challenge of having only a single positive example for a class alongside numerous negative examples. The authors propose a method for selecting batches for CNN training, and the experimental results demonstrate competitive performance.  
L71-L73: "However, supervised formulations for learning similarities require that the supervisory information scales quadratically for pairs of images, or cubically for triplets. This results in very large training times." => While CNNs are trained with minibatches, the training time does not necessarily scale linearly with the number of tuples or triples. Instead, it depends on the (conditional) information content in the data, which generally grows sublinearly with the size of the dataset.  
L93: "(ii) The ratio of one exemplar and many negatives is highly imbalanced, so that the softmax loss over SGD batches overfits against the negatives."  
L110-11: "Since deep learning benefits from large amounts of data and requires more than a single exemplar to avoid biased gradients" => Issues of imbalance or bias can be effectively addressed using importance-weighted gradients and/or a reweighted objective function.  
The paper is well-written overall, but it does not, in my view, provide a sufficiently clear introduction to exemplar-based learning. By the end of page 3, the exact problem being tackled by the paper remains somewhat unclear. The experiments yield promising results in the context of posture analysis and pose estimation. However, due to my limited familiarity with the problem domain and the related literature, I am unable to confidently evaluate the significance and quality of the paper's contributions.