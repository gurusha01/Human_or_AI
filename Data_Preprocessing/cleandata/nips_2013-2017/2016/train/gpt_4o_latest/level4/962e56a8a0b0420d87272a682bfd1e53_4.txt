The paper introduces a methodology and evaluation framework for identifying which ratings in a dataset are likely to have been influenced by a recommender system. The authors propose a straightforward model in which users deviate from their "true" ratings over a series of iterative steps. In each step, (a) recommendations are generated based on item similarity, and (b) users probabilistically select and "adopt" one of the recommendations, incorporating it into their set of ratings. The observed rating matrix is the result of this iterative process. The authors demonstrate that, under specific assumptions (e.g., knowledge of the adoption probability), it is possible to recover the "true" rating matrix from the "observed" ratings, making the inference process tractable. The difference between the two matrices is then used to infer which items were likely recommended rather than reflecting inherent user preferences. 
This approach is first applied to a synthetic dataset, where the authors successfully illustrate the methodology's ability to distinguish between "true" and "recommended" ratings. The same methodology is subsequently applied to real-world datasets, where its validity is supported by anecdotal evidence. For instance, no recommended ratings are detected in a dataset collected in the absence of a recommender system, while in a dataset containing both movies and TV shows, TV shows are found to be less likely to have been recommended. 
I found the proposed model and methodology to be well-conceived and appreciated the authors' efforts in tackling a challenging problem. The assumptions underlying the approach are well-motivated, and I agree that some strong assumptions are necessary to address this task. However, I would have liked to see a discussion on the practical implications of discerning "true" from "recommended" ratings. Why is solving this problem important? User preferences can be influenced by numerous external factorsâ€”does accounting for this specific influence lead to better recommendations? If so, would a recommender system, which has a complete view of the system, face the same information constraints assumed in this study?
I also have a technical question regarding the relationship between true and observed ratings, which is ultimately governed by the singular value relationship (Equation 8, or the more concise Equation 18 in the supplement). Specifically, the process appears to "boost" items with high sigma^true_i. Is there an intuitive explanation for this? How does the gap between sigma^true and sigma^obs impact the algorithm's ability to identify recommended items? Additionally, could the inference process be conducted directly in the singular value space, rather than by observing changes in ratings?
Finally, I found Section 3.2 to be somewhat abrupt. When the authors state, "We develop an algorithm," it is not immediately clear that this refers to an algorithm for classifying items as "true" or "recommended." Additionally, when discussing the validation of the Jester dataset, it would be helpful to mention earlier that Jester did not use a recommender system.