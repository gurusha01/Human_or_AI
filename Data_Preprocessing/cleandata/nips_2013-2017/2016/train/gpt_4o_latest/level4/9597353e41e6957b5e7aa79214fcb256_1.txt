This paper investigates an interactive semi-supervised clustering framework where the learning algorithm aims to minimize the k-means cost with access to an oracle capable of answering pairwise same-cluster queries (i.e., given two points, the oracle reveals whether they belong to the same cluster in the optimal k-means solution). The paper establishes three main results: (1) under a margin assumption, a randomized polynomial-time algorithm can achieve the optimal solution using k²log(n) queries; (2) even with the margin assumption, minimizing the k-means cost without queries is NP-hard; and (3) minimizing the k-means cost remains NP-hard under a weaker margin condition and with a limited number of queries, O(log(k) + log(n)). I find the interplay between the three key aspects of the problem—margin assumption, computational complexity, and query budget—particularly intriguing. It raises the question of whether a more general characterization can be established. For instance, on one extreme, irrespective of the margin, O(n²) queries always suffice to solve the problem in polynomial time, while on the other extreme, Theorem 10 demonstrates that even with a moderately large margin, the problem becomes NP-hard in the absence of queries. This is not a critique of the paper but rather an open question: could one prove a result of the form, "for a given margin γ, the problem is NP-hard if the number of queries is less than f(n, k, γ), but solvable otherwise"? 
Additionally, I am curious about the implications of oracle imperfections, which seem relevant in many practical scenarios. First, consider an oracle that occasionally makes persistent mistakes with some probability q when determining whether two points belong to the same cluster (where "persistent" means the error cannot be mitigated by resampling the query). For small values of q, is it possible to design a clustering algorithm that remains robust in this setting? Second, consider an oracle that is slightly misaligned with the optimal k-means solution, meaning it reflects a clustering with a k-means cost marginally worse than the true optimum. Can such an oracle still be leveraged to efficiently recover the optimal k-means solution? This might be feasible under the margin assumption, as a near-optimal clustering should exhibit significant alignment with the true optimal clustering. Exploring these extensions would be an exciting direction for future work.