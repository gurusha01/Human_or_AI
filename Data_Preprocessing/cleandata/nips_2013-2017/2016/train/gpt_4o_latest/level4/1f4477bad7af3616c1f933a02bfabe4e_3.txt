This paper introduces an elegant theory demonstrating that a specific class of neural networks possesses a global optimum that can be achieved with a linear convergence rate. The primary constraint is that all input data and the weights of the linear layer must be positive, and the activation functions are generalized polynomials. The proposed theory is initially proven for neural networks with a single hidden layer and is subsequently extended to networks with multiple hidden layers. Additionally, the authors propose an optimization algorithm based on the nonlinear spectral method. Experimental results indicate that the proposed algorithm performs well on a small 2D synthetic dataset and achieves reasonable performance on several small real-world datasets (UCI datasets).
Pros: As the authors claim, this is the first work to propose a potentially practical neural network with both optimality and convergence guarantees. Previous works with convergence guarantees often require complex methods to verify their preconditions, some of which are impractical as they depend on the data-generating distribution. In contrast, the theory in this work has relatively simple preconditions that depend only on the spectral radius of a nonnegative matrix composed of the network parameters. Furthermore, the authors demonstrate that their theory is applicable to networks of arbitrary depth and show reasonable results on small real-world datasets.
Cons: The proposed network has limited applicability due to its constraints. The theory requires that both the input data and network parameters be positive, the activation functions be generalized polynomials, and the spectral radius (Theorem 1) be less than 1. While the non-negativity constraint on input data may not pose a significant limitation—since most training data can be shifted to be positive—the requirement for positive network parameters could restrict the model's expressiveness. Additionally, the condition that all hidden layers must have different activation functions is somewhat unnatural and could further limit its applicability. Lastly, the authors note that satisfying the spectral radius constraint necessitates increasing \( p1 \) and \( p2 \) while decreasing \( pw \) and \( pu \), effectively reducing the weights of the linear units. If I understand correctly, as the input dimension and network parameters grow, the linear weights must decrease further to satisfy the spectral radius constraint. When the linear weights become very small, the activation functions may behave almost like linear functions. In such cases, adding more layers would merely stack additional linear functions, which would not enhance the network's expressiveness. This raises concerns about the scalability of this method to deeper architectures. Nevertheless, this work represents an encouraging initial step, and I believe the paper should be accepted.
Minor comments: I am curious about how \( p1 \), \( p2 \), \( pw \), \( pu \), the input dimension, and the size of the network parameters affect the spectral radius \( \rho(A) \). For instance, as the input dimension increases, does \( \rho(A) \) grow linearly or exponentially? To counteract this, should \( pw \) and \( pu \) decrease linearly or exponentially? While it may be challenging to prove this relationship theoretically, it might be possible to empirically plot it using synthetic data.