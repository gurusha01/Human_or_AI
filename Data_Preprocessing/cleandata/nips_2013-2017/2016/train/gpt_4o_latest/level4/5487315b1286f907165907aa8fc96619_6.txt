This paper introduces a novel formulation for two-layer models with latent structures while preserving a jointly convex training objective. The proposed formulation, which is inherently nonconvex, is addressed using semi-definite programming (SDP) relaxation. The authors demonstrate the effectiveness of their approach through superior empirical performance compared to local training methods. The paper is technically sound. However, I find the assumption of a conditional model based on an exponential family for the first layer, along with the replacement of the log-partition function, Omega, by an upper bound, to be questionable. Ultimately, the formulation is further convexified using SDP relaxations. I have a few unresolved concerns: (a) When solving the SDP, what type of solution is obtained for the original problem involving Omega? Is the solution feasible for the original problem or only for the relaxed one? (b) Does Theorem 1 guarantee that the SDP relaxation provides the exact solution to Problem (15)? If not, obtaining a relaxed solution to the optimization problem derived as an upper bound appears to be unjustified.