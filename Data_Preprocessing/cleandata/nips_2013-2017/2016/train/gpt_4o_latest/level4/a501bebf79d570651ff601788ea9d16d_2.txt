Training recurrent neural networks (RNNs) necessitates unfolding the network across time and applying backpropagation "through time" (BPTT). To perform this efficiently and avoid redundant computation of intermediate values, these values are typically stored in memory. However, as sequence lengths grow and network sizes increase, memory constraints can become a significant challenge. A common approach to address this is truncating backpropagation, which leads to approximate weight updates. Alternatively, recomputing the forward pass is an option, though it is computationally inefficient. Heuristic "divide-and-conquer" strategies have also been explored to balance memory usage and computational efficiency. In this paper, the authors propose a novel approach to identify an optimal strategy that achieves both memory efficiency and computational feasibility. Specifically, they define a cost metric based on the number of forward operations required to execute BPTT under a fixed memory budget. Using dynamic programming, they minimize this cost by determining when to store intermediate results in memory versus when to recompute them. The authors analyze three scenarios: storing the hidden state of the RNN (i.e., the output of the recurrent neurons), storing the internal state (required to compute the output), or storing both. Each scenario corresponds to a distinct cost formulation. Their method demonstrates the potential to achieve 95% memory savings while incurring only a 33% increase in computation time. They further derive theoretical bounds related to sequence length and memory constraints. The proposed method is compared against naive baselines and a handcrafted heuristic approach.
This paper is exceptionally well-written and enjoyable to read. Although the figures occupy significant space, each one is essential and enhances the paper's clarity. The use of dynamic programming to determine when to store intermediate values for minimizing forward operations under memory constraints is a straightforward yet well-articulated idea. The notations are precise, and the figures are highly informative. The selection of baselines is appropriate, including both limit cases and the previously published heuristic divide-and-conquer method. Most importantly, the paper addresses a pressing issue, given the widespread use of RNNs and the growing complexity of models and sequence lengths. In practice, many researchers resort to truncated BPTT due to memory limitations, as evidenced by numerous papers in recent years. This work offers a simple, elegant solution, complete with all the necessary implementation details, including pseudo-code, which is highly commendable.
Below are some remarks and minor issues:
- I found it unclear how the 33% increase in computation time is derived from Fig. 5a (end of p. 4).  
- Section 3.2: There is a typo in the second paragraph ("on less memory slot").  
- Figure 2b (caption): Typo in "per time step. sured in ms."  
- Section 4.2: Missing capitalization in "in figure 7" (last paragraph).  
- Supplementary material, p. 3, last line: Reference issue ("Section ??").  
- Right-hand side of Fig. 7: While the intent seems to be to show overlapping curves, these are difficult to discern.  
Additionally, including legends directly on the figures rather than in the captions would improve clarity. Enlarging the titles and axis labels would also enhance the readability of the figures.