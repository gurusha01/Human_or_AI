LSTMs and related architectures, such as GRUs, have established themselves as foundational tools in neural network research and applications involving sequential data. The central innovation of LSTMs lies in their use of gating mechanisms over memory cells, which address the "vanishing gradients" problem inherent in standard recurrent networks. This enables the network to effectively learn tasks requiring long-term dependencies across many timesteps. The paper under review introduces an extension to LSTMs by incorporating "time gates," which regulate memory updates. These gates are driven by oscillators, allowing updates to the memory only at specific points in the oscillation cycle. The periodicity and phase-shift of these gates are learned during training. A notable feature of these time gates is their ability to operate with irregular time intervals, as they are controlled by a continuous-time oscillator. 
The proposed phased LSTMs are evaluated across four experiments: classifying sinusoidal signals (a task tailored to the model's strengths), the adding task (similar to the one used in the original LSTM paper), event-based encoding of MNIST, and a lip-reading dataset. In all these tasks, the phased LSTMs demonstrate superior performance compared to standard LSTMs. LSTMs (and GRUs) are increasingly employed as fundamental components in neural network architectures, not only for inherently sequential problems but also for tasks that can be reformulated as sequential problems using mechanisms like attention. Despite their age, LSTMs remain challenging to surpass as general-purpose tools for modeling sequential structures (e.g., [1]). This paper presents a compelling approach to enhancing LSTM performance, particularly for tasks with cyclical structures. The idea is novel, and the paper provides a clear explanation of the model and its motivations. While there are areas in the analysis and experimental results that could be refined, the work is innovative and will likely attract significant interest in the field. Below, I outline several suggestions for improvement, though these do not detract significantly from the overall quality of the work, which is of a high standard.
One aspect that requires clarification is the method of time-sampling in cases where sampling is "asynchronous." While it is evident that kt can be computed for any time t, the rest of the LSTM appears to follow a discrete-time update process. For instance, two updates occurring in rapid succession when kt ~ 1 would affect ct differently than a single update during the kt ~ 1 phase. It would be valuable if the authors could provide an analysis or insights into the solutions their model learns on simpler tasks, such as the adding task or sinusoid classification. The manuscript also mentions the potential for reduced computational overhead, particularly for high-frequency signals, as the time gates eliminate the need to compute updates to the cell when k_t = 0. Empirical analysis supporting this claim, particularly at test time (where alpha = 0), would strengthen the paper. 
Though implemented differently, the proposed approach shares some conceptual similarities with reservoir computing. Both methods rely on fixed oscillators to maintain memories while learning the connections into and out of the oscillators. It would be beneficial to cite relevant work in reservoir computing and discuss these connections and distinctions. The selection of experimental comparisons is somewhat varied. While it is logical to include one of the original LSTM tasks (adding), the impact of this work would be more compelling if it were tested on a larger, standard sequential benchmark that does not inherently exhibit oscillatory structure. For instance, a language modeling or machine translation task would provide a more comprehensive evaluation of the model's utility (e.g., [3]). 
Another aspect of the model highlighted by the authors is its ability to retain memories over extended timesteps due to the gating effect of the time gates (especially when tau is large). It would be interesting to investigate whether similar behavior could be achieved in a standard LSTM by heavily biasing some forget gates toward 1 and input gates toward 0. A comparison to determine whether simple initialization tricks could replicate the results would be a valuable addition. 
[1] LSTM: A Search Space Odyssey http://arxiv.org/pdf/1503.04069v1.pdf  
[2] A clockwork RNN. http://arxiv.org/abs/1402.3511  
[3] Recurrent neural network regularization http://arxiv.org/pdf/1409.2329v5.pdf  
 After feedback   
Due to the large volume of reviews, the authors were unable to address my comments in detail. Nonetheless, my original ratings remain unchanged (as they were already high). I concur with the majority of other reviewers that this is a strong and valuable contribution to the field.