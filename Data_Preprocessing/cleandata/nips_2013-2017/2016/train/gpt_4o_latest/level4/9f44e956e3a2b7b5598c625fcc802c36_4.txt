This paper introduces a novel approach for employing a rapidly changing secondary matrix of temporary weight values, termed "fast weights," within recurrent neural networks (RNNs) to retain a trace of prior inputs and network states. The experiments presented demonstrate that integrating fast weights into RNNs enhances performance. Specifically, RNNs augmented with fast weights outperform other RNN variants with an equivalent number of recurrent units on tasks such as associative retrieval of characters from strings and image recognition (MNIST and facial expression datasets). The proposed method for incorporating fast weights into RNNs is both elegant and effective, yielding notable performance improvements in the reported experiments. Overall, the paper is well written. While there is a small body of prior work on fast weights, it appears that research in this area has not gained significant traction. This paper has the potential to change that, as fast weights should attract interest from the machine learning community. Furthermore, the biological inspiration behind the proposed method and its results may encourage further exploration in computational neuroscience and cognitive science.
Some feedback, questions, and suggestions arose during the review process: The performance gains observed in the experiments suggest that fast weights hold considerable promise. It would have been valuable to include an analysis or example illustrating the low-level effects of fast weight operations, such as a visualization of the resulting fast weight matrix—potentially in place of the description of a second image recognition task. Additionally, while the paper mentions an appendix detailing implementation specifics, this appendix was not provided for review. Regarding the Conclusion section, its content might be more appropriately framed as a Discussion section, potentially eliminating the need for a separate Conclusion section. Suggestions and comments related to the Conclusion(/Discussion) are as follows:
1) While the authors may choose to disregard this suggestion, I recommend emphasizing the specific contributions of the paper rather than broadly stating that it advances a field of study. For instance, in this paper, the contributions include the development of a method for adapting fast weights in RNNs and the experimental results demonstrating improved RNN performance. The observed performance improvements underscore the significance of these contributions to machine learning. Additionally, these contributions are relevant to computational neuroscience and cognitive science, as they provide a model and evidence suggesting that fast mechanisms of synaptic plasticity may play a role in sequential stimulus processing and working memory.
2) Line 288 contains the statement, "Layer normalization makes this kind of attention work better," which is mentioned earlier in the paper but not explicitly demonstrated. As such, this statement may not be suitable as a conclusion. A more appropriate alternative might be a reminder to the reader of the use of layer normalization in the proposed method.
3) The references to "sequence-to-sequence RNNs used in machine translation" (line 290) and "[t]he ability of people to recursively apply the very same knowledge and processing apparatus to a whole sentence and to an embedded clause within that sentence has long been used as an argument against neural networks as a model of higher-level cognitive abilities" (lines 294–297) would benefit from accompanying citations.
Finally, a few suggestions for presentation and typographical corrections are as follows:
1) The black-and-white printability of the paper could be improved by using thicker line weights for figures 1, 3, and 5, as well as employing distinct line styles or arrowheads to differentiate signals. Additionally, the font size in the plots of figure 5 could be increased for better readability.
2) Line 203: Remove the word "their." On page 8, several opening quotation marks are incorrectly formatted as closing quotation marks. Line 287: The word "unis" appears to be a typographical error and should likely be replaced with "units." 
3) References [8] and [25] contain initialisms that should be capitalized.