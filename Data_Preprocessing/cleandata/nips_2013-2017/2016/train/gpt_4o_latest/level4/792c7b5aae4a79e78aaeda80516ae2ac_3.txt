The authors establish the global convergence of the EM algorithm for a Gaussian mixture model where the covariance matrices and mixing proportions are assumed to be known in advance. While the overall presentation is reasonably clear, I found it difficult to discern the primary contribution or "selling point" of the paper. Although I am not a specialist in this domain, the paper did not sufficiently convey the practical significance of its findings. Models 1 and 2, for instance, can be estimated relatively easily with large sample sizes using methods such as the method of moments (MOM) or MOM combined with a one-step estimator. It seems plausible that MOM, being sqrt(n)-consistent, could also serve as a suitable pilot estimator for applying the techniques outlined in Balakrishnan et al. (2014). The paper might still warrant acceptance if its proof techniques were sufficiently innovative or had the potential to be generalized to more challenging models. However, in that case, the proof techniques should have been at least heuristically outlined in the main text rather than relegated to a lengthy technical supplement with minimal intuitive explanation. Additionally, I struggled to see why these particular models were chosen as meaningful test cases for the EM algorithm. For example, in Model 1, the population likelihood function has only two local maxima (the global maxima at theta and -theta) and a single saddle point. Given the general properties of the EM algorithm as a majorization-maximization method, convergence to one of the global maxima appears almost inevitable.