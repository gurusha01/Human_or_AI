The paper examines a crowdsourcing framework where a taskmaster decomposes a large task (e.g., an inference problem) into smaller subtasks. These subtasks are then assigned to a group of workers, who complete them and report their findings back to the crowdsourcer. The crowdsourcer aggregates the information provided by the workers and processes it to solve the original inference problem. This framework accounts for workers having varying skill levels and being potentially unreliable in their responses. The overarching goal of the system is to achieve a certain level of fidelity or "goodness" in the final outcome at the crowdsourcer, even when some workers provide incorrect information. To achieve this, the taskmaster must carefully design the subtasks (or queries) allocated to the workers. 
The paper defines the total number of queries assigned to workers as the system's budget. It investigates a fundamental trade-off between the budget (i.e., the number of queries) and the fidelity (i.e., the quality of the inference task performed by the crowdsourcer). This trade-off is analyzed by mapping the described crowdsourcing system to the problem of joint source-channel coding in the information theory literature. In this mapping, the taskmaster is treated as a transmitter, the crowdsourcer as a receiver/decoder, and the unreliable communication links between workers and the crowdsourcer as a communication channel. Based on this mapping, a lower bound on the budget (i.e., the number of queries) required to achieve a desired fidelity level is derived using standard results from information theory. This bound is independent of the specific query design scheme employed by the taskmaster or the algorithm used by the crowdsourcer to process the workers' information. 
The authors then propose a specific query design scheme at the taskmaster, referred to as k-ary incidence coding, and analyze its performance. They compare the performance of this scheme against the derived information-theoretic lower bound for a specific channel model. The reviewer finds the analogy between the rate-distortion framework and the crowdsourcing system with unreliable workers to be intriguing. This work has the potential to inspire further research into crowdsourcing systems using information-theoretic tools. However, the reviewer is uncertain about the extent to which the results presented in the paper accurately model real-world crowdsourcing systems. This may stem from the authors' effort to closely align the crowdsourcing problem with the point-to-point communication problem, enabling the direct application of existing information-theoretic results with minimal adaptation.
The reviewer suggests that the authors address the following concerns:  
1) The paper appears to merge worker reliability and skill level into a single channel model. It assumes that the taskmaster allocates the same number of queries to all workers, regardless of their skill levels. However, in real-world systems, workers often carry ratings that reflect their skill levels, and task allocation may vary accordingly.  
2) The realism of the channel models used to represent worker unreliability is unclear. For instance, how would the framework account for workers who possess some knowledge of the task but intentionally act adversarially to degrade fidelity?  
Additionally, the numerical results section could benefit from clearer explanations. The labeling of curves in Fig.~2 should be improved for better readability. For example, what does the case of $k=1$ represent? Does it imply that the worker directly provides the correct label for the presented query, as it would not be possible to present a pair and ask if they share the same label in this case? 
The reviewer also notes the following issues:  
- On page 6, line 246, the authors assume knowledge of $P(B(X))$. This assumption should have been stated earlier in the paper.  
- On page 4, line 135: "Figure 1.a" should be corrected to "Figure 1.b," and "Figure 1.b" should be corrected to "Figure 1.c."  
- The authors frequently use terms such as "Theorem," "Lemma," and "Corollary" without attaching specific numbers to them. The reviewer suggests using lowercase forms (e.g., "theorem," "lemma," "corollary") in such cases for clarity.