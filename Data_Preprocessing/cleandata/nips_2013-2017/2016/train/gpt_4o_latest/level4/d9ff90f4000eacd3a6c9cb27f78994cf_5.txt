A persistent challenge with recurrent neural networks is the issue of vanishing and exploding gradients, which hinders their ability to learn long sequences. To address this, recent research has introduced unitary hidden-to-hidden weight matrices. However, the unitary matrices proposed in prior work are inherently constrained in their representational capacity for high-dimensional spaces. This paper introduces the concept of "Givens capacity" to quantify the representational power of such unitary matrices. Additionally, the authors propose a modified stochastic gradient descent algorithm that enables the training of full-capacity unitary matrices. These full-capacity matrices offer greater representational power compared to previously proposed designs, while maintaining their unitary properties, thereby mitigating the issues of vanishing and exploding gradients during training. The paper tackles a significant problem in the domain of recurrent neural networks. It is well-written, technically robust, and theoretically grounded, presenting methods that advance the current state-of-the-art. While the use of unitary matrices in recurrent neural networks remains a relatively novel approach, and their practical applicability compared to traditional architectures has yet to be thoroughly explored, the paper provides promising theoretical and empirical results. These findings suggest that unitary weight matrices could potentially be applied across a wide range of applications.