This paper introduces a novel method for training recurrent neural networks (RNNs) with unitary transition matrices without constraining the set of unitary matrices that can be learned. The authors offer theoretical motivation for their approach by leveraging Givens operators (akin to Givens rotations) to assess the representational capacity of a specific class of unitary matrices. Using this framework, they demonstrate that the original parameterization of uRNNs cannot represent the entire unitary group for \(N \times N\) matrices when \(N > 22\). To address this limitation, the authors propose a gradient descent optimization technique that operates directly on the Stiefel manifold of unitary matrices. They validate their method on several tasks, showing that it achieves results that are at least comparable to, and in some cases significantly better than, those obtained with the original uRNN parameterization and LSTMs.
Overall, I find this to be a strong paper, offering both theoretical and empirical contributions. The theoretical insights and the proposed optimization algorithm are particularly compelling, while the empirical results, though solid, could benefit from deeper analysis. This work serves as a meaningful extension of the original uRNN paper. With the exception of a few minor issues, the paper is well-written, and I found the discussion of Givens operators and their role in quantifying the representational capacity of unitary matrices to be clear, even though the topic was new to me. The proofs in the supplementary material appear sound and are relatively straightforward. However, the experimental section is less convincing, as it primarily involves synthetic tasks or real data in somewhat artificial setups. While I do not take issue with the results themselves, the paper occasionally left me unclear about the precise experimental designs and the interpretation of the findings.
For example, in line 206, the authors state that they examined matrix dimensionalities \(N\) below, at, and above the critical dimension of 22. However, the value 22 is not actually included among the dimensions tested. Additionally, I would have expected a sharp increase in the performance gap between the restricted-capacity uRNN and the full-capacity uRNN when \(N\) exceeds 22, but the performance differences seem more pronounced for lower-dimensional problems. While I understand that comparing results across different dimensionalities is challenging due to the use of MSE scores, a discussion addressing the absence of a sudden breakdown in the restricted model for \(N > 22\) would strengthen the paper.
In lines 211â€“212, the authors mention reporting the best test results over 100 epochs for several random initializations. While this practice of early stopping on test data is suboptimal, I suspect it does not significantly affect the results. Nonetheless, providing details on the exact number of initializations used and including standard errors or deviations in Table 2 would enhance the robustness of this section. Regarding the "copy memory" task, the restricted uRNN yields negative results. From the original uRNN paper, I know that the restricted variant can solve the task for \(T = 500\) but fails for \(T = 1000\) when trained by the authors of this paper. While I am not suggesting a lack of effort in optimizing the restricted model, evidence of a thorough hyperparameter search for this baseline would have been reassuring. That said, the results for the proposed method on the more challenging version of the task are impressive and highlight its practical potential.
The speech task, however, feels somewhat artificial. Predicting the next frame of speech features is not a common application, except perhaps in training generative speech models. I am not convinced that this task demands an RNN to capture long-term dependencies, as neighboring spectrogram frames with small window sizes are typically highly correlated. The results on this task are somewhat surprising. The restricted uRNN performs unexpectedly well in terms of MSE for up to 70 hidden units, while the full-capacity models consistently outperform the restricted variant on other metrics. I would not claim that the full-capacity model decisively outperforms the restricted version here. Additionally, it would have been interesting to evaluate the performance of larger RNNs with 500 hidden units. Nevertheless, this task still provides evidence that the proposed optimization method is effective and clearly outperforms LSTMs at this scale.
While I have raised several concerns about the quality of the individual experiments, I believe that, taken together, they provide a reasonable evaluation of the proposed methods. However, the paper would benefit from an improved discussion of the results and additional details to facilitate replication.