This paper establishes the lower bound on the number of attributes required to achieve specific precision levels when regression/classification models with various loss functions are restricted to accessing a limited set of attributes. The manuscript is well-written, and the theoretical proofs presented in both the main text and the appendix appear rigorous and sound. Nonetheless, the inclusion of supportive experimental results would enhance the paper's overall persuasiveness. Additionally, I believe the appendix could benefit from a brief explanation of the general proof strategies, such as why the disjointness of sets S1 and S2 is key to proving the theorems. Furthermore, in the proofs, the elements of vector x are constrained to be either 0 or 1. I am curious about how the results might change if x is instead assumed to lie within the range [0, 1]^d.