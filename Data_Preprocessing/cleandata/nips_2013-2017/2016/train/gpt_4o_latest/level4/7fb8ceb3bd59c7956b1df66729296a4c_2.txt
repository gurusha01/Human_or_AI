The paper investigates the problem of matrix completion. Non-convex optimization methods for this task are widely used and perform effectively in practice. Recent advances have demonstrated that many methods in this category can converge to a global optimum, provided they are carefully initialized. The primary objective of this paper is to establish that, for a regularized formulation of the problem, all local minima are also global minima. Consequently, any optimization algorithm that converges to a local minimum will necessarily converge to a global one. Notably, this property holds for several widely-used algorithms, such as SGD, even when initialized randomly or arbitrarily. Additionally, the results presented in the paper are robust to noise. Similar results have been established for a few other problems, such as dictionary learning, but these problems are qualitatively distinct from matrix completion.
A brief overview of the (minor) limitations of the results: the analysis is restricted to symmetric matrices, leaving the asymmetric case as an open problem. Furthermore, it is specific to the scenario where the objective function being minimized is the Frobenius distance to the observed matrix entries. The results rely on standard assumptions, including that the condition number of the matrix is bounded and that no single row dominates the Frobenius norm of the matrix. 
The analysis itself is intriguing. It leverages the first- and second-order optimality conditions that any local minimum \(x\) must satisfy to demonstrate that \(x\) must coincide with a global minimum. Specifically, the second-order condition ensures that \(x\) has a large \(L2\) norm, while the first-order condition guarantees that \(x\) has a small \(L\infty\) norm. Together, these conditions imply that \(x\) is (approximately) a global minimum. 
Overall, this is a valuable contribution to an important and extensively studied problem. The results provide theoretical support for the empirical observation that popular non-convex methods for matrix completion perform well in practice, even with minimal effort in initialization. The analysis is compelling and tailored to the unique characteristics of the matrix completion problem. I believe the submission is well above the acceptance threshold for NeurIPS. One area that could benefit from further elaboration is a discussion of the specific properties of the regularizer that the analysis depends on or exploits. Additionally, how does the proposed regularizer compare to those commonly used in practice?