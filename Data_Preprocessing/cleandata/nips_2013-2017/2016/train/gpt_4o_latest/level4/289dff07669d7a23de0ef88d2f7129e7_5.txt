The manuscript introduces a novel update scheme for CMA-ES, termed Cholesky-CMA-ES, which approximates the original covariance matrix adaptation (with computational complexity O(d^3)) using a new approach that operates with complexity O(μd^2), where d represents the dimensionality of the optimization problem and μ denotes the number of samples per iteration (typically O(log(d))). The core idea behind this update is to maintain and update a lower-triangular Cholesky matrix instead of a full covariance matrix (and its corresponding square root). The key difference between Cholesky-CMA-ES and the original CMA-ES lies in the omission of estimating a (random) orthogonal matrix Et at each step. Under fairly restrictive assumptions—specifically, the convergence of a sequence of covariance matrices to an unknown target covariance matrix—the authors provide an asymptotic argument that the sequence of Et converges to the identity matrix, thereby making the two schemes equivalent. The utility and general speed-up of the Cholesky-CMA-ES are demonstrated through standard runtime and convergence plots on benchmark objective functions, as well as comparisons with two state-of-the-art CMA-ES variants that delay the full covariance matrix update through alternative mechanisms. 
This paper offers a meaningful contribution to accelerating the celebrated CMA-ES algorithm, widely regarded as one of the best methods for non-convex black-box optimization, particularly when little is known about the objective function's topology and when first- or higher-order information is either inaccessible or unhelpful. The proposed method achieves significant speed-ups without compromising optimization performance on synthetic test cases. However, a notable limitation of any speed-up proposal for CMA-ES is that, in practice, the overall optimization process is often dominated by the runtime of the black-box oracle (i.e., the objective function). Consequently, the runtime of the CMA-ES algorithm itself is frequently negligible in comparison. Furthermore, in typical use cases, the dimensionality of the problems rarely exceeds a few tens of variables. The targeted scenario here—where a single function evaluation takes milliseconds or less and the dimensionality is high—is therefore somewhat niche and may be less common in real-world applications.
Given the challenges of deriving theoretical guarantees for a complex scheme like CMA-ES, I suggest a few additional empirical experiments and visualizations to further bolster the results. First, it would be insightful to observe the evolution of Et along selected optimization trajectories, potentially using different norms. An experiment where the full CMA-ES is run alongside the proposed approximation, allowing for a direct comparison of Et over time, could provide valuable empirical evidence regarding its fluctuations. Second, for completeness, it would be helpful to include runtime and performance comparisons with the original CMA-ES (without delay strategies) in this study. Third, while the test functions used are standard in the evolutionary optimization community, I recommend a simulation study on quadratic functions where the spectrum of quadratic forms is continuously varied (e.g., as in Stich and Mueller, PPSN 2012). This would allow for an analysis of solution quality and E_t error as a function of the spectral distribution. Lastly, a real-world experiment to complement the simulation results would strengthen the practical relevance of the work. For instance, applying the method to policy gradient optimization in real-time robotics—where speed is critical and parameter spaces are not excessively large—could serve as an illustrative example.