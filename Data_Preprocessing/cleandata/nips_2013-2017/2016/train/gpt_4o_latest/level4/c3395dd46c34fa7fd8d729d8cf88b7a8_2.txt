The paper introduces a comprehensive framework for cooperative interaction between humans and robots, which integrates established paradigms such as inverse reinforcement learning (IRL) and optimal teaching. The authors propose that this framework can be formulated as a POMDP problem and delve into how apprentice learning fits within this structure. A key insight derived from this work is that if the human understands the robot employs IRL, it might be more advantageous for her to demonstrate a "best response" to the robot's strategy rather than an optimal policy. Additionally, the paper presents an approximation method for scenarios where rewards are expressed as linear combinations of state features and where the human provides only a single trajectory. Experimental results in a navigation task further validate this insight.
My assessment of the paper is somewhat mixed. On the positive side, the paper is generally well-written, and I appreciate the unifying framework and the novel insight it offers. However, the exposition could benefit from greater precision and rigor. Some technical explanations and proofs lack clarity and feel somewhat informal. Furthermore, the theoretical contributions appear relatively straightforward. Below are my specific comments:
- The use of "Nash equilibrium," a concept rooted in non-cooperative game theory, seems misaligned with the cooperative nature of the proposed framework. A more suitable terminology might be considered.
- It is unclear why both agents are assumed to know the probability distribution over \(\theta\). Does the human not simply select \(\theta\)? Since the model represents the robot's perspective, perhaps \(P_0\) could be interpreted as the robot's prior belief.
- The authors should provide a formal definition of the coordination POMDP to enhance clarity. For example, there are inconsistencies between the explanations in the main paper and the supplementary material, particularly regarding the POMDP states (e.g., lines 212 and 288 in the paper versus Definition 2 in the supplementary material).
- In Section 4.2, does the IRL approach also assume only one demonstrated trajectory? If so, could the experimental results be influenced by this assumption?
Additional comments on presentation and minor issues:
- Line 88: Should the histories be defined as \(S \times (A^H \times A^R \times S)^*\)?
- Line 250: "it gets" could be rephrased for clarity.
- Line 270: "gives use the ability" should be corrected to "gives the ability."
- Line 272: While obvious, the term "br" is not formally defined.
- Line 279: Remark 3 does not seem directly related to the preceding content.
- Line 287: "a POMDP" should be revised for grammatical consistency.
- Line 321: The placement of Figure 1 on page 3 feels awkward.
- Line 326: "The robot reward function" should be revised to "The reward function."
- Line 407: "Kand" should be corrected to "K, and."
In summary, while the paper offers a compelling framework and valuable insights, it would benefit from improved precision, more rigorous technical explanations, and clearer definitions. Addressing these issues would significantly enhance the paper's overall quality and impact.