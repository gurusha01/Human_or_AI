This paper introduces an exploration strategy for deep reinforcement learning (RL). Specifically, the authors derive a 'pseudo-count' from a sequential density model of the state space in the Arcade Learning Environment (ALE) and assert an explicit relationship between information gain, prediction gain, and the proposed pseudo-count metric. The paper demonstrates notable advancements on ALE, particularly achieving significant progress in Montezuma's Revenge. Exploration remains a critical challenge in RL, and the authors revisit established ideas while deriving new insights to advance the state of the art. Overall, this is a strong paper with compelling experimental results, though I have a few points that require further clarification:
- The pseudo-count is derived from the recoding probability of a density model and is shown to align with the empirical count. For instance, consider a specific state element \(x\) associated with a pseudo-count \(N(x)\). Updating the density model with this element results in an updated pseudo-count \(N'(x)\), which remains consistent with the empirical count. However, what happens to \(N(x)\) when the density model is updated with a different element \(y\)? My intuition suggests that \(N(x)\) could decrease, which would contradict the empirical count. This issue appears related to the upward and downward spikes observed in the pseudo-count in Fig. 1 (right).
- I find the concept of pseudo-count somewhat redundant in the context of this work. It seems that the authors could directly compute prediction gain (PG) and then derive information gain (IG) without introducing pseudo-counts. IG is a well-established metric for intrinsic motivation. My question is: why rely on pseudo-counts? They appear to provide a loose approximation of PG/IG, potentially making them less stable. If the pseudo-count metric were removed from the paper, the approach would essentially involve measuring the magnitude of updates to the density model and using this as an intrinsic reward.
- The authors' definition of IG seems to diverge from the standard usage in the literature (e.g., "Planning to Be Surprised" by Sun et al., 2011). Specifically, it appears that an agent could accumulate infinite information over time by following a fixed policy without genuinely exploring. This interpretation seems problematic. If the authors' IG metric differs from the conventional definition, how do they relate it to the established notion? Alternatively, if the authors propose a novel IG metric, what is the added value of their contribution regarding the relationship between IG and pseudo-counts?
Minor comments:
- Corollary 1 is presented in a dense manner and could benefit from additional clarification.
- In Fig. 1 (left), the pseudo-count increases during periods without salient events. Is this due to generalization across states?