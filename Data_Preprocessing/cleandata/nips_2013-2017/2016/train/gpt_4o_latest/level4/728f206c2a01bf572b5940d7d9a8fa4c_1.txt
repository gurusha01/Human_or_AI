The paper introduces a method for training RBMs using the gamma-smoothed Wasserstein distance \( W{\gamma}(\hat{p}, p{\theta}) \) (eqs 3 and 4), where \( \hat{p} \) represents the data distribution and \( p{\theta} \) denotes the model. The authors derive the gradients of \( W{\gamma}(\hat{p}, p{\theta}) \) and optimize the model via gradient descent. However, the Gaussian shrinkage example in section 4.3 (alongside the results in section 4.2) raises significant concerns about the consistency of Wasserstein-based training. This issue MUST be thoroughly investigated before the paper can be deemed suitable for publication. The authors reference [2], which claims statistical consistency, but section 4.3 surprisingly reports non-zero shrinkage (even for \( \gamma = 0 \)) when modeling a simple \( N(0, I) \) distribution with \( N(0, \sigma^2 I) \). Why does this occur? A lack of consistency would represent a critical flaw in the formulation of a statistical learning criterion. Additionally, in section 3 (Stability and KL regularization), the authors argue that regularization with respect to the KL divergence is necessary when learning from samples (\( \hat{p}{\theta} \)), which undermines the "purity" of the smoothed Wasserstein objective function. 
The experiments (section 4) are conducted on MNIST-small (the 0s from MNIST), a subset of the UCI PLANTS dataset, and 28-dimensional binary codes (MNIST code). The reported results (lines 168-172 and Figs 3 and 4) suggest the generation of "compact and contiguous regions that are prototypical of real spread, but are less diverse than the data." This observation reinforces my earlier concern (from the Gaussian example in section 4.3) that minimizing the (smoothed) Wasserstein distance does not yield a consistent density estimator. In section 4.4, the authors attempt to frame these weaknesses as strengths in the context of data completion or denoising. 
Overall: The authors should be commended for exploring an intriguing alternative to traditional KL-based training of RBMs. However, the Gaussian shrinkage example in section 4.3 raises serious concerns about the consistency of Wasserstein-based training. This issue MUST be carefully addressed before the paper can be considered for publication.
Additional points:  
- Title: The title is overly broad. The paper specifically focuses on training restricted Boltzmann Machines.  
- Introduction, line 7: RBMs are not restricted to binary \( x \)'s. See, for example, Exponential Family Harmoniums (https://papers.nips.cc/paper/2672-exponential-family-harmoniums-with-an-application-to-information-retrieval.pdf).  
- Figure 5: The figure would be more readable if different colors were used for the various \( \gamma \) values. Additionally, the last entry should be labeled \( \gamma = 0 \) (not OpenCV). The use of OpenCV can be clarified in the text.  
---
COMMENTS POST REBUTTAL  
After reviewing the authors' rebuttal and the feedback from other reviewers, I am willing to withdraw my claim of a "fatal flaw," as the small sample size in the experiments in section 4.3 partially explains the observed issues. However, I remain concerned. For the KL-based version of the Gaussian experiment (fitting a Gaussian with covariance matrix \( \theta^2 I \) to \( n = 100 \) samples drawn from a 10-dimensional Gaussian), the maximum likelihood estimates of \( \theta \) consistently fall within the range 0.95-1.05 (over approximately 10 repetitions). In contrast, for the Wasserstein (\( \gamma = 0 \)) estimator, \( \theta \) appears to converge to around 0.65 (as estimated visually from Fig. 5, left). The convergence rate provided by the authors, \( O(n^{-1/(10+1)}) \), for this simple problem is alarmingly slow and, in general, scales as \( O(n^{-1/(D+1)}) \) for \( D \)-dimensional data. This raises serious concerns about the practical applicability of the proposed method. A detailed study of this simple Gaussian case, showing how \( \theta \) minimizing \( W_0 \) varies with \( n \), would be valuable. The convergence rate suggests that the process will be painfully slow. Can the authors explain the source of the shrinkage bias in such a case?  
Regarding the authors' response to Reviewer 5 (lines 177-178): "the entropy regularizer contributes to the clustering effect [...]. It also appears for the true W distance in Fig. 5." This suggests that using a KL matching criterion \( KL(\hat{p} || p_{\theta}) \) with an entropy penalty (without Wasserstein) would also lead to clustering. Section 4.4 extols the virtues of this clustering effect for data completion and denoising. However, since KL regularization is already applied in section 3, this clustering effect is likely present in many of the experiments regardless. 
I also agree with other reviewers who noted poor clarity in the technical explanations and suggested the use of supplementary materials to address this.  
In conclusion, I am not strongly in favor of accepting this paper (though I could tolerate its acceptance). While the work contains interesting ideas, the source of the clustering effect (whether from the Wasserstein distance or the entropic prior) must be more clearly articulated and disentangled. I suspect that some claims attributed to Wasserstein training are actually due to the entropic prior. This distinction should be clarified by comparing the proposed method against a KL + entropic prior baseline.