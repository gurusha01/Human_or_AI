The paper introduces a novel RNN architecture, where the recurrent matrix is constrained to be unitary (uRNN) and is optimized across the entire space of unitary matrices. The authors also demonstrate that a previously proposed optimization method for unitary RNNs was not "full-capacity," meaning it did not encompass the entire space of unitary matrices. The capacity analysis is conducted using the Givens decomposition.  
Novelty: The use of unitary matrices for RNNs (uRNN) has recently garnered attention. This work tackles an important question: how to optimize a unitary matrix over its entire space. The proposed optimization approach, which operates on the Stiefel manifold, is a straightforward yet effective method to achieve this goal.  
Impact: It remains uncertain whether the research community will adopt unitary matrices as a practical alternative to LSTMs and whether this technique will ultimately prove superior to previously proposed methods. As such, the broader impact of this work is currently unclear.  
Clarity: The introduction and the analysis leveraging the Givens decomposition are well-written and persuasive. However, Section (4), which briefly outlines the optimization method, could be improved by including equation (1) from [16] and providing a concise explanation of why gradient descent on manifolds is performed in the tangent space.  
Technical quality:  
- The authors effectively demonstrate the superiority of their method over LSTMs in the experimental section.  
- A concern with the comparison to the prior uRNN method is that the authors match the size of the matrices rather than the number of trainable parameters (which theoretically also correlates with memory usage and runtime). This raises the question of how the results in Section 5 would differ if the number of trainable parameters were matched instead.  
- In Section 5.2, the MSE results appear to favor the restricted-capacity uRNN rather than the proposed model. To address this, the authors introduce "voice quality" measures, which highlight the superior performance of their model. However, this metric was not the original optimization objective. A potential solution could involve optimizing for MSE in the mel-space instead of the linear frequency space, as the former is known to correlate more strongly with voice quality.  
Minor comments:  
- Line 80: Please clarify that P is chosen randomly and remains fixed throughout training.  
- Line 101: There is an extra "define."  
- Regarding Theorem 3.1, please specify whether the bound is tight; otherwise, it is unclear if a full-capacity matrix exists.