The paper introduces a novel approach for performing back-propagation through time (BPTT) to train recurrent neural networks (RNNs) on machines with significantly less operating memory than typically required by the standard unfolding-in-time method. The proposed technique leverages dynamic programming to determine the optimal strategy for storing internal states during network unfolding. The authors investigate multiple storage strategies, including storing the entire internal state, storing only the hidden unit activations, or using a combination of both. Computational experiments reveal that memory usage can be reduced by up to 95%, with only a modest increase in computational overhead (approximately 33%). This method has strong potential to facilitate the training of RNNs on long sequences using GPUs with constrained memory. I believe this work could make a substantial contribution by enabling BPTT to be applied to significantly longer sequences. However, to strengthen this claim, it would be helpful to include a concrete motivating example where a specific hardware setup (e.g., a particular GPU model) exhausts memory despite having sufficient computational capacity. Without such evidence, it remains unclear whether memory, rather than computational resources, would be the primary bottleneck for long sequences. A minor issue: On page 7, in the third sentence of Section 4, there is a typoâ€”"orward" should be corrected to "forward."