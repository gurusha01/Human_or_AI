This paper introduces conditional generative moment matching networks (CGMMNs), which extend GMMNs to applications involving conditional generation and prediction. The core innovation lies in leveraging the kernel embedding of conditional distributions and the conditional MMD metric to measure discrepancies between conditional distributions. The proposed CGMMN framework is evaluated on tasks such as classification, conditional generation, and Bayesian model distillation. A straightforward extension of GMMNs to the conditional setting involves estimating a GMMN for each conditional distribution, where all these distributions share parameters through a common neural network. However, this naive approach suffers from data sparsity issues, as each conditional distribution typically has very few examples, and in continuous conditioning domains, there may be only a single example per conditional distribution. To address this, the proposed method treats all conditional distributions as a unified family and directly aligns the model with the conditional embedding operator, rather than matching individual conditional distributions. 
The benefits of the proposed approach are evident, though there are scenarios where the naive method could still perform reasonably wellâ€”for instance, in conditional generation tasks where the conditioning variable assumes one of 10 discrete values, as in the MNIST dataset. Comparing the proposed method against such a naive baseline would provide valuable insights. It is commendable that the conditional MMD objective is elegantly formulated and can be computed in a manner similar to the unconditional MMD, as highlighted in Remark 1. Notably, all the \(K\) matrices in Equation 2 are independent of the model parameters, allowing backpropagation to proceed as usual.
One concern with the proposed approach is that the CMMD objective may require large minibatches to mitigate the impact of stochasticity, but this comes at the cost of computational expense. Specifically, the matrix inversions involved could become prohibitively expensive for large minibatches. Despite this, the CMMD framework offers a compelling method for modeling conditional distributions. The experimental results on classification and Bayesian model distillation are also intriguing. However, the clarity of presentation in Sections 2.3 and 3.1 could be improved, as understanding these sections required consulting the referenced papers for additional context. 
There are also a few minor issues: (1) Line 77 incorrectly states that the MMD estimator is unbiased, which is not accurate. (2) On Line 90, the definition of the conditional embedding operator is inconsistent with the one provided in reference [29]. To align with the form used in this paper, it appears that a different definition of the cross-covariance operator is required.