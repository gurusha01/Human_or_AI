This paper introduces two significant modifications to the deep embedding pipeline: (1) The proposed PDDM incorporates absolute feature position information to learn a similarity metric that adapts to the local feature structure. (2) The double-header hinge loss employs hard quadruplet mining to explicitly model and discriminate target similarity distributions. Experimental results demonstrate that the proposed approach achieves faster convergence, improved feature embedding, and enhanced generalization capability. (1) The paper is well-motivated. A global Euclidean metric may be suboptimal in heterogeneous spaces, potentially leading to misleading hard sample mining and, consequently, subpar deep embedding learning. Drawing inspiration from prior work [Xiong 2012], the authors propose leveraging feature position information to learn a locally adaptive similarity metric, which facilitates the selection of genuinely hard samples and guides deep embedding learning. While the novelty is not groundbreaking, the approach appears reasonable and worth investigating. (2) The paper is well-structured and clearly presented. The network is thoughtfully designed to incorporate additional feature position information, and the loss function explicitly enforces a hard quadruplet mining process. Technical details and hyper-parameters are thoroughly documented to ensure reproducibility. The experimental results are compelling, showing improvements over prior methods that do not utilize feature position information.