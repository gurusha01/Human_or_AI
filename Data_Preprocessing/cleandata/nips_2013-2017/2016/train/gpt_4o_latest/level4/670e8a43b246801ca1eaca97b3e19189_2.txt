The paper introduces a novel distribution over partitions of integers designed to support micro-clustering. Specifically, it demonstrates that existing infinitely exchangeable clustering models, such as the Dirichlet Process (DP) and Pitman-Yor Process (PYP) mixture models, exhibit cluster sizes that grow linearly with the number of data points \( N \). For applications like entity resolution, the paper defines the micro-clustering property, where the ratio of cluster size to \( N \) approaches 0 as \( N \) grows to infinity. To address this, the paper proposes a general framework for distributions over partitions of integers that satisfies the micro-clustering property. This is achieved by first sampling the number of clusters from a distribution supported on positive integers, followed by explicitly sampling each cluster size from another distribution over positive integers. The proposed framework achieves the micro-clustering property by sacrificing marginal consistency while retaining exchangeability. Two specific instances of this framework are presented: the first employs the negative binomial distribution for both the number of clusters and cluster sizes, while the second uses a Dirichlet distribution with an infinite-dimensional base distribution for cluster sizes, offering greater flexibility for large datasets. Reseating algorithms analogous to the Chinese Restaurant Process and Pitman-Yor Process are derived for both models, and sampling-based inference algorithms leveraging exchangeability are introduced. Experiments on four semi-synthetic datasets demonstrate that the proposed models outperform models lacking the micro-clustering property (e.g., DP and PYP) in the entity resolution task.
The paper has several notable strengths:  
+ It identifies and formalizes an important property of cluster sizes—micro-clustering—that existing infinitely exchangeable clustering models fail to satisfy. This property has potential applications in various domains, including entity resolution.  
+ It introduces a general framework for constructing infinitely exchangeable clustering models that satisfy the micro-clustering property and provides a detailed analysis of why the DP mixture model fails to meet this criterion. Two specific and compelling instances of this framework are proposed, complete with reseating algorithms.  
+ The experimental results on semi-synthetic datasets provide some evidence of the utility of the proposed models for entity resolution. Additionally, experiments in the supplementary material confirm that the proposed models satisfy the micro-clustering property. (This analysis should be moved to the main paper.)
However, the paper also has several shortcomings, some of which are likely addressable:  
- The paper claims to preserve exchangeability while sacrificing marginal consistency to achieve the micro-clustering property. While the benefits of exchangeability (e.g., for sampling-based inference) are evident, the implications of sacrificing consistency are unclear. The statement on line 109—"It does not produce an exact sample if used to incrementally construct a partition"—is vague. How does this relate to the consistency issue?  
- Although the paper empirically compares the proposed models to the Pitman-Yor Process, it lacks a theoretical analysis of the differences in the reseating algorithms. The PYP achieves heavier-tailed cluster size distributions by making the probability of creating a new cluster proportional to the current number of clusters. The reseating algorithms for the proposed models appear similar. What specific aspect of the proposed reseating mechanism enables it to satisfy the micro-clustering property, unlike the PYP?  
- The relationship between the two proposed models is not thoroughly analyzed. The first model uses the negative binomial distribution, while the second replaces it with a Dirichlet distribution with an infinite-dimensional base. Could the cluster size probabilities in the second model be interpreted as drawn from a GEM/Poisson-Dirichlet distribution? Would this lead to similar restrictions? Or is the difference attributable to the choice of priors over infinite-dimensional multinomial parameters (e.g., negative binomial vs. GEM)?  
- The paper briefly mentions related work (the Uniform Process) that achieves micro-clustering by sacrificing exchangeability instead of consistency but does not revisit this comparison later. A discussion of this prior work is necessary to contextualize the contributions of the proposed framework.  
- The experiments are insufficiently convincing in demonstrating the utility of the proposed models for entity resolution. For the first two datasets, the task appears too simple, as indicated by the low error rates across all models. For the other two datasets, which are described as more challenging, the baseline models seem to perform better.  
Additional, less critical comments:  
- In lines 131–133, the reseating algorithm is missing the parameter \( \alpha \).  
- If the Chaperones algorithm is significant, it should be described at least at a high level in the main paper, rather than being mentioned only by name.  
- The paper's structure could be improved for clarity. For instance, lines 112–115 and 134–139 could be moved to the inference section, while lines 116–118 and 140–144 could be relocated to the experiments section.  
- The name of the proposed model could be reconsidered. A more concise name, such as "Flexible Micro-clustering Model," would be preferable.  
- In Eqn 3 and line 85, \( |C|N \) should be \( |CN| \). In line 84, "cluster \( |c| \)" should be "cluster \( c \)."  
- Eqn 5 should include \( a, q \) in the conditional, and Eqn 7 should include \( a, q, \alpha \).  
- Line 122's phrase "K is one observation" is unclear and should be clarified.  
- In Eqn 11, clarify that \( \theta_{lk} \) represents the \( l \)-th feature for the \( k \)-th partition.  
- For the synthetic datasets, are the experimental results based on a single random draw or averaged over multiple samples? If only a single draw is used, high noise could occur even if the noise parameters are set to be low in expectation.  
- What distinguishes the two Syria datasets?  
- How are error rates defined to account for label permutation issues? Are they computed over pairs of data points?  
- Some sentences contain unnecessary repetition, such as in lines 150 and 214–216.