This paper introduces an unsupervised algorithm for training deep neural networks. It builds upon exemplar-based methods for similarity learning while addressing several inherent limitations of such approaches (lines 91-101). The proposed algorithm alternates between two key steps: (1) partitioning the dataset into batches of compact cliques, and (2) optimizing the neural network using a softmax loss applied to pseudo clique labels. Experiments conducted on posture analysis and pose estimation tasks demonstrate impressive performance, which, in some cases, rivals that of fully supervised methods. The proposed approach is technically robust. By employing "offline" batch/clique data partitioning, it effectively mitigates the challenges associated with traditional exemplar-based methods. This strength is evident from the compelling experimental results presented in the paper. However, my primary concern is whether this alternating strategy might reduce the efficiency or automation of the learning process, particularly if additional hyperparameter tuning is required across iterations or epochs. I would appreciate further discussion on these aspects, as well as insights into how the network's performance evolves over iterations or epochs. The presentation could also be improved in certain areas, such as: - Clarifying the definitions of f() and r() in Eq. (7). - (line 93) Elaborating on the highly imbalanced ratio between one exemplar and many negatives. - (line 88) Providing more detail on learning similarities between all pairs of a large number of exemplars.