The paper introduces a method for bi-level modeling with the structure input → latent → output, where the latent layer typically represents a discrete (potentially combinatorial) structure. This framework is relevant to problems such as transliteration in NLP, where the latent representation corresponds to bipartite matching. The primary objective of the paper is to reformulate the learning problem as a convex optimization problem that can be solved efficiently, assuming that any linear maximization over the latent space constraints (via the polar operator) can also be efficiently addressed. The authors rightly highlight that this problem is conventionally approached as a bi-level optimization task: first selecting the latent layer conditioned on the input, followed by predicting the output conditioned on the latent layer. In this work, the authors reformulate the problem by relaxing the discrete latent variables into continuous ones. Leveraging duality principles, the paper ultimately derives a semidefinite programming (SDP) formulation. With suitable relaxations, the SDP is convexified, enabling efficient solutions. 
The paper tackles a meaningful and well-motivated problem, and the proposed approach appears generally sound. However, the technical derivations could benefit from clearer exposition. Specifically, the derivations on page (4) would be significantly improved if the authors provided a roadmap explaining the purpose and direction of the steps being taken. Additionally, I have some concerns regarding certain claims that require clarification: 
1) Equation 19 involves maximization with respect to \(\pi \in S\). However, the function being maximized is quadratic in \(\pi\). How is this handled, given that the assumption is only that linear functions over \(S\) are tractable? 
2) The authors appear to incorrectly assert that the rank relaxation is the sole relaxation introduced. In fact, the latent variable \(y\) is also relaxed from a discrete space to a continuous one with the inclusion of a perturbed term, which constitutes a critical relaxation assumption. Please provide clarification on these points.