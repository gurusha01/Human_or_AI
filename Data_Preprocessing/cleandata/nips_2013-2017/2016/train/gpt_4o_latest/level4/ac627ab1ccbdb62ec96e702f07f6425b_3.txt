The paper introduces a method for domain adaptation in deep neural networks based on the following key components: 1) leveraging multi-kernel maximum mean discrepancy to improve feature similarity between the source and target domains, 2) utilizing prediction entropy to adapt the classifier to the target domain, and 3) the primary contribution: modeling the source domain classifier as a residual function of the target domain classifier. Overall, the proposed technique demonstrates promising results and includes an extensive comparison with related work. The paper is engaging and has the potential to become a strong contribution if the experimental evaluation is further refined. My main concern is that the paper does not adequately isolate or evaluate the impact of the residual function. It remains unclear whether this residual block is truly necessary. For instance, I suspect that simple L2 regularization could achieve a similar effect by enforcing similarity between the source and target classifiers. Furthermore, Figure 2 is unconvincing, as I do not observe a qualitative difference between the predictions of DAN and RTN. Additionally, the mixing of notations for H and F (i.e., what constitutes the residual versus the original function) was initially confusing. Adopting a single, consistent notation throughout the paper would improve clarity.