Most neural network models enhanced with memory tend to scale inefficiently in terms of both space and time as memory size increases. In this paper, the authors introduce a memory access mechanism termed Sparse Access Memory, demonstrating that it preserves representational capacity while achieving efficient training, being 1000 times faster and 3000 times more memory-efficient. However, I find the technical contributions insufficiently robust for acceptance. The primary technical components—efficient backpropagation and approximate nearest neighbor search via sparse access memory—are relatively straightforward. Consequently, despite the intriguing experimental results, I am not inclined to recommend acceptance.