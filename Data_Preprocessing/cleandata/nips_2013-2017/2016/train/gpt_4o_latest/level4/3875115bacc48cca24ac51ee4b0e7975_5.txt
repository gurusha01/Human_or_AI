The paper addresses the open question posed by Srebro in his 2007 work, "Are there local maxima in the infinite-sample likelihood of Gaussian mixture estimation?" The authors provide a negative answer to this question by constructing a broad class of counterexamples. The paper begins by outlining the mechanisms through which likelihood is maximized in Gaussian Mixture Models, focusing on the EM and gradient EM algorithms. The authors also discuss the role of initialization and present a commonly used choice. They then introduce their main theorems, which establish the counterexamples to Srebro's [2007] question. Furthermore, the authors demonstrate that, even with the standard random initialization, the EM and gradient EM algorithms converge to suboptimal maxima with exponentially high probability. To provide intuition for their theoretical results, the authors prove a simpler case (k=3, d=1). 
The paper is exceptionally clear and well-written, striking a good balance between adhering to space constraints and offering intuition for their theoretical findings. The question tackled by the authors is highly significant for advancing our understanding of challenges in non-convex optimization, even in the asymptotic regime for simple distributions. While I did not thoroughly examine the supplementary material, it would be helpful to include estimates for the constants in the probability term they provide. This is particularly relevant because it seems unlikely that k would be excessively large in practical scenarios. Such estimates would clarify when the convergence to suboptimal solutions begins to manifest in the asymptotic regime (i.e., identifying the phase transition point). If the constants are too complex or impractical to compute, supporting simulations would be valuable to confirm that this phenomenon can be observed empirically. Nevertheless, this limitation does not diminish the novelty or impact of the paper.