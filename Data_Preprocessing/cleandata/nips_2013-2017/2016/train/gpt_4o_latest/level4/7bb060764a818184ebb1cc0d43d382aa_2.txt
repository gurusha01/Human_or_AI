This paper introduces a novel dropout mechanism that samples features or neurons based on a multinomial distribution, assigning distinct probabilities to different features or neurons. For shallow learning, the authors propose a data-dependent dropout approach that leverages the second-order statistics of features to determine sampling probabilities. Theoretical analysis of the risk bound is provided. For deep learning, the paper introduces an evolutional dropout method, which computes sampling probabilities for each layer using the second-order statistics of the layer's output derived from a mini-batch of examples, thereby reducing computational overhead. The proposed distribution-dependent dropout is compared experimentally with standard dropout for both shallow and deep learning, as well as with batch normalization.
TECHNICAL QUALITY
In Section 1 (lines 31-33), it is suggested that features with low or zero variance can be dropped more frequently or entirely. However, it is unclear how this intuition is supported by the theoretical analysis in Section 4, particularly in Equations (8) and (9), given that the features are not automatically zero-meaned. Additionally, the uniform dropout scheme described in line 135 (a special case of multinomial dropout with equal sampling probabilities) is similar but not identical to standard dropout, as the sampling probabilities for different features are not independent and identically distributed. Using this scheme in experiments as if it were standard dropout may not be appropriate.
There are also concerns regarding the experimental design. While the paper emphasizes that its focus is not on achieving better prediction performance by exploring different network architectures or optimization tricks (lines 265-266), its goal of improving standard dropout naturally calls for comparisons with other advanced dropout methods. Baselines such as the adaptive dropout method by Ba and Frey (NIPS 2013) and the variational dropout method by Kingma et al. [6], which are not cited or included, should be considered in the comparative study. Furthermore, the experiments only evaluate a single dropout rate (0.5). Reporting results for multiple dropout rates would provide a more comprehensive evaluation. For shallow learning, it would also be helpful to include a setting for s-dropout applied to data after Z-normalization, as the remark in Section 4.1 (lines 213-217) suggests that its performance would be similar to that of d-dropout.
In Section 5.3, the paper concludes that e-dropout is essentially a randomized version of batch normalization combined with standard dropout, implying that combining e-dropout with batch normalization might not yield further performance gains. Including this combination in the experiments would strengthen this claim. Additionally, Figure 2 (right) shows that as the number of iterations increases, the test accuracy of e-dropout fluctuates more than other methods, including BN+dropout, which is also a randomized scheme. This suggests that e-dropout may exhibit slightly lower learning stability compared to BN+dropout, and this observation warrants further discussion. Lastly, the performance gap between s-dropout and e-dropout is not consistently large, and for MNIST, e-dropout appears to underperform compared to s-dropout. These observations merit additional analysis.
NOVELTY
The primary theoretical contribution of the paper is Theorem 1, which is derived using standard techniques for stochastic gradient descent. The extension from shallow to deep learning is relatively straightforward, as it involves using mini-batches instead of the full dataset to compute sampling probabilities for each layer independently. However, no theoretical guarantees are provided for the deep learning case. Despite this limitation, the proposed data-dependent dropout method appears to be the first of its kind with theoretical justification, albeit only for shallow learning.
IMPACT
The experimental results indicate that the performance gap between s-dropout and e-dropout is not consistently significant, and the improvement achieved by distribution-dependent dropout may diminish for deeper networks. The paper only presents small-scale experiments for deep learning, which raises questions about the potential impact of this work on the broader deep learning community, especially for applications requiring much deeper architectures.
CLARITY & PRESENTATION
The paper is generally well-structured and readable, but the writing could be improved. In addition to issues with English usage, there are several language and formatting errors. Examples include:
- L22-23: "at random samples neurons and sets their outputs to be zeros"
- L32: "can dropout"
- L58: "acts similar to"
- L67: "a faster convergence"
- L73: "reminder"
- L80,85: Errors in LaTeX citation usage
- L86: "to speed-up"
- L93-94: "developed evolutional dropout"
- L101: "in next section"
- L111: "where he operator"
- L135: "we refer as"
- L167-168: "a non-convex optimization"
- L172: "does not loss the generality"
- L179: "upper bond"
- L181: "The detailed proof of theorem"
- L185-186: "the second term also depend on"
- L191: "included in supplement"
Addressing these issues would enhance the overall clarity and presentation of the paper.