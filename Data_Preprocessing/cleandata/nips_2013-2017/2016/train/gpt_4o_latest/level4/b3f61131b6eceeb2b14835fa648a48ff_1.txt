The paper conducts an analysis of a broad class of learning algorithms in smooth games, showcasing fast convergence. This represents an intriguing contribution to the field of game-theoretical control—while traditional results establish convergence, the focus on convergence rates is relatively novel. The authors effectively integrate techniques from learning theory with results grounded in the price of anarchy for smooth games. These findings are compelling and relevant to the game-theoretical control community. As someone with expertise in both game theory and learning theory, I found the paper engaging. The results are innovative, significant, and hold promise for advancing the field of game-theoretical control. The paper covers a substantial amount of material, including fast learning under both full information and bandit feedback, results in expectation and with high probability, as well as analogous findings in dynamic population games. However, this breadth is also the paper's primary shortcoming. The dense presentation leaves the work under-motivated, with insufficiently defined terms, making it challenging to bridge the specialized knowledge required to appreciate the work with the broader audience of a general conference like NIPS.  
Post-rebuttal: In light of my concerns about the paper's dense content and your (reasonable) rebuttal, I suggest revising the introduction to better emphasize that your primary contribution lies in demonstrating the benefits of considering low approximate regret, along with examples illustrating its significance. This would help clarify the paper's focus—I had (mistakenly, as it turns out) interpreted the main contribution to be the demonstration of fast convergence for common learning algorithms in games. A potential title change might also help better reflect this emphasis.