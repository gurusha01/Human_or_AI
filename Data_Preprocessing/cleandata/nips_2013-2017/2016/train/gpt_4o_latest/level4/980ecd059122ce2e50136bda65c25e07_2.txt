The authors introduce two metrics for evaluating the robustness of neural networks using an approximating algorithm for a robustness criterion originally proposed by Szegedy et al. To assess the performance of their proposed methods, they conduct experiments on the MNIST and CIFAR-10 datasets. Additionally, the authors demonstrate how their approach can be leveraged to enhance the robustness of neural networks. Their method relies on identifying the nearest adversarial example (i.e., an example with a different label) based on the L_infinity distance. The proposed metrics are defined using the fraction of examples within a specified distance and their average distance. While these metrics appear reasonable, the complete CDF of adversarial distances provides more comprehensive information. (The inclusion of these CDFs in the experiments is a valuable addition.) However, the assumption that the space is entirely linear is quite strong and not sufficiently justified. Although adversarial examples have been linked to the linear behavior of networks, limiting the search to regions where the network is analytically linear may be problematic. A network could exhibit approximately linear behavior over a much larger region than where it is strictly linear. The most compelling result is that the authors successfully identify numerous adversarial examples that a prior baseline approach failed to detect. Thus, while restricting the analysis to the linear regions of a neural network may seem limiting, it still offers improved insights and measurements of the network's vulnerabilities. The results on CIFAR-10 are noteworthy, though predominantly negative. After fine-tuning for accuracy, the robust network incurs 10% more errors than before and remains vulnerable to adversarial attacks on approximately 60% of the examples, requiring an average of 4 pixel changes (up from 3).  
Minor issues:  
- Numeric citations could be improved by using author names as nouns instead of reference numbers in phrases like: "[6] takes the approach of [20]."  
- The title and abstract suggest a method applicable to any neural network, but the approach is specific to ReLUs. A revised title or at least a revised abstract would be more accurate.  
- The introduction requires restructuring. Specifically, it should provide more background from references 5 and 20 early on, such as justifying the choice of the L_inf norm for robustness. Additionally, the parameter rho is not defined before being mentioned. The current organization makes the paper difficult to follow, forcing readers to repeatedly refer back to those references to understand the content.  
- Typo: "when when" should be corrected to "when."