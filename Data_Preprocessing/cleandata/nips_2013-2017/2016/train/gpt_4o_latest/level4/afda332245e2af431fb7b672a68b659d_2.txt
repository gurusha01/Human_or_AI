This paper explores how the concept of state-action visitation counts can be extended to reinforcement learning (RL) scenarios with large state spaces, where traditional visitation counts become too sparse to be practically informative. The core idea involves leveraging density estimation to compute pseudo-counts, which are shown to be equivalent to actual counts. The authors provide asymptotic analysis to substantiate this equivalence. The utility of these pseudo-counts, derived via density estimation (CTS), is demonstrated by incorporating them into two RL algorithms (DQN and A3C) to enable count-based exploration, and evaluating their performance across various games in the ALE environment. Overall, this is an excellent paper that has the potential to benefit many (deep) RL algorithms operating in large state spaces. However, I have a few comments regarding the pseudo-count approach: (1) At ICML this year, several exploration algorithms were introduced that leverage model uncertainty for exploration. Using model uncertainty appears to be a simpler and more intuitive approach compared to employing an external density model. While this issue is briefly mentioned in the future directions section, how does pseudo-count-based exploration compare to these model uncertainty-based methods? Under what circumstances is it preferable to use pseudo-counts? (2) How does the proposed density-based counting method compare to a much simpler counting scheme, such as partitioning the screen into large regions and using color indicators? (3) In Equation (5), should N(x) be replaced with N_a(x) to ensure that the bonus depends on both states and actions?