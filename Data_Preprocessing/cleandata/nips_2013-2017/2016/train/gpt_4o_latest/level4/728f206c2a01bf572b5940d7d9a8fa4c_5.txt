This paper introduces the use of Wasserstein distance as an alternative to the commonly-used KL divergence for the cost function of RBMs. The authors demonstrate the advantages of Wasserstein distance through illustrative toy examples and promising experimental results. I find the direction of applying Wasserstein distance to machine learning compelling and appreciate the benefits of the proposed Wasserstein RBM, which accounts for the metric structure of {0,1}^dâ€”a consideration absent in classical RBMs. 
However, my primary critique pertains to the writing. The paper is not self-contained, with numerous references to external works, making it challenging for readers unfamiliar with Wasserstein distance to follow without consulting references [4][5]. It appears that the authors did not invest sufficient effort in improving readability or broadening the paper's accessibility. For instance, it would be helpful to introduce the optimal transport problem in the RBM context after equation 3, followed by the dual problem and the optimal dual variable, alpha^\star. In its current form, the derivation of alpha^\star is unclear. Similarly, in equation 6, directing readers to [5] without explaining the expression and significance of alpha^\star(\tilde{x}_n) is unhelpful. I recommend revising section 2 to make it more self-contained and intuitive. To conserve space, the authors could move the proofs to the supplementary material and streamline section 3.
As an unsupervised learning method, the proposed Wasserstein RBM introduces three additional hyper-parameters: gamma in equation 3, and lambda and eta at the end of page 4. The need to tune these hyper-parameters limits the practicality of the method. As the authors note, "the proposed procedures can be seen as fine-tuning a standard RBM." However, it remains unclear whether lambda and eta are essential because they improve the energy surface for optimization or because their absence results in a fundamentally different cost function with trivial solutions. Clarifying this point would strengthen the paper.
The Wasserstein distance is well-defined over the parameter manifold, including its boundaries, unlike KL divergence. This distinction has implications for denoising tasks. For example, if a specific input dimension predominantly contains 0's with only a few 1's, a Wasserstein RBM could eliminate noise and make the dimension deterministic, whereas a KL-based RBM might fail to do so. Do the experimental results reveal similar observations? Highlighting this potential advantage could further bolster the paper.
Minor comments: The title could be misleading, as it suggests an improvement to the training of classical RBMs, whereas the method introduces a fundamentally different cost function and learning approach. In equation 4, the summation would be more readable if expressed over (x, x') instead of (xx'). In line 91, the centering of the dual potential is not immediately intuitive and requires additional explanation. Lastly, the beginning of section 3 overlaps with section 1 and could be streamlined for conciseness.