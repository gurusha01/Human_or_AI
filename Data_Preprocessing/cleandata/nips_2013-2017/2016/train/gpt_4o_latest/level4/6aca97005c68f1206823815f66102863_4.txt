This paper introduces a novel approach for parameter inference. The authors frame the problem as follows: given a set of observed variables, x, and a set of underlying parameters, theta, the goal is to recover the posterior distribution p(theta|x). It is assumed that while we can sample from p(x|theta), its explicit form is unavailable. Additionally, a prior distribution p(theta) over the parameters is provided. The paper highlights that traditional methods for addressing this problem often approximate p(x=x0|theta) by p(||x-x0|| < epsilon|theta) and employ sampling techniques such as MCMC. However, these methods only approximate the true distribution as epsilon approaches 0, which simultaneously causes computational complexity to grow unboundedly.
The proposed solution involves training a neural network to directly learn p(theta|x), renormalized by a known ratio of pt(theta) to p(theta), as detailed later in the paper. The neural network outputs the parameters for a Gaussian mixture model. The training process involves the following steps: first, a distribution pt(theta) is selected for sampling. A batch of N points is then sampled from pt(theta), and these points are passed through the sampler to generate corresponding x values. The network is trained to predict p(x|theta) given theta as input. The choice of pt(theta) is crucial for convergence speed, and the authors propose an adaptive method: start with the prior p(theta) and iteratively refine pt(theta) using the current model as training progresses.
The method is evaluated on multiple datasets, demonstrating strong performance and better convergence compared to MCMC and basic rejection sampling methods. The paper is well-written, and the proposed approach appears sound. Several related works are discussed toward the end of the paper (though it is worth noting that most papers typically present related work earlier). While the authors explain the distinctions between their method and these related approaches, direct comparisons with many of them are absent. Including such comparisons on at least one problem would strengthen the paper.