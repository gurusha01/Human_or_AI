In this paper, the authors propose a tree-structured reinforcement learning (Tree-RL) approach to sequentially search for object candidates, aiming to capture the interdependency among different objects. Starting with an entire image, the agent employs a tree-structured traversing scheme to sequentially discover multiple objects. At each tree node, the agent simultaneously performs scaling and local translation actions, navigating along multiple near-optimal search paths learned through a deep Q-learning framework. The experimental section is designed to evaluate three aspects: a comparison between tree search and single-path search, a comparison of the proposed algorithm with several object proposal algorithms, and a comparison of object detection frameworks with and without the proposed object proposals. 
- As a preprocessing step, object proposal algorithms are expected to be time-efficient. While algorithms [3,4,25,10,5] in Figure 5 report their runtime, this paper lacks time measurements or complexity analysis, despite the authors' claim that their model is computationally efficient. The authors should compare their algorithm with others in terms of time efficiency.  
- The qualitative results in Figure 6 could be expanded. All three examples provided contain two main objects. Including qualitative comparisons of their algorithm with other object proposal methods for cases where images contain only one main object or more than two objects would be beneficial.  
- The motivations and justifications for their specific tree structure are unclear. 1) Why are translation and scaling actions assigned to the left and right child nodes, respectively? Why were alternative variations not considered? 2) Could introducing randomization in the node split improve generalization or offer other advantages?  
- The technical contribution and novelty of the paper appear marginal compared to [18]. Aside from Section 3.2 (Tree-structured Search), the content in Section 3, including the definitions of actions, states, rewards, and the optimization method (Deep Q-learning), closely resembles [18,22]. Another distinction from [18], as noted by the authors, is that [18] runs the entire process multiple times to detect objects, whereas this paper performs the process sequentially in a single pass. This distinction could be clarified further in the experiments through comparisons in terms of time efficiency and additional qualitative and quantitative results.