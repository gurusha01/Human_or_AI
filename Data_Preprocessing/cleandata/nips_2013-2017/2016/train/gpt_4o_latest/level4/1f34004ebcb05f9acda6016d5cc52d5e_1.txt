This paper investigates the convergence properties of a non-convex loss-minimization problem aimed at learning the parameters of a general graph-based ranking model. The model is defined by a random walk governed by the weights of nodes and edges, which themselves are determined by random walks based on the features of nodes and edges. The optimization problem under consideration cannot be addressed using existing methods that require exact evaluations of the objective function. To tackle this, the authors propose a two-level approach. At the first level, they employ a linearly convergent method to approximate the stationary distribution of the Markov random walk. This method is validated, and the authors demonstrate that the value of the loss function can be approximated to any desired precision. Additionally, they develop a gradient-based method for general constrained non-convex optimization problems using an inexact oracle and establish its convergence to a stationary point of the problem. The key contribution lies in adapting the proposed approach to constrained optimization scenarios where the function value can only be computed with a known precision. The authors prove the convergence of this method and integrate it into the second level of their algorithm. The paper is well-written, and the proofs appear to be correct, although I did not verify all of them in detail. My main concern is that the optimization techniques discussed are largely adaptations of those introduced in "Yurii Nesterov and Vladimir Spokoiny, Random Gradient-Free Minimization of Convex Functions, Foundations of Computational Mathematics, 2015, pp. 1â€“40," and the supervised PageRank algorithm has also been previously proposed. On a minor note, the paper would benefit from a conclusion, as there is sufficient space to include one. I would also recommend presenting the optimization problem in a more general context, framing the studied algorithm as a specific instance. This would help better situate the contribution within the broader state-of-the-art.