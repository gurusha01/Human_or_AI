The paper establishes lower bounds for the problem of learning with missing attributes. In this problem, the learner is provided with examples (x, y) drawn from a distribution over R^d Ã— R. The objective is to identify a linear classifier that minimizes a specific loss function (e.g., square loss or hinge loss). The unique aspect of this framework is that the learner cannot access all features; instead, for each example, it can select at most k features to observe. This setting was first proposed by Ben-David and Dichterman in 1998, and since then, algorithms have been developed for the square loss, even in the case where k=2. However, for other loss functions, no algorithms are known for fixed k. This paper demonstrates that such algorithms cannot exist. Specifically, it is shown that for the square loss, learning is impossible when k=1 (indicating that k=2 is tight). For the absolute and hinge losses, it is proven that learning is impossible for any fixed k. The proofs rely on constructing two distributions that cannot be distinguished under the restricted access to features. While the proof for the square loss is relatively straightforward, the arguments for the absolute and hinge losses are more intricate. Overall, the paper offers a meaningful contribution to this area of research. Therefore, I recommend its acceptance.