This paper presents a novel memory access mechanism termed Sparse Access Memory (SAM) within the framework of memory-augmented neural network architectures. The research area is relatively new, originating with the development of Neural Turing Machines (NTMs). This augmentation enables neural networks to surpass the limitations of traditional LSTM models. The primary contribution of the paper is the restriction of all memory reads and writes to a fixed size, thereby operating on a subset of memory words rather than an unbounded memory size. The authors provide both theoretical validation of their methodology and evidence of its optimality. As demonstrated in section 4.1, this approach is likely to yield improved read and write performance due to the reduced memory size. Additionally, the authors account for the learning costs associated with introducing sparsity, as detailed in section 4.2, using standard NTM tasks: 1) Copy, 2) Associative Recall, and 3) Priority Sort. The combined costs are shown to be lower than those of NTMs for certain tasks (specifically tasks 2 and 3). A notable strength of the paper is the inclusion of a real-world example in section 4.4. The benchmarking is conducted using Torch7. The proposed idea is both straightforward and innovative, with significant speedup compared to NTMs, which is particularly impressive. The observation that sparse reads and writes, facilitated by either a linear model or an ANN, can enhance early-stage learning in specific scenarios (section 4.2) is especially intriguing. It would be beneficial if the authors could further elaborate on and explore how this approach could be generalized to other cases, identifying conditions under which learning improvements occur, or by evaluating SAM with alternative ANN architectures. Overall, the paper is well-written, with all assumptions clearly stated and the architecture thoroughly detailed in the appendix. It is an engaging and insightful read.