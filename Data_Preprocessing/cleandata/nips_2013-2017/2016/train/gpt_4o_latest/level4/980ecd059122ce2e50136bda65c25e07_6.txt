The paper introduces a novel approach for generating adversarial examples for neural networks (NNs) that does not rely on the direction of the signed gradient (of incorrect labels). The proposed method involves encoding the NN as a set of constraints and solving a linear programming (LP) problem, where the feasible solution yields both an adversarial example and a robustness measure. Additionally, the authors formalize the concept of robustness and introduce two new statistics to quantify it. 
1) Regarding the equation between lines 212-213: it seems the conditions have been reversed.  
2) The authors claim that current robustness estimations are biased because they are based on data generated by the algorithm being evaluated. However, their proposed method does not appear to differ significantly in this regard, as they still rely on their own algorithm to compute ρ.  
3) Their assertion that their approach is less biased hinges on the fact that it identifies more adversarial examples. This claim requires further substantiation.  
4) I believe the proposed algorithm retains bias due to its deterministic handling of disjunction constraints—always removing the constraint not satisfied by the seed. Could the authors explore a stochastic approach for selecting between disjunctions?  
5) The paper is challenging to follow, as symbols and equations are insufficiently explained. For instance, in line 147: what is the dimensionality of x*? How does (.)_i correspond to rows or columns?  
6) In line 64, I believe the correct phrasing should be "x with true label l."