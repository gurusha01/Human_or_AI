This paper introduces a novel approach to training Boltzmann machines by leveraging the Wasserstein distance between data and model distributions instead of the traditional log-likelihood objective. The idea is innovative and well-articulated, with unsupervised training objectives of this nature holding significant potential impact. However, I have reservations regarding the scalability of the method, as it relies on sampling, which may perform poorly with increasing dimensionality and sample size. The experiments were conducted on unusually small datasets, heavily regularized using standard KL divergence, and demonstrated improvements solely in terms of Wasserstein-like performance metrics.
- Equation 4: The summation should be over "x, x'" rather than "xx'." I found this equation particularly challenging to interpret. It appears to be defined only for γ > 0, and the role of the last term in inducing the correct dependencies is unclear. Additional explanatory text and a revision of the stated γ bound would enhance clarity.  
- Line 67: The term α* has not been defined.  
- Lines 92-93: I am concerned that the method of measuring the distance between two distributions using samples may encounter the same limitations as Parzen window estimates (refer to Section 3.5 in L. Theis, A. van den Oord, and M. Bethge, A note on the evaluation of generative models, 2016). Specifically, the number of samples required grows exponentially with the dimensionality of the space, and with insufficient samples, the model may exhibit a strong bias toward the mean of the data distribution.  
- Line 115: Does this imply that the method is unsuitable for minibatch training?  
- Lines 121-122: The claim that local minima are more problematic for the Wasserstein case than for the KL divergence case is unclear and unconvincing. Could this be elaborated further?  
- Lines 137-139: The datasets used in the experiments are unusually small. Could you provide a discussion on the scalability of the algorithm in terms of both the number of samples and the dimensionality of the problem?  
- Lines 177-178: It seems that the observed clustering behavior arises from the entropy regularizer rather than the Wasserstein distance itself.  
- Section 4.3: While the shrinkage effect is noted, is this property desirable? Additionally, would this effect worsen for higher-dimensional problems, as I suspect?  
- Section 4.4: All reported performance measures are based on Wasserstein or Hamming distance. Could alternative metrics be considered?
---
Post-Rebuttal:  
Thank you for your detailed response! It addressed many of my concerns effectively. I strongly encourage you to include a discussion on the scalability of the method in the final version of the paper. Regarding the statement "When γ = 0, we recover the usual OT dual constraints," I believe this would be more accurate if phrased as "in the limit as γ approaches 0."