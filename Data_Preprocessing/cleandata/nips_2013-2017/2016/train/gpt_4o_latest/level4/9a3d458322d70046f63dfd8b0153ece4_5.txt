This paper tackles the problem of safe policy learning, ensuring that the worst-case performance is guaranteed by a baseline policy. Unlike prior methods that rely on overly conservative formulations, the proposed approach enables the discovery of improved policies while ensuring a better discounted return compared to the baseline policy. Since the proposed method is computationally intractable, the authors introduce a reasonable approximation. The approximation technique is analyzed both theoretically and empirically. This work effectively addresses the limitations of existing methods and proposes a superior alternative. Although the implemented method is an approximation of the ideal approach, the paper convincingly justifies the need for this approximation and demonstrates strong empirical performance compared to other potential methods. Additionally, the authors derive performance bounds for various approaches to learning safe policies, though the comparisons among the derived bounds are not extensively discussed.