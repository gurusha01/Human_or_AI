The training process for recurrent neural network (RNN) models is often highly memory-intensive. Since input sequences are not always of fixed length, memory allocation can vary based on the training sample. Traditionally, RNN/LSTM models are trained using "full memoization," where all states through time are stored during forward propagation for reuse during backpropagation. However, this approach can be prohibitively expensive. An alternative, "Chen's algorithm," reduces memory usage by recomputing each hidden state during backpropagation, though at the cost of significantly increased computation. In this paper, the authors introduce a "slider" mechanism that allows users to trade off between computation and memory usage. By controlling how many states to memoize (e.g., memoizing 1 out of every 3 states, or 1 out of every 4 states, etc.), the method provides a flexible balance between high computation/low memory and low computation/high memory. I have encountered concerns from LSTM researchers regarding memory demands, and this approach appears to be a promising solution. However, the authors do not seem to evaluate their method on any real dataset or train an actual RNN/LSTM model to convergence. (Here, I use "RNN" as shorthand for "RNN, LSTM, and related recurrent models.") 
The problem the authors address is indeed significant. Training RNNs can be extremely memory-intensive, and the sequential nature of these models makes it non-trivial to distribute the training of sequences across multiple machines. Therefore, optimizing the use of memory and computational resources is crucial for efficient RNN training. I appreciate that the authors not only propose a novel solution but also provide a user-adjustable tradeoff between computation and memory usage. 
Real Experiments. The authors should conduct experiments on a real dataset to strengthen their claims. - In brief: I cannot assign a score higher than "2" for "experimental methods are appropriate" because no real experiments are presented. - In detail: RNN models have been widely applied to sequence-based tasks such as text, audio, and video data classification or understanding. Many researchers in these domains have made their RNN-based implementations publicly available. For example, a colleague of mine, who is not an RNN expert, was able to download, install, and train an RNN for UCF-101 video activity recognition within two days. Given this accessibility, I strongly recommend that the authors apply their method to train an actual RNN on a real dataset and report the results. Using the same RNN architecture, training protocol, and random seed, the results (trained model) should ideally be numerically equivalent to traditional RNN training with full memoization. Additionally, the authors could provide concrete metrics for memory savings and the computational overhead introduced by their method. If such experiments yield reasonable results, I would assign a higher score for Technical Quality.
Motivation. The paper would benefit from including a concrete example to illustrate the problem's severity. For instance, the authors could state, "In [specific application], an RNN requires 1000GB of memory, which is infeasible. Our method reduces memory usage by over 100x with only a modest increase in computational cost." This would make the motivation for their approach more compelling.