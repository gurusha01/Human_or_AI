This paper introduces a novel feature selection algorithm, termed CMICOT, which accounts for high-order dependencies among features (up to order t, where t > 3) and employs Mutual Information (MI) to quantify these dependencies. To address challenges related to data scarcity and the increased computational cost of estimating mutual information, the CMICOT algorithm adopts a greedy strategy that utilizes binary representatives for each feature. The proposed method is benchmarked against several state-of-the-art feature selection algorithms and interaction-aware SFS-based filters. Experimental results on 10 publicly available datasets demonstrate that CMICOT generally outperforms competing feature selection algorithms in terms of classification accuracy when kNN or AdaBoost are employed as classifiers. The task of designing a feature selection algorithm capable of efficiently handling high-order feature interactions is both intriguing and unresolved, which makes this paper compelling. However, there are significant concerns regarding the computational cost and experimental setup that must be addressed before the paper can be considered for acceptance.
- My primary concern pertains to the justification and analysis of the binary representatives procedure outlined in Section 3.3, which I believe undermines the paper's acceptance.  
  - The theoretical rationale for this procedure is not sufficiently convincing. Lines 219-222 state that "The described technique has been inspired by the intuition that probably two binary representatives of two different features interact on average better than two binary representatives of one feature," but no references or illustrative examples are provided to substantiate this claim.  
  - Furthermore, the comparison of computational costs between the algorithm with and without binary representations (lines 215-219) is problematic. The same values for t and s are used in both cases, which is an unfair comparison as the two scenarios do not involve the same level of information. Additionally, I have significant reservations about the claimed reduction in computational cost due to binary representatives. For instance, in Algorithm 2, while there is a reduction in the cost of computing mutual information (since binary variables are used), the cardinalities of S^bin U B[f] and S^bin are larger than in the non-binary case, as the entire set B[f_best] is added to S^bin in line 21. This suggests a trade-off between the computational cost of the argmax operation and the mutual information computation.  
- Lines 180-182: If the goal is to design an "optimal" interaction-aware MI-based feature selection method rather than a low-dimensional approximation of G, the proposed method incurs a computational cost of O(i^2), which is comparable to RelaxMRMR. What are reasonable values for t and s? This point requires clarification.  
- Lines 183-191: Please rephrase or clarify these lines for better comprehension.  
- Algorithm 1: I do not understand why the specific case t = s is described, rather than presenting a generic algorithm.  
- Proposition 3: The first sentence in Appendix A.4 is unclear and is critical for the validity of Proposition 3. Specifically, the statement "Proof. The calculation of a joint entropy of m variables over N requires takes O(mN) simple operations. Hence, any MI that involve m variables requires O(mN) simple operations as well" needs to be clarified.  
- Lines 249-250: The statement "but estimation ofâ€¦. BR technique" requires clarification. Isn't this a problem-dependent factor?  
- Experimental setup: The experimental design raises another critical issue. It is unclear whether the curves in Figure 1 are derived from a validation/test set or the training set. While it is mentioned that 10-fold cross-validation is used to assess the significance of classification quality differences, it is not specified whether this procedure was also applied in Figure 1. This ambiguity must be resolved.  
- Results for the NBC classifier: The proposed method does not perform well with the NBC classifier. Although these results are included in the appendices, they are neither mentioned nor discussed in the main paper.  
- Appendices: The appendices contain important information regarding the comparison of different feature selection methods. The average results presented in the main manuscript are insufficient.  
- Experiments: How can it be explained that CMIM, rather than one of the interaction-aware FS methods, is the strongest competitor?  
- Additional comparisons: It would be valuable to empirically compare CMICOT with and without binary representatives in terms of both classification accuracy and computational cost. Moreover, comparisons with other feature selection methods regarding computational cost should be included. It is particularly important to clearly highlight the advantages of the proposed method over other interaction-aware SFS-based filters.  
- Minor comments:  
  - Line 27: Should this be SFS-based filters instead of SBS-based filters?  
  - Lines 144-145: Please clarify the italicized sentence.  
  - Figure 1: Missing axis labels.  
  - Table 1: Indicate the statistical test used.