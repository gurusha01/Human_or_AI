The paper investigates the convergence behavior of the EM algorithm (and iterative heuristics for Maximum Likelihood Estimation) in the context of learning mixtures of spherical Gaussians, presenting the following key findings that demonstrate non-convergence to global optima: 1. Even when the mixture consists of 3 clusters that are well-separated and with infinite samples, algorithms like EM can become trapped in local optima. 2. For mixtures of k Gaussians, random initialization for EM succeeds with probability at most exp(-\Omega(k)). 3. Gradient EM avoids strict saddle points generically, implying that bad local maxima are typically the primary issue. The paper provides a cohesive set of results illustrating that EM fails to converge to global optima (even with infinite samples). However, I find the results to be unsurprising, and the techniques employed follow predictable lines. 
The main illustrative example involves a mixture with 3 components: two components are closer to each other, while the third is significantly farther away. The authors demonstrate that for the log-likelihood objective, there exists a local maximum where one center is positioned near the first two components, and the other two centers are near the distant cluster. This behavior is fairly expected, especially when the separation between clusters is much greater than \sqrt{d}. Under such separation, the clusters do not overlap, and learning mixtures of spherical Gaussians becomes conceptually similar to k-means clustering. Analogous examples have been shown to produce poor outcomes for Lloyd's algorithm in k-means clustering. The authors extend this example to k components using a recursive construction, which is technically non-trivial but still somewhat anticipated. Similarly, the failure of random initialization is explained by reasons analogous to those for k-means clustering, where the failure probability is comparable (motivating strategies like selecting k logk centers or using distance-squared sampling in k-means).
In summary, while the results present a consistent narrative about the non-convergence of EM to global optima, I find the conclusions to align with expectations. 
Comments:  
1. Kumar and Kannan provide convergence guarantees for Lloyd's heuristic in k-means clustering (and consequently for mixtures of Gaussians under sufficient separation conditions). While EM differs from Lloyd's heuristic in scenarios where Gaussians are not well-separated (as shown in Balakrishnan-Wainwright-Yu), the two methods behave similarly under large separation (as is the case in the examples presented here). Therefore, I believe a comparison with k-means (and Lloyd's heuristic) that highlights the similarities and differences would significantly enhance the paper.