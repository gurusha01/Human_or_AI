This paper investigates efficient memory utilization for training recurrent neural networks (RNNs). The authors propose a dynamic programming approach for optimal memory allocation during RNN training, enabling the handling of long sequences under constrained memory conditions while maintaining relatively low computational overhead. The dynamic programming formulation represents a meaningful advancement over prior methods, such as Chen et al.'s fixed square root of t schedule and the fixed recursive schedule. By framing the allocation problem as a dynamic programming task, the authors demonstrate that the optimal allocation can be determined efficiently. Overall, this work constitutes a solid and valuable contribution.
However, the distinction between hidden states and internal states is somewhat unclear. The results for these two cases differ only by a constant factor, and it might enhance clarity to move some of the related material to the appendix. Conversely, I found the theoretical analysis presented in the appendix to be engaging and insightful. One notable limitation of the paper is the absence of applications of the proposed methods to real-world problems. It would be beneficial to evaluate whether the ability to train on longer sequences improves RNN performance and to identify any practical challenges that might arise beyond the theoretical framework.
A few minor issues to address:  
- The paper uses an incorrect format and should adhere to the submission template.  
- At the beginning of Section 3, the authors mention discussing two cases: when memory is scarce and when it is somewhat limited. However, the second case was not explicitly addressed, or perhaps it was unclear. The distinction between "scarce" and "somewhat limited" memory was not adequately clarified.