This paper introduces a novel stochastic gradient descent algorithm designed to train ensemble models. The authors build upon the work of Guzman-Rivera et al. [8], where the ensemble model's loss is defined as the loss of the best-performing single classifier within the ensemble, leading to a diverse set of classifiers. The primary contribution of this work is the application of a stochastic gradient descent algorithm for training, which extends the applicability of this approach to deep neural networks. The paper is well-written and engaging. My main concern is whether the algorithm's novelty is more than an incremental improvement over the work in [8]. However, the authors have successfully demonstrated that their modification significantly enhances the algorithm's utility, which leads me to support its acceptance. I also believe it would be valuable to compare the MCL algorithm's performance when using batches instead of the entire dataset. Additionally, I would like to see a more thorough analysis of the algorithm's impact on classifier "diversity."