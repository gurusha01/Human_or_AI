The authors leverage the "cooperative graph cut" framework introduced by Jegelka and Bilmes to propose a method for approximate probabilistic inference. This task is more complex than standard pairwise graphical models due to the inclusion of the nonlocal "cooperative term" \( f \) in equation (1). The core idea involves linearizing this challenging term and refining it through the outer optimization problem in equation (3). This process relies on outer convex relaxations to lower bound the log-partition function and non-convex inner relaxations (mean-field) for the upper bound. Since probabilistic inference (unlike MAP inference) incorporates the entropy surrogate, the dependency of equation (3) on \( g \) is smooth (i.e., \( C^{1,L} \)), eliminating the need for inefficient subgradient-based methods. The paper's novelty lies in its thoughtful integration of well-established techniques, and the resulting approach has the potential for broad applicability. However, the title feels overly general given the related literature (e.g., http://dx.doi.org/10.1007/s10107-016-1038-y and other prior works). A more specific title, such as "Efficient Probabilistic Inference with Cooperative Graph Cuts," would better reflect the content.
The paper's presentation, while generally clear, suffers from a lack of precision in certain areas, likely due to space constraints. I highlight three key points that require clarification and improvement:  
(i) While the relationship between convexity and smoothness via duality is conceptually clear, a precise reference to a quantitative result in the literature would strengthen the discussion, particularly in connection with Theorem 1.  
(ii) The definition of strong convexity implies that subtracting the squared Euclidean norm (scaled by a constant) preserves convexity. However, this property does not hold for the basic entropy function \( \sumi xi \log(x_i) \), making it non-trivial to establish global strong convexity for \( -\bar{H} \) as stated in Theorem 1. Additionally, the applicability of FISTA requires a global Lipschitz constant, which should be explicitly addressed in the context of bounded feasible sets.  
(iii) If the quantitative claim in Theorem 1 is valid, the Lipschitz constant would depend on the number of labels \( k \). In practical scenarios, \( k \) can be significantly larger than in the toy experiments presented, potentially leading to slower optimization. The authors should provide a discussion on this point.