The paper introduces a nonlinear spectral method designed to optimally train a specific class of feedforward neural networks with a linear convergence rate. The condition ensuring global optimality is tied to the network's architectural parameters and involves calculating the spectral radius of a small nonnegative matrix. The idea of employing a nonlinear spectral method for optimal training of this class of feedforward neural networks appears novel and intriguing. However, I have concerns regarding the approach's limitations; it requires the network's weights to be nonnegative. Furthermore, the activation functions used in the network are non-standard and highly specific. These constraints seem quite restrictive and could potentially limit the neural network's performance. This concern is supported by numerical experiments, which demonstrate subpar performance compared to SVM. Minor comment: L.90-91: The claim that "the nonlinear spectral method has a linear convergence rate and thus converges quickly typically in a few (less than 10) iterations to..." seems overstated, as a linear convergence rate does not inherently guarantee convergence within a few iterations.