The authors address the task of text classification into categories, building upon prior work that proposed the Word Mover's Distance (Wasserstein) by introducing a supervised approach that learns word-specific weights. Their contribution involves leveraging the Sinkhorn distance as a computationally efficient approximation of the Wasserstein distance, and they evaluate this method on 8 datasets against 26 baselines, demonstrating the best average performance. The paper is exceptionally well-written, presenting the topic clearly and delving deeply into the mathematical foundations with remarkable clarity. However, there are some potential weaknesses. The novelty compared to the original Word Mover's Distance paper appears somewhat limited, and the references to the World Centroid Distance seem irrelevant and more confusing than helpful. Additionally, the discussion surrounding the regularization parameter lambda feels insufficient. Based on my experience, the choice of lambda significantly influences the results, often dominating Equation (2). Theoretically, a sufficiently large lambda should yield results akin to the pure Wasserstein distance, albeit with slower convergence, while smaller lambda values typically favor faster convergence but deviate from pure Wasserstein behavior. I am therefore curious whether, in this work, lambda leads to a pure Wasserstein-like behavior or an entropic Sinkhorn-like behavior.