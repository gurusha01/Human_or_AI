This paper addresses a dimension reduction problem, specifically finding a (weighted) subset of n vectors that approximates the sum of squared distances from those n vectors to any k-dimensional affine subspace. This problem can be interpreted as an "online" PCA problem. The primary contribution of the paper is proving the existence of such a subset whose size is independent of both the number of vectors n and their dimensionality d. Additionally, the authors present a computationally efficient algorithm to compute this subset. They further demonstrate the utility of their algorithm by applying it to the latent semantic analysis of English Wikipedia. The paper is interesting, and the results are novel. The authors identify a coreset of size independent of the input dimension, which has the potential to significantly reduce the computational cost of solving the dimension reduction problem. 
However, there appears to be an issue in the proof. Specifically, Theorem 2 seems to be a critical step in establishing the main result, but it is unclear why the authors replace vi (a vector) with vi v_i^T (a matrix) when applying Theorem 2 (line 265). Additionally, the paper would benefit from providing more intuition on how to reduce the running time of Algorithm 1 in the proof of Theorem 1. Another potential improvement could involve providing a lower bound. For instance, is it possible to construct a special input matrix such that any coreset of size o(k^2/ε^2) fails to achieve a 1+ε approximation?
The paper is well-organized and easy to follow, although there are a few typographical issues. For example, the authors inconsistently use || || to denote both the operator norm of a matrix and the vector l_2 norm (Theorem 2, lines 250-252). There also seems to be a typo on line 267—did the authors mean equation (5)? Additionally, the variance term is missing on line 5 of Algorithm 1.