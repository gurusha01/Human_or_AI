This paper introduces an end-to-end deep feature embedding approach that integrates similarity metric learning with hard sample mining. The authors compute the similarity between two feature embeddings using both the feature difference vector and the mean vector. For hard sample mining, they identify the hardest positive pair as the pair with the largest distance within the same class, and then select the sample with the smallest distance from a different class as the hard negative for each sample in the chosen positive pair. The proposed method demonstrates strong performance on fine-grained image retrieval, transfer learning, and zero-shot learning. The paper is well-structured and easy to follow, and the method's performance is promising across three vision tasks. However, I have the following concerns:
1. The use of the feature mean vector to incorporate absolute position information in metric learning is not novel, as it has already been introduced by Xiong et al. [35]. This limits the originality of the proposed PDDM method.  
2. The authors claim to improve hard sample mining by employing a better local Euclidean distance metric, selecting the most dissimilar positive pair and the most similar negative pairs in each batch. However, this approach is inefficient for learning since only four samples are utilized per batch (e.g., with a batch size of 64). Moreover, the authors do not analyze the impact of batch size, an important parameter, on the method's performance.  
3. There is a lack of discussion on how to select other algorithmic parameters. For instance, the absence of experimental analysis for lambda raises questions about the effectiveness of the feature-level metric learning.  
4. The method employs hard sample mining throughout the learning process, not just in the early training stages. This raises concerns about whether selecting the hardest samples might lead to poor local minima. It would be valuable to evaluate the method's performance without hard sample mining.  
5. The method assumes all visual classes are equally unrelated, ignoring potential semantic relationships among them. For example, semantically similar classes like cat and dog should be closer in the feature space than unrelated classes like cat and bicycle. It would be interesting to explore whether incorporating semantic information could enhance the learned feature embeddings.  
6. Typos: In Eq. 2, u1 and v1 should be corrected to u' and v'.