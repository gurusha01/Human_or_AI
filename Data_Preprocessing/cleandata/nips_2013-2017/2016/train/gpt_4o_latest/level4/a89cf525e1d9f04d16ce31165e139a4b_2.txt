The paper extends Gaussian Information Bottleneck (IB) to accommodate sparse variables, which is particularly suitable for data exhibiting sparse or nonlinear manifold-like structures. The authors demonstrate the performance of their approach on toy image patch data and handwritten digits, illustrating the algorithm's ability to uncover relevant structures within the data. Overall, this is an intriguing and valuable contribution. However, it would have been more compelling to include applications to real-world datasets, such as natural images, rather than relying solely on toy image patch data. Additionally, the following sentence from the discussion is unclear: "However, unlike traditional sparse coding models, the encoding (or 'recognition') model p(r|x) is conditioned on a separate set of inputs, X, distinct from the image patches themselves. Thus, the solutions depend on the relation between X and Y, not just the image statistics." Specifically, the phrase "a separate set of inputs, X, distinct from the image patches themselves" is ambiguous. My interpretation is that Y represents the image to be reconstructed, which makes this approach appear similar to sparse coding. 
Specific comments:  
1. Line 39 states that Equation 1 is being maximized, but it should be minimized, as indicated in Tishby's paper (Equation 15). This error is also repeated on line 52.  
2. Regarding Figure 1, why not use natural images instead of toy examples? This would provide a more compelling demonstration.  
3. In the occlusion example with the kernel, a generative model could also handle this scenario. How does the proposed method compare to generative models?