The paper builds upon the ODE interpretation of Nesterov's acceleration introduced by Krichene et al. in NIPS 2015 to propose and evaluate a natural heuristic for adaptively reweighting the iterates of the algorithm to accelerate convergence. Unlike prior adaptive averaging heuristics, this approach is provably superior to the standard fixed-schedule averaging. The empirical results demonstrate that the proposed method achieves notable speed-ups across a broad class of functions. I found this paper to be an enjoyable read. The mathematical reasoning is straightforward and accessible, particularly for readers familiar with the work of Krichene et al. Simultaneously, the empirical findings highlight significant and promising improvements in convergence speed. While the technical contributions are closely tied to the framework of Krichene et al., I believe the paper has substantial potential impact due to the widespread use of Nesterov's acceleration in various applications. It is often the case, even in empirical studies, that new, potentially adaptive methods are benchmarked against Nesterov's acceleration with a fixed weight sequence. This paper convincingly demonstrates that making Nesterov's method adaptive requires only a minor modification and can yield substantial performance gains. To strengthen the technical aspects of the paper, I suggest including an analysis of the discretized process to confirm that the convergence rate is preserved. If I am correct, this analysis should be relatively straightforward to carry out. Additionally, I would be curious to explore potential connections between the adaptive averaging proposed for Nesterov's method and the Conjugate Gradient method in the context of strongly convex quadratic optimization.