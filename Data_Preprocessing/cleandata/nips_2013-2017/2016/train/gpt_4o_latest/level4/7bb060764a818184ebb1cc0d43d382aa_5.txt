In this paper, the authors introduce a distribution-dependent dropout applicable to both shallow and deep learning frameworks. They provide theoretical evidence demonstrating that the proposed dropout achieves reduced risk and faster convergence. Building on this concept, the authors design an efficient evolutionary dropout mechanism for training deep neural networks, which dynamically adjusts sampling probabilities based on the evolving distributions of layer outputs. Additionally, the authors propose a multinomial dropout and validate that the distribution-dependent dropout results in faster convergence and reduced generalization error through a risk-bound analysis for shallow learning. The paper is well-written and straightforward to follow. However, I have the following concerns: (1) I question whether the novelty of this work satisfies the standards required for NIPS. The authors may wish to highlight their contributions more explicitly in the rebuttal. (2) In the experimental section, why do the authors report only the best results for each dataset, focusing on the fastest convergence? (3) Why is the evolutionary dropout compared with BN solely on the CIFAR-10 dataset? The authors are encouraged to conduct similar experiments on additional datasets to further validate the proposed method's effectiveness. Minor comments: The citation package is improperly utilized. In Section 2, there are instances of "author ?".