This paper integrates the information bottleneck (IB) framework with sparse coding, presenting two specific implementations of IB: the first employs sparse priors on the internal representation, while the second extends the first model using kernels. A variational learning algorithm is introduced for training the model, and its performance is assessed on both simulated and real-world data (handwritten digits). I find the paper to be both innovative and engaging. To the best of my knowledge, the algorithm is original and contributes meaningfully to the repertoire of IB-based methodologies. The proposed approach demonstrates superior performance compared to Gaussian IB in denoising and occlusion/inpainting tasks on both simulated and real data. Additionally, it offers novel analytical tools for exploring sparse representations through IB information curves. Overall, I believe this work has significant potential applications in machine learning and neuroscience, making it highly relevant to the NIPS audience. Below, I outline a few questions and suggestions that could help the authors further refine the quality and clarity of the paper:
1) As I understand it, the encoding and decoding dictionaries, $W$ and $U$, are trained under a fixed constraint $\gamma$. How does $\gamma$ influence the learned dictionaries? Do different constraints lead to the emergence of distinct receptive fields?
2) The authors validate their model using simulated data and handwritten digits. While these datasets are essential for demonstrating the method's correctness, they are somewhat limited in scope. It would be valuable to test the algorithm on a more diverse set of stimuli, such as natural image patches. Alternatively, the authors could briefly justify their choice of data (handwritten digits) to clarify its relevance.
3) A brief justification for the choice of the sparse prior would be helpful. Why did the authors select the Student-t distribution? Was this choice motivated by analytical convenience, or were there other considerations? For example, why not use a generalized Gaussian distribution? Different priors have varying entropies, which, as I understand, could influence the model's performance.
4) I appreciate the discussion linking the proposed model to prior infomax and sparse coding approaches. Could these earlier methods be interpreted as special cases of the sparse IB model?
Minor comments: While the paper is generally well-written, I believe a few additional explanations could enhance its clarity. For instance:
- In equation (5), the relationship between the upper bound on I(X;R) and the log-likelihood of R is not immediately clear. A brief explanation would be helpful.
- Line 38: The objective is to minimize the Lagrangian L (as presented by the authors), not maximize it.
- Line 44: The sentence is unclearâ€”what does "and" refer to? Additionally, the inequality in the sentence could benefit from more context for better understanding.
- Figure 2D: As I understand it, "stimuli" in this panel refers to Y, not X. Clarifying this would improve the figure's interpretability.