The paper highlights that multiclass SVMs depend on a fixed mapping and the learning of a linear classifier, whereas multiclass Boosting involves learning a mapping and applying a fixed classifier. The authors propose an approach that simultaneously learns both the mapping and the classifier. The paper is well-written and relatively easy to understand. The experimental results seem reliable. As I am not an expert in either SVMs or Boosting, I am unable to assess the novelty of the work. If the contribution is indeed novel, I would recommend acceptance. As mentioned earlier, the authors identify a duality between multiclass SVMs and Boosting, leveraging this insight to introduce a new algorithm that learns both a mapping and a classifier. To enable the proposed algorithm, the authors replace the hinge loss used in SVMs with the exponential loss employed in Boosting. Consequently, the algorithm feels more akin to a Boosting method than a true unification of Boosting and SVMs (though this distinction is not particularly problematic). The algorithm involves solving a convex optimization problem at each iteration, meaning the overall process does not solve a single convex problem (which I find acceptable). However, the authors do not provide a discussion on the running time of their algorithm. Including runtime comparisons between the proposed algorithm, Boosting, and SVMs would significantly strengthen the paper. Overall, I find the work interesting and the paper well-written, though I cannot assess its importance or novelty due to my lack of expertise in this area. 
Minor comments:  
* The notation for dot products alternates between \langle and \rangle and < and >. Please ensure consistency.  
* You use \argmin but appear to write \arg\max instead of \argmax. Consider standardizing this.  
* When referencing Vapnik's foundational work, it would be helpful to include a citation.