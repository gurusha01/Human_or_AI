The paper investigates a relaxation of the standard no-regret property in online algorithms, introducing a concept termed low approximate regret. This relaxation permits a small multiplicative approximation factor for regret, which is satisfied by many learning algorithms. The flexibility introduced by the multiplicative term allows for a smaller additive error term, as the standard error term can effectively be absorbed into the multiplicative factor. Despite this relaxation, the approximate low regret property still ensures price of anarchy guarantees in (\lambda-\mu)-smooth games, along with a reduced error term after fewer periods of play. Notably, the results rely only on realized feedback, or even bandit feedback in some cases, rather than requiring expectations over other players' actions. I find the approach of transforming the additive error term into a multiplicative error term both interesting and well-leveraged in the paper.
However, there is significant technical overlap with prior works such as SAL15 and LST16, which tempers my enthusiasm for recommending this as an oral presentation. Additionally, the paper's language occasionally oversells the results, which can detract from the technical contributions. For instance, does learning truly converge quickly to equilibria in games? The key insight here is that by relaxing the notion of convergence (from convergence of actual behavior to convergence of time-averages) and by broadening the target states (to include states where agents exhibit low approximate regret), the (\lambda-\mu)-smoothness results remain applicable. While these technical contributions are compelling, the paper's expansive interpretation of convergence and equilibrium to achieve faster "convergence" risks overshadowing its core contributions.
Finally, if this work is intended to apply to general games, as the title suggests, it would be helpful to clarify the practical implications for specific cases, such as two-player games or even simple 2x2 games.