This paper offers several notable contributions: 1. an extension to counting-based exploration leveraging a sequential density model; 2. a straightforward yet impactful modification to DQN; 3. strong empirical results, including significant progress on Montezuma's Revenge; and 4. a connection to intrinsic motivation (further elaborated in the detailed review below). Overall, this is a solid paper that merits acceptance. Notably, the formulation of pseudo-count appears novel, and the Montezuma's Revenge experiments convincingly demonstrate the practical applicability of the proposed method. However, I have two concerns that I would like the authors to address:
1. The paper emphasizes, particularly in the title and on line 206, that the primary contribution is the relationship between pseudo-count and information gain (IG), a well-established concept in the intrinsic motivation literature. While the stated relationship is technically valid, it does not seem to align with the existing intrinsic motivation literature. As far as I can discern, the term "information gain" in this paper is defined in a way that is both technically and philosophically distinct from its traditional usage. Specifically, the IG in this work is defined over the information gain of a mixture of sequential density models of state visitations, whereas IG is conventionally defined as the information gain of a "model of the environment" [1,2]. Philosophically, IG is traditionally introduced to create a "knowledge-seeking agent ... to gain as much information about its environment as possible" [2], a characteristic that the IG definition in this paper seems to lack. This is because, under the proposed definition, any change in the behavior policy will result in information gain for the state density model due to shifts in state visitation frequencies, even if no new information about the environment is revealed. This distinction makes the IG in this paper fundamentally different from the classic notion. While the connection between IG, pseudo-count, and PG is intriguing, I would like the authors to justify how their definition "connects to intrinsic motivation" and "unifies count-based exploration and intrinsic motivation."
2. Could the authors elaborate on why pseudo-count is preferable to PG? From Figure 2 in the appendix, it appears that PG performs competitively without requiring additional tuning, such as selecting a transformation function, whereas the inverse pseudo-count performs poorly with the square-root transformation. I suspect that if similar transformations were applied to PG, it might outperform the adjusted pseudo-count bonus. Additionally, Theorem 2 suggests that 1/pseudo-count ≥ e^PG - 1. Since 1/pseudo-count grows exponentially relative to PG, it would not be surprising if PG exhibits more stable behavior compared to 1/pseudo-count. I would appreciate clarification on whether pseudo-count offers distinct advantages over PG or if the two are largely interchangeable.
[1]: Sun, Yi, Faustino Gomez, and Jürgen Schmidhuber. "Planning to be surprised: Optimal Bayesian exploration in dynamic environments." International Conference on Artificial General Intelligence. Springer Berlin Heidelberg, 2011.  
[2]: Orseau, Laurent, Tor Lattimore, and Marcus Hutter. "Universal knowledge-seeking agents for stochastic environments." International Conference on Algorithmic Learning Theory. Springer Berlin Heidelberg, 2013.