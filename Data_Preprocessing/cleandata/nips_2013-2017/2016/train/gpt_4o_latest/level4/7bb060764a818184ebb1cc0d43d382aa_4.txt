This paper introduces an enhanced dropout technique for both shallow and deep learning models. Unlike traditional dropout, which employs uniform sampling, the proposed method utilizes a multi-normal distribution to determine which neurons to drop. The authors claim that the enhanced dropout achieves performance comparable to batch normalization. A limited set of experiments is presented to demonstrate the effectiveness of the proposed method. Overall, I appreciate the concepts of data-dependent dropout and evolutionary dropout, and I find it intriguing that the method performs similarly to batch normalization. However, the experimental validation is insufficient. I recommend including additional results, such as in tabular format, to provide comparisons with state-of-the-art methods and benchmarks like ImageNet.