This paper tackles the challenge of learning deep feature embeddings, focusing on the issue of non-uniform density in the feature space, which complicates effective hard negative mining. To address this, the authors propose a novel unit inspired by [35] that incorporates both the mean position of a feature pair and their difference to compute similarity. By leveraging positional information, the unit effectively accounts for the non-uniform density in the feature space. The proposed unit is differentiable and can be seamlessly integrated into existing CNN architectures. A double-header hinge loss is introduced, which operates on pairwise differences and similarities both before and after the unit. During each minibatch of SGD, the method employs hard quadruplet mining, identifying positive pairs with low similarity and corresponding negative pairs with high similarity. 
The approach is evaluated on image retrieval tasks using the CUB-200-2011 and CARS196 datasets, as well as on transfer and zero-shot learning tasks using the ImageNet-10K and ImageNet 2010 datasets. The results demonstrate improvements across all tasks, with notable gains in training efficiency. The paper addresses a significant and well-recognized problem in computer vision, and the proposed formulation appears novel to the best of my knowledge. The manuscript is well-written, with appropriate references, and the results are compelling. The combination of the PDDM unit and quadruple hard mining seems to provide meaningful advantages.
However, I have a few points of clarification. In Table 1, I am unclear about the distinction between the "PDDM score" and the "Quadruplet+PDDM" rows. Do these correspond to retrieval using the output of the PDDM module (former) versus the learned embedding before the PDDM module using Euclidean distance (latter)? Additionally, I am curious about the role of the embedding loss \(E_e\) in Equation (4). How critical is it, given that gradients can propagate through the PDDM modules? Lastly, are the features for hard negative mining computed afresh for each minibatch, or are they cached and periodically updated for computational efficiency?