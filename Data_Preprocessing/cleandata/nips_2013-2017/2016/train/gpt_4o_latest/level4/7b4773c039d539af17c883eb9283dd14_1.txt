This paper presents a novel Monte Carlo-based planning algorithm named TrailBlazer. The algorithm assumes access to the generative model of the Markov Decision Process (MDP) and aims to compute the optimal value function of the root node, \( V(s_0) \), while minimizing the number of calls to the generative model. The framework considers a discounted reward setting, and the algorithm is applicable to both finite and infinite state spaces. The paper establishes several theoretical guarantees: PAC consistency, a high-probability upper bound on the number of generative model calls for finite state spaces, and an expected upper bound on the number of calls for infinite state spaces. Depending on the context, the results either 1) improve upon prior worst-case upper bounds (e.g., for finite state spaces with stochastic dynamics), 2) match existing results (e.g., deterministic dynamics or scenarios without control, akin to standard Monte Carlo methods), or 3) introduce novel findings (e.g., for infinite state spaces). 
The TrailBlazer algorithm alternates between two types of nodes: Avg and Max. Avg nodes estimate the average value of their children, which are sampled based on the transition probabilities, effectively functioning as a Monte Carlo estimator. The parameter \( m \) governs the variance of this estimator. Max nodes, on the other hand, aim to identify the child node with the maximum value by systematically eliminating suboptimal children with high confidence. 
This is a strong paper that introduces a new algorithm with significant potential for practical applications. The algorithm is not only theoretically sound (demonstrating consistency) but also offers guarantees that surpass existing results in certain scenarios (e.g., for finite state spaces with generative models and stochastic systems). The paper is generally well-written, though it could benefit from additional intuition to clarify why the proposed algorithm outperforms alternative approaches. This aspect is somewhat underexplored. Additionally, due to the inductive nature of the tree-based analysis, the proofs are somewhat complex, though this may be unavoidable.
I have the following comments and questions:  
- What is the primary reason that the guarantee in Theorem 3 is provided only in expectation?  
- The algorithm does not appear to leverage potential regularities in the value function, such as smoothness. Could it be extended to exploit such properties, perhaps in a manner similar to the StoSO algorithm (Valko, Carpentier, Munos, "Stochastic Simultaneous Optimistic Optimization," ICML 2013)?  
- The paper notes that for non-vanishing action gaps, the dimension \( d \) can be set to zero. In more realistic scenarios, action gaps may follow a distribution over states, as discussed in Farahmand ("Action-Gap Phenomenon in Reinforcement Learning," NIPS 2011). What insights can be provided for such cases?  
- Section 3.1 (Separate bias and variance) lacks clarity. Specifically, the statement "In doing so, their algorithms compute … . However in our planning …" requires further elaboration.  
Minor issues and typos (line numbers refer to the Supplementary Material):  
- L81: "worst" → "worse"  
- L84: "an near-optimal" → "a near-optimal"  
- L220: "and an a term"  
- L229: "an problem-dependent"  
- L472: "i.d.d."  
- Appendix D: Several instances of \( \Delta \) are written as "Delta"  
- L504: "one need" → "one needs"  
*  
Thank you for your attention.