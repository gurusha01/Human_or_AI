This paper addresses two primary themes. First, it establishes a duality between multi-class SVMs and multi-class AdaBoost. Second, it leverages this duality to propose a novel dimensionality reduction technique that maintains discriminability across different classes. The authors extend the MCBoost criterion to enable multi-class boosting in an arbitrary number of dimensions, as opposed to the prior formulation, which restricted the number of dimensions to the number of classes. This relaxation of dimensionality constraints facilitates a boosting framework with a controllable number of boosting functions, making it applicable as a general approach.  
+ The connection between MCBoost and multi-class SVM is intriguing, and the discussion is well-articulated. The comparison of differences in predictors and codewords (e.g., Table 1) is particularly clear and accessible. However, is it widely known that both MC-SVM and MC-Boost aim to maximize the margin?  
+ The authors demonstrate improved performance in terms of error rate and mean average precision (mAP). Specifically, LADDER surpasses MCBoost by 2% in classification accuracy. Furthermore, LADDER consistently outperforms MCBoost across all dimensional settings in terms of error rate (Figure 3).  
- The paper could provide more details regarding the dimensionality reduction process and its relationship with CNNs. This is a promising direction, and prior work (e.g., DrLim, Hadsell et al., 2006) has shown that such approaches can outperform other techniques even when the training dataset is not particularly large. This gap is also evident in the literature review.  
- The results should be compared with recent advancements in CNN-based methods. Representations learned through CNNs are often highly effective, as they jointly learn both feature representations and discriminative projections. While the authors include a comparison using features extracted from conv5 (Table 2) against BoW features, this does not fully capture the strength of deep learning, which lies in the simultaneous optimization of features and projections. Despite the non-convex nature of CNNs, they often outperform convex methods in practice.  
- Another convex method, proposed by Simonyan, Vedaldi, and Zisserman (PAMI 2014), is notably absent from the comparisons. Instead, the authors primarily compare their approach with less competitive methods such as PCA, LDA, and their kernel variants.  
- Overall, the experimental evaluation lacks depth and fails to convincingly demonstrate the advantages of the proposed method. Additional experiments, including convergence analyses and more detailed sub-analyses of the method, are necessary to substantiate its merits.