The pursuit of fast rates of convergence has been an active and vibrant area of research in recent years, with the majority of advancements focusing on bounded loss functions. This paper extends the scope to unbounded loss functions and successfully establishes fast learning rates, albeit at the (mild) cost of introducing a multi-scale Bernstein condition and requiring the existence of the $r$-th moment of the envelope function (an upper bound for $\ell \circ \cdot$, where $\ell$ is the loss function). The central result (Theorem 3.2) provides a sharp oracle inequality that holds with high probability, achieving a convergence rate of $n^{-\beta}$, where $\beta$ can be made arbitrarily close to 1 (from above) by increasing $r$: the more moments of the envelope function exist, the closer $\beta$ approaches 1. Section 3.4 is particularly useful in verifying that the multi-scale Bernstein condition is satisfied for a specific learning setup. Theorem 4.1 demonstrates an application of this framework to quantization, showing that the ERM achieves a convergence rate of $\mathcal{O}(n^{-\beta})$, with $\beta$ being arbitrarily close to 1. 
The paper is highly engaging and makes remarkable contributions. I was deeply impressed by the technical sophistication and the wealth of results and ideas presented within just eight pages. The introduction is well-crafted, clearly outlining the paper's objectives in a manner accessible even to non-experts. The main contributions are explicitly articulated and substantiated by rigorous proofs. I am confident that this work will have a significant impact across multiple research communities. In my view, this is an excellent piece of research. 
Minor comment:  
- Lines 444, 458, 460, 462, 481: Please ensure proper citations are provided, listing all authors instead of using "et al."