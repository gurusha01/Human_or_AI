This paper presents an algorithm for generating adversarial examples for neural networks (NNs), specifically images with imperceptible perturbations that lead to incorrect classifications by the NN. The proposed algorithm ensures that the adversarial examples remain within the linear activation regions of the ReLUs in the NN. Experimental results on the MNIST dataset demonstrate that, for the LeNet architecture, the adversarial examples generated by this method appear to be more damaging than those produced by prior approaches, as measured by the L_inf norm of the perturbation. Similar to [5,20], this work introduces a novel algorithm for generating adversarial examples and employs these examples to re-train the NN. However, the algorithm differs from prior methods by incorporating additional constraints into the optimization process. I have two primary concerns regarding the motivation for these constraints:  
- Theoretical Concerns: The constraints added to the algorithm are motivated by the statement: "Since adversarial examples exist due to the linearity in the neural net [5], we restrict our search to the region around the input in which the neural net is linear." However, adversarial examples have also been observed outside this linear region. For instance, when the adversarial perturbation is scaled by a relatively large constant, the resulting adversarial examples still negatively impact the NN [5]. Furthermore, there are no theoretical guarantees that restricting adversarial examples to this linear region ensures minimal perturbation. It is possible that adversarial examples with smaller norms exist outside the search space defined by the proposed algorithm.  
- Experimental Concerns: The conclusions drawn from the experimental results may lack generality. The NN architectures used are neither state-of-the-art nor standard, and the experiments are conducted on relatively simple datasets (MNIST and CIFAR). Additionally, there is no verification that the baseline from [20] has been accurately reproduced. The results show only marginal improvements over the baseline on MNIST, and for CIFAR, comparisons with the baseline are omitted.  
Additional Comments:  
- The results on CIFAR suggest that the proposed method may have limited applicability in reducing adversarial examples, as generating a single adversarial example requires approximately 15 seconds using 8 CPUs. This computational cost poses challenges for scaling the method to larger datasets like ImageNet and for generating sufficient adversarial examples for re-training.  
- The variable $x_\star$ is introduced without prior definition.  
- The paper assumes that the L_inf norm is the most appropriate metric for evaluating the perceptibility of adversarial perturbations.  
- The paper references [3] as an attempt to explain why neural networks may generalize well despite poor robustness properties. A more detailed explanation of [3] would be beneficial, and it may also provide a stronger foundation for motivating the proposed constraints.