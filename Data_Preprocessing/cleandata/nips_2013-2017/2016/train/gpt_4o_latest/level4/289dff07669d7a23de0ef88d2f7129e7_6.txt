The paper introduces a variant of CMA-ES with "Optimal Covariance Update and Storage Complexity." The authors claim that this modification preserves the algorithm's behavior while reducing computational cost. They name their method Cholesky-CMA-ES, which is problematic as this term was originally introduced by T. Suttorp et al. (2009) for a similar algorithm that is widely recognized. The authors fail to address this naming conflict and instead present their version as "Our CMA-ES variant, referred to as Cholesky-CMA-ES." The proposed approach builds incrementally on the work of Krause et al. (2015). Specifically, the only algorithmic change is the repeated invocation of rankOneUpdate (already present in Krause et al.'s work) in a loop for all individuals, rather than a single call as in Plus strategies. 
Empirical results show differences on the Cigar function in Figure b), where the plateau exhibits distinct objective function values for CMA-ES-Ref and Cholesky-CMA-ES. This discrepancy might stem from the way median results are plotted. The paper should include a comparison of the proposed method with Suttorp's Cholesky-CMA-ES (empirically) and Krause's Cholesky-CMA-ES (algorithmically). The claim of "Optimal Covariance Update and Storage Complexity" is somewhat misleading. For instance, on the Sphere function, where the covariance matrix is unnecessary, or on separable functions, where only the diagonal is required, the proposed method is not optimal. Additionally, the approach is only twice as efficient as the omitted Cholesky-CMA-ES by Suttorp, which is described in this paper as prone to numerical instabilities, despite the original work suggesting otherwise. The omission of Suttorp's method in Figure 3 could mislead readers about the paper's actual contribution. 
I do not believe the proposed approach significantly impacts CMA-ES for large-scale optimization, as its complexity remains quadratic. However, I acknowledge that the modification may represent the best possible improvement in terms of time and memory savings when using the full covariance matrix. Overall, the contribution feels too incremental, as the primary algorithmic change is the addition of a single "for" loop. I recommend the following revisions: a) Address the naming conflict; b) Conduct experiments with the variants proposed by Suttorp et al. and Krause et al., updating all three figures accordingly. If the figures become cluttered, present only the best-performing variant in the main text and include the full figures in the Supplementary Material; c) Clarify that the main algorithmic contribution is the "for" loop in Algorithm 1, or provide justification if this interpretation is incorrect. 
Minor comments: The statement, "Typically, an eigenvalue decomposition of the covariance matrix is performed in each update step," is inaccurate. In proper implementations, this is done periodically rather than in every step.