The paper addresses the problem of robust Markov Decision Processes (MDPs). Specifically, it considers an uncertain model of the MDP's transition probabilities (with rectangular uncertainty) and a baseline policy. The objective is to derive a new policy that is guaranteed to perform no worse than the baseline, a concept referred to as safe policy improvement. A robust approach typically seeks a policy that maximizes the worst-case performance. To ensure safety, the new policy must be compared to the baseline's performance. Since the baseline's performance is not directly known, one could compare it to the baseline's best possible performance. However, this approach may be overly pessimistic because the worst-case model for the robust policy may differ from the best-case model used to evaluate the baseline. A more reasonable alternative is to compare the regret of the new policy relative to the baseline. Regret is defined as the difference between the baseline's performance and the new policy's performance under the same probability model. By optimizing for the policy that minimizes the worst-case regret within the uncertainty set, the resulting policy is both safe and potentially less conservative than the traditional robust approach. The paper provides a theoretical performance bound for this solution (Theorem 5) and demonstrates that such a policy is generally randomized (Theorem 3). However, the associated optimization problem is shown to be NP-Hard (Theorem 6). 
To address this computational challenge, the paper proposes a heuristic approximation. This approximation leverages the observation that if the uncertainty for actions selected by the baseline policy is zero, the problem reduces to a standard robust MDP formulation (Proposition 7), which is solvable in polynomial time. The proposed algorithm enforces this zero uncertainty assumption for baseline actions, effectively treating their model as error-free. This approximation is reasonable when data is predominantly collected by following the baseline policy, with occasional exploratory deviations, as the estimated model for the baseline is more accurate in such cases. However, it remains an approximation, and the paper does not provide theoretical guarantees on its quality. Instead, empirical results demonstrate that the proposed approach outperforms both the standard robust MDP approach that retains the full uncertainty set (Algorithm 2) and a certainty-equivalence approach that disregards uncertainty (Algorithm 1).
Brief summary of evaluation:  
- Technical quality: The results appear technically sound, though the tools used are fairly standard.  
- Novelty/originality: The optimization formulation is novel, while the derivation of error bounds is relatively standard.  
- Potential impact or usefulness: While this work could serve as a foundation for future research, its immediate real-world impact is limited due to a) its restriction to finite MDPs and b) the lack of guarantees for the approximate solution.  
- Clarity and presentation: The paper is relatively well-written.  
Overall, this is a solid paper that tackles an important problem relevant to real-world reinforcement learning applications. It formulates a more reasonable optimization problem than the conventional robust MDP approach. While the NP-Hardness of the new problem (Theorem 6) is unfortunate, establishing this result is valuable. Additionally, the theoretical performance bound for the optimal solution (Theorem 5) is a significant contribution. The proposed algorithm (Algorithm 1) is simple and practical, though the lack of theoretical guarantees for its performance is a limitation. Nonetheless, the empirical results are promising.  
Detailed comments:  
- Algorithm 1 (L4): Should the term ρ(π_B, ξ) be removed? It appears to be constant.  
- L198: The reasoning in point 2 is unclear.  
- L69: Please clarify that the paper focuses on finite state and action spaces.  
- L159 (Theorem 5): The norms and terms \(e{\pi^*}\) and \(e{\pi_B}\) are undefined.  
- L90: What does "randomized uncertainty set" mean? Please elaborate.  
- Please compare the choice of regret as the objective to minimize with the following work: Regan and Boutilier, "Regret-based Reward Elicitation for Markov Decision Process," UAI, 2009. While that work does not address uncertainty in \(P\), it also employs regret as a key concept.  
After Rebuttal:  
Thank you for your response.