The authors present a method to address the limitations of global distance metrics by introducing PDDM, which captures local feature similarity metrics. They further propose the large-margin double-header hinge loss to jointly optimize similarity metric learning. Their approach demonstrates strong performance in image retrieval and transfer learning tasks, showcasing the effectiveness of their method. The paper is well-structured, with clear logic and well-designed figures, and reports promising results in both application areas. The authors effectively illustrate the benefits of integrating distance metrics with local similarity metrics achieved through the PDDM mechanism. The PDDM mechanism contributes a novel local similarity metric that combines feature difference and absolute position information from [35], resulting in surprisingly fast training convergence and improved performance. Additionally, the two double-header hinge losses in Eq (4), which integrate local similarity metrics with distance metrics on hard quadruplets, share conceptual similarities with [27] but achieve lower computational cost, faster convergence, and require fewer samples.
Questions for authors:  
1) To enhance the paper's clarity and self-containment, a more formalized description of the comparative methods (e.g., contrastive embedding, triplet embedding, and lifted structured embedding) should be included. This would better highlight the improvements and advantages of the proposed method beyond the comparison in Figure 3.  
2) The authors should provide additional analysis or demonstrations to explain the key factors driving the effectiveness of concatenating the feature difference vector and feature mean vector in the PDDM unit. This would be valuable and instructive for readers.  
3) The paper mentions that the entire network, consisting of 4 identical CNNs and PDDM units, is trained end-to-end. However, the hard quadruplet mining during a forward pass also involves 2 CNNs and PDDM units. More details on the specific forward pass should be included to clarify the end-to-end architecture.  
4) I would like to confirm whether the authors select only one hard quadruplet ($\hat{i}$, $\hat{j}$, $\hat{k}$, $\hat{l}$) for training per mini-batch (64 samples). If this is the case, the number of samples used to fine-tune the CNNs and train the fully connected layers of the PDDM unit per epoch seems limited. Please provide additional details to clarify whether this is sufficient for training the network parameters on the CUB-200-2011 dataset (5,864 training images).  
Minor issues:  
1) Line 47: "learn an local-adaptive" → "learn a local-adaptive"  
2) Line 103: "into an feature space" → "into a feature space"  
3) Eq (2): $u1$ and $v1$ should be corrected to $u'$ and $v'$.