This study explores the representational capabilities of the unitary matrix employed in the recently introduced unitary evolution RNNs and demonstrates how a complete, full-capacity unitary matrix can be effectively optimized. The original approach proposed a specific composition of individual unitary matrices (diagonal, Householder reflection, permutation, and DFT), which this paper identifies as limited in its ability to represent the entire unitary group when the hidden state dimension exceeds 22. Prior research in optics established that a unitary matrix could be expressed as a product of at most comb(N,2) Givens operators. This paper introduces the concept of the Givens capacity of a unitary matrix, defined as the minimum number of operators required to construct the matrix, with an upper bound of comb(N,2). The authors demonstrate how full-capacity unitary matrices can be optimized using gradient descent, leveraging a method described in a 2011 technical report. This optimization is applied to three tasks and compared against the restricted-capacity unitary matrices from the original work, as well as the LSTM.
The first task, a synthetic data system identification problem, evaluates the performance of restricted-capacity unitary RNNs and full-capacity uRNNs in learning the dynamics of a target uRNN using only input-output samples, with training based on an MSE objective. Across all tested hidden dimensions, the restricted-capacity model performs worse. The second task, the copy memory problem, requires reproducing the first 10 inputs after delays of 1000 and 2000 steps. Here, the full-capacity uRNN outperforms both the restricted-capacity model and the LSTM. The final task involves predicting the log magnitude of the short-time Fourier transform (STFT) of a speech sample, given all prior STFTs, using the TIMIT corpus. Performance is evaluated using objective metrics (MSE, segmental SNR) and perceptual metrics (STOI, PESQ). While there is minimal difference between restricted and full-capacity models in terms of MSE, both significantly outperform the LSTM. For SNR, STOI, and PESQ, the full-capacity model slightly outperforms the restricted-capacity model, with both again substantially surpassing the LSTM.
The uRNN presents a more elegant solution to the vanishing/exploding gradient problem of RNNs compared to the LSTM. The proposed full-capacity optimization further widens the performance gap relative to the LSTM, beyond what was observed with the restricted-capacity uRNN. Notably, in the copy memory problem, increasing the LSTM's hidden state size to match that of the uRNN reduces the performance gap between the LSTM and the restricted-capacity uRNN. However, both models now struggle at T=1000, whereas the restricted-capacity uRNN previously solved the task rapidly and perfectly at T=500. This raises questions: Would further increasing the hidden state size of the restricted-capacity uRNN and LSTM improve their performance on tasks with longer delays? Additionally, could optimizing the LSTM further (e.g., using Adam optimizer) reduce the need for gradient clipping? Lastly, there appears to be a missing word after "here" on line 101.