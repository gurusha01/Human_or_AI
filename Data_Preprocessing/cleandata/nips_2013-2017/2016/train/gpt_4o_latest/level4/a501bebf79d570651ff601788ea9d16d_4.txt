This paper introduces a memory-efficient approach to backpropagation through time (BPTT) by leveraging dynamic programming and also derives an analytical upper bound for their method. Compared to Chen's algorithm, the proposed BPTT-MSM demonstrates the significant advantage of being able to operate within nearly arbitrary constant memory constraints. As is well-known, memory consumption poses a major challenge when training complex recurrent neural networks on very long sequences. While the proposed method addresses this issue effectively, it is worth noting that the strategies presented in this paper are more complex than those in Chen's work. Additionally, Chen evaluated their approach on specific tasks using mxnet. It would be beneficial if the authors could include experimental results on particular RNN architectures, such as those used in sequence-to-sequence (seq2seq) learning, to further validate their method.