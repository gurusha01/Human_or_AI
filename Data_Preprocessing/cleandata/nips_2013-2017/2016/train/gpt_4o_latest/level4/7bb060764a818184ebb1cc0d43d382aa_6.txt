This paper introduces a novel variant of dropout that assigns distinct drop probabilities to each unit. These probabilities are determined based on the second-order statistics of the units. The authors conduct a thorough theoretical analysis, demonstrating that this choice of probabilities minimizes an upper bound on the expected risk, which could lead to faster convergence and reduced final error. Additionally, they provide empirical evidence to support this claim. I found the paper engaging and the theoretical insights valuable. It enhances the understanding of dropout and offers a straightforward improvement to this widely used technique, yielding moderate performance gains. 
However, the paper has notable weaknesses in its empirical evaluation and the corresponding presentation:  
- First, the quality of the plots is poor. They are too small, the color scheme makes it difficult to differentiate between lines (e.g., pink vs. red), the axes are inadequately labeled (e.g., what does "error" refer to?), and the labels for different methods are visually too similar (e.g., s-dropout(tr) vs. e-dropout(tr)). Since these plots are central to presenting the experimental results, they need to be significantly improved. This issue is the primary reason I rated the clarity as "sub-standard."  
- The comparison of standard dropout versus evolutional dropout on shallow models should be presented as an average over multiple runs (at least 10), ideally with error bars. The current plots appear to be based on single runs, which may be affected by significant variability. Given the small size of the models, there is no justification for not including statistical measures.  
- For the deep models, particularly on CIFAR-10 and CIFAR-100, the final learning rates used should be reported. The authors only explored four learning rates, and if the optimal learning rate for the baseline lies outside this range, it could undermine the results.  
Additionally:  
- The claim that evolutional dropout addresses internal covariate shift seems limited. While it may increase the variance of some low-variance units, it does not standardize variance or center activations as Batch Normalization does. These limitations should be explicitly discussed.  
Minor: *