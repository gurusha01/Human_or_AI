This paper investigates the use of Wasserstein Loss as the objective for training generative models, with a particular emphasis on learning restricted Boltzmann Machines. The authors examine the impact of varying the strength of entropy regularization within the Wasserstein objective and analyze its effects on tasks such as image completion and denoising. I appreciate the use of Wasserstein loss as an alternative to minimizing KL divergence and agree that it can exhibit greater robustness in certain scenarios. However, a notable limitation of this approach is its computational expense. Despite leveraging recent advancements in optimization techniques, the authors have only applied this method to small toy datasets. The analysis of how increasing the entropy regularization strength affects the model (Figure 4) is intriguing. Could you provide some intuition as to why increasing lambda results in models that are more concentrated in the image space? Additionally, why is it that \( E{\hat{p}}[\alpha^] = 0 \), while in Equation 5, \( E{p}[\alpha^] \neq 0 \)?