The manuscript explores a prominent contemporary technique for enhancing the performance of deep neural networks, specifically dropout. It begins by analyzing a linear shallow network, deriving theoretical insights through risk bounds and update rules that employ multinomial sampling to select dropout neurons at each update step, leveraging the second-order statistics of the data features. Subsequently, the authors extend these update rules to deep networks, where they utilize the second-order statistics of individual layers for mini-batches of data. This approach also establishes a connection with the internal covariate shift, a well-studied phenomenon in the literature. The proposed method demonstrates significant improvements in both convergence and accuracy for shallow and deep networks across multiple datasets. 
The paper is noteworthy because its implementation appears straightforward, yet the results clearly indicate substantial performance gains over standard dropout. The method sets a new benchmark for dropout techniques, and I anticipate it will become widely adopted in the near future. The work is technically robust, with claims substantiated by both theoretical analysis and empirical experiments. It represents a comprehensive study, beginning with an insightful observation and systematically progressing to experimental validation of the method's performance improvements. The manuscript is well-written, logically structured, informative, and reproducible. 
The proposed approach is innovative, as no prior work has addressed the dropout problem from this perspective. The inclusion of theoretical bounds not only strengthens the contribution but also opens avenues for further exploration, such as considering more complex probability distributions for dropout. The connection to internal covariate shift is particularly illuminating. 
Minor comments: The authors should clarify the computational cost of their method. Is the reported convergence speed measured in terms of the number of iterations (as it appears)? If so, how does the method perform in terms of wall-clock time? What does \(\mathcal{H}\) represent in line 104? Line 242 (iv) could be reformulated for clarity. In the proof of Theorem 1, between lines 15 and 16, the second term is missing the widehat notation for \(\mathcal{L}\), and the same issue occurs on line 16. Additionally, line 16 contains the phrase "the upper bound of in terms of," which needs correction. In the proof of Lemma 1, the second equality should be an inequality. In the supplementary material, after line 35, in the second math line, the first term after the inequality is missing parentheses for the expectation over \(\mathcal{M}\). For Proposition 3 in the supplementary material, although the result is derived using the KKT conditions, more details should be provided since this is one of the paper's key results.