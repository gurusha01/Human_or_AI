This paper introduces a "reviewer" module into attentive recurrent encoder-decoder networks. The core idea is that the attentive decoder can leverage global summary vectors—termed "facts" in the paper—for enhanced performance in tasks like image and source code captioning. After the encoder processes a sequence, the reviewer module performs T_{r} additional passes over the encoded sequence, incrementally adding new vectors to the accumulated set of facts F. The reviewer can be trained either through an auxiliary task, such as predicting the presence of vocabulary words, or in an end-to-end manner for decoding. Variants of the encoder-reviewer-decoder (ERD) framework are demonstrated to outperform the baseline attentive encoder-decoder and are competitive with state-of-the-art models for image captioning. The proposed architectural modification to attentive encoder-decoder networks is well-motivated and seems to provide a performance improvement in image captioning. However, it remains unclear how much of the observed benefit stems from the increased model capacity versus the specific mechanism of generating global "fact" vectors prior to decoding. Overall, this approach appears to offer a marginally better solution, though it does not represent a transformative advancement. Detailed comments/questions: - Does discriminative supervision also improve the performance of the attentive encoder-decoder? - How does the ERD compare to an attentive encoder-decoder with additional layers? - Currently, all facts are derived solely from the image and potentially guided by a discriminative task. Could alternative methods be explored to extract a set of factors, such as leveraging a knowledge base or incorporating other sources beyond the image, e.g., prior knowledge about objects in the scene and their common-sense relationships?