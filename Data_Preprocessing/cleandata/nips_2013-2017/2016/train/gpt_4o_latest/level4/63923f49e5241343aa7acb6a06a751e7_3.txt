This paper establishes learning bounds for hypothesis classes with logarithmic entropy numbers under heavy-tailed loss functions. The authors demonstrate that fast rates (approaching O(n^{-1})) can be achieved if the r-th moment of the supremum loss over all hypotheses is integrable and a novel 'multiscale Bernstein condition' is satisfied. The paper contributes new and significant results to the growing area of fast rates for unbounded and heavy-tailed losses. Existing results in empirical process theory typically assume bounded or sub-Gaussian losses, making the heavy-tailed case substantially more challenging. The presented results appear to be both rigorous and novel, leveraging work by Sara van de Geer and collaborators on concentration inequalities for unbounded empirical processes. The material is highly technical, and I suggest moving additional content to the appendix to allow for a more detailed discussion of key points. 
In particular, the multiscale Bernstein condition is both novel and central to the results, but its intuition and necessity remain somewhat unclear. While the k-means example is helpful in illustrating the utility of the approach, it does not fully clarify the distinction between the microscopic and macroscopic scales discussed by the authors. A simple example to elucidate this distinction would be valuable. Additionally, it is worth noting that Audibert (Fast Learning Rates..., Annals of Statistics, 2009) demonstrates that a standard Bernstein condition suffices to achieve fast rates, even for unbounded and heavy-tailed losses, when using a pseudo-Bayesian estimator with online-to-batch conversion. In contrast, the authors of this paper establish uniform convergence rates for the entire class {\cal F}, which ensures that empirical risk minimization (ERM) achieves these rates. This approach differs from Audibert's, which relies on a randomized estimator and evaluates error in expectation over the randomization. However, Audibert's results raise the question of whether the multiscale Bernstein condition is strictly necessary. Some informal discussion on this point would be helpful—do the authors believe the condition could be avoided? The authors should also consult Audibert's paper. Note that improper learners, which select hypotheses outside the assumed class, often achieve better rates, but Audibert's method involves only randomized estimators.
There is also a recent paper by Mehta and Grunwald (Fast Rates for Unbounded Losses, Arxiv, 2016) that presents results similar in spirit, though based on different conditions. Instead of requiring moments of the envelope function, they introduce a 'witness condition,' and instead of the multiscale Bernstein condition, they use the v-PPC condition. While these conditions are equivalent to the standard Bernstein condition for bounded losses, they diverge significantly for unbounded losses. A discussion on the relationship between these conditions and the authors' results would be valuable, though I acknowledge that this may be challenging given the differences in proof techniques.
Additionally, I would like to see an explanation of why the authors' approach does not immediately extend to polynomial entropy numbers. Since the proofs use chaining, it seems plausible that the method could handle larger entropy numbers. The conclusion briefly mentions that such an extension is expected (though the term 'polynomial' should be corrected to 'logarithmic' there), but the paper does not explain why this generalization is non-trivial. Furthermore, it should be explicitly noted that the in-probability results scale polynomially, rather than logarithmically, with \delta. While this is likely unavoidable for heavy-tailed losses, it differs from standard results in learning theory—see also Sabato and Hsu (2015). 
Despite these points, I emphasize that the main results are distinct from those of Audibert, Grunwald & Mehta, and Sabato & Hsu. Given the difficulty of the problem and the novelty of the contributions, I recommend acceptance.
Minor Issues:  
- The abstract could be shortened.  
- The term 'weak-tailed' (beneath Lemma 2.1) is non-standard.  
- Section 3.4: \gamma-stochastic mixability should be 1-\gamma stochastic mixability (per Van Erven et al. 2015; I did not verify the Mehta & Williamson paper).  
- Theorem 4.2: "fine" should be "finite," and "minimizer" should be "minimizes."  
- References: Avoid overuse of "et al." where inappropriate.