The authors employ variance reduction techniques such as SVRG and SAGA to introduce the first linearly convergent algorithms for saddle point problems. Given that the standard convex minimization framework does not extend to saddle point problems, the primary contribution of this work lies in leveraging monotone operators to establish convergence. This approach has the added benefit of being applicable to a broader class of problems, including variational inequality problems. By framing the saddle point problem, the proposed method is capable of addressing non-separable loss functions and regularizers. The paper thus introduces the first algorithms achieving linear convergence rates for saddle point problems, a significant advancement as it enables the handling of non-separable loss functions and regularizers, which are prevalent in various machine learning models. The proof strategy, which interprets saddle point problems as finding zeros of a monotone operator, is both innovative and potentially far-reaching in its applicability. Additionally, the authors provide simpler proofs for algorithms like FB splitting and its stochastic variant. The proposed algorithms with non-uniform sampling demonstrate competitive performance, often surpassing prior methods. Overall, the paper makes a meaningful contribution to the field, particularly through its novel analytical approach. 
However, since the primary novelty lies in the proof technique (as the algorithms themselves are relatively straightforward extensions of existing methods), the authors might consider elaborating on the proof technique by including a proof sketch in the main paper, space permitting. The paper is generally well-written and clearly presented, though certain aspects could be clarified further. For instance:  
1. What specific form of non-uniform sampling is employed for problems beyond bilinear saddle-point problems, i.e., what is \(\pi\)?  
2. How is the step size \(\sigma\) determined in the experiments?  
3. Do the authors have any insights into why non-uniform sampling appears critical for saddle point problems but not necessarily for convex minimization problems? A discussion on this point could enhance the paper.  
Minor comments:  
1. Typo on line 78: "k" should be "d".  
2. Typo on line 121: there is an extra \(y_{t-1}\) term in the second equality.