The paper introduces several novel findings in the context of decentralized no-regret dynamics for smooth games. The authors propose a new concept termed "Low Approximate Regret," which extends classical algorithms in this domain. Additional contributions include results for various feedback mechanisms, the bandit setting, and population games. These findings build upon prior work, particularly SALS15 and LST16, and are both significant and relevant to the community. The primary contributions—improved convergence bounds across different feedback settings—are achieved through the introduction of the Low Approximate Regret property, which quantifies the gap between the approximate (1-epsilon) cost incurred and the comparator. The authors demonstrate that this property holds for several well-known learning algorithms and derive regret bounds based on it.
A potential drawback of the paper is its density of content, which leads to most derivations being relegated to the appendix, with only key points summarized in the main text. While this approach makes the paper more accessible and facilitates contextualization with related work, it also results in frequent transitions between different settings and topics, which can feel abrupt. Additionally, although not mandatory, experimental validation of the theoretical results would enhance the paper, especially since SALS15 included practical applications. The paper also concludes somewhat abruptly without a more detailed summary or discussion.
One clarification is needed regarding terminology: the authors use "realized feedback" and later "full information feedback" to seemingly refer to the same concept. Consistent terminology should be maintained, or the distinction between these terms should be clarified if they are not identical. Furthermore, in the bandit setting, the authors should explicitly differentiate between the 'expected Low Approximate Regret' in Lemma 4 and the Low Approximate Regret defined in Equation (1).