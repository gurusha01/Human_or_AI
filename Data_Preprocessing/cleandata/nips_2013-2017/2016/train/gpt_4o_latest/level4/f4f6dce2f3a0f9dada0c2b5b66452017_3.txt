The paper introduces a method to accelerate tensor CP decomposition using ALS by approximately solving each least squares problem through leverage score sampling. The authors leverage the structure of the design matrix in each least squares problem (a Khatri-Rao product of the tensor's other modes) to efficiently sample rows based on a (bound on) their leverage scores, enabling the solution of each least squares problem without explicitly forming the KR product. The paper establishes theoretical guarantees on the number of samples required to achieve an additive approximation for the least squares problem. Since errors do not compound across ALS iterations, the approximation requirements are relatively mild, and experimental results demonstrate up to 30x speedups with minimal or no loss in accuracy. The paper's ideas and contributions are commendable, and I look forward to seeing similar speedups applied to other contexts, given ALS's broad utility. While sampling proportionally to leverage scores is not entirely novel, this paper makes significant strides on the application side, meeting the NIPS standard. However, the presentation could be significantly improved by ensuring that all symbols (e.g., R, n, etc.) are defined before use and addressing the typos listed below. Additionally, the claim on line 160 is not immediately clear and requires either a proof or a proper citation.  
Typos:  
- Line 5: the the Khatri-Rao -> the Khatri-Rao  
- Line 13: significantly speedups -> significant speedups  
- Line 22: from the tensor data -> from tensor data  
- Line 25: has became -> has become  
- Line 26: tensor analytic -> tensor analytics  
- Line 52: SPASL -> SPALS  
- Line 62: approximates -> approximating  
- Line 149: \tau{i,j}^{A\otimes B} -> \tau{i,j}^{A \bigodot B}  
- Line 163: matrix -> matrices  
- Line 202: spare -> sparse  
- Line 202: the the -> the