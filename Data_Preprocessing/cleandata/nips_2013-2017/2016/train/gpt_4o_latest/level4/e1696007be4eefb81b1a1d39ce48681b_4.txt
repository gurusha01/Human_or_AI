The manuscript builds on the observation that, under the assumption of normally distributed predictors, the regression coefficients of a GLM with a canonical link function are proportional to the corresponding OLS coefficients. The authors leverage this property to propose a computationally efficient approach for fitting GLMs. Specifically, they estimate the coefficients using OLS and subsequently determine the proportionality constant through a univariate root-finding procedure. They further demonstrate that this approximation holds reasonably well beyond the Gaussian setting and derive bounds on the discrepancy between GLM coefficients and their scaled OLS counterparts. The central idea of the paper is both intriguing and, to the best of my knowledge, novel. While Sections 1 to 3 are exceptionally well-written, the latter half of the paper appears somewhat rushed. It would be beneficial to clarify early on that the proposed method is applicable only when the canonical link is employed (currently mentioned at line 92). However, the statement, "Motivated by the results in the previous section, we design a computationally efficient algorithm for any GLM task that is as simple as solving the least squares problem; it is described in Algorithm 1," suggests otherwise. The use of "any" implies that the canonical link is not a requirement. This should be clarified.
In the right panel of Figure 1, it may be more informative to plot the relative accuracy of the proposed method, specifically MSE(SLS)/MSE(MLE). Additionally, it is unclear whether sub-sampling or the full sample was used for SLS in this figure. In Figure 1, the STS method performs consistently worse than MLE in terms of accuracy. Given the Gaussian design and Proposition 1, it is unclear why this occurs. Could this be attributed to additional variance introduced by estimating the constant \( c \)? The decision to employ sub-sampling here is also somewhat puzzling. Since the STS method is applicable regardless of sub-sampling, its use complicates the interpretation of results in Section 5. Specifically, it becomes unclear how much of the observed speed-up and accuracy is attributable to STS versus sub-sampling.
At line 204, the authors state, "On the other hand, each of the MLE algorithms requires some initial value for \(\beta\), but no such initialization is needed to find the OLS estimator in Algorithm 1. This raises the question of how the MLE algorithms should be initialized, in order to compare them fairly with the proposed method." Including Iteratively Reweighted Least Squares (IRLS) in this comparison would have been valuable, given its widespread use. Moreover, Wood (2006, p. 66) notes that a default initialization is available for IRLS. At line 215, the authors mention, "For each dataset, the minimum achievable test error is set to be the maximum of the final test errors, where the maximum is taken over all of the estimation methods." This approach could lead to an inflated test error if any method fails to converge. Have all methods successfully converged across all datasets? In Section 5, the authors use two simulated datasets. Are the results presented in Figure 2 and Table 1 based on a single run or averaged over multiple runs? A minimum of several runs is necessary to provide confidence in the reported outcomes. Figure 2 is overly dense; space could be conserved by removing redundant legends and axis labels. The legend lines are also very small. Additionally, it would be helpful to note in the caption that STS does not start at zero on the x-axis due to the fixed cost incurred by OLS. Lines 253–257 are somewhat unclear—what is the key takeaway from Corollary 1? It is not immediately evident how the lasso results relate to unpenalized GLM regression. Corollary 1 assumes \(\beta > \eta / s\); the meaning and realism of this assumption should be clarified. Similarly, the choice of \(\lambda = \eta / (c \cdot s)\) warrants explanation. Additionally, \(\lambda_{\text{min}}\) appears undefined in Proposition 2. While it is defined in the Appendix, a brief explanation should also be included in the main text.
Minor Points:
- Line 46: The statement, "For logistic regression with Gaussian design (which is equivalent to Fisher's discriminant analysis)," is questionable. Friedman et al. (2008, p. 127) note that these methods differ significantly, even under Gaussian design. In particular, discriminant analysis is more efficient in this case, as it explicitly leverages the normality of the predictors.
- Line 161: The phrase, "that we can attain up to a cubic rate of convergence (by, for instance, using Hayley's method)," is repetitive.
- Line 296: The statement, "Another interesting line of research is to find similar proportionality relations between the parameters in other large-scale optimization problems such as support vector machines. Then the problem complexity can be significantly reduced as in the case of GLMs," should be rephrased. The latter sentence should be conditional on the former, as it currently reads as an assertion.
References:
- Wood, Simon. Generalized Additive Models: An Introduction with R. CRC Press, 2006.
- Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Elements of Statistical Learning. 2nd Edition. Springer, Berlin: Springer Series in Statistics, 2008.