This paper addresses the problem of approximate low-rank tensor decomposition. Given a 3-dimensional tensor \( T \) and a target rank \( R \), the objective is to identify families of vectors \( \{ar\}, \{br\}, \{cr\} \) for \( r = 1, \dots, R \) such that \( T \) can be closely approximated by \( \sum{r=1}^R ar \otimes br \otimes cr \), where \( \otimes \) denotes the outer product. The matrices \( A, B, C \) are then defined with \( ar, br, cr \) as their respective columns. The approximation error is quantified using the Frobenius norm. The paper focuses on accelerating the Alternating Least Squares (ALS) algorithm for solving this problem by leveraging statistical leverage score sampling. Specifically, ALS iteratively fixes two of the matrices \( A, B, C \) and optimizes the third to minimize the Frobenius norm error, cycling through the three matrices as the variable to optimize.
The key insight of the paper is that once two of the matrices are fixed, the optimization reduces to a standard least squares problem involving a matrix \( X \) of size \( n^2 \times n \). Results from randomized linear algebra suggest that sampling \( O(n \log n) \) rows of \( X \) based on its statistical leverage scores suffices for efficient approximation. However, directly computing the leverage scores of \( X \) is computationally expensive due to its large size. The paper's main contribution is a novel lemma (Theorem 3.2) that exploits the structure of \( X \), which is the Khatri-Rao product of the two fixed matrices, to derive upper bounds on the leverage scores of \( X \) using the leverage scores of the fixed matrices. This enables a faster computation of approximate leverage scores, leading to an accelerated variant of ALS.
As the "LS" in ALS suggests, each iteration of the algorithm involves solving a least squares regression problem. The primary novelty of this work lies in the efficient approximation of leverage score bounds for the underlying matrix \( X \), facilitated by its special structure. Once Theorem 3.2 is established, the remaining steps follow standard techniques. This observation allows the proposed method to achieve competitive performance compared to the state-of-the-art approach [37], as demonstrated in Figure (a) on page 8. The simplicity of the observation and its practical implications are clear strengths of the paper.
However, there are two main limitations: (1) the empirical results show the method is comparable to [37] rather than decisively outperforming it, and (2) Figure (b) lacks an explanation for why [37] was not included in the comparison. Additionally, the non-monotonic behavior of SPALS(\(\alpha\)) in Figure (a) raises questions.
Minor Comments:
- Typos: 
  - "nearly optimality" → "near optimality"
  - "while approximates the" → "while approximating the"
  - "for large tensor" → "for large tensors"
  - "each rank-1 components are" → "each rank-1 component is"
  - "It is a challenging tasks" → "It is a challenging task"
  - "one of the most powerful tool" → "one of the most powerful tools"
  - "we provide the efficient" → "we provide an efficient"
  - "In the remaining of this section" → "In the remainder of this section"
  - "toolset for estimating the statistical leverage score" → "toolset for estimating the statistical leverage scores"
  - "the statistical leverage scores of the i-th row" → "the statistical leverage score of the i-th row"
  - "score of certain row" → "score of a certain row"
  - "first inequality is because of" → "first inequality is because"
  - "equals to \( R \)" → "equals \( R \)"
  - "an rank-1" → "a rank-1"
  - "separate the calculation to two parts" → "separate the calculation into two parts"
  - "evaluating former expression" → "evaluating the former expression"
  - "second term is spare" → "second term is sparse"
  - "by the leverage score of the design" → "by the leverage scores of the design"
  - "not directly utilize" → "not directly utilizing"
  - "requires an one-time" → "requires a one-time"
  - "routines fairly recent" → "routines was fairly recent"
  - "moves the non-zeros around" → "moves the non-zeroes around"
  - "considered as the data" → "considered as data"
  - "piecemeal invocation" → "piecemeal invocations"
  - "each rank-1 components" → "each rank-1 component"
- Page 2, line 80: The explanation "(i,j) also represents the index \( i + Ij \) between 1 and \( IJ \)" is unclear. Please clarify its meaning.
- Page 4, lines 128–129: The claim "Its optimality in solving ... of linear regression" is not straightforward. The derivation of the upper bound likely requires matrix concentration results, such as matrix Bernstein, matrix Chernoff, or the non-commutative Khintchine inequality.
- Page 4, line 133: The term \( O(r \log n) \) introduces \( n \), but it is unclear why \( n \) appears in this context. Please elaborate.
- Figure (a), page 8: The definition of "error" is unclear. Additionally, why is the SPALS(\(\alpha\)) error not monotonic with respect to \(\alpha\)? What are the units for time and error?
- Page 8: Does \( n = 1000 \) imply that the tensor is \( 1000 \times 1000 \times 1000 \)? Please confirm whether all three dimensions are \( n \).