The paper introduces a game-theoretic model for cooperative learning in a Human-Robot (H-R) interaction context. It formalizes an inverse reinforcement learning (IRL) problem within the framework of a cooperative two-player game with partial information. The study addresses an intriguing problem of cooperative learning in H-R setups and demonstrates how the proposed approach aligns with the current state-of-the-art (SoA). The paper is well-structured and clearly written; however, concerns remain regarding the overall contribution, particularly in terms of technical rigor and novelty. The key points supporting this assessment are as follows:  
• The DBE assumption appears overly strong, raising doubts about its applicability to practical scenarios. Specifically, the strict dependence of the human's (H's) reward on the success of the robot's (R's) learning (via H's internal reward function) imposes a significant influence on R's learning, effectively creating a positive feedback loop.  
• The nature of cooperation in the described H-R scenario, where H and R jointly solve a single decision-making problem, is not sufficiently clarified. It remains unclear how this form of cooperation impacts the reward function, and this aspect warrants further discussion.  
• The proof of Theorem 1, provided in the supplementary material, is either incorrect or inadequately explained. For instance, the statement "R can also simulate C" appears to contradict Definition 2, particularly the definition of C's actions, which include a decision rule for H.  
• The proof of Theorem 3 in the supplementary material lacks mathematical soundness.  
• The role of the feature function is not clearly articulated, leaving its significance ambiguous.  
• Minor issues: Some symbols and notations are not properly introduced. For example, \(\delta_{\Theta}\) is neither explained in the main text (p.2, line 217) nor in the supplementary material (p.2, Corollary 1). Similarly, \(\eta\) (page 7, line 299) is not defined.