This paper leverages a straightforward proportionality relationship between ordinary least squares (OLS) problems and Generalized Linear Models (GLMs). By first solving the OLS problem and subsequently estimating the proportionality constant, the paper proposes faster solvers for GLMs. The theoretical analysis demonstrates that, under reasonable assumptions, this approach can accurately estimate GLM parameters in the regime where the sample size \( n \) is much larger than the number of predictors \( p \). The reduction of GLM to OLS is intriguing and noteworthy. However, for what class of regularizers (Section 3.1) does this proportionality relationship remain valid? Additionally, which classes of link functions are particularly well-suited to this reduction? 
Regarding Section 5, the reported speedup results are expected. It is intuitive that an OLS-based approach combined with a single-step estimation of the proportionality constant would outperform the iterative optimization of the GLM objective function in terms of computational efficiency. A more compelling question, however, is how far this methodology can be extended beyond the exact equivalence established in Proposition 1. In Figure 2, it is surprising that the SLS solution consistently performs as well as, if not better than, the GLM solution. This observation is striking and raises the question: does this suggest that, for these datasets and the given sample size \( n \), the error terms (as defined in Eqns 10 and 14) are negligibly small?