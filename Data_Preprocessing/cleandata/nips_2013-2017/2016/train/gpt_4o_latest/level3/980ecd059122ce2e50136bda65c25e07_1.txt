This paper addresses the critical issue of neural network robustness to adversarial examples by introducing formal metrics and a novel linear programming-based approach for their approximation. The authors propose two new metrics, adversarial frequency and adversarial severity, which provide complementary insights into robustness. These metrics are applied to evaluate the robustness of neural networks on MNIST and CIFAR-10 datasets, and the results reveal limitations in existing adversarial robustness methods, including overfitting to specific adversarial examples. The paper further demonstrates that the proposed approach not only improves robustness according to the new metrics but also enhances performance on previously established metrics.
Strengths:
1. Technical Rigor: The paper is technically sound and provides a solid theoretical foundation for its proposed metrics. The use of linear programming to approximate robustness metrics is innovative and well-justified.
2. Novel Metrics: The introduction of adversarial severity and frequency is a significant contribution, as these metrics capture different aspects of robustness and allow for a more nuanced evaluation of neural networks.
3. Empirical Validation: The experimental results on MNIST and CIFAR-10 are thorough, demonstrating the effectiveness of the proposed approach in identifying overfitting in prior methods and improving robustness.
4. Scalability: The authors address computational challenges by focusing on the local linear regime of ReLU-based networks, making the approach scalable to larger networks like NiN.
5. Critical Insights: The paper highlights an important issue—overfitting to specific adversarial examples generated by certain algorithms—and provides empirical evidence to support this claim.
Weaknesses:
1. Clarity on Approximation: While the paper discusses the convex restriction used to approximate robustness metrics, it is unclear whether this leads to an exact convex reformulation or merely an approximation. This ambiguity should be clarified.
2. Limited Practical Impact on CIFAR-10: Although the approach improves robustness on MNIST, its impact on CIFAR-10 is modest. This suggests that the method may not generalize well to more complex datasets, and further work is needed to address this limitation.
3. Experimental Scope: The experiments focus primarily on adversarial robustness but do not explore broader implications, such as the trade-offs between robustness and accuracy or the impact on other datasets and architectures.
4. Related Work: While the paper references prior work extensively, it could benefit from a more detailed comparison with recent advancements in adversarial robustness, particularly those published in recent NeurIPS proceedings.
Arguments for Acceptance:
- The paper introduces novel, well-defined metrics that advance the state of the art in evaluating neural network robustness.
- The proposed linear programming-based approach is innovative and provides a more accurate estimation of robustness metrics compared to prior methods.
- The work identifies and addresses a critical issue in adversarial robustness research—overfitting to specific adversarial examples.
Arguments Against Acceptance:
- The practical impact of the proposed method on more complex datasets like CIFAR-10 is limited, raising questions about its generalizability.
- The paper lacks clarity on whether the convex restriction leads to an exact or approximate solution, which could affect the validity of the results.
Recommendation:
Overall, this paper is a strong contribution to the field of adversarial robustness. While there are some limitations, particularly in its practical impact on complex datasets, the novelty of the metrics and the rigor of the approach make it a valuable addition to the conference. I recommend acceptance as a poster presentation, with the expectation that the authors address the noted weaknesses in a future revision.