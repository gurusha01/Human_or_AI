The paper introduces a novel algorithm for generating adversarial examples constrained to the linear region of ReLU activations in neural networks, with the goal of better estimating robustness metrics for neural networks. The authors propose two new statistics—adversarial frequency and adversarial severity—to measure robustness, and they demonstrate their algorithm's effectiveness on MNIST and CIFAR-10 datasets. The key innovation lies in encoding robustness as a linear program and restricting the search for adversarial examples to convex regions where the network behaves linearly. This approach is shown to produce more harmful adversarial examples (in terms of L∞ norm) compared to prior methods, and the authors also highlight the tendency of existing robustness-improving techniques to overfit to specific adversarial algorithms.
Strengths:  
1. Novelty and Technical Contribution: The paper proposes a unique approach to adversarial example generation by constraining the search to the linear region of ReLU activations. This is a meaningful contribution as it aligns with the known linear vulnerabilities of neural networks. The iterative optimization for solving the linear program is also a practical improvement, offering significant speed-ups.
2. Metrics for Robustness: The introduction of adversarial frequency and severity as robustness metrics is a valuable addition to the field, providing a more nuanced understanding of robustness beyond simple accuracy on adversarial examples.
3. Experimental Validation: The experiments on MNIST demonstrate that the proposed algorithm generates more harmful adversarial examples and provides better robustness estimates compared to the baseline L-BFGS-B algorithm. The results also highlight the overfitting issue in robustness-improving methods, which is an important insight.
Weaknesses:  
1. Theoretical Concerns: While the algorithm focuses on the linear region of ReLU activations, adversarial examples can exist outside this region. The paper does not provide guarantees that the proposed constraints yield the minimum perturbation, limiting the theoretical rigor of the approach.
2. Experimental Generality: The experiments are primarily conducted on simple datasets (MNIST and CIFAR-10) and non-standard architectures (e.g., LeNet). The results on CIFAR-10 show only marginal improvements in robustness, and the scalability issues (15 seconds per example on 8 CPUs) limit applicability to larger datasets like ImageNet.
3. Baseline Comparisons: The CIFAR-10 experiments lack baseline comparisons, making it difficult to evaluate the relative performance of the proposed method in this setting.
4. Clarity Issues: The definition of \( x_\star \) is missing when first introduced, and the explanation of [3], which could better motivate the constraints used in the algorithm, is insufficiently detailed. Additionally, the assumption that the L∞ norm is the best measure of perceptibility for adversarial perturbations is not justified.
5. Computational Efficiency: The method is computationally expensive, particularly for larger networks like NiN, which limits its practical utility.
Arguments for Acceptance:  
- The paper addresses a significant problem in adversarial robustness and introduces a novel approach with promising insights.  
- The proposed metrics (adversarial frequency and severity) are valuable contributions that could influence future research.  
- The experimental results on MNIST demonstrate the effectiveness of the method in generating stronger adversarial examples and identifying overfitting in robustness-improving techniques.
Arguments Against Acceptance:  
- The lack of theoretical guarantees and reliance on restrictive assumptions (e.g., linearity of ReLU regions) weakens the generality of the approach.  
- The experiments are limited to simple datasets and architectures, with scalability issues preventing application to more complex settings.  
- Missing baseline comparisons for CIFAR-10 and marginal improvements in robustness raise concerns about the method's practical impact.  
- Clarity issues and unjustified assumptions reduce the accessibility and rigor of the paper.
Recommendation: Weak Reject. While the paper introduces interesting ideas and metrics, the limited scope of experiments, scalability concerns, and lack of theoretical guarantees hinder its impact. Addressing these issues in a future iteration could make it a stronger contribution.