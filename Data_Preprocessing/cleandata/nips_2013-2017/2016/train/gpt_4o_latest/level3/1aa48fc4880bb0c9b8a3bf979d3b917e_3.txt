This paper addresses the challenging problem of solving strongly convex-concave saddle-point problems with a finite-sum structure, extending stochastic variance reduction methods (SVRG, SAGA) and Catalyst acceleration techniques to this domain. The authors provide a novel and technically non-trivial contribution by adapting these methods, which were originally designed for convex minimization, to saddle-point problems. This extension is significant as it broadens the applicability of variance reduction techniques to a wider class of problems, including variational inequalities and machine learning tasks with non-separable losses or regularizers.
Strengths:
1. Novelty and Technical Depth: The extension of variance reduction methods to saddle-point problems is a meaningful innovation. The use of monotone operator theory for convergence analysis is particularly noteworthy, as it generalizes the applicability of the proposed algorithms beyond saddle-point problems.
2. Comprehensive Complexity Analysis: The paper provides detailed complexity bounds for various algorithmic configurations, including individual and factored splits, as well as uniform and non-uniform sampling strategies. This level of rigor is commendable and helps clarify the theoretical underpinnings of the proposed methods.
3. Empirical Validation: The experiments convincingly demonstrate the importance of non-uniform sampling and acceleration, showing that these techniques outperform competitive batch algorithms, particularly for ill-conditioned problems.
4. Alignment with Convex Minimization Insights: For bilinear saddle-point problems, the results align well with known insights from convex minimization, reinforcing the validity of the proposed methods.
Weaknesses:
1. Proximal Operator Cost: A significant limitation is the \(O(n + d)\) cost of computing the proximal operator per iteration, which can negate the theoretical complexity improvements, particularly when \(n \gg d\).
2. Mixed Empirical Results: Accelerated SVRG with non-uniform sampling does not consistently outperform batch algorithms in all settings, especially when \(n \gg d\). This raises questions about the practical utility of the proposed methods in such scenarios.
3. Factored Splits: The factored splits used in simulations may lead to worse complexity compared to individual splits, which could limit the generalizability of the results.
4. Theoretical Clarity: While the paper claims theoretical advantages for variance reduction in saddle-point problems, these advantages are not as clearly articulated or demonstrated as they are for convex minimization. The dependence on \(n\) and \(d\) in the complexity bounds, as well as the conditions under which accelerated SVRG outperforms batch methods, require further clarification.
Recommendation:
This paper makes a solid theoretical and empirical contribution to the field of optimization and machine learning by extending variance reduction techniques to saddle-point problems. However, the practical limitations (e.g., proximal operator cost) and the lack of clarity in some theoretical claims temper its impact. I recommend acceptance, provided the authors address the concerns about theoretical tightness and clarify the conditions favoring their methods over batch algorithms. This work is likely to inspire further research in scalable optimization for structured machine learning problems.