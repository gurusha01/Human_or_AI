The paper presents a novel approach to unsupervised learning of structured predictors by introducing a two-layer model with latent structure and a jointly convex training objective. The authors address the inherent nonconvexity of the problem using a semi-definite programming (SDP) relaxation, enabling global optimization. The model is evaluated on tasks such as transliteration and image inpainting, demonstrating superior empirical performance compared to locally trained methods. The work builds on prior research in structured prediction and deep learning, incorporating techniques like maximum a-posteriori (MAP) inference and convex relaxations, and contributes to advancing the state of the art in unsupervised learning with structured outputs.
Strengths:
1. Technical Novelty: The paper introduces a significant innovation by convexifying a two-layer model with latent structure, which is a challenging problem in machine learning. The use of SDP relaxation to address nonconvexity is well-motivated and technically sound.
2. Empirical Results: The proposed method demonstrates superior performance over local training methods in transliteration and image inpainting tasks. This empirical validation highlights the practical utility of the approach.
3. Theoretical Contributions: The characterization of the extreme points of the feasible region as low-rank solutions is an interesting theoretical result, adding depth to the paper.
4. Clarity in Applications: The paper provides concrete examples, such as graph matching and temporal models, to illustrate the flexibility and applicability of the framework.
Weaknesses:
1. Questionable Assumptions: The reliance on an exponential family-based conditional model and the replacement of the log-partition function with an upper bound are strong assumptions that may limit the generalizability of the approach. The authors do not adequately justify these choices.
2. Unclear Feasibility: The feasibility of the SDP-relaxed solution to the original nonconvex problem is not sufficiently addressed. It is unclear whether Theorem 1 guarantees that the relaxation provides an exact solution to Problem (15).
3. Relaxation Validity: The reasonability of using a relaxed solution as an upper bound for the optimization problem is questionable, and the authors do not provide a rigorous analysis of the potential gap between the relaxed and original problems.
4. Clarity of Presentation: While the paper is dense with technical details, some sections, particularly those on optimization and theoretical guarantees, are difficult to follow. A more intuitive explanation of the SDP relaxation and its implications would improve accessibility.
Pro and Con Arguments for Acceptance:
Pros:
- Innovative approach to a challenging problem.
- Strong empirical results demonstrating practical value.
- Theoretical contributions to convex relaxation and low-rank characterization.
Cons:
- Questionable assumptions that may limit applicability.
- Insufficient clarity on the feasibility and exactness of the relaxation.
- Dense presentation that may hinder understanding for a broader audience.
Recommendation:
Overall, the paper makes a valuable contribution to the field of unsupervised learning with structured predictors. While the assumptions and clarity issues warrant further attention, the novelty and empirical success of the approach justify its acceptance. I recommend acceptance with minor revisions to address the concerns regarding assumptions and the feasibility of the relaxation.