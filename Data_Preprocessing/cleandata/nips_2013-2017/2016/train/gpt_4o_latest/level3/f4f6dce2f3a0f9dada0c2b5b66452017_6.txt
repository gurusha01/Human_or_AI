This paper introduces SPALS, a novel algorithm leveraging the spectral structures of the Khatri-Rao product to efficiently compute statistical leverage scores for row sampling in tensor CP decomposition. By exploiting these structures, the authors achieve sublinear time complexity per iteration, a significant advancement for large-scale tensor analytics. The work extends leverage score computation from matrices to tensors and demonstrates its applicability to CP decomposition tasks. Experimental results validate the method's efficiency and accuracy on both synthetic and real-world datasets, such as the Amazon review tensor.
Strengths
1. Technical Contribution: The paper presents a theoretically sound and computationally efficient algorithm for tensor CP decomposition, addressing a critical bottleneck in alternating least squares (ALS) methods. The use of statistical leverage scores for importance sampling is innovative and well-justified.
2. Scalability: The proposed SPALS algorithm achieves sublinear time complexity per iteration, which is crucial for handling large-scale tensors with billions of nonzeros.
3. Experimental Validation: Empirical results demonstrate significant speedups over existing methods, with competitive or improved accuracy. The experiments on both synthetic and real-world datasets provide a comprehensive evaluation.
4. Generalizability: The study hints at the potential applicability of the proposed techniques to other tensor-related tasks, such as stochastic gradient descent (SGD) and high-order singular value decomposition (HOSVD).
Weaknesses
1. Unclear Sampling Size: The size of sampled rows (\(\alpha r^2 \log^2 n\)) in Line 294 is not clearly explained. A more detailed discussion or justification would enhance clarity.
2. Incomplete Experimental Analysis: While the paper claims applicability to other tensor-related applications, such as SGD and HOSVD, no experimental evidence is provided to support these claims.
3. Performance Degradation: The authors do not explain the observed performance degradation of SPALS with increasing sampling size in Table (a) at \(nsr = 0.1\). This omission raises questions about the robustness of the method.
4. Presentation Issues: Table 1 on Page 8 lacks a caption, which hinders readability. Additionally, the grammar error in Section 7 ("Its worth noting that…") should be corrected to "It's worth noting that…".
Arguments for Acceptance
- The paper addresses a significant computational challenge in tensor analytics and provides a novel, theoretically grounded solution.
- The experimental results demonstrate the practical utility and scalability of the proposed method.
- The extension of leverage score computation from matrices to tensors is a meaningful contribution to the field.
Arguments Against Acceptance
- The lack of clarity regarding the sampling size and the unexplained performance degradation in Table (a) weaken the experimental analysis.
- Claims about broader applicability to other tensor-related tasks are not substantiated with experiments.
- Minor presentation and grammar issues detract from the overall clarity and polish of the paper.
Recommendation
I recommend acceptance with minor revisions. The paper makes a strong contribution to tensor decomposition and randomized numerical linear algebra. Addressing the concerns about sampling size, performance degradation, and experimental evidence for broader applicability would significantly strengthen the work.