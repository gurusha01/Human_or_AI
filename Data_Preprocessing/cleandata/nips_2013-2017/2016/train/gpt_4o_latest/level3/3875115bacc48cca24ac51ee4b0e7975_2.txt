This paper investigates the behavior of the Expectation-Maximization (EM) algorithm for estimating mean parameters in Gaussian mixture models (GMMs) under a simplified setting with fixed components, uniform weights, and identity covariance matrices. The authors present three significant negative theoretical results: (1) the existence of local maxima with arbitrarily lower likelihood values than the global maximum, (2) high-probability convergence of the EM algorithm (with random initialization) to suboptimal critical points, and (3) the extension of these negative results to the gradient variant of the EM algorithm. These findings challenge the conjecture by Srebro (2007) that the population log-likelihood of GMMs lacks spurious local maxima, even in favorable scenarios.
The paper's primary contribution lies in its rigorous theoretical analysis, which resolves an open question in the field and highlights the necessity of careful initialization for the EM algorithm. The authors justify their focus on simplified GMMs, given the technical complexity of the proofs, and their results provide valuable insights into the practical limitations of the EM algorithm. However, the scope of the analysis is restricted to one-dimensional cases, and it remains unclear whether the results generalize to higher dimensions. The authors should address this limitation explicitly or discuss the challenges involved in extending their findings.
The proofs are mathematically dense but insightful. That said, the explanation of Theorem 1 is too brief, particularly regarding the limits of likelihood function supremums over unbounded domains. Additionally, the paper's claim that the continuity of the likelihood function implies certain types of convergence requires more rigorous justification. The section on the gradient EM algorithm (lines 162â€“172) is also challenging to follow, and further clarification, especially for Equation (5), would enhance readability.
Strengths of the paper include its originality, as it addresses a longstanding open problem, and its significance, as it provides a deeper understanding of the EM algorithm's limitations. The results are likely to influence future research on initialization strategies and alternative optimization methods for GMMs. However, the paper's clarity could be improved, particularly in the presentation of proofs and technical details.
Arguments for Acceptance:
1. Resolves an open question in the field and contributes novel theoretical insights.
2. Highlights practical limitations of widely used algorithms, which is significant for both researchers and practitioners.
3. Introduces new proof techniques that may inspire further research in non-convex optimization.
Arguments Against Acceptance:
1. Restricted to one-dimensional cases, with insufficient discussion on generalization to higher dimensions.
2. Some proofs lack sufficient detail, particularly Theorem 1 and the discussion of likelihood function limits.
3. Certain sections, such as the gradient EM algorithm analysis, are difficult to follow and require additional clarification.
In conclusion, this paper makes a valuable theoretical contribution to the understanding of the EM algorithm in GMMs. While some aspects of clarity and generalization could be improved, the significance and originality of the results merit strong consideration for acceptance.