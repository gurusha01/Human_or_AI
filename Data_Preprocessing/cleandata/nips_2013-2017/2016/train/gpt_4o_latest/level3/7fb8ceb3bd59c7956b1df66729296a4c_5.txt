This paper investigates the geometrical properties of a nonconvex objective function for completing positive semidefinite (PSD) matrices. The authors prove that all critical points satisfying specific conditions are global minima, addressing a key theoretical gap in understanding why nonconvex optimization algorithms, such as gradient descent, succeed with arbitrary initialization in practice. The main theorem (Thm 2.3) establishes that critical points with zero gradient and Hessian bounded below by \(-\tau I\) are globally optimal. This result implies that saddle points exhibit Hessian curvature less than \(-\tau I\), aligning with prior work on strictly ridable saddle points and optimization techniques like cubic regularization and stochastic gradient descent (SGD).
The paper's contributions are significant. It rigorously excludes critical points with large coherence or large negative curvature from being local minima, leaving only global optima as valid critical points. The authors also provide a lower bound on the \(l_2\) norm of critical points satisfying optimality conditions (Lemma 3.8), further strengthening their results. The proof strategy is noteworthy for its generalizability, relying on "simple" inequalities that extend to partial observation cases, making it applicable to other statistical problems involving noisy or incomplete data.
Strengths of the paper include its theoretical rigor, clear connection to practical algorithms, and relevance to foundational problems in machine learning, such as matrix completion in recommender systems. The results advance the state of the art by eliminating the need for careful initialization, a common limitation in prior work. Additionally, the authors contextualize their findings within the broader literature, referencing related work on nonconvex optimization and matrix sensing.
However, the paper has some weaknesses. The chain of reasoning in the proofs is lengthy and could benefit from better organization. For example, Lemma C.1, which is crucial to the main argument, should be integrated into the main text for clarity. Additionally, the presentation could be improved by compacting the proofs and explicitly connecting technical definitions to prior work for accessibility to a broader audience. Minor corrections are also needed, such as updating Thm 3.2 to state \(\tau \leq 0.1p\) and clarifying Figure 1's \(l_\infty\) norm bound.
Arguments for Acceptance:
1. The paper addresses a fundamental theoretical question in nonconvex optimization with strong results.
2. The proofs are rigorous and generalizable, with potential applications beyond matrix completion.
3. The work is well-grounded in prior literature and has practical implications for widely used algorithms.
Arguments Against Acceptance:
1. The presentation of the proofs is verbose and could be streamlined for better readability.
2. Some technical details and figures require clarification or correction.
3. Accessibility to non-expert readers could be improved by connecting results more explicitly to prior work.
In conclusion, this paper makes a strong contribution to the theoretical understanding of nonconvex optimization and matrix completion. While the presentation could be improved, the results are significant and warrant acceptance.