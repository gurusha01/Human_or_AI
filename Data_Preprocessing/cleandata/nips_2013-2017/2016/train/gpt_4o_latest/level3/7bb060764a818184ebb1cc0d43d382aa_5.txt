This paper introduces a novel approach to dropout, proposing a distribution-dependent dropout for both shallow and deep learning. The authors present three key contributions: (1) a multinomial dropout that adapts sampling probabilities based on the second-order statistics of data, theoretically demonstrating faster convergence and smaller generalization error; (2) an evolutional dropout for deep learning that dynamically adjusts sampling probabilities to evolving layer distributions; and (3) theoretical risk bound analysis and empirical validation across multiple datasets. The work builds on prior research on dropout and optimization techniques, offering a theoretically grounded and computationally efficient alternative to standard dropout and batch normalization.
Strengths:
1. Theoretical Rigor: The paper provides a solid theoretical foundation for the proposed methods, including risk bound analysis and optimization insights. The analysis is well-structured and supports the claims of faster convergence and reduced generalization error.
2. Practical Contributions: The evolutional dropout is computationally efficient, leveraging mini-batch statistics to adapt sampling probabilities dynamically. This is particularly useful for deep learning, where layer distributions evolve over time.
3. Empirical Validation: The proposed methods are evaluated on multiple datasets for both shallow and deep learning, showing significant improvements in convergence speed and testing error compared to standard dropout.
4. Clarity: The paper is well-written and easy to follow, with clear explanations of the theoretical and practical aspects of the proposed methods.
Weaknesses:
1. Novelty Concerns: While the multinomial and evolutional dropouts are interesting extensions, their novelty may not fully meet the high standards of NIPS. The authors should clarify how their work significantly advances the state of the art compared to existing techniques like adaptive dropout or batch normalization.
2. Limited Experimental Scope: The evolutional dropout is only compared to batch normalization on CIFAR-10, leaving its broader effectiveness unverified. Additional experiments on diverse datasets and architectures would strengthen the claims.
3. Selective Reporting: The authors report only the best results for convergence speed on each dataset, which raises concerns about the robustness and generalizability of the proposed methods.
4. Minor Issues: The citation package is incorrectly used, with placeholders like "author ?" in Section 2. This oversight detracts from the paper's professionalism.
Recommendations:
- Clarify Novelty: In the rebuttal, the authors should emphasize how their work differs from and improves upon prior methods, particularly in terms of theoretical contributions and practical implications.
- Expand Experiments: Include comparisons with batch normalization and other baselines across more datasets to demonstrate the generalizability of evolutional dropout.
- Address Reporting Concerns: Provide a more comprehensive analysis of results, including average performance and variance, to address concerns about selective reporting.
- Fix Minor Errors: Correct citation issues and placeholders to improve the manuscript's presentation.
Decision:
While the paper presents a theoretically sound and practically useful contribution, concerns about novelty and experimental scope need to be addressed. Pending a strong rebuttal and additional experiments, this paper could make a meaningful contribution to the field.