The paper introduces a novel approach to addressing the sparsity of traditional visitation counts in large state-space reinforcement learning (RL) by proposing pseudo-counts derived from density estimation. This method generalizes count-based exploration to non-tabular settings, a significant step forward in tackling the exploration problem in RL. The authors provide asymptotic analysis to demonstrate the theoretical equivalence of pseudo-counts to traditional counts, and they integrate this approach into DQN and A3C algorithms. Empirical results on challenging Atari 2600 games, including MONTEZUMA'S REVENGE, showcase the effectiveness of pseudo-counts in improving exploration.
Strengths:
1. Novelty and Originality: The paper presents a novel method that bridges count-based exploration and intrinsic motivation, offering a unified perspective. The introduction of pseudo-counts as a generalization of visitation counts is an innovative contribution to the field.
2. Theoretical Rigor: The asymptotic analysis provides a solid theoretical foundation, demonstrating the equivalence of pseudo-counts to traditional counts and their connection to information gain.
3. Empirical Validation: The integration of pseudo-counts into DQN and A3C algorithms and their application to Atari games is compelling. The dramatic improvement in MONTEZUMA'S REVENGE, a notoriously difficult game, highlights the practical utility of the approach.
4. Clarity and Organization: The paper is well-written and logically structured, with clear explanations of the methodology, theoretical results, and empirical findings.
Weaknesses:
1. Comparison to Simpler Methods: While the paper demonstrates the effectiveness of pseudo-counts, it does not adequately compare its approach to simpler alternatives, such as partitioning the screen into chunks with color indicators. This omission leaves questions about the relative benefits of pseudo-counts unanswered.
2. Lack of Comparison to Model Uncertainty-Based Exploration: The reviewer notes that model uncertainty-based exploration methods, such as those presented at ICML, are not directly compared. This comparison would strengthen the case for pseudo-counts as a preferred method.
3. Equation Correction: The potential correction in equation (5), where \(N(x)\) should be \(N_a(x)\) to account for state-action dependency, suggests a minor oversight in the technical formulation.
4. Generality Across Domains: While the results on Atari games are impressive, the paper does not explore whether pseudo-counts generalize well to other RL domains, such as robotics or continuous control tasks.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a critical challenge in RL exploration, provides a novel and theoretically sound solution, and demonstrates significant empirical improvements.
- Con: The lack of comparison to simpler and alternative methods limits the ability to fully assess the practical advantages of pseudo-counts.
Recommendation: The paper is a strong contribution to the field of RL, particularly in exploration strategies. However, the authors should address the reviewer's concerns by including comparisons to simpler schemes and model uncertainty-based methods, as well as clarifying the potential correction in equation (5). With these improvements, the paper would make an even more compelling case for acceptance.