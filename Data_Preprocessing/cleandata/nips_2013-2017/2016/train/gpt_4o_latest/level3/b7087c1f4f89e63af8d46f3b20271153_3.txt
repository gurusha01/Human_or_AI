This paper presents a novel coreset-based method for feature selection in large-scale sparse matrices, addressing both theoretical and practical challenges in dimensionality reduction. The authors propose an efficient algorithm that constructs a small, weighted subset of rows (coreset) to approximate the sum of squared distances to any k-dimensional affine subspace. This approach is particularly significant for sparse, high-dimensional datasets, such as the Wikipedia document-term matrix, where existing methods fail due to memory and computational constraints. The authors provide rigorous theoretical guarantees on the size and runtime of the coreset, and experimental results demonstrate its effectiveness and scalability.
The proposed method builds on prior work in dimensionality reduction, including sensitivity sampling and sketching techniques, but advances the state of the art by constructing coresets that are independent of both the input size (n) and dimensionality (d). Unlike existing algorithms, which often fail for "fat" or square matrices, the authors' approach is deterministic and preserves sparsity, making it highly practical for real-world applications. The paper also benchmarks the algorithm against state-of-the-art methods, showing superior performance in terms of both accuracy and runtime, particularly for large-scale datasets.
Strengths:
1. Technical Soundness: The paper provides a solid theoretical foundation, with detailed proofs and guarantees for the coreset size and runtime. The proposed algorithm is both novel and technically rigorous.
2. Practical Relevance: The method addresses a critical bottleneck in handling large-scale sparse data, with applications to tasks like PCA and latent semantic analysis (LSA). The successful application to the entire English Wikipedia is a compelling demonstration of its scalability.
3. Clarity and Organization: The paper is well-written and logically structured, with clear explanations of the problem, methodology, and results. The inclusion of pseudocode and detailed experimental analysis enhances reproducibility.
4. Experimental Validation: The authors conduct thorough experiments on both synthetic and real-world data, comparing their method against baselines and demonstrating its advantages.
Weaknesses:
1. Minor Inconsistencies: There are some inconsistencies in the constraints of Definition 1, which should be clarified to avoid confusion.
2. Limited Discussion of Limitations: While the paper highlights the strengths of the method, a more explicit discussion of its limitations (e.g., potential challenges in parallelization or edge cases where the method may underperform) would strengthen the contribution.
3. Related Work Coverage: Although the related work section is comprehensive, it could benefit from a more explicit comparison to recent advances in randomized and approximate methods for dimensionality reduction.
Arguments for Acceptance:
- The paper addresses a long-standing open problem in coreset construction for PCA and provides a practical, scalable solution with strong theoretical guarantees.
- The experimental results convincingly demonstrate the method's superiority over existing approaches.
- The work has significant implications for both theoretical research and practical applications in machine learning and data science.
Arguments Against Acceptance:
- The minor inconsistencies in the definitions and constraints should be resolved to ensure clarity.
- The paper could provide a deeper discussion of its limitations and potential extensions.
Recommendation:
I recommend acceptance of this paper, as it makes a substantial contribution to the field of dimensionality reduction and provides a practical solution to a critical problem. Addressing the minor issues raised would further enhance its impact.