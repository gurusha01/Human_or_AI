This paper introduces a novel Tree-structured Reinforcement Learning (Tree-RL) approach for sequential object bounding box proposals, leveraging a deep Q-learning framework. The method incorporates global interdependencies between objects, enabling a single feedforward pass to localize multiple objects. By employing a tree-structured search strategy, the approach generates diverse near-optimal search paths, effectively handling objects of varying scales. Experimental results on the PASCAL VOC 2007 and 2012 datasets demonstrate that Tree-RL achieves competitive recall rates with significantly fewer proposals and state-of-the-art object detection performance when combined with a Fast R-CNN detector.
Strengths:
1. Novelty and Originality: The application of reinforcement learning to object proposal generation is innovative, particularly the tree-structured search scheme. This approach departs from traditional methods that treat object proposals as independent, instead modeling the process as a Markov Decision Process (MDP) to capture global interdependencies.
2. Performance: The method achieves competitive recall rates and state-of-the-art detection mAP on the PASCAL VOC datasets, outperforming Faster R-CNN in some scenarios despite using a shallower backbone (VGG-16). This demonstrates the practical utility of Tree-RL in object detection pipelines.
3. Efficiency: Tree-RL achieves comparable recall to Region Proposal Networks (RPN) with significantly fewer proposals, which is particularly advantageous for computational efficiency in downstream tasks.
4. Clarity: The paper is well-organized, with clear explanations of the MDP formulation, reward design, and tree search strategy. The experimental setup and results are presented comprehensively, with visualizations that effectively illustrate the method's strengths.
Weaknesses:
1. Limited Evaluation Range: The authors only evaluate performance up to 1500 proposals, which limits direct comparison with baseline methods like Selective Search and Edge Boxes that achieve near-perfect recall (~1.0) at higher proposal counts (e.g., 4000 boxes). This omission weakens the scientific rigor and leaves questions about the method's scalability.
2. Early Saturation: The recall performance of Tree-RL appears to saturate earlier than baseline methods at higher proposal counts. This could indicate limitations in the method's ability to cover all objects in complex scenes, particularly small or occluded ones.
3. Comparative Analysis: While the authors compare Tree-RL to several baseline methods, the discussion could benefit from deeper analysis of why Tree-RL underperforms at higher IoU thresholds in some cases and how it could be improved.
Suggestions for Improvement:
1. Extend the evaluation to include performance metrics beyond 1500 proposals to enable fairer comparisons with baseline methods at higher proposal counts.
2. Investigate and address the early saturation of recall performance, particularly for small objects.
3. Provide additional analysis on failure cases and propose potential solutions or future directions to address these limitations.
Recommendation:
Accept with Minor Revisions. The paper presents a novel and effective approach to object proposal generation, with significant contributions to the field of reinforcement learning and object detection. However, extending the evaluation range and addressing the noted weaknesses would strengthen the paper's impact and rigor.