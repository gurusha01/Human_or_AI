Review
This paper introduces a novel embedding algorithm, Riemannian Relaxation (RR), which aims to preserve the geometry of data embeddings by minimizing distortion using a push-forward Riemannian metric. The authors propose a gradient descent approach to optimize a non-convex loss function that directly measures deviation from isometry. The method is tested on synthetic datasets and applied to real-world data from the Sloan Digital Sky Survey (SDSS). While the paper presents an interesting approach to manifold learning, several critical issues limit its contribution.
Strengths:
1. Novelty: The use of a Riemannian metric to directly optimize geometric distortion is a fresh perspective in manifold learning. Unlike existing methods that optimize heuristic losses (e.g., pairwise distances or reconstruction errors), this approach explicitly targets isometry, which is theoretically appealing.
2. Flexibility: The algorithm accommodates embeddings in dimensions \(s > d\), which many existing methods cannot handle. This is a valuable extension for practical applications.
3. Scalability: The proposed PCS-RR extension for large or noisy datasets demonstrates adaptability to real-world challenges, such as high-dimensional data or approximate manifold assumptions.
Weaknesses:
1. Lack of Motivation for Riemannian Metric Superiority: While the authors claim that the Riemannian metric is superior for preserving geometry, they fail to provide sufficient theoretical or intuitive justification. A comparison with simpler metrics (e.g., pairwise distances) would strengthen the argument.
2. Overlooked Related Work: The paper does not adequately discuss existing gradient-descent-based embedding methods, such as t-SNE, which also optimize non-convex objectives. This omission undermines the claim of uniqueness and novelty.
3. Insufficient Experimental Validation: The experiments are limited to a small number of datasets, and the results do not convincingly demonstrate the algorithm's practical advantages over established methods like Isomap or Laplacian Eigenmaps. Additionally, runtime performance and scalability metrics are not reported.
4. Figures and Presentation: Figures 1 and 2 are poorly executed. For instance, the initialization in Figure 1 is too close to the true shape, making it unclear how the algorithm performs without prior information. The MSE/loss vs. sigma plot lacks sufficient data points and intervals. Figure 2 suffers from poor formatting and labeling, making it difficult to interpret the results.
5. Real-World Dataset Analysis: The application to SDSS data is incomplete and lacks meaningful scientific validation. While the authors claim improvements in isometry, the utility of these embeddings for downstream tasks (e.g., astrophysical analysis) is not demonstrated.
Pro and Con Arguments for Acceptance:
Pros:
- Novel approach to embedding with explicit geometric preservation.
- Flexibility to handle \(s > d\) embeddings.
- Scalable extension for large datasets.
Cons:
- Insufficient theoretical and experimental support for claims.
- Poor comparison with related work.
- Weak experimental design and presentation.
- Limited evidence of practical significance.
Recommendation:
While the paper introduces an interesting idea, the lack of rigorous motivation, insufficient experimental validation, and poor presentation significantly weaken its contribution. The authors should address these issues by providing stronger theoretical justification, testing on diverse and larger datasets, and improving the clarity of figures and results. At its current state, I recommend rejection but encourage resubmission after substantial revisions.