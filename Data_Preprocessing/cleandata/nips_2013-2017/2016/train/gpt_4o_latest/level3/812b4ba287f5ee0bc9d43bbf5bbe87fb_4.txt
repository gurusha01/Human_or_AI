The paper proposes a novel Tree-structured Reinforcement Learning (Tree-RL) approach for object proposal generation, modeled as a Markov Decision Process (MDP). The method integrates global interdependencies among objects and employs a tree-structured search scheme to discover multiple objects in a single pass. By leveraging translation and scale search actions, the algorithm enables efficient object localization across varying scales. The approach builds on prior reinforcement learning-based object localization methods, extending them to handle multi-object scenarios with a single traversal.
The paper is technically sound and demonstrates novelty in its tree-structured search mechanism and reward design. The use of deep Q-learning for policy optimization and the bifurcated action space (scaling and translation) are well-motivated and effectively implemented. The experiments on PASCAL VOC 2007 and 2012 datasets validate the efficacy of Tree-RL, showing competitive recall rates with fewer proposals compared to existing methods like RPN. However, the reported mAP (~76%) is lower than the best-performing object detection systems, such as Faster R-CNN with ResNet-101, which raises concerns about the state-of-the-art claim. Including additional baseline comparisons using VGG or ResNet would strengthen the evaluation and contextualize the results better.
The paper is clearly written and well-organized, with sufficient technical details to enable reproducibility. The authors provide a thorough explanation of the MDP components, reward function, and tree search strategy. However, the clarity of some experimental results could be improved, particularly the recall comparisons for small objects, where Tree-RL underperforms. Additionally, the visualization examples are helpful but could be expanded to include failure cases for a more balanced analysis.
In terms of originality, the paper introduces a novel combination of reinforcement learning and tree-structured search for object proposal generation. While related work is adequately referenced, the distinction from prior methods like RPN and single-path RL approaches is clearly articulated. The proposed method addresses a challenging problem and offers a unique perspective on multi-object localization.
The significance of the work lies in its potential to inspire further research in reinforcement learning-based object detection. While the mAP results are not state-of-the-art, the reduced number of proposals and the ability to handle multiple objects in a single pass make Tree-RL a valuable contribution. Future work could focus on improving detection accuracy and extending the approach to more complex datasets.
Pros:  
- Novel tree-structured search scheme for multi-object localization.  
- Efficient proposal generation with fewer windows.  
- Clear and detailed presentation of technical contributions.  
Cons:  
- mAP performance is not state-of-the-art.  
- Limited baseline comparisons with alternative architectures.  
- Underperformance in small object recall in some scenarios.  
Overall, the paper is a strong contribution to the field and has the potential for significant impact, though further refinement and evaluation are needed to fully establish its competitiveness.