The paper presents a novel approach to approximate low-rank tensor decomposition using alternating least squares (ALS) with leverage score sampling, leading to the Sparse Alternating Least Squares (SPALS) algorithm. The authors aim to address the computational bottleneck of ALS by efficiently sampling rows of the Khatri-Rao product, a key intermediate matrix in tensor decomposition. The main contribution is Theorem 3.2, which provides an efficient method to approximate leverage score upper bounds for the Khatri-Rao product, enabling sublinear time per iteration. Empirical evaluations demonstrate significant speedups on large-scale tensors, such as the Amazon review dataset, while maintaining comparable accuracy to state-of-the-art methods.
Strengths:
1. Problem Relevance and Novelty: The paper tackles a critical challenge in tensor decomposition—scalability for large datasets. The proposed SPALS algorithm is innovative in leveraging randomized linear algebra techniques to achieve sublinear time complexity, which is a significant advancement over deterministic ALS.
2. Theoretical Contributions: Theorem 3.2 is a key theoretical result that efficiently estimates leverage score upper bounds for the Khatri-Rao product. This insight is not only valuable for tensor decomposition but may also have broader applications in randomized numerical linear algebra.
3. Empirical Validation: The experimental results on both synthetic and real-world datasets, such as the Amazon review tensor, highlight the practical utility of SPALS. The algorithm demonstrates substantial speedups while achieving comparable error rates to existing methods.
4. Scalability: The ability to handle tensors with billions of nonzeros, as shown in the Amazon dataset, underscores the scalability of the proposed method.
Weaknesses:
1. Empirical Comparison: While the paper claims comparable performance to state-of-the-art methods like [37], the empirical results do not consistently demonstrate superiority. For instance, SPALS does not outperform [37] in all cases, and the lack of detailed comparisons in Figure (b) raises questions about its relative advantages.
2. Unexplained Behavior: The non-monotonic error behavior of SPALS(α) in Figure (a) is not adequately explained. This could undermine confidence in the robustness of the algorithm.
3. Clarity Issues: Certain notations and explanations are unclear. For example, the role of "n" in O(r log n) and the index mapping on page 2 require better clarification. Additionally, the tensor dimensions (e.g., whether n = 1000 implies a 1000×1000×1000 tensor) should be explicitly stated.
4. Figures and Presentation: Figure (a) lacks clear units for time and error, making it difficult to interpret the results. The presentation of empirical findings could be improved for better readability.
5. Typos and Grammar: The manuscript contains several grammatical and typographical errors, which detract from its overall readability and professionalism.
Recommendation:
The paper makes a solid theoretical and practical contribution to scalable tensor decomposition, addressing a critical bottleneck in ALS. However, the unclear empirical comparisons, unexplained error behavior, and presentation issues need to be addressed before publication. 
Arguments for Acceptance:
- Theoretical novelty in leveraging statistical leverage scores for the Khatri-Rao product.
- Significant speedups demonstrated on large-scale datasets.
- Potential applicability of the proposed techniques to other tensor-related tasks.
Arguments Against Acceptance:
- Lack of consistent empirical superiority over state-of-the-art methods.
- Unclear explanations and presentation issues, particularly in figures and notations.
- Presence of grammatical and typographical errors.
Final Decision:
Conditional acceptance, provided the authors address the clarity issues, improve the empirical comparisons, and resolve the presentation and typographical errors.