The paper introduces a novel nonlinear spectral method for training a specific class of feedforward neural networks with a guarantee of global optimality and linear convergence. The authors leverage the spectral radius of a small nonnegative matrix, dependent on network architecture parameters, to ensure convergence to the global optimum. This approach is conceptually innovative and addresses a critical challenge in neural network optimization: the non-convexity of the training problem. However, the method imposes significant constraints, including non-negativity of network weights and the use of non-standard activation functions, which may limit its practical applicability.
Strengths:  
The paper makes a noteworthy theoretical contribution by providing a globally optimal training algorithm for neural networks, a rare achievement in the field. The use of spectral radius as a convergence criterion is mathematically elegant and avoids the need for hyperparameter tuning, such as learning rates, which are typically required in stochastic gradient descent (SGD). The authors also provide rigorous theoretical guarantees, including proofs of convergence and uniqueness of the global optimum. The method's linear convergence rate is a significant advantage, and the experiments demonstrate its feasibility on real-world datasets, albeit with limitations.
Weaknesses:  
The primary limitation of the proposed method is its restrictive assumptions. The non-negativity constraint on weights and the requirement for specialized activation functions reduce the generalizability of the approach. These constraints make it incompatible with widely used architectures, such as those employing ReLU activations. Furthermore, the numerical experiments reveal subpar performance compared to support vector machines (SVMs) and standard neural networks trained with SGD, raising concerns about the method's practical effectiveness. While the authors claim convergence in "less than 10 iterations," this assertion appears exaggerated given the linear convergence rate and is not robustly supported by empirical evidence. Additionally, the method's scalability to high-dimensional datasets and deeper architectures remains unclear, as the spectral radius grows with the number of hidden units, potentially limiting its applicability to larger networks.
Pro and Con Arguments for Acceptance:  
- Pro: The paper provides a novel and theoretically sound approach to a challenging problem, with rigorous proofs and a clear mathematical foundation. It advances the understanding of global optimization in neural networks.  
- Con: The practical limitations, including restrictive assumptions and subpar empirical performance, reduce the method's relevance for real-world applications. The scalability to larger datasets and architectures is not convincingly addressed.
Recommendation:  
While the paper makes a valuable theoretical contribution, its practical limitations and restricted applicability suggest it may be better suited for a specialized audience rather than the broader NIPS community. I recommend acceptance only if the conference prioritizes theoretical advancements over practical impact. Otherwise, the paper may benefit from further refinement and empirical validation.