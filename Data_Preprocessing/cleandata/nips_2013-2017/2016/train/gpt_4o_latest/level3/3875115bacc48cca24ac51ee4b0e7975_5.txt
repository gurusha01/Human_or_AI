This paper addresses a long-standing open question posed by Srebro (2007) regarding the existence of local maxima in the infinite-sample likelihood of Gaussian Mixture Models (GMMs). The authors provide a negative resolution to this conjecture by constructing counterexamples where bad local maxima exist, even in the idealized setting of equally weighted, well-separated, spherical Gaussians. This result is significant as it challenges prior assumptions about the benign nature of the likelihood surface in such settings and has implications for the theoretical guarantees of the Expectation-Maximization (EM) algorithm.
The paper's contributions are twofold. First, it establishes the existence of arbitrarily bad local maxima in the population likelihood for GMMs with three or more components. Second, it demonstrates that both the EM algorithm and its first-order variant, under random initialization, converge to suboptimal critical points with high probability. The authors provide intuition for their results by analyzing a simple case (k=3, d=1) and extend their findings to higher dimensions and more components. Notably, the work highlights the critical role of initialization in ensuring the success of EM-based methods, even in asymptotic settings.
The paper is well-written, balancing rigorous theoretical results with intuitive explanations. The authors effectively communicate the implications of their findings for non-convex optimization and provide detailed proofs in the appendix. However, the paper would benefit from including estimates of constants in the probability bounds or supporting simulations to empirically observe the phase transition for convergence to bad solutions. Such additions would enhance the practical relevance of the theoretical results.
Strengths of the paper include its resolution of a significant open question, the construction of a general class of counterexamples, and the clear exposition of the implications for EM and gradient EM algorithms. The work is original and advances the understanding of non-convex optimization challenges in GMMs. Its insights are likely to influence future research on initialization strategies and algorithmic design for mixture models.
Weaknesses are relatively minor and include the lack of empirical validation or numerical experiments to complement the theoretical results. Additionally, while the authors conjecture that at least three components are necessary for bad local maxima, this remains unresolved and could be an avenue for further exploration.
In conclusion, this paper makes a strong theoretical contribution to the understanding of GMM likelihood surfaces and the limitations of EM-based methods. Its novelty and significance make it a valuable addition to the field, and I recommend its acceptance, with the suggestion to include empirical validation in future iterations.