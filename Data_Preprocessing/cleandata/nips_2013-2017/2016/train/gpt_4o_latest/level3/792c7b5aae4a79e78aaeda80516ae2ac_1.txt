The paper provides a detailed analysis of the Expectation Maximization (EM) algorithm for mixtures of two Gaussian distributions, focusing on both the Population EM and Sample-based EM algorithms. The authors aim to bridge the gap between the statistical principles underlying EM and its algorithmic behavior, particularly in the context of parameter estimation. They demonstrate that the Population EM algorithm, which assumes an infinite sample size, converges to specific fixed points depending on the initialization. Furthermore, they establish that the Sample-based EM algorithm shares the same asymptotic properties as the Population EM when initialized identically. This result is significant as it provides a global characterization of EM's behavior for this specific model, offering insights into its convergence properties and statistical consistency.
The paper builds on prior work, such as that of Balakrishnan et al. (2014), by removing assumptions like separation conditions or specific initialization schemes, which were often required in earlier analyses. The authors also connect their findings to the expected log-likelihood function, showing that the fixed points of Population EM correspond to stationary points of the log-likelihood. This work complements existing literature by offering a global analysis of EM's behavior without relying on restrictive conditions, making it a valuable contribution to the understanding of EM for Gaussian mixtures.
Strengths:
1. Comprehensive Analysis: The paper provides a thorough analysis of EM's behavior for mixtures of two Gaussians, covering all possible cases of initialization and convergence.
2. Theoretical Contribution: The results are rigorous and well-supported by mathematical proofs, offering new insights into the asymptotic properties of EM.
3. Significance: The work addresses a fundamental problem in machine learning and statistics, advancing the understanding of EM's limitations and strengths.
4. Clarity of Results: The characterization of fixed points and their dependence on initialization is clear and well-articulated.
Weaknesses:
1. Narrow Scope: The focus is limited to mixtures of two Gaussian distributions, which, while important, may restrict the broader applicability of the results.
2. Lengthy Proofs: The proofs, while rigorous, are excessively long and could be streamlined to improve readability.
3. Practical Implications: The paper primarily focuses on asymptotic behavior, which may not fully capture EM's performance in finite-sample scenarios, a critical consideration for practitioners.
Arguments for Acceptance:
- The paper addresses a significant gap in the understanding of EM for Gaussian mixtures.
- The results are rigorous, novel, and advance the state of the art in this area.
- The work has potential implications for both theoretical research and practical applications of EM.
Arguments Against Acceptance:
- The narrow focus on two-component Gaussian mixtures may limit the paper's generalizability.
- The length and complexity of the proofs may deter readers and reduce accessibility.
Overall, despite its narrow scope and lengthy proofs, the paper makes a meaningful contribution to the field by providing a global analysis of EM's behavior for a fundamental model. Given the importance of the problem and the rigor of the results, I recommend acceptance at NIPS. However, the authors are encouraged to streamline their presentation and discuss practical implications in more detail.