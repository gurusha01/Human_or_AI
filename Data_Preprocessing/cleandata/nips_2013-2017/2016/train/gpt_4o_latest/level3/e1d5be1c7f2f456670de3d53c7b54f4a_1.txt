The paper addresses the contextual semi-bandit problem, where a learner selects composite actions (lists of items) based on contextual information, receives semi-bandit feedback for individual items, and aims to maximize a reward that is a weighted linear combination of the feedback. The authors propose two novel algorithms: VCEE for the setting with known weights and EELS for the previously unstudied setting with unknown weights. Both algorithms leverage supervised learning oracles to achieve computational efficiency and provide theoretical guarantees on regret. The paper's contributions are significant, particularly in extending semi-bandit learning to a contextual setting and demonstrating the practical advantages of oracle-based approaches.
Strengths:
1. Novelty and Significance: The paper extends the semi-bandit framework to a contextual setting, which is highly relevant for applications like recommendation systems and personalized search. The introduction of EELS for unknown weights is particularly noteworthy.
2. Theoretical Contributions: The regret bounds for VCEE (√T) and EELS (T^(2/3)) are competitive with prior work, and the algorithms achieve computational efficiency with a polynomial number of calls to the supervised learning oracle. This is a clear improvement over existing methods like Kale et al., which require maintaining a distribution over all policies and are computationally expensive.
3. Empirical Validation: The experimental results are robust, demonstrating that VCEE outperforms state-of-the-art methods like LinUCB and ε-Greedy, particularly when using rich policy classes. This highlights the practical utility of the proposed methods.
4. Clarity and Organization: The paper is well-written, with a clear exposition of the problem, algorithms, and theoretical results. The experimental setup is detailed, and the results are presented comprehensively.
Weaknesses:
1. Questionable Claim: The claim that Kale et al.'s method is limited to weight vector w = 1 in the known-weights setting is debatable. The authors should provide a more rigorous justification or revise this statement.
2. Suboptimal Regret for Unknown Weights: While EELS is novel, its T^(2/3) regret is suboptimal compared to the √T dependence achieved by some contextual bandit algorithms. This leaves room for improvement in the unknown-weights setting.
3. Limited Exploration of Structured Action Spaces: The regret bounds for VCEE assume uniform smoothing, which may not be optimal for structured action spaces. Addressing this limitation could further strengthen the paper.
4. Empirical Evaluation Scope: The experiments focus only on datasets with known weights. Evaluating EELS on datasets where weights are unknown would provide a more complete picture of its practical performance.
Recommendation:
The paper makes significant theoretical and practical contributions to the field of contextual semi-bandits, particularly through its computationally efficient algorithms and robust empirical evaluation. While there are some minor weaknesses, they do not detract substantially from the overall quality and impact of the work. I recommend acceptance, with minor revisions to address the questionable claim about Kale et al. and to clarify the limitations of the proposed methods.
Arguments for Acceptance:
- Advances the state-of-the-art in contextual semi-bandit learning with computationally efficient algorithms.
- Provides strong theoretical guarantees and robust empirical validation.
- Addresses a significant problem with wide-ranging applications.
Arguments Against Acceptance:
- Suboptimal regret for the unknown-weights setting.
- Limited empirical evaluation of EELS.
Overall, the paper is a valuable contribution to the field and aligns well with the scope and quality standards of the conference.