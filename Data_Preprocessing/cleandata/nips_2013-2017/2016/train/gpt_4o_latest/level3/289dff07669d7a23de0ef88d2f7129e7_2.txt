The paper introduces the Cholesky-CMA-ES, a variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) that replaces eigen-decomposition with Cholesky decomposition to improve time and storage efficiency. CMA-ES is a widely used derivative-free optimization algorithm, and the proposed method addresses the computational bottleneck of updating and storing the covariance matrix. By maintaining triangular Cholesky factors, the authors achieve quadratic time complexity and reduced memory requirements without compromising the algorithm's performance in terms of objective function evaluations. The experimental results demonstrate significant speedups, particularly in high-dimensional settings, making the method a promising candidate for large-scale optimization problems.
Strengths:
1. Clarity and Writing: The paper is well-written and organized, providing a clear explanation of the proposed method and its theoretical underpinnings. The empirical results are presented effectively, highlighting the speedup achieved by the Cholesky-CMA-ES.
2. Practical Impact: The proposed method offers a significant improvement in runtime and memory efficiency, which is crucial for scaling CMA-ES to high-dimensional problems. The results suggest that this approach could become a new standard for CMA-ES in scenarios where computational resources are a concern.
3. Experimental Validation: The experiments are thorough, comparing the proposed method to state-of-the-art CMA-ES variants across a range of benchmark functions and dimensions. The findings are consistent with the theoretical claims, showing that the Cholesky-CMA-ES achieves the same optimization performance as the standard CMA-ES while being significantly faster.
Weaknesses:
1. Originality: The work is a relatively minor extension of existing methods, particularly the Rank-one Covariance Matrix Update and prior work on Cholesky-based updates. While the adaptation to the standard CMA-ES is novel, the conceptual contribution is incremental.
2. Theoretical Gaps: The paper lacks a formal proof of equivalence to formula (2), which is critical for establishing the theoretical soundness of the proposed method. Additionally, the claims about error in the path are not rigorously validated.
3. Computational Complexity Discussion: While the authors mention the asymptotic complexity of the method, a detailed analysis of the computational trade-offs compared to other approaches (e.g., CMA-ES/d or Suttorp-CMA-ES) is missing.
4. Experimental Limitations: The experiments focus primarily on final performance metrics, such as runtime and function evaluations, but do not thoroughly validate intermediate claims, such as the impact of the introduced error in the step-size adaptation path.
5. Notational and Typographical Issues: Minor inconsistencies in notation (e.g., \(wk\) vs. \(\alphak\), \(n\) vs. \(d\)) and typographical errors (e.g., 'F' vs. 'f') detract from the overall polish of the manuscript.
Recommendation:
While the proposed method is promising and demonstrates significant practical benefits, the paper is not ready for acceptance in its current form. The authors should address the missing theoretical proofs, provide a more detailed discussion of computational complexity, and validate intermediate claims through additional experiments. Furthermore, the notational and typographical issues should be corrected. With these improvements, the paper could make a valuable contribution to the field.
Arguments for Acceptance:
- Promising experimental results with significant speedups.
- Practical relevance for scaling CMA-ES to high-dimensional problems.
- Clear writing and organization.
Arguments Against Acceptance:
- Limited originality as the work is an incremental extension.
- Missing theoretical and experimental validation of key claims.
- Minor but noticeable issues with notation and presentation.
In summary, the paper demonstrates potential but requires substantial revisions to meet the standards of NIPS.