The paper presents a novel batch Bayesian optimization algorithm, the parallel knowledge gradient (q-KG), which generalizes the sequential KG criterion to the parallel setting. The authors provide a clear and thorough background on Bayesian optimization and Gaussian processes, situating their work within the context of prior research. The q-KG algorithm is derived using a decision-theoretic framework, and the authors detail its computation, including strategies for efficient gradient estimation and domain discretization. Numerical experiments demonstrate that q-KG consistently outperforms or matches state-of-the-art batch-sequential Bayesian optimization methods, particularly in noisy settings, and achieves significant improvements in hyperparameter tuning tasks for machine learning models.
Strengths:
1. Technical Soundness: The paper is technically robust, with a solid theoretical foundation for the q-KG algorithm. The derivation of the acquisition function and its gradient computation is rigorous, and the use of infinitesimal perturbation analysis (IPA) for gradient estimation is well-justified.
2. Empirical Validation: The experiments are comprehensive, covering both synthetic benchmarks and practical applications like tuning logistic regression and convolutional neural networks. The results convincingly demonstrate the superiority of q-KG, especially in noisy scenarios, which is a significant contribution to the field.
3. Clarity and Organization: The paper is well-written and logically structured, with clear explanations of the methodology, experimental setup, and results. The inclusion of open-source code enhances reproducibility.
4. Societal Impact: By enabling efficient parallel evaluations, the proposed method has the potential to accelerate optimization tasks in resource-intensive applications, such as machine learning hyperparameter tuning, which is of broad interest to the community.
Weaknesses:
1. Speed-Up Analysis: While the paper focuses on the performance gains of q-KG in terms of solution quality, it lacks a detailed analysis of the computational speed-ups achieved by transitioning from sequential to batch-sequential optimization. This omission limits the practical insights for real-world deployment.
2. Literature Review Precision: Some statements in the literature review are imprecise, particularly regarding prior work on integrating expected improvement (EI) and q-EI maximization. A more nuanced discussion of these contributions would strengthen the paper.
3. Technical Assumptions: The paper does not fully address certain assumptions, such as the compactness of the domain \(A\) and the continuity of the objective function \(f\). Additionally, details on hyperparameter re-estimation and the smoothness of the acquisition function \(g\) are sparse and could benefit from further elaboration.
4. Software Comparisons: The use of different software for fitting Gaussian processes in Figure 1 experiments raises questions about consistency. A unified implementation would provide a fairer comparison.
Arguments for Acceptance:
- The paper introduces a novel and impactful algorithm that advances the state-of-the-art in batch Bayesian optimization.
- The methodology is rigorous, and the empirical results are compelling, particularly in noisy settings.
- The work has significant practical implications for machine learning and other fields requiring black-box optimization.
Arguments Against Acceptance:
- The lack of a detailed speed-up analysis and imprecise literature review weaken the practical and scholarly contributions.
- Some technical assumptions and implementation details are underexplored, leaving gaps in the reproducibility and generalizability of the method.
Recommendation:
Overall, the paper makes a strong contribution to the field of Bayesian optimization and is well-suited for the conference. With minor revisions addressing the noted weaknesses, it has the potential to be a highly impactful work. I recommend acceptance with minor revisions.