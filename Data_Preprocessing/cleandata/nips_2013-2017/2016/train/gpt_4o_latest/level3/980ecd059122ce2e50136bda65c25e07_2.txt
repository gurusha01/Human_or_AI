The paper presents a novel approach to assessing the robustness of neural networks by proposing two metrics—adversarial frequency and adversarial severity—based on the L∞ distance to the nearest adversarial example. These metrics aim to provide a more objective and informative evaluation of robustness compared to prior methods. The authors also introduce an efficient algorithm for approximating these metrics by formulating robustness as a linear programming problem and restricting the search space to convex regions where the network behaves linearly. Experiments on MNIST and CIFAR-10 datasets demonstrate the efficacy of the proposed method, highlighting its ability to uncover more adversarial examples than existing baselines, thereby offering deeper insights into network vulnerabilities.
Strengths:  
1. Novel Metrics: The proposed metrics (adversarial frequency and severity) provide a nuanced view of robustness, capturing both the occurrence and severity of adversarial examples. This is a significant improvement over prior methods that often rely on a single algorithm to generate adversarial examples, leading to potential overfitting.
2. Algorithmic Efficiency: The use of convex restrictions and iterative constraint solving yields substantial speed-ups, making the approach computationally feasible even for larger networks like NiN on CIFAR-10.
3. Experimental Rigor: The inclusion of the full cumulative distribution function (CDF) of adversarial distances adds depth to the analysis. The method's ability to outperform the baseline (L-BFGS-B) in identifying adversarial examples on MNIST is compelling.
4. Insightful Observations: The paper highlights the issue of overfitting to specific adversarial generation algorithms, a critical limitation in prior robustness evaluation techniques.
Weaknesses:  
1. Assumption of Linearity: The method assumes linearity in regions of the input space, which may not hold universally for neural networks. This limitation is acknowledged but not adequately addressed, particularly for complex datasets like CIFAR-10, where results were less promising.
2. Negative Results on CIFAR-10: The approach struggles to significantly improve robustness on the NiN network, with adversarial examples still fooling the fine-tuned network in ~60% of cases. This raises concerns about scalability to more complex architectures.
3. Clarity and Structure: The introduction is poorly structured, requiring more background on adversarial robustness and related work for accessibility. Additionally, the title and abstract are somewhat misleading, implying broader applicability than demonstrated.
4. Minor Issues: Numeric citations are distracting, and a typo ("when when") should be corrected.
Pro vs. Con for Acceptance:  
- Pro: The paper introduces innovative metrics and a robust algorithm that advance the state of the art in adversarial robustness evaluation. The work provides valuable insights into the limitations of existing methods and offers a computationally efficient alternative.  
- Con: The method's reliance on linearity assumptions and its limited success on CIFAR-10 suggest that its applicability may be restricted to simpler datasets or architectures. The presentation could also be improved for clarity and accessibility.
Recommendation:  
While the paper has notable limitations, particularly in its scalability and assumptions, its contributions to robustness evaluation are significant and address a critical gap in the field. I recommend acceptance, provided the authors address the clarity issues and temper claims of general applicability.