Review
This paper proposes a novel computational approach to estimating generalized linear model (GLM) coefficients by leveraging a proportionality relationship between GLM and ordinary least squares (OLS) coefficients under certain conditions. The method, termed Scaled Least Squares (SLS), estimates the OLS coefficients and then determines a proportionality constant via a univariate root-finding algorithm. The authors provide theoretical guarantees for the method, extend the proportionality relationship to non-Gaussian contexts, and demonstrate its computational efficiency and accuracy through experiments.
Strengths:
1. Novelty and Contribution: The paper introduces an efficient algorithm for GLM coefficient estimation, which is computationally cheaper than traditional maximum likelihood estimation (MLE) methods. The focus on reducing computational costs in large-scale problems (n ≫ p) is timely and relevant.
2. Theoretical Insights: The authors rigorously analyze the proportionality relationship between GLM and OLS coefficients, extending it beyond Gaussian predictors. The bounds provided on the discrepancy between the two coefficients are valuable for understanding the method's limitations.
3. Computational Efficiency: The proposed method achieves up to cubic convergence rates with O(n) per-iteration costs, making it significantly faster than batch optimization methods. This is particularly advantageous for large datasets.
4. Practical Applicability: The experimental results demonstrate that SLS performs comparably to MLE in terms of accuracy while being computationally more efficient. The method's robustness to initialization is also a practical advantage.
Weaknesses:
1. Clarity and Organization: While the first half of the paper is well-written, the second half appears rushed. Key points, such as the assumptions in Corollary 1 and the definition of λ_min, are unclear and require better explanation. The applicability of the method to only canonical links should be stated earlier for clarity.
2. Misleading Claims: The claim that the method works for "any GLM task" (Algorithm 1) is misleading, as it is restricted to canonical links. This should be clarified to avoid overgeneralization.
3. Experimental Design: The use of sub-sampling in Section 5 complicates the interpretation of results, as it is unclear how much of the performance gain is due to sub-sampling versus the SLS method itself. Additionally, results in Figure 2 and Table 1 should be averaged over multiple runs to ensure statistical confidence.
4. Comparative Analysis: The omission of Iteratively Reweighted Least Squares (IRLS) in the experimental comparisons is a missed opportunity, as IRLS is a widely used method and would provide a more comprehensive benchmark.
5. Visual Presentation: Figure 2 is visually cluttered, with redundant legends and unclear x-axis starting points for SLS. Improving the figure's design would enhance readability.
Suggestions for Improvement:
1. Provide clearer explanations of Corollary 1, λ_min, and the assumptions underlying the theoretical results.
2. Clarify the scope of the method early in the paper, particularly its restriction to canonical links.
3. Include IRLS in the experimental comparisons and plot relative accuracy (e.g., MSE ratios) to better contextualize the performance of SLS.
4. Average experimental results over multiple runs to ensure robustness.
5. Simplify and declutter Figure 2, and explicitly state whether sub-sampling or the full sample is used in each experiment.
Pro and Con Arguments for Acceptance:
Pros:
- Novel and computationally efficient approach to GLM coefficient estimation.
- Strong theoretical foundation with bounds on approximation errors.
- Practical relevance for large-scale datasets with n ≫ p.
- Promising experimental results demonstrating computational advantages.
Cons:
- Lack of clarity in parts of the paper and misleading claims about general applicability.
- Experimental design issues, including sub-sampling confounding and lack of statistical averaging.
- Missing comparisons with IRLS, a key baseline for GLM estimation.
Recommendation:
This paper makes a meaningful contribution to efficient GLM estimation and addresses a relevant problem in large-scale machine learning. However, the clarity, experimental design, and scope of claims need improvement. I recommend acceptance with major revisions, contingent on addressing the identified weaknesses.