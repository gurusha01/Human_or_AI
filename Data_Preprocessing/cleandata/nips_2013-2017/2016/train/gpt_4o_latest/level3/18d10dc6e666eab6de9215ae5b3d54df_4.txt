The paper presents a novel approach to batch Bayesian Optimization (BO) through the introduction of the parallel knowledge gradient (q-KG) acquisition function. This method enables the simultaneous evaluation of multiple points, which is particularly relevant for distributed and parallel computing environments. The authors address a critical challenge in BO by proposing an efficient computational strategy for evaluating the q-KG function, leveraging techniques such as infinitesimal perturbation analysis (IPA) and domain discretization. Empirical results demonstrate the superiority of q-KG over state-of-the-art methods, particularly in noisy settings and high-dimensional optimization tasks, such as hyperparameter tuning for machine learning models.
Strengths:
1. Novelty and Significance: The q-KG acquisition function is derived from a decision-theoretic perspective, offering a principled and theoretically grounded approach to batch BO. The method addresses an important problem in distributed optimization, with clear applications to machine learning tasks like hyperparameter tuning.
2. Efficiency: The authors propose a computationally efficient strategy to optimize the q-KG function, which is traditionally expensive to compute. This makes the method scalable and practical for real-world applications.
3. Empirical Validation: The paper provides extensive experimental results on synthetic functions and practical machine learning tasks (e.g., tuning logistic regression and CNNs). The results consistently show that q-KG outperforms or is competitive with existing methods, particularly in noisy settings.
4. Clarity and Organization: The paper is well-structured, with clear explanations of the methodology, theoretical contributions, and experimental setup. The inclusion of open-source code enhances reproducibility.
Weaknesses:
1. Gaussian Process Assumptions: The reliance on Gaussian processes (GPs) may limit the scalability of q-KG to high-dimensional problems, such as those involving neural network hyperparameter tuning. While the authors acknowledge this, a more detailed discussion or empirical analysis of the method's limitations in high-dimensional spaces would strengthen the paper.
2. Theoretical Analysis: While the paper provides strong empirical results, it lacks theoretical error bounds and complexity analysis, particularly with respect to input dimensionality. This omission leaves open questions about the method's robustness and scalability.
3. Minor Clarity Issues: Some notations and explanations are unclear (e.g., lines 36, 95, 104, 128, 140), and a few typos are present. Addressing these would improve readability.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded acquisition function for batch BO, addressing a significant problem in distributed optimization.
- The proposed method demonstrates strong empirical performance, outperforming benchmarks in both synthetic and practical tasks.
- The computational efficiency of the q-KG method makes it a valuable contribution to the field.
Arguments Against Acceptance:
- The reliance on GPs may limit scalability to high-dimensional problems, and the paper does not provide sufficient theoretical guarantees or analysis to address this concern.
- Minor clarity issues and unclear notations detract from the overall presentation.
Recommendation:
Overall, this is a high-quality paper with significant contributions to batch Bayesian Optimization. While the concerns about scalability and theoretical analysis are valid, they do not overshadow the paper's strengths. I recommend acceptance, with minor revisions to address clarity issues and provide additional theoretical insights.