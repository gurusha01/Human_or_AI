This paper introduces a novel approach to exploration in non-tabular reinforcement learning by leveraging density models to derive pseudo-counts, which generalize traditional count-based exploration methods. The authors present multiple contributions, including a sequential density model-based extension to counting-based exploration, a modification to DQN, and impressive empirical results, particularly on the notoriously challenging game Montezuma's Revenge. The connection drawn between pseudo-counts and intrinsic motivation is particularly intriguing, as it bridges two distinct paradigms of exploration in reinforcement learning.
The formulation of pseudo-counts is both novel and technically sound. By deriving pseudo-counts from density models, the authors provide a generalized notion of visit counts applicable to high-dimensional state spaces, such as raw pixel inputs in Atari games. The empirical results are compelling, with the pseudo-count-based exploration bonus significantly outperforming baseline methods on several hard Atari games. The dramatic improvement on Montezuma's Revenge, where the agent explores 15 rooms and achieves scores far exceeding prior work, is a notable highlight and a strong testament to the method's efficacy.
However, there are some concerns that merit further clarification. First, while the relationship between pseudo-counts and information gain (IG) is technically valid, it diverges from the classical IG concept commonly used in intrinsic motivation literature. This raises questions about its philosophical alignment and relevance to the broader intrinsic motivation framework. The authors could strengthen their argument by explicitly addressing this divergence and its implications. Second, the paper does not provide a clear justification for why pseudo-counts are preferable to prediction gain (PG), which appears competitive and potentially better-behaved in certain scenarios. A more detailed comparison, both theoretically and empirically, would enhance the paper's rigor.
In terms of quality, the paper is technically robust, with well-supported claims and thorough experimental validation. The clarity of the writing is generally strong, though certain sections, such as the derivation of pseudo-counts and their connection to IG, could benefit from additional explanation to improve accessibility. The work is highly original, introducing a novel concept that bridges intrinsic motivation and count-based exploration, and it is well-situated within the broader reinforcement learning literature. The significance of the results is substantial, as the proposed method addresses a critical challenge in reinforcement learningâ€”efficient exploration in sparse-reward environments.
Arguments for Acceptance:
1. Novel and well-formulated concept of pseudo-counts with strong theoretical underpinnings.
2. Impressive empirical results, particularly on Montezuma's Revenge, demonstrating the method's practical utility.
3. Clear connection to intrinsic motivation, advancing the state of the art in exploration strategies.
Arguments Against Acceptance:
1. Lack of clarity on the philosophical alignment of pseudo-counts with classical IG concepts.
2. Insufficient justification for the superiority of pseudo-counts over prediction gain.
In conclusion, this paper makes a significant contribution to reinforcement learning by introducing a novel exploration method with strong empirical and theoretical results. Addressing the outlined concerns would further enhance its impact. I recommend acceptance, contingent on minor revisions for clarity and additional comparisons.