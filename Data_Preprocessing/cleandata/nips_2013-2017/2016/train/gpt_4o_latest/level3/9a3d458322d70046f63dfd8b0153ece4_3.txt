The paper introduces a novel notion of safety in reinforcement learning (RL), focusing on maximin improvement over a baseline policy. This approach addresses a critical challenge in sequential decision-making under uncertainty: ensuring that a learned policy outperforms a baseline strategy while accounting for model inaccuracies. The authors propose a robust optimization framework that minimizes regret relative to the baseline policy, allowing for state-wise improvements where the model is accurate and fallback to the baseline policy otherwise. The paper provides theoretical results, including NP-hardness of the problem, bounds on performance loss, and the necessity of randomized policies, alongside an approximate algorithm for practical implementation.
Strengths:  
The paper presents a fresh perspective on safe RL by shifting the focus from pessimistic performance guarantees to robust regret minimization. This formulation is less conservative than standard approaches, which often reject policies with marginal improvements due to discrepancies in optimistic and pessimistic estimates. The theoretical analysis is rigorous, offering insights into the computational complexity and performance bounds of the proposed method. The approximate algorithm is simple yet effective, as demonstrated in experiments across diverse domains, including grid navigation and energy arbitrage. These experiments highlight the practical utility of the approach, particularly in scenarios with significant model uncertainty.
Weaknesses:  
Despite its contributions, the paper suffers from several clarity and organizational issues. The definition of safety (Line 104) requires further elaboration, particularly in comparison to existing definitions, such as those in Thomas et al.'s work. The placement of full algorithms in the appendix detracts from the readability of the main text, as these are central to understanding the proposed methods. Theorem 3's choice of a multiplicative bound over an additive one is not well-justified, and Proposition 7 lacks a bound on approximation error when assumptions fail, raising concerns about the robustness of Algorithm 1. The proof of Theorem 5 is unclear, especially the transition involving \( \rho(\pi_S, \xi) \) and \( \rho(\pi^, \xi) \). Additionally, the NP-hardness proof is difficult to follow due to missing states in Figure 6 and ambiguities in the formulation of the uncertainty set. Minor issues, such as redundant notation (e.g., \( P \) vs. \( P^ \)) and imprecise vector usage in Theorem 8 and Lemma 11, further detract from the paper's clarity.
Arguments for Acceptance:  
1. The paper addresses a significant and underexplored problem in safe RL, introducing a novel and less conservative safety concept.  
2. Theoretical contributions are robust, with clear implications for both the computational complexity and practical implementation of safe RL.  
3. Experimental results demonstrate the practical utility of the proposed approach in diverse domains.  
Arguments Against Acceptance:  
1. The paper lacks clarity in key definitions, proofs, and algorithmic descriptions, making it difficult to fully assess its contributions.  
2. Organizational issues, such as relegating critical algorithms to the appendix, hinder comprehension.  
3. Certain theoretical results, such as Proposition 7 and Theorem 3, require stronger justification and contextualization.  
Recommendation:  
The paper makes a valuable contribution to the field of safe RL, but significant revisions are needed to improve clarity and address the identified weaknesses. I recommend acceptance conditional on these revisions, as the core ideas are novel and impactful.