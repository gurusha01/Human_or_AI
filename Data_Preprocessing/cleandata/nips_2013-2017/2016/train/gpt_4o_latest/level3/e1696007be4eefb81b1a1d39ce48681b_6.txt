The paper presents a novel and computationally efficient algorithm for estimating generalized linear model (GLM) coefficients in large-scale settings, where the number of observations \(n\) far exceeds the number of predictors \(p\). By leveraging the relationship between GLM coefficients and ordinary least squares (OLS) coefficients, the authors propose the Scaled Least Squares (SLS) estimator, which achieves the same accuracy as the maximum likelihood estimator (MLE) but at a significantly reduced computational cost. The algorithm combines an initial OLS estimation step with a root-finding procedure to estimate a proportionality constant, achieving up to cubic convergence with \(O(n)\) per-iteration cost.
Strengths:
1. Efficiency and Scalability: The proposed algorithm is computationally efficient, with a per-iteration cost that is at least \(O(p)\) cheaper than traditional batch optimization methods. This makes it highly suitable for large-scale problems, as demonstrated by both theoretical analysis and numerical experiments.
2. Theoretical Rigor: The paper provides strong theoretical guarantees, including convergence rates for the estimation error. The results are further generalized to non-Gaussian designs using zero-bias transformations, which is a significant contribution to the field.
3. Empirical Validation: Extensive experiments on synthetic and real-world datasets (e.g., Higgs and Covertype datasets) demonstrate that the SLS estimator achieves comparable accuracy to the MLE while being computationally faster.
4. Innovation: The paper creatively applies a classical statistical insight—the approximate proportionality between GLM and OLS coefficients—to modern large-scale settings, offering a fresh perspective on GLM optimization.
5. Clarity and Organization: The paper is well-written and logically structured, making it easy to follow the methodology and results. The inclusion of detailed comparisons with existing methods (e.g., Newton-Raphson, BFGS, and gradient descent) adds depth to the analysis.
Weaknesses:
1. Assumption on OLS Accuracy: The approach assumes that the OLS estimator can be computed accurately, even in high-dimensional settings. While subsampling is suggested as a remedy, the impact of this step on the overall performance is not fully explored.
2. Typographical Ambiguity: The term "cubic convergence" might refer to Halley's method, but this is not explicitly clarified, which could lead to confusion.
3. Subsampling Step: The paper raises but does not fully address the question of whether alternative subsampling strategies for OLS estimation could affect the results. This could be an area for further investigation.
4. Regularization: While the authors briefly discuss extensions to regularized GLMs (e.g., ridge regression), this aspect is not explored in depth. Regularization is critical in many large-scale problems, and a more thorough treatment would strengthen the paper.
Arguments for Acceptance:
- The paper addresses an important problem in large-scale machine learning and proposes a method that is both theoretically sound and practically impactful.
- The innovative use of the OLS-GLM relationship is likely to inspire further research in computationally efficient optimization methods.
- The results are well-supported by both theory and experiments, and the paper is clearly written.
Arguments Against Acceptance:
- The reliance on accurate OLS estimation and the limited discussion of regularization may reduce the applicability of the method in some practical scenarios.
- The paper could benefit from a more detailed exploration of alternative subsampling strategies and their impact on performance.
Recommendation:
Overall, this paper makes a significant contribution to the field of large-scale optimization for GLMs. Its combination of theoretical insights, computational efficiency, and empirical validation makes it a strong candidate for acceptance. However, addressing the noted weaknesses in a future revision would further enhance its impact.