The paper introduces Cholesky-CMA-ES, a novel variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) that significantly reduces computational complexity by maintaining a lower-triangular Cholesky factor instead of the full covariance matrix. This approach reduces the time complexity of the covariance matrix update from \(O(d^3)\) to \(O(\mu d^2)\), where \(\mu\) is the number of selected samples, making it particularly valuable for high-dimensional optimization problems. The authors provide theoretical justification for their method, arguing that the omission of the random orthogonal matrix \(E_t\) introduces negligible error under asymptotic conditions. Empirical results demonstrate that Cholesky-CMA-ES achieves substantial speed-ups without compromising optimization performance compared to state-of-the-art CMA-ES alternatives.
Strengths:
1. Technical Contribution: The proposed method achieves a significant reduction in runtime complexity while preserving the performance of the original CMA-ES. This is a valuable contribution for high-dimensional optimization problems where the computational cost of CMA-ES becomes prohibitive.
2. Theoretical Justification: The authors provide a rigorous analysis to support the convergence of the algorithm and the diminishing impact of the omitted \(E_t\) matrix, which adds credibility to their claims.
3. Empirical Validation: The experiments are comprehensive, comparing Cholesky-CMA-ES to multiple CMA-ES variants across a range of benchmark functions and dimensionalities. The results convincingly demonstrate the runtime advantages of the proposed method.
4. Practical Implications: The reduced memory footprint and computational cost make the algorithm particularly appealing for scenarios with fast objective function evaluations, such as certain machine learning applications (e.g., policy search in reinforcement learning).
Weaknesses:
1. Limited Practical Scenarios: While the method is advantageous for high-dimensional problems with fast objective evaluations, such scenarios are relatively niche. In many real-world applications, the runtime of the black-box objective function dominates, making the speed-up less impactful.
2. Empirical Gaps: The paper could benefit from additional experiments, such as tracking the evolution of \(E_t\) to empirically validate the theoretical assumptions, and analyzing runtime without delay strategies for a more direct comparison.
3. Real-World Relevance: The absence of real-world experiments (e.g., in robotics or hyperparameter tuning) limits the practical applicability of the results. Such experiments would strengthen the paper's case for adoption in applied settings.
4. Clarity: While the paper is technically sound, certain sections, particularly the theoretical analysis, are dense and may be challenging for readers unfamiliar with CMA-ES or Cholesky decomposition. Simplifying or providing more intuitive explanations could improve accessibility.
Arguments for Acceptance:
- The paper addresses a well-recognized bottleneck in CMA-ES and proposes a solution with strong theoretical and empirical support.
- The method advances the state-of-the-art in derivative-free optimization by enabling faster optimization in high-dimensional spaces.
- The contribution is novel and builds on prior work in a meaningful way, with clear improvements over existing approaches.
Arguments Against Acceptance:
- The practical impact may be limited in typical optimization scenarios where objective function evaluations dominate runtime.
- The lack of real-world experiments reduces the paper's relevance to practitioners.
- Some aspects of the theoretical analysis and empirical results could be expanded for greater clarity and completeness.
Recommendation:
I recommend acceptance of this paper, as it provides a significant and well-supported contribution to the field of derivative-free optimization. However, I strongly encourage the authors to address the suggested empirical gaps and consider adding real-world experiments in future work to enhance the practical relevance of their method.