This paper introduces the concept of "Givens capacity" to quantify the representational complexity of unitary matrices and evaluates it for a previously proposed parameterized class of unitary matrices. The authors argue that this parameterization only spans a subset of unitary matrices for dimensions greater than 7 and propose an alternative method to optimize over the full set of unitary matrices. While the proposed full-capacity unitary recurrent neural networks (uRNNs) demonstrate improved performance in certain tasks, the method raises computational and experimental concerns.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous theoretical framework to assess the limitations of restricted-capacity parameterizations using Sard's theorem. This is a valuable contribution to understanding the representational capacity of unitary matrices.
2. Novel Optimization Approach: The authors propose a method to optimize full-capacity unitary matrices by constraining gradients to the Stiefel manifold. This approach is mathematically sound and avoids the need for gradient clipping, a common issue in training recurrent neural networks.
3. Empirical Validation: The paper evaluates the proposed full-capacity uRNNs on a variety of tasks, including synthetic data, speech prediction, and pixel-by-pixel MNIST classification. The results demonstrate that full-capacity uRNNs outperform restricted-capacity uRNNs and LSTMs in several scenarios, particularly for tasks requiring long-term memory.
Weaknesses:
1. Computational Overhead: The proposed method requires matrix inversion, which is computationally expensive, especially for large matrices. The authors' claim of faster matrix inversion appears to be due to implementation details rather than theoretical improvements, which undermines the practicality of the approach.
2. Experimental Inconsistencies: Different optimization methods (RMSprop for restricted-capacity uRNNs and SGD for full-capacity uRNNs) are used without justification. This inconsistency introduces confounding factors, making it difficult to attribute performance improvements solely to the proposed method.
3. Unnecessary Complexity: The introduction of Stiefel manifolds seems excessive, as the problem is restricted to square unitary matrices. A simpler formulation might have sufficed.
4. Limited Discussion of Trade-offs: While the paper highlights the advantages of full-capacity uRNNs, it does not adequately discuss the trade-offs between computational cost and performance gains, particularly for tasks where restricted-capacity uRNNs perform comparably.
Recommendation:
While the paper makes a meaningful theoretical and empirical contribution, the computational inefficiencies and experimental inconsistencies weaken its impact. The authors should address the optimization method inconsistencies, provide a more detailed analysis of computational trade-offs, and clarify the necessity of introducing Stiefel manifolds. If these issues are resolved, the paper could be a strong addition to the literature on unitary recurrent neural networks. For now, I recommend acceptance with major revisions.
Arguments for Acceptance:
- Strong theoretical foundation and novel optimization approach.
- Demonstrated performance improvements in challenging tasks.
- Addresses an important limitation of existing uRNN parameterizations.
Arguments Against Acceptance:
- Significant computational overhead due to matrix inversion.
- Experimental design flaws that confound results.
- Unnecessary complexity in the proposed method.
Overall Rating: 6/10 (Borderline Accept)