The paper introduces SPALS, a novel low-rank tensor CP decomposition method that accelerates Alternating Least Squares (ALS) algorithms through a sampling-based approach. By leveraging the statistical properties of the Khatri-Rao product, the authors propose a method to estimate leverage scores efficiently, enabling sublinear per-iteration runtime for ALS. This is a significant advancement, as ALS has traditionally been computationally expensive for large-scale tensor decomposition tasks. The authors demonstrate the effectiveness of SPALS through theoretical guarantees and empirical evaluations, showcasing its superior performance on both synthetic and real-world datasets, such as the Amazon review tensor with over 2 billion nonzeros.
Strengths:
1. Technical Contribution: The paper provides a solid theoretical foundation for the proposed sampling method, including a novel connection between the leverage scores of the Khatri-Rao product and the input matrices. This insight is a valuable contribution to randomized numerical linear algebra and tensor analytics.
2. Efficiency: SPALS achieves state-of-the-art computational efficiency, with sublinear runtime per iteration, making it highly scalable for large sparse tensors. The empirical results demonstrate significant speedups over existing deterministic and randomized methods.
3. Practical Impact: The application of SPALS to real-world datasets, such as the Amazon tensor, highlights its potential for practical use in large-scale data analytics.
4. Clarity of Experiments: The experiments are well-designed, with comparisons to baseline methods and detailed runtime and error metrics. The results convincingly demonstrate the advantages of SPALS in terms of both speed and accuracy.
Weaknesses:
1. Global Convergence Analysis: While the paper provides theoretical guarantees for the sampling method, it lacks a detailed analysis of how the proposed sampling impacts the global convergence behavior of ALS. This is a critical aspect for understanding the robustness of SPALS in practice.
2. Parameter Sensitivity: The paper does not sufficiently discuss the sensitivity of SPALS to parameter initialization or the stopping criteria used in experiments. These factors can significantly influence the performance and reproducibility of the method.
3. Omission of Sketching-Based Methods: The omission of results for sketching-based methods in Table 1b is notable, as these methods are closely related and would provide a more comprehensive comparison. The authors should clarify this decision.
Recommendation:
While the paper makes a strong technical contribution and demonstrates impressive empirical results, the lack of analysis on global convergence and parameter sensitivity leaves some gaps in its evaluation. Addressing these concerns would strengthen the paper further. Additionally, including comparisons with sketching-based methods in all experimental settings would provide a more complete picture of SPALS' performance.
Arguments for Acceptance:
- The paper introduces a novel and efficient approach to tensor decomposition, addressing a significant computational bottleneck in ALS.
- Theoretical insights into leverage scores of the Khatri-Rao product are valuable for the broader community.
- Experimental results demonstrate clear advantages over existing methods in both runtime and accuracy.
Arguments Against Acceptance:
- The lack of global convergence analysis raises questions about the robustness of SPALS.
- Insufficient discussion on parameter sensitivity and stopping criteria may hinder reproducibility.
- The omission of sketching-based methods in some results limits the scope of the evaluation.
Overall, the paper is a strong candidate for acceptance, but addressing the noted weaknesses would enhance its impact and clarity.