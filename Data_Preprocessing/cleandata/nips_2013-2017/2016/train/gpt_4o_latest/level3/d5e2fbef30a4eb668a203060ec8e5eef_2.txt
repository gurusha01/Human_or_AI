The paper proposes CMICOT, a novel information-theoretic feature selection algorithm that addresses high-order feature interactions using a sequential forward selection (SFS) strategy. By formulating a saddle point optimization problem, the method approximates conditional mutual information (CMI) to identify joint dependencies among features. The authors mitigate the computational and sample complexity of estimating high-dimensional MI through a two-stage greedy search and binary feature representations. Experimental results demonstrate CMICOT's superiority over state-of-the-art interaction-aware and interaction-agnostic filters on synthetic and real-world datasets.
Strengths:
1. Novelty and Contribution: The paper addresses a significant gap in feature selection by extending interaction-aware methods to account for higher-order dependencies. The use of a max-min optimization framework and binary feature representations is innovative and well-motivated.
2. Theoretical Rigor: The authors provide a detailed theoretical foundation, including proofs and propositions, to support their claims. The greedy approximation and binary representation techniques are well-justified and practical.
3. Empirical Validation: The experiments are extensive, covering synthetic datasets with known interactions and real-world benchmarks. CMICOT consistently outperforms competitors like RelaxMRMR, IWFS, and RCDFS, particularly for classifiers like k-NN and AdaBoost.
4. Scalability: The algorithm's linear complexity with respect to SFS iterations and its ability to handle high-dimensional datasets make it computationally appealing.
5. Generalizability: The binary representation technique is a standalone contribution that can be applied to other SFS-based filters, enhancing its broader applicability.
Weaknesses:
1. Unnecessary Theoretical Components: Statement 1 and its proof are redundant, as the equivalence of the maximum score to combined mutual information is self-evident.
2. Experimental Details: Key parameters, such as the value of \(k\) in k-NN, the specifics of multiclass AdaBoost (base learner and ensemble size), and dataset preprocessing, are missing. This lack of transparency hinders reproducibility.
3. Notation Issues: The supplementary material suffers from unclear and inconsistent notation, particularly in subset labeling (e.g., \(G_s\)).
4. Binary Approximation Impact: The effect of binary representation on performance remains ambiguous. It is unclear whether it improves results by uncovering interactions or introduces noise that limits information compared to JMI or RelaxMRMR.
5. Dataset-Specific Insights: Separating results for datasets like poker, ranking, and semeion would clarify the binary approximation's impact on performance.
6. Competitor Comparisons: While the complexity analysis of competitor techniques is accurate, it assumes naive implementations. Memoized versions could alter the comparative results.
Suggestions for Improvement:
1. Remove redundant theoretical statements (e.g., Statement 1) to streamline the presentation.
2. Provide detailed experimental settings, including classifier parameters and dataset preprocessing steps.
3. Clarify the impact of binary feature representation by running competitor algorithms on binary-expanded datasets.
4. Improve the clarity and consistency of notation in the supplementary material.
5. Separate dataset-specific results to better illustrate the algorithm's strengths and limitations.
Recommendation:
The paper makes a strong theoretical and empirical contribution to feature selection, particularly in identifying high-order feature interactions. However, the lack of experimental details and some unclear aspects of the binary approximation's impact slightly detract from its overall clarity. I recommend acceptance with minor revisions to address these issues.