The paper proposes a supervised extension to the Word Mover's Distance (WMD), termed Supervised Word Mover's Distance (S-WMD), which incorporates supervision into the metric learning process. The authors achieve this by introducing a linear projection of word embeddings and word-specific weighting to re-weight word frequencies, optimizing these parameters to minimize leave-one-out kNN classification error. The approach leverages an efficient optimization strategy using a relaxed version of the optimal transport problem, enabling practical training. Experimental results on eight datasets demonstrate the method's effectiveness, outperforming most of the 26 competitive baselines in kNN text classification tasks.
Strengths:  
1. Technical Soundness: The paper is technically well-grounded, leveraging established concepts like optimal transport, Sinkhorn scaling, and metric learning. The proposed optimization strategy is efficient and addresses the computational challenges of WMD.  
2. Experimental Rigor: The evaluation spans eight datasets and compares S-WMD against 26 baselines, including both unsupervised and supervised methods. The results consistently show S-WMD achieving competitive or superior performance.  
3. Clarity: The paper is well-written and logically structured, making it accessible to readers. The inclusion of visualizations (e.g., t-SNE embeddings and word clouds) enhances interpretability.  
4. Practical Contribution: The method is computationally efficient, with training times comparable to or faster than baseline methods, making it feasible for real-world applications.  
Weaknesses:  
1. Incremental Contribution: While the idea of optimizing WMD with supervision is novel, the contribution is somewhat incremental, as it builds on existing concepts like WMD and supervised metric learning.  
2. Limited Evaluation Scope: The experimental setup lacks evaluations isolating the impact of the word-specific weights and the projection matrix. Additionally, comparisons with existing metric learning methods applied to "word centroid embeddings" are missing, which could provide a more comprehensive baseline.  
3. Performance Ceiling: The method performs similarly to a simpler initialization method (S-WCD) in some cases, raising questions about the added value of the full optimization process.  
4. Minor Clarity Issues: Some clarifications are needed, such as missing references in Line 152, notation in Equation 11, and recalling Sinkhorn scaling in Line 170.  
Pro and Con Arguments for Acceptance:  
Pro: The paper addresses a relevant problem in text classification, demonstrates competitive performance, and provides a computationally efficient solution. Its methodological clarity and experimental rigor make it a valuable contribution to the field.  
Con: The contribution is incremental, and the experimental evaluation could be more thorough in isolating the effects of individual components.  
Recommendation:  
While the contribution is incremental, the paper is technically sound, well-written, and demonstrates competitive results. It would benefit from additional evaluations to strengthen its claims. I recommend acceptance with minor revisions, particularly addressing the missing evaluations and clarifications.