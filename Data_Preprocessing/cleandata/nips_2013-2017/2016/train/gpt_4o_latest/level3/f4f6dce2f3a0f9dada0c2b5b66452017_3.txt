The paper proposes a novel method, SPALS, to accelerate tensor CP decomposition using Alternating Least Squares (ALS) by leveraging statistical leverage score sampling. The authors address the computational bottleneck of forming and operating on the Khatri-Rao product (KRP) by efficiently estimating leverage scores without explicitly constructing the KRP. This innovation enables sublinear time per-iteration complexity while maintaining provable additive approximation guarantees, as errors do not accumulate across ALS iterations. Empirical results demonstrate up to 30x speedups over deterministic ALS with minimal or no accuracy loss, particularly on large-scale datasets like the Amazon review tensor.
Strengths:
1. Quality and Theoretical Contributions: The paper is technically sound, with a clear theoretical foundation supporting its claims. The leverage score estimation for KRPs is novel and enables efficient sampling, which is a significant contribution to randomized numerical linear algebra and tensor analytics. The approximation guarantees are rigorously proven, and the method is shown to generalize to other tensor-related applications like stochastic gradient descent and higher-order singular value decomposition.
2. Experimental Validation: The empirical results are compelling, showcasing substantial speedups on both synthetic and real-world datasets. The experiments are well-designed, with comparisons to state-of-the-art methods, demonstrating the practical utility of SPALS.
3. Originality: The application of leverage score sampling to ALS for CP decomposition is a novel contribution. The method's ability to handle both dense and sparse tensors efficiently distinguishes it from prior work, such as sketching-based approaches.
4. Significance: The proposed method addresses a critical scalability challenge in tensor decomposition, making it highly relevant for large-scale data analytics. Its potential applications in areas like signal processing, deep learning, and spatiotemporal modeling enhance its impact.
Weaknesses:
1. Clarity and Presentation: The paper suffers from several presentation issues. Symbols are often used before being defined, leading to confusion. Typos, redundant words, and formatting errors in equations detract from readability. For example, the notation for leverage scores and sampling probabilities could be clarified.
2. Incomplete Justifications: Some assertions lack proof or proper citations, such as the claim about the tightness of leverage score estimates. Providing these would strengthen the paper's rigor.
3. Limited Discussion of Related Work: While the paper references prior work, it could more explicitly differentiate SPALS from recent advancements in randomized tensor decomposition, such as [37]. A deeper discussion of the trade-offs between SPALS and sketching-based methods would be valuable.
Arguments for Acceptance:
- The paper presents a novel and impactful method with strong theoretical and empirical support.
- It addresses a significant problem in tensor decomposition, advancing the state of the art.
- The method is efficient, scalable, and broadly applicable.
Arguments Against Acceptance:
- The presentation issues and lack of clarity may hinder comprehension for readers unfamiliar with the topic.
- Some claims require additional justification or citations.
Recommendation: Accept with minor revisions. The technical contributions and empirical results are strong, but the paper would benefit from improved clarity, additional proofs/citations, and a more polished presentation.