This paper introduces a novel regularization method for matrix factorization, termed Smooth Sparse Online Dictionary-Learning (Smooth-SODL), which enforces sparsity and spatial smoothness in the learned dictionary atoms. The authors extend the Sparse Online Dictionary-Learning (SODL) framework by introducing a Laplacian penalty to promote spatially structured and interpretable components, specifically tailored for neuroimaging data. While the existence of a solution to the proposed optimization problem is not theoretically proven, the authors provide a practical algorithm based on block-coordinate descent (BCD) and demonstrate its convergence to a local optimum. Experimental results on task fMRI data from the Human Connectome Project (HCP) show that Smooth-SODL outperforms state-of-the-art methods, including SODL and Canonical ICA (CanICA), in terms of interpretability, stability, and predictive utility.
Strengths:
1. Originality and Contribution: The introduction of a Laplacian regularizer to enforce spatial smoothness is a significant contribution, addressing a key limitation of existing dictionary-learning methods in neuroimaging. This innovation bridges the gap between sparse and structured decomposition methods.
2. Experimental Validation: The authors provide comprehensive experimental results, demonstrating the superiority of Smooth-SODL over competing methods in terms of interpretability, explained variance, and prediction of behavioral variables. The method's ability to capture inter-subject variability across different data scales is particularly noteworthy.
3. Scalability: The proposed algorithm retains the scalability of the SODL framework, making it suitable for large-scale neuroimaging datasets, a critical requirement in the field.
4. Implementation: The authors' commitment to releasing their implementation as part of the Nilearn package enhances the reproducibility and practical impact of their work.
Weaknesses:
1. Theoretical Gaps: The lack of a formal proof for the existence of a solution to the proposed optimization problem is a notable limitation. While the experimental results are compelling, theoretical guarantees would strengthen the paper's claims.
2. Non-Convexity: The problem's non-convex nature means that convergence is only guaranteed to a local optimum. The authors do not provide an in-depth discussion of potential pitfalls or sensitivity to initialization.
3. Clarity and Writing Quality: The manuscript's clarity is hindered by dense technical descriptions and occasional grammatical errors (e.g., line 146). While the rebuttal improved the clarity score, further refinement is needed to make the paper more accessible to a broader audience.
4. Hyperparameter Tuning: The hyperparameter selection process, though empirically validated, remains heuristic and may pose challenges for practitioners.
Arguments for Acceptance:
- The paper addresses a critical challenge in neuroimaging by proposing a novel and effective method for structured dictionary learning.
- The experimental results convincingly demonstrate the method's advantages over state-of-the-art approaches.
- The scalability and planned open-source implementation increase the method's potential impact.
Arguments Against Acceptance:
- The lack of theoretical guarantees for the existence of a solution weakens the methodological rigor.
- The writing quality and clarity could be further improved to enhance readability.
Recommendation:
Overall, this paper makes a strong contribution to the field of neuroimaging and matrix factorization. While the theoretical gaps and writing issues are notable, the experimental results and practical utility outweigh these concerns. I recommend acceptance with minor revisions to address clarity and provide additional discussion on theoretical limitations.