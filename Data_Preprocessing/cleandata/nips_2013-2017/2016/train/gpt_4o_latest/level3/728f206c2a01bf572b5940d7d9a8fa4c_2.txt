The paper presents a novel approach to training Restricted Boltzmann Machines (RBMs) using Wasserstein loss as an objective, diverging from the traditional Kullback-Leibler (KL) divergence. The authors argue that Wasserstein loss, which incorporates a meaningful metric between observations, provides a more direct approach to density estimation. They derive the gradient of the Wasserstein distance from its dual formulation and demonstrate its application to RBMs, focusing on tasks such as image completion and denoising. The experiments highlight the impact of varying entropy regularization strength, revealing that stronger regularization leads to more concentrated models in image space. The paper also compares Wasserstein-trained RBMs to standard RBMs, showing improved performance in metric-sensitive tasks.
Strengths:
1. Novelty and Relevance: The use of Wasserstein loss for training RBMs is a novel contribution, addressing the limitations of KL divergence in capturing metric-sensitive data distributions. This aligns with recent trends in machine learning that emphasize the importance of incorporating data geometry into training objectives.
2. Theoretical Rigor: The derivation of the Wasserstein gradient and its integration into RBM training is well-grounded in theory. The authors provide clear mathematical formulations and justify their approach with sensitivity analyses and comparisons to KL divergence.
3. Practical Insights: The experiments on image completion and denoising tasks demonstrate the practical utility of Wasserstein-trained RBMs. The analysis of bias-variance trade-offs and the shrinkage effect provides valuable insights into the behavior of the proposed models.
4. Interesting Observations: The effect of increasing entropy regularization strength (Figure 4) is particularly intriguing, as it leads to more concentrated distributions in image space. This observation could inspire further research into the role of regularization in generative modeling.
Weaknesses:
1. Computational Limitations: A significant limitation is the high computational cost of Wasserstein training, which restricts experiments to small toy datasets. This raises concerns about the scalability of the approach to larger and more complex data distributions.
2. Lack of Intuition: While the paper highlights the effects of entropy regularization strength, it does not provide sufficient intuition for why stronger regularization leads to more concentrated models. A deeper exploration of this phenomenon would strengthen the paper.
3. Clarification Needed: The discrepancy between \( E{\hat{p}}[\alpha^] = 0 \) and \( E{p}[\alpha^] \neq 0 \) in the context of Equation 5 is not adequately explained. This could confuse readers and warrants further clarification.
Recommendation:
The paper makes a strong theoretical and empirical contribution to the field of generative modeling. However, the computational limitations and lack of intuition in key areas slightly detract from its impact. I recommend acceptance with minor revisions, specifically addressing the intuition behind entropy regularization effects and clarifying the discrepancy in Equation 5.
Arguments for Acceptance:
- Novel and theoretically sound approach to training RBMs.
- Demonstrates practical improvements in metric-sensitive tasks.
- Provides a foundation for future work on scaling Wasserstein training.
Arguments Against Acceptance:
- Limited scalability due to computational costs.
- Insufficient explanation of certain phenomena, such as the effects of entropy regularization.
In conclusion, the paper advances the state of the art in generative modeling and offers a promising direction for future research, despite some limitations in scalability and interpretability.