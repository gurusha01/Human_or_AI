Review
This paper proposes a nonlinear spectral method for training a specific class of feedforward neural networks with global optimality guarantees and linear convergence rates. The authors focus on networks with one and two hidden layers, utilizing generalized polynomial activation functions and non-negative weights. The method is tested on small-scale UCI datasets, and the authors claim competitive performance compared to standard models like ReLU networks and kernel SVMs.
Strengths:
1. Theoretical Contribution: The paper addresses the challenging problem of global optimization in neural network training, which is typically non-convex and lacks convergence guarantees. The use of nonlinear spectral methods to achieve global optimality with linear convergence is novel and theoretically interesting.
2. Mathematical Rigor: The authors provide detailed proofs and a clear theoretical framework, leveraging fixed-point theory and spectral radius conditions. The inclusion of convergence guarantees is a significant strength.
3. Efficiency: The method avoids the need for hyperparameter tuning, such as learning rates, which is a common challenge in stochastic gradient descent (SGD). The experiments demonstrate faster convergence compared to SGD.
4. Practical Implementation: The authors implement the method and test it on real-world datasets, which is a step forward compared to prior theoretical works that lacked experimental validation.
Weaknesses:
1. Restrictive Assumptions: The requirement for non-negative weights and generalized polynomial activation functions is highly restrictive and counterintuitive for practical applications. These constraints limit the expressive power of the model and its applicability to real-world problems.
2. Lack of Justification for Assumptions: The paper does not provide sufficient justification for these assumptions or explore ways to relax them. This weakens the practical relevance of the method.
3. Dataset Limitations: The experiments are conducted on small, low-dimensional datasets, which do not adequately demonstrate the scalability or effectiveness of the method for high-dimensional, complex real-world data.
4. Unfair Baseline Comparisons: The comparison with linear SVMs is not meaningful, as nonlinear SVMs (e.g., with RBF or polynomial kernels) are more appropriate baselines. This undermines the claims of competitive performance.
5. Expressive Power: While the authors claim that the networks are expressive enough, the experimental results do not strongly support this assertion. The method underperforms compared to kernel SVMs and struggles with datasets where ReLU networks excel.
6. Scalability Concerns: The spectral radius of the matrix grows with the number of hidden units, making the method computationally challenging for high-dimensional data. This limitation is acknowledged but not addressed in the paper.
Pro and Con Arguments for Acceptance:
Pros:
- Novel theoretical contribution with rigorous proofs.
- First practical implementation of a globally optimal training method for neural networks.
- Faster convergence compared to SGD.
Cons:
- Restrictive and impractical assumptions limit real-world applicability.
- Experimental validation is insufficient due to small datasets and unfair baselines.
- Scalability issues remain unresolved.
Recommendation:
While the paper makes a significant theoretical contribution, its practical relevance is limited by restrictive assumptions, insufficient experimental validation, and scalability concerns. The work is better suited for a theoretical venue or as a foundation for further research that addresses these limitations. I recommend rejection for NIPS but encourage the authors to refine their approach and explore more practical extensions.