The paper investigates the efficient estimation of generalized linear models (GLMs) in the large-scale regime where \( n \gg p \). The authors leverage a proportionality relationship between GLM and ordinary least squares (OLS) estimators, a concept previously noted in specific contexts but underexplored for its computational implications. They propose a novel two-step estimator, termed the Scaled Least Squares (SLS) estimator, which first computes OLS coefficients and then estimates a proportionality constant via a root-finding algorithm. Theoretical guarantees are provided, and the method is shown to achieve the same accuracy as the maximum likelihood estimator (MLE) with significantly reduced computational cost. Benchmark experiments demonstrate that SLS achieves the desired test error faster than standard first- and second-order optimization algorithms, such as Newton-Raphson and BFGS.
Strengths:  
The paper is well-written, clear, and well-organized, making it accessible to readers familiar with GLMs and optimization methods. The proposed method is novel and addresses a critical challenge in large-scale data analysis, where computational efficiency is paramount. The theoretical analysis is rigorous, providing bounds on the estimator's performance and demonstrating its advantages in the \( n \gg p \) regime. The experimental results are compelling, showing that SLS outperforms traditional optimization methods in terms of computational efficiency while maintaining comparable accuracy. The authors also highlight the potential for extending their approach to regularized GLMs, opening avenues for future research.
Weaknesses:  
A key limitation is the reliance on sub-Gaussian assumptions for the theoretical results, which restricts the applicability of the method to datasets with heavy-tailed distributions. While the authors acknowledge this limitation, they do not provide experiments to assess the robustness of SLS in such scenarios. Additionally, the paper omits comparisons with stochastic optimization methods, such as SVRG, which are known to perform well in large-scale settings and may offer similar computational benefits without the constraints of sub-Gaussian assumptions. The naive subsampling approach for OLS estimation may also degrade performance in the presence of heavy-tailed data, where random-projection-based estimators might be more robust, albeit at higher computational cost.
Pro and Con Arguments for Acceptance:  
Pro:  
- The paper addresses a significant problem in large-scale GLM estimation with a novel and computationally efficient approach.  
- Theoretical contributions are rigorous and well-supported by experimental results.  
- The method has practical implications for large-scale machine learning and statistics.  
Con:  
- Limited applicability to heavy-tailed data due to sub-Gaussian assumptions.  
- Lack of comparisons with stochastic optimization methods, which are relevant baselines in this context.  
- No experiments demonstrating robustness to non-ideal conditions, such as heavy-tailed predictors or response variables.  
Conclusion:  
This paper makes a valuable contribution to the field of large-scale optimization for GLMs by introducing a novel, efficient algorithm with strong theoretical and empirical support. However, addressing the limitations regarding heavy-tailed data and including comparisons with stochastic methods would strengthen the work further. I recommend acceptance, provided the authors address these concerns in a revised version.