The paper presents a novel objective function for training Restricted Boltzmann Machines (RBMs), replacing the traditional Kullback-Leibler (KL) divergence with a smoothed Wasserstein distance. This approach is motivated by the limitations of KL divergence in capturing metric-sensitive problems, where the distance between observations plays a critical role. The authors provide a thorough theoretical derivation of the gradient for the proposed Wasserstein objective and demonstrate its practical utility through applications in data completion and denoising tasks.
Strengths:
1. Novelty and Motivation: The paper addresses a well-motivated problem by proposing a metric-aware training objective for RBMs. The use of Wasserstein distance, which inherently incorporates the geometry of the data, is a significant departure from traditional KL-based objectives. The authors clearly articulate the limitations of KL divergence and the advantages of their approach.
   
2. Theoretical Rigor: The paper provides a detailed derivation of the gradient for the smoothed Wasserstein distance, supported by sensitivity analysis and stability considerations. This theoretical contribution is valuable for the broader machine learning community.
3. Experimental Design: The experiments are well-designed, with comparisons against standard RBMs, kernel density estimation, and Wasserstein RBMs. The results convincingly demonstrate the advantages of the proposed method in metric-sensitive tasks like data completion and denoising.
4. Clarity and Organization: The paper is well-written, with a logical structure that guides the reader through the motivation, theoretical framework, and experimental results. The inclusion of visualizations, such as PCA plots and error decompositions, enhances the presentation.
5. Significance: The proposed method advances the state of the art in RBM training by addressing a fundamental limitation of existing approaches. The demonstrated improvements in practical tasks suggest that this work has the potential to influence future research in generative modeling and metric-sensitive applications.
Weaknesses:
1. Limited Comparisons: While the paper compares the proposed method against standard RBMs and kernel density estimation, it lacks comparisons with other distance metrics, such as Euclidean or Hamming distances. Including these baselines would strengthen the empirical evaluation.
2. Cluster Bias: The Wasserstein RBM tends to produce shrinked, cluster-like distributions, which may limit its applicability to datasets requiring diverse representations. This trade-off between bias and variance is acknowledged but could be explored further.
3. Scalability: The computational cost of Wasserstein distance, even in its smoothed form, may limit its applicability to larger datasets or higher-dimensional problems. While the authors discuss scalability briefly, a more detailed analysis or experiments on larger datasets would be beneficial.
Recommendation:
I recommend acceptance of this paper, as it makes a significant theoretical and practical contribution to the field of generative modeling. The introduction of a Wasserstein-based objective for RBMs is a novel idea with clear advantages for metric-sensitive tasks. However, addressing the noted weaknesses, particularly the inclusion of additional baselines and scalability analysis, would further strengthen the work.
Arguments for Acceptance:
- Novel and well-motivated approach.
- Strong theoretical foundation and experimental validation.
- Clear writing and logical organization.
- Practical utility demonstrated in real-world tasks.
Arguments Against Acceptance:
- Limited comparisons with other distance metrics.
- Potential limitations in scalability and diversity of learned distributions.
In summary, the paper is a high-quality contribution to the field and is likely to inspire further research on metric-aware generative modeling.