This paper provides a detailed and rigorous analysis of the Expectation Maximization (EM) algorithm for balanced mixtures of two Gaussians with known variance, focusing specifically on mean estimation. The authors leverage Population EM as their primary analytical tool to bridge the gap between maximum likelihood theory and the practical computational properties of EM. The paper characterizes the stationary points of both Population EM and Sample-based EM, proving convergence properties and statistical consistency under specific conditions. By focusing on asymptotic limits, the authors complement existing literature, which often emphasizes initialization schemes or finite-sample behavior.
Strengths:
1. Technical Depth and Rigor: The paper provides a thorough theoretical analysis of EM, including convergence rates, fixed points, and statistical consistency. The use of Population EM as a lens to study the behavior of Sample-based EM is both insightful and well-executed.
2. Novel Contributions: The global analysis of EM without separation or initialization conditions is a significant advancement over prior work, which often required strong assumptions about initialization or component separation.
3. Practical Relevance: By connecting EM's computational behavior to maximum likelihood theory, the paper offers insights that are valuable for both theoretical understanding and practical applications.
4. Clarity of Results: The characterization of stationary points and their implications for the expected log-likelihood function is clearly presented, with results that are easy to interpret and connect to broader statistical principles.
5. Relation to Prior Work: The paper situates its contributions well within the existing literature, referencing key works and highlighting how its results extend or complement prior findings.
Weaknesses:
1. Limited Scope: While the focus on mixtures of two Gaussians is a reasonable starting point, the results may not generalize to more complex settings, such as mixtures with unequal variances, unequal mixing weights, or more than two components. This limits the broader applicability of the findings.
2. Asymptotic Focus: The emphasis on asymptotic behavior, while valuable, may not fully address practical scenarios where sample sizes are finite. Non-asymptotic analyses or empirical validations would strengthen the paper.
3. Robustness to Model Misspecification: The paper does not explore how EM performs when the data deviate from the assumed Gaussian mixture model, which is a common issue in real-world applications.
4. Variance Estimation: The analysis is restricted to mean estimation with known variance. Extending the framework to jointly estimate both means and variances would make the results more comprehensive.
Suggestions for Improvement:
1. Extend the analysis to include mixtures with more than two Gaussians or unequal variances and mixing weights.
2. Investigate the robustness of EM to model misspecification, such as data drawn from a single Gaussian or other distributions.
3. Provide empirical results or simulations to validate the theoretical findings, especially in finite-sample settings.
4. Explore the implications of the results for high-dimensional settings where the dimensionality grows with the sample size.
Recommendation:
This paper makes a solid theoretical contribution to the understanding of EM for Gaussian mixtures, addressing important gaps in the literature. While its scope is somewhat narrow and its practical implications could be further explored, the depth and rigor of the analysis make it a valuable addition to the field. I recommend acceptance, with the suggestion that the authors address the outlined weaknesses in future work.