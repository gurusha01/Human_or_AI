Review
This paper addresses the problem of learning Supervised PageRank models by formulating it as a constrained non-convex optimization problem. The authors propose two solution approaches: a gradient-free method (GFN) using an inexact zero-order oracle and a gradient-based method (GBN) using an inexact first-order oracle. They provide theoretical convergence guarantees for both methods and evaluate their performance on a real-world web page ranking task.
Strengths:
1. Novelty and Contributions: The paper introduces two optimization methods tailored for constrained non-convex problems with inexact oracles, which is a significant extension of existing approaches. The gradient-free method is particularly noteworthy as it avoids the need for derivative computations, which can be computationally expensive for large-scale problems.
2. Theoretical Guarantees: The authors provide rigorous theoretical analysis, including convergence rate guarantees for both methods. This is a major improvement over the state-of-the-art gradient-based method [21], which lacks such guarantees.
3. Practical Relevance: The application of these methods to a real-world ranking task in a commercial search engine setting (Yandex) demonstrates the practical utility of the proposed approaches. The results show that both GFN and GBN outperform the baseline methods in terms of ranking quality.
4. Algorithmic Insights: The paper explores the trade-off between computational complexity and accuracy, which is crucial for large-scale optimization problems. The use of the Nesterovâ€“Nemirovski method for approximating stationary distributions is another valuable contribution.
Weaknesses:
1. Incomplete Main Text: The experimental results, which are critical for assessing the practical impact of the proposed methods, are relegated to the supplementary material. This leaves the main text incomplete and makes it harder for readers to evaluate the empirical performance without consulting additional documents.
2. Convexity Assumption in Theorem 1: Theorem 1 assumes convexity, but the function in Equation (2.4) is non-convex. The authors do not adequately justify the choice of the set $\Phi$ to ensure convexity, which raises concerns about the applicability of the theorem.
3. Convergence Analysis in Theorem 3: While Theorem 3 provides a bound for $MK(xK - x{K+1})$, it is unclear whether this bound guarantees convergence to a stationary point. This ambiguity weakens the theoretical foundation of the gradient-based method.
4. Algorithm Efficiency: Figure 1 in the supplementary material shows that the gradient-free method converges slowly. The authors should report the computational efficiency of both algorithms more explicitly, especially in comparison to the state-of-the-art method.
5. Clarity and Organization: The paper is dense and technical, which may hinder accessibility for a broader audience. The presentation of the algorithms and theoretical results could benefit from clearer explanations and more intuitive insights.
Recommendation:
- Arguments for Acceptance: The paper makes significant theoretical and practical contributions to the field of optimization for graph-based learning tasks. The proposed methods are novel, well-analyzed, and demonstrate superior performance in experiments. The work is relevant to the NIPS community, particularly for researchers in optimization and graph learning.
- Arguments Against Acceptance: The incomplete presentation of experimental results in the main text, the lack of clarity regarding convexity assumptions, and the slow convergence of the gradient-free method are notable drawbacks.
Final Decision:
I recommend acceptance with minor revisions. The authors should move key experimental results from the supplementary material to the main text, clarify the convexity assumptions in Theorem 1, and provide a more detailed discussion of algorithm efficiency. These changes would significantly improve the paper's clarity and impact.