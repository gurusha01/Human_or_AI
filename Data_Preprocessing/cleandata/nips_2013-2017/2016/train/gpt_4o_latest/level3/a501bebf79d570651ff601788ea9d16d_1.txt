The paper presents a dynamic programming-based memory allocation strategy to optimize GPU memory usage during backpropagation through time (BPTT) for recurrent neural networks (RNNs). The proposed method balances the trade-off between caching intermediate results and recomputation, enabling memory-constrained training of RNNs. The authors claim that their approach can fit within almost any user-defined memory budget while minimizing computational cost. Empirical results demonstrate significant memory savings, with the method reducing memory usage by 95% for sequences of length 1000 while incurring only a 33% increase in computational time compared to standard BPTT. The paper builds on prior work, particularly Chen's divide-and-conquer algorithm, and provides a more flexible memory-saving strategy.
Strengths:  
1. Technical Contribution: The use of dynamic programming to derive an optimal memory allocation policy is a clear and well-motivated contribution. The approach is theoretically sound, with detailed derivations and asymptotic bounds provided.  
2. Practical Relevance: The method addresses a critical bottleneck in training RNNs on GPUs, where memory limitations often constrain sequence length or model size. The ability to adapt to arbitrary memory budgets is a notable advantage over existing methods.  
3. Empirical Validation: The paper provides empirical results demonstrating significant memory savings (up to 95%) with acceptable computational overhead, which is a meaningful improvement for long sequences.  
Weaknesses:  
1. Incremental Contribution: While the proposed method generalizes Chen's divide-and-conquer algorithm, the improvement appears incremental rather than groundbreaking. The memory savings, while significant, are not orders of magnitude greater, limiting the transformative impact of the work.  
2. Limited Experiments: The experiments are primarily conducted on synthetic setups and lack evaluation on large-scale datasets or real-world applications. This omission weakens the practical significance of the results and raises questions about the method's generalizability.  
3. Minimal Speedup: For very long sequences, the method shows only minimal speedup, which may reduce its appeal for practitioners prioritizing computational efficiency.  
4. Broader Impact: Memory savings alone may not substantially improve deep neural network training, as other factors like convergence speed and computational cost often dominate.  
Clarity: The paper is generally well-written and organized, but minor issues, such as typos ("as expensive than a orward operation" on page 7), detract from its polish. The supplementary materials also contain errors that should be addressed.  
Arguments for Acceptance:  
- The paper provides a technically sound and practical solution to an important problem in RNN training.  
- The dynamic programming approach is novel in its ability to adapt to arbitrary memory budgets.  
- The empirical results demonstrate meaningful memory savings, particularly for long sequences.  
Arguments Against Acceptance:  
- The contribution is incremental and does not represent a significant leap over existing methods.  
- The lack of experiments on real-world datasets limits the paper's practical impact.  
- The method's minimal speedup for long sequences reduces its overall utility.  
Recommendation: Weak Accept. While the paper is technically solid and addresses a relevant problem, its incremental nature and limited experimental scope temper its impact. Expanding the experimental evaluation and addressing minor clarity issues would strengthen the paper.