The paper presents a novel method, Stochastic Multiple Choice Learning (sMCL), for training ensembles of deep networks to minimize oracle loss, which is particularly useful in scenarios requiring multiple plausible predictions. The authors effectively position their work within the Multiple Choice Learning (MCL) paradigm, extending it to deep networks, and propose a stochastic block gradient descent optimization approach that is compatible with back-propagation and supports end-to-end training. This method is shown to outperform existing baselines across diverse tasks, including image classification, semantic segmentation, and image captioning, while being computationally efficient.
Strengths:
1. Technical Contribution: The introduction of sMCL is a significant advancement over traditional MCL approaches, as it avoids the computational overhead of retraining models multiple times. The "winner-take-gradient" strategy is elegant and well-suited to modern deep learning architectures.
2. Broad Applicability: The method is architecture-agnostic and task-agnostic, as demonstrated by its application to three distinct tasks. This generality enhances its potential impact.
3. Experimental Validation: The authors provide comprehensive experiments showing that sMCL achieves superior oracle accuracy compared to classical ensembles, MCL, and other baselines. The 5x speedup over MCL is particularly noteworthy.
4. Interpretability: The paper highlights how sMCL induces specialization among ensemble members, which is both interpretable and beneficial for capturing task-specific ambiguities.
5. Clarity and Simplicity: The method is parameter-free and straightforward to implement, making it accessible to practitioners.
Weaknesses:
1. Initialization Concerns: The paper does not adequately address the impact of initialization on the performance of sMCL. Specifically, it is unclear how the use of random initialization compares to pre-trained networks, which could significantly influence results.
2. Baseline Comparisons: The comparison settings lack clarity, particularly regarding whether baseline methods use the same pre-trained networks as sMCL. This omission could affect the fairness of the comparisons.
3. Ensemble Diversity: While oracle accuracy is used as the primary evaluation metric, the paper does not provide sufficient analysis of ensemble diversity, which is a key aspect of ensemble learning.
4. Related Work: The paper does not compare its method to "Why M Heads are Better than One" (Lee et al., 2015), a relevant prior work that could provide additional context for the contributions of sMCL.
Suggestions for Improvement:
- Include experiments to analyze the sensitivity of sMCL to different initialization strategies.
- Clarify whether baseline methods use the same pre-trained networks as sMCL to ensure fair comparisons.
- Provide a deeper analysis of ensemble diversity, potentially using metrics such as pairwise disagreement or diversity indices.
- Compare sMCL to the work of Lee et al. (2015) to strengthen the positioning of the method within the literature.
Recommendation:
This paper makes a strong contribution to the field of ensemble learning for deep networks, addressing a practical and impactful problem. Despite some concerns about initialization and baseline comparisons, the method's simplicity, efficiency, and broad applicability make it a valuable addition to the literature. I recommend acceptance, provided the authors address the noted weaknesses in a revision.