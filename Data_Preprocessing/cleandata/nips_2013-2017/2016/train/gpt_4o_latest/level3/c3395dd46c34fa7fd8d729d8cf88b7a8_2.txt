This paper introduces a general framework for cooperative human-robot interaction, termed Cooperative Inverse Reinforcement Learning (CIRL), which formalizes value alignment as a cooperative, partial-information game between a human and a robot. The robot seeks to maximize the human's reward function, which it initially does not know. The authors reduce the CIRL problem to a POMDP, enabling computationally efficient solutions compared to general Dec-POMDPs. They also demonstrate that traditional IRL approaches, which assume humans act optimally in isolation, are suboptimal in CIRL settings. A key insight is that humans may adopt a "best response" strategy to teach robots more effectively, deviating from purely optimal behavior. The paper provides an approximation scheme for linear reward functions and validates its findings through experiments in a navigation task, showing that the "best response" strategy outperforms expert demonstrations in teaching robots.
Strengths:
The paper provides a unifying framework that bridges inverse reinforcement learning, optimal teaching, and cooperative decision-making. By reducing CIRL to a POMDP, the authors offer a tractable approach to solving what would otherwise be an intractable problem. The insight that humans may not act optimally but instead strategically teach is both novel and practically significant, with implications for designing better human-robot interaction systems. The experimental results convincingly demonstrate the advantages of the proposed approach, and the paper provides a clear connection between theoretical results and practical applications. The work is timely and addresses a critical challenge in AI safety and human-robot collaboration.
Weaknesses:
While the framework is conceptually valuable, the theoretical contributions lack significant novelty. The reduction to a POMDP, while useful, is relatively straightforward given existing results in Dec-POMDP literature. The use of Nash equilibrium terminology is questionable, as the framework is cooperative rather than non-cooperative. Additionally, the assumption that both agents know the probability distribution over \(\theta\) is unclear and may not hold in real-world scenarios. The exposition is sometimes imprecise, with hand-wavy technical explanations and inconsistent definitions of the coordination POMDP. The supplementary material does not fully resolve these ambiguities. Furthermore, it is unclear whether the IRL approach assumes only one demonstrated trajectory, which could affect the generalizability of the experimental results. Lastly, the manuscript contains several typos, misplaced figures, and grammatical errors, which detract from its clarity.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses an important problem in human-robot interaction, provides a unifying framework, and offers practical insights into human teaching strategies.
- Pro: The experimental results are compelling and demonstrate the practical utility of the proposed approach.
- Con: The theoretical contributions are incremental, and some assumptions (e.g., shared knowledge of \(\theta\)) are unrealistic.
- Con: The exposition lacks rigor in places, and the manuscript requires significant polishing.
Recommendation: Weak Accept. While the paper's theoretical contributions are not groundbreaking, its practical insights and experimental validation make it a valuable addition to the conference. However, the authors should address the clarity and rigor issues before publication.