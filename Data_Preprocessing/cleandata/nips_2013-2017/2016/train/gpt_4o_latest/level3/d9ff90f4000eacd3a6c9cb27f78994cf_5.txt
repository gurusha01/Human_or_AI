This paper addresses a critical challenge in recurrent neural networks (RNNs): the vanishing and exploding gradient problem, particularly in learning long sequences. The authors focus on unitary recurrent neural networks (uRNNs), which leverage unitary hidden-to-hidden weight matrices to stabilize gradients. While prior work has utilized restricted-capacity unitary matrices, this paper identifies their limitations in representational power for dimensions greater than 7 and introduces "Givens capacity" as a theoretical framework to quantify these limitations. The authors propose a novel stochastic gradient descent method for training full-capacity unitary matrices, which preserves unitary properties while offering higher representational power. Experimental results demonstrate that full-capacity uRNNs outperform both restricted-capacity uRNNs and LSTMs across synthetic and real-world tasks, including speech prediction and permuted pixel-by-pixel MNIST classification.
Strengths:  
1. Technical Soundness: The paper is well-grounded in theory, providing a rigorous argument based on Sard's theorem to quantify the limitations of restricted-capacity unitary matrices. The proposed optimization method on the Stiefel manifold is mathematically elegant and avoids common pitfalls like gradient clipping.  
2. Empirical Validation: The experimental results are comprehensive, covering synthetic tasks (e.g., system identification, copy memory) and real-world applications (e.g., speech prediction, MNIST). The performance gains of full-capacity uRNNs over restricted-capacity uRNNs and LSTMs are significant and consistent.  
3. Novelty: The introduction of "Givens capacity" and the development of a full-capacity optimization method represent meaningful contributions to the field. The work advances the state-of-the-art in addressing long-term dependencies in RNNs.  
4. Clarity: The paper is well-organized, with clear theoretical derivations and detailed experimental setups. The inclusion of open-source code enhances reproducibility.  
Weaknesses:  
1. Practical Applicability: While the theoretical and empirical results are promising, the paper does not fully explore the practical scalability of full-capacity uRNNs to larger datasets or more complex tasks.  
2. Computational Overhead: The proposed method involves matrix inversions and Stiefel manifold optimization, which may introduce computational overhead compared to simpler architectures like LSTMs. A discussion of computational trade-offs would strengthen the paper.  
3. Limited Exploration of Trade-offs: The paper briefly mentions the trade-off between hidden state dimension and capacity but does not explore this in depth. Further analysis could provide insights into designing efficient architectures.  
Arguments for Acceptance:  
- The paper addresses a fundamental problem in RNNs, making a significant theoretical and practical contribution.  
- The proposed method outperforms state-of-the-art models in multiple tasks, demonstrating its potential for broad applicability.  
- The work is rigorous, well-written, and reproducible, meeting the standards of a high-quality scientific contribution.  
Arguments Against Acceptance:  
- The computational feasibility of the proposed method for large-scale applications remains unclear.  
- The exploration of trade-offs and practical limitations is somewhat limited.  
Recommendation: Accept with minor revisions. The paper is a strong contribution to the field, but a deeper discussion of computational trade-offs and practical scalability would enhance its impact.