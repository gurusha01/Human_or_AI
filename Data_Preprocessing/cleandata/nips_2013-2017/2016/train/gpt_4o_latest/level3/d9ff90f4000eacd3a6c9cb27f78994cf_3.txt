The paper introduces a novel recurrent neural network (RNN) architecture, the full-capacity unitary recurrent neural network (uRNN), which optimizes its recurrent matrix over the entire space of unitary matrices using the Stiefel manifold. This approach addresses a critical limitation in prior work, where unitary matrices were restricted to parameterized subsets, leading to constrained representational capacity. By leveraging Givens decomposition and gradient descent on the Stiefel manifold, the authors propose a simple yet effective optimization method that avoids gradient clipping and learning rate adaptation.
Strengths:
The paper makes a significant theoretical and practical contribution to the field of RNNs. The theoretical analysis rigorously demonstrates the limitations of restricted-capacity unitary matrices and provides a clear argument for the need for full-capacity optimization. The proposed optimization method is elegant and computationally efficient, requiring only a matrix inverse as the additional computational overhead. Experimentally, the full-capacity uRNN outperforms both LSTMs and restricted-capacity uRNNs across a variety of tasks, including synthetic system identification, long-term memory tasks, speech frame prediction, and pixel-by-pixel MNIST classification. Notably, the model achieves state-of-the-art performance on the permuted MNIST task, highlighting its superior ability to model long-term dependencies.
Weaknesses:
The paper has some limitations that should be addressed. First, while the introduction and Givens decomposition analysis are clear, the optimization method section could benefit from additional details, particularly regarding the practical implementation of the Stiefel manifold optimization. Second, the experimental comparisons with prior uRNN methods are limited by mismatched trainable parameters, which may confound the results. Additionally, the optimization objective for the speech prediction task (MSE in linear frequency space) does not align well with perceptual metrics like PESQ or STOI, and optimizing directly in mel-space could yield better results. Minor issues include the need for clarification on the random initialization of P (line 80), the removal of an extra "define" (line 101), and whether the bound in Theorem 3.1 is tight.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a critical limitation in prior RNN architectures, provides a novel and theoretically sound optimization method, and demonstrates strong empirical performance across diverse tasks.
- Con: The clarity of the optimization method section and the alignment of the optimization objective with perceptual metrics could be improved. Experimental comparisons with prior methods are not entirely fair due to parameter mismatches.
Conclusion:
This paper represents a high-quality scientific contribution, advancing the state of the art in RNN architectures by addressing the vanishing/exploding gradient problem through full-capacity unitary matrices. While some improvements in clarity and experimental design are needed, the paper's strengths outweigh its weaknesses. I recommend acceptance, with minor revisions to address the noted issues.