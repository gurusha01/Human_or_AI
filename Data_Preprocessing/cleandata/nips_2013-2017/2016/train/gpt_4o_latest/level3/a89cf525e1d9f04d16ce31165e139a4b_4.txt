The paper presents a method to incorporate sparse priors within the Information Bottleneck (IB) framework for learning encoders and decoders, using a variational approximation to solve the optimization problem. The proposed method is capable of learning Gabor-like filters on image patches, akin to traditional sparse coding, and extends to a kernelized version for non-linear encoders. The authors demonstrate the method's effectiveness on image inpainting tasks for occluded handwritten digits.
Strengths:  
The incorporation of sparse priors into the IB framework is an interesting direction, as it bridges the gap between sparse coding and information-theoretic approaches. The use of variational approximations to make the IB framework tractable for high-dimensional data is a notable contribution. The kernelized extension is a valuable addition, enabling the method to handle non-linear relationships between input and relevance variables. The paper demonstrates the ability of the sparse IB method to recover meaningful features, such as Gabor-like filters, and highlights its potential application in perceptual tasks like image inpainting and denoising. The experiments on handwritten digits provide a concrete example of the method's utility.
Weaknesses:  
While the sparse IB framework is conceptually appealing, its novelty is limited. Sparse priors have been extensively studied in methods like LASSO and sparse autoencoders, and the paper does not sufficiently differentiate its approach from these existing methods. The experimental evaluation is limited in scope, focusing only on simple 9x9 image patches, which makes it unclear how the method would scale to more complex datasets or tasks. Additionally, the example tasks (e.g., image denoising and inpainting) are already well-addressed by conventional sparse coding techniques, raising questions about the practical advantages of the IB framework. The paper lacks quantitative comparisons with baseline methods, making it difficult to assess the proposed method's relative performance. Furthermore, some aspects of the methodology require clarification, such as the equation \( q(y|r) = N(y|Ur) \) (Line 73), and the guidance on choosing relevance variables \( Y \) is insufficient.
Pro and Con Arguments for Acceptance:  
- Pro: The paper introduces a novel application of the IB framework with sparse priors and provides a kernelized extension for non-linear encoders. The method demonstrates the ability to learn interpretable features and has potential applications in perceptual tasks.  
- Con: The novelty of incorporating sparsity is limited, and the experimental scope is narrow. The lack of quantitative comparisons and insufficient justification for the method's advantages over existing approaches weaken its contribution.
Recommendation:  
While the paper presents an interesting extension of the IB framework, the limited novelty, insufficient experimental validation, and lack of clarity in certain aspects of the methodology suggest that it is not yet ready for acceptance. The authors are encouraged to expand the experimental evaluation, provide quantitative comparisons with baselines, and clarify methodological details in a future submission.