The paper introduces a novel dropout technique that leverages feature-specific multinomial distributions, departing from the traditional independent Bernoulli sampling. This approach, termed "multinomial dropout," assigns non-uniform sampling probabilities to features or neurons, enabling the model to focus on more informative features. The authors further extend this concept to propose an "evolutional dropout" for deep learning, which dynamically adapts sampling probabilities based on the evolving distributions of layer outputs during training. Theoretical analysis is provided, demonstrating that these methods achieve faster convergence and lower generalization error compared to standard dropout. Empirical results on benchmark datasets, including CIFAR-100, validate the practical benefits, showing significant improvements in both convergence speed (over 50%) and testing accuracy (over 10%).
Strengths:
1. Novelty and Originality: The proposed multinomial dropout introduces a theoretically grounded, data-dependent approach to dropout, which is a significant departure from the uniform sampling used in standard dropout. The extension to evolutional dropout for deep learning is innovative and addresses the internal covariate shift problem, drawing parallels to batch normalization.
   
2. Theoretical Rigor: The paper provides a comprehensive theoretical analysis, including risk bounds and optimization strategies, to justify the superiority of the proposed methods. This adds depth to the contribution and distinguishes it from purely empirical studies.
3. Practical Impact: The experimental results are compelling, with Figures 2 and 3 clearly illustrating the advantages of the proposed methods over standard dropout. The improvements in convergence speed and testing error are substantial, especially for deep learning tasks on datasets like CIFAR-100.
4. Clarity and Organization: The paper is well-structured, with a logical flow from theoretical foundations to practical implementations and experimental validation. The inclusion of comparisons with batch normalization further contextualizes the contribution.
Weaknesses:
1. Limited Scope of Experiments: While the experiments are thorough, they focus primarily on standard benchmark datasets. Additional experiments on larger-scale or more diverse datasets could strengthen the claims of generalizability.
2. Comparison with Related Work: Although the paper discusses related work, it could benefit from a more detailed comparison with other adaptive dropout techniques, such as Bayesian dropout or variational dropout, to better position its contribution.
3. Computational Overhead: The evolutional dropout requires on-the-fly computation of sampling probabilities, which may introduce additional computational overhead. While this is briefly addressed, a more detailed analysis of the trade-offs between computational cost and performance gains would be valuable.
Arguments for Acceptance:
- The paper presents a significant theoretical and practical advancement in dropout techniques, with clear empirical evidence of its benefits.
- The proposed methods address a critical limitation of standard dropout (uniform sampling) and provide a novel perspective on data-dependent regularization.
- The work has potential applications in both shallow and deep learning, making it broadly relevant to the NeurIPS community.
Arguments Against Acceptance:
- The computational overhead of evolutional dropout is not thoroughly analyzed, which may limit its practical applicability in resource-constrained settings.
- The experimental scope could be expanded to include more diverse datasets and comparisons with other advanced dropout techniques.
Recommendation:
Overall, the paper makes a strong contribution to the field of neural network regularization and optimization. Its theoretical rigor, coupled with practical improvements, makes it a valuable addition to the conference. I recommend acceptance, with minor revisions to address the computational overhead and expand the experimental comparisons.