The paper presents a novel approach to the "online" Principal Component Analysis (PCA) problem by constructing a coreset—a small, weighted subset of vectors—that approximates the sum of squared distances to any k-dimensional affine subspace. The authors address a long-standing theoretical challenge by proving the existence of such a coreset whose size is independent of both the number of input vectors (n) and their dimensionality (d). They also propose a computationally efficient algorithm to construct this coreset and demonstrate its utility by applying it to latent semantic analysis (LSA) of the entire English Wikipedia dataset.
Strengths:
1. Novelty and Significance: The result is groundbreaking, as it provides a deterministic method to compute a coreset of size independent of n and d, addressing a critical bottleneck in large-scale dimensionality reduction. The application to the Wikipedia dataset showcases the practical relevance of the approach.
2. Efficiency: The proposed algorithm is computationally efficient, with a running time and memory usage that scale well for sparse matrices, making it suitable for real-world datasets.
3. Experimental Validation: The paper benchmarks the algorithm against state-of-the-art methods and demonstrates its scalability and accuracy on synthetic data and the Wikipedia dataset. The ability to process such a large dataset is a significant achievement.
4. Clarity of Contribution: The paper clearly outlines its contributions, including theoretical guarantees, algorithmic efficiency, and practical applications.
Weaknesses:
1. Potential Error in Theorem 2: There appears to be a critical error in the proof of Theorem 2, where a vector is replaced with a matrix. This could undermine the theoretical guarantees of the algorithm and requires immediate clarification.
2. Intuition for Algorithm 1: While the algorithm is efficient, the paper lacks sufficient intuition or explanation for how the running time is reduced in proving Theorem 1. This could make the work less accessible to a broader audience.
3. Evaluation Scope: The experimental evaluation could be improved by testing the algorithm on special input matrices, such as those with specific sparsity patterns or adversarial structures, to better understand its robustness.
4. Lower Bound for Coreset Size: The paper does not provide a theoretical lower bound for the coreset size, which would strengthen the claim of optimality.
5. Presentation Issues: Although the paper is well-organized, there are several typos and inconsistencies in notation (e.g., operator norm vs. vector l2 norm). Specific issues include a missing variance term in line 5 of Algorithm 1 and a typo in line 267 (possibly referring to equation (5)).
Recommendation:
The paper is a strong candidate for acceptance due to its significant theoretical and practical contributions. However, the authors must address the potential error in Theorem 2 and provide more intuition for Algorithm 1. Additionally, improving the experimental evaluation and fixing the presentation issues would enhance the overall quality of the paper.
Arguments for Acceptance:
- Novel theoretical contribution with practical implications.
- Efficient algorithm with provable guarantees.
- Successful application to a large-scale, real-world dataset.
Arguments Against Acceptance:
- Potential error in a key theorem.
- Lack of clarity in some theoretical and algorithmic explanations.
- Limited evaluation on diverse input matrices.
In summary, while the paper has some weaknesses, its contributions to the field of dimensionality reduction are substantial and merit publication, provided the identified issues are addressed.