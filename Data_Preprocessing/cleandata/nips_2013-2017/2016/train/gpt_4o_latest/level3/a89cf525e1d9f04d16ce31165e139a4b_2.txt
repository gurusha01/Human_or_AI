This paper presents an extension of the Gaussian Information Bottleneck (IB) framework to sparse variables, enabling it to handle data with sparse or nonlinear manifold structures. The authors propose a variational approximation to the IB objective, analogous to variational EM, and introduce a kernelized version to address nonlinear relationships between input (X) and relevance variables (Y). The algorithm is evaluated on toy datasets, including image patches and handwritten digits, demonstrating its ability to extract sparse and relevant features.
Strengths:
1. Novelty and Technical Contribution: The extension of the IB framework to sparse variables is a significant contribution, particularly for data with sparse or nonlinear structures. The use of a student-t distribution for sparsity and the kernelized approach for nonlinearity are well-motivated and innovative.
2. Performance Demonstration: The experiments on toy datasets effectively illustrate the algorithm's ability to discover relevant structures. The sparse IB model outperforms the Gaussian IB model in recovering sparse features and demonstrates improved performance in occlusion tasks.
3. Theoretical Insights: The paper provides a clear connection between the IB framework and sparse coding, highlighting the differences in how the two approaches handle encoding and compression. The discussion of the kernelized IB framework and its relationship to kernel ridge regression (KRR) is insightful.
Weaknesses:
1. Limited Real-World Applications: The experiments are restricted to toy datasets, such as synthetic image patches and handwritten digits. While these are useful for illustrating the algorithm's capabilities, the lack of evaluation on real-world data (e.g., natural images) limits the practical significance of the work.
2. Clarity Issues: The discussion of "a separate set of inputs, X" in the context of sparse coding is unclear and may confuse readers. The authors should clarify how this differs from traditional sparse coding models.
3. Technical Errors: Equation 1 is incorrectly described as being maximized instead of minimized, which contradicts Tishby's original formulation. This error could mislead readers unfamiliar with the IB framework.
4. Visualization Choices: Figure 1, which uses toy image patches, would be more compelling if natural images were included to demonstrate the algorithm's applicability to real-world data.
5. Comparative Analysis: The occlusion example could benefit from a comparison with generative models, which are commonly used for similar tasks. This would provide better context for the algorithm's performance.
Recommendation:
The paper makes a strong technical contribution by extending the IB framework to sparse and nonlinear settings. However, its practical impact is limited by the lack of real-world applications and clarity in certain sections. I recommend acceptance with revisions, focusing on:
1. Adding experiments on real-world datasets to demonstrate broader applicability.
2. Clarifying the discussion on the separate input variable X and its connection to sparse coding.
3. Correcting the error in Equation 1 and improving the clarity of the manuscript overall.
4. Including natural images in visualizations and comparing the occlusion example to generative models.
Arguments for Acceptance:
- Novel and technically sound extension of the IB framework.
- Demonstrates improved performance in sparse and nonlinear settings.
- Provides theoretical insights into the relationship between IB and sparse coding.
Arguments Against Acceptance:
- Limited evaluation on real-world data.
- Clarity issues in the discussion and technical errors in the formulation.
- Lack of comparison to alternative methods in certain experiments.
Overall, the paper advances the state of the art in information-theoretic approaches to feature extraction but requires revisions to enhance its clarity and practical relevance.