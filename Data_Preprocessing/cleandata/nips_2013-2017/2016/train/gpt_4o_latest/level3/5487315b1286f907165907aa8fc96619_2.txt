The paper presents a novel convex training formulation for a two-layer probabilistic model \( p(z|y)p(y|x) \), addressing key challenges in unsupervised learning of structured predictors. By embedding the first-order optimality conditions of the inner optimization into the outer optimization using Lagrange multipliers and a saddle-point formulation, the authors avoid the bi-level optimization issues typically encountered in such models. The proposed semi-definite programming (SDP) relaxation, achieved by dropping the rank constraint, ensures global convexity while maintaining flexibility in modeling latent structures. The method relies on efficient MAP inference over the latent space, which is a weaker requirement than marginalization or sampling, though the paper lacks sufficient details on the computational efficiency of this step.
Strengths:
1. Novelty and Technical Contribution: The convex relaxation approach is innovative, particularly in its use of SDP to address bi-level optimization challenges. The embedding of first-order conditions into the optimization framework is a significant theoretical contribution.
2. Soundness of Derivations: The mathematical derivations are rigorous, and the relaxation appears well-justified. The characterization of low-rank solutions in the SDP relaxation is a notable technical insight.
3. Applications and Empirical Performance: The method is applied to transliteration and image inpainting tasks, demonstrating superior performance over baselines like CRF-AE and locally trained models. The transliteration task, in particular, showcases the model's ability to handle latent structured representations effectively.
Weaknesses:
1. Clarity: The paper is dense and challenging to follow, especially for readers unfamiliar with SDP or convex optimization. Key aspects, such as the tractability of certain maximization tasks and the rationale for parameter constraints, are not well-explained. For instance, the efficiency of MAP inference over the latent space is assumed but not substantiated with sufficient empirical or theoretical evidence.
2. Experimental Evaluation: The experimental comparison is limited, relying on outdated baselines. For transliteration, the comparison with [12] is insufficient, as more recent methods could provide a stronger benchmark. Additionally, the authors do not compare their SDP approach with gradient descent on the objective, which would provide valuable insights into the trade-offs between scalability and performance.
3. Scalability: The SDP formulation raises concerns about computational complexity for larger problems. While the authors propose refinements to improve scalability, these are not thoroughly evaluated.
4. Notation and Minor Issues: The notation is occasionally unclear, such as the use of the \( ^\prime \) operator, which should be explicitly clarified as denoting the transpose.
Pro vs. Con Acceptance:
- Pro: The paper introduces a novel and theoretically sound approach to a challenging problem, with promising empirical results in structured prediction tasks.
- Con: The lack of clarity, limited experimental evaluation, and scalability concerns detract from its overall impact.
Recommendation: While the paper makes a strong theoretical contribution, its practical applicability and clarity need improvement. I recommend acceptance conditional on revisions to address clarity issues, provide stronger experimental comparisons, and discuss scalability more thoroughly.