The paper proposes a novel approach to training Restricted Boltzmann Machines (RBMs) using the gamma-smoothed Wasserstein distance \( W\gamma(\hat{p}, p\theta) \) as the objective function, instead of the traditional Kullback-Leibler (KL) divergence. This method incorporates a metric between observations into the training process, which is argued to be more aligned with practical tasks like data completion and denoising. The authors derive the gradient of the Wasserstein distance and demonstrate its application to RBMs, presenting experimental results on datasets such as MNIST-small, UCI PLANTS, and binary codes.
Strengths:
1. Novelty: The use of Wasserstein distance for RBM training is a fresh perspective, particularly in its explicit incorporation of data metrics into the learning objective. This approach could inspire further exploration of metric-based training in generative models.
2. Practical Applications: The focus on tasks like data completion and denoising highlights the practical relevance of the proposed method, with experimental results showing competitive performance compared to standard RBMs.
3. Theoretical Contribution: The derivation of the Wasserstein gradient and its integration into RBM training is a significant technical contribution, supported by the use of the Sinkhorn algorithm for efficient computation.
4. Experimental Insights: The experiments provide valuable insights into the behavior of Wasserstein-trained RBMs, particularly the observed "shrinkage" and clustering effects, which are framed as both strengths and weaknesses depending on the application.
Weaknesses:
1. Key Concern - Shrinkage: The Gaussian shrinkage example in Section 4.3 raises serious doubts about the consistency of Wasserstein training. The observed bias toward compact clusters, even in simple cases like \( N(0, I) \), suggests potential flaws in the statistical consistency of the proposed approach.
2. KL Regularization: The addition of KL divergence regularization undermines the "purity" of the Wasserstein objective, making it unclear whether the observed improvements stem from Wasserstein training or the regularization.
3. Experimental Limitations: The experiments, while illustrative, are limited to small datasets and specific tasks. The lack of a direct comparison with a KL + entropic prior baseline leaves the source of observed effects ambiguous.
4. Clarity and Overclaiming: The technical explanations are often unclear, and the title overstates the scope of the work, as it focuses solely on RBMs rather than Boltzmann machines in general. Supplemental material could improve clarity.
5. Convergence Issues: The convergence rate of \( O(n^{-1/(D+1)}) \) in high dimensions is impractically slow, raising concerns about scalability to larger datasets.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded approach to RBM training.
- The focus on metric-based training is innovative and relevant for practical tasks like denoising and data completion.
- The experimental results, while limited, demonstrate the potential of the approach.
Arguments Against Acceptance:
- The observed shrinkage bias raises fundamental questions about the consistency of the Wasserstein objective.
- The reliance on KL regularization dilutes the contribution of the Wasserstein distance itself.
- The unclear technical presentation and limited experimental scope weaken the paper's overall impact.
Recommendation:
While the paper presents an interesting and novel idea, the concerns about statistical consistency, reliance on regularization, and unclear explanations significantly limit its contribution. A more thorough investigation of the shrinkage effect, clearer articulation of the method, and broader experiments are needed. I recommend rejection but encourage the authors to address these issues and resubmit.