The paper introduces a novel approach to training Boltzmann Machines (BMs) using the Wasserstein distance as an alternative to the conventional Kullback-Leibler (KL) divergence. This idea is well-motivated, leveraging the Wasserstein distance's ability to incorporate a meaningful metric between observations, which is particularly relevant for tasks like data completion and denoising. The work builds on prior theoretical studies of Wasserstein-based optimization but extends it to practical applications in the context of Restricted Boltzmann Machines (RBMs). While the paper presents a clear and innovative idea, several critical issues limit its overall impact.
Strengths:
1. Novelty and Relevance: The use of Wasserstein distance as a training objective is a fresh perspective in the field of generative modeling. The paper highlights the importance of incorporating metrics directly into the learning objective, which could inspire further research in density estimation and generative modeling.
2. Applications: The proposed method demonstrates practical utility in tasks like data completion and denoising, where the metric between observations plays a crucial role. This aligns with the broader goals of improving generative models for real-world applications.
3. Theoretical Contributions: The derivation of the Wasserstein gradient and its integration into RBM training is a significant technical contribution, showcasing a rigorous approach to adapting the Wasserstein distance for generative modeling.
Weaknesses:
1. Scalability Concerns: The sample-based nature of the Wasserstein distance raises questions about the method's scalability to high-dimensional data and large sample sizes. The experiments were conducted on unusually small datasets, which limits the generalizability of the results.
2. Experimental Limitations: The experiments are heavily regularized with KL divergence, and the improvements are only demonstrated on Wasserstein-like performance measures. Broader evaluations, such as comparisons on standard generative modeling benchmarks, are missing.
3. Clarity Issues: Equation 4 is difficult to interpret, particularly its dependency on the parameter γ > 0, which lacks sufficient explanation and clear bounds. Additionally, the term α* (line 67) is undefined and needs clarification.
4. Minibatch Training: The method appears unsuitable for minibatch training (line 115), which is a significant limitation for scaling to larger datasets.
5. Local Minima: The claim that local minima are more problematic for Wasserstein than KL divergence is unsubstantiated and requires further justification.
6. Shrinkage Behavior: The observed shrinkage effect (Section 4.3) raises concerns about its desirability, especially in higher-dimensional problems, where it may exacerbate issues like mode collapse.
7. Clustering Attribution: The clustering behavior observed in the experiments seems to stem more from the entropy regularizer than the Wasserstein distance itself (lines 177-178), which undermines the core claim of the paper.
Suggestions for Improvement:
1. Include a detailed discussion on the scalability of the proposed method with respect to dimensionality and sample size.
2. Provide additional experiments on larger and more diverse datasets to validate the method's robustness and generalizability.
3. Clarify the γ constraint and define α* explicitly in the final version.
4. Address the limitations of the sample-based distance measurement approach, particularly its potential exponential sample requirements in high-dimensional spaces.
5. Justify claims about local minima and disentangle the effects of the entropy regularizer from the Wasserstein objective.
Recommendation:
While the paper presents an interesting idea with potential, the significant concerns regarding scalability, experimental rigor, and clarity prevent it from being a strong candidate for acceptance in its current form. The authors are encouraged to address these issues in a revised version. Recommendation: Weak Reject.