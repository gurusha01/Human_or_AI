The paper presents a novel supervised extension of the Word Mover's Distance (WMD) for text classification, termed Supervised Word Mover's Distance (S-WMD). By incorporating supervised learning, the authors propose a method to learn word-specific importance weights and a linear transformation of word embeddings, optimizing for classification tasks. To address the computational inefficiency of WMD, the paper employs the Sinkhorn distance as a faster approximation, enabling practical scalability. The method is evaluated on eight datasets against 26 baselines, demonstrating superior average performance.
Strengths:
1. Technical Soundness and Clarity: The paper is well-written and provides a thorough mathematical analysis of the proposed method. The derivations of gradients and the optimization process are detailed and clear, ensuring reproducibility.
2. Performance: The empirical results are compelling, with S-WMD achieving the best average performance across diverse datasets. The use of 26 baselines highlights the robustness of the method.
3. Efficiency: By leveraging the Sinkhorn distance, the authors significantly reduce computational complexity, making the approach scalable to larger datasets.
4. Visualization and Interpretability: The visualization of learned word weights and t-SNE embeddings effectively demonstrates the interpretability and impact of the method.
Weaknesses:
1. Limited Novelty: While the paper extends WMD with supervision, the conceptual novelty is somewhat incremental. The core idea of supervised metric learning is well-established, and the contribution primarily lies in its adaptation to WMD.
2. Comparisons to Word Centroid Distance (WCD): The relevance of comparisons to WCD is unclear, as it is a much simpler baseline and not directly competitive with WMD or S-WMD.
3. Discussion on Regularization Parameter (λ): The role of the regularization parameter λ in the Sinkhorn distance is insufficiently explored. Its impact on convergence and whether the method achieves pure Wasserstein behavior or entropic Sinkhorn behavior remains ambiguous.
4. Initialization Sensitivity: The method's reliance on a good initialization (S-WCD) raises concerns about robustness. Poor initialization could degrade performance, as observed in some datasets.
Pro and Con Arguments for Acceptance:
Pros:
- Strong empirical performance across multiple datasets.
- Clear and rigorous mathematical exposition.
- Efficient approximation of WMD through Sinkhorn distance.
- Practical utility for text classification tasks.
Cons:
- Incremental novelty compared to prior work on WMD.
- Insufficient exploration of key hyperparameters (e.g., λ).
- Limited discussion on failure cases or datasets where S-WMD underperforms.
Recommendation:
The paper is a solid contribution to supervised metric learning for text classification, particularly in its efficient adaptation of WMD. However, the incremental nature of the contribution and the lack of deeper analysis on certain aspects (e.g., λ) temper its impact. I recommend acceptance, provided the authors address the weaknesses, especially by elaborating on the role of λ and clarifying the relevance of WCD comparisons.