This paper provides a global convergence analysis of the Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMMs) with known covariance and mixing proportions. The authors focus on two specific models: Model 1, where the mixture components are symmetric about the origin, and Model 2, where the means of the components are distinct but unknown. The key contributions include a characterization of the stationary points and convergence dynamics of the EM algorithm in the infinite sample limit (Population EM) and a proof of statistical consistency for Sample-based EM under certain initializations. The work builds on prior studies, such as Balakrishnan et al. (2014), by extending the analysis to global convergence without requiring separation or specific initialization conditions.
Strengths:
1. Technical Rigor: The paper provides a thorough theoretical analysis of the EM algorithm, including convergence guarantees for both Population EM and Sample-based EM. The results are mathematically sound and well-supported by detailed proofs.
2. Clarity of Presentation: The paper is well-organized, with clear definitions, theorem statements, and connections to related work. The inclusion of re-parameterizations for Model 2 aids in understanding the dynamics of the algorithm.
3. Relation to Prior Work: The authors position their contributions well within the existing literature, highlighting how their results complement or extend prior analyses of EM and GMMs.
Weaknesses:
1. Unclear Practical Significance: While the theoretical results are interesting, the paper does not convincingly articulate the practical importance of its findings. For instance, the scenarios where these results would provide a meaningful advantage over existing methods, such as the Method of Moments (MOM), are not well-delineated.
2. Lack of Novelty in Test Models: The choice of models (simple Gaussian mixtures) as test cases is not sufficiently justified. Model 1, in particular, has trivial likelihood properties that make global convergence unsurprising, limiting the impact of the results.
3. Overly Long Technical Supplement: The technical supplement is dense and lacks intuitive explanations of the proof techniques, making it challenging for readers to grasp the novelty or generalizability of the methods.
4. Limited Generalizability: The proof techniques, while rigorous, do not appear to generalize easily to more complex or realistic models, such as mixtures with unequal covariances or more than two components. This limits the broader applicability of the results.
5. Redundancy with Existing Methods: The paper does not adequately address why MOM, which can serve as a sufficient pilot estimator for EM (as shown by Balakrishnan et al., 2014), is insufficient for the studied models. This raises questions about the novelty of the contribution.
Arguments for Acceptance:
- The paper provides a rigorous and complete analysis of EM's global convergence for specific GMMs, which is a theoretically valuable contribution.
- The results extend existing work by removing separation and initialization constraints, which may inspire further research in this area.
Arguments Against Acceptance:
- The practical significance of the results is unclear, and the choice of models does not convincingly demonstrate the utility of the analysis.
- The lack of novelty in proof techniques and their limited generalizability reduce the broader impact of the work.
- Existing methods like MOM already address the estimation problem effectively, making the contribution incremental.
Recommendation:
While the paper is technically sound and well-presented, its limited practical significance, lack of novelty in proof techniques, and redundancy with existing methods make it a borderline submission. If the authors can better justify the practical importance of their results and clarify the novelty of their techniques, the paper could be more compelling. For now, I recommend reject with encouragement to revise and resubmit.