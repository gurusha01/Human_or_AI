This paper introduces a novel theoretical framework to evaluate the capacity of unitary matrix families and demonstrates its practical implications for training recurrent neural networks (RNNs). The authors argue that prior parameterizations of unitary matrices, such as those used in restricted-capacity unitary RNNs (uRNNs), fail to represent the full unitary group for hidden state dimensions greater than 7. To address this limitation, the paper proposes a full-capacity uRNN that optimizes over the entire unitary group using gradient descent on the Stiefel manifold. The proposed approach is validated through experiments on synthetic and real-world tasks, showing superior performance compared to restricted-capacity uRNNs and LSTMs. Surprisingly, the method is also computationally efficient, avoiding gradient clipping and learning rate adaptation.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous theoretical framework to evaluate the capacity of unitary matrix parameterizations. The use of Sard's theorem to demonstrate the limitations of prior parameterizations is a significant contribution.
2. Novel Optimization Approach: The authors introduce a method for optimizing full-capacity unitary matrices on the Stiefel manifold, which is both elegant and practical. The proposed gradient descent update is simple and avoids common pitfalls like gradient clipping.
3. Empirical Validation: The experiments convincingly demonstrate the advantages of full-capacity uRNNs across diverse tasks, including synthetic system identification, long-term memory tasks, speech frame prediction, and pixel-by-pixel MNIST classification. The results highlight the method's effectiveness in addressing long-term dependency challenges.
4. Computational Efficiency: Despite optimizing over the full unitary group, the method is computationally efficient and scales well, which is a surprising and valuable outcome.
5. Significance: The work addresses a fundamental limitation in prior uRNNs and advances the state-of-the-art in handling vanishing and exploding gradients in RNNs.
Weaknesses:
1. Experimental Scope: While the experiments are comprehensive, replicating results from prior work, such as on permuted MNIST, would strengthen the paper's claims and provide a direct comparison.
2. Capacity Evaluation: The paper does not fully address why a simple counting argument is insufficient for evaluating the capacity of unitary matrix families. Clarifying this point would enhance the theoretical discussion.
3. Trade-offs: The interplay between hidden state dimension \(N\) and the capacity of unitary parameterizations is briefly mentioned but not explored in depth. This could be an important direction for future work.
Pro and Con Arguments for Acceptance:
- Pro: The paper makes a significant theoretical and practical contribution by addressing a critical limitation in unitary RNNs, demonstrating superior performance across tasks, and proposing a computationally efficient optimization method.
- Con: The experimental validation could be further strengthened by replicating results from prior work and exploring the trade-offs between capacity and hidden state dimensions.
Recommendation: Accept. This paper is a strong scientific contribution, advancing both the theoretical understanding and practical application of unitary RNNs. It is well-aligned with the conference's focus on foundational and applied machine learning research.