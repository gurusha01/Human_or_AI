Review of "Multinomial Dropout for Shallow and Deep Learning"
This paper proposes a multinomial distribution-based dropout scheme, introducing data-dependent dropout (s-dropout) for shallow learning and evolutional dropout (e-dropout) for deep learning. The authors provide theoretical risk-bound analysis for s-dropout, demonstrating its convergence and generalization benefits. For deep learning, e-dropout adapts dropout probabilities dynamically based on mini-batch statistics, addressing internal covariate shift similarly to batch normalization. Empirical results show faster convergence and improved test performance over standard dropout (s-dropout) on several datasets.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous risk-bound analysis for s-dropout, offering a solid theoretical foundation for its effectiveness in shallow learning. This is a novel contribution, as prior dropout methods lack such theoretical guarantees.
2. Practical Innovation: The proposed e-dropout is computationally efficient and aligns well with mini-batch-based training in deep learning. Its connection to batch normalization is insightful, offering an alternative perspective on addressing internal covariate shift.
3. Empirical Results: The experiments demonstrate that both s-dropout and e-dropout achieve faster convergence and lower test errors compared to standard dropout, particularly on CIFAR-100 and logistic regression tasks.
4. Clarity of Motivation: The motivation for multinomial dropout, emphasizing data-dependent sampling probabilities, is well-justified and aligns with practical needs for faster convergence and better generalization.
Weaknesses:
1. Theoretical Gaps for Deep Learning: While the theoretical analysis for s-dropout is commendable, the paper lacks similar guarantees for e-dropout in deep learning. This omission weakens the overall theoretical contribution.
2. Experimental Limitations: The experiments fail to compare the proposed methods with other advanced dropout variants, such as adaptive dropout or variational dropout. This limits the ability to contextualize the performance gains.
3. Single Dropout Rate: The experiments use a fixed dropout rate (0.5) across all settings, which may not fully explore the robustness of the proposed methods.
4. Z-Normalization: The authors acknowledge that s-dropout reduces to standard dropout under Z-normalization but do not empirically evaluate this scenario, leaving a gap in understanding its practical implications.
5. E-Dropout vs. Batch Normalization: While e-dropout is described as a randomized alternative to batch normalization, its learning stability is reported to be slightly worse. This raises questions about its practical advantages.
6. Language and Presentation: The paper contains several language and formatting errors, which detract from its readability and professionalism.
Arguments for Acceptance:
- The paper introduces a novel data-dependent dropout method with theoretical guarantees for shallow learning, which is a significant contribution.
- The proposed e-dropout is computationally efficient and shows promise as an alternative to batch normalization in deep learning.
- The empirical results demonstrate clear performance improvements over standard dropout.
Arguments Against Acceptance:
- The lack of theoretical analysis for e-dropout limits its scientific rigor.
- The experimental setup is incomplete, as it does not compare with other advanced dropout methods or explore multiple dropout rates.
- The paper's presentation needs significant improvement to meet the standards of a top-tier conference.
Recommendation:
This paper offers an interesting and theoretically grounded contribution to dropout methods, particularly for shallow learning. However, the lack of theoretical guarantees for e-dropout, limited experimental comparisons, and presentation issues reduce its overall impact. I recommend acceptance with major revisions, focusing on extending the theoretical analysis for deep learning, broadening experimental comparisons, and improving the clarity and language of the manuscript.