This paper investigates the convergence behavior of the Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMMs), focusing on the population likelihood function in the infinite-sample setting. The authors provide two key results: (1) the existence of arbitrarily bad local maxima in the population likelihood function for mixtures of three or more well-separated spherical Gaussians, disproving a conjecture by Srebro (2007), and (2) the high probability of EM, even with random initialization, converging to suboptimal critical points. These findings highlight the limitations of EM and its first-order variant, emphasizing the necessity of careful initialization for effective parameter estimation.
The paper is technically sound and provides rigorous theoretical results supported by detailed proofs. The authors construct explicit examples of bad local maxima for mixtures of three components and extend this recursively for larger mixtures, demonstrating predictable failure probabilities for random initialization. They also show that gradient-based EM avoids strict saddle points but still converges to poor local maxima, reinforcing the role of bad local optima as the primary bottleneck. The recursive construction of failure cases and the probabilistic analysis of random initialization are particularly noteworthy contributions.
However, the paper has some limitations. While the results align with expected behavior for non-convex optimization problems, they lack surprising insights or novel techniques that significantly advance the state of the art. The failure of random initialization for EM is conceptually similar to known issues with k-means clustering, and the paper does not explore alternative initialization strategies or practical remedies in depth. Additionally, the discussion would benefit from a more explicit comparison with related algorithms, such as the convergence guarantees for k-means provided by Kumar and Kannan, to contextualize the findings within the broader literature.
The paper is clearly written and well-organized, with sufficient background provided for readers to follow the technical arguments. However, the inclusion of more empirical results or practical implications would enhance its significance. While the theoretical contributions are solid, their direct impact on practical applications of GMMs remains somewhat limited.
Pros:
1. Rigorous theoretical analysis of EM's limitations for GMMs.
2. Explicit construction of bad local maxima and recursive extension to larger mixtures.
3. Clear exposition of the role of initialization in algorithmic performance.
Cons:
1. Lack of surprising insights or novel techniques.
2. Limited discussion of practical remedies or alternative approaches.
3. Insufficient comparison with related algorithms like k-means.
In conclusion, the paper makes a valuable theoretical contribution by resolving an open question and characterizing the limitations of EM for GMMs. However, its impact could be strengthened by exploring practical implications, alternative methods, or broader comparisons. I recommend acceptance with minor revisions to address these points.