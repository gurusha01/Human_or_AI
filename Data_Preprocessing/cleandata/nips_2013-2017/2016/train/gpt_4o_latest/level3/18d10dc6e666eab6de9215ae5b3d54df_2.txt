This paper introduces a novel batch Bayesian optimization algorithm, the parallel knowledge gradient (q-KG), which computes Bayes-optimal batches for parallel evaluations. By leveraging a decision-theoretic analysis, the authors propose an elegant acquisition function that collectively evaluates batches of points, outperforming existing methods such as parallel EI and UCB-based approaches in both noise-free and noisy settings. The paper also addresses the computational challenges of optimizing q-KG by employing techniques like infinitesimal perturbation analysis (IPA) and discretization strategies, making the method computationally feasible.
Strengths:  
The paper makes a strong theoretical contribution by deriving the q-KG acquisition function and demonstrating its Bayes-optimality in the batch setting. The decision-theoretic foundation is rigorous, and the use of IPA for efficient gradient computation is a notable innovation. The writing is clear and well-organized, making the methodology accessible to readers. The proposed method shows promising results in synthetic benchmarks and practical hyperparameter optimization tasks, particularly in noisy settings, where it significantly outperforms existing methods. The availability of open-source code further enhances the reproducibility and practical impact of the work.
Weaknesses:  
Despite its theoretical rigor, the experimental evaluation has notable limitations. The benchmarks are restricted to only two datasets (MNIST and CIFAR10), one task (classification), and two algorithms (logistic regression and CNN). This narrow scope limits the generalizability of the results to other tasks, datasets, or machine learning models. Additionally, the use of test set error as the optimization criterion is suboptimal; metrics like AUC or log loss would have been more appropriate for classification tasks. The assumption of independent normally distributed errors, while common, is problematic in hyperparameter optimization due to potential structural biases in certain regions of the hyperparameter space. Finally, the paper would have benefited from benchmarking against a more diverse suite of tasks, such as those available in HPOlib, to strengthen its empirical validation.
Pro and Con Arguments for Acceptance:  
- Pro: The paper makes a significant theoretical contribution to batch Bayesian optimization, with a novel acquisition function and efficient computational techniques. The method demonstrates clear advantages in noisy settings and has practical implications for hyperparameter optimization.  
- Con: The empirical evaluation is limited in scope, reducing the generalizability of the findings. The reliance on test set error as the optimization metric and the assumption of independent errors are methodological weaknesses.
Recommendation:  
While the theoretical contributions are strong and the method shows promise, the limited empirical evaluation weakens the paper's overall impact. I recommend acceptance with revisions to address the benchmarking limitations and consider alternative evaluation metrics. This work is a valuable addition to the Bayesian optimization literature but would benefit from broader empirical validation.