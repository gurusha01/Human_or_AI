The paper addresses the important problem of solving convex-concave saddle-point problems, which are prevalent in machine learning, using stochastic variance-reduced methods (e.g., SVRG, SAGA) and proximal operators. The authors extend these methods to handle non-separable functions, a significant advancement over prior work that primarily focused on separable objectives. This extension is particularly relevant for problems with non-separable losses or regularizers, such as those arising in structured prediction or robust optimization. The paper also introduces an accelerated variant and a non-uniform sampling scheme, demonstrating linear convergence in the monotone operator setting and extending the applicability of the proposed methods to variational inequalities.
Strengths:
1. Theoretical Contributions: The authors provide rigorous convergence guarantees, including linear convergence rates, for their proposed methods. The use of monotone operator theory to generalize the analysis is a notable contribution, as it extends the applicability of the algorithms beyond saddle-point problems to variational inequalities.
2. Novelty: The extension to non-separable functions is a significant innovation, filling a gap in the literature where existing methods were limited to separable objectives. The introduction of non-uniform sampling and acceleration further enhances the efficiency of the algorithms.
3. Practical Relevance: The proposed methods are well-suited for large-scale machine learning problems, as demonstrated by their application to binary classification tasks with non-separable losses and regularizers.
Weaknesses:
1. Clarity and Accessibility: The connection between the proposed methods and monotone operators is not clearly articulated, making it challenging for readers unfamiliar with monotone operator theory to follow the analysis. Additionally, the paper could benefit from a more accessible presentation of Assumptions A-C and their implications.
2. Experimental Limitations: The experiments are limited to two datasets, which may not fully demonstrate the generality of the proposed methods. Furthermore, comparisons with other accelerated methods, such as the stochastic Chambolle-Pock algorithm, are missing, leaving the practical advantages of the proposed methods somewhat underexplored.
3. Minor Issues: There are inconsistencies in the use of the constant \(L\), which is alternately referred to as the condition number and the Lipschitz constant. Additionally, references for the Forward-Backward algorithm are missing, which detracts from the completeness of the related work section.
Arguments for Acceptance:
- The paper makes significant theoretical contributions by extending variance-reduced methods to non-separable saddle-point problems and demonstrating linear convergence.
- The proposed methods are novel and address a critical gap in the literature, with potential applications in various machine learning tasks.
Arguments Against Acceptance:
- The presentation of the paper is not sufficiently clear, particularly regarding the connection to monotone operators and the role of key assumptions.
- The experimental evaluation is limited in scope and lacks comparisons with state-of-the-art accelerated methods.
Recommendation: While the paper has strong theoretical contributions and novelty, the clarity and experimental limitations need to be addressed. I recommend acceptance conditional on revisions to improve the presentation and expand the experimental evaluation.