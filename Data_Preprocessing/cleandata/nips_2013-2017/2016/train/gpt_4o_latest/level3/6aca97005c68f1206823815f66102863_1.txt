The paper introduces a novel approach to likelihood-free inference by replacing standard Monte Carlo methods for Approximate Bayesian Computation (ABC) with Bayesian conditional density estimation. This method directly learns a parametric approximation to the posterior, addressing key limitations of ABC, such as inefficiency with small tolerance values and reliance on sample-based posterior representations. The paper builds on prior work in neural density estimation, stochastic variational inference (SVI), and recognition networks, and its scope is comparable to ABC with variational inference (Tran et al., 2015). 
Strengths:
1. Novelty and Contribution: The proposed method is innovative, particularly in its use of Bayesian neural density estimators (MDN-SVI) to improve robustness and efficiency. The extension of Mixture Density Networks (MDN) to SVI is a moderate but meaningful contribution, enabling better handling of overfitting and improving simulation efficiency.
2. Clarity and Presentation: The paper is well-written, technically clear, and accessible to experts. Proposition 1 is elegantly utilized to justify the approach and is central to both proposal prior selection and posterior approximation. The authors also provide sufficient experimental evidence to support their claims.
3. Experimental Validation: The experiments are comprehensive, covering simple (e.g., Gaussian mixture) and complex (e.g., Lotkaâ€“Volterra) datasets. Results are well-presented, showing that the proposed method achieves superior posterior approximations with fewer simulations compared to ABC methods. The disentanglement of proposal distribution effects is particularly insightful.
4. Efficiency Gains: The proposed method demonstrates significant simulation efficiency by iteratively refining the proposal prior and avoiding sample rejection, which is a notable improvement over traditional ABC methods.
Weaknesses:
1. Theoretical Depth: While the theoretical underpinning is sufficient for the scope of the paper, it is somewhat limited. A deeper theoretical analysis, such as convergence guarantees or error bounds, would strengthen the paper.
2. Comparison to Related Work: The paper does not adequately discuss related work on SVI with ABC, which could provide additional context for the contributions. For example, it would be helpful to clarify how this approach compares to other parametric methods like synthetic likelihood or regression adjustment.
3. Metric for Evaluation: The use of effective sample size as a metric is questioned. While it provides some insight, CPU time or wall-clock time may be more practical and relevant for evaluating simulation efficiency.
4. Generality: The experiments focus on relatively narrow domains. It would be beneficial to explore the method's applicability to broader or higher-dimensional problems to better demonstrate its generalizability.
Recommendation:
The paper makes a solid contribution to likelihood-free inference by introducing a novel and efficient approach that improves upon traditional ABC methods. Its strengths in clarity, experimental validation, and practical efficiency outweigh its weaknesses in theoretical depth and related work discussion. I recommend acceptance, with minor revisions to address the related work discussion and evaluation metrics.
Arguments for Acceptance:
- Novel and efficient approach to likelihood-free inference.
- Comprehensive experimental validation with clear improvements over ABC methods.
- Well-written and accessible to the target audience.
Arguments Against Acceptance:
- Limited theoretical analysis.
- Insufficient discussion of related work, particularly SVI with ABC.
- Evaluation metrics could be improved to reflect practical considerations.
In summary, this paper advances the state of the art in likelihood-free inference and is a valuable contribution to the field.