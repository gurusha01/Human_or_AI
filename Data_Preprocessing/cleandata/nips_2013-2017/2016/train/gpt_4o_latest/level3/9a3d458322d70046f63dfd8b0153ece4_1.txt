This paper addresses the critical problem of robust Markov Decision Processes (MDPs) with uncertain transition probabilities, focusing on the challenge of improving a baseline policy while ensuring safety guarantees. The authors propose a novel regret-based approach for policy comparison, which is less conservative than traditional worst-case performance methods. This approach allows for safe policy improvement by leveraging accurate dynamics in certain states while falling back to the baseline policy in others. The paper's contributions are both theoretical and practical, with significant implications for sequential decision-making under uncertainty.
The theoretical contributions are robust and well-supported. The authors prove that the regret-based optimization problem is NP-hard (Theorem 6) and provide a performance bound for the regret-based solution (Theorem 5). Additionally, they demonstrate that the optimal policy in this framework is randomized (Theorem 3), which is a notable departure from deterministic policy spaces in conventional robust MDPs. The heuristic approximation proposed, which assumes zero uncertainty for baseline policy actions, is a practical and computationally efficient solution. This approximation is particularly reasonable when data is predominantly collected from the baseline policy with minimal exploration, as is often the case in real-world applications.
The empirical results are compelling, showing that the proposed approximation outperforms standard robust MDP and certainty-equivalence approaches across several domains. The experiments highlight the method's ability to adapt to varying levels of model uncertainty, effectively interleaving the baseline and improved policies. However, the paper's scope is limited to finite MDPs, and it lacks guarantees on the quality of the heuristic approximation. Additionally, while the optimization formulation is novel, the technical tools and error bounds employed are standard, which slightly diminishes the originality of the technical contributions.
The paper is well-written and clearly organized, making it accessible to readers. However, certain terms and norms could be better clarified, and a more thorough comparison with prior work on regret-based objectives would strengthen the paper. For instance, the relationship between this approach and existing robust MDP methods could be more explicitly discussed.
Strengths:
1. Novel regret-based optimization formulation that is less conservative than traditional approaches.
2. Strong theoretical contributions, including proofs of NP-hardness and performance bounds.
3. Practical heuristic approximation with demonstrated empirical effectiveness.
4. Well-written and clearly structured presentation.
Weaknesses:
1. Limited to finite MDPs and lacks guarantees on the heuristic approximation's quality.
2. Technical tools and error bounds are standard, reducing the novelty of the contributions.
3. Insufficient comparison with prior work on regret-based objectives.
4. Certain terms and norms require better clarification.
Recommendation:
This paper makes a significant contribution to the field of robust MDPs and safe policy improvement. Despite some limitations, the novel regret-based approach and its practical implications warrant acceptance. The authors should address the noted weaknesses, particularly by providing more detailed comparisons with prior work and clarifying key terms.