This paper presents a novel extension to the Word Mover's Distance (WMD) metric, termed Supervised Word Mover's Distance (S-WMD), which incorporates supervision to learn domain-specific weights and transformations. The authors address a critical limitation of the original WMD, which operates in an entirely unsupervised manner, by introducing a supervised learning framework that optimizes the metric for specific classification tasks. The proposed method combines a linear transformation of word embeddings with a re-weighting of word importance, enabling the metric to better reflect task-specific semantic similarities. The paper leverages efficient approximations of the optimal transport problem, such as the Sinkhorn distance, to ensure computational feasibility, and demonstrates state-of-the-art performance across eight text classification datasets.
Strengths:
1. Quality: The methodology is sound, well-motivated, and rigorously explained. The authors provide detailed derivations of the loss function, gradients, and optimization process, ensuring reproducibility. The use of entropy-regularized transport for computational efficiency is particularly noteworthy.
2. Clarity: The paper is well-organized and clearly written, with a logical progression from problem formulation to experimental results. The inclusion of visualizations (e.g., t-SNE embeddings and word importance visualizations) enhances understanding.
3. Originality: The work is highly original, building on the foundational WMD metric and introducing a novel supervised extension. The combination of metric learning with optimal transport in the context of document classification is innovative and impactful.
4. Significance: The results are impressive, with S-WMD outperforming 26 competitive baselines across diverse datasets. The method's ability to adapt to domain-specific tasks makes it broadly applicable and likely to influence future research in text classification and metric learning.
Weaknesses:
1. Initialization Sensitivity: The method relies on a good initialization of the transformation matrix (A) and word importance weights (w). While the authors propose using NCA with Word Centroid Distance (WCD) for initialization, this approach may not generalize well to all datasets, as evidenced by suboptimal performance on certain datasets like CLASSIC.
2. Dataset-Specific Observations: The method performs less competitively on datasets with long documents (e.g., BBCSPORT and OHSUMED), suggesting that the squared Euclidean distance used in the word embedding space may not always be optimal. This limitation could be addressed by exploring alternative distance metrics.
3. Computational Complexity: While the authors employ efficient approximations, the method still involves quadratic scaling with respect to the number of unique words in a document. This could pose challenges for very large datasets or corpora with extensive vocabularies.
Recommendation:
I strongly endorse this paper for presentation at NeurIPS. Its contributions are both theoretically robust and practically significant, advancing the state of the art in document distance metrics. The combination of supervised metric learning with optimal transport is likely to inspire further research and applications in text classification and beyond.
Arguments for Acceptance:
- Novel and impactful extension to a widely-used metric (WMD).
- Rigorous methodology with clear theoretical and experimental validation.
- Demonstrated superiority over a large number of competitive baselines.
- Broad applicability to diverse text classification tasks.
Arguments Against Acceptance:
- Sensitivity to initialization and potential suboptimality on certain datasets.
- Computational demands may limit scalability for very large datasets.
Overall, the strengths of this paper far outweigh its weaknesses, making it a valuable contribution to the field.