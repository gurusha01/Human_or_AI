The paper introduces Stochastic Gradient Richardson-Romberg MCMC (SG-RR-MCMC), a novel algorithm designed to reduce the bias of Stochastic Gradient MCMC (SG-MCMC) methods while maintaining reasonable variance. The authors leverage Richardson-Romberg (RR) extrapolation, a numerical acceleration technique, to achieve this goal. By running two SG-MCMC chains in parallel with different step sizes, SG-RR-MCMC extrapolates the results to achieve higher accuracy. The method is demonstrated on Stochastic Gradient Langevin Dynamics (SGLD), resulting in the proposed Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD). Theoretical analysis confirms that SGRRLD achieves asymptotic consistency, satisfies a central limit theorem, and improves convergence rates compared to traditional SG-MCMC methods. Empirical results on synthetic and real-world datasets, including a large-scale matrix factorization task, validate the algorithm's effectiveness.
Strengths:
1. Novelty and Simplicity: The combination of SG-MCMC with RR extrapolation is a creative and straightforward approach. The method is generic and can be extended to other SG-MCMC algorithms beyond SGLD, such as SGHMC.
2. Theoretical Contributions: The paper provides rigorous theoretical analysis, including asymptotic consistency, central limit theorems, and non-asymptotic bounds for bias and mean squared error (MSE). The improved convergence rates (O(K^-4/5) for MSE) are significant.
3. Empirical Validation: The experiments, both on synthetic data and large-scale matrix factorization tasks, convincingly demonstrate the benefits of SGRRLD over SGLD. The results show reduced bias and MSE, as well as faster convergence in terms of computation time.
4. Scalability: The algorithm's parallel nature makes it well-suited for distributed architectures, which is a practical advantage in large-scale applications.
Weaknesses:
1. Experimental Comparisons: While the paper claims that SGRRLD achieves the accuracy of higher-order integrators, no direct experimental comparison with such methods (e.g., SGHMC-s) is provided. This omission leaves a gap in understanding how SGRRLD performs relative to state-of-the-art high-order integrators.
2. Inconsistencies in Presentation: There are discrepancies between Algorithm 1 and Equation (5) in the supplementary material, as well as unclear step size descriptions. These issues could confuse readers and hinder reproducibility.
3. Limited Exploration of Variance Control: Although the authors discuss variance control using correlated Brownian motions, the practical impact of this choice is not thoroughly explored in experiments.
4. Scope of Applications: The experiments focus primarily on SGLD and matrix factorization. Broader applications, such as other SG-MCMC variants or diverse Bayesian inference tasks, would strengthen the paper's significance.
Pro vs. Con for Acceptance:
- Pro: The paper presents a novel and theoretically sound method with clear empirical benefits. It addresses a critical limitation of SG-MCMC (bias) and is scalable to large datasets.
- Con: The lack of direct comparisons with high-order integrators and inconsistencies in the presentation detract from its completeness.
Recommendation: Accept with Minor Revisions. The paper makes a meaningful contribution to the field of scalable Bayesian inference. Addressing the noted inconsistencies and including experimental comparisons with high-order integrators would further strengthen its impact.