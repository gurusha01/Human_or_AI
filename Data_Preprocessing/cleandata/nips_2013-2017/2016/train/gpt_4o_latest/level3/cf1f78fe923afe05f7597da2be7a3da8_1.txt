The paper presents a novel algorithm, Riemannian Relaxation (RR), for near-isometric embedding of manifold data. Unlike traditional non-linear dimensionality reduction methods, RR directly optimizes a loss function based on the push-forward Riemannian metric, enabling embeddings that preserve geometric properties such as lengths, angles, and volumes. The algorithm is iterative, utilizing gradient descent to minimize distortion, and accommodates embedding dimensions \( s \) greater than the intrinsic dimension \( d \), addressing a limitation in many existing methods. The authors also propose an extension, PCS-RR, for handling large or noisy datasets through subsampling.
Strengths:
1. Theoretical Contribution: The paper introduces a principled loss function grounded in Riemannian geometry, offering a rigorous approach to measuring and minimizing distortion. This represents a significant departure from heuristic-based losses in existing methods.
2. Practical Utility: By allowing \( s \geq d \), the algorithm overcomes a key limitation of many geometry-preserving methods, making it applicable to a broader range of datasets.
3. Empirical Validation: Experiments demonstrate the superiority of RR over established methods like Isomap, Laplacian Eigenmaps, and HLLE in terms of distortion minimization and geometric recovery. The application to SDSS galaxy spectra further highlights its potential for real-world data.
4. Scalability: The PCS-RR extension for large datasets is a valuable addition, addressing practical challenges in manifold learning.
Weaknesses and Suggestions:
1. Impact of Initialization: While the iterative nature of RR is a strength, the paper lacks a detailed discussion on how the choice of initial embedding affects convergence and final performance. Including such an analysis would enhance reproducibility and practical usability.
2. Algorithm Speed: The computational complexity of RR is briefly mentioned, but a quantitative comparison with standard eigendecomposition methods (e.g., Isomap or Laplacian Eigenmaps) is missing. This would clarify its scalability and efficiency.
3. Related Work: The paper omits a discussion of the NuMax algorithm, which also addresses distortion minimization. Including this would provide a more comprehensive context for the contributions.
4. Terminology: The term "relaxation" in the title is not explicitly clarified. A brief explanation of its relevance to the algorithm would improve clarity.
5. Typos and Errors: Minor typographical issues and a potential error in Eq. 4 should be corrected.
Arguments for Acceptance:
- The paper makes a strong theoretical and practical contribution to manifold learning, addressing limitations of existing methods.
- The empirical results convincingly demonstrate the algorithm's advantages in distortion minimization and scalability.
- The extension to large datasets (PCS-RR) is timely and relevant for real-world applications.
Arguments Against Acceptance:
- The lack of a detailed discussion on initialization and computational efficiency limits the practical insights provided.
- Omissions in related work and minor clarity issues detract from the overall presentation.
Recommendation:
I recommend acceptance, provided the authors address the suggested clarifications and corrections. The paper is a valuable contribution to the field, advancing the state of the art in manifold learning.