This paper presents a significant extension of stochastic variance reduction methods, specifically SVRG and SAGA, to strongly convex-concave saddle-point problems. The authors provide a rigorous theoretical framework demonstrating linear convergence for these algorithms, supported by empirical evidence. The work addresses a critical gap in optimization for machine learning, where saddle-point problems with non-separable losses and regularizers often arise, such as in robust optimization, structured prediction, and variational inequalities in game theory.
Strengths:
1. Theoretical Contributions: The paper provides the first convergence analysis for variance reduction methods applied to saddle-point problems, leveraging monotone operator theory. This is a notable advancement, as the complexity of saddle-point problems often exceeds that of convex minimization.
2. Algorithmic Innovation: The proposed gradient operator splits (element-wise for SVRG and factored for SAGA) and the use of non-uniform sampling are well-motivated and shown to outperform uniform sampling both theoretically and empirically.
3. Generality: The extended algorithms are applicable to a broader class of problems, including variational inequalities, making the work highly relevant to fields like game theory and robust optimization.
4. Empirical Validation: Experimental results on real-world datasets demonstrate the effectiveness of the proposed methods, particularly for non-separable loss and regularizers. The accelerated SVRG algorithm shows strong performance gains for ill-conditioned problems.
5. Clarity of Analysis: The paper provides detailed proofs and discussions, particularly in its use of monotone operators, which are critical for understanding the convergence guarantees.
Weaknesses:
1. Clarity in Adaptive Updates: The adaptive update of the iterate point for acceleration in Section 5 is not well-explained, leaving some ambiguity about its practical implementation.
2. Convergence Proof for SAGA: While the authors suggest that the acceleration framework for SVRG could apply to SAGA, this is not explicitly proven. This omission leaves a gap in the theoretical completeness of the work.
3. Notation Confusion: The use of the same symbol \( K(.) \) for the smooth function and the design matrix is potentially confusing. Distinct symbols should be used to improve clarity.
Arguments for Acceptance:
- The paper addresses a challenging and important problem, extending variance reduction methods to a broader class of optimization problems.
- The theoretical contributions are novel and significant, advancing the state of the art in optimization for machine learning.
- The empirical results are robust and validate the theoretical claims.
Arguments Against Acceptance:
- The unclear explanation of the adaptive acceleration update may hinder reproducibility.
- The lack of a formal proof for SAGA's acceleration limits the completeness of the theoretical framework.
Suggestions for Improvement:
- Provide a clearer explanation of the adaptive update mechanism in Section 5, perhaps with pseudocode or additional examples.
- Include a formal proof or discussion of whether the acceleration framework for SVRG applies to SAGA.
- Use distinct symbols for the smooth function and the design matrix to avoid confusion.
Conclusion:
This paper is a high-quality contribution to the field of optimization and machine learning. Its theoretical advancements, coupled with strong empirical validation, make it a valuable addition to the literature. While minor issues of clarity and completeness exist, they do not detract significantly from the overall impact of the work. I recommend acceptance, with minor revisions to address the noted weaknesses.