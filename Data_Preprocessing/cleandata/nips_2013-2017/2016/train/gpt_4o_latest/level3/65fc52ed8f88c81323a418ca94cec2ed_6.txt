The paper introduces an innovative unsupervised algorithm for training deep neural networks using exemplar-based methods for similarity learning. The approach alternates between partitioning the dataset into compact cliques and optimizing the network with a softmax loss on pseudo clique labels. This iterative process addresses key limitations of traditional exemplar-based methods, such as the imbalance between positive and negative samples and the lack of transitivity in similarity relationships. By leveraging "offline" batch/clique partitioning, the method ensures that training batches are composed of mutually consistent cliques, effectively mitigating the issues of label inconsistency and biased gradients in stochastic gradient descent (SGD). The algorithm demonstrates competitive performance in posture analysis and pose estimation tasks, rivaling fully supervised approaches in some cases.
Strengths:
1. Novelty and Originality: The paper presents a novel unsupervised approach to similarity learning by framing it as a sequence of clique categorization tasks. This is a significant departure from traditional exemplar-based methods and supervised similarity learning, which often require extensive labeled data.
2. Technical Soundness: The proposed optimization framework for clique partitioning and batch selection is well-motivated and rigorously formulated. The alternating process of training and similarity imputation is shown to converge effectively within a few iterations.
3. Experimental Validation: The method achieves state-of-the-art results on posture analysis (Olympic Sports dataset), pose estimation (Leeds Sports dataset), and object classification (PASCAL VOC 2007). The results highlight its ability to generalize across datasets and tasks.
4. Practical Implications: By eliminating the need for labeled data, the approach is highly relevant for real-world applications where annotation is costly or infeasible.
Weaknesses:
1. Efficiency Concerns: The alternating process of clique partitioning and CNN optimization introduces potential inefficiencies. The computational cost of solving the optimization problem for batch selection and the iterative nature of the method could limit scalability to very large datasets.
2. Hyperparameter Sensitivity: The method relies on several hyperparameters (e.g., batch size, clique size, regularization terms), which may require extensive tuning. The paper does not provide sufficient discussion on the sensitivity of the results to these parameters.
3. Clarity and Reproducibility: While the paper is technically sound, certain aspects of the methodology, such as the equations for batch selection and the imbalanced ratios in the optimization problem, could benefit from clearer explanations. Additionally, performance trends over iterations/epochs are not discussed in detail, which would help in understanding the convergence behavior.
4. Limited Discussion of Related Work: The paper could better situate its contributions within the broader context of unsupervised deep learning and exemplar-based methods by providing a more comprehensive comparison to recent advances.
Recommendation:
The paper makes a significant contribution to unsupervised similarity learning and addresses a challenging problem with a novel and effective approach. However, the concerns about efficiency, hyperparameter tuning, and clarity should be addressed to strengthen the paper. I recommend acceptance with minor revisions.
Arguments for Acceptance:
- Novel and impactful approach to unsupervised similarity learning.
- Strong experimental results demonstrating competitive performance.
- Addresses key limitations of traditional exemplar-based methods.
Arguments Against Acceptance:
- Potential inefficiencies in the alternating optimization process.
- Lack of clarity in some methodological details and sensitivity to hyperparameters. 
Overall, the paper is a valuable contribution to the field and aligns well with the scope of the conference.