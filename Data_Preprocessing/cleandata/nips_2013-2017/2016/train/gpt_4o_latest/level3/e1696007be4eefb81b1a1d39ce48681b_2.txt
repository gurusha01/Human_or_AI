Review
The paper introduces a Scaled Ordinary Least Squares (SLS) estimator for efficiently approximating generalized linear model (GLM) parameters in the large-scale regime where \( n \gg p \). The authors leverage the approximate proportionality between GLM coefficients and OLS coefficients, a relationship previously established for Gaussian designs, and extend it to non-Gaussian random designs. The proposed algorithm achieves computational efficiency by first estimating OLS coefficients and then refining them using a proportionality constant, which is estimated via a root-finding method. Theoretical guarantees and empirical results demonstrate that the SLS estimator achieves similar accuracy to the maximum likelihood estimator (MLE) while being computationally cheaper.
Strengths:
1. Novelty in Extending to Non-Gaussian Designs: While the proportionality relationship between GLM and OLS coefficients is not new for Gaussian designs, the extension to non-Gaussian settings is a significant contribution. The use of zero-bias transformations to generalize this relationship is elegant and theoretically grounded.
2. Computational Efficiency: The SLS algorithm is computationally efficient, with a per-iteration cost of \( O(n) \), making it well-suited for large-scale datasets. The authors provide a detailed comparison with existing optimization methods (e.g., Newton-Raphson, BFGS, LBFGS) and demonstrate that SLS achieves similar accuracy with significantly lower computational cost.
3. Theoretical Rigor: The paper provides strong theoretical guarantees, including bounds on the approximation error and convergence rates. The results are well-supported by mathematical derivations and align with empirical observations.
4. Practical Relevance: The algorithm is evaluated on both synthetic and real datasets, including logistic and Poisson regression tasks. The results consistently show that SLS outperforms traditional MLE algorithms in terms of computational efficiency while maintaining accuracy.
Weaknesses:
1. Theorem 1's Tightness: The inequality in Theorem 1 is not tight and does not align well with Proposition 1 for Gaussian covariates. This discrepancy requires clarification, as it may affect the theoretical guarantees for the Gaussian case.
2. Experimental Clarity: The choice of the subset size (\( |S| \)) for subsampling in experiments (e.g., Figure 1 vs. Figure 2/Table 1) is unclear. This should be elaborated to ensure reproducibility and to clarify how the computational gains scale with \( |S| \).
3. Visualization Suggestion: While Figure 2 provides a comparison of computational efficiency, adding a time vs. accuracy plot (as suggested by the reviewer) in Figure 1 would provide a more comprehensive view of the trade-offs between computational cost and accuracy.
4. Minor Issues: There are minor typographical errors (e.g., "Hayley" should be corrected to "Halley") and a missing definition of \( \lambda_{\text{min}} \) in the main text. These should be addressed for clarity.
Arguments for Acceptance:
- The paper makes a significant theoretical and practical contribution by extending a well-known relationship to non-Gaussian designs.
- The proposed algorithm is computationally efficient and demonstrated to be effective on large-scale datasets.
- The theoretical analysis is rigorous and provides valuable insights into the behavior of the SLS estimator.
Arguments Against Acceptance:
- The discrepancy in Theorem 1 and the lack of clarity in experimental details (e.g., choice of \( |S| \)) may limit the reproducibility and confidence in the results.
- The Gaussian case ideas are not novel, which slightly diminishes the originality of the work.
Recommendation:
Overall, the paper is a strong contribution to the field of scalable optimization for GLMs. Addressing the noted weaknesses, particularly the clarity of Theorem 1 and experimental details, would further strengthen the work. I recommend acceptance with minor revisions.