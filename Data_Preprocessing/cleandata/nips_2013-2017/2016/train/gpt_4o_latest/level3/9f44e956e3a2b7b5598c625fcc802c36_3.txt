Review of the Paper
This paper introduces the concept of "intermediate timescales" in neural networks, leveraging fast weights to store temporary memories that decay over time. The authors propose a novel mechanism where fast weights act as a form of attention to the recent past, weighted by scalar products with the current state. This approach extends the idea of intermediate timescales from reservoir computing to standard machine learning tasks, demonstrating its utility across associative retrieval, visual attention, facial expression recognition, and reinforcement learning tasks. The paper also provides a biologically plausible perspective, connecting the proposed mechanism to short-term synaptic plasticity observed in neuroscience.
Strengths:
1. Novelty and Originality: The paper presents a novel extension of fast weights to standard machine learning tasks, providing a unique mechanism for temporary memory storage. The proposed approach is distinct from existing attention mechanisms and contributes to both machine learning and computational neuroscience.
2. Clarity and Presentation: The paper is well-organized and clearly written, with motivations and theoretical underpinnings explained in detail. The inclusion of layer normalization to stabilize fast weights is a thoughtful addition.
3. Experimental Validation: The authors validate their approach through diverse tasks, including associative retrieval, MNIST classification, facial expression recognition, and reinforcement learning. The results consistently show improvements over baseline models, especially in scenarios with limited network capacity.
4. Biological Plausibility: The connection to neuroscience, particularly the parallels with short-term synaptic plasticity, adds depth to the work and broadens its interdisciplinary appeal.
Weaknesses:
1. Normalization Procedure: While the normalization of fast weights is crucial for stability, the biophysical plausibility of the inner loop and layer normalization remains unclear. This raises questions about the alignment of the proposed mechanism with real-world neural dynamics.
2. Mini-Batch Size Impact: The authors note the impact of mini-batch size on memory but fail to explore or discuss this aspect in detail. This omission leaves a gap in understanding the practical implications of the approach.
3. Scalability Concerns: The advantage of fast weights appears limited to small networks, as highlighted in the experiments. This limitation suggests potential challenges in scaling the approach to larger, more complex architectures.
4. Presentation Issues: Table 3 should report percent correct instead of error, which would align better with standard reporting practices. Additionally, a missing reference at Line 277 detracts from the completeness of the paper.
Arguments for Acceptance:
- The paper introduces a novel and biologically inspired mechanism that improves memory in RNNs.
- It demonstrates strong performance across diverse tasks, suggesting broad applicability.
- The work bridges machine learning and neuroscience, offering insights for both fields.
Arguments Against Acceptance:
- Questions about the biophysical plausibility of the normalization procedure and inner loop remain unresolved.
- The scalability of the approach to larger networks is not adequately addressed.
- Some aspects, such as the impact of mini-batch size, are underexplored.
Conclusion:
This paper makes a valuable contribution to the field by proposing a novel mechanism for intermediate timescales in neural networks. While there are some concerns regarding scalability and biophysical plausibility, the strengths of the work outweigh its weaknesses. I recommend acceptance, contingent on addressing the noted issues in a revised version.