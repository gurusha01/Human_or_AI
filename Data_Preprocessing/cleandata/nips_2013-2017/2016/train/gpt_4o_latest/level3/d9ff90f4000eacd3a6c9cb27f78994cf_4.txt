This paper investigates the representation power of unitary matrices in unitary recurrent neural networks (uRNNs) and proposes a method to optimize full-capacity unitary matrices. The authors address the limitations of prior parameterizations of unitary matrices, such as diagonal, Householder reflection, permutation, and DFT-based compositions, which fail to represent all unitary matrices for hidden state dimensions exceeding 7. By defining the Givens capacity of a unitary matrix and leveraging gradient descent on the Stiefel manifold, the paper introduces a method to optimize full-capacity unitary matrices, overcoming these representational constraints.
The paper is technically sound and well-supported by both theoretical analysis and empirical results. The authors provide a rigorous theoretical framework, using Sard's theorem to demonstrate the limitations of restricted-capacity parameterizations. They further validate their claims through experiments on synthetic and natural data tasks, including system identification, the copy memory problem, and speech prediction using the TIMIT corpus. Notably, the full-capacity uRNN consistently outperforms restricted-capacity uRNNs and LSTMs, particularly in tasks requiring long-term memory, such as the copy memory problem with delays up to T=2000. The results are significant, as they demonstrate the practical utility of full-capacity unitary matrices in addressing vanishing/exploding gradients and improving performance on challenging sequential tasks.
The paper is clearly written and well-organized, with sufficient detail to reproduce the results. However, a minor typographical issue (a missing word on line 101) should be addressed. The authors provide adequate references to related work, situating their contributions within the broader context of recurrent neural network research.
In terms of originality, the work is novel in its theoretical analysis of unitary matrix capacity and its application of Stiefel manifold optimization to train full-capacity uRNNs. The approach represents a meaningful advancement over prior restricted-capacity methods and offers a more elegant solution to gradient instability compared to LSTMs.
While the results are compelling, the paper raises questions about scalability. For instance, it remains unclear whether further increases in hidden dimensions or improved LSTM optimization (e.g., with the Adam optimizer) could narrow the performance gap. Additionally, the trade-off between hidden state size and parameterization complexity warrants further exploration.
Strengths:
1. Rigorous theoretical analysis of unitary matrix capacity.
2. Novel optimization method leveraging Stiefel manifold gradient descent.
3. Strong empirical results demonstrating superiority over restricted-capacity uRNNs and LSTMs.
4. Clear writing and reproducibility of experiments.
Weaknesses:
1. Limited exploration of scalability and trade-offs in hidden state size.
2. Minor typographical error (line 101).
Recommendation: Accept. The paper makes a significant contribution to the field of recurrent neural networks, advancing the state of the art in addressing long-term memory and gradient instability issues.