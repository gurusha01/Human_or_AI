The paper introduces a Tree-structured Reinforcement Learning (Tree-RL) approach for object proposal generation, which sequentially searches for object candidates using a tree-traversing scheme. The proposed method leverages deep Q-learning to train an agent that performs scaling and local translation actions at each tree node, enabling it to navigate near-optimal search paths. By incorporating global interdependencies and historical search paths, Tree-RL aims to improve object localization efficiency and accuracy. The authors validate their approach through experiments on PASCAL VOC datasets, demonstrating competitive recall rates and detection mAP compared to state-of-the-art methods like RPN and Faster R-CNN.
Strengths:
1. Novel Tree-Structured Search: The tree-based search mechanism is a creative extension of reinforcement learning for object localization, allowing the agent to explore multiple near-optimal paths. This approach effectively balances exploration and refinement, which is crucial for multi-object localization.
2. Reduced Candidate Windows: The method achieves comparable recall rates to RPN while using significantly fewer proposals, which is a notable improvement in efficiency.
3. Integration with Detection Frameworks: The combination of Tree-RL with Fast R-CNN demonstrates superior detection mAP compared to Faster R-CNN, showcasing the practical utility of the proposed method.
4. Experimental Comparisons: The paper provides a thorough evaluation, comparing Tree-RL to single-path RL, other object proposal methods, and detection frameworks. This comprehensive analysis strengthens the validity of the claims.
Weaknesses:
1. Lack of Runtime Analysis: Despite claims of computational efficiency, the paper does not provide runtime measurements or complexity analysis. This omission makes it difficult to assess the practical scalability of Tree-RL compared to other methods.
2. Limited Qualitative Results: The qualitative examples are not diverse enough to illustrate the method's performance across different scenarios, such as images with varying object densities or occlusions.
3. Tree Structure Justification: The rationale behind specific tree structure choices, such as the predefined scaling and translation actions, is not well-explained. Alternative strategies or randomization could have been explored for comparison.
4. Marginal Novelty: While the tree-structured search is novel, the technical contributions in terms of actions, states, and rewards are similar to prior work ([18]). The distinction of running the process sequentially rather than multiple times requires stronger experimental validation.
5. Significance of Results: Although the method achieves competitive performance, the improvements over state-of-the-art methods are incremental, particularly given the reliance on a simpler backbone (VGG-16) compared to ResNet-101.
Recommendation:
The paper presents an interesting adaptation of reinforcement learning for object proposal generation, with a novel tree-structured search strategy. However, the lack of runtime analysis, limited qualitative results, and marginal novelty compared to prior work reduce its impact. If the authors address these concerns in a revision, the paper could make a stronger contribution to the field. For now, I recommend borderline acceptance, contingent on additional clarity and experimental validation. 
Pros:
- Novel tree-structured search mechanism.
- Reduced candidate windows with competitive recall rates.
- Comprehensive experimental comparisons.
Cons:
- Missing runtime and complexity analysis.
- Limited novelty compared to prior work.
- Insufficient qualitative results and tree structure justification.