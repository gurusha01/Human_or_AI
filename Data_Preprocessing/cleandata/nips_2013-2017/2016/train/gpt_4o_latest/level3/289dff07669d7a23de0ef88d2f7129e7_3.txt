This paper presents an extension of the Cholesky CMA-ES algorithm, incorporating both rank-1 and rank-\(\mu\) updates for the triangular Cholesky factor, aiming to improve the runtime and memory efficiency of the covariance matrix adaptation evolution strategy (CMA-ES). The authors propose a numerically stable quadratic-time update scheme that reduces the computational complexity of covariance matrix updates to \(O(\mu d^2)\), while maintaining the algorithm's performance in terms of function evaluations. The theoretical justification for the proposed modifications, particularly the analysis of the error introduced by replacing the matrix square root with the Cholesky factor, is a notable contribution. Empirical results demonstrate significant runtime improvements over existing CMA-ES variants, especially in high-dimensional settings.
Strengths:
1. Theoretical Rigor: The paper provides a solid theoretical foundation for the proposed modifications. Lemma 1, supported by prior work (e.g., Beyer, 2014), effectively establishes confidence in the convergence properties of the modified algorithm.
2. Practical Impact: The proposed Cholesky-CMA-ES achieves substantial speed-ups in wall-clock runtime without increasing the number of function evaluations. This makes it highly relevant for large-scale optimization problems, particularly in machine learning applications such as neural network training and hyperparameter tuning.
3. Empirical Validation: The experiments are comprehensive, covering a range of benchmark functions and dimensionalities. The results convincingly demonstrate the scalability and efficiency of the proposed method compared to state-of-the-art CMA-ES variants.
Weaknesses:
1. Limited Novelty: While the paper extends the work of Krause et al. (2015) to include rank-\(\mu\) updates, the novelty of the contribution is somewhat incremental. The core idea of using Cholesky updates for covariance adaptation has already been explored in prior work.
2. Minor Presentation Issues: There are a few typographical and notational errors, such as the incorrect notation in Equation (1) and footnote 2 (\(\sigma\) should be \(\sigma_t\)) and the extraneous "by" in line 94. These should be corrected for clarity.
3. Empirical Scope: While the experiments are thorough, the paper could benefit from additional comparisons with more recent low-rank or diagonal approximation methods (e.g., Loshchilov, 2015) to contextualize the trade-offs in performance.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a practical bottleneck in CMA-ES, providing a theoretically sound and empirically validated solution that significantly improves runtime efficiency.
- Con: The contribution is incremental, with limited novelty beyond extending existing methods. Additionally, the paper could better position its work in the context of recent advancements in scalable CMA-ES variants.
Recommendation: While the paper lacks groundbreaking novelty, its strong theoretical foundation, practical relevance, and significant empirical improvements make it a valuable contribution to the field. I recommend acceptance, provided the authors address the minor presentation issues and better contextualize their work in relation to recent advancements.