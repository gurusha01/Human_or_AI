The paper presents a novel batch Bayesian optimization algorithm, the parallel knowledge gradient (q-KG), aimed at efficiently finding the Bayes-optimal batch of points for evaluation in black-box optimization problems. The authors propose a gradient estimation method using Monte Carlo simulation and infinitesimal perturbation analysis (IPA) to optimize the q-KG acquisition function, which is computationally intensive in the parallel setting. The algorithm demonstrates superior performance in both noise-free and noisy settings, particularly in hyperparameter tuning tasks for machine learning models.
Strengths:
1. Novelty and Decision-Theoretic Foundation: The paper builds on the knowledge gradient (KG) acquisition function and extends it to the parallel setting, offering a theoretically grounded approach to batch Bayesian optimization. The decision-theoretic perspective is a strong conceptual contribution.
2. Efficient Gradient Estimation: The use of IPA for gradient estimation is a notable technical innovation, enabling the efficient optimization of the q-KG acquisition function.
3. Comprehensive Experiments: The authors provide extensive empirical evaluations on synthetic functions and real-world tasks, demonstrating that q-KG outperforms or is competitive with state-of-the-art methods, particularly in noisy settings.
4. Practical Relevance: The application to hyperparameter tuning for logistic regression and convolutional neural networks highlights the practical utility of the proposed method.
5. Open-Source Code: The availability of the implementation enhances reproducibility and encourages adoption by the community.
Weaknesses:
1. Lack of Clarity in Novelty: While the paper claims to improve upon prior batch KG methods, the novelty compared to existing approaches (e.g., [1], [2]) is not sufficiently clarified. A more explicit discussion of how q-KG advances efficiency or effectiveness relative to these methods is needed.
2. Limited Comparisons with Prior Work: The experimental results lack direct mathematical or computational comparisons with prior batch KG methods, which would strengthen the validation of the contribution.
3. Section 5.2 Requires Elaboration: The description of the gradient estimation method in Section 5.2 is central to the paper but is not sufficiently detailed. Additional explanations or examples would improve clarity.
4. Potential Bias in Comparisons: The comparison with Expected Improvement (EI) in noisy settings may be unfair, as the authors do not use a noise-aware EI variant. This should be addressed to ensure a fair evaluation.
5. Inconsistent Terminology: The redefinition of the abbreviation "IPA" in line 162 is confusing and should be avoided for consistency.
Recommendation:
The paper is a strong candidate for acceptance due to its theoretical contributions, practical relevance, and empirical performance. However, the authors should address the weaknesses to improve the clarity and rigor of the work. Specifically, they should:
1. Clearly articulate the novelty of q-KG compared to prior batch KG methods.
2. Include mathematical or computational comparisons with prior batch KG approaches.
3. Provide additional details in Section 5.2 to enhance understanding.
4. Reassess the fairness of comparisons with EI by incorporating a noise-aware variant.
5. Ensure consistent terminology throughout the paper.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded method for batch Bayesian optimization.
- It demonstrates strong empirical performance across diverse tasks, particularly in noisy settings.
- The method has practical relevance for hyperparameter tuning in machine learning.
Arguments Against Acceptance:
- The novelty compared to prior batch KG methods is not sufficiently clarified.
- The lack of direct comparisons with prior batch KG approaches weakens the validation of the contribution.
- Certain sections, particularly Section 5.2, require further elaboration for clarity.
Overall, the paper makes a valuable contribution to the field of Bayesian optimization, and with revisions, it could be a strong addition to the conference.