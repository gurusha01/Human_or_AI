The paper proposes Conditional Generative Moment-Matching Networks (CGMMN), an extension of Generative Moment-Matching Networks (GMMN) that incorporates conditional variables using a novel Conditional Maximum Mean Discrepancy (CMMD) loss function. This approach leverages conditional kernel mean embeddings to model conditional distributions, enabling applications in predictive modeling, contextual generation, and Bayesian model distillation. The authors demonstrate competitive results on tasks such as classification (MNIST, SVHN), digit/face generation, and Bayesian inference distillation, showcasing the model's versatility.
Strengths:
1. Novelty and Originality: The paper introduces a significant extension of GMMN by incorporating conditional variables, filling a gap in the literature on moment-matching networks. The use of CMMD as a criterion for training conditional generative models is innovative and well-motivated.
2. Technical Soundness: The theoretical foundation, including the derivation of CMMD and its connections to MMD, is well-articulated. The proposed method is grounded in kernel mean embedding theory, and the authors provide a clear explanation of the generative process.
3. Empirical Validation: The experiments demonstrate competitive performance in predictive modeling (e.g., MNIST and SVHN classification) and generative tasks (e.g., digit and face generation). The application to Bayesian model distillation is particularly interesting, showcasing the method's potential for practical use in compressing complex models.
4. Clarity of Presentation: The paper is generally well-written and organized, with a logical flow from theoretical foundations to experimental results. The inclusion of visualizations for generative tasks adds clarity.
Weaknesses:
1. Scalability: While the authors address concerns about scalability in low-dimensional spaces, the empirical results are limited to relatively simple datasets (e.g., MNIST, Yale Faces). The method's performance on more challenging datasets like CIFAR-10 or LSUN remains unexplored, which limits its broader applicability.
2. Theoretical Gaps: Theorem 3, which is central to the paper's theoretical contributions, lacks a formal proof or reference, undermining the rigor of the theoretical claims. This should be addressed for completeness.
3. Notation and Clarity Issues: The mapping of Equation (2) to the weighted CMMD form is unclear, and the distinction between matrix multiplication and the Hadamard product needs clarification. Additionally, the dual use of the symbol "C" for conditional embedding and another parameter is confusing and should be revised.
4. Limited Dataset Diversity: The Bayesian model distillation experiments are conducted on a single dataset (Boston Housing), raising questions about the method's generalizability to other domains or larger-scale problems.
5. Supervised Learning Applications: While the method performs well in supervised tasks, its utility in such settings is debatable, as discriminative models often outperform generative ones in purely predictive tasks.
Recommendation:
The paper presents a novel and technically sound contribution to conditional generative modeling. However, the lack of results on more challenging datasets and the absence of a proof for Theorem 3 are significant limitations. If these issues are addressed, the paper would represent a meaningful advancement in the field. I recommend acceptance with minor revisions, contingent on clarifying the theoretical gaps and improving the notation. Expanding the empirical evaluation to include more diverse datasets would further strengthen the paper. 
Pro/Con Summary:
Pros:
- Novel extension of GMMN with conditional variables.
- Competitive results on predictive and generative tasks.
- Interesting application to Bayesian model distillation.
Cons:
- Limited scalability and dataset diversity.
- Missing proof for a key theorem.
- Notation and clarity issues in the mathematical formulation.