The paper introduces a novel deep learning architecture, Residual Transfer Network (RTN), for unsupervised domain adaptation. It addresses the challenge of adapting classifiers and features simultaneously when labeled data is available only in the source domain and not in the target domain. The proposed approach relaxes the shared-classifier assumption of prior methods by modeling the target classifier as a residual function of the source classifier. It integrates ideas from deep adaptation networks (MK-MMD), residual networks, and entropy minimization into a unified framework. The method demonstrates state-of-the-art performance on standard benchmarks, such as Office-31 and Office-Caltech, and provides an end-to-end trainable architecture.
Strengths:
1. Novelty and Incremental Contribution: The paper introduces a novel combination of residual learning and entropy minimization for classifier adaptation, which is an incremental but meaningful improvement over prior deep adaptation networks (e.g., DAN). The use of residual blocks to bridge source and target classifiers is an innovative application of residual learning in the domain adaptation context.
2. Practical Effectiveness: The experimental results convincingly demonstrate that RTN outperforms existing methods on challenging domain adaptation benchmarks. The ablation study highlights the contributions of individual components (e.g., tensor MMD, entropy minimization, and residual blocks) to the overall performance.
3. Clarity and Organization: The paper is well-written and provides sufficient details for an expert reader to understand and reproduce the results. The inclusion of visualizations (e.g., t-SNE embeddings and layer responses) and sensitivity analyses further supports the claims.
4. Scalability: The architecture is compatible with standard deep learning frameworks and can be trained efficiently using back-propagation, making it accessible for practical applications.
Weaknesses:
1. Lack of Theoretical Guarantees: While the paper claims that residual learning ensures small residuals between source and target classifiers, this is based on empirical observations rather than theoretical guarantees. A formal analysis would strengthen the contribution.
2. Unclear Advantages of MK-MMD: The paper does not clearly articulate how the use of MK-MMD in RTN differs from or improves upon its use in prior work (e.g., DAN). A direct experimental comparison with DAN on this aspect is missing.
3. Benchmark Discrepancies: The results for baseline methods like TCA and GFK are significantly better than those reported in prior work, likely due to the use of deep features instead of raw features. This should be explicitly clarified to avoid confusion.
4. Limited Scope of Evaluation: While the benchmarks used are standard, the evaluation is limited to image datasets. Testing on other modalities (e.g., text or speech) would demonstrate broader applicability.
Recommendation:
The paper makes a solid contribution to the field of unsupervised domain adaptation by introducing a novel and effective architecture. However, the incremental nature of the contribution and the lack of theoretical guarantees slightly limit its impact. The paper is well-suited for acceptance, provided the authors address the issues of experimental clarity and provide a more detailed comparison with prior work. 
Arguments for Acceptance:
- Innovative combination of residual learning and entropy minimization.
- Strong empirical results on standard benchmarks.
- Clear and reproducible methodology.
Arguments Against Acceptance:
- Incremental improvement over existing methods.
- Lack of theoretical analysis for key claims.
- Missing clarity on certain experimental aspects.
Overall, the paper is a valuable addition to the domain adaptation literature and meets the quality standards of the conference.