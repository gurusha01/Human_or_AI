Review of "Conditional Generative Moment-Matching Networks (CGMMN)"
This paper introduces Conditional Generative Moment-Matching Networks (CGMMN), an extension of Generative Moment-Matching Networks (GMMN) to conditional distributions. The authors propose a novel Conditional Maximum Mean Discrepancy (CMMD) criterion to train CGMMN, enabling the model to learn conditional distributions. The paper demonstrates the applicability of CGMMN across tasks such as predictive modeling, contextual generation, and Bayesian dark knowledge distillation, showing competitive results against state-of-the-art methods.
Strengths:
1. Novelty and Extension of GMMN: The paper addresses a significant gap in the literature by extending GMMN to conditional distributions. This is a non-trivial contribution, as it requires matching distributions of mini-batches with varying conditionals, which is challenging.
2. Broad Applicability: The authors evaluate CGMMN on diverse tasks, including predictive modeling, generative tasks, and Bayesian model distillation. The results, particularly on MNIST and SVHN datasets, demonstrate that CGMMN is competitive with state-of-the-art methods.
3. Simplified Training Process: The use of back-propagation for optimizing the CMMD objective ensures that the training process remains computationally feasible, which is a practical advantage over more complex models like GANs.
4. Potential for Generalization: The framework is flexible and compatible with various network architectures, such as MLPs and CNNs, making it adaptable to different use cases.
Weaknesses:
1. Lack of Intuition and Explanation: The paper fails to provide an intuitive explanation for the CMMD formula (Eq. (2)) and its derivation. This omission makes it difficult for readers to grasp the underlying principles and practical steps from the formula to the algorithm.
2. Clarity Issues: The paper starts with formal mathematics without offering an intuitive overview, which can alienate readers unfamiliar with the topic. While the non-conditional version of GMMN has an intuitive explanation, the conditional case lacks a similar treatment.
3. Incomplete Details in the Main Text: Key computational details are relegated to the supplementary material, which is difficult to follow. For example, the practical steps for estimating the CMMD objective and the computational efficiency of the algorithm are not adequately discussed in the main paper.
4. Unanswered Questions: The paper leaves critical questions unanswered, such as how to derive Eq. (2) intuitively and how the CMMD criterion compares to other conditional modeling approaches (e.g., conditional GANs or conditional VAEs) in terms of computational efficiency and scalability.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses an important problem and provides a novel extension of GMMN to conditional distributions. The results are competitive, and the framework is flexible and broadly applicable.
- Con: The lack of clarity and intuition in presenting the methodology, coupled with incomplete computational details, limits the accessibility and reproducibility of the work.
Recommendation:
While the paper makes a meaningful contribution to the field, the lack of clarity and intuition in explaining the methodology is a significant drawback. If the authors address these issues in a revised version, the paper would be a strong candidate for acceptance. As it stands, I recommend weak acceptance, contingent on the authors improving the clarity and providing more intuitive explanations for their approach.