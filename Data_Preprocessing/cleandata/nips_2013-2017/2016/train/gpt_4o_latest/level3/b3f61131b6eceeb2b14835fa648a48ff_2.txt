Review
This paper introduces the concept of "Low Approximate Regret" (LAR), a novel property that generalizes classical no-regret dynamics in decentralized learning settings, particularly in smooth games. The authors demonstrate that LAR enables fast convergence to approximate optimality under various feedback settings, including realized feedback and bandit feedback, and extends to dynamic population games. The paper builds on prior work, notably Syrgkanis et al. (2015) and Lykouris et al. (2016), while offering significant improvements in convergence rates and broadening the class of applicable algorithms.
The strengths of the paper are notable. First, the LAR property is shown to be ubiquitous, encompassing a wide range of well-known algorithms, including Hedge and its variants. The theoretical results improve upon prior work by relaxing the feedback requirements (e.g., from expected to realized feedback) and achieving faster convergence rates (e.g., O(n/T) compared to O(nÂ²/T) in prior work). The analysis of bandit feedback is particularly compelling, as the proposed algorithm achieves improved regret bounds with efficient dependence on the number of actions. Furthermore, the extension to dynamic population games is a valuable contribution, as it demonstrates robustness in settings with player turnover. The authors effectively contextualize their results within the literature, highlighting both theoretical advancements and practical implications.
However, the paper has several weaknesses. The dense content and frequent topic jumps make it challenging to follow, especially since detailed derivations are relegated to the appendix. This compromises clarity and accessibility for readers. Additionally, while the theoretical results are robust, the lack of experimental validation is a significant drawback. Unlike Syrgkanis et al. (2015), which included simulations to demonstrate practical applicability, this paper does not provide empirical evidence to support its claims. The conclusion is abrupt and lacks a cohesive summary of the implications of the work. Inconsistent terminology, such as "realized feedback" versus "full information feedback," adds to the confusion. Finally, the distinction between "expected Low Approximate Regret" and LAR in the bandit setting needs explicit clarification to avoid ambiguity.
Pro Acceptance Arguments:
1. Introduces a novel and generalizable property (LAR) with significant theoretical contributions.
2. Improves convergence rates and feedback requirements compared to prior work.
3. Extends results to dynamic population games and bandit feedback settings.
4. Demonstrates the ubiquity of LAR across a wide range of algorithms.
Con Acceptance Arguments:
1. Dense and poorly organized content reduces clarity and accessibility.
2. Lack of experimental validation limits practical applicability.
3. Inconsistent terminology and insufficient explanation of key distinctions (e.g., bandit feedback) detract from the paper's rigor.
4. Abrupt conclusion fails to adequately summarize contributions and implications.
Overall, this paper makes a strong theoretical contribution to the field of decentralized learning in games. However, addressing the clarity issues and providing experimental validation would significantly enhance its impact. I recommend acceptance with the condition that the authors improve the organization and address the identified weaknesses.