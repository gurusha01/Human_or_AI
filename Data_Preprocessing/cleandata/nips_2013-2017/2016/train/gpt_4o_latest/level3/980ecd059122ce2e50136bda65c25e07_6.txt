The paper introduces a novel method for generating adversarial examples by encoding neural networks as constraints and solving a linear programming (LP) problem. This approach is innovative, leveraging the piecewise linearity of neural networks to approximate robustness metrics efficiently. The authors also formalize robustness through two new statistics: adversarial frequency and adversarial severity, which provide complementary insights into the robustness of neural networks. These contributions are significant, as they address the limitations of existing robustness evaluation methods that often overfit to specific adversarial example generation algorithms.
The paper demonstrates technical soundness in its formulation of robustness as an optimization problem and its approximation using convex restrictions. The iterative constraint-solving optimization for LPs is a practical addition, achieving substantial speed-ups. Experimental results on MNIST and CIFAR-10 datasets validate the proposed method, showing that it produces more accurate robustness estimates compared to baseline methods like L-BFGS-B. The authors also highlight the overfitting of neural networks to adversarial examples generated by specific algorithms, a critical observation that underscores the need for unbiased robustness measures.
However, there are several weaknesses in the paper. First, the clarity of the manuscript is a concern. Symbols and equations are often unclear, such as the dimensionality of \(x^*\) and the notation in line 147. Additionally, there is a potential error in the conditions between lines 212-123, which appear to be switched. These issues hinder the readability and reproducibility of the work. The claim that the proposed method is less biased due to finding more adversarial examples requires stronger empirical evidence. While the authors argue that their method reduces bias, it still relies on their algorithm to measure robustness, raising questions about objectivity. Furthermore, the deterministic removal of disjunction constraints based on the seed input may introduce bias; a stochastic approach could be explored to mitigate this.
In terms of originality, the work builds on prior research, particularly the robustness definitions in [21], but extends it meaningfully by introducing tractable approximations and new robustness metrics. The related work is well-referenced, situating the contributions within the broader context of adversarial robustness research.
The significance of the work lies in its potential to advance robustness evaluation methodologies. However, the improvements in robustness for larger networks like NiN are modest, suggesting that the approach may not scale effectively to more complex architectures.
Pros for acceptance:
- Novel method for encoding robustness as an LP problem.
- Introduction of two new robustness metrics.
- Experimental validation showing improved robustness estimates.
Cons for acceptance:
- Clarity issues with symbols, equations, and conditions.
- Insufficient evidence for claims of reduced bias.
- Limited scalability to larger neural networks.
Overall, the paper makes a meaningful contribution to the field of adversarial robustness but requires revisions to improve clarity and address methodological concerns.