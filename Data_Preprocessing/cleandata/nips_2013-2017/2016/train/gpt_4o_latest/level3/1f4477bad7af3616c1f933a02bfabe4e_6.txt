The paper proposes a novel nonlinear spectral method for training a specific class of feedforward neural networks, achieving global optimality under certain assumptions. The authors provide theoretical guarantees, including linear convergence rates, and demonstrate the method's applicability to one- and two-hidden-layer networks. While the theoretical contributions are significant, the practical utility and broader applicability of the proposed method remain unclear.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous mathematical framework, including convergence guarantees and a novel fixed-point theory approach. This is a valuable addition to the literature on optimization in neural networks.
2. Novelty: The nonlinear spectral method is innovative, and the authors claim it is the first practically feasible algorithm to achieve global optimality for non-trivial neural network models.
3. Clarity in Introduction: The introduction is well-written, providing a clear motivation for the work and situating it within the broader context of neural network optimization.
4. Proof of Concept: Experimental results on UCI datasets demonstrate that the method can achieve competitive performance, albeit on low-dimensional datasets.
Weaknesses:
1. Practical Utility: The proposed method imposes strong constraints, such as nonnegative weights and specific activation functions, which limit its expressiveness. The authors acknowledge this but do not provide a thorough discussion of the implications for real-world applications.
2. Comparison with Related Work: The paper does not adequately contrast its method with other neural network training approaches beyond stochastic gradient descent (SGD) and support vector machines (SVMs). Recent advances in neural network optimization, such as Adam or second-order methods, are notably absent from the discussion.
3. Experimental Design: The experiments are limited to small, low-dimensional datasets, and the results do not convincingly demonstrate the scalability or superiority of the method for more complex, high-dimensional problems. Additionally, the comparison to SVMs and ReLU networks is insufficiently detailed.
4. Resource Requirements: The computational resource requirements for the nonlinear spectral method and the compared algorithms are not reported, making it difficult to assess the method's efficiency.
5. Clarity Issues: Section 2 is less fluent than the introduction, and Theorem 1 is difficult to parse without an informal explanation. The loss function and the role of the $\Phi$ function and epsilon term are not well-motivated. Typos and unclear notations (e.g., "polyomial," "R_{++}") detract from the paper's readability.
6. Experimental Limitations: The experiments do not explore the limitations of the model, such as its behavior on datasets where the nonnegativity assumption on weights is violated.
Recommendation:
While the paper makes a strong theoretical contribution, its practical significance and experimental validation are limited. To strengthen the paper:
- Provide a more detailed discussion of the method's expressiveness and limitations.
- Compare the method with a broader range of neural network training algorithms.
- Include experiments on more complex datasets and report computational resource requirements.
- Improve the clarity of Section 2 and provide informal explanations for key theorems.
Arguments for Acceptance:
- Theoretical rigor and novelty.
- First demonstration of global optimality for a non-trivial neural network model.
Arguments Against Acceptance:
- Limited practical utility and scalability.
- Insufficient experimental validation and comparison with related work.
- Clarity and presentation issues.
Overall, the paper is a promising theoretical contribution but requires significant improvements in practical validation and clarity to meet the standards of a top-tier conference like NeurIPS.