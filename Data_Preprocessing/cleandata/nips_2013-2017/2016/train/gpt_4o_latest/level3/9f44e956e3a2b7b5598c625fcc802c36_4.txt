The paper introduces the concept of "fast weights" in recurrent neural networks (RNNs) as a mechanism to store temporary memories of recent inputs and states. This approach is biologically inspired, drawing from evidence of short-term synaptic plasticity in the brain, and offers a neurally plausible way to implement attention to the recent past. The authors demonstrate that fast weights can significantly enhance the performance of RNNs on tasks such as associative retrieval, MNIST classification, facial expression recognition, and reinforcement learning. The proposed method is elegant, leveraging an outer product rule for fast weight updates and layer normalization to stabilize training. Experimental results show that RNNs with fast weights outperform standard RNNs and LSTM variants, particularly when memory capacity is constrained.
Strengths:  
The paper makes a compelling case for fast weights as a novel and effective memory mechanism in RNNs. The method is technically sound, with clear mathematical formulations and well-designed experiments that validate its utility across diverse tasks. The biological inspiration adds interdisciplinary significance, potentially bridging machine learning, computational neuroscience, and cognitive science. The paper is well-written and organized, making the ideas accessible to readers. The experimental results are robust, demonstrating both improved accuracy and faster convergence compared to baseline models. The use of fast weights as a "cache" in visual attention models is particularly innovative, offering a plausible alternative to biologically implausible architectures like Neural Turing Machines.
Weaknesses:  
While the paper is strong overall, there are areas for improvement. First, the authors do not provide an analysis or visualization of the low-level outcomes of fast weight operations, which could help readers better understand their impact. Second, the statement on the benefits of layer normalization lacks empirical evidence and should be supported with ablation studies. Third, some related work on fast weights is mentioned but not adequately cited or compared, which could strengthen the originality claim. The Conclusion section could be reframed as a Discussion to provide a more nuanced interpretation of the contributions and limitations. Additionally, the figures need improved printability, with thicker lines and larger fonts. Finally, the absence of the promised appendix with implementation details is a notable omission.
Arguments for Acceptance:  
- The paper introduces a novel and biologically inspired mechanism that significantly improves RNN performance.  
- The method is elegant, technically sound, and applicable to a wide range of tasks.  
- The interdisciplinary relevance enhances its significance beyond machine learning.  
Arguments Against Acceptance:  
- Missing implementation details in the appendix hinder reproducibility.  
- Insufficient discussion of prior work and lack of citations for some claims.  
- Figures and formatting issues detract from the paper's clarity.  
Recommendation: Accept with minor revisions. Addressing the weaknesses, especially the missing appendix and additional citations, will further strengthen the paper's contribution.