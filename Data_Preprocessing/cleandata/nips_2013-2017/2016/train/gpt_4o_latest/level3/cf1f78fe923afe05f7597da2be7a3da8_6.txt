The paper presents a novel algorithm, Riemannian Relaxation (RR), for embedding data points in a low-dimensional space while minimizing geometric distortion. The authors propose a loss function based on the push-forward Riemannian metric, which directly measures deviations from isometry. The algorithm iteratively refines the embedding using gradient descent, and its flexibility allows embeddings in dimensions \( s \geq d \), where \( d \) is the intrinsic dimension of the data. This approach builds incrementally on prior work, leveraging discrete estimators of the Laplace-Beltrami operator and the push-forward metric.
Strengths:
1. Clarity and Writing: The paper is well-structured and clearly written, with research questions and contributions explicitly stated. The theoretical foundations are explained in detail, and the algorithm is presented with sufficient rigor for reproducibility.
2. Novelty: The proposed method departs from traditional manifold learning algorithms by directly optimizing the Riemannian metric, rather than relying on heuristic loss functions like pairwise distances or local reconstruction errors. This is a significant conceptual advancement.
3. Experimental Validation: The algorithm demonstrates promising results on synthetic and real-world datasets. Comparisons with state-of-the-art methods (e.g., Isomap, Laplacian Eigenmaps, MVU) highlight its ability to achieve lower distortion and better geometric preservation.
4. Flexibility: Unlike many existing methods, RR supports embeddings in \( s > d \) dimensions, addressing a key limitation of algorithms that require \( s = d \). This flexibility is particularly useful for data with complex geometries.
5. Scalability: The extension to large or noisy datasets via subsampling (PCS-RR) is a practical contribution, making the method applicable to real-world problems like the SDSS galaxy dataset.
Weaknesses:
1. Parameter Selection: The algorithm requires both the intrinsic dimension \( d \) and the embedding dimension \( s \) as inputs. While the authors suggest heuristics for choosing these parameters, the lack of automated or principled methods for their determination is a limitation.
2. Non-Convexity: The loss function is non-convex, which may lead to convergence to local minima. While the authors acknowledge this and propose alternate minimization strategies, the practical implications of this limitation are not fully explored.
3. Computational Complexity: Although the algorithm is competitive with existing methods, the iterative nature of RR and the need to compute the Riemannian metric at each step may pose challenges for very large datasets.
4. Limited Discussion of Related Work: While the paper references key prior works, it could benefit from a more comprehensive discussion of how RR compares to recent advances in manifold learning, particularly in terms of computational trade-offs and scalability.
Recommendation:
The paper makes a solid scientific contribution by introducing a novel approach to manifold learning that advances the state of the art in geometric preservation. However, the lack of clarity on parameter selection and the computational demands of the algorithm warrant further investigation. I recommend acceptance, provided the authors address these concerns in the final version. The work is likely to be of interest to both theoretical and applied researchers in the field.