The paper addresses the challenging problem of non-convex loss minimization in supervised PageRank models, proposing two novel optimization methods: a gradient-based approach and a random gradient-free method. Both methods leverage the concept of an inexact oracle, providing theoretical convergence rate guaranteesâ€”a significant improvement over the state-of-the-art gradient-based approach, which lacks such guarantees. The authors also explore the trade-off between computational complexity and accuracy in their two-level optimization framework and validate their methods on a real-world ranking task using a user web browsing graph dataset.
Strengths:  
The paper makes several noteworthy contributions. First, it advances the state of the art in supervised PageRank optimization by addressing the limitations of the existing gradient-based method, particularly its lack of convergence guarantees. The theoretical analysis provided for both the gradient-based and gradient-free methods is rigorous, with detailed proofs and complexity bounds. The authors also adapt the random gradient-free framework to handle constrained optimization problems with inexact oracles, which is a novel contribution. Experimentally, the proposed methods outperform the baseline in terms of ranking quality and loss minimization, demonstrating their practical utility. The inclusion of both theoretical and experimental results strengthens the paper's impact and relevance.
Weaknesses:  
While the theoretical contributions are solid, the paper could benefit from a more comprehensive discussion of time efficiency. Although the authors provide complexity bounds, they do not explicitly report the runtime of their methods compared to the baseline, which is crucial for practical applications. Additionally, while the experiments are well-designed, they are limited to a single dataset. Testing the methods on additional datasets or graph structures would enhance the generalizability of the results. Finally, the paper is dense and highly technical, which may hinder accessibility for readers less familiar with optimization theory.
Pro and Con Arguments for Acceptance:  
Pro:  
1. The paper provides a significant theoretical advancement with convergence guarantees for both proposed methods.  
2. Experimental results demonstrate superior performance over the baseline on a real-world task.  
3. The methods address a critical problem in graph-based ranking models, which is highly relevant to the NIPS community.  
Con:  
1. The lack of explicit runtime comparisons limits the practical evaluation of the methods.  
2. The experiments are confined to a single dataset, raising questions about generalizability.  
3. The dense presentation could be more reader-friendly with additional clarifications or visual aids.  
Conclusion:  
Overall, the paper makes a strong scientific contribution by addressing a well-defined problem with novel methods that are both theoretically sound and empirically validated. However, the authors should address the time efficiency of their methods and consider expanding their experimental evaluation. With these improvements, the paper would be an excellent addition to the conference. I recommend acceptance with minor revisions.