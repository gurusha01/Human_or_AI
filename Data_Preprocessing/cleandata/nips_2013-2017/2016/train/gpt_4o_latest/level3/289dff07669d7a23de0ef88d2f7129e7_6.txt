The paper introduces Cholesky-CMA-ES, a modified variant of the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), aimed at reducing computational cost while preserving algorithmic behavior. The authors propose a quadratic-time covariance matrix update using triangular Cholesky factors, which reduces memory requirements and computational overhead. They claim this approach achieves "optimal covariance update and storage complexity" and demonstrate its empirical performance on benchmark functions, showing significant runtime improvements over standard CMA-ES implementations.
While the paper addresses an important challenge in CMA-ES—reducing computational cost—it has several critical weaknesses. First, the naming of Cholesky-CMA-ES is misleading, as a similar approach was introduced by Suttorp et al. (2009), which the authors fail to acknowledge. This omission is problematic, as it obscures the relationship between their work and prior contributions. Furthermore, the proposed method is an incremental modification of Krause et al. (2015), with the primary algorithmic change being the addition of a "for" loop for rankOneUpdate applied to all individuals. This limited novelty makes the work feel overly incremental.
The empirical evaluation is another area of concern. While the authors compare their method to a standard CMA-ES implementation, they fail to include comparisons with Suttorp's and Krause's Cholesky-CMA-ES variants, both algorithmically and empirically. This omission weakens the paper's claims of superiority and leaves readers unable to assess the true contribution of the proposed method. Additionally, the claim of "optimal covariance update and storage complexity" is misleading, as the approach is not optimal for separable functions or cases where the covariance matrix is unnecessary. The authors also incorrectly assert that Suttorp's method is numerically unstable, without providing sufficient evidence.
Despite these issues, the proposed approach does offer practical benefits. It is twice as efficient as Suttorp's method and may be the best option for reducing time and memory costs when using a full covariance matrix. However, the approach does not address large-scale optimization challenges, as its complexity remains quadratic. The paper is well-written and organized, but the clarity of contributions could be improved by resolving the naming issue, conducting comparisons with related methods, and better articulating the algorithmic novelty.
Pros:  
- Significant runtime improvements demonstrated empirically.  
- Practical benefits for scenarios requiring full covariance matrix adaptation.  
Cons:  
- Incremental contribution with limited novelty.  
- Lack of comparison with Suttorp's and Krause's variants.  
- Misleading claims about optimality and numerical stability.  
Recommendation: The paper is not ready for acceptance in its current form. The authors should address the naming issue, conduct thorough comparisons with related methods, and clarify their contributions. While the work has potential, it currently falls short of advancing the state of the art in a meaningful way.