The paper presents a novel Tree-structured Reinforcement Learning (Tree-RL) approach for object proposal generation, leveraging Q-learning to sequentially search for objects while considering global interdependencies. The authors make two key contributions: (1) At test time, the model takes two actions—one from each of two action classes—enabling better multimodal object prediction, and (2) a carefully designed reward signal that improves object localization while encouraging the discovery of new objects without imposing an arbitrary order. The method achieves competitive recall rates and improves Fast R-CNN performance by approximately 2 points over RPN, demonstrating its practical utility.
Strengths:  
The paper addresses a significant limitation in existing object proposal methods by incorporating global interdependencies and avoiding the imposition of arbitrary box order during training. The proposed Tree-RL approach is novel, particularly in its use of a tree-structured search strategy that allows for multiple near-optimal search paths, improving coverage of objects across scales. The reward design is well-thought-out, balancing object refinement and exploration of new objects, which is critical for multi-object localization. The experimental results on PASCAL VOC datasets are compelling, showing that Tree-RL achieves comparable recall rates to RPN with significantly fewer proposals and higher localization accuracy. The improvement in detection mAP when combined with Fast R-CNN further underscores its practical relevance.
Weaknesses:  
While the paper is technically sound, some aspects require clarification. For instance, the effectiveness of ranking proposals based on tree depth is not adequately justified, and it remains unclear whether the training process explicitly encourages quick object discovery. Additionally, the evaluation could be more comprehensive. Comparisons to stronger proposal methods like MCG and DeepMask are missing, which would provide a better understanding of Tree-RL's relative performance. Furthermore, the experiments are limited to the PASCAL VOC dataset. Evaluating on a more challenging dataset like COCO, which includes smaller and more diverse objects, would strengthen the claims. Finally, the detection AP at higher overlap thresholds (e.g., IoU > 0.75) should be analyzed to assess the gains under stricter metrics.
Pro and Con Arguments for Acceptance:  
Pros:  
- Novel and well-motivated approach to object proposal generation.  
- Strong experimental results demonstrating improved recall and detection performance.  
- Effective reward design that balances exploration and refinement.  
- Avoids arbitrary box order, aligning better with human perception.  
Cons:  
- Limited evaluation scope; lacks comparisons to stronger baselines like MCG and DeepMask.  
- No experiments on COCO to assess performance on small or unseen objects.  
- Insufficient analysis of detection AP under stricter metrics.  
- Some methodological details, such as ranking by tree depth, need further clarification.  
Conclusion:  
This paper makes a meaningful contribution to object proposal generation by introducing a novel Tree-RL approach. While the results are promising, the evaluation could be more thorough, and some methodological aspects require clarification. I recommend acceptance, provided the authors address the suggested improvements and expand the evaluation in the final version.