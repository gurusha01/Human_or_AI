This paper provides a rigorous and insightful analysis of the conditions under which the critical points of a regularized non-convex optimization problem align with the global optima of the matrix completion problem. The authors prove that the commonly used non-convex objective for positive semidefinite matrix completion has no spurious local minima, ensuring that all local minima are also global. This result is significant as it explains why practical algorithms like gradient descent, even with random initialization, converge to optimal solutions in polynomial time. The paper also extends these results to noisy settings, demonstrating robustness under realistic conditions.
The clarity of the exposition is exceptional, making this the most impressive paper I have reviewed this year. The authors systematically build their arguments, starting with the rank-1 case and generalizing to higher ranks. Their use of "simple" and generalizable proofs is particularly commendable, as it not only makes the results accessible but also highlights the potential for applying similar techniques to other statistical problems involving partial or noisy observations. The paper is well-organized, and the theoretical claims are supported by rigorous mathematical proofs.
The work is original and advances the state of the art in non-convex optimization for matrix completion. While previous studies required careful initialization schemes, this paper provides the first comprehensive explanation for the success of arbitrary initialization in practice. The authors effectively situate their contributions within the broader context of related work, including recent advances in non-convex optimization and matrix sensing. However, the paper should cite the closely related work (arXiv:1605.08101) to acknowledge parallel developments.
One intriguing direction for future work is extending these results to ensembles with uniformly distributed entries. This could broaden the applicability of the findings and provide deeper insights into the geometry of other non-convex problems. Additionally, exploring alternative distance measures beyond the Frobenius norm could be valuable, as suggested in the conclusion.
Minor issues include the capitalization of "F" in Frobenius, correcting "our attention" (line 229), and removing the "s" in "ensures" (line 251). Addressing these would improve the manuscript's polish.
Pros:  
- Rigorous and novel theoretical contributions.  
- Clear and well-structured presentation.  
- Strong connections to recent work in the field.  
Cons:  
- Missing citation of related work (arXiv:1605.08101).  
- Minor typographical errors.  
In summary, this paper makes a significant and well-supported contribution to the field of non-convex optimization and matrix completion. I strongly recommend its acceptance.