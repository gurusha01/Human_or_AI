The paper introduces Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD), a novel algorithm that applies Richardson-Romberg (RR) extrapolation to Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods, specifically targeting bias reduction in Bayesian inference for large datasets. The authors address a key limitation of SG-MCMC methods—bias in posterior estimates—by running two SG-MCMC chains in parallel with different step sizes and correlated Gaussian noise. The proposed method reduces the bias from \(O(\gamma)\) to \(O(\gamma^2)\), where \(\gamma\) is the step size, and achieves a higher rate of convergence for the mean squared error (MSE). This approach is theoretically supported and empirically validated on synthetic data and a large-scale matrix factorization task.
Strengths:
1. Novelty and Generality: While RR extrapolation is a well-known technique in numerical analysis, its application to SG-MCMC is novel. The method is broadly applicable to various SG-MCMC algorithms, including SGLD and SGHMC, making it impactful for the community.
2. Bias and MSE Reduction: The theoretical analysis convincingly demonstrates that SGRRLD achieves significant improvements in both bias and MSE compared to standard SG-MCMC methods. The empirical results corroborate these findings, showing consistent performance gains across experiments.
3. Simplicity and Practicality: The method is simple to implement and does not require substantial modifications to existing SG-MCMC frameworks. Its compatibility with parallel and distributed architectures enhances its practical utility.
4. Clarity: The paper is well-written and provides detailed theoretical and empirical analyses. The inclusion of supplementary material strengthens the rigor of the work.
Weaknesses:
1. Computational Efficiency: The use of two chains introduces computational inefficiency, as one chain remains idle during certain steps. Exploring the use of three chains or other optimizations could improve efficiency.
2. Noise Correlation: The effect of noise correlation between mini-batches is not fully explored, leaving some ambiguity in practical implementations.
3. Experimentation: The matrix factorization experiment could be extended to run until convergence and compare against stochastic gradient descent (SGD) to highlight the Bayesian advantages of SGRRLD.
4. Accessibility: While the paper is clear, adding pseudo-code or an algorithm box early on would make the method more accessible to practitioners.
Questions and Suggestions:
1. Can the method be extended to use more than two step sizes for further bias reduction?
2. How does SGRRLD perform with other approximate MCMC schemes or in distributed settings like SGLD with distributed mini-batches?
3. Could the authors explore adaptive step sizes to further optimize performance?
Recommendation:
The paper makes a significant contribution to the field of Bayesian inference and SG-MCMC methods, addressing a critical limitation with a theoretically sound and empirically validated solution. Despite minor concerns about computational efficiency and experimental scope, the strengths far outweigh the weaknesses. I recommend acceptance, with minor revisions to address the questions and suggestions raised above.