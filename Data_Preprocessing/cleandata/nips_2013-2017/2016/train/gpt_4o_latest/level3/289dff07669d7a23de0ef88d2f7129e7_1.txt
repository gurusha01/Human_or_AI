The paper presents a novel variant of the covariance matrix adaptation evolution strategy (CMA-ES), a widely recognized derivative-free optimization algorithm, by introducing a Cholesky factor-based covariance matrix update. This modification addresses the computational and memory bottlenecks of the standard CMA-ES, particularly in high-dimensional optimization problems. The authors propose replacing the traditional covariance matrix square root with a triangular Cholesky factor, enabling a quadratic-time update with minimal memory requirements. This approach also eliminates the need for explicit eigenvalue computation, which is a significant advantage in terms of computational efficiency.
The proposed Cholesky-CMA-ES algorithm is theoretically justified, with the authors demonstrating that the approximation does not compromise the algorithm's performance in terms of objective function evaluations. Empirical evaluations on standard benchmark functions confirm that the new variant maintains the optimization performance of the original CMA-ES while significantly reducing wall-clock time, especially in higher dimensions. For instance, the Cholesky-CMA-ES achieves up to a 20x speed-up compared to the reference CMA-ES implementation for dimensions as low as 64. The reduction in computational overhead makes the algorithm particularly suitable for medium- to large-scale optimization problems, such as neural network training in direct policy search.
Strengths:  
1. Significant Contribution: The paper addresses a critical limitation of CMA-ES, offering a practical solution that scales well with dimensionality while preserving performance.  
2. Theoretical Rigor: The authors provide a solid theoretical foundation for the proposed modifications, including convergence guarantees and error analysis.  
3. Empirical Validation: Comprehensive experiments on benchmark functions demonstrate the efficacy of the proposed approach, with clear performance gains in terms of runtime.  
4. Practical Relevance: The reduced memory footprint and computational complexity make the algorithm highly relevant for real-world applications in machine learning and optimization.  
5. Clarity: The paper is well-written, with clear explanations of the methodology, theoretical insights, and experimental results.
Weaknesses:  
1. CPU Comparability: The paper does not clarify whether the computational environments for the different CMA-ES variants are directly comparable, which could affect runtime conclusions.  
2. Discrepancies in High Dimensions: Figure 2 shows unexplained discrepancies in performance for certain high-dimensional cases, which warrants further clarification.  
3. Scope of Benchmarks: While the benchmarks are standard, additional real-world optimization tasks could strengthen the empirical validation.
Pro Acceptance Arguments:  
- The paper makes a significant advancement in the field of derivative-free optimization by addressing a well-known bottleneck in CMA-ES.  
- The proposed algorithm is both theoretically sound and empirically validated, with clear practical implications.  
- The reduction in computational complexity and memory usage is highly impactful for scaling optimization methods to higher dimensions.
Con Acceptance Arguments:  
- Minor issues, such as CPU comparability and discrepancies in high-dimensional results, need clarification but do not undermine the overall contribution.  
- The scope of benchmarks could be expanded, but the current evaluation is sufficient to demonstrate the algorithm's advantages.
In conclusion, this paper represents a valuable contribution to the optimization and machine learning communities. I recommend its acceptance, provided the authors address the minor issues raised.