This paper introduces a novel batch Bayesian optimization algorithm, the parallel knowledge gradient (q-KG), aimed at optimizing black-box functions in parallel settings. The method leverages a decision-theoretic framework to compute Bayes-optimal configurations for batch evaluations and employs infinitesimal perturbation analysis (IPA) to efficiently estimate gradients of the q-KG acquisition function. The authors demonstrate the algorithm's effectiveness on synthetic functions and practical machine learning tasks, particularly in noisy settings.
Strengths:
1. Relevance and Impact: The problem of parallel Bayesian optimization is highly relevant, especially for hyperparameter tuning in machine learning, where parallel evaluations can significantly reduce computational costs. The proposed q-KG method addresses this need and demonstrates strong performance in noisy environments, making it a valuable contribution to the field.
2. Technical Clarity: The paper is well-written, with clear explanations of the q-KG acquisition function, its derivation, and the computational strategies employed. The inclusion of implementation details and open-source code enhances reproducibility.
3. Experimental Results: The empirical evaluation spans both synthetic benchmarks and real-world applications (e.g., tuning CNNs on CIFAR-10 and logistic regression on MNIST). The results show that q-KG is competitive with or superior to state-of-the-art methods, particularly in noisy scenarios, which is a notable strength.
4. Efficient Gradient Estimation: The use of IPA for gradient estimation is a practical contribution, enabling efficient optimization of the computationally expensive q-KG acquisition function.
Weaknesses:
1. Originality: While the combination of the knowledge gradient and IPA is novel in this context, both techniques have been explored independently in prior work. The paper lacks a clear articulation of how it advances beyond existing methods like Wang et al. (2015) or other parallel Bayesian optimization approaches.
2. Experimental Limitations: The experimental results, though promising, are not entirely convincing. The authors do not provide wall-clock time comparisons, which are critical for evaluating the practical utility of the method. Additionally, the choice of batch size (q=4) is not justified, and scalability to larger batch sizes (e.g., q=10, q=25) is not explored.
3. Asynchronous Execution: The paper does not address whether the method can be extended to asynchronous settings, a feature supported by other approaches like Spearmint. This limits its applicability in real-world distributed systems.
4. Plot Clarity: The meaning of "iterations" in the plots is unclear, and it is not specified whether all methods start from the same initial design. This ambiguity detracts from the interpretability of the results.
5. Minor Issues: There are minor grammatical errors, missing terms in equations, and incorrect references for IPA, which require correction.
Pro and Con Arguments for Acceptance:
Pros:
- Addresses a practical and impactful problem in parallel Bayesian optimization.
- Demonstrates strong performance in noisy settings, where many existing methods struggle.
- Provides a clear and reproducible implementation.
Cons:
- Limited originality due to reliance on existing techniques.
- Lack of wall-clock time analysis and scalability studies.
- Unclear applicability to asynchronous settings.
Recommendation:
This paper makes a meaningful contribution to parallel Bayesian optimization, particularly for noisy problems, and is well-suited for the conference audience. However, the lack of novelty, incomplete experimental evaluation, and limited discussion of scalability and asynchronous execution are significant drawbacks. I recommend acceptance with major revisions, focusing on addressing these issues to strengthen the paper's impact and practical utility.