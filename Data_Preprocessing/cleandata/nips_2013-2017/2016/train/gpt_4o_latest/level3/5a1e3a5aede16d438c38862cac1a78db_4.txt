This paper investigates the support recovery guarantees for sparse regression problems using the \( l1 \)-norm as a regularizer and non-smooth loss functions (\( l1 \) and \( l\infty \)) for data fidelity, contrasting them with the more commonly studied \( l2 \)-norm. The authors extend the theoretical understanding of sparse regression by deriving sharp conditions for support stability under small additive noise, even in cases where the support is unstable. A novel contribution is the introduction of the concept of "extended support," which remains stable even when the original support does not. The paper also includes numerical experiments in a compressed sensing setting to validate the theoretical findings and highlight the differences between the loss functions.
Strengths:
1. Quality: The paper is technically rigorous, with a well-structured theoretical framework that extends existing results for the \( l2 \)-norm to the non-smooth \( l1 \) and \( l_\infty \) cases. The proofs are detailed and address the unique challenges posed by non-smooth loss functions. The authors also acknowledge limitations, such as the inability to generalize results to arbitrary \( \alpha \) or large noise regimes, which reflects a balanced and honest evaluation of their work.
   
2. Clarity: The paper is well-organized, with clear definitions, theorems, and proofs. The inclusion of numerical experiments enhances understanding, and the figures effectively illustrate the theoretical results. However, some sections, particularly the proofs, are dense and may require additional clarification for non-expert readers.
3. Originality: The study addresses a significant gap in the literature by focusing on non-smooth loss functions for sparse regression. While the \( l2 \)-norm has been extensively studied, the extension to \( l1 \) and \( l_\infty \) is novel and important for applications involving robust or uniform noise models. The introduction of the "extended support" concept is particularly innovative.
4. Significance: The results have practical implications for compressed sensing and other high-dimensional inverse problems. The theoretical guarantees and numerical insights provide a foundation for further research and potential applications in areas requiring robust sparse recovery under non-smooth loss functions.
Weaknesses:
1. The paper primarily focuses on small noise regimes, leaving the analysis of larger noise levels as future work. While this is a reasonable limitation, it restricts the immediate applicability of the results.
2. The proofs, while rigorous, are highly technical and may not be easily accessible to a broader audience. A more intuitive explanation of key results could improve accessibility.
3. The experimental section, though informative, could benefit from additional real-world examples or applications to demonstrate the practical utility of the proposed methods.
Recommendation:
This paper makes a significant theoretical contribution to the field of sparse regression and addresses an important gap in the literature. While there are minor weaknesses in terms of accessibility and scope, the strengths far outweigh these limitations. I recommend acceptance, with a suggestion to include more intuitive explanations and potentially expand the experimental section to include real-world applications.
Pros:
- Rigorous theoretical analysis.
- Novel extension to non-smooth loss functions.
- Practical implications for compressed sensing and related fields.
Cons:
- Limited to small noise regimes.
- Dense proofs may hinder accessibility.
- Experiments could include more real-world scenarios.