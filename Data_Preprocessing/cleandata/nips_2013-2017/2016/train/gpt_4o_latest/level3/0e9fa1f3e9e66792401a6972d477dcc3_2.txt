This paper presents an innovative extension of the bidirectional Monte Carlo (BDMC) technique to evaluate MCMC-based posterior inference algorithms, specifically by bounding the symmetrized KL divergence (Jeffreys divergence) between approximate samples and the true posterior distribution. The authors integrate their method, termed BREAD (Bounding Divergences with REverse Annealing), into two widely used probabilistic programming frameworks, WebPPL and Stan, and validate its effectiveness through experiments on simulated and real-world datasets. The paper addresses a critical challenge in probabilistic inference—quantifying the quality of approximate posterior samples—a problem of significant interest to the community.
Strengths:
1. Technical Quality and Rigor: The paper is technically sound, with a well-supported theoretical foundation. The derivation of bounds on the Jeffreys divergence is rigorous, and the authors validate their method on toy distributions and real-world models, demonstrating its reliability.
2. Significance: The work addresses a pressing issue in probabilistic inference: the lack of reliable measures for evaluating MCMC-based algorithms. By providing a quantitative and rigorous framework, BREAD has the potential to significantly impact both researchers developing inference algorithms and practitioners using probabilistic programming languages.
3. Integration with Probabilistic Programming: The integration of BREAD into WebPPL and Stan is a practical contribution, making the method accessible to a wide audience. The experiments highlight its utility in guiding model representation choices and debugging probabilistic programming implementations.
4. Originality: Extending BDMC to bound the Jeffreys divergence and applying it to evaluate posterior inference quality is a novel contribution. The proposed protocol for using simulated data as a proxy for real-world data is also an interesting addition.
5. Clarity: The paper is well-organized and clearly written, with detailed explanations of the methodology and experiments. The inclusion of validation experiments and debugging use cases strengthens the narrative.
Weaknesses:
1. Scope of Applicability: The method relies on access to exact posterior samples, which limits its direct applicability to simulated data. While the authors propose a heuristic for real-world data, its generalizability remains to be fully explored.
2. Computational Overhead: The approach requires running both forward and reverse AIS chains, which may be computationally expensive for large-scale models or datasets. This tradeoff is not thoroughly discussed.
3. Limited Comparison to Related Work: While the paper references related work, a more detailed comparison with alternative methods for evaluating MCMC convergence (e.g., diagnostic metrics) would strengthen the contribution.
Recommendation:
The paper is a high-quality contribution that addresses a significant problem in probabilistic inference. Its theoretical rigor, practical integration, and experimental validation make it a valuable addition to the field. Despite some limitations in scope and computational cost, the strengths far outweigh the weaknesses. I recommend accepting the paper. 
Arguments for Acceptance:
- Novel and rigorous extension of BDMC to bound Jeffreys divergence.
- Practical integration into widely used frameworks (WebPPL and Stan).
- Demonstrated utility in guiding model design and debugging.
- Addresses a critical and timely problem in the field.
Arguments Against Acceptance:
- Limited applicability to real-world data without exact posterior samples.
- Computational cost of the method is not fully analyzed.
Overall, the paper is a strong contribution and aligns well with the goals of the conference.