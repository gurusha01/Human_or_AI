Review
This paper introduces the concept of "low approximate regret" and demonstrates its utility in achieving faster convergence to approximate optimality in repeated games. The authors argue that this property is broadly satisfied by many common learning algorithms, including the vanilla Hedge algorithm, and extend their analysis to dynamic population games and bandit feedback settings. The work builds on and improves prior results, particularly those of Syrgkanis et al. [28], by relaxing the feedback model to rely on realized outcomes rather than expected payoffs and achieving faster convergence rates in certain settings. The authors also propose a new bandit algorithm with improved dependence on the number of actions, which is both simple and efficient.
Strengths:
1. Novelty and Generalization: The introduction of the low approximate regret property is a meaningful contribution, as it generalizes existing regret properties like RVU and provides a unifying framework for analyzing convergence in games. The results are also extended to dynamic population games, improving upon Lykouris et al. [19].
2. Broader Applicability: The paper demonstrates that many common algorithms, such as Hedge and its variants, satisfy the low approximate regret property, making the results widely applicable. The proposed framework also accommodates more realistic feedback models, such as realized loss vectors and bandit feedback.
3. Theoretical Contributions: High-probability bounds are derived for the proposed approach, and the authors achieve faster convergence rates (e.g., \(O(n/T)\)) compared to prior work. The new bandit algorithm with \(O(d \log T)\) regret is a notable improvement over existing methods.
4. Clarity of Results: The paper effectively contrasts its contributions with prior work, particularly [28], highlighting differences in feedback models, convergence rates, and approximation guarantees.
Weaknesses:
1. Overstated Claims: The claim of improvement over [28] in feedback models is overstated, as Theorem 23 from [28] already applies to realized feedback. This should be clarified to avoid misleading readers.
2. Approximation Quality: While faster convergence is achieved, it is only to an approximation of the Price of Anarchy (PoA), making the results less directly comparable to prior work that achieves exact PoA bounds.
3. Unclear Aspects: The role of the doubling trick in transitioning from weak to strong low approximate regret is unclear and requires further elaboration. Similarly, Theorem 3 includes informal statements that should be replaced with precise expressions.
4. Presentation Issues: Definitions and propositions need refinement, with missing summations, unclear parameter ranges, and unnatural variable expressions. Examples and related paragraphs should be moved to Section 2 for better flow. Minor typos and formatting issues are present throughout the paper.
5. Algorithm Similarity: The proposed noisy Hedge algorithm closely resembles Fixed Share (Cesa-Bianchi et al., 2012), and this similarity should be acknowledged more explicitly.
Suggestions for Improvement:
1. Clarify why expectation feedback models are unnecessary in this setting and address overstatements in Lines 49 and 80.
2. Refine definitions and propositions, ensuring mathematical rigor and completeness.
3. Provide more detailed explanations of the doubling trick and its role in achieving strong low approximate regret.
4. Relocate examples to Section 2 for better organization and flow.
5. Acknowledge the resemblance of the noisy Hedge algorithm to Fixed Share and discuss any differences explicitly.
Recommendation:
While the paper makes significant theoretical contributions and broadens the applicability of regret-based learning dynamics, the overstated claims, unclear aspects, and presentation issues detract from its overall quality. I recommend acceptance with major revisions, as the core ideas are valuable but require better clarity and refinement. 
Pros:
- Novel framework with broad applicability.
- Improved convergence rates and high-probability guarantees.
- Extensions to dynamic and bandit settings.
Cons:
- Overstated claims and unclear aspects.
- Approximation quality limits direct comparability.
- Presentation and organization issues.