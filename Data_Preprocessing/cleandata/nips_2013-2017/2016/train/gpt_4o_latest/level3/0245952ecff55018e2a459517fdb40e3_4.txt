This paper introduces Conditional Generative Moment-Matching Networks (CGMMN), an extension of Generative Moment-Matching Networks (GMMN) to enable conditional generation using a kernel embedding of moments. The authors propose a novel conditional maximum mean discrepancy (CMMD) criterion to train the model and demonstrate its utility across diverse tasks, including predictive modeling, contextual generation, and Bayesian model distillation. The paper builds on prior work in deep generative models (DGMs) and kernel methods, such as GMMN and conditional GANs, while addressing the need for a simpler and more flexible framework for conditional distributions.
Strengths:
1. Technical Soundness: The paper is technically robust, presenting a well-grounded derivation of the CMMD objective. While not all equations were independently verified, the theoretical framework appears consistent and well-supported by prior work in kernel methods and DGMs.
2. Novelty and Utility: The conditional extension of GMMN is a natural idea, but the derivation of the empirical estimator for CMMD adds significant novelty. This innovation enhances the applicability of moment-matching networks to conditional tasks, filling a gap in the literature.
3. Experimental Validation: The proposed CGMMN demonstrates competitive performance across a variety of tasks:
   - Predictive Modeling: On MNIST and SVHN datasets, CGMMN achieves results comparable to state-of-the-art DGMs, such as max-margin DGMs and deeply-supervised nets.
   - Contextual Generation: The model generates diverse and high-quality samples conditioned on labels, as shown in experiments on MNIST and the Yale Face dataset.
   - Bayesian Model Distillation: CGMMN effectively distills Bayesian models without degrading predictive performance, as evidenced by results on the Boston housing dataset.
4. Practical Relevance: The framework is easy to implement, as demonstrated in Algorithm 1. Its simplicity and compatibility with existing architectures (e.g., MLP, CNN) make it accessible for practitioners.
5. Clarity and Organization: The paper is well-written, with clear explanations of the methodology and detailed experimental results. The appendix provides additional insights into theoretical derivations and experimental setups.
Weaknesses:
1. Limited Theoretical Discussion: While the CMMD criterion is well-motivated, the paper could benefit from a deeper theoretical analysis of its properties, such as robustness to noise or scalability to high-dimensional data.
2. Experimental Comparisons: Although CGMMN performs competitively, comparisons with conditional GANs or conditional variational autoencoders (CVAEs) are limited. Including these baselines would strengthen the empirical evaluation.
3. Scalability: The cubic complexity of kernel gram matrix computations may limit scalability to very large datasets. While the mini-batch algorithm mitigates this issue, a discussion of potential trade-offs or approximations would be helpful.
Arguments for Acceptance:
- The paper makes a significant contribution by extending GMMN to conditional tasks, with a novel and practical CMMD criterion.
- The experimental results are thorough and demonstrate the model's versatility across diverse applications.
- The work is well-written, technically sound, and relevant to the NeurIPS audience, aligning with topics such as generative modeling and kernel methods.
Arguments Against Acceptance:
- The lack of comparisons with conditional GANs and CVAEs leaves some questions about the model's relative performance.
- Scalability concerns for large datasets are not fully addressed.
Recommendation:
I recommend acceptance of this paper. While there are minor limitations, the novelty, technical rigor, and practical relevance of CGMMN make it a valuable contribution to the field of generative modeling and conditional learning.