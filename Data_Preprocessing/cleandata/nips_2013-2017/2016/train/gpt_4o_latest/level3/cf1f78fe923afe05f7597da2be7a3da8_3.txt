This paper introduces Riemannian Relaxation (RR), a novel algorithm for improving low-dimensional embeddings of data sampled from manifolds. Unlike existing methods, RR directly optimizes a distortion loss function based on the push-forward Riemannian metric, which measures deviation from isometry. The algorithm iteratively updates embedding coordinates to minimize this non-convex loss, allowing for embeddings in dimensions \(s \geq d\) (where \(d\) is the intrinsic dimension). Experimental results demonstrate that RR achieves lower distortion than methods like Laplacian Eigenmaps, Isomap, HLLE, and MVU on synthetic datasets, and it is extended to large and noisy datasets using a subsampling approach.
Strengths:
1. Novelty and Theoretical Contribution: The paper proposes a distortion loss function grounded in Riemannian geometry, offering a principled way to quantify and minimize deviations from isometry. This is a significant departure from heuristic loss functions used in existing methods.
2. Flexibility in Embedding Dimensions: Unlike most manifold learning algorithms that restrict \(s = d\), RR allows embeddings in higher dimensions (\(s > d\)), aligning with theoretical guarantees like the Whitney and Nash embedding theorems.
3. Empirical Performance: The method achieves lower distortion than competing algorithms across several synthetic datasets, demonstrating its effectiveness.
4. Scalability: The PCS-RR extension for large and noisy datasets is a practical addition, making the algorithm applicable to real-world problems, as shown in the SDSS galaxy spectra experiment.
5. Clarity of Writing: The paper is well-organized, with clear explanations of the algorithm, loss function, and experimental setup.
Weaknesses:
1. Lack of Theoretical Guarantees: While the loss function is well-motivated, the paper does not provide a theorem or proof to establish conditions under which near-zero loss guarantees isometry. This omission weakens the theoretical rigor.
2. Non-Convex Optimization: The non-convex nature of the loss raises concerns about convergence to a global optimum. The paper does not discuss strategies to mitigate this issue or provide empirical evidence on the robustness of local minima.
3. Sensitivity to Initialization: The algorithm relies on initialization from methods like Laplacian Eigenmaps or Isomap, but the sensitivity of results to different initializations is not thoroughly investigated.
4. Computational Cost: RR is computationally expensive, and the improvements in distortion are sometimes marginal. A detailed comparison of running times with other methods is missing.
5. Practical Utility: The paper does not justify the practical utility of low-distortion embeddings in downstream tasks like classification or regression, which limits its broader appeal.
6. Minor Typos: Equations (7) and \(\mathcal{L}_k\) require clarification, and there are minor typographical errors.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded approach to manifold learning, which is a core topic at NIPS.
- The flexibility of embedding dimensions (\(s \geq d\)) and the focus on isometry preservation are significant contributions to the field.
- The PCS-RR extension demonstrates scalability, making the method relevant for large datasets.
Arguments Against Acceptance:
- The lack of theoretical guarantees and insufficient exploration of the non-convex optimization challenge weaken the paper's scientific rigor.
- The computational cost and marginal improvements in distortion may limit the practical impact of the method.
- The absence of downstream task evaluations leaves the broader utility of the method unclear.
Recommendation:
Borderline Accept. While the paper makes a novel contribution to manifold learning, addressing the theoretical and computational concerns would significantly strengthen its impact.