This paper introduces Stochastic Multiple Choice Learning (sMCL), a novel stochastic gradient descent (SGD)-based algorithm for training ensembles of deep networks to minimize oracle loss. Building on the Multiple Choice Learning (MCL) paradigm proposed by Guzman-Rivera et al. [8], the authors address the inefficiencies of prior methods, such as costly retraining or sequential training, by proposing a parameter-free, architecture-agnostic, and efficient approach. The paper demonstrates the broad applicability of sMCL across diverse tasks, including image classification, semantic segmentation, and image captioning, and highlights its ability to induce interpretable specialization among ensemble members.
Strengths:
1. Technical Contribution: The paper makes a significant contribution by extending MCL to deep neural networks through a stochastic optimization framework. The proposed sMCL algorithm is computationally efficient, requiring only a single training pass, and achieves a 5x speedup over the original MCL method while maintaining or improving performance.
2. Broad Applicability: The authors convincingly demonstrate the versatility of sMCL across multiple domains and architectures, including CNNs, FCNs, and CNN+RNNs. The experiments show consistent improvements in oracle metrics over classical ensembles, MCL, and other baselines.
3. Interpretability: The paper provides insightful analyses of the emergent specialization among ensemble members, such as class-wise clustering in classification tasks and diverse segmentations or captions in more complex tasks. This interpretability is a valuable byproduct of the algorithm.
4. Clarity and Writing: The paper is well-organized and clearly written, with detailed explanations of the methodology, experiments, and results. The inclusion of qualitative examples (e.g., diverse segmentations and captions) enhances the reader's understanding of the method's impact.
Weaknesses:
1. Incremental Novelty: While the authors convincingly address concerns about the algorithm's novelty relative to Guzman-Rivera et al. [8], the contribution may still be perceived as incremental by some, as it primarily adapts existing ideas to deep learning.
2. Baseline Comparisons: The paper compares sMCL to MCL and classical ensembles but does not explore comparisons with batch-based training approaches using subsets of the dataset, as suggested in the review guidelines. This could provide additional insights into the algorithm's efficiency and performance.
3. Diversity Analysis: Although the paper emphasizes the importance of diversity in ensemble outputs, the analysis of this aspect remains somewhat qualitative. A more rigorous quantitative evaluation of diversity (e.g., diversity metrics) would strengthen the claims.
Arguments for Acceptance:
- The paper addresses a practical and important problem in ensemble learning for deep networks.
- sMCL is simple, efficient, and broadly applicable, making it a valuable contribution to the field.
- The experimental results are thorough and demonstrate significant improvements over strong baselines.
Arguments Against Acceptance:
- The novelty of the algorithm may be viewed as incremental.
- The lack of comparison with batch-based training approaches and limited quantitative diversity analysis are minor gaps.
Recommendation:
Overall, this paper presents a well-motivated and impactful contribution to ensemble learning in deep networks. While there are minor areas for improvement, the strengths far outweigh the weaknesses. I recommend acceptance with minor revisions to address the diversity analysis and additional baseline comparisons.