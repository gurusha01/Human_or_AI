The paper presents a deterministic algorithm for constructing a (k, ε)-coreset of size \( O(k^2/\epsilon^2) \) for dimensionality reduction, specifically targeting sparse, large-scale matrices such as the Wikipedia document-term matrix. The authors claim their method is the first to achieve a coreset size independent of both the number of rows \( n \) and columns \( d \), providing provable performance guarantees. The algorithm is evaluated on synthetic data and applied to compute Latent Semantic Analysis (LSA) on the entire English Wikipedia, demonstrating scalability and practical utility.
Strengths
1. Practical Contribution: The deterministic nature of the algorithm and its ability to handle sparse data without requiring random projections or dense sketches is a significant practical advancement. This is particularly relevant for large-scale datasets like Wikipedia, where existing methods fail due to memory constraints.
2. Scalability: The authors demonstrate the algorithm's scalability by applying it to the Wikipedia dataset, achieving results previously unattainable with state-of-the-art methods.
3. Provable Guarantees: The theoretical analysis provides bounds on the coreset size and approximation error, which are supported by experimental results.
4. Implementation and Benchmarking: The authors implement the algorithm and compare it against existing methods, showing competitive performance in terms of accuracy and runtime.
Weaknesses
1. Incorrect Citations: The paper incorrectly cites prior work, referencing [8] instead of the stronger result by Cohen et al., which achieves a coreset size of \( O(k/\epsilon^2) \). This oversight undermines the novelty of the claimed contributions.
2. Contradictory Claims: The authors assert that their algorithm is the first to compute a (k, ε)-coreset of size independent of \( n \) and \( d \), but this is contradicted by prior work, including Theorem 8 in another relevant paper. A proper comparison to these results is missing.
3. Unclear Comparisons: The comparison to [7] is vague, as [7] might also be made deterministic for rank-k approximation using similar techniques. This weakens the claim of novelty.
4. Weaker Bounds: While the deterministic sampling approach is practical, the \( O(k^2/\epsilon^2) \) bound is weaker than the \( O(k/\epsilon^2) \) bound achieved by randomized methods. The trade-offs between determinism and coreset size are not adequately discussed.
5. Clarity: The paper is dense and could benefit from clearer explanations of the algorithm and its theoretical guarantees. Some sections, such as the proofs, are difficult to follow without supplementary material.
Arguments for Acceptance
- The deterministic construction of coresets for sparse data is a valuable contribution with practical implications.
- The application to large-scale datasets like Wikipedia demonstrates the algorithm's utility and scalability.
- The paper addresses an important problem in dimensionality reduction and provides a novel approach.
Arguments Against Acceptance
- The novelty of the contributions is overstated due to incorrect citations and lack of proper comparisons to prior work.
- The weaker \( O(k^2/\epsilon^2) \) bound limits the theoretical significance of the results.
- The paper's clarity and organization could be improved to make it more accessible to readers.
Recommendation
The paper could be accepted if the authors address the issues of incorrect citations, provide a thorough comparison to prior work (e.g., Cohen et al.), and emphasize the practical contributions of their deterministic approach. Without these revisions, the paper's claims of novelty and significance are weakened.