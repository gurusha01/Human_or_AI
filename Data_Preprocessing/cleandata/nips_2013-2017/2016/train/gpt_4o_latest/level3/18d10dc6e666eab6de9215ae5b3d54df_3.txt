The paper introduces a novel batch Bayesian optimization algorithm, the parallel knowledge gradient (q-KG), which leverages a stochastic gradient approach combined with Bayesian modeling to optimize multivariate objective functions over finite domains. The key innovation lies in the extension of the Knowledge Gradient (KG) framework to the parallel setting, enabling the algorithm to identify multiple sampling points per iteration for parallel computation. This is achieved by maximizing the q-KG acquisition function, which quantifies the expected improvement in solution quality from evaluating a batch of points. To address the computational challenges of this approach, the authors employ Monte Carlo sampling and Infinitesimal Perturbation Analysis (IPA) to estimate gradients efficiently.
Strengths:  
The paper is technically sound, with a solid theoretical foundation and rigorous empirical evaluation. The authors provide a clear derivation of the q-KG acquisition function and propose an efficient computational strategy to optimize it, making the method practical for real-world applications. The experimental results are compelling, demonstrating that q-KG consistently outperforms or competes with state-of-the-art methods, particularly in noisy settings. The method's ability to handle noisy evaluations and its superior performance in hyperparameter tuning tasks for machine learning models (e.g., logistic regression and CNNs) highlight its practical significance. The paper is well-written, with a logical structure and sufficient detail to allow reproducibility.
Weaknesses:  
While the paper presents a strong contribution, its novelty is somewhat limited. The q-KG algorithm builds on the existing Knowledge Gradient framework, and the use of IPA for gradient estimation is standard in the field. The main contribution lies in extending KG to the parallel setting and implementing it efficiently, which, while valuable, may not represent a groundbreaking advance. Additionally, there is a specific concern regarding the definition of the mean function (Âµ) and kernel function (K) in equation (3.1) for finite sequences of vectors, which could benefit from further clarification. Finally, the paper could have explored a broader range of benchmarks or provided a deeper analysis of the computational trade-offs compared to other methods.
Arguments for Acceptance:  
1. The paper addresses a relevant and challenging problem in Bayesian optimization, with clear practical applications.  
2. The proposed method demonstrates strong empirical performance, particularly in noisy settings, advancing the state of the art.  
3. The theoretical and computational contributions are well-executed and sufficiently detailed.  
Arguments Against Acceptance:  
1. The novelty is incremental, as the work builds on established frameworks (KG and IPA).  
2. Minor concerns about clarity in specific equations and definitions.  
Conclusion:  
Overall, the paper represents a high-quality contribution to the field of Bayesian optimization. While the novelty is modest, the practical significance and strong empirical results justify its acceptance. I recommend acceptance, with minor revisions to address the noted concerns.