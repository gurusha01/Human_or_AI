This paper presents a novel and practical approach to dimensionality reduction for extremely large sparse matrices, specifically focusing on computing Principal Component Analysis (PCA) using coresets. The authors address two longstanding challenges: constructing a coreset independent of both the number of rows (n) and columns (d) of the input matrix, and applying this method to real-world, large-scale datasets such as the Wikipedia document-term matrix. The proposed algorithm is deterministic, efficient, and provably correct, making it a significant contribution to the field of dimensionality reduction.
The paper builds on prior work in coreset construction and dimensionality reduction, such as sensitivity sampling and the Frank-Wolfe algorithm, while addressing their limitations. Unlike previous methods, which either fail to preserve sparsity or scale poorly for "fat" matrices where d â‰ˆ n, the authors' approach constructs coresets of size independent of both n and d. This is a notable advancement over existing techniques, which often rely on random projections or sketches that do not preserve sparsity. The authors also demonstrate the practicality of their method by successfully applying it to the English Wikipedia dataset, a matrix with 3.69 million rows and 7.96 million columns, achieving results that were previously computationally infeasible.
Strengths:
1. Technical Soundness: The paper provides rigorous theoretical guarantees for the proposed algorithm, including proofs of correctness and bounds on error and runtime. The experimental results corroborate these claims.
2. Clarity and Organization: The paper is well-written, with a clear problem formulation, detailed technical explanations, and a logical flow. The inclusion of pseudocode and experimental results enhances reproducibility.
3. Originality: The approach is highly original, particularly in its use of coresets to tackle the challenges of dimensionality reduction for large, sparse datasets. The independence of the coreset size from n and d is a significant theoretical breakthrough.
4. Significance: The ability to compute PCA for datasets as large as Wikipedia has broad implications for fields such as natural language processing, information retrieval, and machine learning. The method is likely to inspire further research and practical applications.
Weaknesses:
1. Complexity of Presentation: While the paper is thorough, some sections, particularly the proofs and algorithmic details, may be difficult for non-experts to follow. Simplifying or summarizing key technical results could improve accessibility.
2. Experimental Scope: The experiments focus primarily on Wikipedia and synthetic data. Additional benchmarks on other large-scale, real-world datasets (e.g., image or graph data) would strengthen the paper's generalizability claims.
3. Comparison with Alternatives: Although the paper compares its method to state-of-the-art algorithms like svds and random sampling, a more comprehensive evaluation against other recent coreset-based or sketching methods would provide a fuller picture of its advantages.
Pro Acceptance Arguments:
- The paper addresses a critical and challenging problem with a novel, theoretically sound, and practical solution.
- The results are significant, demonstrating scalability and applicability to real-world datasets.
- The work advances the state of the art in dimensionality reduction and opens avenues for future research.
Con Acceptance Arguments:
- The complexity of the presentation may limit its accessibility to a broader audience.
- The experimental evaluation, while strong, could be expanded to include more diverse datasets and comparisons.
In conclusion, this paper makes a substantial contribution to the field of dimensionality reduction and is well-suited for acceptance at NIPS. Its combination of theoretical rigor, practical applicability, and scalability makes it a valuable addition to the conference.