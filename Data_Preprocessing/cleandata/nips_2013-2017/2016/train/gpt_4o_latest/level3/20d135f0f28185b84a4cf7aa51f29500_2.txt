The paper introduces Stochastic Multiple Choice Learning (sMCL), a novel method for training ensembles of deep neural networks (DNNs) that minimizes oracle loss by guiding backpropagation updates to the optimal network for each data instance. This approach builds on the Multiple Choice Learning (MCL) paradigm, extending it to deep learning with a stochastic gradient descent (SGD)-based algorithm. Unlike prior methods, sMCL trains all ensemble members concurrently, avoiding the computational overhead of retraining or sequential training. The proposed method is architecture-agnostic, parameter-free, and applicable across diverse tasks, including image classification, semantic segmentation, and image captioning.
The paper is well-written and provides a clear exposition of the problem, methodology, and experimental results. The authors effectively position their work within the context of existing ensemble learning and multiple-output generation methods, citing relevant prior work such as Guzman-Rivera et al. (2012) and Dey et al. (2015). The novelty lies in the use of a "winner-take-gradient" strategy to enforce specialization among ensemble members, which is demonstrated to yield diverse and interpretable outputs. This specialization aligns well with top-k metrics, making the method particularly effective for tasks like ImageNet classification.
The experimental results are compelling, showcasing significant improvements in oracle metrics across multiple domains. For instance, sMCL achieves higher oracle accuracy in image classification and semantic segmentation tasks compared to classical ensembles, MCL, and other baselines, while being computationally more efficient. The qualitative analysis further highlights the interpretability of the specialized outputs, such as class clustering in classification tasks and diverse segmentations in semantic segmentation. Additionally, the method's ability to generate diverse captions in image captioning tasks underscores its broad applicability.
However, the paper has some limitations. While the method produces diverse outputs, it lacks a detailed discussion on how to exploit these trained ensembles in practical downstream applications. For example, guidance on selecting the most appropriate prediction from the ensemble for real-world tasks is missing. Additionally, the pseudo-code in Figure 2 is embedded as a raster image, which detracts from clarity and reproducibility. Using vector graphics or inline text would improve readability.
Strengths:
1. Novel and computationally efficient method for training diverse DNN ensembles.
2. Strong empirical results across multiple domains and tasks.
3. Clear presentation and well-structured experiments.
4. Demonstrates interpretable specialization among ensemble members.
Weaknesses:
1. Limited discussion on practical deployment of the trained ensembles.
2. Pseudo-code presentation could be improved for clarity.
Recommendation:
I recommend acceptance of this paper, as it represents a significant contribution to ensemble learning in deep networks. Addressing the practical deployment of ensembles and improving the pseudo-code presentation would further strengthen the work.