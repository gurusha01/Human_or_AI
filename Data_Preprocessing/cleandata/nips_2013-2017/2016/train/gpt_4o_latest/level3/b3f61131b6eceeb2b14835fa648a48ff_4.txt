This paper presents a significant generalization of recent work on repeated games, introducing the concept of "realized feedback" instead of the more commonly used "expected feedback." This shift makes the setting more realistic and applicable to practical scenarios where players only observe outcomes of their actual actions, not hypothetical distributions. The authors extend their analysis to bandit feedback and dynamic population games, broadening the scope of applicability. The proposed framework is based on the novel "Low Approximate Regret" property, which is shown to be satisfied by a wide range of learning algorithms, including simple ones like Hedge. The paper also improves upon prior work (SAL15 and LST16) by achieving faster convergence rates and accommodating a broader class of algorithms.
Strengths:
1. Conceptual Contribution: The introduction of realized feedback is a notable advancement, addressing a critical limitation in prior work. It aligns theoretical models more closely with real-world scenarios.
2. Generality: The Low Approximate Regret property is shown to be ubiquitous among learning algorithms, making the results broadly applicable.
3. Extensions: The paper extends its analysis to bandit feedback and dynamic population games, which are important and challenging settings.
4. Improved Convergence: The authors achieve faster convergence rates compared to prior work, which is a meaningful improvement.
5. Clarity and Organization: The paper is well-written and logically structured, making its contributions accessible to readers.
Weaknesses:
1. Technical Similarity to Prior Work: While the conceptual contributions are significant, the technical methods appear to be incremental extensions of SAL15 and LST16. This raises questions about the novelty of the techniques.
2. Lack of Empirical Validation: The absence of experimental comparisons with SAL15 and LST16 is a notable omission. Without empirical results, it is difficult to assess the practical implications of the proposed methods.
3. Complexity of Bandit Algorithm: While the bandit algorithm achieves improved regret bounds, its practical efficiency and scalability are not thoroughly discussed.
Arguments for Acceptance:
- The paper addresses an important limitation in the literature by introducing realized feedback.
- Its results are broadly applicable and advance the state of the art in repeated games.
- The theoretical contributions are well-supported and clearly presented.
Arguments Against Acceptance:
- The lack of experimental results limits the practical impact of the work.
- The technical contributions, while meaningful, may not be sufficiently novel to warrant acceptance at a top-tier conference.
Suggestions for Improvement:
- Include empirical comparisons with SAL15 and LST16 in the final version to demonstrate the practical advantages of the proposed methods.
- Discuss the computational efficiency and scalability of the bandit algorithm in more detail.
- Highlight any unique technical innovations that differentiate this work from prior research.
In summary, this paper makes valuable contributions to the theory of repeated games and is likely to be of interest to game theorists. However, addressing the noted weaknesses, particularly the lack of empirical results, would significantly strengthen its impact.