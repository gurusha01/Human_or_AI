This paper addresses the critical problem of computing safe policies in sequential decision-making under uncertainty, specifically in the context of infinite-horizon discounted Markov Decision Processes (MDPs). The authors propose a novel model-based approach that minimizes robust regret with respect to a baseline policy, allowing for state-wise improvements where model accuracy is high while defaulting to the baseline policy in uncertain regions. This approach contrasts with existing methods that either adopt a learned policy with provable improvements or revert entirely to the baseline policy, which can be overly conservative in settings with non-uniform model uncertainties.
Strengths:
1. Novelty and Significance: The paper introduces a robust regret minimization framework that unifies uncertainties in the learned and baseline policies. This approach is novel and addresses a key limitation of existing methods, which often fail to leverage partial improvements due to conservative assumptions. The proposed method has significant implications for real-world applications, such as online marketing and energy arbitrage, where safety guarantees are paramount.
   
2. Theoretical Contributions: The authors rigorously analyze the properties of their optimization problem, proving its NP-hardness and deriving performance bounds. They also demonstrate that randomized policies can outperform deterministic ones in this setting, which is a valuable insight for robust decision-making.
3. Practical Approximation: The paper proposes a simple, practical approximate algorithm (Algorithm 1) that modifies the uncertainty model to assume no error in baseline transitions. This heuristic is computationally efficient and performs well in experiments, making it a promising solution for real-world applications.
4. Empirical Validation: The experimental results are compelling, demonstrating that the proposed method outperforms standard approaches in both synthetic (grid problem) and realistic (energy arbitrage) domains. The results highlight the method's ability to balance safety and performance, particularly in high-uncertainty settings.
Weaknesses:
1. Complexity of Presentation: While the theoretical contributions are strong, the paper is dense and could benefit from clearer explanations of key concepts, such as the robust regret formulation. For example, the distinction between the proposed method and standard robust MDP approaches could be more explicitly emphasized early in the paper.
2. Limited Baseline Comparisons: The experimental evaluation, while thorough, could include additional comparisons with more recent safe reinforcement learning methods to better contextualize the contributions.
3. Scalability: The paper does not discuss the scalability of the proposed approach to very large state-action spaces, which could limit its applicability to high-dimensional problems.
4. Acknowledgment of Limitations: While the authors acknowledge the NP-hardness of their optimization problem, the discussion of limitations is somewhat brief. For instance, the reliance on accurate error bounds for the uncertainty model may not always be feasible in practice.
Recommendation:
This paper makes a significant contribution to the field of safe sequential decision-making by proposing a novel framework for robust regret minimization. The theoretical insights and practical algorithm are well-supported by empirical results, and the work addresses an important gap in the literature. However, the clarity of presentation and broader comparisons with state-of-the-art methods could be improved. I recommend acceptance, provided the authors address these concerns in the final version.
Pros:
- Novel and impactful approach to safe policy improvement.
- Strong theoretical analysis and practical algorithm.
- Solid empirical validation in diverse domains.
Cons:
- Dense presentation; clarity could be improved.
- Limited baseline comparisons.
- Scalability and practical limitations not fully addressed.