The paper presents a significant advancement in understanding learning dynamics in repeated games, focusing on algorithms satisfying the Low Approximate Regret property. The authors demonstrate that these algorithms achieve fast convergence to approximate optimality in smooth games, even under realistic feedback settings such as realized and bandit feedback. The work improves upon prior results, particularly Syrgkanis et al. [28], by broadening the class of applicable algorithms, reducing the dependence on unrealistic feedback assumptions, and achieving faster convergence rates.
Strengths:
1. Novelty and Scope: The introduction of the Low Approximate Regret property is a key innovation. It generalizes previous regret-based approaches and applies to a broader class of algorithms, including the widely used Hedge algorithm and its variants. This makes the results more practical and widely applicable.
2. Improved Feedback Assumptions: Unlike prior work that relies on expected feedback, the paper demonstrates convergence using realized feedback, which is more realistic in many game-theoretic settings. This is a significant step forward in bridging theory and practice.
3. Convergence Speed: The paper improves the convergence rate by a factor of \(n\) (number of players) compared to [28], achieving \(O(n/T)\) convergence with only a small loss in approximation. This is a notable improvement for large-scale games.
4. Bandit Feedback and Dynamic Settings: The extension of results to bandit feedback and dynamic population games is impressive. The proposed bandit algorithm achieves competitive regret bounds, and the dynamic population analysis strengthens prior results by accommodating higher turnover rates.
5. Clarity and Rigor: The theoretical results are well-supported by detailed proofs, and the paper provides a clear comparison with prior work, highlighting its contributions.
Weaknesses:
1. Experimental Validation: While the theoretical contributions are strong, the paper lacks empirical validation. Simulations or real-world experiments would strengthen the claims and demonstrate practical applicability.
2. Complexity of Bandit Algorithm: The proposed bandit algorithm, while efficient, may still face scalability challenges in high-dimensional settings. A discussion of computational overhead would be beneficial.
3. Limited Discussion of Limitations: The paper does not explicitly discuss potential limitations, such as the dependence on smoothness parameters or the impact of approximation factors (\(\epsilon\)) on practical performance.
Arguments for Acceptance:
- The paper makes a substantial theoretical contribution by generalizing regret-based learning dynamics and improving convergence rates.
- It addresses practical concerns by relaxing feedback assumptions and extending results to dynamic and bandit settings.
- The work is well-positioned within the literature, offering clear advancements over prior results.
Arguments Against Acceptance:
- The lack of experimental validation limits the immediate applicability of the results.
- Some practical aspects, such as computational complexity and scalability, are not thoroughly addressed.
Recommendation:
I recommend acceptance of this paper, as its theoretical contributions significantly advance the state of the art in learning dynamics for repeated games. However, the authors are encouraged to include empirical results and discuss practical limitations in a future revision.