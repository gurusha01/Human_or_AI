This paper introduces a novel algorithm, Riemannian Relaxation (RR), for manifold learning that aims to produce low-distortion embeddings by directly optimizing the push-forward Riemannian metric. Unlike traditional methods, which often rely on heuristic loss functions such as pairwise distances or local reconstruction errors, RR minimizes a distortion loss derived from the deviation of the embedding's metric from isometry. The algorithm supports embeddings in dimensions \(s \geq d\), where \(d\) is the intrinsic dimension of the data, addressing a key limitation of many existing methods that restrict embeddings to \(s = d\).
Strengths:
1. Novelty and Theoretical Rigor: The paper presents a significant departure from existing manifold learning approaches by directly optimizing the Riemannian metric. This is a theoretically sound and innovative contribution that advances the state of the art.
2. Flexibility in Embedding Dimensions: The ability to embed in dimensions \(s > d\) is a major improvement over traditional algorithms like Isomap and LLE, which are limited to \(s = d\). This flexibility is particularly useful for real-world datasets where isometric embeddings in \(d\) dimensions may not exist.
3. Experimental Validation: The experiments convincingly demonstrate the algorithm's ability to recover low-distortion embeddings, even in challenging scenarios such as noisy data or intrinsically curved manifolds. The comparison with other methods (e.g., Isomap, HLLE, MVU) highlights RR's superior performance in reducing distortion.
4. Scalability: The extension to large datasets via the PCS-RR variant is well-motivated and practical, leveraging subsampling and principal curves to handle noisy or high-dimensional data efficiently.
5. Real-World Application: The application to the SDSS galaxy spectra dataset demonstrates the algorithm's utility in uncovering meaningful geometric structures in complex, high-dimensional data.
Weaknesses:
1. Intrinsic Dimension Estimation: The algorithm requires the intrinsic dimension \(d\) as input, but the paper does not provide a robust method for estimating \(d\). While this is a common assumption in manifold learning, it limits the algorithm's usability in fully unsupervised settings.
2. Non-Convex Optimization: The loss function is non-convex, and while the paper discusses its properties, the optimization process may be sensitive to initialization and hyperparameters. This could lead to inconsistent results across datasets.
3. Computational Complexity: Although the paper claims computational competitiveness, the gradient computation involves eigenvalue decompositions, which may become expensive for very large datasets or high embedding dimensions.
4. Limited Comparison Metrics: The evaluation primarily focuses on geometric distortion and pairwise distance errors. Broader metrics, such as interpretability or downstream task performance, could provide a more comprehensive assessment of the embeddings.
Pro and Con Arguments for Acceptance:
Pros:
- The paper introduces a theoretically grounded and novel approach to manifold learning.
- It addresses a key limitation of existing methods by supporting embeddings in \(s > d\).
- Experimental results are compelling, with clear improvements over baseline methods.
Cons:
- The reliance on a known intrinsic dimension \(d\) limits its applicability in fully unsupervised scenarios.
- The optimization process may be computationally intensive and sensitive to initialization.
Recommendation:
Overall, this paper makes a strong contribution to the field of manifold learning by introducing a novel algorithm with theoretical and practical advantages. While there are some limitations, particularly regarding intrinsic dimension estimation and optimization complexity, these do not detract significantly from the paper's value. I recommend acceptance, with minor revisions to address the intrinsic dimension estimation and to provide more clarity on computational costs.