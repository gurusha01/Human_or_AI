The paper presents a significant advancement in optimization for convex-concave saddle-point problems by extending stochastic variance reduction methods (e.g., SVRG and SAGA) to this domain. The authors provide the first linearly convergent algorithms for large-scale saddle-point problems, addressing challenges such as non-separability of losses/regularizers and the need for non-uniform sampling. The paper also demonstrates how these methods can be accelerated using the "catalyst" framework, yielding algorithms that outperform existing batch and stochastic methods both theoretically and empirically.
Strengths:
1. Novelty and Significance: The work addresses a critical gap in optimization for saddle-point problems, which are pervasive in machine learning applications such as robust optimization, structured prediction, and unsupervised learning. The extension of variance reduction methods to this domain is novel and impactful, as it enables faster convergence for problems that were previously intractable with existing methods.
2. Theoretical Contributions: The authors provide rigorous convergence proofs using monotone operator theory, which broadens the applicability of the proposed algorithms beyond saddle-point problems to variational inequalities. This is a notable theoretical contribution.
3. Practical Relevance: The paper emphasizes practical considerations, such as the importance of non-uniform sampling and efficient computation of proximal operators. The experimental results validate the theoretical claims and demonstrate the superiority of the proposed methods over existing approaches.
4. Clarity of Contributions: The paper clearly outlines its contributions, including the extension of SVRG/SAGA, the use of monotone operators for analysis, and the acceleration via the catalyst framework. These are well-supported by both theoretical analysis and empirical results.
Weaknesses:
1. Assumptions on Strong Convexity-Concavity: The algorithms rely on knowing the strong convexity-concavity constants, which may not always be practical. While the authors acknowledge this and suggest future work on adaptivity, this limitation could hinder real-world applicability.
2. Complexity of Presentation: While the paper is mathematically rigorous, some sections (e.g., the monotone operator analysis) are dense and may be challenging for readers unfamiliar with the underlying theory. A more intuitive explanation or additional diagrams could improve accessibility.
3. Limited Discussion of Limitations: Although the paper acknowledges the need for strong convexity-concavity, it does not explore other potential limitations, such as scalability to extremely high-dimensional problems or sensitivity to hyperparameter tuning (e.g., step sizes, sampling probabilities).
Arguments for Acceptance:
- The paper makes substantial theoretical and practical contributions to the field of optimization, particularly for saddle-point problems.
- The proposed methods are rigorously analyzed and empirically validated, demonstrating clear advantages over existing approaches.
- The work is highly relevant to the NeurIPS community, given the ubiquity of saddle-point problems in machine learning.
Arguments Against Acceptance:
- The reliance on strong convexity-concavity assumptions could limit the applicability of the methods in practice.
- The dense presentation may reduce accessibility for a broader audience.
Recommendation:
I recommend acceptance of this paper. Its contributions are both novel and significant, and it advances the state of the art in optimization for saddle-point problems. While there are some limitations, they are outweighed by the strengths of the work. The paper would benefit from a more detailed discussion of practical challenges and potential extensions to address the reliance on strong convexity-concavity.