This paper presents a novel convex relaxation framework for two-layer conditional models with latent structured representations, addressing key challenges in unsupervised learning of structured predictors. The authors propose a method that jointly infers latent structures and optimizes model parameters, leveraging maximum a-posteriori (MAP) inference to improve computational tractability. The framework is demonstrated on two applications—graph matching and structured inpainting—using total unimodularity constraints. Experimental results show promising performance compared to state-of-the-art methods.
Strengths:
1. Novelty and Contribution: The paper provides a significant advancement by introducing a convex relaxation for bi-level optimization in two-layer models, which is a challenging problem in structured prediction. The approach is novel and well-motivated, addressing limitations of prior methods like CRF-AE and max-margin estimation.
2. Theoretical Rigor: The authors rigorously derive the convex relaxation using semi-definite programming (SDP) and provide theoretical guarantees, such as the rank characterization of extreme points in the feasible region. This adds credibility to the proposed method.
3. Practical Applicability: The framework is applied to two distinct and relevant machine learning problems—transliteration and image inpainting—demonstrating its flexibility and practical utility. The results show that the proposed method outperforms existing approaches in both tasks.
4. Clarity of Assumptions: The authors clearly outline the assumptions (e.g., PO-tractability of the latent structure) and limitations of their method, which is commendable.
5. Experimental Validation: The experiments are well-designed, with comparisons to state-of-the-art methods (e.g., CRF-AE, Local). Metrics like MRR and reconstruction error are appropriate for the tasks, and results convincingly demonstrate the superiority of the proposed approach.
Weaknesses:
1. Complexity of Presentation: While the theoretical contributions are strong, the paper is dense and difficult to follow, especially for readers unfamiliar with convex optimization or structured prediction. Simplifying some derivations or providing more intuitive explanations would improve accessibility.
2. Scalability Concerns: The reliance on SDP relaxation may limit scalability to larger datasets or more complex structures. While the authors briefly discuss optimization techniques, further exploration of computational efficiency is warranted.
3. Limited Applications: Although the method is applied to two tasks, the generalizability to other domains or more complex latent structures (e.g., submodular functions) is not fully explored.
4. Empirical Comparisons: The experiments lack comparisons with other recent methods in structured prediction beyond CRF-AE and Local. Including additional baselines would strengthen the empirical evaluation.
Suggestions for Improvement:
1. Simplify the presentation of theoretical results and provide more intuition behind the convex relaxation and SDP formulation.
2. Discuss potential extensions to handle larger datasets or more complex latent structures, such as those involving submodularity.
3. Include additional baselines in the experiments to provide a more comprehensive evaluation.
4. Explore the impact of hyperparameters (e.g., σ) and provide guidelines for tuning them in practice.
Recommendation:
Overall, this paper makes a strong theoretical and empirical contribution to unsupervised learning of structured predictors. Despite some concerns about complexity and scalability, the novelty and rigor of the approach justify its acceptance. I recommend acceptance with minor revisions, focusing on improving clarity and expanding empirical comparisons. 
Pro: Novel convex relaxation for structured latent models, strong theoretical foundation, promising empirical results.  
Con: Dense presentation, scalability concerns, limited baselines.