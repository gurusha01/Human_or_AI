This paper investigates the support recovery guarantees of underdetermined sparse regression using the `1-norm as a regularizer, focusing on non-smooth loss functions (`1 and `∞ losses) and contrasting them with the more commonly studied `2 loss. The authors extend existing theoretical results for the smooth `2 case to these non-smooth cases, deriving sharp conditions for support stability under small additive noise. They also introduce the concept of "extended support" to characterize instability scenarios and validate their theoretical findings with numerical experiments in compressed sensing. The paper's main contribution is Theorem 1, which provides a rigorous framework for understanding support stability and instability for non-smooth loss functions.
Strengths
1. Novelty: The paper addresses a significant gap in the literature by extending support stability analysis to non-smooth loss functions (`1 and `∞). This is a meaningful contribution, as these loss functions are frequently used in practice but lack theoretical guarantees.
2. Theoretical Rigor: The authors provide a detailed and mathematically robust analysis, including proofs for their main results. The introduction of the "extended support" concept is particularly insightful for understanding instability in sparse recovery.
3. Practical Relevance: The paper's focus on compressed sensing and its numerical experiments demonstrate the practical implications of the theoretical findings. The comparison of support stability across different loss functions (`1, `2, `∞) provides actionable insights for practitioners.
4. Clarity of Results: The numerical experiments are well-designed and effectively illustrate the theoretical claims, particularly the trade-offs in support stability across different loss functions.
Weaknesses
1. Limited Scope: While the paper makes significant progress for `1 and `∞ losses, it does not extend its theoretical results to the general case of `α-norms for α ∈ (1, ∞), despite acknowledging the potential for such generalizations. This limits the broader applicability of the work.
2. Assumptions: The results rely on specific assumptions, such as the identifiability of the sparse vector and small noise levels. The paper does not address scenarios with larger noise or more complex noise models, which are common in real-world applications.
3. Clarity: While the theoretical sections are rigorous, they may be challenging for readers unfamiliar with convex optimization or dual certificates. The presentation could benefit from additional intuition or simplified explanations for key concepts like dual certificates and restricted injectivity conditions.
4. Comparative Analysis: Although the paper compares `1, `2, and `∞ losses, it does not benchmark its findings against alternative sparse recovery methods, such as those using non-convex penalties or other regularization techniques.
Arguments for Acceptance
- The paper addresses an important and underexplored problem, providing novel theoretical insights with practical implications.
- The results are rigorous and supported by well-designed numerical experiments.
- The work aligns with the conference's focus on advancing the theoretical and practical understanding of machine learning and optimization.
Arguments Against Acceptance
- The scope is somewhat narrow, and the generalization to other loss functions (`α for α ∈ (1, ∞)) is left as future work.
- The paper's reliance on specific assumptions (e.g., small noise) limits its applicability.
- The presentation could be more accessible to a broader audience.
Recommendation
I recommend acceptance with minor revisions. The paper makes a valuable contribution to the field of sparse recovery and optimization, but it would benefit from clarifying some theoretical concepts and discussing broader implications or extensions. Specifically, the authors should consider adding more intuition for their theoretical results and addressing the practical limitations of their assumptions.