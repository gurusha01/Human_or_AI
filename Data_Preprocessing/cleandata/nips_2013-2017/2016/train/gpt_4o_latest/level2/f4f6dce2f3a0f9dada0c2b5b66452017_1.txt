The paper introduces SPALS, a novel method for performing sparse alternating least squares (ALS) for tensor CANDECOMP/PARAFAC (CP) decomposition, which is a computationally intensive task in large-scale data analytics. The authors propose sampling the Khatri-Rao product, a key computational bottleneck in ALS, using statistical leverage scores. This enables sublinear time per iteration and provable approximation guarantees. The empirical results demonstrate significant speedups over existing deterministic and randomized methods, particularly on large-scale datasets like the Amazon review tensor.
Strengths:
1. Novelty and Significance: The paper presents the first sublinear-per-iteration ALS algorithm for tensor CP decomposition with provable guarantees. This is a significant contribution to scalable tensor analytics, addressing a critical bottleneck in big data applications.
2. Theoretical Contributions: The authors establish a connection between the statistical leverage scores of the Khatri-Rao product and the input matrices, enabling efficient sampling. Their theoretical results are robust and extend to other tensor-related tasks, such as stochastic gradient descent and higher-order singular value decomposition.
3. Empirical Validation: The experiments convincingly demonstrate the practical utility of SPALS. On a massive Amazon review tensor, SPALS achieves comparable accuracy to deterministic ALS while reducing runtime by orders of magnitude. The results on synthetic data further validate its efficiency and accuracy.
4. Clarity of Contributions: The paper clearly outlines its contributions, including theoretical insights, algorithmic design, and empirical evaluation. The proposed sampling scheme is well-motivated and explained in detail.
Weaknesses:
1. Limited Discussion of Limitations: While the paper mentions that SPALS is most effective for low-rank tensors, it does not explicitly discuss potential limitations, such as its performance on high-rank tensors or tensors with extreme sparsity patterns.
2. Comparison with Related Work: Although the paper compares SPALS to a sketching-based method, the discussion of other recent randomized tensor decomposition techniques is somewhat limited. For example, a deeper comparison with methods that use global randomization or other sampling strategies would strengthen the evaluation.
3. Reproducibility: While the authors provide a link to the source code, the paper lacks detailed information on hyperparameter tuning and experimental setup, which could hinder reproducibility.
Suggestions for Improvement:
1. Include a more comprehensive discussion of the limitations of SPALS, particularly in scenarios where its assumptions (e.g., low-rank structure) may not hold.
2. Expand the related work section to include a broader comparison with other randomized tensor decomposition methods, citing recent advances in the field.
3. Provide additional details on the experimental setup, such as hardware specifications, parameter settings, and preprocessing steps, to facilitate reproducibility.
Recommendation:
I recommend acceptance of this paper, as it makes a significant theoretical and practical contribution to scalable tensor decomposition. The proposed SPALS algorithm is both novel and impactful, with strong empirical results to support its claims. Addressing the aforementioned weaknesses in the final version would further enhance the paper's quality.