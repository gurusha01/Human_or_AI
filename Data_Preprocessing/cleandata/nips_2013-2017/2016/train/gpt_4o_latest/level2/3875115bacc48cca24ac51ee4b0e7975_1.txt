This paper addresses two fundamental issues in the population likelihood function of Gaussian Mixture Models (GMMs) with M â‰¥ 3 components. The authors provide a negative resolution to Srebro's [2007] conjecture, demonstrating the existence of arbitrarily bad local maxima in the population log-likelihood function, even for well-separated, equally weighted, spherical Gaussians. Additionally, they show that the Expectation-Maximization (EM) algorithm and its first-order variant, when initialized randomly, converge to suboptimal critical points with high probability, highlighting the necessity of careful initialization.
Strengths:
1. Novelty and Significance: The paper resolves a long-standing open question by proving the existence of bad local maxima in the population likelihood of GMMs, challenging prior assumptions about the well-behaved nature of the likelihood surface. This result has significant implications for the theoretical understanding of GMMs and the limitations of EM-based optimization.
2. Rigorous Theoretical Contributions: The authors introduce new techniques to analyze the structure of the population log-likelihood and provide detailed proofs for their claims. The recursive construction of GMMs with bad local maxima is particularly insightful.
3. Practical Relevance: The findings underscore the limitations of widely used algorithms like EM and gradient ascent, emphasizing the importance of initialization strategies in practice. This has direct implications for practitioners working with GMMs in real-world applications.
4. Clarity of Results: The paper clearly articulates its main contributions, particularly the distinction between bad local maxima and strict saddle points, and provides a strong theoretical foundation for its claims.
Weaknesses:
1. Limited Scope of Experimental Validation: While the theoretical results are compelling, the paper lacks empirical demonstrations to corroborate the practical implications of the findings. For instance, experiments comparing the performance of EM with random initialization versus alternative initialization strategies would strengthen the paper.
2. Focus on Idealized Settings: The analysis is restricted to uniformly weighted, spherical, well-separated Gaussians. While this simplifies the theoretical treatment, it limits the generalizability of the results to more complex or realistic GMM scenarios.
3. Open Questions Remain: The paper leaves unresolved whether bad local maxima exist for mixtures with fewer than three components, and why EM performs well empirically despite its theoretical shortcomings. Addressing these questions would further enhance the paper's impact.
Evaluation:
- Quality: The paper is technically sound, with well-supported claims and rigorous proofs. The results are complete and provide a strong theoretical contribution to the field.
- Clarity: The paper is well-written and organized, though some technical sections may be challenging for non-experts.
- Originality: The work is highly original, resolving an open problem and providing new insights into the behavior of EM and GMMs.
- Significance: The results are important for both theoretical and practical advancements in non-convex optimization and mixture model estimation.
Recommendation:
I recommend acceptance of this paper, as it provides a significant theoretical contribution to the understanding of GMMs and the limitations of EM-based algorithms. However, I encourage the authors to include empirical evaluations in future work to validate the practical implications of their findings and address the open questions raised.