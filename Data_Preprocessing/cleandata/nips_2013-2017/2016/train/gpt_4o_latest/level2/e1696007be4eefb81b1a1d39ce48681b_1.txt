The paper presents a novel approach to efficiently estimate the coefficients of generalized linear models (GLMs) in large-scale settings, where the number of observations \(n\) far exceeds the number of predictors \(p\) (\(n \gg p\)). The authors propose the Scaled Least Squares (SLS) estimator, which leverages a proportionality relationship between GLM coefficients and ordinary least squares (OLS) coefficients. This relationship is rigorously analyzed and extended to non-Gaussian predictors. The proposed algorithm achieves the accuracy of the maximum likelihood estimator (MLE) while being computationally cheaper by at least a factor of \(O(p)\). The paper demonstrates the theoretical guarantees of the method, provides convergence analysis, and validates its performance through extensive experiments on synthetic and real-world datasets.
Strengths
1. Novelty and Originality: The paper introduces a novel computational approach by exploiting a proportionality relationship between GLM and OLS coefficients, which has been underexplored in large-scale settings. The extension of this relationship to non-Gaussian predictors is a significant contribution.
2. Efficiency: The SLS algorithm is computationally efficient, with an \(O(n)\) per-iteration cost for the second step, making it highly suitable for large-scale datasets. The cubic convergence rate of the root-finding step is another strength.
3. Theoretical Rigor: The paper provides strong theoretical foundations, including error bounds and convergence guarantees, which are well-supported by the use of zero-bias transformations and sub-Gaussian assumptions.
4. Empirical Validation: The experiments convincingly demonstrate that SLS achieves similar accuracy to MLE while being significantly faster. The comparison with widely used optimization algorithms (e.g., Newton-Raphson, BFGS, and gradient descent) is thorough and highlights the computational advantages of SLS.
5. Clarity and Structure: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup.
Weaknesses
1. Limited Scope of Regularization: While the paper briefly discusses the extension of the proportionality relationship to regularized GLMs (e.g., ridge regression), this aspect is not explored in depth. Regularization is critical in high-dimensional settings, and further analysis would strengthen the paper.
2. Practical Considerations: The initialization of the proportionality constant \(c\) in the root-finding step is briefly justified, but its robustness across diverse datasets and real-world scenarios could be further elaborated.
3. Generalization to Other Models: While the authors mention potential extensions to other optimization problems (e.g., support vector machines), these ideas are not pursued, leaving room for further exploration.
Arguments for Acceptance
- The paper addresses a significant challenge in large-scale GLM estimation with a novel and computationally efficient solution.
- Theoretical contributions are robust and well-supported by empirical results.
- The work has practical implications for large-scale machine learning tasks, making it relevant to the NIPS audience.
Arguments Against Acceptance
- The limited exploration of regularization and its implications for high-dimensional problems could be a drawback for some applications.
- The generalization to other models is only mentioned as future work, which may limit the immediate impact of the paper.
Recommendation
I recommend acceptance of this paper. Its contributions to computational efficiency, theoretical rigor, and practical applicability make it a valuable addition to the field. However, the authors are encouraged to expand on regularization and practical robustness in future revisions.