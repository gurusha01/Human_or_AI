The paper proposes a novel training approach for Boltzmann Machines (BMs) by minimizing the Wasserstein distance, rather than the conventional Kullback-Leibler (KL) divergence, between the model's distribution and the empirical data distribution. The authors derive a gradient for the Wasserstein distance and demonstrate its practical utility in tasks such as data completion and denoising. The key claim is that incorporating a meaningful metric into the training objective leads to generative models with different statistical properties, which are particularly advantageous in metric-sensitive tasks.
Strengths:
1. Novelty and Originality: The work introduces a new objective for training BMs, leveraging the Wasserstein distance, which directly incorporates the metric between observations. This is a significant departure from traditional KL-based training and provides a fresh perspective on density estimation.
2. Theoretical Rigor: The paper is technically sound, with a detailed derivation of the Wasserstein gradient and its application to Restricted Boltzmann Machines (RBMs). The use of the Sinkhorn algorithm for efficient computation of the Wasserstein distance is well-motivated and aligns with recent advances in the field.
3. Practical Relevance: The experiments on data completion and denoising demonstrate the practical utility of the proposed approach. The Wasserstein-trained RBMs (RBM-W) consistently outperform standard RBMs in metric-sensitive tasks, showcasing the method's potential for real-world applications.
4. Comprehensive Evaluation: The paper provides a thorough analysis of the impact of hyperparameters, the shrinkage effect, and the bias-variance tradeoff, offering valuable insights into the behavior of Wasserstein-trained models.
Weaknesses:
1. Limited Scope of Experiments: While the experiments are compelling, they are restricted to relatively small datasets (e.g., MNIST-small, UCI PLANTS). The scalability of the proposed approach to larger and more complex datasets is not thoroughly explored.
2. Cluster Bias: The Wasserstein-trained RBMs tend to produce distributions biased toward dense clusters, as noted in the PCA analysis. While this behavior is advantageous for certain tasks, it may limit the model's ability to capture the full diversity of the data distribution.
3. Computational Complexity: The reliance on the Sinkhorn algorithm and the need for pretraining with KL divergence suggest that the approach may be computationally expensive, particularly for high-dimensional data or large-scale applications.
4. Limited Discussion of Limitations: The paper briefly acknowledges stability issues and the need for KL regularization but does not delve deeply into the limitations of the proposed method or its potential drawbacks in broader contexts.
Suggestions for Improvement:
1. Extend the experimental evaluation to larger and more diverse datasets to demonstrate scalability and generalizability.
2. Investigate techniques to mitigate the cluster bias observed in Wasserstein-trained models, potentially by incorporating additional regularization terms.
3. Provide a more detailed comparison of computational costs between the proposed method and traditional KL-based training.
4. Include a more explicit discussion of limitations and potential failure cases to guide future research.
Recommendation:
This paper presents a novel and well-executed contribution to the field of generative modeling, with strong theoretical underpinnings and practical implications. While there are some limitations, the strengths outweigh the weaknesses, and the work is likely to stimulate further research on metric-based training objectives. I recommend acceptance, with the expectation that the authors address the scalability and computational concerns in future work.