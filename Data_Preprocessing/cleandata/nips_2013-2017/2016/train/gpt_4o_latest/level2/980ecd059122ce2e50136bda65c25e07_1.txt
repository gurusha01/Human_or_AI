The paper addresses the critical issue of neural network robustness against adversarial examples by proposing novel metrics and an efficient algorithm for their estimation. The authors introduce two robustness metrics—adversarial frequency and adversarial severity—that provide complementary insights into how often and how severely a neural network fails under adversarial perturbations. The key contribution lies in encoding pointwise robustness as a linear program, enabling tractable approximations and iterative optimization for faster computation. The proposed approach is validated through experiments on MNIST and CIFAR-10 datasets, demonstrating its superiority over existing methods like L-BFGS-B in estimating robustness metrics and identifying overfitting to specific adversarial example generation algorithms.
Strengths
1. Novel Metrics and Formalization: The paper formalizes robustness in a principled manner and introduces adversarial frequency and severity as metrics, which are intuitive and practical for evaluating robustness comprehensively.
2. Algorithmic Innovation: The use of convex restriction and iterative constraint solving to approximate robustness metrics is both novel and effective, achieving significant speed-ups over naive approaches.
3. Experimental Rigor: The experiments are thorough, comparing the proposed algorithm against a strong baseline (L-BFGS-B) and demonstrating its advantages in terms of accuracy and robustness estimation. The results are well-presented, with clear visualizations and quantitative metrics.
4. Practical Relevance: The paper highlights the issue of overfitting to specific adversarial generation algorithms, which is a critical insight for the field. The proposed metrics and algorithm are shown to generalize better across different adversarial scenarios.
5. Scalability: The application of the method to larger networks like NiN for CIFAR-10 demonstrates its scalability, even though the robustness improvements for such networks remain limited.
Weaknesses
1. Limited Robustness Improvement: While the proposed algorithm improves robustness for MNIST, its impact on larger networks like NiN is marginal. This suggests that the method may not scale effectively to more complex architectures or datasets.
2. Comparative Analysis: The paper does not compare its approach to more recent adversarial training techniques beyond L-BFGS-B, such as PGD-based adversarial training, which limits the scope of its claims.
3. Reproducibility Concerns: While the methodology is detailed, the paper does not provide sufficient implementation details or code for reproducibility, especially for the iterative constraint-solving optimization.
4. Focus on L∞ Norm: The robustness metrics and experiments are restricted to the L∞ norm, which, while common, may not capture robustness comprehensively across other perturbation norms (e.g., L2 or L1).
Suggestions for Improvement
1. Extend the evaluation to include comparisons with more recent adversarial training methods to position the work within the broader literature.
2. Explore robustness metrics and adversarial examples under alternative norms to generalize the findings.
3. Provide implementation details or open-source code to facilitate reproducibility and adoption by the community.
4. Investigate methods to improve robustness for large-scale networks like NiN, potentially by integrating the proposed metrics with advanced adversarial training techniques.
Recommendation
The paper makes a significant contribution to the field by proposing novel robustness metrics and an efficient algorithm for their estimation. While there are limitations in scalability and comparative analysis, the work is well-motivated, technically sound, and experimentally validated. I recommend acceptance, with the suggestion that the authors address the outlined weaknesses in future iterations. 
Pro Arguments:
- Novel and practical robustness metrics.
- Strong experimental validation on MNIST.
- Insightful discussion on overfitting to adversarial examples.
Con Arguments:
- Limited robustness improvement for complex networks.
- Lack of comparison with more recent adversarial training methods. 
Final Score: 7/10