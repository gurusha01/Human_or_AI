The paper presents a novel supervised metric learning approach, Supervised Word Mover's Distance (S-WMD), which extends the unsupervised Word Mover's Distance (WMD) by incorporating supervision to improve document classification. The authors propose an efficient training method that minimizes leave-one-out classification error by learning an affine transformation of the word embedding space and a word-importance weight vector. They address the computational challenge of WMD by leveraging the Sinkhorn distance, a relaxed version of the optimal transport problem, to achieve efficient gradient updates. The method is evaluated on eight real-world text classification datasets, where it consistently outperforms 26 competitive baselines, including both unsupervised and supervised methods.
Strengths:
1. Novelty and Contribution: The paper makes a significant contribution by introducing a supervised extension to WMD, which was previously unsupervised. The combination of learning a linear transformation and word-importance weights is innovative and adds flexibility to the metric.
2. Technical Soundness: The proposed method is well-grounded in theory, and the authors provide detailed derivations for the gradient computation and optimization. The use of the Sinkhorn distance to approximate the WMD gradient is a clever and practical solution to the computational bottleneck.
3. Empirical Results: The experimental evaluation is thorough, with comparisons against a wide range of baselines across diverse datasets. The consistent improvement in kNN classification error demonstrates the effectiveness of S-WMD.
4. Efficiency: The authors address computational challenges effectively, showing that S-WMD can be trained efficiently with batch gradient descent and fast approximations.
5. Visualization: The t-SNE visualizations and word importance analysis provide intuitive insights into the learned metric and its impact on classification.
Weaknesses:
1. Initialization Sensitivity: The method relies heavily on a good initialization of the metric (via S-WCD), which may not always generalize well across datasets. Poor initialization could explain suboptimal performance on certain datasets like CLASSIC.
2. Dataset-Specific Performance: While S-WMD performs well overall, it does not consistently outperform all baselines on every dataset. For example, it achieves only the fourth-lowest error on BBCSPORT and CLASSIC, suggesting potential limitations in handling long documents or specific data distributions.
3. Scalability: Although the authors improve computational efficiency, the quadratic complexity with respect to document size may still pose challenges for very large datasets or corpora with high vocabulary sizes.
4. Clarity of Presentation: The paper is dense with technical details, which may make it less accessible to readers unfamiliar with optimal transport or metric learning. Simplifying some explanations or including more intuitive examples could improve clarity.
Recommendation:
I recommend accepting this paper for its strong contributions to supervised metric learning and its demonstrated empirical success. While there are minor concerns regarding initialization sensitivity and scalability, the proposed method is a significant advancement over existing approaches. The authors could further strengthen the paper by discussing potential extensions to address these limitations, such as exploring alternative initialization strategies or adapting the method for larger datasets.
Arguments for Acceptance:
- Novel and impactful contribution to supervised document distance metrics.
- Strong empirical results across diverse datasets.
- Efficient and practical solution to computational challenges.
Arguments Against Acceptance:
- Sensitivity to initialization and dataset-specific performance variations.
- Potential scalability issues for very large datasets.
Overall, the strengths of this work outweigh its weaknesses, and it represents a valuable addition to the field of text classification and metric learning.