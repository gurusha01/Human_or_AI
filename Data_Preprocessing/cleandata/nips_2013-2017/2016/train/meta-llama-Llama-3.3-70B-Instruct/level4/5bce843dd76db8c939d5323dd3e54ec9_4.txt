This paper presents a modified LSTM model that incorporates a time moderator gate, enabling it to handle longer sequence inputs and retain memories for extended periods. The novel time gate, a cyclic function of time, regulates the extent to which cell and hidden states are updated based on the current input or preserved from the previous time step. Empirical results demonstrate the proposed model's superiority over standard LSTM in capturing longer time memories. Enhancing RNN's capacity to capture longer memories and handle longer sequences is a crucial problem. The paper proposes an interesting and effective idea for new gates whose value is computed by the input time, allowing each cell in LSTM to capture memories of varying lengths, as illustrated in figure 2(b), making it suitable for longer sequences and over-sampled sequence inputs. This concept can also be generalized to other gated RNNs, such as GRU. However, I have several concerns: while I agree that the proposed model can handle long sequences, I am not convinced that the designed time gate explicitly addresses event-based or irregularly sampled data. The primary purpose of introducing a time gate, in my view, is to limit hidden state updates to a small fraction of the time. To be convinced of the proposed time gate's effectiveness for irregular or event-based sequences, I would like to see a comparison with a similar model featuring a random time gate, where the time gate kt is a random number with the same distribution but independent of input time t. This random model may also handle long sequences due to its small open ratio. I would like to see whether the proposed model can better handle event-based sequences. Currently, the input time to the gate kt seems to serve only as a random seed to generate the gate value with a mean (r_on) and has no relation to the time step itself. In the experiments, Figure 3(d) shows that the proposed LSTM outperforms baselines in both high-resolution sampling and async sampling settings. However, what is the average sampling rate in the async setting? It appears that even the async setting has more input steps than the standard condition, making it unclear whether the baselines fail due to the longer sequences or other factors. The authors claim that the new LSTM has "an order-of-magnitude fewer computes," but it is unclear whether this refers to the number of training epochs before convergence or the computing time for applying the model to one input sequence. The introduction section suggests that the authors aim to reduce unnecessary computational load and power consumption by only updating a portion of the network's units at each time step. However, this may not be true for the proposed model, as all nodes need to be updated during training due to the "leaky" behavior, and updating only a small portion of states may not lead to significant efficiency gains, especially since neural network operations are typically performed using matrix operations on GPUs. The authors mention that the proposed LSTM requires 5% of updates per neuron, but this may not translate to a 5% reduction in time; the authors should clarify if their models are implemented in a more efficient way.