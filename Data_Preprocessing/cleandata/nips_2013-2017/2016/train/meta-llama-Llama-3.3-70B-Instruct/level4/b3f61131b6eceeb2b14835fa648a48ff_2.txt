This manuscript introduces several novel findings in the realm of decentralized no-regret dynamics within smooth games, building upon the foundational work presented in SALS15 and LST16. The authors propose a novel property, termed "Low Approximate Regret," which serves as a generalization of traditional algorithms. Notable contributions include the exploration of various feedback mechanisms, the bandit scenario, and population games. The significance and interest of these results to the community are evident. The primary contributions, including enhanced convergence bounds and adaptations to different feedback settings, are facilitated by the introduction of the Low Approximate Regret property. This property effectively bounds the discrepancy between the approximate cost incurred and that of a comparator, demonstrating its applicability to several well-established learning algorithms and deriving regret bounds accordingly. A potential drawback of the paper is its dense content, leading to the relegation of detailed derivations to the appendix, with only key points summarized in the main text. While this approach renders the paper more readable and facilitates easier contextualization with related research, it also results in abrupt transitions between diverse settings and topics. Furthermore, the absence of experimental validation to complement the theoretical findings is noteworthy, particularly given the inclusion of such experiments in SALS15 to demonstrate practical applicability. The conclusion is also somewhat abrupt. For clarity, the authors should ensure consistent terminology throughout, specifically regarding the use of "realized feedback" and "full information feedback," which appear to be used interchangeably. In the bandit case, a distinction should be made between the expected Low Approximate Regret as stated in Lemma 4 and the Low Approximate Regret defined in Equation (1) to avoid confusion.