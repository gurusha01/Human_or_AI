This manuscript proposes an extension of state-action visitation counts to reinforcement learning environments characterized by large state spaces, where conventional visitation counts become too sparse to provide meaningful information. The core concept involves leveraging density estimation to obtain pseudo-counts that are analogous to traditional counts. An asymptotic analysis is provided to demonstrate the equivalence of these pseudo-counts. The efficacy of the density-estimation-derived pseudo-count (CTS) is validated by integrating it into two reinforcement learning algorithms (DQN and A3C) to facilitate count-based exploration, with evaluations conducted across a variety of games in the Arcade Learning Environment (ALE). This paper offers a valuable contribution that could enhance numerous deep reinforcement learning algorithms operating in large state spaces. Several aspects of the pseudo-count approach warrant further discussion: (1) Recent exploration algorithms presented at ICML this year utilized model uncertainty for exploration, which appears to be a more straightforward and intuitive approach compared to employing an external density model. Although this is touched upon in the future directions section, a more detailed comparison between pseudo-count exploration and model uncertainty exploration methods would be beneficial, including scenarios where pseudo-counts are preferable. (2) A comparison of the density-based counting scheme to a simpler counting method, such as dividing the screen into large segments and using color indicators, could provide insight into the relative advantages of the proposed approach. (3) In equation (5), should N(x) be revised to N_a(x) to ensure the bonus is dependent on both states and actions, thereby aligning with the intended functionality?