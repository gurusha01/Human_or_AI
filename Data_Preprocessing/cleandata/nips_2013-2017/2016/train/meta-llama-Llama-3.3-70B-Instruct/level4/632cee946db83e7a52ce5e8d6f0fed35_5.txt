This manuscript explores two primary topics: the duality between multi-class Support Vector Machines (SVMs) and multiclass AdaBoost, and the development of a novel dimensionality reduction technique that preserves class discriminability based on this duality. The authors modify the MCBoost criterion to enable multi-class boosting with an arbitrary number of dimensions, overcoming the previous limitation of dimensionality being restricted to the number of classes. This extension allows for a boosting framework with a controllable number of boosting functions. 
The connection between MC-Boost and MV-SVM is noteworthy, and the discussion, including the comparison of predictors and codewords (e.g., Table 1), is clear and engaging. However, it is unclear if the fact that both MC-SVM and MC-Boost aim to maximize the margin is a well-established concept.
The authors report improved results in terms of error rate and mean Average Precision (mAP), with LADDER outperforming MCBoost by 2% in classification accuracy and across all dimensions in terms of error rate (Figure 3). 
Despite these contributions, more details on dimensionality reduction and its integration with Convolutional Neural Networks (CNNs) would be beneficial. This line of work shows promise, even with moderately sized training datasets, as demonstrated in previous studies (e.g., DrLim Hadsell et al., 2006). A more comprehensive literature review could further contextualize these findings.
Moreover, comparing the proposed method's results with those obtained using CNNs, which can learn both features and discriminative projections simultaneously, would be insightful. Although CNNs are highly non-convex and may not guarantee optimal solutions, they often yield better results than convex methods in practice. The current comparison using features from conv5 (Table 2) against Bag-of-Words features does not fully leverage the capabilities of deep learning.
Additionally, the method presented by Simonyan, Vedaldi, and Zisserman (PAMI 2014) is not included in the comparison, which primarily focuses on lower-accuracy methods like PCA, LDA, and their kernel equivalents. 
In summary, while the manuscript presents an interesting approach, the experimental evaluation lacks conviction due to the limited scope of comparisons and the absence of detailed analyses, such as convergence characteristics and sub-analyses of the proposed method. Further experiments are necessary to fully demonstrate the merits of the proposed technique.