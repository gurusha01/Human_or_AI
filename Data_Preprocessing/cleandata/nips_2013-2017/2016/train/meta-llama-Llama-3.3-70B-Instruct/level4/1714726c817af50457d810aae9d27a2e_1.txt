This paper builds upon the ODE interpretation of Nesterov's acceleration introduced by Krichene et al. at NIPS 2015, presenting a heuristic approach to adaptively adjust the weights of the algorithm's iterates, thereby accelerating convergence. Notably, this method guarantees improved performance over traditional fixed-schedule averaging techniques, unlike previous adaptive averaging heuristics. The empirical results demonstrate significant speed-ups across a broad range of functions. The paper is well-written, with a clear and straightforward mathematical argument that is easily accessible to those familiar with Krichene et al.'s work. The empirical findings are also promising, showcasing substantial speed-ups. Although the technical development closely extends the work of Krichene et al., the paper's potential impact is substantial due to the widespread application of Nesterov's acceleration. Often, new methods are compared to Nesterov's acceleration using a fixed weight sequence, even in empirical evaluations. However, this paper highlights that making Nesterov's method adaptive requires only a minor modification, leading to considerable improvements. To further enhance the paper technically, an analysis of the discretized process demonstrating the preservation of the convergence rate would be beneficial. This appears to be a relatively straightforward task. Additionally, exploring potential connections between the adaptive averaging in Nesterov's method and the Conjugate Gradient method for strongly convex quadratic optimization could provide valuable insights.