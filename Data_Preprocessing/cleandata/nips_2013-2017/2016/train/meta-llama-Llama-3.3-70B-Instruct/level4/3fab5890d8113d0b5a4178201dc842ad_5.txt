This paper presents a novel Sparse Access Memory architecture designed for neural networks integrated with memory, such as Neural Turing Machines and Memory Networks, yielding significant runtime and memory efficiency enhancements of up to three orders of magnitude over existing memory-augmented neural networks. Furthermore, this architecture appears to facilitate substantial performance improvements in curriculum learning settings and enables generalization to sequences that are notably longer than those encountered during training, extending from sequences of up to 10,000 in training to 200,000 in testing. This contribution has the potential to be groundbreaking. The field of neural networks augmented with memory, which originated with Neural Turing Machines and Memory Networks, has been actively evolving. While previous research in this area has shown promising results, it lacked the practical applicability that this paper now seems to offer, thanks to its reported efficiency and performance enhancements. The implications of making memory-augmented neural networks practically usable could be profound. However, one aspect that is not thoroughly discussed in the paper is the underlying reason for the significant performance improvements observed in curriculum learning and the ability to predict sequences of lengths far exceeding the training data. Given that runtime and memory efficiency alone cannot account for these improvements, it suggests that other factors are at play. A plausible hypothesis, akin to the role of Sparse Auto-encoders in similar contexts, might be the contribution of the Sparse Memory Access architecture, although the authors' insights, even if preliminary, would be valuable in clarifying this point for readers.