This paper introduces a novel deep learning architecture designed to tackle the unsupervised adaptation problem, where labeled training data is available for a source domain, but only unlabeled data is available for the target domain. The proposed approach combines concepts from deep adaptation networks (as seen in reference 5), residual networks (reference 8), and entropy minimization for unsupervised learning (reference 28) to create a principled and accurate method. The architecture cleverly integrates previously published ideas, including the use of MK-MMD from deep adaptation networks, the incorporation of residual network blocks to relate source and target domain classifiers, and entropy minimization to guide learning in the target domain. This combination enables the authors to outperform state-of-the-art methods on standard benchmarks. The innovative use of residual learning, combined with an entropy minimization learning criterion, allows for the joint learning of source and target classifiers, which appears to be effective in practice. However, despite the novelty of combining residual learning with MK-MMD, the work seems more like an incremental improvement over reference 5 rather than a completely new model, as the authors suggest. The distinction between the use of MK-MMD in this work and its original proposal in reference 5 is not entirely clear, raising questions about what specifically is different, why it is better, and what motivated these changes. This lack of clarity is further compounded by the absence of an experimental comparison between the method proposed in reference 5 and the variant presented here. The claim that the residual learning framework guarantees a small residual part is also not fully convincing, as this seems more like an experimental observation in a different context rather than a guaranteed outcome. The experimental results provided for the benchmarks from reference 5 do show convincing improvements over the state of the art and highlight the complementarity of the MMD criterion and the residual learning idea. Nonetheless, a direct comparison with the same MMD use as in reference 5 would be beneficial to assess the efficiency of this variant. There are discrepancies in the results presented for methods like DAN, TCA, and GFK compared to those reported in reference 5, which warrant explanation. For instance, the average accuracy of DAN on the Office-31 dataset is reported as 72.9% in reference 5 but as 70.0% in this paper. Additionally, the significantly better performance of TCA and GFK in this work compared to reference 5 could be due to the use of deep features as input, whereas reference 5 used raw features. More precision on these differences is needed. Overall, the paper is well-written and proposes a valuable addition to existing deep architectures for unsupervised adaptation, albeit with some incremental elements. The use of residual learning is a nice contribution, even if the work builds upon previous foundations.