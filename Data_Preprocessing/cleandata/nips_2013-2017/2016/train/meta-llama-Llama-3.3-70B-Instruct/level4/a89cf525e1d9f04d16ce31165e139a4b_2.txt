This paper presents an extension of Gaussian Information Bottleneck (IB) to accommodate sparse variables, which is particularly suitable for data exhibiting sparse or nonlinear manifold structures. The algorithm's performance is showcased on both toy image patch data and handwritten digits, demonstrating its ability to uncover relevant data structures. Overall, the paper is intriguing, but its impact would be enhanced by the inclusion of applications to real-world data, such as natural images, rather than relying solely on toy image patch data. A sentence in the discussion section is perplexing, stating that the encoding model p(r|x) is conditioned on a separate input set X, distinct from the image patches, implying that the solutions depend on the relationship between X and Y, rather than just image statistics. However, it is unclear what is meant by 'a separate set of inputs, X,' as it appears that Y represents the image being reconstructed, making this seem akin to traditional sparse coding. Several specific points warrant attention: the claim that equation 1 is being maximized (Line 39) seems incorrect, as the minimization of this equation is consistent with Tishby's paper (eq. 15), and this error is repeated on Line 52. The use of a toy example in Figure 1 would be more convincing if natural images were utilized instead. Furthermore, the occlusion example using a kernel could also be achieved with a generative model, prompting the question of how this method compares to alternative approaches.