This manuscript presents a novel framework for 3D object generation using generative adversarial networks (GANs), referred to as Volumetric Adversarial Networks (VAN). To accommodate the 3D object generation task, the authors employ a 3D volumetric convolutional architecture for both the generator and discriminator components of the GAN. Furthermore, the VAN is combined with a variational autoencoder (VAE) to enable the synthesis of 3D shapes from 2D query images, resulting in the VAE-VAN framework. The authors provide a comprehensive set of experimental results to evaluate their proposed method, including: (1) a visual comparison of generated 3D shapes with prior work, (2) an assessment of shape classification performance using unsupervised features extracted from the discriminator network, benchmarked against several prior works, (3) a visualization of generated 3D objects from color images using the VAE-VAN framework, and (4) a quantitative comparison of the generated 3D objects from color images using the VAE-VAN framework with several existing works. Additionally, the manuscript demonstrates the capability of shape arithmetic operations enabled by the VAN and visualizes the neurons in the discriminator. While the paper's novelty lies in its application of GAN and DCGAN concepts to 3D object generation, its primary contribution is the empirical demonstration that desirable properties of GANs and DCGANs for 2D image generation, such as novel object generation and robust feature extraction, also apply to 3D object generation. In terms of potential impact, the paper shows that unsupervised features from the discriminator network are highly discriminative, comparable to state-of-the-art supervised methods. However, the advantage of the proposed unsupervised feature learning approach is unclear, as it requires labeled samples to train a classification layer, which does not reduce the number of samples needed to achieve a target performance. A more convincing demonstration would involve achieving comparable or superior performance with fewer labeled examples using the proposed unsupervised features. The manuscript is well-organized and easy to follow, with clear explanations that facilitate understanding of the proposed methodology.