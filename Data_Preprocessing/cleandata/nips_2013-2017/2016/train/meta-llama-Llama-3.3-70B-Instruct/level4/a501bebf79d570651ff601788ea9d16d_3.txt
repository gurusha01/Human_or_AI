This paper presents a novel approach to back-propagation through time for training recurrent neural networks, enabling its implementation on computers with significantly limited operating memory compared to the traditional method of temporal unfolding. By leveraging dynamic programming, the method determines the optimal strategy for storing internal states during network unfolding, examining various storage schemes that include storing the entire internal state, the activations of hidden units, or a combination of both. A computational analysis reveals that this approach can decrease memory requirements by up to 95%, albeit with a moderate 33% increase in computational overhead. This development has the potential to be highly beneficial for training RNNs on GPUs with restricted memory, particularly for long sequences. The paper's impact could be substantial, potentially extending the sequence lengths that back-propagation through time can practically handle. However, the inclusion of a concrete motivating example, such as a specific GPU model that encounters memory limitations despite having sufficient computational resources, would strengthen the argument. Without such evidence, it remains uncertain whether memory constraints would indeed become the primary bottleneck for long sequences, rather than computational capacity. Additionally, a minor correction is noted: on page 7, in the third sentence of Section 4, "orward" should be corrected to "forward".