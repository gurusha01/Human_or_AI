This paper proposes a novel property, low approximate regret, for expert algorithms, which enables faster convergence in games compared to existing work. The authors demonstrate that various common algorithms, including the standard Hedge algorithm, satisfy this property. The paper also adopts a more natural and less restrictive feedback model, where the player receives either the realized loss vector based on other players' actions or just the chosen coordinate of this loss vector, referred to as bandit feedback. Additionally, the authors derive high-probability bounds and extend their results to dynamic population games, improving upon previous work by [LST16]. The results presented in this paper are generally interesting. However, from a technical perspective, the primary findings appear to be a straightforward generalization of the concepts discussed in Theorem 23 of [SALS15], which establishes that a small loss bound leads to a faster convergence rate. As noted in Section 3 of this paper, a small loss bound implies low approximate regret. Moreover, although the main results in [SALS15] focus on the expectation feedback model, Theorem 23 is actually applicable to the same realized feedback model. Therefore, the claim that this paper improves upon [SALS15] in terms of the feedback model seems exaggerated. The claim regarding improved speed is also somewhat misleading, at least in the initial part of the paper, as it converges faster but only to an approximation of the Price of Anarchy (PoA), making the convergence rates not directly comparable. In the bandit setting, a known algorithm provides a small loss bound against non-oblivious adversaries, as seen in the work by Allenberg et al. (2006), which could potentially be applied here. Some detailed comments are provided below: 
1. It would be beneficial to explain why most previous work requires the expectation feedback model, whereas it is not necessary in this paper.
2. The statement in Line 49 (the second bullet point for improvements) appears to be an overstatement, as does the claim in Line 80, which mentions "without a constant factor loss" but still does not exactly achieve the PoA.
3. In Definition 1, it might be helpful to explicitly require that \epsilon falls within the interval (0,1).
4. In Definition 2, the right-hand side of the inequality is missing the sum over i.
5. At the end of Proposition 2, it might be more natural to express \gamma in terms of \epsilon instead of writing \epsilon in terms of \gamma.
6. Consider moving Example 1-3 (and related paragraphs) to Section 2, immediately after introducing the definition of low approximate regret.
7. The explanation of how the doubling trick can convert an algorithm with weak low approximate regret to one with strong low approximate regret is unclear.
8. In Theorem 3, it is unusual to use informal statements like "regret is bounded by \sqrt{Total loss of best action}" in a theorem; instead, consider writing down the exact expression.
9. In Line 222, the phrase "within a (1+\epsilon) favor" is unclear; it seems that the comparison is between \lambda/(1-\mu-\epsilon) and \lambda/(1-\mu).
10. The proposed noisy Hedge algorithm bears some resemblance to Fixed Share, as seen in the work by Cesa-Bianchi et al. (2012).
11. Several typos and minor errors were found: 
    a. Line 3: "a small a multiplicative" 
    b. Line 34: remove "in"
    c. Line 42: "...more information than is..."
    d. Line 155: "required that is s is..."
    e. Line 207: "...For instance [FS97, YEYS04]..."; consider placing this in parentheses and correcting the punctuation.