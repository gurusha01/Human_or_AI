This paper proposes a Neural Turing Machine (NTM) with sparse read and write operations, addressing a significant limitation of NTMs that renders them impractical for real-world applications. Specifically, NTMs typically require excessively large physical memories and computational resources for backpropagation, which the presented architecture aims to mitigate while maintaining performance comparable to state-of-the-art models. The paper is well-organized and accessible. To enhance readability for non-experts, consider adding a concise overview of memory-augmented neural networks in the background section or providing an explanation of the memory mechanism to make the paper more self-contained. For instance, clarifying the role of the write operator in relation to conventional memories would be beneficial, as its purpose may not be immediately clear to all readers. Additionally, the comparison in Figure 4 may be perceived as unfair, as it evaluates models with equivalent physical memory requirements. While this approach is justified, comparing performance using the same memory configuration, rather than physical memory, might provide more insightful results. Nonetheless, it is intriguing to observe the algorithm's performance approaching theoretical bounds while demonstrating promising results.