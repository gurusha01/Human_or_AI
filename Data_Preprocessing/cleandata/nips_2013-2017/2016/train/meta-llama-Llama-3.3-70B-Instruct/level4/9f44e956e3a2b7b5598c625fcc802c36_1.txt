This manuscript introduces a novel recurrent neural network (RNN) architecture that incorporates 'fast weights,' which are parameters that evolve at a timescale intermediate between the rapid fluctuations of hidden activity patterns and the slower adjustments of weights via gradient descent. These fast weights are updated automatically using a Hebbian learning rule, effectively embedding an associative memory within the RNN's dynamics. Experimental results demonstrate that this approach enables rapid learning of various tasks, primarily visual, with faster convergence and often superior error rates compared to LSTMs or LSTMs augmented with external memories. 
The key idea presented in this paper, namely, a weight matrix constrained to utilize a specific learning rule and update itself at multiple points during processing, is noteworthy and likely to spawn numerous variants in the future. The performance on the considered tasks is robust, making the technique worthy of further exploration. This paper contributes meaningfully to the field of machine learning. However, the claim in the conclusion that the primary contribution lies in computational neuroscience and cognitive science is not supported by the results. The paper lacks engagement with experimental data, whether from neural or psychological sources, and only interacts with a limited subset of the relevant literature in computational neuroscience or cognitive science.
For instance, it does not reference works such as Buonomano and Maass's discussion on state-dependent computations in cortical networks or the extensive literature on neural networks and recursive processing in human linguistic abilities, such as Prince and Smolensky's work on optimality from neural networks to universal grammar. While it is plausible that this research could contribute to understanding how quasi-recursive computations might be implemented in the brain, a worthy goal, substantial additional work is necessary to substantiate this claim. Demonstrating the capability of fast weights to enable recursive, compositional processing, particularly through tasks where this is a crucial component, such as natural language processing, would be more convincing. Applying the model to natural language processing and demonstrating more than just improved performance, such as reproducing error patterns made by human subjects on complex sentences, would significantly enhance the paper.
Currently, the paper aligns with the engineering tradition, where success is defined by performance improvement on benchmark tasks. From a cognitive perspective, the ability to obtain multi-scale views of objects and their parts, akin to zooming in with attention rather than just visually, is not addressed. The presentation of different models for each experiment was somewhat confusing. The figures depicting architecture variants have subtle differences, such as the representation of 'sustained' transitions by red arrows in Fig 1 and 'integration' transitions by red arrows in Fig 2. The definition of the 'integration transition' is not clear. 
Minor issues include the lack of detail in Table 3 regarding classification accuracy and the scarcity of information about the RL agent and MNIST experiments. The mention of an appendix in the submission is noteworthy, but its absence hinders the reproducibility of the results. Including more details in the main text or providing the appendix could address this. Additionally, citing the 'asynchronous advantage actor-critic method' would be appropriate. Overall, while the paper presents a solid contribution to machine learning, its claims regarding computational neuroscience and cognitive science require further substantiation through engagement with relevant literature and experimental data.