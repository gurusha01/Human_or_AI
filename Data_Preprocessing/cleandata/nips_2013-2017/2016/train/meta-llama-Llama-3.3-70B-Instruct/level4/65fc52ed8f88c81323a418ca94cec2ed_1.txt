This manuscript explores the unsupervised construction of similarity features from exemplars, introducing a novel training paradigm and loss function that integrates a CNN architecture with clustering algorithms. This approach effectively addresses the issue of imbalanced training data when utilizing exemplars with large neural networks, by employing a differentiable version of a discrete clustering loss. The authors demonstrate the efficacy of their method on three benchmark datasets: Olympic images, pose estimation, and the Pascal object recognition dataset. The paper presents an innovative method for leveraging CNNs to learn unsupervised similarity features from exemplars, developing a unique differentiable loss function and utilizing clustering to partition the data into distinct groups, thereby mitigating the common problem of limited exemplars versus numerous negative examples. Overall, the manuscript is well-written, motivated, and builds upon the emerging trend of combining discrete optimization algorithms with deep networks to achieve notable effects. Although the results across multiple real-world datasets appear promising, further reinforcement would be beneficial. One concern is the switch to a different feature initialization for the Pascal dataset, from HOG-LDA to Wang et al.; it is unclear why HOG-LDA features are not effective for initialization in this case, and whether the proposed approach can recover from poor initialization. Minor issues include unclear wording on line 41, the need for a citation for complete-linkage clustering on line 130, and the limited reporting of results for the Pascal dataset, which only includes k=5. It would be informative to provide results for k=1 to 10, and to clarify whether the 3% boost is contingent upon k=5. Additionally, there are grammar and spelling issues on lines 2, 76, 126, and 225, which require attention to ensure clarity and precision.