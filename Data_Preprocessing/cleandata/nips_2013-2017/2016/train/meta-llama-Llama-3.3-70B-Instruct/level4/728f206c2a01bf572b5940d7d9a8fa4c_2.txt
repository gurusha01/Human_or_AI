This manuscript investigates the application of Wasserstein Loss as an objective function for training generative models, specifically restricted Boltzmann Machines. The authors examine the influence of adjusting the entropy regularization strength within the Wasserstein objective on tasks such as image completion and denoising. The use of Wasserstein loss as an alternative to KL divergence minimization is a compelling approach, offering potential robustness advantages in certain scenarios. However, a notable drawback of this method is its computational expense, which, despite recent advances in optimization techniques, has limited the authors' experiments to small-scale datasets. The results presented in Figure 4, illustrating the effects of intensified entropy regularization, are noteworthy. Could the authors provide insight into why stronger regularization leads to models with increased concentration in the image space? Furthermore, it is unclear why $E{\hat{p}}[\alpha^] = 0$ whereas $E{p}[\alpha^]$ in Equation 5 is non-zero, warranting further clarification on this discrepancy.