This paper integrates a count-based exploration bonus into various reinforcement learning algorithms, which is particularly useful when dealing with large state spaces that require generalization across neighboring states for accurate count measurements. The authors present a novel transformation that converts density estimation into a count measure, characterized by its simplicity and locality. The resulting pseudo-count exhibits several desirable properties, making it a valuable contribution. The efficacy of this approach is showcased through its application to Atari2600 games, demonstrating enhanced exploration capabilities. I find the proposed methodology to be both sophisticated and efficacious. The clarity of the presentation is commendable. However, providing additional details on the density estimation technique employed in the application would further enhance the understanding and utility of the proposed scheme.