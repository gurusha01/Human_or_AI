This paper proposes a novel approach to identifying ratings in a dataset that are likely to have been influenced by a recommender system. The authors introduce a simple model that simulates how users deviate from their genuine ratings over time, where item similarity informs recommendations and users adopt these suggestions with a certain probability. The observed rating matrix is the culmination of this process. Under specific assumptions, such as knowledge of the adoption probability, the authors demonstrate that it is possible to recover the true rating matrix from the observed ratings. The discrepancy between the true and observed ratings serves as an indicator of which items were recommended rather than being inherently preferred by users. This methodology is first evaluated on a synthetic dataset, showing its effectiveness in distinguishing between true and recommended ratings. The approach is then applied to real datasets, with its validity supported by anecdotal evidence, including the absence of recommended ratings in a dataset collected without a recommender system and the lower likelihood of TV shows being recommended in a mixed dataset of movies and TV shows. The proposed model and methodology are commendable, and the authors successfully motivate their assumptions. However, it would be beneficial to discuss the practical applications of discerning true from recommended ratings. For instance, why is solving this problem important, and does accounting for the influence of recommender systems lead to improved recommendations? A technical inquiry arises regarding the relationship between true and observed ratings, as determined by the singular value relationship. It appears that this relationship "boosts" items with high sigma^true_i, but further intuition on this aspect would be valuable. Additionally, how does the algorithm's identification of recommended items depend on the gap between sigma^true and sigma^obs, and could the inference process be conducted in the singular value space rather than by analyzing changes in ratings? Some minor clarifications are needed, such as in Section 3.2, where the introduction of the algorithm could be more explicitly connected to its purpose of classifying items as truly or recommendedly rated. Furthermore, when mentioning the validation using the Jester dataset, it would be helpful to note that Jester did not utilize a recommender system initially.