The authors present a novel approach to generating multiple outputs by training an ensemble of deep networks to minimize the oracle loss, which is particularly useful in interactive user scenarios. Experimental results on three tasks, using oracle experiments, validate the efficacy of the proposed method. This work incorporates deep networks into the MCL paradigm and introduces a stochastic block gradient descent optimization technique to minimize MCL's oracle loss. Notably, the proposed optimization method can be seamlessly integrated into the back-propagation learning framework, enabling end-to-end training of the entire model. However, several questions arise: what impact does initialization have on the model's performance, and how would results differ if models were initialized with random values rather than a pre-trained network? Furthermore, the comparison setting lacks clarity - do the authors utilize the same pre-trained network for baseline methods as they do for sMCL initialization? The evaluation metric of choice is oracle accuracy, but an analysis of the diversity among ensemble members is noticeably absent and would be a valuable addition. Additionally, a similar concept has been explored in prior work, specifically by Stefan Lee et al. in "Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks" (CoRR abs/1511.06314, 2015), which the authors should consider comparing their work to.