The paper presents a domain adaptation approach for deep neural networks, leveraging three key components: (1) multi-kernel maximum mean discrepancy to increase feature similarity between the target and source domains, (2) utilizing prediction entropy to adapt the classifier to the target domain, and (3) introducing a residual function that defines the source domain classifier in terms of the target domain classifier, which is the primary contribution. The technique demonstrates promising results, with an extensive comparison to related work. The paper shows potential to be highly effective, but would benefit from enhanced experimental evaluation. However, a major concern is the lack of thorough evaluation of the residual function's contribution, making it unclear whether this component is essential. Alternatively, simple L2 regularization could potentially achieve similar results by maintaining similarity between source and target classifiers. Furthermore, Figure 2 does not convincingly demonstrate a qualitative difference between DAN and RTN predictions. Additionally, the notation inconsistency, particularly the interchange between H and F (residual and original functions), is confusing and would be improved by adopting a uniform notation throughout.