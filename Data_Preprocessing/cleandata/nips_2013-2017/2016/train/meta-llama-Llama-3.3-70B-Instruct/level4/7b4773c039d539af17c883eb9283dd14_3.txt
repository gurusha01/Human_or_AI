This manuscript presents an enhanced algorithm for Monte-Carlo planning, accompanied by a thorough analysis. The proposed approach innovatively integrates Monte Carlo sampling, excluding action selection, with planning, by disregarding other action branches when sufficient evidence supports the selection of the best action at a given node. This strategy enables the exploration of a limited subset of states that are reachable by adhering to near-optimal policies. The analysis reveals that the sample complexity is dependent on the quantity of near-optimal states. Furthermore, the paper demonstrates that the algorithm and its analysis surpass the best previous worst-case bounds under various conditions. The algorithm's implementation appears to be straightforward. Notably, the authors achieve multiple advancements with a single, relatively simple algorithm: improved worst-case bounds for finite state spaces, bounds that are contingent upon the size of the space explored by near-optimal policies, conditions that facilitate polynomial sample complexity for infinite state spaces, and behavior akin to Monte-Carlo sampling when a significant gap exists between the value of the optimal action and other actions. This constitutes significant theoretical progress. However, despite the algorithm's simplicity, its practical applications seem limited due to the requirement of exploring an extensive tree before obtaining meaningful results at the root, in contrast to practical, anytime algorithms. The development of an anytime version of this algorithm would be beneficial to enhance its impact beyond theoretical contributions. Following the authors' feedback, which indicated the possibility of creating an anytime version, future experiments incorporating this algorithm would be intriguing to explore.