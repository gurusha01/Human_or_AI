This paper presents a novel parallel Bayesian optimization algorithm, which calculates Bayes-optimal batches, offering a significant improvement over traditional sequential approaches in machine learning tasks such as hyperparameter optimization. The authors demonstrate the superiority of their method over existing Bayesian optimization techniques. The paper is well-structured and clear, making it easy to follow. The concept of parallelizing Bayesian optimization is crucial for practical hyperparameter optimization, and the proposed approach is not only interesting but also more refined than most existing methods. The determination of a Bayes-optimal batch is particularly promising. However, the assumption of independent normally distributed errors, common in Gaussian process-based BO methods, may be problematic in hyperparameter optimization due to the nature of measurement errors, which can introduce structural bias, especially with small sample sizes. The independence assumption may not always reflect real-world scenarios.
The theoretical foundation of the paper is robust, but the experimental section falls short in several aspects. Firstly, utilizing test set error as an optimization criterion is questionable, as it is known that such score functions are not ideal for hyperparameter tuning. Metrics like area under the ROC curve or log loss would be more appropriate, as threshold-based metrics can complicate model selection and hyperparameter optimization. Secondly, while MNIST and CIFAR10 are standard benchmarks, using HPOlib, a library specifically designed for benchmarking hyperparameter optimization algorithms, would have provided more comprehensive insights. Lastly, the benchmark's scope is limited, with only two datasets and a single machine learning task (classification) using just two learning algorithms (logistic regression and CNN). Given that some optimizers perform better with certain types of learning algorithms, a more diverse benchmark would have been more convincing.
Despite these limitations, the paper's theoretical contribution is valuable, and the concept presented is appealing. However, the current experimental setup does not fully demonstrate the potential of the proposed parallel Bayesian optimization algorithm, highlighting the need for more extensive and varied benchmarks to solidify its effectiveness. References to previous works, such as those by Provost, Foster J., Tom Fawcett, and Ron Kohavi, and Eggensperger, Katharina, et al., support the argument for more nuanced evaluation metrics and benchmarking practices in hyperparameter optimization.