The paper explores likelihood-free inference, specifically parametric inference for models with expensive-to-evaluate likelihood functions. It proposes approximating the posterior distribution of parameters by using a Gaussian mixture model (a mixture density network) to approximate the conditional distribution of data given parameters. The authors claim their approach has two main advantages over standard approximate Bayesian computation (ABC): it returns a "parametric approximation to the exact posterior" and is computationally more efficient.
The paper includes a brief theoretical section demonstrating that the approach yields the correct posterior in the limit of infinitely many simulations, provided the mixture model can represent any density. The method is verified on two toy models with known true posteriors and two more complex models with intractable likelihoods.
From a technical standpoint, several concerns arise:
1. The claim that the approach approximates or targets the exact posterior is disputed, as it only concerns the approximation of the posterior after converting data to summary statistics, which often results in lost information.
2. The statement that the approximation can be made as accurate as required is questioned, particularly in regards to model choice and the need for increased computational resources to fit more complex models.
3. The theoretical justification provided by Proposition 1 is deemed insufficient, as asymptotically, rejection ABC also recovers the posterior under weaker conditions.
4. The lack of comparison with regression adjustment, a classical method in ABC for reducing computational cost, is noted.
Additional comments highlight potential issues:
1. The more advanced algorithm (MDN with proposal) seems to produce a less accurate posterior than the simpler algorithm (MDN with prior), which is concerning.
2. The use of log probability to assess accuracy may be misleading, as it does not account for the spread of the posterior.
3. The approach may struggle with uniform priors on bounded intervals, resulting in truncated mixture of Gaussians, which can be problematic.
4. The relationship between learning qphi and learning the likelihood function is questioned, given that Eq (7) suggests qphi(theta|x_0)/pTilde(theta) equals the likelihood function.
Regarding novelty and originality, the paper's claims are considered too strong. The proposed method conceptually belongs to existing likelihood-free inference approaches, such as ABC via conditional density estimation, and does not significantly differ from previous work. The use of neural networks to model the relation between data and parameters is not new, and the paper's presentation as a "new approach to likelihood-free inference" is misleading. The advantages of the proposed method over recent efficient likelihood-free inference methods are not clearly presented or discussed.
References, including Blum (2010), Beaumont (2010), and Horrace (2005), support the notion that the proposed method is not as novel as claimed. The paper's update and promises in the author reply have led to an increased score, but revisions are suggested to clarify the approach's limitations, differences from existing methods, and novelty claims.