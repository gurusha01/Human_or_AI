This manuscript presents lower bounds for the problem of learning with missing attributes, where the goal is to find a linear classifier that minimizes a specific loss function, such as square loss or hinge loss, given examples (x,y) sampled from a distribution on R^d \times R. The key challenge in this framework is that the learner is restricted to observing at most k features for each example. Originally proposed by Ben David and Dichterman in 1998, this setting has yielded algorithms for the square loss, even when k=2. However, for other loss functions, no algorithms exist for fixed k, and this paper demonstrates that such algorithms are indeed impossible. Specifically, it shows that learning is infeasible for k=1 in the case of square loss, establishing that k=2 is a tight bound. Furthermore, for absolute and hinge losses, learning is impossible for any fixed k. The authors achieve this result by constructing two distributions that are indistinguishable given the limited access to examples, with the proof being relatively straightforward for square loss but more complex for absolute and hinge losses. Overall, the paper makes a significant contribution to the field, and I recommend its acceptance.