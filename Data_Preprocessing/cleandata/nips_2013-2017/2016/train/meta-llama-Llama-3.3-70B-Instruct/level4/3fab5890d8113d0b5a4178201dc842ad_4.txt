This manuscript presents a novel, end-to-end differentiable memory access approach that retains the expressive capabilities of the traditional Neural Turing Machine (NTM) while offering significantly improved training efficiency, particularly when dealing with exceptionally large memory sizes. The paper is replete with innovative ideas that demonstrate considerable technical ingenuity. The problem addressed is well-justified and effectively tackled. Given the substantial implications of scaling memory systems for neural networks, this work clearly and succinctly introduces both the problem and its solution, providing a well-articulated and concise explanation.