The authors propose utilizing fast approximate-nearest-neighbors structures and arg-top-K operations to compute attention weights in a Neural Turing Machine, demonstrating through experiments that this approach does not compromise training performance and enables training on larger problems. Although the novelty of this work is somewhat limited, as evidenced by similar concepts in memory network papers such as http://arxiv.org/pdf/1410.3916.pdf, which employ memories with millions of items and hashing methods for scalable lookups, the application of these tools to writable memories is a notable contribution. Despite the lack of a significant conceptual breakthrough, the practical aspects of implementing this approach are worthwhile reporting. I recommend accepting the paper, subject to some revisions. 
Firstly, I suggest that the authors provide more detailed descriptions of the tasks and their construction, as the current descriptions are insufficient for replication, particularly given the unavailability of the original NTM paper's code and the lack of a standard version of the tasks. Additionally, I request that the authors commit to releasing the code for their experiments to facilitate reproducibility.
Secondly, it is essential to discuss the potential limitations and failure cases of the proposed model. The authors often emphasize the smoothness of their model, but it is crucial to acknowledge that the argmax (or K-argmax) operation is not smooth. With K=1 in the read-only setting, their model bears similarities to the MemNN-WSH in http://arxiv.org/pdf/1503.08895.pdf, which was reported to perform less well than the fully smooth model. I believe that an analysis of failure cases and a discussion on the types of tasks and setups that allow successful training would significantly enhance the value of this paper. The model's discrete action and lack of consideration for this aspect during training, unlike http://arxiv.org/abs/1511.07275, should be examined. While the authors' results are a valuable contribution, understanding the limits of their approach is essential.
Lastly, I recommend removing section 3.6 and suggest that the authors avoid referring to the Omniglot data as "non-synthetic" or "real-world".