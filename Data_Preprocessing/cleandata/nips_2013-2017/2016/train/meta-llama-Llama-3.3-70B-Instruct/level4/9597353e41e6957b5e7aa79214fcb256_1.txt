This manuscript examines an interactive semi-supervised clustering framework where the learning algorithm aims to minimize the k-means cost with access to an oracle that provides answers to pairwise same-cluster queries, indicating whether two points belong to the same cluster in the optimal k-means solution. The paper establishes three key results: (1) under a margin assumption, a randomized polynomial-time algorithm can achieve the optimum using k^2log(n) queries; (2) even with the margin assumption, minimizing k-means cost without queries is NP-hard; and (3) minimizing k-means cost remains NP-hard with a weaker margin condition and fewer queries, specifically O(log(k) + log(n)). I am intrigued by the interplay between the margin assumption, running time, and the number of queries in this problem. It prompts the question of whether a more general statement can be proven, such as the problem being NP-hard if the number of queries is less than a certain function f(n, k, gamma) of the margin gamma, and solvable otherwise. Furthermore, I am interested in exploring scenarios where the oracle is either erroneous or its clustering objective diverges from the k-means optimum. Specifically, if the oracle makes persistent mistakes with a certain probability q for each pair of points, can an effective clustering algorithm be designed for small q? Additionally, if the oracle targets a clustering with a k-means cost slightly worse than optimal, can this oracle still be used to efficiently find the optimal k-means solution, potentially under the margin assumption, given that its clustering should align well with the optimal one? Investigating these questions could yield valuable insights.