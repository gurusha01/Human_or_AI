This manuscript presents learning bounds for hypothesis classes with logarithmic entropy numbers under heavy-tailed losses. The authors establish that fast rates, approaching O(n^{-1}), can be achieved if the r-th moment of the supremum loss over all hypotheses is integrable and a novel 'multiscale Bernstein condition' is satisfied. This work contributes to an important and increasingly active area of research: obtaining fast rates for unbounded and heavy-tailed loss functions. Unlike existing results based on empirical process theory, which often rely on bounded or sub-Gaussian losses, this study tackles the more challenging case of heavy-tailed losses. The results appear sound and novel, building upon concentration inequalities for unbounded empirical processes developed by Sara van de Geer and collaborators. 
The material is highly technical, and it is suggested that some content be relocated to the appendix to facilitate a more extensive discussion of key points. For instance, the 'multiscale Bernstein condition' is both novel and crucial, and a simple example illustrating the distinction between microscopic and macroscopic scales would be beneficial. The k-means example provided is useful in demonstrating the approach's utility but does not fully elucidate the intuition behind or the necessity for the multiscale Bernstein condition. 
It is noteworthy that Audibert (2009) demonstrates that a standard Bernstein condition can suffice for achieving fast rates, even with unbounded and potentially heavy-tailed losses, when using a pseudo-Bayesian estimator with online-to-batch conversion. In contrast, the present paper proves uniform convergence rates for the entire class {\cal F}, implying that methods like ERM will attain these rates. This differs from Audibert's approach, which employs a specific randomized estimator and measures error based on the predictor's randomization. The authors are encouraged to provide an informal explanation for why the multiscale condition is necessary and whether it could potentially be dispensed with, especially in light of Audibert's findings.
Furthermore, a recent paper by Mehta and Grunwald (2016) presents similar results in spirit but with different conditions, including a 'witness condition' and a v-PPC condition instead of the multiscale Bernstein condition. A discussion on the relationship between these papers would be valuable, although it is acknowledged that the proof techniques and conditions are quite distinct. 
Additionally, clarification is needed on why the approach does not immediately generalize to polynomial entropy numbers, given the use of chaining in the proofs. The conclusion mentions an expected extension to logarithmic entropy numbers, but the reason for the non-immediate generalization should be explained. It should also be explicitly stated that the in-probability results are polynomial in \delta, which, while presumably unavoidable with heavy tails, differs from typical learning theory results, as seen in the work by Sabato and Hsu (2015). 
Despite these suggestions, the main results are distinct from those of Audibert, Grunwald & Mehta, and Sabato & Su, and given the challenging nature of this area, acceptance of the paper is recommended. Minor issues include an abstract that could be condensed, the use of non-standard terminology ('weak-tailed'), and several typographical errors and formatting issues in the references and throughout the text.