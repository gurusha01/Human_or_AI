This manuscript explores the concept of unsupervised domain adaptation, highlighting the presence of a mismatch between source and target classifiers in addition to differences in marginal distributions. The disparity between classifiers is represented through a perturbation function, akin to those described in [25,26], which is learned using a novel deep residual network. This network simultaneously performs feature learning and adaptation, thereby mitigating the marginal distribution shift. By leveraging the deep network that won the ILSVRC challenge in 2015, the authors propose an innovative approach to domain adaptation. The description of how a residual block is incorporated into the CNN architecture to estimate the perturbation function for the target classifier is both lucid and constitutes a noteworthy novel contribution. Notably, the integration of the entropy minimization principle [28] into a domain adaptation network, as well as the introduction of the MK-MMD variant, differing from [5], are unprecedented. The experimental results are compelling, although testing on more demanding datasets might be necessary. Furthermore, clarification on the metrics used to determine the superiority of figures 2-c and 2-d over 2-a and 2-b would be beneficial; specifying the average class-to-class distance could enhance the interpretability of the figure, which currently lacks self-explanatory clarity.