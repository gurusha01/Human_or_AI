The authors present a theoretical framework for assessing the capacity of a family of unitary matrices, revealing that prior approaches utilizing a simplified family of unitary matrices to parameterize a unitary recurrent neural network have limited capacity compared to the full unitary group. By leveraging an existing gradient descent method on the Stiefel manifold, they optimize over the entire unitary group, demonstrating its superiority through experiments on both synthetic and real data, with the added benefit of improved computational efficiency. This well-structured and clearly written paper introduces a novel concept for evaluating the capacity of unitary matrix families, which they use to show that previous parameterizations restricted the neural network's representational capacity. However, it would be beneficial for the authors to address why a straightforward counting argument cannot also be applied, given the significant difference in parameterization between the previous family of unitary matrices (7n real parameters) and U(N) (n^2 real parameters), suggesting a potential lower bound for when the previous family's capacity becomes insufficient. To further enhance the paper, additional experiments, such as replicating studies from the original unitary evolution recurrent neural network paper (e.g., the permuted MNIST experiments), would provide more comprehensive insights.