This paper introduces a nonlinear spectral method that achieves optimal training of a specific class of feedforward neural networks, characterized by a linear convergence rate. The guarantee of global optimality is contingent upon the network's architectural parameters, simplifying to the calculation of the spectral radius of a small nonnegative matrix. The proposal of a nonlinear spectral method for optimal training of feedforward neural networks appears novel and intriguing. However, concerns arise regarding the limitations of this approach, notably the constraint of non-negativity imposed on the network's weights. Furthermore, the selection of activation functions in the neural network is unconventional and highly specific. These limitations seem overly restrictive, potentially compromising the neural network's performance. Consistent with this, numerical experiments demonstrate subpar performance compared to SVM. A minor note: the claim on lines 90-91 that the nonlinear spectral method "converges quickly, typically in a few (less than 10) iterations" due to its linear convergence rate may be overstated, as a linear convergence rate does not necessarily translate to rapid convergence within a few iterations.