This manuscript introduces conditional generative moment matching networks, a novel extension of GMMNs tailored for conditional generation and prediction tasks. The core of the proposed methodology lies in the utilization of kernel embedding for conditional distributions and the conditional MMD metric to assess the discrepancy between conditional distributions. The authors demonstrate the efficacy of their proposed CGMMN through experiments on classification, conditional generation, and the distillation of Bayesian models. A straightforward yet naive approach to adapting GMMNs for conditional settings involves estimating a separate GMMN for each conditional distribution, with these distributions sharing parameters via a common neural network. However, this method is plagued by the data sparsity issue, where each conditional distribution has limited examples, particularly in continuous conditioning variable domains. In contrast, the proposed approach treats all conditional distributions as a unified family, aiming to match the model directly with the conditional embedding operator rather than individual distributions. While the advantages of this approach are apparent, there are scenarios, such as conditional generation with discrete conditioning variables (e.g., MNIST with 10 classes), where the naive method yields reasonable performance, suggesting a comparison with this baseline could be insightful. The formulation of the conditional MMD objective is noteworthy for its simplicity and computational efficiency, similar to its unconditional counterpart, as highlighted in Remark 1. Moreover, the independence of the K matrices in equation 2 from model parameters facilitates backpropagation in a manner analogous to previous methods. A potential concern with the proposed CMMD objective is its requirement for large minibatches to mitigate stochasticity-induced errors, although this increases computational costs, particularly for matrix inverses. Overall, the CMMD offers a compelling framework for modeling conditional distributions, with intriguing results on classification and Bayesian model distillation. However, sections 2.3 and 3.1 could benefit from clearer exposition, as their current presentation necessitates reference to external sources for full comprehension. Additionally, there are minor inaccuracies: the MMD estimator is described as unbiased on line 77, which is incorrect, and the conditional embedding operator on line 90 diverges from the definition provided in reference [29], suggesting a need for an alternative cross-covariance operator definition to align with the form used in this paper.