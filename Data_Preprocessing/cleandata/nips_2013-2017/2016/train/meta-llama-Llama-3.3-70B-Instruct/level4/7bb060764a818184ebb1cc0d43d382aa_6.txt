This paper introduces a novel dropout variant that utilizes distinct drop probabilities for each unit, determined by the units' second-order statistics. The authors provide a thorough analysis, demonstrating that this probability selection minimizes an upper bound on the expected risk, potentially leading to accelerated convergence and reduced error. Empirical evidence is also presented to support this claim. The paper is engaging and offers insightful theoretical analysis, contributing to the understanding of dropout and presenting a straightforward improvement with moderate benefits. However, the paper's primary weaknesses lie in its empirical evaluation and presentation: 
- The plots are of poor quality, being too small, with indistinguishable colors (e.g., pink vs. red), poorly labeled axes (with unclear definitions of "error"), and visually similar labels (e.g., s-dropout(tr) vs. e-dropout(tr)). As the primary means of presenting experimental results, these plots should be significantly clearer, which is why I rated the clarity as "sub-standard".
- The comparison between standard and evolutionary dropout on shallow models should be presented as the mean of multiple runs (at least 10), preferably with error bars. The plotted curves appear to be from single runs and may be subject to substantial fluctuations. Given the small model sizes, there is no justification for not providing statistical analysis.
- The final learning rates used for the deep models, especially for CIFAR-10 and CIFAR-100, should be disclosed. Since the authors only explored four different learning rates, if the optimal learning rate for the baseline fell outside the tested range, it could compromise the results.
Additionally, it is noted that the claim regarding evolutionary dropout addressing internal covariate shift is limited, as it can only increase the variance of low-variance units. In contrast, Batch Normalization standardizes variance and centers activation. These limitations should be explicitly discussed. Minor issues are also present.