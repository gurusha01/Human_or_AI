This manuscript presents a novel approach to training recurrent neural networks (RNNs) with unitary transition matrices, eliminating the need to restrict the set of learnable unitary matrices. The authors provide a theoretical foundation for their method by introducing Givens operators, which enable the quantification of the representational power of specific unitary matrices. They demonstrate that the original parameterization of unitary RNNs (uRNNs) is incapable of representing the entire unitary group for N x N matrices when N exceeds 22. To address this limitation, the authors propose a gradient descent optimization algorithm that operates directly on the Stiefel manifold of unitary matrices. Experimental results on several tasks show that this approach yields performance that is at least comparable to, and often surpasses, that of the original uRNN parameterization and long short-term memory (LSTM) networks. 
The paper's theoretical contributions and proposed optimization algorithm are notable, and the empirical results, although decent, could benefit from more in-depth analysis. The manuscript is generally well-written, making it accessible to follow the discussion on Givens operators and their application to unitary matrices, despite this being unfamiliar territory. The supplementary material contains sound and relatively straightforward proofs. However, the experimental section is less convincing, as it relies on synthetic tasks or artificial setups with real data. While the results themselves are not problematic, the experimental designs and result interpretations are sometimes unclear.
Specifically, the investigation of matrix dimensionality N below, at, and above the critical value of 22 is incomplete, as the value 22 itself is not included in the examined dimensions. Furthermore, the expected sudden increase in performance gap between the restricted-capacity uRNN and the full-capacity uRNN after exceeding N = 22 is not observed; instead, the difference in performance is more pronounced for lower-dimensional problems. A discussion on the absence of a sudden breakdown of the restricted model for N > 22 would enhance the paper. 
Additionally, the reporting of the best test results over 100 epochs for multiple random initializations raises concerns, as it involves early stopping on the test data. Providing the number of initializations used and including standard errors or deviations in Table 2 would strengthen this section. The 'copy memory' task yields a negative result for the restricted uRNN, but it would be beneficial to include an indication of a thorough hyperparameter search for this baseline. The results for the new method on this task are impressive, demonstrating its potential practical usefulness. 
The speech task seems somewhat artificial, as next-frame prediction for speech features is not a common application, and the results are surprising, with the restricted uRNN performing well in terms of mean squared error (MSE) for up to 70 hidden units. The full-capacity models consistently outperform the restricted variant on other metrics, but it would be interesting to see the performance of larger RNNs with 500 hidden units. While the task provides evidence for the optimization method's effectiveness, it would be beneficial to include more details to facilitate replication of the results. 
Although the individual experiments have some limitations, collectively they provide a decent investigation of the proposed methods. However, the paper would benefit from an improved discussion of the results and additional details to enhance replicability. Overall, the manuscript presents a strong contribution to the field, with notable theoretical and empirical advancements, and its limitations can be addressed with further refinement and clarification.