This paper presents a innovative approach to memory access, termed Sparse Access Memory (SAM), within the context of memory-augmented neural network architectures, a field that has gained significant attention since the introduction of Neural Turing Machines. By enabling neural networks to transcend the limitations of traditional LSTM models, this augmentation allows for more complex learning patterns. The key contribution of this work lies in its constraint of all writes to and reads from external memory to a fixed size, effectively limiting interactions to a subset of memory words rather than allowing unlimited access. The authors demonstrate the methodology's validity and optimality, suggesting that the reduced memory size will lead to enhanced read and write performance, as evidenced in section 4.1. Furthermore, they assess the learning costs associated with introducing sparsity in section 4.2 through the use of standard NTM tasks, including copy, associated recall, and priority sorting. Notably, the combined costs for certain tasks (specifically tasks 2 and 3) are lower than those of NTM. The inclusion of a real-world example in section 4.4, benchmarked using Torch7, is also commendable. The concept is both straightforward and novel, with significant speedup compared to NTM, which is particularly impressive. The observation that sparse reads and writes, facilitated by a linear or artificial neural network (ANN), can benefit early-stage learning in certain cases (as discussed in section 4.2) is intriguing. To further enhance the paper, it would be beneficial for the authors to elaborate on how to generalize these findings to other scenarios, exploring under which conditions learning could be improved, or by investigating the application of different ANN approaches in conjunction with SAM. This would provide valuable insights for the audience. Overall, the paper is well-structured, with clear assumptions and a detailed architecture outlined in the appendix, making it an engaging read.