This paper proposes an algorithm for addressing Markov Decision Processes (MDPs) with finite or infinite state-action to next state transitions, aiming to estimate the optimal value function at any state while minimizing the sample complexity by reducing calls to the generative model. However, a significant concern is the absence of experimental results, either from simulated domains or real-world applications, which are crucial for demonstrating the practicality and effectiveness of the proposed algorithm. Given the authors' claim of their approach being easily implementable and computationally efficient, such empirical validation is necessary. The manuscript requires additional proofreading to correct various typos and improve clarity. Specifically, corrections are needed at several points: Line 58 should be revised to use proper notation; Line 107 should specify "finite or infinite" for accuracy; Line 111 contains a grammatical error that needs correction; the concept of "opened" introduced at Line 174 requires explanation; Line 220 contains a minor typo; and Line 355 needs correction for proper preposition usage. Furthermore, Figure 3 lacks a sufficient explanation, which is essential for understanding the algorithm's performance and implications.