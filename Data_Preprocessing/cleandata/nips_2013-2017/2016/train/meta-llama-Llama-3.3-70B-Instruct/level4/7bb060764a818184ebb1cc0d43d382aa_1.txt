This paper explores a recent technique for enhancing the performance of deep networks, specifically dropout, by providing a theoretical framework and experimental validation. The authors commence by analyzing a linear shallow network, deriving risk bounds and update rules that utilize multinomial sampling for selecting dropout neurons at each update step, incorporating second-order statistics of the data's features. They then extend these update rules to deep networks, leveraging second-order statistics of a layer for mini-batches of data, which also reveals a connection to the internal covariate shift, a well-studied phenomenon in the literature. The proposed technique demonstrates superior convergence and accuracy for both shallow and deep learning across multiple datasets. Notably, the implementation appears straightforward, yet the results show significant performance gains compared to standard dropout, establishing a new state-of-the-art for dropout techniques. This work is expected to have a profound impact and become widely adopted in the near future. The paper is technically sound, with claims supported by both theoretical analysis and practical experiments, presenting a comprehensive piece of work that progresses from an initial insightful observation to final experiments showcasing the performance gain of the proposed method. The writing is clear, and the organization is logical, making the content informative and reproducible. The approach is innovative, as it tackles the dropout problem from a unique perspective, backed by theoretical bounds, which opens up avenues for further investigations and variations, such as exploring more complex probability distributions for dropout. The connection to internal covariate shift is particularly insightful. Some minor suggestions for improvement include specifying the cost of the technique, clarifying whether the convergence speed refers to the number of iterations and its performance in terms of wall-time, and addressing a few notation and formatting issues, such as defining \mathcal{H} in line 104, reformulating line 242 (iv), and correcting minor errors in the proof of Theorem 1 and Lemma 1, as well as in the supplementary material. Additionally, providing more details on Proposition 3, particularly its derivation from the KKT conditions, would enhance the clarity and impact of one of the paper's main results.