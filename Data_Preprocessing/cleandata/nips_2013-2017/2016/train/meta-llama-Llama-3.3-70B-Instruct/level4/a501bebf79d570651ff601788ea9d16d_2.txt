The process of training recurrent neural networks involves unfolding the network over time and performing backpropagation through time, which can be computationally intensive. To efficiently execute this process and avoid redundant calculations of intermediate values, these values are typically stored in memory. However, as sequence lengths increase and network sizes expand, available memory may become insufficient. A common workaround is to truncate backpropagation, resulting in approximate weight updates, or to recompute the forward pass, which is inefficient in terms of computation time. Alternatively, heuristic "divide-and-conquer" approaches have been proposed to balance computation time and memory efficiency. This paper introduces a novel solution aimed at determining the optimal strategy for achieving an algorithm that is both memory-efficient and computationally feasible. Specifically, the authors define a cost function representing the number of forward operations required to compute backpropagation through time within a fixed memory budget, which is then minimized using dynamic programming. This approach determines the optimal timing for storing intermediate results in memory versus recomputing them, considering the storage of hidden states, internal states, or both in recurrent networks. The results demonstrate that this method can achieve a 95% reduction in memory usage while incurring only a 33% increase in computation time, with theoretical bounds derived in terms of sequence length and memory budget. The proposed method is compared to naive baselines and a handcrafted heuristic method, showcasing its effectiveness. The paper is well-written and enjoyable to read, with clear figures that enhance the clarity of the explanations. The idea of utilizing dynamic programming to optimize intermediate storage and limit forward operations within a fixed memory budget is straightforward and well-defined. The notation is clear, and the figures are helpful in illustrating the concepts. The chosen baselines are relevant, including limit cases and a previously published heuristic divide-and-conquer method. Notably, this paper addresses a pressing issue in the field, given the growing popularity of recurrent neural networks and the increasing size of models and sequences being processed. Many researchers currently resort to truncated backpropagation through time, sacrificing exact computation for the sake of feasibility. This paper provides a simple and elegant solution, complete with pseudo-code and implementation details, making it highly appreciable. Some minor remarks and cosmetic issues were noted: the 33% time increase claimed in Fig. 5a was unclear, a typo was found in Section 3.2, and minor errors were identified in figures and captions, including missing capitals and unclear curve overlaps in Fig. 7. Additionally, legends within captions rather than on the figures themselves would improve clarity, and larger titles and axis labels would enhance readability.