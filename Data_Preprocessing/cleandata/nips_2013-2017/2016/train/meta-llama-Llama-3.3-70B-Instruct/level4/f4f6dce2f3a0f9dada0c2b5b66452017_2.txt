This submission focuses on approximate low-rank tensor approximation, aiming to find vector families ar, br, and cr for r=1,...,R that well approximate a given 3-dimensional tensor T by the sum{r=1}^R ar  br  c_r, where * denotes the outer product. The error in the approximation is measured using the Frobenius norm. The submission concentrates on the alternating least squares (ALS) algorithm to solve this problem and explores how to accelerate it using leverage score sampling. 
ALS involves fixing two out of three matrices A, B, and C, then minimizing the Frobenius norm error by optimizing the third. This process is iterated over the three families of vectors. A key observation in the submission is that once two-thirds of these matrices are fixed, the optimization problem becomes equivalent to a traditional least squares problem, where the matrix X is n^2 x n. Standard results from the randomized linear algebra literature imply that it suffices to sample only O(n log n) rows of X via nonuniform sampling based on its statistical leverage scores.
However, computing the statistical leverage scores of X is computationally expensive due to its n^2 rows. The main contribution of this paper is the observation that, for ALS, X has a special structure - it is the Khatri-Rao product of the two fixed matrices. A lemma is proven to show that the leverage scores of X can be upper-bounded by simple functions of the leverage scores of the two fixed matrices. This leads to a faster algorithm for computing upper bounds on the leverage scores of X, resulting in a faster ALS variant.
Each iteration of ALS for tensor decomposition involves solving a least squares regression problem. The main novelty of this submission is the observation that good upper bounds on the leverage scores of the underlying matrix can be quickly approximated due to its special structure, as stated in Theorem 3.2. Once this theorem is established, the rest of the details follow standard procedures. This observation enables the authors to compare their method favorably with [37], as seen in Figure (a) on page 8.
The advantages of this submission include its ability to compete well with the state-of-the-art based on a simple yet important observation. However, there are two main drawbacks: (1) the empirical performance is comparable to, rather than significantly better than, [37], and (2) the absence of a comparison with [37] in Figure (b) is not explained. Additionally, the non-monotonic behavior of SPALS(alpha) figure numbers with respect to alpha in Figure (a) on page 8 is puzzling.
Minor comments include several typos and areas of confusion, such as the explanation of "(i,j) also represents the index i+Ij between 1 and IJ" on page 2, line 80, and the lack of clarity in the upper bound explanation on page 4, lines 128 and 129. The introduction of "n" in the O(r log n) bound on page 4, line 133, is also unclear. Figure (a) on page 8 is confusing due to the unclear measurement of "error" and the non-monotonic behavior of SPALS(alpha) with respect to alpha. Lastly, the dimensions of the tensor when n = 1000 are not explicitly stated, leaving ambiguity as to whether all three dimensions are equal to n.