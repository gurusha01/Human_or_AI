The authors investigate the problem of categorizing text through an extension of prior work on Word Mover's Distance (Wasserstein), incorporating supervised learning of word-specific weights. This study proposes utilizing Sinkhorn distance as a proxy for Wasserstein distance to enhance computational efficiency and demonstrates superior average performance across 8 datasets compared to 26 baseline methods. The paper is exceptionally well-written, providing a clear and in-depth examination of the mathematical underpinnings. However, potential drawbacks include the incremental novelty relative to the original Word Mover's Distance paper and the perplexing comparison to World Centroid Distance, which seems unnecessary. Furthermore, the discussion of lambda (the regularization factor) appears to be inadequate. In practice, the value of lambda significantly influences the outcome, often becoming the dominant factor in equation (2). Theoretically, with a sufficiently large lambda, results should approximate those of the pure Wasserstein distance, albeit at the cost of slower convergence. Nonetheless, the common practice of using a small lambda to accelerate convergence yields behavior distinct from Wasserstein, characteristic of Sinkhorn (entropic) behavior. It would be enlightening to understand whether, in this contribution, lambda drives the method towards pure Wasserstein or Sinkhorn behavior.