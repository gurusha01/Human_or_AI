This manuscript offers a compelling approximation that renders the information bottleneck method viable in high-dimensional settings, as demonstrated through experimental examples. The authors' technique, which relies on scalar products, is further enhanced by a nonlinear kernel-based generalization. Notably, the proposed kernel-information-bottleneck approach boasts an advantage over other kernel-based feature extraction methods, as it yields an intermediate representation that facilitates feature visualization. The synergy between the simplified information bottleneck and kernel generalization is a significant contribution worthy of publication at NIPS. Following a clear exposition of the proposed simplification and its kernel-based extension, the authors raise an intriguing point: unlike other feature extraction techniques that solely depend on the input signal's statistics, their information-bottleneck features also capture the relevant signal's statistics. It would be fascinating to explore the practical implications of this connection, such as how sparse features (e.g., Gabor-like edge detectors in natural images) adapt when the target signals are specific classes of images (e.g., faces). This could potentially be related to the top-down adaptation of early vision mechanisms, warranting further investigation.