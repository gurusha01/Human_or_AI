This manuscript examines accelerated first-order methods for constrained convex optimization in both continuous and discrete contexts, building upon the foundation established by Krichene et al. in [7]. The authors expand on the concept that Nesterov's method can be viewed as a discretization of an ODE system, which intertwines a primal trajectory with a dual one, where the primal trajectory emerges from a specific averaging process of the mirror of the dual variable. The current paper delves into more generalized averaging schemes, with Theorem 2 providing convergence rates under appropriate conditions for the averaging coefficients. Notably, in the continuous setting, the paper derives convergence rates that surpass 1/t^2. Furthermore, it explores an adaptive selection of weights that comes with convergence guarantees, which is then applied in the experimental section. This adaptive choice is compared against restarting techniques, known for their practical enhancement of convergence in accelerated methods. However, the theoretical analysis for the discrete algorithm version is not included in the discussion. Overall, I find this paper to be both interesting and well-crafted. The presentation of the new averaging scheme for accelerated methods in the continuous setting is rigorous and contributes meaningfully to the field. Although the core ideas behind the analysis of continuous dynamics have been well-established in preceding literature, this work introduces novel contributions and results. It would be pertinent to acknowledge the related work by Attouch, Peypouquet, and their coauthors in this context. The main theorem, which demonstrates the existence of averaging schemes that achieve faster convergence rates than 1/t^2, is particularly noteworthy. Nonetheless, it would be intriguing to investigate whether this enhanced rate persists when transitioning to the discrete procedure, a question that remains open.