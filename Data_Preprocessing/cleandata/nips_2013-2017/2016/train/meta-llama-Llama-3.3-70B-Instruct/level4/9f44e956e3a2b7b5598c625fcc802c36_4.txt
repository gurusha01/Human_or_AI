This manuscript introduces a novel approach to utilizing a dynamic secondary matrix of temporary weight values, termed "fast weights," in recurrent neural networks (RNNs) to retain a record of past inputs and network states. The experiments presented demonstrate that the incorporation of fast weights in RNNs yields improved performance. Notably, RNNs equipped with fast weights outperform other RNN variants with equivalent numbers of recurrent units on tasks such as associative retrieval of characters from strings and image recognition tasks, including MNIST and facial expression recognition. The method proposed for integrating fast weights into RNNs is elegant and results in noticeable performance enhancements in the presented experiments. The paper is, for the most part, well-written. Although some related prior work on fast weights exists, it appears that research in this area has not gained significant traction. This paper has the potential to revitalize interest in fast weights, which should be of considerable interest to the machine learning community. Furthermore, the developments and results presented in this paper draw inspiration from biology and may stimulate research in computational neuroscience and cognitive science. Upon reviewing the manuscript, several points of feedback, questions, and suggestions arose: The performance improvements demonstrated in the experimental results suggest that research on fast weights holds significant promise. It would have been informative to include an analysis or example of the low-level outcomes of fast weight operation, such as a resulting fast weight matrix, potentially in place of the description of the second image recognition task. An appendix detailing implementation specifics was mentioned in the paper but not submitted for review. The contents of the Conclusion section might be more appropriately described as, and restructured into, a Discussion section, potentially rendering an additional Conclusion section unnecessary. The following suggestions and comments pertain to the Conclusion/Discussion section: 1) Rather than stating that the paper contributes to a field of study, it might be preferable to refer to the specific developments as "contributions" and then elaborate on their significance and relevance to the respective fields of study. For instance, the contributions of this paper include a method for adapting fast weights in RNNs and the experimental results demonstrating improved RNN performance. The demonstration of improved RNN performance serves as evidence of the significance of these contributions to machine learning, and their relevance to computational neuroscience and cognitive science as a model and evidence that fast mechanisms of synaptic plasticity may contribute to sequential processing of stimuli and working memory. 2) The statement "Layer normalization makes this kind of attention work better" (Line 288) is mentioned earlier but not explicitly shown in the paper, which might make it more suitable as a reminder of the use of layer normalization rather than a conclusion. 3) References to "sequence-to-sequence RNNs used in machine translation" (Line 290) and the argument against neural networks as a model of higher-level cognitive abilities (Lines 294-297) would ideally be accompanied by citations. Several suggestions for presentation and typographical corrections are also offered: 1) The black-and-white printability of the paper could be enhanced by selecting thicker line weights for figures 1, 3, and 5, and utilizing different line styles or arrow heads to distinguish signals. Figure 5 would benefit from a larger font size for plots. 2) Line 203: remove "their." Page 8 contains instances where the opening quotation mark is incorrectly used as a closing quotation mark. Line 287: "unis" was likely intended to be "units." References [8] and [25] contain initialisms that should be capitalized.