This paper presents a novel approach to using Wasserstein distance as a cost function in Restricted Boltzmann Machines (RBMs), deviating from the conventional use of KL divergence. Through illustrative examples and promising experimental results, the authors demonstrate the advantages of employing Wasserstein distance. The direction taken by this paper in applying Wasserstein distance to machine learning is commendable, and the benefits of Wasserstein RBM, which considers the metric structure of {0,1}^d unlike classical RBM, are convincing. However, the writing could be improved for better clarity and accessibility to a broader audience. 
The current form of the paper is not self-contained, with numerous references to external sources, such as [4] and [5], which may necessitate additional reading for those unfamiliar with Wasserstein distance. For instance, introducing the optimal transport problem after equation 3, followed by the dual problem and the optimal dual alpha^\star, would enhance understanding. As it stands, the derivation of alpha^\star is unclear. Similarly, equation 6 would benefit from an explanation of alpha^\star(\tilde{x}_n) and its significance, rather than simply referencing [5]. 
The authors are advised to revise section 2 to make it more intuitive and self-contained. To conserve space, proofs could be relocated to supplementary materials, and section 3 could be streamlined. The proposed Wasserstein RBM, as an unsupervised learning method, introduces three additional hyper-parameters: gamma in equation 3 and lambda and eta at the end of page 4. The need to tune these hyper-parameters may limit the method's usefulness. It is essential to clarify whether lambda and eta are necessary for improving the energy surface for easier optimization or if their absence would result in a fundamentally different cost function minimum, leading to trivial solutions.
The well-defined nature of Wasserstein distance on the parameter manifold, including its boundaries, contrasts with KL divergence and has implications for de-noising problems. For instance, if an input dimension mostly consists of 0's with a few 1's, a Wasserstein RBM can potentially remove noise and make the dimension deterministic, whereas a KL-based RBM cannot. It would be interesting to know if similar observations were made in the experiments, as this could be a significant advantage of the proposed method.
Minor suggestions include reconsidering the title to avoid potential misunderstanding that the method improves the training of classical RBM, when in fact it proposes a different cost function and learning method. Equation 4 would be easier to follow if the sum were over (x, x') instead of (xx'). The centering of the dual potential at line 91 requires further explanation, as it is not immediately clear. Finally, the beginning of section 3 overlaps with section 1 and could be condensed for better flow.