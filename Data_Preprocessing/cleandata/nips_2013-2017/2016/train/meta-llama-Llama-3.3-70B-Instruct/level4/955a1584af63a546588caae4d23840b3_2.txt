This manuscript presents novel lower bounds for regression and classification tasks under the limited attributes observation model, where the learning algorithm only has access to a subset of attributes and aims to minimize excess loss relative to the Bayes optimal. The authors' contributions include tightened lower bounds for squared loss in this setting, as well as new lower bounds for absolute loss and hinge loss in classification. Additionally, they propose an algorithm that achieves a loss bound up to a certain precision limit, which matches the lower bound up to a polynomial factor in dimensionality for regression. The paper is well-written, and the results and proof techniques are sufficiently novel. However, the classification results seem less compelling due to an exponential gap between the lower bound and the algorithm's guaranteed precision limits for hinge loss. Furthermore, the authors do not provide further discussion on Theorem 5 after its initial statement in Section 2.1, which appears to have a different nature and seems weaker compared to Theorems 1 and 3 for regression. It would be beneficial for the authors to address this aspect and potentially include an explanation in the main paper regarding the challenges of obtaining a stronger result. Despite this, I recommend acceptance, having taken into account the author's response.