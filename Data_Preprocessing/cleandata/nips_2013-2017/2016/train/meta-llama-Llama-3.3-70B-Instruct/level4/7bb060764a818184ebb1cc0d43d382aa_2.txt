This paper introduces a novel dropout approach that samples features or neurons based on a multinomial distribution, where different features or neurons have distinct probabilities. For shallow learning, the authors propose a data-dependent dropout method, which utilizes the second-order statistics of the features to determine the sampling probabilities. Theoretical analysis of the risk bound is provided to support this approach. In the context of deep learning, an evolutional dropout method is proposed, which calculates the sampling probabilities for each layer using the second-order statistics of the layer's output based on a mini-batch of examples. Experimental results are presented to compare the proposed distribution-dependent dropout with standard dropout and batch normalization for both shallow and deep learning.
TECHNICAL QUALITY
The authors discuss in Section 1 that features with low or zero variance can be dropped more frequently or even completely. However, it is unclear how this intuition is supported by the theoretical analysis in Section 4, particularly in equations (8) and (9), considering that the features are not automatically zero-meaned. The uniform dropout scheme described in line 135, which is a special case of multinomial dropout when all sampling probabilities are equal, is similar but not identical to standard dropout, as the sampling probabilities for different features are not independent and identically distributed (i.i.d.). Using this scheme in experiments as if it were the standard dropout scheme may not be accurate. There are other concerns regarding the experiments, such as the lack of comparison with other improved dropout schemes, including the adaptive dropout method by Ba and Frey (NIPS 2013) and the variational dropout method by Kingma et al. [6]. The experiments only consider a dropout rate of 0.5, and it would be beneficial to report results for different dropout rates. For shallow learning, it is suggested that the authors include a setting for s-dropout using data after Z-normalization, as its performance is expected to be similar to that of d-dropout. The combination of e-dropout with batch normalization should also be included in the comparison, as the paper concludes that e-dropout is roughly a randomized version of batch normalization plus standard dropout. Additionally, the learning stability of e-dropout should be discussed, as its test accuracy fluctuates more than other methods, including BN+dropout, as the number of iterations increases.
NOVELTY
The main theoretical contribution of the paper is Theorem 1, which is proven using standard techniques for stochastic gradient descent. The extension to deep learning is straightforward, using mini-batches instead of the full data set to establish the sampling probabilities for each layer separately. Although no theoretical guarantee is available for the deep learning case, this is the first data-dependent dropout method proposed with theoretical justification, albeit only for shallow learning.
IMPACT
The experiments show that the performance gap between s-dropout and e-dropout is not always significant, and one may question whether the improvement obtained by distribution-dependent dropout will still be substantial for deeper networks. The paper only presents small-scale experiments for deep learning, which may not accurately represent more realistic deep learning applications that require much deeper networks. Consequently, the potential impact of this work on the deep learning community is unclear.
CLARITY & PRESENTATION
The paper is generally well-organized and easy to read. However, there are some issues with English usage, language, and formatting errors. Examples include: L22-23: "at random samples neurons and sets their outputs to be zeros" should be rephrased; L32: "can dropout" is incorrect; L58: "acts similar to" should be "acts similarly to"; L67: "a faster convergence" should be "faster convergence"; L73: "reminder" should be "remark"; L80 and L85: there are errors in using citations in LaTeX; L86: "to speed-up" should be "to speed up"; L93-94: "developed evolutional dropout" should be rephrased; L101: "in next section" should be "in the next section"; L111: "where he operator" is incorrect; L135: "we refer as" should be "we refer to"; L167-168: "a non-convex optimization" should be "a non-convex optimization problem"; L172: "does not loss the generality" should be "does not lose generality"; L179: "upper bond" should be "upper bound"; L181: "The detailed proof of theorem" should be rephrased; L185-186: "the second term also depend on" should be "the second term also depends on"; L191: "included in supplement" should be rephrased.