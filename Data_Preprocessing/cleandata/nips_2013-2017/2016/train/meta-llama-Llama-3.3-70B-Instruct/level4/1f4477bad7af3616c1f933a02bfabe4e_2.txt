This manuscript examines a specific type of feedforward neural network that can achieve global optimality with a linear convergence rate through the nonlinear spectral method. The approach is demonstrated on deep networks with one and two hidden layers, with experiments conducted on various real-world datasets. However, the class of neural networks under investigation is limited by the imposition of non-negativity on network weights and the maximization of weight regularization, which may not be practically reasonable. Furthermore, the requirement for a generalized polynomial activation function to achieve optimality conditions may not be widely applicable. Instead of making restrictive assumptions to attain global optimality, more detailed explanations from an application or real-world dataset perspective would be beneficial. The authors claim that the targeted neural networks retain sufficient expressive power to model complex decision boundaries and achieve good performance, but the empirical results do not strongly support this assertion. Notably, linear SVM outperforms NLSM1 and NLSM2 in cases where linear SVM appears to be suboptimally tuned, while NLSM tunes its network structure. It is surprising that a linear SVM is used as a baseline rather than a nonlinear SVM, such as polynomial kernel or Gaussian kernel, which makes the comparisons unfair and the conclusions unconvincing. The experimental datasets from the UCI repository are limited in terms of sample size and feature dimensionality, which raises concerns about the applicability of this method to high-dimensional data, given that the bounds on the spectral radius of matrix A grow with the number of hidden units. Addressing this issue is crucial for expanding the method's applicability. Additionally, ensuring the optimal condition without prior knowledge of parameter settings or an extremely large parameter space is problematic. A fast approach to identifying a small set of valid candidates would be beneficial. It would be interesting to investigate whether the optimal property of the model stems from the special objective function and whether stochastic gradient descent can empirically converge to global optimality on tested datasets.