This paper proposes a novel approach to bi-level modeling, where the input is mapped to a latent representation, and then to an output, with the latent layer typically being a discrete, potentially combinatorial structure. The methodology is relevant to problems such as transliteration in NLP, where the latent representation corresponds to a bipartite matching. The primary objective is to transform the learning problem into a convex optimization problem that can be efficiently solved, provided that any linear maximization subject to the constraints of the latent space (via the polar operator) can be efficiently solved. The authors acknowledge that this problem is usually formulated as a bi-level optimization, where the latent layer is first selected based on the input, and then the output is predicted based on the latent layer. The paper's key contribution lies in relaxing the discrete latent variables into continuous variables, leveraging duality properties to ultimately derive an SDP formulation. With suitable relaxations, the SDP can be convexified and solved efficiently. The paper tackles a substantial problem, and the approach appears to be sound. However, the technical derivations could be improved for better clarity. Specifically, the derivations on page 4 would benefit from a clear outline of the roadmap and the rationale behind the steps taken. Furthermore, there are certain claims that require clarification: 1) Equation 19 involves maximization with respect to π ∈ S, but the function being maximized is quadratic with respect to π; it is unclear how this is addressed, given the assumption that only linear functions over S are tractable. 2) The authors seem to incorrectly state that the rank relaxation is the only relaxation introduced, as they also relax the latent variable 'y' from a discrete to a continuous space with a perturbed term, which is a crucial relaxation assumption. These points need to be clarified to strengthen the paper's contributions.