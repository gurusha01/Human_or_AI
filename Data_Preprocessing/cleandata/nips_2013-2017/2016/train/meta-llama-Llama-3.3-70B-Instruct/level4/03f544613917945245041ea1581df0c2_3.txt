This paper presents a significant advancement in Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods by introducing Richardson-Romberg extrapolation to reduce asymptotic bias. SG-MCMC methods have gained popularity for Bayesian posterior inference in large datasets, despite being asymptotically biased. The proposed technique involves running two SG-MCMC chains in parallel, with the second chain having half the step size of the first and running for twice the number of steps. The Gaussian noise injected into the chains is correlated, following a specific formula. By subtracting the estimate from the first chain from the estimate of the second chain, the resulting estimate exhibits lower bias, with an order of O(gamma^2) compared to O(gamma) from individual chains. The method is straightforward, requires minimal modifications to the base SG-MCMC algorithm, and has the potential to significantly impact the SG-MCMC community. 
Although Richardson-Romberg extrapolation is well-established in numerical analysis, its application in the machine learning and SG-MCMC community is novel. The paper is well-written, and the presentation is clear. However, several questions and comments arise: 
- The applicability of this technique to all SG-MCMC methods is unclear, and it would be beneficial to specify any conditions beyond those outlined in section 2.2. 
- The potential extension of this method to other approximate MCMC schemes, such as sub-sampling based Metropolis-Hastings tests or Distributed SGLD, warrants exploration. 
- To mitigate wasted computation due to the idle chain with the larger step size, using three chains with different step sizes and running them for the same number of steps could be considered, although this may increase the variance of the estimate. 
- The possibility of extending this method to more than two step sizes is an interesting avenue for future research. 
- The effect of correlation between mini-batches on the variance of the estimate is not entirely clear, and further clarification would be helpful. 
- The matrix factorization experiment could be strengthened by running it until convergence and comparing the results to SGD, which would provide a more comprehensive understanding of the method's performance. 
- Including an algorithm box with pseudo-code early in the paper would enhance its accessibility to a broader audience, particularly practitioners who may not be familiar with the theoretical aspects of SG-MCMC methods. 
Overall, the paper presents a valuable contribution to the SG-MCMC community, and addressing these questions and comments could further improve its impact and clarity.