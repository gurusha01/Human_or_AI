The authors propose that in scenarios where the number of samples greatly exceeds the number of features (n >> p), the parameters of Generalized Linear Models (GLMs) can be approximated using a scaled Ordinary Least Squares (OLS) estimator. They introduce an algorithm leveraging this property and demonstrate through experiments that it outperforms several alternative algorithms in estimating GLM parameters, both in terms of speed and across various GLMs and datasets. Furthermore, the authors provide theoretical justification that this phenomenon applies beyond simple Gaussian designs, addressing a crucial problem of finding efficient surrogates for large-scale optimization issues. They highlight an elegant approach for estimating GLMs under the n >> p condition. Although the authors acknowledge that the fundamental concepts related to the Gaussian case are not novel, the extension to non-Gaussian designs is both valuable and intriguing. Overall, the paper is well-structured and clearly written. 
Key suggestions for improvement include:
- Enhancing Figure 1 by adding a plot of time versus accuracy for the compared methods, similar to Figure 2, to provide a comprehensive view of the trade-off between speed and accuracy.
- Examining the tightness of the inequality in Theorem 1, as it does not seem to recover the result of Proposition 1 when the covariates are Gaussian, and the authors should address this discrepancy.
- Clarifying the conditions under which |S| = n and |S| ≤ n are used, such as in Figure 1 and the experiments in Figure 2 and Table 1, to ensure consistency and transparency.
Minor corrections and suggestions:
- Correcting the spelling error on Line 159 from "Hayley" to "Halley".
- Defining λ_min in the main paper for Proposition 2 and Theorem 2 to ensure completeness and readability.