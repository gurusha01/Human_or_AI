This manuscript introduces an enhanced dropout methodology tailored for both shallow and deep learning architectures. Diverging from the traditional dropout technique that relies on uniform sampling, the proposed method employs a multi-normal distribution to determine which neurons to drop. The authors assert that this improved dropout technique achieves performance comparable to that of batch normalization. Several experiments have been undertaken to demonstrate the efficacy of the proposed approach. Overall, the concepts of data-dependent dropout and evolutionary dropout are commendable, and it is noteworthy that the method's performance parallels that of batch normalization. However, the experimental validation falls short of expectations. To strengthen the manuscript, it is recommended that additional results be included, potentially in tabular form, to facilitate a comprehensive comparison with state-of-the-art methods and to provide results on datasets like ImageNet.