This manuscript presents a novel approach to incorporating sparse priors into the information bottleneck framework for learning encoders and decoders, leveraging variational approximation to tackle the optimization problem. The proposed method is demonstrated to learn Gabor-like filters from image patches, akin to those obtained through traditional sparse coding techniques. Furthermore, the authors extend their method to accommodate non-linear encoders via a kernel-based approach. The efficacy of the proposed technique is showcased through an image inpainting task involving occluded handwritten digits. The paper is well-structured, and the methodologies employed are robust. It is intriguing to observe how the incorporation of sparse priors into the information bottleneck framework yields results visually comparable to conventional sparse coding. Nevertheless, the benefits of sparse priors are well-established, and various formulations, such as LASSO and sparse autoencoders, have already exploited this concept. To significantly enhance the paper, the authors should provide more in-depth analyses and quantitative comparisons to substantiate the advantages of the information bottleneck method. The proposed method bears a notable resemblance to sparse autoencoders, as both can yield linear encoders and decoders. The experiments conducted, primarily on 9x9 image patches, appear somewhat simplistic and lack comprehensive analysis, making it unclear under what circumstances the information bottleneck method would be preferred. The example applications presented, including image denoising and inpainting, can also be addressed using conventional sparse coding techniques, prompting the question of whether the information bottleneck method offers superior performance in these tasks. Overall, while the paper is sound, more extensive comparisons with existing methods are necessary to fully justify its value. Specific comments include: 
1. The notion of adjusting gamma to balance encoding quality and compression, as mentioned on line 32, can be similarly achieved in other sparse coding methods, such as by tuning the strength of l1-norm regularization in LASSO, which does not seem to be a unique attribute of sparse information bottleneck.
2. A key distinction between the proposed method and conventional sparse coding lies in the learning of specific input-output relationships. However, the process of selecting an appropriate output is not clearly elucidated.
3. On line 73, it appears there might be a typographical error, and the intended meaning could be q(y|r) = N(y|Ur), which requires clarification.