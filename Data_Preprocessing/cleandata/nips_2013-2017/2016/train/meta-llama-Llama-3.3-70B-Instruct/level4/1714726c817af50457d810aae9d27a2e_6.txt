This manuscript proposes an adaptive averaging heuristic designed to accelerate constrained convex optimization, offering a convergence rate preservation guarantee even when the objective function lacks strong convexity. This represents an improvement over existing restarting heuristics, as demonstrated through both theoretical analysis and experimental results. However, due to the specialized nature of the content, which falls outside my primary area of expertise, I encountered difficulties in fully grasping the paper's nuances. It is presumed that the targeted audience is familiar with the employed abbreviations and technical terms, but incorporating additional background information would enhance readability. Specifically, the equations presented between lines 33-34 and 46-47 would benefit from more intuitive explanations to facilitate understanding. Moreover, an exploration of the relationship between the proposed adaptive averaging method and those utilized in non-convex optimization, such as Adagrad and Adam, would provide valuable context. The practical implications of this research also warrant further discussion, particularly given that the experiments were conducted in a relatively low-dimensional space (R^3), which may not accurately reflect real-world scenarios. Therefore, insights into how the proposed method and related approaches scale to higher dimensions would be beneficial. On a minor note, the reference to the solid line in Figure 1 could be clarified by using a dotted line or color differentiation to illustrate the trajectory of the primal variable more effectively. Relevant literature includes the works of Duchi et al. on adaptive subgradient methods and Kingma and Ba on the Adam optimization method.