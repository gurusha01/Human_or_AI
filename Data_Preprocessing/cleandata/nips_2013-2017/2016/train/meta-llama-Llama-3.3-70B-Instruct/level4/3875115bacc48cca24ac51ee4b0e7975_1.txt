This paper examines the convergence properties of the Expectation-Maximization (EM) algorithm and related iterative heuristics for Maximum Likelihood Estimation in the context of learning mixtures of spherical Gaussians. The key findings presented include: 
1. The EM algorithm can become stuck in local optima even with an infinite number of samples and well-separated clusters, as demonstrated by a mixture of three clusters with significant distance between them.
2. For mixtures comprising k Gaussians, the probability of successful convergence using random initialization for EM is at most exp(-Ω(k)), indicating a rapidly diminishing likelihood of success as the number of components increases.
3. The analysis of Gradient EM reveals that it generically avoids convergence to strict saddle points, suggesting that the primary issue lies with bad local maxima.
The paper cohesively presents these results to demonstrate that EM does not converge to global maxima, even with infinite samples. However, the reviewer finds these outcomes somewhat predictable, with the methodologies employed following anticipated lines. A primary example illustrating this involves a mixture of three components, where two are proximal and one is distant, leading to a local maximum with one center near the first two components and two centers near the distant cluster. This behavior is foreseeable, especially when the separation between centers exceeds √d, causing the problem to resemble k-means clustering, where similar challenges with local maxima are known.
The extension of this example to k components, while requiring effort, is also considered somewhat expected. Similarly, the failure of random initialization parallels the issues seen in k-means clustering, with comparable failure probabilities. In conclusion, while the paper tells a consistent story about the non-convergence to global optima, it largely aligns with anticipated outcomes.
Notable comments include the observation that the Kumar-Kannan work provides convergence guarantees for Lloyd's heuristic in k-means clustering, which, under sufficient separation conditions, shares similarities with mixtures of Gaussians. A comparison contrasting the EM algorithm's behavior with that of k-means and Lloyd's heuristic, especially under large separation conditions, would be beneficial for contextualizing these results.