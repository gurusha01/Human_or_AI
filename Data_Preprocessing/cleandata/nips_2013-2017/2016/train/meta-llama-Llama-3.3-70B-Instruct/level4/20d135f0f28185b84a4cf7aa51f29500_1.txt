This manuscript presents a novel stochastic gradient descent approach for training ensemble models, extending the work of Guzman-Rivera et al [8] by utilizing the loss of the best output from a single classifier within the ensemble to foster diversity among classifiers. The key contribution lies in the application of stochastic gradient descent, enabling the integration of this algorithm with deep neural networks. The paper is well-structured and engaging. However, my primary concern revolves around the novelty of the algorithm, questioning whether it offers more than an incremental improvement over [8]. The authors successfully argue that their modification significantly enhances the algorithm's utility, which inclines me towards recommending acceptance. A potential avenue for further exploration could involve comparing the MCL algorithm's performance when using batches versus the entire dataset. Additionally, more in-depth analysis on the "diversity" effect would provide valuable insights, further strengthening the manuscript.