This paper proposes the utilization of linear programming to generate adversarial examples for ReLU networks, while also establishing metrics for robustness evaluation. Notably, it differentiates between the methods used for generating adversarial examples and those for assessing robustness. Given the current fascination with the robustness properties of deep neural networks, this topic is undoubtedly worthy of exploration. The paper's clarity and coherence make it easily comprehensible, and the results presented are persuasive, suggesting that deep learning researchers would benefit significantly from this work. However, two primary concerns arise: Firstly, the reliance of the proposed approach on the linear properties of ReLU networks limits its applicability, as it leverages linear programming to find adversarial examples. Secondly, although the methods outlined in the paper appear to outperform previous approaches in identifying adversarial examples, they fail to yield substantial improvements in test set performance. Furthermore, the authors do not demonstrate the superiority of their method on datasets beyond MNIST. Additional minor observations include the formalization of linear constraints, which seems unnecessarily complex. It is apparent that at any given point x, only one of the disjunctive constraints is necessary, rendering the introduction of disjunctions, only to subsequently dismiss them, somewhat perplexing.