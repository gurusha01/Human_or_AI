This study examines the representational capabilities of unitary matrices employed in recently proposed unitary evolution Recurrent Neural Networks (RNNs) and demonstrates how to optimize a complete, full-capacity unitary matrix. Initially, a specific combination of individual unitary matrices (diagonal, Householder reflection, permutation, and Discrete Fourier Transform (DFT)) was proposed, but the current paper reveals that this composition is limited in its ability to represent the entire unitary group when the hidden state dimension exceeds 22. Previous research in optics has shown that a unitary matrix can be represented as a product of at most comb(N,2) Givens operators. The current work introduces the concept of Givens capacity, defined as the minimum number of operators required to construct a unitary matrix, which is upper-bounded by comb(N,2). The authors demonstrate how to optimize full-capacity unitary matrices using gradient descent, leveraging a method discussed in a 2011 technical report. This full-capacity optimization is applied to three tasks and compared to the restricted-capacity unitary matrices proposed in the original work, as well as the Long Short-Term Memory (LSTM) network. 
The synthetic data system identification task involves comparing the restricted-capacity unitary RNN to the full-capacity uRNN in learning the dynamics of a target uRNN given only input and output samples. The Mean Squared Error (MSE) objective is used for training, and the results show that the restricted-capacity unitary RNN achieves worse performance for all tested hidden dimensions. The copy memory problem, which requires outputting the first 10 inputs after long delays of 1000 and 2000 steps, demonstrates superior performance of the full-capacity unitary RNN over the restricted-capacity and LSTM networks. 
Finally, a prediction problem involving estimating the log magnitude of the short-time Fourier transform of a speech sample given all previous Short-Time Fourier Transforms (STFTs) is evaluated using the TIMIT corpus. The results are assessed using objective measures, including MSE, segmental Signal-to-Noise Ratio (SNR), and perceptual metrics such as Short-Time Objective Intelligibility (STOI) and Perceptual Evaluation of Speech Quality (PESQ). While there is little difference between the restricted-capacity and full-capacity unitary RNNs in terms of MSE, both outperform the LSTM substantially. For SNR, STOI, and PESQ, the full-capacity unitary RNN slightly outperforms the restricted-capacity unitary RNN, with both being significantly better than the LSTM. 
The uRNN is a more elegant solution to the vanishing and exploding gradients problem in RNNs compared to the LSTM. With the proposed full-capacity optimization, an even larger performance gap is opened up between the uRNN and the LSTM, compared to the original findings with the restricted-capacity uRNN. Notably, increasing the hidden state size of the LSTM to match that of the uRNN in the copy memory problem seems to close the gap between the LSTM and the restricted-capacity uRNN, but both still lag behind at T=1000, whereas the restricted-capacity uRNN previously solved the problem rapidly and perfectly at T=500. It is worth investigating whether further increasing the hidden dimension of the restricted-capacity and LSTM networks improves their performance on the copy memory problem. Additionally, it may be necessary to put more effort into optimizing the LSTM, such as using adaptive optimization methods like Adam, and exploring the need for gradient clipping. Furthermore, there appears to be a missing word on line 101, which requires clarification to ensure the text's meaning is preserved.