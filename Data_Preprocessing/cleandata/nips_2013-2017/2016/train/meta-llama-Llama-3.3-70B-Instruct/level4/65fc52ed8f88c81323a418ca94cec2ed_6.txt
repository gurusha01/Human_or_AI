This manuscript introduces an unsupervised deep learning approach, building upon exemplar-based similarity learning methods while addressing inherent limitations associated with these techniques (as discussed in lines 91-101). The proposed methodology involves a two-stage process: (1) dividing the dataset into compact cliques, and (2) optimizing the neural network using a softmax loss function based on pseudo-clique labels. The experimental results on posture analysis and pose estimation tasks show promising performance, often rivaling that of fully supervised methods. From a technical standpoint, the method appears sound, effectively mitigating issues associated with traditional exemplar-based approaches through its "offline" batch and clique data partitioning strategy, as evidenced by the compelling experimental outcomes. However, a concern arises regarding the potential impact of this alternating approach on the efficiency and automation of the learning process, particularly if additional hyper-parameter tuning is necessary across iterations or epochs. Further discussion on these aspects, as well as an analysis of how network performance evolves over iterations and epochs, would be beneficial. Additionally, clarifications and improvements in presentation are needed, such as explaining the functions f() and r() in Equation (7), addressing the imbalance in the ratio of one exemplar to many negatives (line 93), and providing more insight into learning similarities between all pairs of a large number of exemplars (line 88).