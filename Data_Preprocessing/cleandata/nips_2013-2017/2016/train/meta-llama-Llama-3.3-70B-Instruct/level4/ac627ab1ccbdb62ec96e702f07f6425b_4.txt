This manuscript presents an unsupervised domain adaptation approach that utilizes residual connections to equate the target classifier to the source classifier plus a residual function, differing from recent deep adaptation techniques that learn a unified source and target model. The authors also employ kernel Maximum Mean Discrepancy (MMD) to align domain representations, similar to the related Domain Adversarial Network (DAN) and Deep Domain Confusion (DDC) methods. The concept of learning a residual difference to generate a target classifier is innovative, straightforward, and intriguing. The paper is well-structured, and the methodology is clearly explained. However, the approach comprises three key components: entropy loss on the final target scores, MMD loss for domain invariance, and residual connections. Experimental results suggest that the entropy loss provides the most significant benefit, followed by MMD loss, and then the residual connection. Although the MMD loss is not novel, and the entropy loss and residual connection require further clarification, the methodology appears effective. Nonetheless, it remains unclear whether the innovative aspects of the approach substantially contribute to the overall system's performance.