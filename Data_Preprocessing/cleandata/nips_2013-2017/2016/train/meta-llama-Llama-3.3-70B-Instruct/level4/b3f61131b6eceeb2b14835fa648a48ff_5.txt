This paper introduces a relaxed version of the standard no-regret property in online algorithms, referred to as the low approximate regret property, which permits a small multiplicative approximation factor of regret. This property is applicable to numerous learning algorithms, and the flexibility of the multiplicative term enables a reduction in the error term, as the standard error term can be incorporated into the multiplicative factor. The resulting approximate low regret outcomes still yield price of anarchy guarantees in (\lambda-\mu) smooth games, with a smaller error term achievable after a shorter number of periods. Notably, the results only require realized feedback or, in some cases, bandit feedback, rather than the expectation over actions of other players. The technique of converting the additive error term to a multiplicative error term is intriguing and well-utilized in the paper. However, the significant technical overlap with SAL15 and LST16 diminishes my enthusiasm for an oral presentation. Furthermore, I believe the paper's language occasionally exaggerates the results, creating a distraction. The claim that learning converges rapidly to equilibria in games is misleading, as the actual contribution lies in the relaxation of convergence notions, from behavioral convergence to convergence of time-averages, and the targeting of states where agents exhibit low approximate regret, which still satisfies the (\lambda-\mu) smoothness results. The technical aspects are interesting in their own right, but the overextension of convergence and equilibrium interpretations to achieve faster "convergence" detracts from these points. Additionally, if this result is intended to be general, what are the practical implications for specific classes of games, such as two-player games or even 2x2 games?