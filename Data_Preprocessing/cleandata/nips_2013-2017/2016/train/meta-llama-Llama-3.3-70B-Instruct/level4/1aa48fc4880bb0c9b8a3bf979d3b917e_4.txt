This manuscript presents an extension of the stochastic variance reduction techniques, specifically SVRG and SAGA, to tackle strongly convex-concave saddle-point problems, achieving linear convergence. Theoretical and empirical evidence collectively demonstrate the efficacy of the variance reduction approach in addressing saddle-point problems. By building upon existing stochastic variance reduction methods, the authors successfully adapt them to handle a broader range of non-separable saddle-point problems, where both the loss function (as seen in structured output prediction) and the regularizer (such as a fusion term) can be non-separable. A key contribution of this work lies in its convergence analysis for saddle-point problems, which is distinctly more complex than that for convex problems, and is adeptly addressed through the utilization of monotone operators. This analysis further reveals the applicability of the extended SVRG and SAGA algorithms to a wider array of problems, including variational inequalities in game theory. The authors propose two distinct splits of the gradient operator - an element-wise split for SVRG and a factored split for SAGA - within a stochastic framework. The variance reduction technique is applied to stochastic forward-backward algorithms, commonly employed for saddle-point problems, with consideration given to non-uniform sampling. Both theoretical and empirical results underscore the superiority of variance reduction with non-uniform sampling over its uniform counterpart. The experimental results, presented in both the main body and the appendix, provide substantial validation of the theoretical findings and demonstrate the effectiveness of the proposed methodologies on both non-separable loss and regularizer. Some minor observations include: the acceleration method outlined in Section 5, which follows the approach in [8] by incorporating an additional regularization term to align updates with an iterate point, lacks clarity on how to adaptively update this iterate point, (\bar{x}, \bar{y}), for acceleration purposes. Furthermore, while a convergence and acceleration proof is provided for SVRG, it remains unclear whether analogous theoretical results hold for SAGA. Lastly, to avoid confusion, distinct symbols should be used for the smooth function K(.) and the design matrix K.