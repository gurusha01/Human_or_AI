This paper presents a novel exploration strategy for deep reinforcement learning (RL), leveraging a sequential density model of the state space in the Arcade Learning Environment (ALE). The authors introduce a 'pseudo-count' concept, allegedly establishing a connection between information gain, prediction gain, and this proposed quantity. They demonstrate significant breakthroughs in ALE, notably in Montezuma's Revenge, by revisiting and improving upon existing ideas. The paper is well-structured, with robust experimental results, but requires additional clarification on several points. 
Firstly, the pseudo-count quantity, derived from the recoding probability of a density model, is shown to be consistent with empirical counts. However, it is unclear how updating the density model with a different state element affects the pseudo-count of a specific state. For instance, if a state element x corresponds to a pseudo-count N(x), updating the model with a different element y could potentially decrease N(x), which seems inconsistent with empirical counts. This concern appears to be related to the fluctuations in pseudo-count observed in Figure 1 (right).
Furthermore, the notion of pseudo-count seems somewhat redundant, as the authors could potentially derive prediction gain and subsequently information gain without requiring additional data. Information gain is a well-established concept in intrinsic motivation, raising the question of why pseudo-count is necessary. The pseudo-count appears to be a loose bound on prediction and information gain, which may compromise its stability. If the pseudo-count measure were removed, the paper would essentially propose measuring updates to the density model as an intrinsic reward, which seems a more straightforward approach.
Additionally, the authors' definition of information gain appears to differ from the conventional definition used in the literature, such as in "Planning to Be Surprised" by Sun et al. (2011). Specifically, their definition seems to allow an agent to accumulate infinite information over time by following a predefined policy without actual exploration, which seems invalid. If the authors' information gain quantity is indeed distinct from the established concept, it is essential to relate it to the existing literature or clearly articulate the added value of their contribution, namely, the relationship between information gain and pseudo-counts.
Minor suggestions for improvement include clarifying Corollary 1, which is somewhat dense, and providing insight into why the pseudo-count increases during periods without salient events in Figure 1 (left), potentially due to generalization across states. Overall, while the paper presents significant advancements, addressing these points would enhance its clarity and robustness.