This manuscript presents a novel approach by incorporating a "reviewer" component into attentive recurrent encoder-decoder architectures. The core concept revolves around enhancing the attentive decoder's performance in tasks such as image and source code captioning by leveraging global summary vectors, termed "facts." Following the encoder's processing of a sequence, the reviewer module performs additional passes, denoted as T_{r}, over the encoded sequence. Each pass results in the addition of a new vector to the cumulative set of facts, F. The reviewer module can be trained using an auxiliary task, such as predicting vocabulary word presence, or through end-to-end training for decoding purposes. The proposed encoder-reviewer-decoder (ERD) variants demonstrate superior performance compared to the baseline attentive encoder-decoder model and remain competitive with state-of-the-art models in image captioning. The modification to the attentive encoder-decoder network architecture appears sensible and yields a performance improvement in image captioning. However, it remains unclear whether the observed benefits stem from the increased model capacity or the specific architectural design of extracting global "fact" vectors prior to decoding. Overall, the approach seems to offer a marginal improvement but does not constitute a groundbreaking finding. Key questions and comments include: 
- Would discriminative supervision also enhance the performance of the attentive encoder-decoder?
- How does the ERD model compare to an attentive encoder-decoder with additional layers?
- Currently, all facts are derived solely from the image and are potentially grounded in a discriminative task. Are there alternative methods to extract a set of factors that could utilize a knowledge base or other sources beyond the image alone, such as prior knowledge about objects in the scene and their common-sense relationships?