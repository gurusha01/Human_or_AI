This manuscript provides an in-depth examination of a broad range of learning algorithms in smooth games, showcasing rapid convergence. The finding is noteworthy within the realm of game-theoretical control, as conventional results typically establish convergence without specifying the rate, which is a relatively novel aspect. By integrating techniques from learning theory with outcomes rooted in the price of anarchy in smooth games, the authors offer a unique perspective. The results are likely to resonate with the game-theoretical control community. As someone familiar with both game theory and learning theory, I found the paper engaging. The novelty and interest of the results make them significant for the field of game-theoretical control. The paper covers a substantial amount of material, including fast learning under full information feedback and bandit feedback, both in expectation and with high probability, as well as analogous results in dynamic population games. However, this comprehensive coverage also constitutes the paper's primary weakness. The attempt to condense so much content into a limited space has resulted in poor motivation, inadequate definition of terms, and a challenge in reconciling the specialized knowledge required to appreciate the work with the broader scope of a conference like NIPS. Following the rebuttal: Considering my criticism regarding the excessive amount of material and your rebuttal, it might be beneficial to revise the introduction to place greater emphasis on the major contribution, which is demonstrating the advantages of considering low approximate regret, and to provide illustrative examples of its potential. Initially, I interpreted the paper as primarily focusing on Fast Convergence of Common Learning Algorithms in Games, which turned out to be incorrect. A potential title change could also be considered to better reflect the core contribution.