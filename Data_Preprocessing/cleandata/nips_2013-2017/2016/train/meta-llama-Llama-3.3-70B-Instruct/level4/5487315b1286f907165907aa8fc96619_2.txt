A novel convex training formulation is presented for a two-layer model, characterized by the conditional probabilities p(z|y) and p(y|x), where x represents the input, y denotes the latent space, and z signifies the output. Unlike existing approaches, the authors only necessitate tractable Maximum A Posteriori (MAP) inference on p(y|x), which is readily available for graph-matching type models or integer linear programs with totally unimodular constraint sets. The traditional method for two-layer models involves bi-level optimization, solving p(z|\argmax_y p(y|x)), which entails optimizing the encoding model over y in an inner step and then inferring the solution over the decoder in an outer step. However, training the model parameters is challenging due to the discrete nature of the latent representation y, resulting in zero gradients almost everywhere. The proposed approach addresses this by incorporating the first-order optimality conditions of the inner optimization into the outer optimization using Lagrange multipliers and a saddle-point formulation. Although the resulting problem is non-convex in the training parameters, a semi-definite programming (SDP) formulation is introduced, yielding a convex relaxation after dropping the rank constraint. This enables learning to depend solely on the MAP operation over the latent space, assumed to be efficient. The method is demonstrated on transliteration and image inpainting tasks, outperforming the presented baselines.
Review summary: The proposed technique offers a promising solution to circumvent issues arising from bi-level optimization in probabilistic modeling. However, several details are lacking to fully assess the quality of the proposed solution, including potential additional assumptions, unexplained important aspects, and a limited experimental evaluation. 
Technical quality: The derivations are sound, but additional assumptions may be necessary to ensure efficiency. 
Novelty: The approach presents a novel method for two-layer modeling.
Potential impact: The potential impact is difficult to assess due to the lack of details regarding the efficiency of the proposed method.
Clarity: The explanation of some important aspects is lacking.
Review comments:
1. Assuming z is discrete, the decoder probability in Eq. (4) depends on the function G, which should correspond to the log-partition function to ensure proper normalization. The proposed formulation requires maximization over its dual, involving entropy terms and simplex constraints. The tractability of this maximization task seems to be assumed; however, the resulting program appears to involve quadratic and entropy terms, along with simplex constraints. The authors should comment on how this task is solved efficiently, as Section 5 lacks details.
2. Although not part of the proposed approach, the replacement of the partition function `\Omega(Ux)' with an upper bound `\max -y'Ux' in line 113 may require verification. The maximization might not be an upper bound, as `\Omega(Ux) = \log\sumy\exp(-y'Ux) = -y^\ast'UX + \log\sumy\exp(-y'Ux + y^\ast'Ux)', where `y^\ast = \arg\max_y -y'Ux'. The log term can be positive or negative, suggesting the maximization is not a bound.
3. The introduction of bound constraints on the model parameters in Eq. (10) seems counterintuitive, given the absence of constraints in the original formulation (Eq. (7)). A more detailed explanation beyond "to simplify the presentation" would be beneficial, as the reason for these constraints is unclear.
4. The authors should comment on the quality of the proposed baselines, particularly the comparison to an approach from 2010 on the transliteration task. More recent baselines would provide a more comprehensive evaluation.
5. The computational complexity of the proposed method, which utilizes an SDP formulation, should be discussed, as it may be computationally expensive and not scalable to larger problem sizes.
6. A comparison between the proposed SDP approach and gradient descent on the objective given in Eq. (15) would be interesting to assess the effectiveness of the method.
Minor comments:
- The authors could clarify that the `^\' operator denotes the transpose.