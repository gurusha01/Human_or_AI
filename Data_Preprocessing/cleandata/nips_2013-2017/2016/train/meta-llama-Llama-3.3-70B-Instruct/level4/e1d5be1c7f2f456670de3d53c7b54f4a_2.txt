The authors investigate the problem of contextual semi-bandits, focusing on the development of efficient algorithms when a supervised learning oracle is available. They examine two scenarios: one where the reward is a known linear function of individual feedback, and another where this linear transformation is unknown. For the known transformation case, the proposed algorithm bears similarities to the one presented by Agarwal et al [1], albeit with distinct differences. In contrast, the unknown transformation case is tackled as a phased explore-exploit problem, where the reward vector is first estimated and then utilized in conjunction with an optimal policy. The paper is generally well-written. Several points warrant further consideration: 
1) The original problem is framed as a feasibility problem. An alternative approach could involve formulating an optimization problem, where the goal is to identify the policy that minimizes empirical regret, subject to variance constraints (i.e., optimizing the left-hand side of equation 4, under the constraint of equation 5). It is plausible that the Adaptive Mirror Descent (AMO) method is better suited for feasibility problems, and explicitly stating this would be beneficial. 
2) A more detailed explanation of how the AMO is integrated into the paper's main framework would be valuable. For instance, if the policy class consists of linear classifiers, it would be helpful to understand the specific role of the AMO in this context.