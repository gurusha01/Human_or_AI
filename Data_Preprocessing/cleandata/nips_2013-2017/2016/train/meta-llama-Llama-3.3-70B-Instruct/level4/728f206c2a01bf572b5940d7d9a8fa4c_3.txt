This manuscript presents a novel approach to training Boltzmann machines, utilizing the Wasserstein distance between the data and model distributions as an alternative to the traditional log likelihood method. The concept is well-articulated and intriguing, with potential for significant impact due to the importance of novel unsupervised training objectives. However, concerns arise regarding the scalability of this sample-based approach, particularly with increasing dimensionality and sample size. The experiments conducted on small datasets, heavily regularized with standard KL divergence, only demonstrated improvements in terms of Wasserstein-like performance measures.
Equation 4 requires clarification, specifically the summation over "x, x'" instead of "xx'", and its definition appears to be restricted to gamma > 0. The last term's role in inducing correct dependencies is also unclear, suggesting the need for additional explanatory text and a revised gamma bound.
Several notation and reference issues were encountered, including the undefined term "alpha*" on line 67. Furthermore, the approach's reliance on sample-based distance measurements between distributions may be susceptible to the same limitations as Parzen window estimates, such as requiring an exponentially large sample size with increasing dimensionality and introducing bias towards the data distribution's mean when samples are scarce.
The suitability of this method for minibatch training is questioned, particularly in light of the comments on line 115. The relative impact of local minima on the Wasserstein and KL cases is also unclear, as stated on lines 121-122. The small size of the datasets used in the experiments raises concerns about the algorithm's scaling properties, both in terms of sample size and problem dimensionality, as noted on lines 137-139.
The clustering observed appears to be a result of the entropy regularizer rather than the Wasserstein distance itself, as suggested on lines 177-178. The shrinkage property discussed in section 4.3 may not be desirable, especially in higher-dimensional problems, where it could be exacerbated. The performance measures presented in section 4.4 are limited to Wasserstein or Hamming distance, which may not provide a comprehensive evaluation.
In response to the rebuttal, the discussion on scaling properties should be included in the final paper. Additionally, the statement "When γ=0, we recover the usual OT dual constraints" should be revised to reflect the limit as γ approaches 0, rather than being equal to 0.