The paper presents a novel manifold learning approach that aims to embed data in a low-dimensional space, where the data lies on a submanifold of the embedding space. This is accomplished through gradient-based optimization of a straightforward energy function that measures the deviation of the intrinsic Riemannian metric from the identity. Although this concept is innovative, its practical utility is not clearly established. The paper's primary contribution lies in the idea that the intrinsic manifold dimension can be lower than the embedding dimension, which is a new concept but lacks sufficient motivation. The authors' sole justification appears to be the potential for reduced distortion. Several concerns arise from this: (1) it is evident that lower distortions can be achieved by embedding a d-dimensional manifold in R^s (where s > d) rather than R^d, which is a straightforward observation; (2) it is unclear why existing methods like Isomap and its variants, which embed data in R^s, are not sufficient; and (3) the usefulness of this distinction is not well-established. From a mathematical perspective, the idea is helpful, but it is uncertain which real-world problems this approach can solve that were previously unsolvable.