This paper expands on prior research linking accelerated Mirror Descent to continuous-time ODE by introducing a more general averaging scheme and a novel adaptive averaging strategy for its discretization. The new adaptive averaging variant of AMD outperforms existing algorithms with restarting in certain scenarios. The proposed strategy is innovative and intuitively sound. However, the overall paper raises concerns regarding its originality and usefulness. 
Many components, such as the Lyapunov function and discretization, are directly derived from an earlier NIPS paper [7], making the extension to generalized averaging seem somewhat trivial, especially since it only considers quadratic rates. Faster rates would necessitate higher-order methods, which may not always be applicable. 
Some findings, including the interpretation of the energy function and the reformulation into primal form, contribute to a deeper understanding of accelerated Mirror Descent but do not significantly advance machine learning. The introduction of adaptive averaging to ensure ODE convergence is not fully theoretically grounded due to discretization approximations, and no explicit convergence results are provided, rendering it heuristic. 
Furthermore, without comparing this adaptive variant to other averaging schemes for Nesterov's algorithms, its impact remains unclear. The experimental results on a simple quadratic/linear objective toy example are not convincing in demonstrating the advantages of adaptive averaging, particularly for strongly convex cases. 
Following the authors' rebuttal, which promises to include additional examples, a theoretical proof of convergence for adaptive averaging, its implementation on Nesterov's cubic-regularized Newton method, and a comparison with other adaptive averaging strategies, I have revised my evaluation accordingly.