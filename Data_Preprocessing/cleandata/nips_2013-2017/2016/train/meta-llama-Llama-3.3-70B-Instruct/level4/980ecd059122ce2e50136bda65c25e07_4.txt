This manuscript presents a novel algorithm for generating adversarial examples for neural networks (NNs), specifically images with subtle perturbations that are misclassified by the NN. The algorithm is designed to constrain the adversarial examples within the linear region of the ReLU activations in the NN. Experimental results on the MNIST dataset demonstrate that the proposed algorithm produces more damaging adversarial examples for the LeNet NN compared to previous works, as measured by the L_inf norm of the perturbation. Similar to existing research [5,20], this paper introduces a new algorithm for generating adversarial examples and utilizes them to re-train the NN. The proposed algorithm differs from previous approaches by incorporating additional constraints into the optimization process. However, I have two primary concerns regarding the motivation behind these constraints: 
From a theoretical perspective, the constraints added to the algorithm are justified by the claim that "adversarial examples exist due to the linearity in the neural net [5], so we restrict our search to the region around the input where the neural net is linear." Nevertheless, adversarial examples have also been discovered outside these linear regions, as evidenced by the fact that multiplying the adversarial perturbation by a relatively large constant still negatively impacts the NN [5]. Furthermore, there are no theoretical guarantees that constraining the adversarial examples to this linear region will yield the minimum perturbation. It is possible that adversarial examples with smaller norms exist outside the search space defined by the algorithm presented in this paper.
From an experimental standpoint, the conclusions drawn from the results appear to be limited in their generality. The NN architectures employed are not state-of-the-art or standard, and are applied to relatively simple datasets (MNIST and CIFAR). Additionally, there is no verification that the baseline results from [20] have been accurately reproduced. The results show only marginal improvements on the MNIST dataset compared to the baseline, and the comparison with the baseline is omitted for the CIFAR dataset. 
Other notable comments include: 
- The results on the CIFAR dataset suggest that this method may have limited applicability in reducing adversarial examples, as it requires approximately 15 seconds to generate one adversarial example using 8 CPUs, which would make it challenging to scale to larger datasets like ImageNet and generate sufficient adversarial examples for re-training the NN.
- The variable $x_\star$ is introduced without prior definition.
- The assumption that the L_inf norm is the optimal measure for analyzing the perceptibility of adversarial perturbations is not justified.
- The mention of "[3] seeking to explain why neural nets may generalize well despite poor robustness properties" could benefit from a more detailed explanation, potentially providing an alternative motivation for the constraints based on [3].