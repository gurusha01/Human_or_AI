The paper proposes a novel approach to generating adversarial examples for neural networks (NNs) that deviate from the traditional direction of the signed gradient of incorrect labels. By encoding the NN as a set of constraints, the authors formulate a linear programming (LP) problem, whose feasible solution yields both an adversarial example and a measure of robustness. Additionally, they introduce a formal definition of robustness and propose two new statistical measures for it. However, several concerns arise: 
1) The equation between lines 212-213 appears to have switched conditions, which requires clarification. 
2) The authors criticize current robustness estimation methods for being biased due to their reliance on data produced by the algorithm being inspected, yet their proposed method still uses its own algorithm to measure rho, which seems similarly flawed. 
3) The claim of reduced bias due to the discovery of more adverse examples lacks substantial support and warrants further justification. 
4) The algorithm's bias is suspected due to its deterministic removal of disjunction constraints, always choosing the one not satisfied by the seed, prompting the question of whether a stochastic version could mitigate this by randomly selecting between disjunctions. 
5) The paper's readability is compromised by insufficient formulation of symbols and equations, such as the dimensionality of x* and the meaning of (.)_i in line 147, which could refer to either rows or columns. 
6) A correction is suggested for line 64, where "x with true label l" seems more appropriate. Overall, while the paper presents an innovative approach, it requires significant clarification and addressing of the mentioned concerns to enhance its validity and readability.