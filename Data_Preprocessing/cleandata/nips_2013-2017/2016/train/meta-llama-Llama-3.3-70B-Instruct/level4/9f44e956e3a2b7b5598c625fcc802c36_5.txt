This paper introduces a novel concept to ordinary recurrent neural networks by incorporating a second type of synapses, referred to as "fast weights," which update at a pace slower than neural activities but faster than conventional synapses, or "slow weights." This approach is grounded in physiological evidence suggesting that synaptic modulation in the brain occurs at multiple time scales. The fast weights are utilized within an associative network designed to store memories, with their update rule following a Hebbian-like learning principle aimed at recalling past hidden activities. This mechanism functions similarly to attention mechanisms used to enhance sequence-to-sequence RNNs, but with the distinction that the strength of attention to past hidden activities is determined by the scalar product of past and current hidden activities, rather than being specified by additional parameters. The algorithm's efficacy is demonstrated across a range of tasks, with mathematical arguments regarding memory capacity and neuroscientific insights into synaptic plasticity time scales supporting the introduction of fast weights. The proposed model establishes connections with prior work on memory and attention mechanisms while offering a more biologically plausible approach, potentially bridging the gap between computational models and neuroscience. From a computational standpoint, an efficient trick avoids the need to compute the full fast weight matrix, allowing the algorithm to apply to mini-batches and enhancing its efficiency. However, the training process for slow weights is not detailed extensively, with only the mention of using the Adam optimizer. The addition of a detailed computational graph figure would be beneficial for clarity, especially given the inner loop for computing \(h_s(t)\), which complicates the visualization of automatic differentiation. A specific question arises regarding whether gradients are backpropagated through \(A(t)\), which is a function of \(h(\tau)\) and thus of \(W\) and \(C\), or if these variables are treated as constants during gradient computation. Lastly, a minor typo is noted on line 287, referencing "units."