The authors introduce stochastic gradient Richardson-Romberg MCMC as a means to mitigate the bias associated with SG-MCMC, leveraging Richardson-Romberg extrapolation, a readily available technique for stochastic differential equations (SDEs). Notably, this approach, termed SG-RR-MCMC, operates by running two chains in parallel without the need for inter-chain communication. A thorough theoretical examination confirms the asymptotic consistency of SG-RR-MCMC. Moreover, empirical findings demonstrate satisfactory performance, underscoring the efficacy of this concept. Essentially, SG-RR-MCMC can be viewed as an innovative fusion of Richardson-Romberg extrapolation and stochastic gradient Langevin dynamics (SGLD), yielding promising results. Theoretical validation of its efficiency is achievable through the demonstration of an improved convergence rate, comparable to what could be attained using a high-order integrator, although a comparative analysis with such an integrator in experimental settings is notably absent. However, discrepancies are observed, including an inconsistency regarding step size and a mismatch between Algorithm 1 in the supplementary materials and Equation (5), which warrant clarification.