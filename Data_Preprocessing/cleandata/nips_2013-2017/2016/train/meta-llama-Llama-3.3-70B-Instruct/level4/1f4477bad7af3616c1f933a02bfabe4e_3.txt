This manuscript introduces a compelling theoretical framework, demonstrating that a specific class of neural networks achieves a global optimum at a linear convergence rate, given that the input data, weights of the linear layer, and generalized polynomial activation functions are all positive. The theory is initially established for neural networks with a single hidden layer and subsequently generalized to networks with multiple hidden layers. Additionally, the authors propose an optimization algorithm based on the nonlinear spectral method. Experimental results indicate that the proposed algorithm performs well on a small 2D synthetic dataset and achieves reasonable performance on several small real-world datasets from the UCI repository. 
The primary advantages of this work are that it presents the first potentially practical neural network with both optimal and convergence guarantees, as claimed by the authors. Unlike previous works with convergence guarantees that require complex methods to verify their preconditions, which can be impractical due to their dependence on the data-generating measure, the theory proposed here has relatively simple preconditions. These preconditions depend solely on the spectral radius of a nonnegative matrix composed of the network's parameters. The authors also demonstrate the applicability of their theory to networks of arbitrary depth and show promising results on small real datasets.
However, there are limitations to the proposed network's applications. The theory imposes constraints requiring both the input data and network parameters to be positive, the activation functions to be generalized polynomials, and the spectral radius (as per Theorem 1) to be less than 1. While the non-negativity constraint on the input data may not significantly limit potential applications, as most training data can be easily shifted to be all positive, the requirement for positive network parameters might restrict the model's expressiveness. Furthermore, the necessity for all activation functions across hidden layers to be different seems unnatural and could also limit the network's applicability. 
The authors note that to satisfy the spectral radius constraint, they must increase $p1$ and $p2$ while decreasing $pw$ and $pu$, effectively reducing the weights of linear units. It appears that to meet the spectral radius constraint, especially for higher-dimensional inputs and network parameters, the weights of linear units must be decreased. When these linear weights are very small, the activation functions behave similarly to linear functions, and adding more layers essentially adds another linear function, which does not enhance the network's expressiveness. This raises concerns about the ease of extending this method to deep architectures.
Despite these concerns, this work represents an encouraging initial attempt. The paper's contributions outweigh its limitations, making it worthy of acceptance. 
Minor comments include a curiosity about how parameters $p1$, $p2$, $pw$, $pu$, the dimension of the input, and the size of the network parameters influence the spectral radius $\rho(A)$. For instance, as the input dimension increases, does $\rho(A)$ increase linearly or exponentially? To counteract this effect, should $pw$ and $pu$ be decreased linearly or exponentially? While a formal proof may be challenging, empirically plotting this relationship on synthetic data could provide valuable insights.