This manuscript investigates the development of safe reinforcement learning algorithms, with safety defined relative to a specified baseline. It introduces a novel safety concept, focusing on achieving the maximin improvement over a given baseline policy. The paper presents several key results related to this concept, along with an approximate algorithm, offering a fresh perspective on safe RL algorithms. However, the exposition can be challenging to follow at times, and a review of existing safety notions prior to introducing the new concept would have been beneficial. The relegation of entire algorithms to the appendix seems inappropriate.
Several issues and questions arise:
- The definition of safety on Line 104 lacks clear attribution. Notably, Thomas et al.'s "High Confidence Policy Improvement" provides a value-based definition with high probability. The paper should clarify the rationale behind multiple definitions and the preference for one over another.
- Theorem 3 employs a multiplicative bound. The rationale for this choice over an additive bound, which would yield a different conclusion, is not evident.
- Proposition 7 raises concerns about the approximation error when assumptions are violated. Specifically, Remark 1 suggests that Algorithm 1 could potentially perform arbitrarily worse than the exact solution.
- The proof of Theorem 5 contains a non-obvious step, transitioning from rho(pi_S, xi) to rho(pi^, xi) on Line 392, given that pi^ is the optimal policy for P^*, not for any xi.
- The NP-hardness proof is difficult to follow, with unclear reasoning for the omission of certain states (e.g., l_13) in Figure 6. The statement requires more careful formulation and is currently incorrect. The uncertainty set should be described in terms of independent factors to avoid trivial polynomial minimization in the exponentially sized Xi.
Minor issues include:
- The notation P^ seems redundant, as the MDP is already defined using P. Using P instead of P^ could simplify the notation.
- Definition 2 lacks clarity on whether it seeks a maximal zeta or a max-achieving pi.
- The term "standard approach" on Line 118 sounds definitive and is somewhat grating, especially given the lack of a comprehensive review of existing safe methods at that point.
- Line 160 should clarify that state occupancy measures are with respect to p_0 and are discounted.
- Theorem 8 and Lemma 11 involve somewhat ambiguous use of the e and e_pi vectors, requiring guesswork. A careful definition of these vectors would be beneficial.