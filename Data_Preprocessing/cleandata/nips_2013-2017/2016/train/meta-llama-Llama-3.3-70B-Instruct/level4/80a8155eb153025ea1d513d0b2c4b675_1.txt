This paper examines the theoretical aspects of the robust k-means (RKM) formulation, as proposed in [5,23], with a focus on its robustness and consistency properties. The authors initially investigate the robustness of RKM, demonstrating that a single outlier can compromise the algorithm if the function $f\lambda$ is convex, whereas two outliers are sufficient to cause breakdown if $f\lambda$ is non-convex. However, under specific structural assumptions regarding the non-outlier data, a non-trivial breakdown point can be established for RKM. The authors also delve into the consistency issue, extending existing consistency results for convex $f_\lambda$ to the non-convex case. 
A primary concern with this paper is its potential appeal to a broader machine learning audience, as the results seem highly specialized. The RKM formulation, upon which the results are based, has not yet become a standard approach in the field. Furthermore, it appears that the techniques employed to develop these results may not be easily adaptable to other methods. 
In section 3.3, the presentation seems to primarily recap existing knowledge from the literature, lacking a formal theorem and referencing Theorem 3 from [2] without adding substantial new insights. Regarding the consistency results, there seems to be a discrepancy between the abstract, which mentions an extension of non-asymptotic results to the non-convex $f_\lambda$ case, and Theorem 2, which appears to present an asymptotic convergence result. Clarification from the authors on this point would be beneficial. 
The simulation results are also somewhat inconclusive, particularly in terms of comparing RKM with trimmed k-means, as it is unclear what specific conclusions can be drawn from the simulations. Overall, while the paper contributes to the understanding of RKM's theoretical properties, its broader impact and applicability within the machine learning community remain uncertain.