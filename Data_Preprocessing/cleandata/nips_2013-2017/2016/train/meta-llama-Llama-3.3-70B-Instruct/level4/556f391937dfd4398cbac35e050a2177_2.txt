This manuscript introduces a innovative approach to local similarity-aware embedding leveraging a deep neural network. The key contributions of this work include the ability to selectively choose challenging samples during training, thereby accelerating convergence and enhancing performance, the introduction of a novel objective function termed double-header hinge loss to capture local data structures, the incorporation of absolute position information to accommodate heterogeneous feature distributions, and the utilization of quadruplet sampling within mini-batches for efficient sampling. The authors demonstrate that their proposed method surpasses existing competitors in both image retrieval and transfer learning tasks. However, several major aspects require clarification: 
1. While the manuscript asserts that the proposed method offers advantages in both performance and computational speed, it fails to provide runtime measurements, which are essential for substantiating these claims.
2. The architectural details presented in Figure 2, particularly the CNN depicted in Figure 2(a), necessitate a more comprehensive explanation to facilitate a thorough understanding.
3. Additional elucidation on Figure 3 would be beneficial, specifically regarding the distinctions between contrastive embedding, triplet embedding, and lifted structured embedding, as well as their respective strengths and weaknesses in comparison to the presented methodology.
4. The manuscript lacks a detailed description of the objective function optimization process, including the specification of the gradient used.
5. An analysis of how the mini-batch size influences performance would provide valuable insights into the method's scalability and efficiency.
6. The initialization of parameters is not adequately addressed, and it is crucial to understand how these were initialized in the experimental setup to ensure reproducibility.
7. Including the standard deviation of the results presented in Table 1 would enhance the interpretation of the statistical significance of the findings. 
To strengthen the manuscript, it is essential to provide more detailed technical information regarding optimization strategies, implementation specifics, and network architecture, to the extent that other researchers can replicate the results. Furthermore, the results section should comprehensively validate each claim made in the manuscript, including the assertion of improved speed, to ensure the robustness and reliability of the proposed method.