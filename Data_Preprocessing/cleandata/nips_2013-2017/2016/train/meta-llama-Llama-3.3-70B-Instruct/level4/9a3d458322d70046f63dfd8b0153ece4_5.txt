This manuscript tackles the issue of safe policy design, where the worst-case scenario is bounded by a baseline policy. Unlike preceding methods that employ excessively cautious formulations, the proposed methodology enables the discovery of superior policies while ensuring a higher discounted return compared to the baseline policy. Although the suggested approach is computationally prohibitive, the authors propose a viable approximation technique. This approximation method undergoes rigorous theoretical and empirical analysis. The work presents a robust solution to the limitations of existing research, offering an improved approach. While the implemented method approximates the ideal approach, the paper successfully justifies the necessity of this approximation and demonstrates empirical superiority over alternative methods. Furthermore, the authors derive performance bounds for various safe policy learning approaches, although a more detailed comparison of these derived bounds would be beneficial for a comprehensive understanding.