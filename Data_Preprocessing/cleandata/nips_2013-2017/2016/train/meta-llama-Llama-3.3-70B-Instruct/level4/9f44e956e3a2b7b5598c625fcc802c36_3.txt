The authors propose the incorporation of an intermediate timescale into neural networks, showcasing its computational benefits. This innovation essentially integrates an input that correlates the current network state with its past states, thereby enhancing the network's capabilities. This work is both well-timed and elegantly presented, with clear motivations and suitable simulations. In terms of novelty, while intermediate timescales have been explored within the realm of reservoir computing, their application to standard machine learning tasks, as seen here, is a fresh approach. Moreover, the specific method of incorporation presented in this paper is original. The paper positions its primary contribution within the field of computational neuroscience. From this vantage point, several intriguing questions arise. For instance, how realistic is the proposed normalization procedure? Additionally, the application of matrices W and A to different activity vectors in equation 2 raises questions about the biophysical equivalent of the inner loop and its implications. Several specific points warrant further consideration: the size of mini-batches appears to impose memory limitations, yet this issue is not adequately addressed. Table 3 would be more effectively presented as percent correct rather than error. A reference is missing on Line 277. It is observed that the advantage of fast weights seems to be most pronounced in smaller networks, which could indicate a broader issue, although the available data does not provide a clear judgment on this matter.