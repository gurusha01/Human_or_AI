This manuscript investigates the optimization of memory utilization for recurrent neural network (RNN) training. The authors propose a dynamic programming approach to achieve optimal memory allocation, enabling the training of RNNs on lengthy sequences with restricted memory resources while maintaining a relatively low computational overhead. The introduction of dynamic programming builds upon existing work by Chen et al., which relied on a fixed square root of t and a fixed recursive schedule, thereby representing a significant advancement. By formulating the memory allocation problem as a dynamic programming problem, the authors demonstrate that the optimal allocation can be determined efficiently. Overall, this contribution is noteworthy and well-founded. However, the distinction between hidden states and internal states is somewhat ambiguous, and the results for these two cases differ only by a constant factor, suggesting that relocating some of the material to the appendix might enhance clarity. In contrast, the theoretical analysis presented in the appendix is engaging and provides valuable insights. A notable omission is the application of the proposed methodology to real-world problems, which would help assess whether the ability to train on longer sequences translates to improved RNN performance and reveal any practical issues that may arise beyond theoretical considerations. Several minor issues were also identified: the manuscript does not adhere to the required submission template, and the discussion in section 3 appears to omit the case of "somewhat limited" memory, as the distinction between "scarce" and "somewhat limited" memory scenarios is not clearly defined, potentially leading to confusion.