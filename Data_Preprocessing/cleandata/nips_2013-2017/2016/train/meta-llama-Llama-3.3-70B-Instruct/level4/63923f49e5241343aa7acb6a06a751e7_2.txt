This field of research has experienced significant growth in recent years, with a substantial number of advancements made in the context of bounded loss functions. The present paper explores the realm of unbounded loss functions, achieving rapid learning rates albeit with the requirement of a multi-scale Bernstein's condition and the existence of the $r$-th moment of the envelope function, which serves as an upper bound for the composition of the loss function $\ell$ with other functions. The primary result, as stated in Theorem 3.2, establishes a sharp oracle inequality that holds with high probability, characterized by a convergence rate of $n^{-\beta}$, where $\beta$ can be made arbitrarily close to 1 from above, contingent upon the value of $r$ (with more moments of the envelope existing, $\beta$ approaches 1 more closely). Section 3.4 provides valuable insight into the fulfillment of the multi-scale Bernstein's condition within a specific learning context. Furthermore, Theorem 4.1 demonstrates an application of this framework to quantization, yielding an Empirical Risk Minimization (ERM) rate of $\mathcal{O}(n^{-\beta})$, where $\beta$ may be arbitrarily close to 1. The paper is not only a pleasure to read but also contains groundbreaking contributions. The technical expertise and the dense array of results and ideas presented within the concise eight-page limit are truly impressive. The introduction effectively communicates the paper's objectives, making it accessible even to non-expert readers. The main contributions are clearly articulated and substantiated by rigorous proofs. In my assessment, this work has the potential to significantly impact multiple research communities. Overall, this is an exemplary piece of work. One minor suggestion: the citations on lines 444, 458, 460, 462, and 481 should be revised to properly credit all authors, avoiding the use of "et al."