The authors investigate the robust k-means algorithm, a variant of the traditional k-means algorithm that incorporates error terms for each point and penalizes these errors to minimize their impact. They demonstrate that the robust k-means algorithm can be compromised by adversarially manipulating just two points, leading to arbitrarily poor estimates of cluster centers. However, they also establish a condition under which the algorithm exhibits robustness to noise, specifically when the dataset is well-structured, meaning that at least half of the points can be divided into k clusters of bounded size, with a lower bound on the distance between cluster centers. Furthermore, the authors show that the robust k-means algorithm shares most consistency results with the traditional k-means algorithm and derive optimality conditions for the robust k-means minimization problem. Experimental results indicate that robust k-means outperforms a variant, trimmed k-means, on several datasets. This research addresses a significant question, given the frequent use of k-means in practice, and the robust k-means algorithm is a promising candidate for surpassing traditional k-means in certain scenarios due to its similar computational simplicity. A theoretical examination of robust k-means is thus justified. The authors provide two key findings on the algorithm's robustness: a worst-case lower bound and an upper bound under well-structured conditions, although the latter conditions are somewhat stringent and have been explored in previous research, including an ICML paper. The consistency result for robust k-means is noteworthy and offers additional justification for this variant, highlighting its potential benefits.