This paper introduces a novel framework called Cooperative Inverse Reinforcement Learning, which models scenarios where a human and a robot collaborate to maximize the human's reward, despite the robot's initial lack of knowledge about the human's reward function, prompting the human to make informative decisions that may compromise immediate rewards. The proposed model has the potential to make a significant practical impact, supported by both theoretical and experimental evaluations. Although the theoretical results may not be exceptionally profound, the value of this contribution lies in the realism and importance of the new conceptual model in capturing realistic scenarios. To enhance the paper, it would be beneficial to present a motivating example earlier on, providing more insight into its significance and relevance to real-world applications. The first example, which appears on page 6, line 238, could be preceded by a more in-depth discussion on its importance and how it differs from a traditional Inverse Reinforcement Learning (IRL) setting. Additionally, the example in lines 313-320 could be clarified, particularly in explaining the roles of the human and the robot in the grid navigation scenario. The substantial performance improvement achieved by the new best-response approach over the prior expert demonstration approach in this example is noteworthy. The authors provide a comprehensive review of prior and related work, although the King Midas analogy could be expanded upon for better understanding. The mention of multiple optimal policies in footnote 1, deferred to future work, warrants further discussion, especially considering the potential for multiple Nash equilibria in two-player general sum games, each with different values for the players. It would be beneficial to justify the chosen solution concept for this setting. Finally, in section 4.1, using distinct notations, such as T for the time horizon instead of H (which is also used for the human agent), would improve clarity.