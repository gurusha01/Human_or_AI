This paper builds upon the conventional encoder-decoder framework by introducing an intermediary reviewer module for end-to-end visual caption generation, demonstrating improved performance in both image and source code captioning compared to the traditional attentive encoder-decoder approach. The incorporation of a multi-step attention mechanism over hidden units within the reviewer module enables the capture of global information across multiple attentions, while a discriminative loss function guides the learning process. The concept of a multi-step reviewer module is noteworthy. The paper is well-structured and clearly written. However, the method section would benefit from a more detailed explanation of the two attentive mechanism variants, namely input reviewer and output reviewer, including their differences and applications in various settings. Furthermore, given the similarity between the proposed multi-step attention and the multi-hop attention used in memory networks, where performance is known to be step-dependent, the authors should provide an analysis on how the number of reviewing steps impacts the results, offering insight into the optimal configuration.