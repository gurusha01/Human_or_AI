This paper presents a game-theoretic model for cooperative learning in human-robot interactions, formulating an inverse reinforcement learning problem within a cooperative partial information two-player game framework. The topic of cooperative learning in human-robot settings is intriguing, and the author demonstrates a clear understanding of how their approach aligns with the current state of the art. The paper is well-structured and easy to follow; however, the overall impact, in terms of both technical merit and innovation, is questionable. Several key concerns support this assessment: 
1. The assumption of Dynamic Bayesian Equilibrium (DBE) seems overly stringent, and its applicability to real-world scenarios is doubtful due to the rigid dependency of the human's reward on the robot's learning success, which introduces a significant positive feedback loop influencing the robot's learning process.
2. The nature of cooperation in the described human-robot scenario, where both entities collectively address a single decision problem, lacks clarity. Specifically, how the proposed form of cooperation affects the reward structure needs further explanation.
3. The proof of Theorem 1, as provided in the supplementary material, appears to be either incorrect or insufficiently clear. Notably, the statement "R can also simulate C" seems to contradict Definition 2, particularly the definition of C's actions, which encompasses a decision rule for the human.
4. The proof of Theorem 3, also found in the supplementary material, does not hold up to mathematical scrutiny.
5. The role and definition of the feature function are not adequately explained, leading to confusion.
6. On a minor note, several symbols and notations are introduced without proper explanation: for example, \delta_{\Theta} is not defined in either the main text (page 2, line 217) or the supplementary material (page 2, Corollary 1), and \eta (page 7, line 299) is also not introduced, which hinders the readability and understanding of the paper.