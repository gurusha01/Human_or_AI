The authors explore a "semi-bandit" learning scenario where a learner chooses a subset of L actions from a set of K actions over multiple rounds. In each round, nature assigns rewards to each base action, and the learner receives feedback on the L chosen actions (semi-bandit feedback). The learner's reward for the composite action is a weighted linear combination of the L base rewards, with potentially unknown weights that depend on the ordering of the base actions. A key innovation is the extension of this problem to a contextual setting, where the goal is to compete with the optimal policy from a given class. The authors assume access to a supervised learning oracle that can identify the best policy for a finite dataset of contexts, base rewards, and weightings. They propose an algorithm that achieves a sqrt{T} regret bound while making a polynomial number of oracle calls. This work builds upon previous research, such as Kale et al., which addresses a similar problem with comparable rates. However, the authors' approach differs in that it employs a technique similar to Agarwal et al., using coordinate descent and oracle calls to solve an optimization problem while maintaining a sparse policy distribution. The authors argue that Kale et al.'s work is limited by requiring a weight vector w = 1, although this claim is questionable in the known-weights setting. The paper's core ideas are largely adapted from Agarwal et al., but the analysis remains complex and nuanced. Ultimately, the problem's significance stems from its applications in online content recommendation, making it an important area of study.