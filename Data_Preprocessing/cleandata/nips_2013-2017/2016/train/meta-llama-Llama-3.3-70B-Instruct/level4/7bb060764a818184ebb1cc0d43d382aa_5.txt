This paper presents a novel approach to dropout, introducing a distribution-dependent method applicable to both shallow and deep learning frameworks. The authors provide theoretical evidence that their proposed dropout technique yields a reduced risk and achieves faster convergence. Building on this foundation, they develop an efficient evolutionary dropout strategy for deep neural networks, which dynamically adjusts sampling probabilities based on the evolving output distributions of each layer. Furthermore, they introduce a multinomial dropout and demonstrate, through risk bound analysis for shallow learning, that distribution-dependent dropout leads to faster convergence and smaller generalization errors. The paper is well-structured and clear. However, several concerns arise: (1) The submission's novelty, in relation to NIPS standards, is questionable and may require further emphasis in the rebuttal. (2) The experimental section only reports the best results for each dataset, which achieved the fastest convergence, prompting the question of why other outcomes were not included. (3) The comparison of evolutionary dropout with Batch Normalization (BN) is limited to the CIFAR-10 dataset; extending these experiments to other datasets could provide broader validation of the proposed approach. Additionally, minor issues include the incorrect use of the citation package and the presence of "author ?" in Section 2, which should be addressed for clarity and completeness.