This manuscript explores the application of the parallel knowledge gradient method to batch Bayesian Optimization, focusing on scenarios where function evaluations are computationally expensive. The authors introduce a novel acquisition function, q-KG, designed to propose multiple points for simultaneous evaluation, thereby enhancing the efficiency of the optimization process. They also outline an efficient method for evaluating the q-KG function. The empirical results presented support the theoretical foundations of their approach. The problem addressed in this paper is significant, as the ability to distribute the workload can substantially improve the performance of data-driven applications. By considering the evaluation of multiple points simultaneously through the q-KG acquisition function, the algorithm significantly accelerates the optimization process. This concept is intriguing and warrants further exploration. However, the assumption of a Gaussian process, particularly in the context of neural networks with high-dimensional input spaces, may not always be valid. It would be beneficial for the authors to provide theoretical analyses on the error bounds of their method and a quantitative assessment of its complexity in relation to dimensionality. Nonetheless, this is a robust contribution to the field of Bayesian Optimization, with a sound methodology and well-designed experiments. The paper is generally well-written, although there are a few minor errors and areas for clarification. Specifically, at line 36, the phrase "the set of points to evaluate next that is" could be rephrased for better clarity. Additionally, definitions for "A" (line 95) and the input of "mu^n" (line 104) would be helpful. A brief discussion on the parallel Expected Improvement (EI) algorithm (line 128) could provide useful context, and using distinct notations for "A" (line 140) might enhance readability.