This study's primary contribution lies in its application of triangular Cholesky factorization, which eliminates the need for square roots in the conventional CMA-ES, thereby reducing computational costs associated with matrix inversion. The author proposes a theoretically grounded approach, demonstrating that this modification does not compromise the algorithm's performance. By leveraging this method, the runtime complexity of the algorithm is decreased without significantly altering the number of objective function evaluations required. This paper presents a noteworthy implementation of the Cholesky factorization technique for CMA-ES. A more detailed theoretical explanation would be beneficial to understand why this approach yields 'numerically more stable' computations. Notably, the Cholesky-CMA-ES appears to require a comparable number of objective function evaluations as the standard CMA-ES, while reducing wall-clock time, with this advantage increasing as the dimensionality of the search space grows. Consideration should be given to integrating the low-dimensional approximation proposed by Loshchilov in 2015 into the existing framework, as high-dimensionality is a critical concern in many practical applications, such as training neural networks in direct policy search.