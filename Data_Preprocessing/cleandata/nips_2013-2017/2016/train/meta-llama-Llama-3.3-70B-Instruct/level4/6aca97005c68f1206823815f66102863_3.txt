The authors present a novel approach to approximating the posterior distribution of intractable models using a neural network-based density estimator. A key advantage of this method over Approximate Bayesian Computation (ABC) techniques is the elimination of the need to select a tolerance. The innovative aspect of this work lies in directly modeling the posterior, whereas common approaches typically focus on approximating or estimating the intractable likelihood. Proposition 1 is identified as the central result of the paper, enabling the use of various conditional density estimators, with the authors opting for a Mixture Density Network (MDN). They outline the estimation processes for the proposal prior and posterior density using Algorithms 1 and 2, respectively, and demonstrate their method through several simple examples, including cases with intractable likelihoods.
The most novel contribution of the paper, Proposition 1, is of particular interest. However, concerns arise regarding the assumptions underlying formula (2), which, as explained in the appendix, relies on the complexity of $q\theta$ to achieve a zero KL distance. In practical scenarios with finite sample sizes, $q\theta$ cannot be overly complex to avoid overfitting, implying that formula (2) holds only approximately.
The provided examples are somewhat disappointing, particularly in the context of high-dimensional scenarios where tolerance-based ABC methods are known to struggle. The absence of at least one relatively high-dimensional example (e.g., 20D) raises questions about the scalability of the MDN estimator as the number of model parameters or summary statistics increases. The practical utility of the method heavily depends on its scalability, which is not clearly demonstrated. The complexity of the MDN fit, influenced by the hyperparameter $\lambda$ and the number of components, is also a concern. Notably, the selection process for $\lambda$ is not reported, which is crucial for understanding the method's performance.
Further comments are organized by section:
- Section 2.3: 
  1. Is a proper prior necessary for the method to be effective?
  2. The convergence assessment for Algorithm 1, which appears stochastic, is not clear.
- Section 2.4:
  1. The claim that $q_\phi(\theta | x)$ learns the posterior for all $x$ if $p(\theta)$ is taken as the actual prior may not hold universally, especially considering the influence of the prior on learning the posterior for different $x$ values.
  2. The use of one Gaussian component for the proposal prior and multiple components for the posterior is puzzling. Is sampling from a MDN with multiple components computationally expensive? Unifying Algorithms 1 and 2 by iteratively using the estimated posterior as the proposal prior for the next step could be explored.
  3. The initialization of MDN at each iteration in Algorithm 1 is unclear, particularly how it relates to maintaining a small $N$ by leveraging previous iterations.
- Section 2.5:
  1. The mechanism by which MVN-SVI avoids overfitting is not transparent and likely depends on the hyperparameter $\lambda$, whose selection process is not described.
- Section 3.1:
  1. The differences in densities in the left plot of Figure 1 are barely perceptible; plotting log-densities might enhance visibility.
  2. The value of $\lambda$ used to obtain the results is not reported, a omission repeated in subsequent examples.
- Section 3.2:
  1. The correctness of formula (5) is questionable, as $x$ is a vector, yet its mean and variance are scalar.
  2. In Figure 2, explaining why the largest number of simulations in ABC does not correspond to the smallest KL distance would be beneficial, potentially attributing this to an overly small $\epsilon$ leading to a high rejection rate.
- Section 3.3:
  1. The observation that MDNs choose to use only one component and deactivate the rest, consistent with the near-Gaussianity of the posterior, may depend on the value of $\lambda$, which is not specified.