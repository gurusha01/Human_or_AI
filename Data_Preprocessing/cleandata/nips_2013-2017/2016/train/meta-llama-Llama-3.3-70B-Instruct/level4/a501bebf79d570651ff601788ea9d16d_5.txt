The training of recurrent neural network models is notoriously memory-intensive, particularly when dealing with variable-length sequences, which can lead to fluctuating memory allocation during the training process. Conventionally, RNN/LSTM models have been trained using "full memoization," where all states are stored during forward-propagation for reuse in backpropagation. Alternatively, "Chen's algorithm" reduces memory usage at the cost of increased computation by recomputing hidden states during backpropagation. This paper introduces a flexible approach that balances resource utilization, allowing for a trade-off between computation and memory usage by selectively memoizing states. For instance, the method can store every third state or every fourth state, thereby creating a spectrum of resource usage from high computation/low memory to low computation/high memory. This innovation addresses a significant concern in the field, as memory constraints can hinder the training of RNNs, especially given the challenges of scaling sequence training across multiple machines. The authors' proposal to offer a user-selectable balance between computational load and memory consumption is commendable. However, the absence of experimental results on actual datasets or the training of a real RNN/LSTM model to convergence is a notable omission. The importance of this research lies in its potential to efficiently utilize memory and computational resources in RNN training, a critical issue given the sequential nature of the data and the difficulty in scaling training processes. To strengthen their contribution, the authors should conduct real experiments, such as training an RNN on a dataset like UCF-101 for video activity recognition, and report the outcomes. This would involve comparing their method's performance, memory savings, and computational requirements against traditional full memoization techniques, using the same RNN architecture, training protocol, and random seed. Such an experiment would not only demonstrate the practicality of their approach but also provide quantitative insights into its benefits. Furthermore, including a motivating example that highlights the memory constraints in a specific application and demonstrates how their method can significantly reduce memory requirements with a modest increase in computational cost would enhance the paper's motivation and impact. By addressing these suggestions, the authors can substantially improve the technical quality and persuasive power of their work.