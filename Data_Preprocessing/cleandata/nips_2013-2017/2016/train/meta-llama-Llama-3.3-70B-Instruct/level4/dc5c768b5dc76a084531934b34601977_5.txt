This manuscript presents a methodology aimed at minimizing the net-zero win and loss changes, referred to as churn, between two successively trained models that utilize increasing features and training sizes. The authors propose a stabilization technique, integrated into a Markov chain, to mitigate churn. The paper provides both theoretical foundations and empirical evidence to support the approach. Specifically, it employs a stabilization operator to ensure that model B, trained after model A, maintains consistency with its predecessor. Theoretical and empirical results substantiate the rationale behind this method. However, the necessity of employing successive training, particularly when model B essentially inherits information from model A (which was trained on a potentially less comprehensive dataset, TA), raises questions. Essentially, model B is influenced by both its own training dataset, TB, and the dataset used for model A, TA. It would be insightful to compare the outcomes of directly training model B on the combined dataset TA+TB or, more pragmatically, training on TB with the introduction of random perturbations (such as feature dropout) to enhance robustness and accuracy. Given the current popularity and success of dropout-style techniques, a comparative analysis with the proposed method would be beneficial. Furthermore, the selection of perturbation PT (as mentioned on Line 94) and the assertion that classifier A is pre-trained to be robust to specific changes (Line 104) lack clear intuition. It is not immediately apparent why A would inherently be robust to changes encoded by PT, despite the acknowledgment that PT facilitates better generalization of Fk^. The impact of this on B's robustness also warrants explanation. Additionally, the calculation method for "True WLR Needed" in Table 1 is not explicitly stated and should be clarified. There is a minor typo in Line 178, where "re-labels" should be corrected to "re-label." The presentation of Table 3 could be improved for readability by potentially separating different metrics (such as WLR) into distinct tables to facilitate easier comparison.