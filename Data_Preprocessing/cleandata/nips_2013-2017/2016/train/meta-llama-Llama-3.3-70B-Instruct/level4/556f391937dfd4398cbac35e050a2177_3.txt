This manuscript introduces two significant modifications to the deep embedding pipeline. Firstly, the proposed PDDM framework incorporates absolute feature position information to learn a similarity metric that adapts to the local structure of features. Secondly, the double-header hinge loss function is designed to leverage hard quadruplet mining, allowing for explicit discrimination between target similarity distributions. The experimental results demonstrate that the proposed approach yields faster convergence, improved feature embedding, and enhanced generalization capabilities. The motivation behind the paper is clear: a globally applied Euclidean metric may be suboptimal in heterogeneous spaces, potentially misleading hard sample mining and, by extension, deep embedding learning. Drawing inspiration from earlier research [Xiong 2012], the authors propose utilizing feature position information to develop a locally adaptive similarity metric. This metric can identify genuinely hard samples and guide deep embedding learning, a concept that, although not markedly novel, appears reasonable and worthy of exploration. The presentation of the paper is also commendable, with a well-architected network that effectively integrates additional feature position information and a loss function tailored to require an explicit hard quadruplet mining process. The manuscript provides detailed technical specifications and hyperparameters, ensuring repeatability, and the experimental outcomes are promising when compared to preceding methods that do not utilize feature position information.