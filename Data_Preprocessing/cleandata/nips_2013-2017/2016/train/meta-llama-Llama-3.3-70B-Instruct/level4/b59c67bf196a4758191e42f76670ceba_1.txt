This paper presents a novel deep learning approach that integrates transductive learning for transfer learning, with experiments conducted in a fully transductive setup. Although the results appear promising, there are significant concerns regarding both the proposed model and the experimental design, which are elaborated upon in the detailed comments below.
Concerning the proposed model, several key issues are identified:
1. The authors emphasize in the Abstract and Introduction that transfer learning aims to mitigate the mismatch between training and testing data distributions to achieve good generalization across domains or tasks. They also state that $\hat{x}i$ and $xi$ follow different distributions $ps$ and $pt$. However, unlike existing methods such as [19] and [Pan et al., Domain adaptation via transfer component analysis, IEEE TNN, 2011], the proposed model does not explicitly minimize the distance between the training and testing distributions. Therefore, it is suggested that the authors rephrase their claims, as the new representation learned by deep learning may reduce domain differences but does not explicitly align distributions.
2. The introduction of a transductive step to utilize predicted labels on target domain unlabeled data appears to be a new concept in transfer learning. However, this idea is borrowed from co-training techniques in semi-supervised learning. The adaptation step, which learns common and task-specific parameters, is standard in deep learning-based transfer learning methods. Thus, the proposed model can be considered a combination of existing techniques.
3. The solution for labeling target domain unlabeled data, as presented in equation (2), is somewhat heuristic. While the Reject Opinion introduces a confidence measure on predicted labels, theoretical analysis would provide a more solid foundation.
4. Discussions on the convergence of the alternating optimization procedure are lacking.
Regarding the experiments:
1. The performance of the proposed method may be sensitive to the value of $k$ in $k$-nn on different datasets. However, sensitivity analysis on $k$ is missing, and there is a lack of discussion on how to tune the value of $k$ in practice.
2. Similarly, the performance may also be sensitive to the value of $\gamma$ in the proposed Rejection Option, but sensitivity analysis is missing. Additionally, there is no discussion on how to tune the value of $\gamma$ in practice.
3. Experiments are only conducted in a fully transductive setup, which is not practical. After estimating labels for target domain unlabeled data using the proposed model, a $k$-nn can be applied to make predictions on out-of-sample target domain test data. It would be more interesting to present results on an out-of-sample test dataset.