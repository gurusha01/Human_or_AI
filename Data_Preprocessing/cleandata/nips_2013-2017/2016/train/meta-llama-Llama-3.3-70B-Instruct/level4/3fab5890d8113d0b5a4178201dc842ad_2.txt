The majority of existing neural network models that incorporate memory exhibit significant scalability issues, both in terms of space and time, as memory size increases. The authors introduce a novel memory access approach, termed Sparse Access Memory, which they demonstrate can maintain representational capabilities while facilitating efficient training, achieving substantial improvements of 1000-fold in speed and 3000-fold in memory efficiency. However, I believe the technical merits of the paper fall short of the standards required for acceptance. The core technical components, including efficient backpropagation and approximate nearest neighbor search using sparse access memory, are relatively straightforward and lack complexity. Although the experimental results appear intriguing, I do not support acceptance due to the paper's overall technical weaknesses.