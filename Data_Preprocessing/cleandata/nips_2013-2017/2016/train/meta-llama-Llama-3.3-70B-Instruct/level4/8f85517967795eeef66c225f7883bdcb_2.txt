The authors have employed the "cooperative graph cut" method, initially proposed by Jegelka and Bilmes, to develop an approach for approximate probabilistic inference. This approach is more complex than those used for basic pairwise graphical models due to the non-local "cooperative term" $f$ in equation (1). The core concept involves linearizing this problematic term and adapting it through the outer optimization task in equation (3). This adaptation is based on outer convex relaxations to lower bound the log-partition function and non-convex inner relaxations (mean-field) for the upper bound. Given that probabilistic inference, as opposed to MAP, involves the surrogate of the entropy, the dependency of equation (3) on $g$ is smooth, specifically $C^{1,L}$, thereby eliminating the need for inefficient subgradient-based schemes. The novelty of this paper lies in the informed assembly of established components, which should yield numerous applications. However, the title seems overly broad in light of related work, such as http://dx.doi.org/10.1007/s10107-016-1038-y and prior research. A more fitting title could be "Efficient Probabilistic Inference with Cooperative Graph Cuts."
The presentation occasionally lacks precision, partly due to space constraints. Three key points require clarification and improvement: (i) While the interplay between convexity and smoothness via duality is clear, a precise reference to a quantitative statement in the literature would be beneficial in connection with Theorem 1. (ii) The strong convexity of a function implies that subtracting the Euclidean squared norm (multiplied by a fixed constant) preserves convexity. Since this does not hold for the basic entropy function $\sumi xi \log(x_i)$, it is not immediately obvious that global strong convexity applies to $-\bar{H}$ (Theorem 1). The claimed applicability of FISTA also requires a global Lipschitz constant; please specify this in the context of bounded feasible sets. (iii) If the quantitative assertion of Theorem 1 holds, the Lipschitz constant depends on the number of labels $k$. In realistic applications, $k$ can be significantly larger than in the toy experiments, potentially slowing down optimization. Please provide commentary on this aspect.