This manuscript introduces a novel architectural variant of Long Short-Term Memory (LSTM) networks, termed "Phased" LSTMs, which incorporates multiple periodic masks with varying frequencies to facilitate the learning of long-term dependencies at an accelerated pace. This approach, analogous to Fourier decomposition, offers several benefits, including enhanced training efficiency and state-of-the-art performance across diverse tasks. Notably, Phased LSTMs enable the processing of asynchronous data feeds, eliminating the requirement for regularly sampled inputs. The authors provide a clear and concise explanation of the motivations and design underlying the Phased LSTM architecture, which is subsequently validated through three distinct experiments spanning different tasks. These experiments demonstrate the capability of Phased LSTMs to handle asynchronous data, outperform standard LSTMs, and exhibit faster training times. The paper is well-structured, engaging, and presents a compelling innovation in the realm of recurrent neural networks, with deep connections to the theoretical framework of time series frequency decomposition. The potential impact of Phased LSTMs is substantial, given their ability to accommodate asynchronous data feeds, which is a common challenge in modern sensing applications involving asynchronous time series data. The introduction of the phased input gate is an elegant concept, reminiscent of projecting a time series onto a Fourier basis, which enhances the learning speed of LSTMs by reducing the number of necessary updates and facilitating full gradient propagation over time. The experiments demonstrate significant improvements over existing methods and appear to be conducted in a rigorous and systematic manner across varied and compelling tasks. The writing quality is exceptional. However, two minor remarks are noteworthy: Equation (11) and the associated section provide a simplified explanation for the Phased LSTM's ability to capture long-range dependencies, but the simplification may be considered extreme, as it renders the model similar to an AutoRegressive (AR) model. The authors should acknowledge that this simplification serves a pedagogical purpose. Furthermore, the experiments lack specific details regarding the number of oscillation periods employed in practice, including the number of samples drawn from the exponential distributions. Overall, this manuscript is a pleasure to read, offering a straightforward yet innovative contribution that merits presentation to the community due to its potential impact on neuromorphic computing and time series analysis.