The paper presents a novel RNN architecture that utilizes a unitary recurrent matrix (uRNN) and is optimized across the entire space of unitary matrices. Notably, the authors demonstrate that a previously proposed method for optimizing unitary RNNs was not "full-capacity," meaning it did not span the entire space of unitary matrices, as evidenced by their capacity analysis using Givens decomposition. 
The concept of employing unitary matrices in RNNs (uRNNs) has garnered attention recently, and this paper addresses the compelling question of how to optimize a unitary matrix over its entire space. The proposed optimization technique, which operates on the Stiefel manifold, is both straightforward and powerful, achieving the desired optimization. 
However, the potential impact of this work remains uncertain, as it is unclear whether the research community will adopt unitary matrices as a viable alternative to LSTMs, or if this technique will prove more effective than previously proposed methods. 
The introduction and the analysis using Givens decomposition are well-written and convincing. Nevertheless, Section 4, which briefly outlines the optimization method, could be enhanced by incorporating equation (1) from reference [16] and providing a brief explanation for why gradient descent over manifolds is performed in the tangent space. 
From a technical standpoint, the authors effectively demonstrate the superiority of their method over LSTMs in the experimental section. However, a concern arises when comparing the proposed uRNN technique to previous methods: the comparison is based on matching the size of the matrices rather than the number of trainable parameters, which theoretically corresponds to memory usage and running time. Therefore, it would be intriguing to explore the outcomes if these parameters were matched. 
Furthermore, in Section 5.2, the MSE results surprisingly support the restricted capacity uRNN more than the proposed model. The authors address this by introducing "voice quality" measures, which highlight the superior quality of their model. Nonetheless, this was not the optimization objective. A potential solution could be optimizing MSE in mel-space instead of linear frequency space, as the former is known to correlate better with voice quality. 
Minor suggestions include: 
- At line 80, it would be beneficial to mention that P is chosen randomly and remains fixed throughout the training process. 
- At line 101, there is an extra instance of the word "define." 
- Regarding Theorem 3.1, it would be helpful to clarify whether the bound is tight; otherwise, it remains unclear if a full-capacity matrix exists.