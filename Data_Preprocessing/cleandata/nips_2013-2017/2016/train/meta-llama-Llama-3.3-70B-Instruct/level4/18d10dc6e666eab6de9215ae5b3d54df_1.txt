This manuscript presents a batch-sequential approach to Bayesian Optimization, introducing a novel variant of the Knowledge Gradient criterion. Following a thorough background discussion on related work and Gaussian processes, the authors define the parallel q-KG criterion and elaborate on its computational details, mirroring the standard KG criterion. The paper is supplemented by numerical experiments demonstrating the superiority of q-KG over state-of-the-art batch-sequential Bayesian Optimization algorithms. Overall, this is an excellent paper that warrants publication in NIPS due to its significant potential impact on society through the parallelization of Bayesian optimization algorithms. However, a few minor criticisms arise: the manuscript does not investigate speed-ups from sequential to batch-sequential methods, and some statements in the literature review are slightly inaccurate. For instance, the concept of integrating Expected Improvement with respect to posterior distributions was previously explored in "Kriging is well-suited to parallelize optimization" and by Chevalier et al. in their work on CL-mix. Additionally, q-EI maximization using natural gradient has been studied in "Differentiating the multipoint Expected Improvement for optimal batch design". Several questions and remarks are also posed: 
* Are the set A and function f compact and continuous, respectively?
* What is the rationale behind restricting A to be a Latin Hypercube Sample at the initial stage?
* Does Algorithm 1 account for hyperparameter re-estimation or Bayesian updating?
* In Section 5.2, is the definition of g a maximization or minimization problem?
* The smoothness of g and the derivative of the Cholesky factor may not be immediately apparent and could be clarified.
* The Mat√©rn parameter is likely 5/2, not 2/5.
* Regarding Figure 1 and the associated experiments, could the same Gaussian processes have been fitted using different software packages?