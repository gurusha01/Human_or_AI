This paper presents a novel ensemble training approach for deep learning models, where only one ensemble member is active for each training example, differing from traditional deep and classical ensemble methods. The algorithm identifies the member with the lowest loss relative to the true label and updates it using the gradient, resulting in a straightforward yet effective concept that yields impressive experimental results and addresses some limitations of existing techniques, warranting a weak acceptance recommendation. 
Several aspects of the paper require clarification or improvement: 
1. The testing phase lacks clear explanation, suggesting that the authors may utilize ground truth to identify the best-performing ensemble member, as hinted at in line 159 on page 5.
2. The terminology 'independent ensembles' and 'regular ensembles' (abbreviated as 'Indp.' in figures and mentioned in line 181 on page 5) is not adequately defined, making it difficult to understand the baseline being referenced.
3. The authors' emphasis on ensemble members specializing in different output classes seems unjustified, given that reassignment occurs on a per-example basis during training. Incorporating specialization into the loss function could potentially strengthen this argument.
4. The paper overlooks related work on diversity regularization in neural networks, such as Chen et al., which provides a more rigorous approach that has yet to be explored in the context of deep learning.