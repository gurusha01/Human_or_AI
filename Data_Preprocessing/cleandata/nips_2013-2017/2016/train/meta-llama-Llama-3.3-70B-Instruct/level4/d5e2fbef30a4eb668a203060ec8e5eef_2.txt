This manuscript presents an information-theoretic feature selection algorithm that approximates high-order mutual information by transforming categorical variables into binary ones. The algorithm decomposes the high-order mutual information into two sets: one seeking the best complementary subset to increase information and another identifying the subset containing most of that information. 
Key observations include that Statement 1 and its associated proof are unnecessary, as the maximum score assignable to a variable is the mutual information between the variable combined with all selected features and the target when the number of selected features exceeds the parameters t and s. The behavior of this criterion is noteworthy only when approximating the conditional mutual information (CMI), specifically when t and s are less than the number of features. The interplay between the max and min steps is intriguing, with the expectation that G often covers H, minimizing information, but the outcome depends heavily on the search procedure in cases of complementary features. The binary representation approach expands the search space, enabling the detection of interesting interactions.
However, the complexity analysis of competitor techniques, while correct in a naive implementation, overlooks the potential for memoization, as seen in tools like the FEAST toolbox. With memoization, mutual information calculations can be reduced to O(d) per iteration, with total computations and memory requirements of O(kd), contrasting with the growing computational complexity of CMICOT due to the dynamic selection of binary features.
The experimental study is lacking in several details, including specifics about the k-NN algorithm, the multiclass Adaboost method, the base learner for Adaboost, and the number of ensemble members. Notation in parts of the supplementary material is unclear, such as the equation above line 21, where different subsets are denoted by the same letter.
Minor issues include the undefined "simple operation" in the proof of Proposition 3 and the miscalculated complexity of joint entropy computation as O(Nm) instead of O(N + |X|). Separating results for different datasets could help distinguish the effects of the binary approximation from the max/min criterion. Comparing the performance of the binary approximation to other methods like JMI or RelaxMRMR, both on original and binary-expanded datasets, could provide insights into whether the binary approach adds noise or limits information. Running other algorithms on binary-expanded datasets could further elucidate the impact of this transformation on performance.