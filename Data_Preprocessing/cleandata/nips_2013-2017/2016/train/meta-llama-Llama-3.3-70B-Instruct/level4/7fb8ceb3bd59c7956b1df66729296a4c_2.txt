This submission investigates the problem of matrix completion, which has seen significant progress in recent years, particularly with non-convex optimization methods that often perform well in practice when initialized carefully. The authors aim to demonstrate that for a regularized version of this problem, all local minima are indeed global minima, implying that any optimization method converging to a local minimum will also converge to a global one. Notably, popular algorithms like SGD, when initialized randomly or arbitrarily, satisfy this property. The results are also resilient to noise, a valuable attribute. Although similar outcomes are known for other problems, such as dictionary learning, matrix completion presents a distinct challenge. The study has some limitations: it focuses on symmetric matrices, leaving the asymmetric case open, and assumes the objective function is the Frobenius distance from observed matrix entries, with bounded condition numbers and no row dominating the Frobenius norm. The analysis leverages first- and second-order optimality conditions to show that any local minimum must be a global minimum. Specifically, the second-order condition ensures a large L2 norm, while the first-order condition ensures a small L{infty} norm; combining these, the first-order condition further implies the local minimum is close to a global minimum. This contribution is significant and theoretically justifies the empirical success of non-convex methods in matrix completion, even with minimal initialization effort. The analysis is engaging and tailored to matrix completion's specific properties. The submission exceeds the standards for NIPS. However, an area for further discussion is the precise properties of the regularizer that the analysis depends on and how the proposed regularizer compares to those used in practical applications.