A persistent challenge in recurrent neural networks is the issue of vanishing and exploding gradients, which hinders their ability to learn from long sequences. To mitigate this, unitary hidden-to-hidden weight matrices have been recently introduced. Nevertheless, the unitary matrices proposed in prior literature are inherently limited in their representational capacity, particularly for high-dimensional spaces. The current paper introduces the concept of "Givens capacity" to measure the representational power of these unitary matrices. Additionally, the authors propose a modified stochastic gradient descent approach that enables the training of full-capacity unitary matrices. These enhanced matrices offer greater representational power than their predecessors while maintaining the desirable unitary properties, thereby reducing the impact of vanishing and exploding gradients during training. This study tackles a crucial problem in recurrent neural networks and presents a well-structured, technically sound, and theoretically grounded approach, with the authors proposing theoretically justified techniques that surpass current state-of-the-art methods. Although the application of unitary matrices in recurrent neural networks is a relatively new area of research, with its practical viability and comparison to traditional architectures not yet thoroughly examined, the paper presents promising theoretical and empirical results. These findings suggest that utilizing unitary weight matrices in recurrent neural networks may have broad applicability across various domains.