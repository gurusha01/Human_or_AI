This manuscript presents a novel end-to-end deep feature embedding approach that integrates similarity metric learning with hard sample mining techniques. The similarity between two feature embeddings is calculated using both the feature difference vector and the mean vector. In the hard sample mining process, the authors identify the hard positive pair with the maximum distance within the same class and then select the sample with the minimum distance from different classes as the hard negative for each sample in the chosen positive pair. The proposed method demonstrates impressive results in fine-grained image retrieval, transfer learning, and zero-shot learning tasks. The paper is well-structured and clear. The performance of the proposed method is notable across three computer vision tasks. However, several concerns need to be addressed: 
1. The utilization of the feature mean vector to incorporate absolute position information in metric learning, as introduced by Xiong et al. [35], may limit the novelty of the proposed PDDM method. 
2. The claim of using a superior local Euclidean distance metric for hard sample mining by selecting the most dissimilar positive pair and the most similar negative pair in each batch may not be efficient for learning, especially given the small batch size of 64, which only utilizes 4 samples. Moreover, the impact of the crucial parameter of batch size on this method is not evaluated. 
3. The selection process for other algorithmic parameters lacks discussion. For instance, the absence of experimental analysis on the parameter lambda makes the effectiveness of feature-level metric learning uncertain. 
4. It is unclear why selecting the hardest samples would not lead to bad local minima in this method, as hard sample mining is performed throughout the learning process, not just in the initial training iterations. It would be beneficial to demonstrate the performance of the method without hard sample mining. 
5. The method treats all visual classes as equal and unrelated, disregarding the semantic relationships among them. For example, cat and dog should be semantically closer than cat and bicycle. It is worth investigating whether incorporating semantic information could enhance the learning of feature embeddings in this method. 
6. There is a typo in Eq. 2, where u1 and v1 should be denoted as u' and v'.