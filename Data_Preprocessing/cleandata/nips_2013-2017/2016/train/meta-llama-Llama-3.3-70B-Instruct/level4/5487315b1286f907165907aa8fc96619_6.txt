This manuscript presents a novel formulation of two-layer models incorporating latent structure, which preserves a jointly convex training objective. Although the resulting formulation is nonconvex, the authors effectively address this challenge by employing semi-definite programming (SDP) relaxation. The empirical results demonstrate the formulation's superiority over local training methods, showcasing its technical soundness. However, the assumption of a conditional model based on an exponential family for the first layer, coupled with the replacement of the log-partition function Omega with an upper bound, seems unjustified. Furthermore, the formulation is convexified using SDP relaxations. Several concerns remain unclear: (a) What type of solution does the SDP yield for the original problem involving Omega - is it a feasible solution to the original problem or the relaxed version? (b) Does Theorem 1 guarantee that the SDP relaxation provides an exact solution to Problem (15), or merely an approximation? If the latter, obtaining a relaxed solution as an upper bound appears questionable, as it may not accurately represent the original optimization problem.