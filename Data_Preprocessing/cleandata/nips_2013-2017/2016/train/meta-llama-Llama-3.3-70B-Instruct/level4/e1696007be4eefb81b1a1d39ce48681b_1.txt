The authors explore the estimation of Generalized Linear Models (GLMs) in scenarios where the sample size far exceeds the number of parameters. They leverage the property that the GLM estimator can be proportional to the Ordinary Least Squares (OLS) estimator and demonstrate the broader applicability of this relationship in random design problems. A two-step estimation approach is proposed, consisting of an initial fast OLS estimation followed by the utilization of a rapid root finder to determine the proportionality constant. The efficacy of their method is showcased on benchmark datasets, where it achieves the desired test error significantly faster than a diverse range of standard first and second-order optimization algorithms. Overall, the contribution is deemed valuable, with the paper being well-structured and clearly presented. The central idea appears both novel and promising. However, several concerns are raised: the experimental comparisons are noted to be extensive yet lacking in the inclusion of stochastic optimization methods, such as SVRG (Johnson & Zhang, 2013), which could potentially outperform the compared methods. Furthermore, the theorems, particularly Proposition 2, are limited to sub-Gaussian random variables, which may not perform adequately with data exhibiting heavier tails. This limitation underscores the importance of random-projection based estimators proposed by Dhillon et al., despite the potential increase in computational time. The omission of experiments involving heavier-tailed data raises questions about the algorithm's robustness and practical utility. As a result, the usefulness of the algorithm in its current form is unclear, suggesting that practitioners must exercise caution, especially with potentially heavy-tailed data. Alternatively, employing stochastic variance-reduced optimization methods might yield similar speedups without the need for vigilant monitoring.