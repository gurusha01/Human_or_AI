The paper explores the relationship between the regression coefficients of a Generalized Linear Model (GLM) with canonical link and those obtained through Ordinary Least Squares (OLS) when predictors are normally distributed. The authors leverage this relationship to propose a method for reducing the computational complexity of fitting GLMs by first estimating coefficients via OLS and then determining a proportionality constant using a univariate root-finding method. They demonstrate that this approach yields approximate results even outside the Gaussian context and provide bounds on the discrepancy between GLM and scaled OLS coefficients. The core idea is intriguing and appears novel. The initial sections (1-3) are well-written, whereas the latter part of the paper seems somewhat rushed. 
It would be beneficial to clarify early on that the proposed method is applicable only when the canonical link is used, as currently stated in line 92, to avoid confusion with statements like "any GLM task" that suggest broader applicability without the canonical link requirement. In Figure 1, plotting the relative accuracy of the proposed method (MSE(SLS)/MSE(MLE)) could offer more insights, and it should be specified whether sub-sampling or the full sample is used for SLS. The performance of STS (the proposed method) doing strictly worse than MLE in terms of accuracy in the right plot of Figure 1, despite Gaussian design and Proposition 1, is puzzling and may be due to the extra variance introduced by estimating the constant c.
The use of sub-sampling in the evaluation of STS complicates the interpretation of results in Section 5, as it becomes challenging to distinguish between the contributions of STS and sub-sampling to speed-up and accuracy. Including Iteratively Reweighted Least Squares (IRLS) in the comparison, as suggested in the discussion about initialization for MLE algorithms, would be valuable, especially given its popularity and the availability of default initialization methods as noted by Wood (2006).
The method for determining the minimum achievable test error, which sets it as the maximum of the final test errors across all estimation methods, could lead to high test error values if any method fails to converge. It should be confirmed whether all methods converged on all datasets. The results presented in Figure 2 and Table 1 should ideally be based on multiple runs to ensure reliability, rather than a single run. Figure 2 is dense and could be improved by removing redundant elements and enlarging the lines in the legend. It's also worth noting in the caption that STS does not start from zero due to the fixed cost of OLS.
Corollary 1 and its implications, particularly the assumption beta > eta / s and the choice of lambda = eta / (c*s), are not immediately clear and require further explanation. The definition of Lambda_min in Proposition 2 should be clarified in the main text, not just the Appendix. 
Minor points include the equivalence statement between logistic regression with Gaussian design and Fisher's discriminant analysis, which contradicts Friedman et al. (2008), suggesting they are different even with Gaussian design due to discriminant analysis exploiting predictor normality. The repetition in discussing the rate of convergence and the sentence structure in the future research direction regarding support vector machines could be improved for clarity. 
References include Wood (2006) and Friedman et al. (2008), which are relevant to the discussions on GLMs, discriminant analysis, and statistical learning elements.