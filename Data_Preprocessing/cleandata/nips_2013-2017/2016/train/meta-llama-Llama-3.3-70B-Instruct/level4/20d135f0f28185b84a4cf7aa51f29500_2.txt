This paper introduces a novel approach to training ensembles of Deep Neural Networks (DNNs) using an oracle that identifies the optimal network for each training instance, applying backpropagation only to the selected network. This method allows the networks to specialize, resulting in ensembles that effectively cover a wide range of possible outputs. The authors demonstrate the efficacy of their approach across various domains, achieving impressive performance. Overall, I find this to be a strong paper that presents an intriguing idea for training DNN ensembles with an oracle-guided backpropagation update, promoting specialization in classification tasks. The presentation is clear, and the experiments are comprehensive, spanning multiple domains. This work deserves publication, and its potential would be further enhanced if it addressed the exploitation of the trained DNN ensembles, currently a notable omission. While the paper shows that the ensemble can provide a broad spectrum of results and aligns with metrics like top-k for specific problems (e.g., ImageNet), it lacks a detailed exploration of how to utilize the resulting ensemble in practical applications. This gap, although somewhat problem-specific, is crucial for the approach's practical usability. Additionally, Figure 2 contains pseudo-code as a raster image; it would be more readable and professional to integrate the pseudo-code directly into the text or embed it as a vector image.