This paper presents a novel variant of the CMA-ES, a state-of-the-art stochastic optimization algorithm, which employs a derivative-free approach. By integrating multiple learning mechanisms, the algorithm adapts the parameters of a multivariate normal sampling distribution, effectively modifying an underlying metric. The proposed method introduces an innovative update for the covariance matrix using triangular Cholesky factors, resulting in significantly reduced computational complexity and memory requirements compared to existing updates. A thorough numerical evaluation demonstrates that this new approach maintains the performance of the standard CMA-ES in terms of function evaluations while substantially decreasing the algorithm's wall-clock time, particularly in high-dimensional settings. This contribution is highly significant in the field of optimization, and it is anticipated that this variant will become the default choice for medium to large-scale applications due to its advantages, including reduced memory usage, complexity, and the ability to obtain eigenvalues of the covariance matrix without additional cost. The paper is well-written, and the methodology is carefully assessed. The connection to machine learning, information geometry, and its applications in reinforcement and supervised learning makes this contribution a valuable addition to the NIPS conference. However, a minor suggestion is to clarify how the comparability of different implementations is ensured in terms of CPU usage, including details on the implementation language used. Additionally, the comment on Figure 2 suggests that differences become more pronounced with higher dimensionality, but this does not appear to hold for the Cigar and Discus functions in 256 dimensions, warranting further explanation.