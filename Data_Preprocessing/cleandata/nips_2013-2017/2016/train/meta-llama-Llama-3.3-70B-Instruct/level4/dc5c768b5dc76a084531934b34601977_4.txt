This paper addresses a crucial issue in machine learning - minimizing unnecessary changes in prediction outputs, referred to as "prediction churn," which affects the usability and statistical validation of trained models. The authors propose a conceptually intriguing fixed-point approach using Markov Chain Monte Carlo (MCMC) techniques, combining regularization of new classifiers with respect to previous ones and training on perturbed datasets. This method aims to yield a stable classifier through iteration, robust to real-world data drift. The paper introduces two stability operators with tunable hyperparameters, balancing churn and accuracy, and provides theoretical bounds on churn in limited settings. Although lacking a comprehensive theoretical analysis of the MCMC process, the paper presents experiments on real-world datasets and various learning algorithms, demonstrating the effectiveness of one-step and iterative de-churning in reducing churn without compromising accuracy. The fixed-point MCMC approach is compelling but would benefit from rigorous theoretical backing, which could enhance the paper's technical strength and self-containment. Alternatively, focusing on single-step stabilization with more general theoretical results could improve the paper's cohesion. The presentation lacks polish, with issues such as unclear references and unconventional notation order, which hampers readability. Exploring connections to online Bayesian learning could also enrich the analysis, particularly for the Markov chain aspect. A more detailed discussion on the potential impact of this research would be a valuable addition, highlighting its significance and future directions. Overall, the problem tackled is significant, the techniques are broadly applicable, and the experimental results are promising, making the paper a valuable contribution despite areas for improvement.