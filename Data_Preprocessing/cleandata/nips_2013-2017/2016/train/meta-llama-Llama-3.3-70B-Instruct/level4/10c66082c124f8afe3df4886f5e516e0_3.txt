The authors introduce a novel approach to learning the parameters of the Word Mover's Distance (WMD), essentially the Earth Mover's Distance (EMD) computed on text documents utilizing word embeddings to determine bin-to-bin dissimilarities. They parameterize the EMD cost matrix, or ground metric, using a Mahalanobis distance between vector representations. Additionally, the authors propose learning histogram coefficients reweighting, a concept also explored in Lebanon's work, as noted below. The paper provides the gradients for NCA-type costs for this problem and presents compelling experimental evidence. Overall, the paper appears to be a well-crafted combination of existing techniques, yielding convincing experimental results. I would support its acceptance. The paper's strengths lie in its simplicity and the relatively straightforward idea, although its implementation and testing are non-trivial. Consequently, the experimental section is a strong aspect of this paper. Areas for improvement include better handling of the interplay between regularized and non-regularized formulations, enhanced mathematical rigor (as computations and notations are somewhat sloppy), and ideally, the inclusion of an algorithmic box to provide clearer insight into the authors' proposal. Several minor comments are also noteworthy: 
- The use of Euclidean distance between word embeddings as a cost in Eq.1, which later becomes the squared Euclidean metric for Mahalanobis metric learning (essentially the 2-Wasserstein distance), introduces inconsistency. It might be more straightforward to define WMD as the p-Wasserstein distance between clouds of word embeddings, regardless of p. 
- The "reweighting" approach in Eq. 5 is not novel, having been previously proposed by Guy Lebanon in the context of text document classification ("metric learning for text documents"), and also explored in "Unsupervised Riemannian Metric Learning for Histograms Using Aitchison Transformations" by Le/Cuturi. 
- It would be beneficial to provide experimental insights into the importance of the weight vector w in the learned metric, assessing whether this innovation has a significant impact or relevance. 
- In the related work section, it would be interesting to more clearly delineate the differences with previous studies. For instance, this paper builds upon the work of Cuturi/Avis but incorporates entropy regularization, which significantly enhances scalability and speed. The cost function also differs, being continuous and parameterized with linear maps, unlike the discrete cost in the aforementioned work. These distinctions are currently not highlighted. The paper also draws from Wang et al, but their work, as it stands, is not publishable, and the OT optimization, which is run only once, does not function as proposed.