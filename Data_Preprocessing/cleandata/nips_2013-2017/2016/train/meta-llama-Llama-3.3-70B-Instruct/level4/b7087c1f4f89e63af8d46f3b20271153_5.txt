This paper proposes a pragmatic approach to dimensionality reduction for extremely large-scale sparse matrices, leveraging a weighted subset of the data that remains unaffected by the size and dimensionality of the input. The authors also present an efficient algorithm for achieving this reduction, accompanied by rigorous bounds on its size and computational time. Additionally, they demonstrate a system implementation of this algorithm and its application to latent semantic analysis (LSA) of the entire English Wikipedia, which is an intriguing contribution. However, the paper would benefit from a more comprehensive discussion of its relation to existing algorithms designed for large-scale data processing. The writing is somewhat opaque, and the text requires further proofreading to eliminate numerous typos and enhance clarity. It would be helpful to explicitly define the notation used, such as the meaning of "||" in equations (2)-(i,ii), and to provide a clearer explanation of equation (3). Furthermore, the concept of distance between A and S appears to align with the manifold distance concept presented in [R1], and citing this work would be appropriate. The reference format should also be standardized throughout the paper. Despite these suggestions, the paper exhibits novelty, but significant improvements in language and presentation are necessary to fully convey its potential.