This manuscript presents a novel Monte Carlo-based planning algorithm, TrailBlazer, which operates under the assumption of access to a generative model of the Markov Decision Process (MDP). The primary objective of TrailBlazer is to efficiently compute the optimal value function of the root node, V(s0), by minimizing the number of calls to the generative model. The algorithm is designed to handle both finite and infinite state spaces within a discounted reward setting. 
The paper provides several key theoretical guarantees for TrailBlazer, including Probably Approximately Correct (PAC) consistency, a high-probability upper bound on the number of calls to the generative model for finite state spaces, and an upper bound on the expected number of calls for infinite state spaces. These guarantees position TrailBlazer as either improving upon previous worst-case upper bounds in certain scenarios (such as finite state spaces with stochastic dynamics), matching existing results in others (like deterministic dynamics or scenarios without control), or offering new insights, particularly for infinite state spaces.
TrailBlazer's operational mechanism involves alternating between two node types: Avg and Max. The Avg node calculates the average value of its children, generated based on transition probabilities, effectively functioning as a Monte Carlo estimator. The parameter m is crucial as it controls the variance of this estimator. In contrast, the Max node aims to identify the child node with the maximum value by probabilistically eliminating nodes that cannot achieve the maximum value.
This paper is commendable for introducing a novel algorithm with significant potential for application across various domains. TrailBlazer not only boasts basic theoretical justification, such as consistency, but also offers stronger guarantees than existing knowledge, particularly for finite state spaces with generative models and stochastic systems. The manuscript is generally well-written, although it could benefit from additional intuitive explanations to clarify why TrailBlazer outperforms other approaches. The induction-based proofs on the tree structure, while not straightforward, contribute to the paper's technical depth.
Several aspects warrant further clarification or consideration: 
1. The expectation-based guarantee in Theorem 3 prompts the question of whether there are fundamental limitations preventing a stronger guarantee.
2. The algorithm's current formulation does not leverage potential regularities in the value function, such as smoothness. Exploring whether incorporating such properties, akin to the StoSO algorithm, could enhance performance is worthwhile.
3. For scenarios with non-vanishing action-gaps, especially when these gaps follow a state-dependent distribution, an analysis similar to that by Farahmand could provide valuable insights into TrailBlazer's applicability and efficiency.
4. Section 3.1 could be expanded to better contrast the separate bias and variance computation in existing algorithms versus TrailBlazer's approach, enhancing clarity for readers.
5. Several typographical errors were noted, including those at lines L81, L84, L220, L229, and L472, as well as the inconsistent representation of \Delta in Appendix D.
Overall, the paper contributes meaningfully to the field by presenting a novel algorithm with robust theoretical backing, suggesting avenues for both theoretical refinement and practical application. Addressing the raised questions and revising for clarity and readability will further strengthen the manuscript.