This paper proposes a novel extension to Long Short-Term Memory (LSTM) models, incorporating "time gates" that control updates to the memory cells, modulated by oscillators that only permit updates at specific points during the oscillation cycle. The periodicity and phase shift of these gates are learned during training, allowing the model to capture cyclical structures in data. The introduction of time gates enables the model to sample the gate values at irregular time intervals, which is a unique property of this approach. The phased LSTMs are evaluated on four experiments, including classifying sinusoidal signals, the adding task, event-based encoding of MNIST, and a lip-reading dataset, outperforming traditional LSTMs in all cases.
LSTMs have become a fundamental component in neural network architectures, particularly in sequential problems, due to their ability to learn tasks that require persisting memories over many time steps. The key idea behind LSTMs is to use gating mechanisms to mitigate the "vanishing gradients" problem that occurs in standard recurrent networks. The proposed extension builds upon this concept, introducing time gates that are controlled by continuous-time oscillators, allowing for more flexible and efficient processing of sequential data.
The paper presents an interesting idea for improving the performance of LSTMs, particularly on tasks with cyclical structures. The model and motivations are well-explained, and the experimental results demonstrate the effectiveness of the proposed approach. However, there are aspects of the analysis and results that could be improved. For instance, clarifying the time-sampling procedure in cases where sampling is "asynchronous" would be beneficial, as the remainder of the LSTM appears to be a discrete-time update.
It would be fascinating to include an analysis or insight into the solutions the model learns on simple tasks, such as adding or classifying sinusoids. Additionally, empirical analysis supporting the potential for reduced computational load due to the gating mechanism would strengthen the manuscript. The connection to reservoir computing, which also maintains memories using fixed oscillators, should be acknowledged and discussed.
The choice of experimental comparisons is somewhat eclectic, and including a larger, standard sequential benchmark without obvious oscillatory structure, such as language modeling or machine translation, would help assess the potential impact of this work. Comparing the proposed model to a standard LSTM with heavily biased forget gates and input gates would also be a useful evaluation to determine if simple initialization tricks can reproduce the results.
Overall, the paper presents a novel approach that will be of interest to the field, and with some revisions to address the suggested improvements, it has the potential to make a significant contribution to the development of more efficient and effective sequential models. The ideas presented in this paper bear some resemblance to existing work, such as clockwork RNNs and recurrent neural network regularization, and citing these works would provide a more comprehensive understanding of the proposed approach.