This paper proposes a novel sampling algorithm, Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD), which aims to reduce the bias of Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) methods while keeping the variance at a reasonable level. The algorithm is based on the Richardson-Romberg extrapolation method, which involves running two SG-MCMC chains in parallel with different step sizes.
The paper provides a thorough theoretical analysis of SGRRLD, including its asymptotic consistency, central limit theorem, and non-asymptotic bounds for bias and mean squared error. The results show that SGRRLD attains higher rates of convergence than existing SG-MCMC methods with first-order integrators, both in finite-time and asymptotically.
The paper also presents experimental results on both synthetic and real data, demonstrating the effectiveness of SGRRLD in reducing bias and improving convergence rates. The experiments include a linear Gaussian model and a large-scale matrix factorization problem, where SGRRLD is compared to existing SG-MCMC methods, including Stochastic Gradient Langevin Dynamics (SGLD) and Stochastic Gradient Hamiltonian Monte Carlo (SGHMC).
The strengths of the paper include:
* A clear and well-motivated introduction to the problem of bias in SG-MCMC methods
* A thorough and rigorous theoretical analysis of SGRRLD
* Experimental results that demonstrate the effectiveness of SGRRLD in reducing bias and improving convergence rates
* A clear and well-organized presentation of the results
The weaknesses of the paper include:
* The paper assumes a significant amount of background knowledge in SG-MCMC methods and Markov chain Monte Carlo theory, which may make it difficult for non-experts to follow
* Some of the theoretical results, such as the central limit theorem, are presented without proof, which may make it difficult for readers to fully understand the underlying mathematics
* The experimental results are limited to a few specific examples, and it is not clear how well SGRRLD will perform in other settings
Overall, the paper presents a significant contribution to the field of SG-MCMC methods, and the results have the potential to improve the accuracy and efficiency of Bayesian inference in large-scale applications.
Arguments for acceptance:
* The paper presents a novel and well-motivated algorithm for reducing bias in SG-MCMC methods
* The theoretical analysis is thorough and rigorous, and the results are well-supported by experimental evidence
* The paper has the potential to make a significant impact in the field of Bayesian inference and machine learning
Arguments against acceptance:
* The paper assumes a significant amount of background knowledge in SG-MCMC methods and Markov chain Monte Carlo theory, which may limit its accessibility to non-experts
* The experimental results are limited to a few specific examples, and it is not clear how well SGRRLD will perform in other settings
* Some of the theoretical results are presented without proof, which may make it difficult for readers to fully understand the underlying mathematics.