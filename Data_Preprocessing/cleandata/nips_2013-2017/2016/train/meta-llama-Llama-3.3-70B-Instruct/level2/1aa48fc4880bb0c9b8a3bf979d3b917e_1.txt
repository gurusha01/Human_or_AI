This paper presents a significant contribution to the field of machine learning by extending stochastic variance reduction methods to convex-concave saddle-point problems. The authors provide a thorough analysis of the proposed algorithms, including their convergence rates and computational complexities. The paper is well-structured, and the writing is clear and concise.
The main claims of the paper are well-supported by theoretical analysis and experimental results. The authors demonstrate the effectiveness of their algorithms on various machine learning problems, including supervised learning with non-separable losses or regularizers, robust optimization, and convex relaxation of unsupervised learning. The experimental results show that the proposed algorithms outperform existing batch and stochastic algorithms, especially for ill-conditioned problems.
The paper reflects a good understanding of the field, and the authors provide a comprehensive review of existing work on saddle-point problems and stochastic variance reduction methods. The notation and terminology used are standard in the field, and the authors provide clear explanations of the key concepts and techniques.
The novelty of the paper lies in the extension of stochastic variance reduction methods to saddle-point problems, which is a significant improvement over existing algorithms. The authors also provide a simple extension of the "catalyst" framework to accelerate the proposed algorithms, leading to improved convergence rates.
The paper is well-organized, and the authors provide a clear and concise presentation of the main results. The experimental results are thorough and well-presented, and the authors provide a detailed analysis of the results.
However, there are some limitations to the paper. The authors assume that the strong convexity-concavity constants are known, which may not always be the case in practice. The authors also do not provide a detailed analysis of the computational complexity of the proposed algorithms, which could be an important consideration in practice.
Overall, the paper is well-written, and the authors provide a significant contribution to the field of machine learning. The proposed algorithms have the potential to improve the efficiency and effectiveness of machine learning models, especially for large-scale problems.
Arguments for acceptance:
* The paper presents a significant contribution to the field of machine learning.
* The proposed algorithms are well-supported by theoretical analysis and experimental results.
* The paper reflects a good understanding of the field, and the authors provide a comprehensive review of existing work.
* The novelty of the paper lies in the extension of stochastic variance reduction methods to saddle-point problems.
Arguments against acceptance:
* The authors assume that the strong convexity-concavity constants are known, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithms.
* The paper could benefit from a more detailed analysis of the experimental results and a comparison with other state-of-the-art algorithms.