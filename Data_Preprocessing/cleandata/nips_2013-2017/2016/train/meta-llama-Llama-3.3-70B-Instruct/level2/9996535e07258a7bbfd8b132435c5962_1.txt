This paper proposes a novel extension of the encoder-decoder framework, called the review network, which enhances the existing encoder-decoder model by performing multiple review steps with attention on the encoder hidden states. The review network outputs a set of thought vectors that capture global properties in a compact vector representation, which are then used as input to the attention mechanism in the decoder. The authors demonstrate that conventional encoder-decoders are a special case of their framework and show that their model improves over state-of-the-art encoder-decoder systems on image captioning and source code captioning tasks.
The paper is well-written, and the authors provide a clear explanation of their model and its components. The experimental results are impressive, and the authors provide a thorough analysis of their model's performance on different tasks. The paper also provides a good review of related work and clearly explains how their model differs from existing approaches.
One of the strengths of the paper is its ability to provide a generic architecture that can be integrated into existing encoder-decoder models. The authors demonstrate the effectiveness of their model on two different tasks, image captioning and source code captioning, which shows the robustness and generalizability of their approach. The use of discriminative supervision is also a nice touch, as it allows the model to learn more informative representations.
However, there are some limitations to the paper. One of the main limitations is that the authors do not provide a clear explanation of how the number of review steps is determined. The authors mention that they set the number of review steps to 8, but it is not clear why this number was chosen or how it was determined. Additionally, the authors do not provide a thorough analysis of the computational cost of their model, which could be a limitation for large-scale applications.
Overall, I would argue that the paper is well-written and provides a significant contribution to the field of natural language processing. The model is novel and effective, and the authors provide a thorough analysis of its performance on different tasks. However, there are some limitations to the paper that could be addressed in future work.
Arguments pro acceptance:
* The paper proposes a novel and effective architecture for encoder-decoder models
* The authors provide a thorough analysis of their model's performance on different tasks
* The model is generic and can be integrated into existing encoder-decoder models
* The use of discriminative supervision is a nice touch and allows the model to learn more informative representations
Arguments con acceptance:
* The authors do not provide a clear explanation of how the number of review steps is determined
* The authors do not provide a thorough analysis of the computational cost of their model
* The paper could benefit from more experiments on different tasks and datasets to demonstrate the robustness and generalizability of the approach.