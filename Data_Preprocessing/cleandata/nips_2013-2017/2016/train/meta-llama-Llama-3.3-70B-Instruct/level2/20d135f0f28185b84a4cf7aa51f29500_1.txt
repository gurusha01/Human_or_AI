This paper proposes a novel approach to training deep neural networks, called Stochastic Multiple Choice Learning (sMCL), which enables the production of multiple diverse and accurate predictions for a given input. The authors argue that in many real-world applications, it is beneficial to provide multiple highly likely hypotheses rather than a single prediction, and they formulate this task as a learning problem over an ensemble of deep networks.
The main claim of the paper is that sMCL achieves lower oracle error compared to existing methods on a wide range of tasks and deep architectures. The authors support this claim through extensive experiments on image classification, semantic segmentation, and image captioning tasks, demonstrating the broad applicability and efficacy of sMCL.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of multiple choice learning and the limitations of existing approaches. The technical contribution of the paper is significant, as it introduces a novel stochastic gradient descent-based approach to minimize the loss with respect to an oracle. The authors also provide a thorough analysis of the results, including qualitative examples of the diverse solutions produced by sMCL.
The strengths of the paper include its novelty, technical soundness, and clarity of presentation. The authors provide a comprehensive review of related work and demonstrate a clear understanding of the field. The experiments are well-designed and thoroughly evaluated, providing strong evidence for the effectiveness of sMCL.
The weaknesses of the paper are minor. One potential limitation is that the authors do not provide a detailed analysis of the computational cost of sMCL compared to existing methods. Additionally, the authors could provide more insight into the interpretability of the specializations that emerge among ensemble members.
Overall, I believe that this paper makes a significant contribution to the field of deep learning and multiple choice learning. The authors provide a well-motivated and clearly presented approach to training diverse deep ensembles, and the experimental results demonstrate the efficacy of sMCL. I would recommend accepting this paper for publication.
Arguments pro acceptance:
* The paper proposes a novel and technically sound approach to training deep neural networks.
* The authors provide a comprehensive review of related work and demonstrate a clear understanding of the field.
* The experiments are well-designed and thoroughly evaluated, providing strong evidence for the effectiveness of sMCL.
* The paper is well-written and clearly presented.
Arguments con acceptance:
* The authors do not provide a detailed analysis of the computational cost of sMCL compared to existing methods.
* The authors could provide more insight into the interpretability of the specializations that emerge among ensemble members.