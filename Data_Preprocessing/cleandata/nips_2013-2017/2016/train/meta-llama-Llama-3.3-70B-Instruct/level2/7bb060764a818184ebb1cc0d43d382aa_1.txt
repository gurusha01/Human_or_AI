This paper proposes a novel dropout technique, called multinomial dropout, which samples features or neurons according to a multinomial distribution with different probabilities for different features/neurons. The authors analyze the risk bound of shallow learning with multinomial dropout and establish a distribution-dependent dropout that leads to faster convergence and smaller generalization error. They also propose an efficient evolutional dropout for deep learning, which adapts the sampling probabilities to the evolving distributions of layers' outputs.
The paper is well-written, and the authors provide a clear and thorough explanation of their methodology and theoretical analysis. The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts, achieving faster convergence and smaller testing error compared to the standard dropout.
The strengths of the paper include:
* The proposal of a novel dropout technique that takes into account the importance of different features/neurons
* A thorough theoretical analysis of the risk bound of shallow learning with multinomial dropout
* The development of an efficient evolutional dropout for deep learning that adapts to the evolving distributions of layers' outputs
* Experimental results that demonstrate the effectiveness of the proposed dropouts
The weaknesses of the paper include:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The authors do not provide a clear comparison with other dropout techniques, such as batch normalization
* The experimental results are limited to a few benchmark datasets, and it is unclear how the proposed dropouts will perform on other datasets
Overall, the paper presents a significant contribution to the field of deep learning, and the proposed dropouts have the potential to improve the performance of neural networks. However, further research is needed to fully understand the limitations and potential applications of the proposed techniques.
Arguments for acceptance:
* The paper proposes a novel and interesting dropout technique that takes into account the importance of different features/neurons
* The theoretical analysis is thorough and well-explained
* The experimental results demonstrate the effectiveness of the proposed dropouts
Arguments against acceptance:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The comparison with other dropout techniques is limited
* The experimental results are limited to a few benchmark datasets
Recommendation: Accept with minor revisions. The authors should address the weaknesses of the paper, including providing a clear comparison with other dropout techniques and experimenting with more datasets. Additionally, the authors should consider providing more details on how to implement the proposed dropouts in practice.