This paper presents a novel method, Bounding Divergences with Reverse Annealing (BREAD), for evaluating the accuracy of approximate posterior inference algorithms, specifically those based on Markov chain Monte Carlo (MCMC) and annealed importance sampling (AIS). The authors extend the bidirectional Monte Carlo (BDMC) technique to bound the Jeffreys divergence between the true posterior distribution and the distribution of approximate samples. The method is applicable to algorithms that can be viewed as importance sampling over an extended state space, such as AIS or sequential Monte Carlo (SMC).
The paper makes several key contributions. First, it shows that BDMC yields an estimator that upper bounds in expectation the Jeffreys divergence of approximate samples from the true posterior. Second, it presents a technique for exactly computing both the true Jeffreys divergence and the upper bound on small examples, demonstrating that the upper bound is often a good match in practice. Third, it proposes a protocol for using BDMC to evaluate the accuracy of approximate inference on real-world datasets by simulating data from fitted hyperparameters. Finally, it extends both WebPPL and Stan to implement BREAD and validates its effectiveness on various probabilistic models.
The strengths of the paper include its rigorous theoretical analysis, thorough experimental evaluation, and demonstration of the method's usefulness in practice. The authors provide a clear and well-structured presentation of their work, making it easy to follow and understand. The experiments are well-designed and comprehensive, covering various models and datasets.
However, there are some limitations and potential areas for improvement. One limitation is that the method is only directly applicable to certain algorithms, such as AIS or SMC. Additionally, the protocol for using BDMC on real-world data relies on simulating data from fitted hyperparameters, which may not always be representative of the real data. The authors acknowledge these limitations and provide some validation of their approach, but further work may be needed to fully address these concerns.
In terms of novelty, the paper presents a significant improvement over existing methods for evaluating approximate posterior inference algorithms. The use of BDMC to bound the Jeffreys divergence is a new and innovative approach that provides a rigorous and general framework for evaluating the accuracy of approximate inference. The paper also demonstrates the potential of BREAD as a tool for debugging and testing the correctness of probabilistic programming languages.
Overall, the paper is well-written, clearly presented, and makes a significant contribution to the field of probabilistic inference. The authors demonstrate the effectiveness of their method on various models and datasets, and provide a thorough analysis of its strengths and limitations. The paper has the potential to be a valuable resource for researchers and practitioners working in the field of probabilistic inference.
Arguments for acceptance:
* The paper presents a novel and innovative method for evaluating approximate posterior inference algorithms.
* The method is rigorously analyzed and demonstrated to be effective on various models and datasets.
* The paper provides a clear and well-structured presentation of the work, making it easy to follow and understand.
* The authors demonstrate the potential of BREAD as a tool for debugging and testing the correctness of probabilistic programming languages.
Arguments against acceptance:
* The method is only directly applicable to certain algorithms, such as AIS or SMC.
* The protocol for using BDMC on real-world data relies on simulating data from fitted hyperparameters, which may not always be representative of the real data.
* Further work may be needed to fully address the limitations of the method and to demonstrate its effectiveness on a wider range of models and datasets.