This paper presents a significant contribution to the field of online learning and game theory, introducing the concept of Low Approximate Regret (LAR) and its application to repeated games. The authors demonstrate that LAR is a ubiquitous property, satisfied by many simple and efficient algorithms, including the vanilla Hedge algorithm. The main claim of the paper is that LAR implies fast convergence to approximate optimality in a large class of repeated games, including smooth games and dynamic population games.
The support for this claim is thorough and well-structured, with a clear explanation of the LAR property and its relationship to existing concepts in online learning. The authors provide a comprehensive analysis of the convergence of LAR algorithms in various settings, including full information feedback, bandit feedback, and dynamic population games. The theoretical results are supported by examples of simple LAR algorithms, such as Hedge and Optimistic Hedge, and a new bandit algorithm that achieves LAR with competitive parameters.
The paper is well-written, and the authors demonstrate a deep understanding of the field, citing relevant literature and providing clear explanations of complex concepts. The organization of the paper is logical, with a clear introduction, problem formulation, and analysis of the results.
The significance of this work lies in its ability to provide fast convergence guarantees for a broad class of algorithms in various settings, including realistic scenarios with limited feedback. The authors demonstrate that LAR algorithms can achieve approximate optimality with high probability, even in the presence of churn in the population. The results have important implications for the design of learning algorithms in repeated games and can be applied to various domains, such as auctions and congestion games.
The novelty of the paper lies in the introduction of the LAR property and its application to repeated games. The authors provide a new perspective on the convergence of online learning algorithms, demonstrating that LAR is a key property that enables fast convergence to approximate optimality. The paper also strengthens previous results, such as those in [28], by providing a more general and realistic framework for analyzing the convergence of learning algorithms in repeated games.
In terms of completeness, the paper provides sufficient details for reproducibility, including proofs and examples of LAR algorithms. The authors also discuss the limitations of their results, including the assumption of smooth games and the need for further research on the application of LAR to more general settings.
Overall, this paper is a significant contribution to the field of online learning and game theory, providing a new perspective on the convergence of learning algorithms in repeated games. The results are well-supported, and the paper is well-written and easy to follow. The authors demonstrate a deep understanding of the field and provide a clear explanation of the significance and novelty of their work.
Arguments pro acceptance:
* The paper introduces a new and significant concept, Low Approximate Regret, which has important implications for the design of learning algorithms in repeated games.
* The authors provide a comprehensive analysis of the convergence of LAR algorithms in various settings, including full information feedback, bandit feedback, and dynamic population games.
* The paper strengthens previous results, such as those in [28], by providing a more general and realistic framework for analyzing the convergence of learning algorithms in repeated games.
* The authors demonstrate a deep understanding of the field, citing relevant literature and providing clear explanations of complex concepts.
Arguments con acceptance:
* The paper assumes smooth games, which may not be realistic in all settings.
* The authors do not provide experimental results to support their theoretical claims.
* The paper may benefit from further discussion of the limitations of the results and the need for further research on the application of LAR to more general settings.