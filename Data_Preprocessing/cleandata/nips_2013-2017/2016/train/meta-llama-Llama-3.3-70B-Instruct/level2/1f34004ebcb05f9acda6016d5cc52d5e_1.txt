This paper proposes two novel optimization methods, a gradient-free and a gradient-based approach, to solve the non-convex loss-minimization problem of learning Supervised PageRank models. The authors provide theoretical guarantees on the convergence rate and complexity bounds for both methods, which is a significant improvement over the state-of-the-art gradient-based method. The paper is well-structured, and the authors clearly outline the main claims, which are supported by theoretical analysis and experimental results.
The main strengths of the paper are:
1. Theoretical guarantees: The authors provide rigorous theoretical analysis of the convergence rates and complexity bounds for both methods, which is essential for establishing the reliability and efficiency of the proposed approaches.
2. Improved performance: The experimental results demonstrate that the proposed methods outperform the state-of-the-art gradient-based method in terms of the loss function, which is a significant improvement.
3. Novelty: The combination of random gradient-free and gradient optimization methods with the concept of an inexact oracle is a new and innovative approach, which has the potential to be applied to other optimization problems.
However, there are some weaknesses and limitations:
1. Complexity: The paper assumes a certain level of familiarity with optimization methods and linear algebra, which might make it challenging for non-experts to follow.
2. Limited scope: The paper focuses on a specific problem, learning Supervised PageRank models, which might limit the broader applicability of the proposed methods.
3. Hyperparameter tuning: The authors mention that the choice of hyperparameters, such as the Lipschitz constant and the accuracy, can affect the performance of the algorithms, but they do not provide a detailed analysis of the sensitivity of the methods to these parameters.
To improve the paper, I suggest:
1. Providing more intuitive explanations: Adding more intuitive explanations and examples to help non-experts understand the proposed methods and their significance.
2. Conducting a more comprehensive analysis: Performing a more comprehensive analysis of the sensitivity of the methods to hyperparameters and exploring the applicability of the proposed approaches to other optimization problems.
3. Including more comparisons: Comparing the proposed methods with other state-of-the-art optimization algorithms to further establish their superiority.
Overall, the paper presents a significant contribution to the field of optimization and has the potential to impact the development of more efficient and reliable algorithms for learning Supervised PageRank models. With some revisions to address the weaknesses and limitations, the paper can become even more robust and impactful.
Arguments pro acceptance:
* The paper presents novel and innovative optimization methods with theoretical guarantees.
* The experimental results demonstrate improved performance over the state-of-the-art method.
* The paper has the potential to impact the development of more efficient and reliable algorithms for learning Supervised PageRank models.
Arguments con acceptance:
* The paper assumes a certain level of familiarity with optimization methods and linear algebra.
* The scope of the paper is limited to a specific problem.
* The sensitivity of the methods to hyperparameters is not thoroughly analyzed.