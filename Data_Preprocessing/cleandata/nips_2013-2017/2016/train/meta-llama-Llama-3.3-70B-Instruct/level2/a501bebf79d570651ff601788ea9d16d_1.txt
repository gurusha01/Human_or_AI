This paper proposes a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). The authors introduce three algorithms: BPTT-HSM, BPTT-ISM, and BPTT-MSM, which balance the trade-off between caching of intermediate results and recomputation. The algorithms are capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost.
The paper is well-written, and the authors provide a clear explanation of the problem, the proposed approach, and the experimental results. The use of dynamic programming to find an optimal memory usage policy is a significant contribution. The authors also provide a thorough analysis of the computational cost and memory usage of the proposed algorithms, as well as a comparison with existing approaches.
The strengths of the paper include:
* The proposal of a novel approach to reduce memory consumption of BPTT, which is a significant problem in training RNNs.
* The use of dynamic programming to find an optimal memory usage policy, which is a powerful technique for solving complex optimization problems.
* The provision of a thorough analysis of the computational cost and memory usage of the proposed algorithms.
* The comparison with existing approaches, which demonstrates the effectiveness of the proposed algorithms.
The weaknesses of the paper include:
* The paper assumes that the memory budget is known in advance, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the time complexity of the proposed algorithms, which is an important consideration in practice.
* The experimental results are limited to a specific setup, and it is not clear how the proposed algorithms would perform in other scenarios.
Overall, the paper is well-written, and the proposed approach is significant. The authors demonstrate the effectiveness of the proposed algorithms, and the paper is a valuable contribution to the field of RNNs.
Arguments pro acceptance:
* The paper proposes a novel approach to reduce memory consumption of BPTT, which is a significant problem in training RNNs.
* The use of dynamic programming to find an optimal memory usage policy is a significant contribution.
* The paper provides a thorough analysis of the computational cost and memory usage of the proposed algorithms.
Arguments con acceptance:
* The paper assumes that the memory budget is known in advance, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the time complexity of the proposed algorithms.
* The experimental results are limited to a specific setup, and it is not clear how the proposed algorithms would perform in other scenarios.
Recommendation: Accept, with minor revisions to address the weaknesses mentioned above.