This paper proposes a novel batch Bayesian optimization algorithm, the parallel knowledge gradient method (q-KG), which is designed to optimize expensive-to-evaluate functions in parallel. The main claim of the paper is that q-KG outperforms existing batch Bayesian optimization algorithms, especially in the presence of noisy function evaluations.
The paper provides a clear and well-structured introduction to the problem of batch Bayesian optimization, and the authors do a good job of motivating the need for a new algorithm. The proposed q-KG algorithm is derived from a decision-theoretical perspective, and the authors provide a detailed explanation of how it works.
The paper also provides an extensive evaluation of the q-KG algorithm on several synthetic functions and real-world problems, including hyperparameter tuning of machine learning algorithms. The results show that q-KG consistently outperforms or is competitive with state-of-the-art benchmark algorithms, especially in the presence of noise.
The strengths of the paper include:
* The proposed q-KG algorithm is novel and has a strong theoretical foundation.
* The paper provides a thorough evaluation of the algorithm on several benchmark problems.
* The authors do a good job of explaining the algorithm and its implementation details.
The weaknesses of the paper include:
* The paper assumes that the function evaluations are independent and identically distributed, which may not always be the case in practice.
* The algorithm requires a significant amount of computational resources to optimize the q-KG factor, which may be a limitation in practice.
* The paper could benefit from a more detailed analysis of the computational complexity of the algorithm.
Overall, the paper is well-written and provides a significant contribution to the field of batch Bayesian optimization. The proposed q-KG algorithm has the potential to be widely adopted in practice, especially in applications where function evaluations are expensive and noisy.
Arguments for acceptance:
* The paper proposes a novel and well-motivated algorithm for batch Bayesian optimization.
* The algorithm has a strong theoretical foundation and is evaluated extensively on several benchmark problems.
* The results show that the algorithm outperforms or is competitive with state-of-the-art benchmark algorithms.
Arguments against acceptance:
* The paper assumes that the function evaluations are independent and identically distributed, which may not always be the case in practice.
* The algorithm requires a significant amount of computational resources to optimize the q-KG factor, which may be a limitation in practice.
* The paper could benefit from a more detailed analysis of the computational complexity of the algorithm.
Recommendation: Accept with minor revisions. The paper is well-written and provides a significant contribution to the field of batch Bayesian optimization. However, the authors should address the limitations of the algorithm and provide a more detailed analysis of its computational complexity.