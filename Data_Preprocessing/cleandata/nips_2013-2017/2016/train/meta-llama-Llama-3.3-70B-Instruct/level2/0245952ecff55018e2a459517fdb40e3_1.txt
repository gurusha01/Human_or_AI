This paper presents Conditional Generative Moment-Matching Networks (CGMMN), a novel framework for learning conditional distributions using a conditional maximum mean discrepancy (CMMD) criterion. The authors extend the capability of Generative Moment-Matching Networks (GMMN) to address a wide range of application problems, including predictive modeling, contextual generation, and Bayesian dark knowledge.
The main claims of the paper are: (1) CGMMN can learn a flexible conditional distribution given some input variables, and (2) the proposed CMMD criterion can effectively measure the difference between two conditional distributions. The authors support these claims with theoretical analysis and experimental results on various tasks, demonstrating competitive performance with state-of-the-art methods.
The paper is well-written, and the authors provide a clear and concise introduction to the background and related work. The technical sections are well-organized, and the notation is consistent throughout the paper. The experimental results are thorough and demonstrate the effectiveness of CGMMN in various applications.
The strengths of the paper include:
* The proposed CMMD criterion is a novel and effective way to measure the difference between two conditional distributions.
* The CGMMN framework is flexible and can be applied to various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge.
* The experimental results are thorough and demonstrate the competitive performance of CGMMN with state-of-the-art methods.
The weaknesses of the paper include:
* The paper assumes that the conditional mean embedding operator CY|X ∈ F ⊗ G, which may not hold in practice. The authors acknowledge this limitation and provide a well-justified approximation.
* The computational cost of the kernel gram matrix grows cubically with the sample size, which may limit the scalability of the method.
Overall, the paper presents a significant contribution to the field of deep generative models and conditional distribution learning. The proposed CGMMN framework and CMMD criterion have the potential to be widely applicable and effective in various tasks. I recommend accepting this paper for publication.
Arguments for acceptance:
* The paper presents a novel and effective framework for learning conditional distributions.
* The experimental results demonstrate the competitive performance of CGMMN with state-of-the-art methods.
* The paper is well-written, and the technical sections are well-organized.
Arguments against acceptance:
* The paper assumes that the conditional mean embedding operator CY|X ∈ F ⊗ G, which may not hold in practice.
* The computational cost of the kernel gram matrix grows cubically with the sample size, which may limit the scalability of the method.
However, the strengths of the paper outweigh the weaknesses, and I believe that the paper makes a significant contribution to the field. Therefore, I recommend accepting this paper for publication.