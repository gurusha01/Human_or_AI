This paper presents a novel approach to dimensionality reduction for large-scale sparse matrices using coresets, which are weighted subsets of the input data. The authors claim to have developed an algorithm that can compute a coreset of size independent of both the number of rows (n) and columns (d) of the input matrix, with a guaranteed approximation error. The main contributions of the paper are: (1) a new algorithm for dimensionality reduction of sparse data, (2) an efficient algorithm for computing the coreset, and (3) an application of the algorithm to compute latent semantic analysis (LSA) of the entire English Wikipedia.
The paper is well-written, and the authors provide a clear overview of the problem, related work, and their approach. The technical sections are detailed and provide a thorough explanation of the algorithms and proofs. The experimental results demonstrate the effectiveness and efficiency of the proposed algorithm, especially on large-scale datasets like Wikipedia.
The strengths of the paper are:
* The authors address a long-standing open problem in dimensionality reduction, which is to compute a coreset that is both small in size and a subset of the original data.
* The proposed algorithm is deterministic, which is a significant improvement over existing randomized algorithms.
* The experimental results demonstrate the scalability and efficiency of the algorithm, even on very large datasets.
The weaknesses of the paper are:
* The paper assumes that the input matrix is sparse, which may not always be the case in practice.
* The algorithm has a high computational complexity, which may make it impractical for very large datasets.
* The authors do not provide a detailed comparison with other state-of-the-art dimensionality reduction algorithms, which would be helpful to understand the strengths and weaknesses of their approach.
Overall, the paper presents a significant contribution to the field of dimensionality reduction and provides a novel approach to computing coresets for large-scale sparse matrices. The authors demonstrate the effectiveness and efficiency of their algorithm through extensive experiments, and the paper is well-written and easy to follow.
Arguments for acceptance:
* The paper presents a novel approach to dimensionality reduction, which addresses a long-standing open problem.
* The proposed algorithm is deterministic and has a guaranteed approximation error.
* The experimental results demonstrate the scalability and efficiency of the algorithm, even on very large datasets.
Arguments against acceptance:
* The paper assumes that the input matrix is sparse, which may not always be the case in practice.
* The algorithm has a high computational complexity, which may make it impractical for very large datasets.
* The authors do not provide a detailed comparison with other state-of-the-art dimensionality reduction algorithms.
Rating: 8/10
Recommendation: Accept, with minor revisions to address the weaknesses mentioned above.