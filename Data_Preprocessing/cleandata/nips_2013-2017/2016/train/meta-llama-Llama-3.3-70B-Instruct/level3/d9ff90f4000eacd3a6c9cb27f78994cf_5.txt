This paper addresses the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations is much larger than the number of predictors. The authors propose a novel approach based on the relationship between GLM coefficients and ordinary least squares (OLS) coefficients, which is shown to be approximately proportional in random design GLMs. This relationship is leveraged to design a computationally efficient algorithm, called Scaled Least Squares (SLS), that achieves the same accuracy as the maximum likelihood estimator (MLE) with a significantly reduced computational cost.
The paper is well-written, and the authors provide a clear and concise introduction to the problem, related work, and their contributions. The technical sections are also well-organized, and the proofs of the main results are provided in the supplementary material. The experimental results demonstrate the effectiveness of the proposed algorithm in terms of computational efficiency and accuracy.
The strengths of the paper include:
* The proposal of a novel and efficient algorithm for estimating GLM coefficients, which has the potential to be widely applicable in large-scale machine learning and statistics problems.
* The provision of theoretical guarantees for the algorithm, including bounds on the estimation error and convergence rates.
* The demonstration of the algorithm's effectiveness through extensive numerical studies on large-scale real and synthetic datasets.
The weaknesses of the paper include:
* The assumption of random design, which may not always hold in practice. It would be interesting to explore the robustness of the algorithm to deviations from this assumption.
* The lack of comparison to other state-of-the-art algorithms for GLM estimation, which would provide a more comprehensive understanding of the proposed algorithm's performance.
Overall, the paper makes a significant contribution to the field of large-scale machine learning and statistics, and the proposed algorithm has the potential to be widely adopted in practice. The authors have demonstrated the effectiveness of their approach through a combination of theoretical analysis and numerical experiments, and the paper is well-written and easy to follow.
Arguments for acceptance:
* The paper proposes a novel and efficient algorithm for estimating GLM coefficients, which has the potential to be widely applicable in large-scale machine learning and statistics problems.
* The authors provide theoretical guarantees for the algorithm, including bounds on the estimation error and convergence rates.
* The experimental results demonstrate the effectiveness of the proposed algorithm in terms of computational efficiency and accuracy.
Arguments against acceptance:
* The assumption of random design may not always hold in practice, and it is unclear how robust the algorithm is to deviations from this assumption.
* The lack of comparison to other state-of-the-art algorithms for GLM estimation makes it difficult to fully evaluate the proposed algorithm's performance.
Rating: 8/10
Recommendation: Accept, with minor revisions to address the weaknesses mentioned above.