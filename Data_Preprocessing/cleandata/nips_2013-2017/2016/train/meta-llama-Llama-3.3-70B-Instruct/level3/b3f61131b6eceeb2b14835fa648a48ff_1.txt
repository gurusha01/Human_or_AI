This paper presents a comprehensive analysis of the population likelihood function of Gaussian mixture models, providing two fundamental results that shed light on the behavior of the Expectation-Maximization (EM) algorithm and its first-order variant. The authors demonstrate that the population likelihood function can have bad local maxima, even in the case of equally-weighted mixtures of well-separated and spherical Gaussians, and that the EM algorithm with random initialization can converge to these suboptimal critical points with high probability.
The paper is well-organized and clearly written, with a thorough introduction to the background and motivation of the problem. The authors provide a detailed explanation of the Gaussian mixture model, the EM algorithm, and the first-order EM algorithm, making it easy to follow for readers who are not experts in the field.
The main results of the paper are significant and well-supported by theoretical analysis. The authors prove that the population likelihood function can have bad local maxima, which can be arbitrarily worse than the global maximum, and that the EM algorithm with random initialization can converge to these suboptimal critical points with high probability. They also show that the first-order EM algorithm with a suitable step size does not converge to strict saddle points with probability one.
The paper has several strengths, including its thorough analysis of the population likelihood function, its clear explanation of the EM algorithm and its variants, and its significant theoretical results. The authors also provide a detailed discussion of the implications of their results, including the necessity of careful initialization for local search methods and the potential failure of EM-style algorithms with random initialization schemes.
However, the paper also has some weaknesses. The authors could have provided more numerical experiments to illustrate the behavior of the EM algorithm and its variants in practice. Additionally, the paper could have benefited from a more detailed discussion of the related work in the field, including the recent results on the convergence of the EM algorithm and its variants.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides significant new insights into the behavior of the EM algorithm and its variants. The authors have also clearly explained the background and motivation of the problem, making it easy to follow for readers who are not experts in the field.
Overall, I would recommend accepting this paper for publication. The paper provides significant new insights into the behavior of the EM algorithm and its variants, and its results have important implications for the practice of machine learning and statistics.
Arguments pro acceptance:
* The paper provides significant new insights into the behavior of the EM algorithm and its variants.
* The authors have clearly explained the background and motivation of the problem, making it easy to follow for readers who are not experts in the field.
* The paper is technically sound and well-written.
* The results have important implications for the practice of machine learning and statistics.
Arguments con acceptance:
* The paper could have benefited from more numerical experiments to illustrate the behavior of the EM algorithm and its variants in practice.
* The paper could have provided a more detailed discussion of the related work in the field.
* Some readers may find the paper to be too technical or dense, which could make it difficult to follow for non-experts.