This paper proposes two novel metrics for assessing the robustness of neural networks, along with an approximating algorithm to efficiently compute these metrics. The approach is based on finding the nearest adversarial example by L_infinity distance, which is a common measure of robustness. The authors evaluate their method on the MNIST and CIFAR-10 datasets, demonstrating that it can find many adversarial examples that a previous baseline did not, while also revealing limitations such as decreased accuracy and remaining vulnerabilities.
The strengths of this paper include its novel approach to measuring robustness, its ability to find more adversarial examples than previous methods, and its evaluation on multiple datasets. The authors also provide a thorough analysis of their results, including a discussion of the limitations of their approach.
However, there are also some weaknesses to the paper. The method's assumption of a completely linear space is strong and not fully justified, which may limit its applicability to more complex neural networks. Additionally, the paper has some minor issues, such as unclear citations, a misleading title and abstract, and a disorganized introduction that lacks necessary background information and definitions.
In terms of the conference guidelines, this paper meets the criteria for quality, as it is technically sound and well-supported by theoretical analysis and experimental results. The paper is also clear and well-organized, making it easy to follow and understand. The originality of the paper is also high, as it proposes a novel approach to measuring robustness and evaluating neural network vulnerability.
The significance of the paper is also high, as it addresses a difficult problem in the field of neural networks and provides a new perspective on how to measure and improve robustness. The results of the paper are important, as they demonstrate the effectiveness of the proposed approach and highlight the need for more robust neural networks.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of neural networks and provides a novel approach to measuring and improving robustness. However, I would suggest that the authors address the minor issues mentioned above, such as clarifying the citations and reorganizing the introduction, to improve the overall quality of the paper.
Arguments pro acceptance:
* Novel approach to measuring robustness
* Ability to find more adversarial examples than previous methods
* Thorough analysis of results
* High originality and significance
* Addresses a difficult problem in the field of neural networks
Arguments con acceptance:
* Strong assumption of a completely linear space
* Minor issues with citations, title, and introduction
* Limited applicability to more complex neural networks
* Decreased accuracy and remaining vulnerabilities in some cases.