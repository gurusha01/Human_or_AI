This paper proposes a novel approach to dropout, a widely used technique for preventing overfitting in deep neural networks. The authors introduce a multinomial dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features or neurons. The paper provides a rigorous analysis of the risk bound for shallow learning with multinomial dropout and demonstrates that a distribution-dependent dropout leads to faster convergence and smaller generalization error.
The paper's main contributions include the proposal of a multinomial dropout and the development of an efficient evolutional dropout for deep learning. The evolutional dropout adapts the sampling probabilities to the evolving distributions of layers' outputs, addressing the internal covariate shift issue. Experimental results on various benchmark datasets verify that the proposed dropouts can dramatically improve convergence and reduce testing error.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The theoretical analysis is sound, and the experimental results are convincing. The paper also provides a good review of related work and clearly explains the differences between the proposed approach and existing methods.
One potential weakness of the paper is that the authors assume that the sampling probabilities are known, which may not always be the case in practice. Additionally, the paper could benefit from more numerical examples to support the proposed methods.
Overall, the paper makes a significant contribution to the field of deep learning and provides a novel approach to dropout. The paper is well-organized, and the authors provide a clear and concise explanation of their approach. The theoretical analysis is sound, and the experimental results are convincing.
Arguments pro acceptance:
* The paper proposes a novel approach to dropout, which is a widely used technique in deep learning.
* The paper provides a rigorous analysis of the risk bound for shallow learning with multinomial dropout.
* The experimental results are convincing and demonstrate the effectiveness of the proposed approach.
* The paper is well-written, and the authors provide a clear and concise explanation of their approach.
Arguments con acceptance:
* The authors assume that the sampling probabilities are known, which may not always be the case in practice.
* The paper could benefit from more numerical examples to support the proposed methods.
* The paper may not be suitable for all types of deep learning models, and the authors could provide more discussion on the limitations of their approach. 
Quality: 8/10
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The theoretical analysis is sound, and the experimental results are convincing.
Clarity: 9/10
The paper is well-organized, and the authors provide a clear and concise explanation of their approach.
Originality: 8/10
The paper proposes a novel approach to dropout, which is a widely used technique in deep learning.
Significance: 9/10
The paper makes a significant contribution to the field of deep learning and provides a novel approach to dropout. The experimental results are convincing and demonstrate the effectiveness of the proposed approach.