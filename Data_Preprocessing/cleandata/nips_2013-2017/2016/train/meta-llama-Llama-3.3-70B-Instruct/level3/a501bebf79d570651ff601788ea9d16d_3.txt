This paper proposes a novel approach to reduce memory consumption of the back-propagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). The method uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation, allowing it to tightly fit within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost.
The paper is well-written and clearly explains the proposed approach, including the use of dynamic programming to compute the optimal policy for storing internal state during network unfolding. The authors also explore multiple storage schemes to reduce memory usage, including backpropagation through time with selective hidden state memorization (BPTT-HSM), backpropagation though time with selective internal state memorization (BPTT-ISM), and backpropagation though time with mixed state memorization (BPTT-MSM).
The computational analysis shows that the method can reduce memory requirements by 95% with a 33% increase in computations, making it useful for training RNNs with long sequences on GPUs with limited memory. The authors also provide asymptotic computational upper bounds for various regimes and demonstrate the effectiveness of their approach through numerical experiments.
The paper's impact could be strengthened by including a motivating example that demonstrates the method's usefulness on a specific computing architecture with limited memory. Additionally, there is a minor typo in the paper, where "orward" should be corrected to "forward" on page 7, Section 4.
Overall, the paper is technically sound, well-organized, and clearly written. The proposed approach is novel and has the potential to make a significant impact in the field of RNNs. The authors have adequately referenced related work and have provided a thorough analysis of the strengths and weaknesses of their approach.
Arguments for acceptance:
* The paper proposes a novel approach to reduce memory consumption of BPTT, which is a significant problem in training RNNs.
* The method is technically sound and has been thoroughly analyzed through computational experiments.
* The paper is well-written and clearly explains the proposed approach.
* The authors have adequately referenced related work and have provided a thorough analysis of the strengths and weaknesses of their approach.
Arguments against acceptance:
* The paper could benefit from a motivating example that demonstrates the method's usefulness on a specific computing architecture with limited memory.
* There is a minor typo in the paper that should be corrected.
Overall, I recommend accepting this paper as it makes a significant contribution to the field of RNNs and has the potential to make a significant impact.