This paper proposes a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). The authors introduce a dynamic programming approach to balance the trade-off between caching of intermediate results and recomputation, allowing the algorithm to fit within almost any user-set memory budget while finding an optimal execution policy that minimizes computational cost.
The proposed BPTT-MSM algorithm has several advantages, including the ability to tightly fit within arbitrary constant memory constraints, outperforming Chen's algorithm in terms of memory efficiency. The algorithm is particularly effective for long sequences, saving 95% of memory usage while using only one-third more time per iteration than the standard BPTT.
The paper is well-written, and the authors provide a clear explanation of the proposed approach, including a detailed description of the dynamic programming formulation and the optimization procedure. The authors also provide a thorough analysis of the computational cost and memory usage of the proposed algorithm, including a comparison with Chen's algorithm.
One potential weakness of the paper is that the authors do not provide results on specific RNN networks, such as seq2seq learning, to further demonstrate the effectiveness of the proposed method. Additionally, the authors could provide more details on the implementation of the algorithm and the experimental setup used to evaluate its performance.
Overall, the paper makes a significant contribution to the field of RNNs and BPTT algorithms, and the proposed approach has the potential to be widely adopted in practice. The authors demonstrate a good understanding of the related work and provide a clear and well-structured presentation of their approach.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to reducing memory consumption of BPTT algorithms.
* The authors provide a thorough analysis of the computational cost and memory usage of the proposed algorithm.
* The algorithm has the potential to be widely adopted in practice, particularly for long sequences and limited memory devices.
Arguments con acceptance:
* The authors do not provide results on specific RNN networks to further demonstrate the effectiveness of the proposed method.
* The implementation details and experimental setup could be more thoroughly described.
* The paper could benefit from a more detailed comparison with other related work in the field.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Recommendation: Accept with minor revisions. The authors should provide more details on the implementation and experimental setup, as well as results on specific RNN networks to further demonstrate the effectiveness of the proposed method.