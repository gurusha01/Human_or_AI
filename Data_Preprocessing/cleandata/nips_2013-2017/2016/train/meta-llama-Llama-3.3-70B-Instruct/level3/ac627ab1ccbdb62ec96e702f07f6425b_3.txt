This paper proposes a novel approach to domain adaptation for deep neural networks, leveraging a duality between boosting and support vector machines (SVM). The authors introduce a new algorithm, Large Margin Discriminant Dimensionality Reduction (LADDER), which jointly learns the mapping and linear classifiers in a margin-maximizing objective function. The technique shows promising results, outperforming traditional multiclass boosting and other dimensionality reduction methods in various experiments, including traffic sign detection, hashing, and scene understanding.
The paper's main contribution is the use of a residual function, where the source domain classifier is a residual of the target domain classifier, to adapt to the target domain. This approach allows for learning an embedding of arbitrary dimension, which can be beneficial for tasks such as hashing and image classification. The authors also provide a comprehensive comparison to related work, highlighting the advantages of their approach over existing methods.
However, the paper's experimental evaluation can be improved. The authors do not properly isolate the contribution of the residual function, making it unclear whether it is necessary or can be replaced with simpler methods like L2 regularization. Additionally, the notation used in the paper is confusing, with inconsistent mixing of H and F, and figures like Figure 2 fail to demonstrate a clear qualitative difference between different predictions.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is well-organized, and the writing is clear, although some sections could be improved for better readability.
The originality of the paper lies in the proposed duality between boosting and SVM, which is a novel contribution to the field. The authors provide a clear explanation of how this duality leads to the development of the LADDER algorithm. The paper also provides a comprehensive review of related work, highlighting the differences between their approach and existing methods.
The significance of the paper lies in its potential to improve the state of the art in domain adaptation and dimensionality reduction. The authors demonstrate the effectiveness of their approach in various experiments, showing that it can outperform traditional methods in certain tasks. However, further experimentation and evaluation are needed to fully assess the impact of this work.
Overall, I would recommend accepting this paper, but with revisions to address the issues mentioned above. The authors should provide more detailed experimental evaluations, clarify the notation, and improve the presentation of their results. With these revisions, the paper has the potential to make a significant contribution to the field of machine learning.
Arguments pro acceptance:
* The paper proposes a novel approach to domain adaptation, leveraging a duality between boosting and SVM.
* The authors introduce a new algorithm, LADDER, which jointly learns the mapping and linear classifiers in a margin-maximizing objective function.
* The technique shows promising results, outperforming traditional multiclass boosting and other dimensionality reduction methods in various experiments.
Arguments con acceptance:
* The paper's experimental evaluation can be improved, with more detailed comparisons to existing methods and a clearer demonstration of the benefits of the proposed approach.
* The notation used in the paper is confusing, and some sections could be improved for better readability.
* Further experimentation and evaluation are needed to fully assess the impact of this work.