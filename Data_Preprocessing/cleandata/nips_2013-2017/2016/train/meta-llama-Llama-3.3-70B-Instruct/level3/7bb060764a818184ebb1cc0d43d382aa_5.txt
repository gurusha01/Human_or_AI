This paper proposes a novel convex formulation for two-layer conditional models with latent structured representation. The authors develop an efficient algorithm for training deep neural networks that adapts to evolving distributions of layers' outputs. The paper is well-written and easy to follow, making it a pleasure to read.
The strengths of the paper include its ability to capture latent structure and estimate model parameters jointly and optimally. The authors demonstrate the flexibility of the model on two structures based on total unimodularity, namely graph matching and linear chain. The experimental results confirm the promise of the method, showing superior performance over local training and low-rank characterization of the extreme points of the feasible region.
However, there are some concerns regarding the novelty of the submission meeting the NIPS requirements. The authors build upon existing work, and it is unclear how much of the contribution is entirely new. Additionally, the experimental methodology is limited, with only the best results reported and comparisons limited to the CIFAR-10 dataset. It would be beneficial to see more comprehensive experiments and comparisons to other state-of-the-art methods.
Minor issues include incorrect use of the citation package and formatting errors in section 2. Nevertheless, these do not detract from the overall quality of the paper.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and significance. The authors provide a clear and concise summary of the main ideas and relate them to previous work. The paper is well-organized, and the authors adequately inform the reader about the methodology and results.
Arguments for acceptance include:
* The paper proposes a novel convex formulation for two-layer conditional models with latent structured representation.
* The authors demonstrate the flexibility of the model on two structures based on total unimodularity.
* The experimental results show superior performance over local training and low-rank characterization of the extreme points of the feasible region.
Arguments against acceptance include:
* The novelty of the submission may not meet the NIPS requirements.
* The experimental methodology is limited, with only the best results reported and comparisons limited to the CIFAR-10 dataset.
* Minor issues with formatting and citation package usage.
Overall, I recommend acceptance of the paper, as it presents a significant contribution to the field of machine learning and provides a well-written and easy-to-follow narrative. However, the authors should address the concerns regarding novelty and experimental methodology to strengthen the paper.