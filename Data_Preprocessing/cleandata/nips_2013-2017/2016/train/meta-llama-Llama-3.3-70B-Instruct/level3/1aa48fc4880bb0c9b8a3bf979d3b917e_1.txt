This paper proposes a novel approach to dropout, a widely used technique for preventing overfitting in deep neural networks. The authors introduce a multinomial dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features or neurons. The paper provides a rigorous analysis of the risk bound of shallow learning with multinomial dropout and demonstrates that a distribution-dependent dropout leads to a smaller expected risk and faster convergence.
The paper's technical quality is thorough, and the authors provide a clear and well-organized presentation of their ideas. The proposed approach is novel and differs from previous contributions, and the authors provide a good discussion of related work. The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts, achieving faster convergence and smaller testing errors compared to the standard dropout.
However, there are some areas that could be improved. The connection to monotone operators could be made clearer, and more experimental results would be beneficial to further demonstrate the effectiveness of the proposed approach. Additionally, some minor issues, such as inconsistent notation and the need for additional references, should be addressed.
The paper raises several questions and concerns, including the strength of assumptions A-C, the differences between the stochastic primal-dual method and the suggested method, and the meaning of certain constants and conditions. These questions and concerns should be carefully considered and addressed in future work.
Overall, the paper makes significant contributions to the field, including the proposal of a novel dropout approach and the demonstration of its effectiveness through theoretical analysis and experimental results. The paper is well-written, and the authors provide a clear and concise presentation of their ideas.
Arguments pro acceptance:
* The paper proposes a novel approach to dropout that differs from previous contributions.
* The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout.
* The experimental results demonstrate the effectiveness of the proposed dropouts.
* The paper is well-written, and the authors provide a clear and concise presentation of their ideas.
Arguments con acceptance:
* The connection to monotone operators could be made clearer.
* More experimental results would be beneficial to further demonstrate the effectiveness of the proposed approach.
* Some minor issues, such as inconsistent notation and the need for additional references, should be addressed.
* The paper raises several questions and concerns that should be carefully considered and addressed in future work.