This paper proposes a novel extension of the encoder-decoder framework, called the review network, which performs multiple review steps with attention on the encoder hidden states to output a set of thought vectors. The authors demonstrate the effectiveness of the review network on image captioning and source code captioning tasks, achieving state-of-the-art performance on the MSCOCO benchmark. The paper is well-organized, and the authors provide a clear explanation of the review network architecture and its components.
The strengths of the paper include the novelty of the proposed architecture, the thorough experimentation on two different tasks, and the impressive results achieved on the MSCOCO benchmark. The authors also provide a detailed analysis of the review network's performance, including visualization of the attention weights and thought vectors.
However, there are some weaknesses in the paper. The testing phase is not clearly explained, and it is unclear how the authors select the best model for evaluation. Additionally, the authors' argument for the specialization of ensemble members towards different output classes is not sufficiently justified, and it is unclear how this specialization is achieved in the review network. The paper also lacks a discussion on related work, particularly on diversity regularization in neural networks, which is a relevant technique that has been studied in the context of deep learning.
In terms of quality, the paper is technically sound, and the claims are well-supported by experimental results. However, the paper could benefit from a more detailed analysis of the review network's performance, including an ablation study to understand the contribution of each component. The paper is clearly written, and the authors provide sufficient information for the reader to reproduce the results.
The originality of the paper is high, as the review network is a novel architecture that extends the encoder-decoder framework. The significance of the paper is also high, as the review network achieves state-of-the-art performance on a challenging benchmark and has the potential to be applied to other tasks.
Arguments for acceptance:
* The paper proposes a novel and effective architecture for encoder-decoder models
* The authors demonstrate state-of-the-art performance on a challenging benchmark
* The paper is well-organized and clearly written
Arguments against acceptance:
* The testing phase is not clearly explained
* The authors' argument for specialization of ensemble members is not sufficiently justified
* The paper lacks a discussion on related work, particularly on diversity regularization in neural networks
Overall, I recommend accepting the paper, as the strengths outweigh the weaknesses, and the paper makes a significant contribution to the field of deep learning. However, the authors should address the weaknesses in the paper, particularly the lack of clarity in the testing phase and the insufficient justification for the specialization of ensemble members.