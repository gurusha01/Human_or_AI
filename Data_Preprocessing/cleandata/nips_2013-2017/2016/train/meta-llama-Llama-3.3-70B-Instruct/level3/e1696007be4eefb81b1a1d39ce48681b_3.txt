This paper explores the relationship between ordinary least squares (OLS) problems and Generalized Linear models (GLMs) to develop faster GLM solvers. The authors prove that in GLMs with random design, the GLM coefficients are approximately proportional to the corresponding OLS coefficients. This proportionality relationship is used to design an algorithm, called Scaled Least Squares (SLS), which achieves the same accuracy as the maximum likelihood estimator (MLE) with a significant reduction in computational cost.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach. The theoretical results are sound, and the authors provide a thorough analysis of the convergence behavior of the SLS algorithm. The experimental results demonstrate the effectiveness of the SLS algorithm in achieving the minimum test error substantially faster than commonly used batch algorithms for finding the MLE.
The strengths of the paper include:
* The authors provide a novel approach to solving GLMs by exploiting the proportionality relationship between OLS and GLM coefficients.
* The SLS algorithm is computationally efficient and achieves the same accuracy as the MLE with a significant reduction in computational cost.
* The authors provide a thorough analysis of the convergence behavior of the SLS algorithm and demonstrate its effectiveness through experimental results.
The weaknesses of the paper include:
* The proportionality relationship may not hold for all classes of regularizers and link functions, which could limit the applicability of the SLS algorithm.
* The authors do not provide a detailed comparison of the SLS algorithm with other state-of-the-art methods for solving GLMs.
Overall, the paper is well-written, and the authors provide a significant contribution to the field of machine learning. The SLS algorithm has the potential to be a valuable tool for solving large-scale GLMs, and the authors' results demonstrate its effectiveness in achieving the minimum test error substantially faster than commonly used batch algorithms.
Arguments for acceptance:
* The paper provides a novel approach to solving GLMs that has the potential to be a valuable tool for large-scale machine learning problems.
* The authors provide a thorough analysis of the convergence behavior of the SLS algorithm and demonstrate its effectiveness through experimental results.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach.
Arguments against acceptance:
* The proportionality relationship may not hold for all classes of regularizers and link functions, which could limit the applicability of the SLS algorithm.
* The authors do not provide a detailed comparison of the SLS algorithm with other state-of-the-art methods for solving GLMs.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should provide a more detailed comparison of the SLS algorithm with other state-of-the-art methods for solving GLMs and discuss the limitations of the proportionality relationship in more detail.