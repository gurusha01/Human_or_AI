This paper proposes a novel batch Bayesian optimization algorithm, called parallel knowledge gradient (q-KG), which is designed to efficiently optimize expensive-to-evaluate functions in parallel settings. The method is based on a decision-theoretical analysis and uses infinitesimal perturbation analysis to estimate the gradient of the q-KG acquisition function, allowing for efficient optimization.
The paper is well-written and clearly presents the proposed method, its theoretical foundations, and experimental results. The authors demonstrate the effectiveness of q-KG on several synthetic functions and real-world problems, including hyperparameter tuning of machine learning algorithms. The results show that q-KG outperforms or is competitive with state-of-the-art benchmark algorithms, especially in noisy settings.
The technical quality of the paper is good, and the authors provide a thorough discussion of the related work and the limitations of their approach. However, the novelty of the paper is limited, as the methods used are not new and have been previously applied in similar contexts. The paper could benefit from additional experiments to demonstrate the scalability of the method and its performance in more complex scenarios.
The impact and usefulness of the paper are high, as parallel hyperparameter tuning methods are highly relevant for many real-world applications, including machine learning, computer vision, biology, and robotics. The clarity and presentation of the paper are good, but the experiment section could be improved by providing more details on the differences between the proposed method and existing methods.
Some potential questions and areas for future work include:
* How does the method scale to larger problem sizes and more complex functions?
* Can the method be extended to run asynchronously, and how would this affect its performance?
* How does the method compare to other parallel Bayesian optimization algorithms in terms of wall-clock time and computational resources?
* Can the method be applied to other types of optimization problems, such as multi-objective optimization or optimization with constraints?
Overall, the paper presents a solid contribution to the field of Bayesian optimization, and the proposed method has the potential to be useful in a wide range of applications. 
Arguments pro acceptance:
- The paper presents a novel batch Bayesian optimization algorithm that is effective in parallel settings.
- The method is based on a decision-theoretical analysis and uses infinitesimal perturbation analysis to estimate the gradient of the q-KG acquisition function.
- The authors demonstrate the effectiveness of q-KG on several synthetic functions and real-world problems.
- The paper is well-written and clearly presents the proposed method, its theoretical foundations, and experimental results.
Arguments con acceptance:
- The novelty of the paper is limited, as the methods used are not new and have been previously applied in similar contexts.
- The paper could benefit from additional experiments to demonstrate the scalability of the method and its performance in more complex scenarios.
- The experiment section could be improved by providing more details on the differences between the proposed method and existing methods.