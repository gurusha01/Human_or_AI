This paper introduces an algorithm to generate adversarial examples for neural networks (NNs) by constraining the examples to be in the linear region of the ReLUs of the NN. The algorithm shows promising results on the MNIST dataset, but its theoretical basis is questionable. The idea that adversarial examples exist due to linearity in NNs is not entirely convincing, as there are examples of adversarial examples existing outside linear regions.
The experimental methodology used in the paper is also concerning. The use of non-state-of-the-art NN architectures, simple datasets, and lack of sanity checks may limit the generalizability of the results. The comparison with the baseline is omitted on CIFAR, and the method's scalability is questionable due to its computational requirements. The results show marginal improvements over the baseline on MNIST, but it is unclear whether this is due to the algorithm itself or other factors.
There are also several minor issues with the paper, including the lack of definition for $x\star$, the assumption that Linf norm is the best measure for perceptibility, and the need for a more detailed explanation of reference [3]. Additionally, the paper could benefit from more discussion on the implications of the results and the potential applications of the algorithm.
In terms of the conference guidelines, the paper meets some of the criteria, but falls short in others. The paper is well-organized and clearly written, but it lacks clarity in some sections, particularly in the explanation of the algorithm. The paper also lacks originality, as the idea of using constraint systems to generate adversarial examples is not new. However, the paper does provide some new insights into the robustness of NNs and the importance of using impartial measures to compare robustness.
Overall, I would give this paper a score of 6 out of 10. While it has some interesting ideas and results, it falls short in terms of theoretical basis, experimental methodology, and clarity. With some revisions to address these issues, the paper could be stronger and more convincing.
Arguments pro acceptance:
* The paper introduces a new algorithm for generating adversarial examples, which could be useful for improving the robustness of NNs.
* The paper provides some interesting results on the MNIST dataset, which could be useful for understanding the robustness of NNs.
* The paper highlights the importance of using impartial measures to compare robustness, which is an important consideration in the field.
Arguments con acceptance:
* The paper's theoretical basis is questionable, and the idea that adversarial examples exist due to linearity in NNs is not entirely convincing.
* The experimental methodology used in the paper is concerning, and the results may not be generalizable to other datasets or NN architectures.
* The paper lacks clarity in some sections, particularly in the explanation of the algorithm, and could benefit from more discussion on the implications of the results and the potential applications of the algorithm.