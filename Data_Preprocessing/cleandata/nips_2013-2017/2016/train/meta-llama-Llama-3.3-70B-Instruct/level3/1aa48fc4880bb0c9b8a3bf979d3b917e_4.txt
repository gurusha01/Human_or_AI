This paper proposes a novel dropout technique, called multinomial dropout, which samples features or neurons according to a multinomial distribution with different probabilities for different features/neurons. The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout and demonstrate that a distribution-dependent dropout leads to a faster convergence and a smaller generalization error. They also propose an efficient evolutional dropout for deep learning, which adapts the sampling probabilities to the evolving distributions of layers' outputs.
The paper is well-written, and the authors provide a clear and concise explanation of their methodology and results. The experimental results demonstrate the effectiveness of the proposed dropouts on several benchmark datasets, showing significant improvements in convergence speed and testing error compared to the standard dropout.
The strengths of the paper include:
* The proposal of a novel dropout technique that can adapt to the evolving distributions of layers' outputs
* A rigorous analysis of the risk bound of shallow learning with multinomial dropout
* Experimental results that demonstrate the effectiveness of the proposed dropouts on several benchmark datasets
The weaknesses of the paper include:
* The lack of clarity on how to adaptively update the iterate point for acceleration
* The absence of a convergence and acceleration proof for the evolutional dropout
* The comparison with batch normalization is limited to a single dataset and may not be comprehensive
Overall, the paper makes a significant contribution to the field of deep learning by proposing a novel dropout technique that can improve the convergence speed and testing error of neural networks. The results are well-supported by theoretical analysis and experimental results, and the paper is well-written and easy to follow.
Arguments for acceptance:
* The paper proposes a novel dropout technique that can adapt to the evolving distributions of layers' outputs
* The paper provides a rigorous analysis of the risk bound of shallow learning with multinomial dropout
* The experimental results demonstrate the effectiveness of the proposed dropouts on several benchmark datasets
Arguments against acceptance:
* The lack of clarity on how to adaptively update the iterate point for acceleration
* The absence of a convergence and acceleration proof for the evolutional dropout
* The comparison with batch normalization is limited to a single dataset and may not be comprehensive
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 8/10
Recommendation: Accept with minor revisions to address the weaknesses mentioned above.