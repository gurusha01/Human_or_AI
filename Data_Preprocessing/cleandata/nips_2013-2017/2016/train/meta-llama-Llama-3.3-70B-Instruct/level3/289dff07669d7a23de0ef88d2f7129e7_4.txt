This paper presents a novel approach to scaling memory-augmented neural networks (MANNs) by introducing a sparse access memory (SAM) architecture. The main contribution of this work is the utilization of triangular Cholesky factorization to eliminate the square root in standard CMA-ES, reducing computation cost. The proposed method comes with a theoretical justification, illustrating that it does not deteriorate the algorithm's performance and reduces runtime complexity without significant change in objective function evaluations.
The implementation of the Cholesky factorization method for CMA-ES is valuable, and the results demonstrate that SAM achieves asymptotic lower bounds in space and time complexity. The paper also shows that SAM requires the same number of objective function evaluations as the standard CMA-ES, but reduces wall-clock time, with this benefit increasing with search space dimensionality.
The strengths of this paper include its ability to scale to large memories and long sequences, making it a promising approach for applications such as language modeling and machine translation. The use of an approximate nearest neighbor index (ANN) to reduce the forward pass of training to sublinear time is also a notable contribution. The experimental results demonstrate that SAM is able to learn with comparable efficiency to dense models and even outperforms them in some cases.
However, there are some weaknesses to this paper. One potential concern is the lack of further theoretical explanation for the achieved numerical stability. Additionally, the paper could benefit from a more detailed analysis of the trade-offs between the sparse and dense approaches. The use of an exponential curriculum to scale tasks to larger sizes is also an interesting approach, but it would be useful to see more analysis on the effectiveness of this method.
In terms of originality, this paper presents a novel approach to scaling MANNs, and the use of SAM and ANN indexes is a new contribution to the field. The paper also provides a thorough analysis of the related work and demonstrates a clear understanding of the current state of the field.
The significance of this paper lies in its potential to enable the training of neural networks with large memories, which could have a significant impact on applications such as language modeling and machine translation. The results demonstrate that SAM is able to learn with comparable efficiency to dense models and even outperforms them in some cases, making it a promising approach for future research.
Overall, this paper presents a strong contribution to the field of neural networks and MANNs, and the results demonstrate the potential of SAM to scale to large memories and long sequences. With some further analysis and explanation, this paper could be even stronger.
Arguments pro acceptance:
* The paper presents a novel approach to scaling MANNs, which is a significant contribution to the field.
* The use of SAM and ANN indexes is a new contribution to the field, and the results demonstrate its effectiveness.
* The paper provides a thorough analysis of the related work and demonstrates a clear understanding of the current state of the field.
* The results demonstrate that SAM is able to learn with comparable efficiency to dense models and even outperforms them in some cases.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the trade-offs between the sparse and dense approaches.
* The use of an exponential curriculum to scale tasks to larger sizes is an interesting approach, but it would be useful to see more analysis on the effectiveness of this method.
* The paper could benefit from further theoretical explanation for the achieved numerical stability.