This paper proposes a method for training recurrent neural networks (RNNs) with unitary transition matrices without restricting the set of unitary matrices that can be learned. The authors provide a theoretical motivation using Givens operators and show that the original parameterization of unitary RNNs (uRNNs) is not able to represent the entire unitary group for N x N matrices for which N > 22. They propose a gradient descent optimization method on the Stiefel manifold of unitary matrices, which leads to results that are at least as good and sometimes superior to results obtained with the original uRNN parameterization and Long Short-Term Memory (LSTM) networks on several tasks.
The paper presents multiple theoretical and empirical contributions, with the theoretical ideas and proposed optimization algorithm being more impressive than the empirical work. The experimental section is less convincing due to the use of synthetic or artificial setups, and could benefit from a more thorough analysis and additional details for replication. The paper has some minor issues, such as unclear experimental designs and interpretations of results, and could improve with discussions on the lack of sudden breakdown of the restricted model for N > 22.
The results on the 'copy memory' task are impressive, but the comparison with the restricted uRNN is limited by the lack of a thorough hyperparameter search for the baseline. The speech task feels artificial, and the results are somewhat surprising, with the restricted uRNN performing well in terms of Mean Squared Error (MSE), but the full-capacity model performing better on other metrics.
Overall, the paper is well-written and provides a significant contribution to the field of RNNs. The proposed method has the potential to improve the performance of RNNs on tasks that require long-term memory and stable gradients. However, further experimentation and analysis are needed to fully understand the benefits and limitations of the proposed approach.
Arguments pro acceptance:
* The paper proposes a novel method for training uRNNs with unitary transition matrices without restricting the set of unitary matrices that can be learned.
* The theoretical motivation using Givens operators is sound and well-explained.
* The experimental results show that the proposed method can outperform the original uRNN parameterization and LSTM networks on several tasks.
* The paper provides a significant contribution to the field of RNNs and has the potential to improve the performance of RNNs on tasks that require long-term memory and stable gradients.
Arguments con acceptance:
* The experimental section is less convincing due to the use of synthetic or artificial setups.
* The paper has some minor issues, such as unclear experimental designs and interpretations of results.
* The comparison with the restricted uRNN is limited by the lack of a thorough hyperparameter search for the baseline.
* The speech task feels artificial, and the results are somewhat surprising.