This paper introduces a novel method, Bounding Divergences with Reverse Annealing (BREAD), for evaluating the accuracy of approximate posterior inference in probabilistic programming languages. The method is based on bidirectional Monte Carlo (BDMC) and provides a rigorous upper bound on the Jeffreys divergence between the true posterior distribution and the distribution of approximate samples. The authors demonstrate the effectiveness of BREAD on several models and datasets, including linear regression, logistic regression, matrix factorization, autoregressive time series modeling, and mixture-of-Gaussians clustering.
The paper is well-written, and the authors provide a clear and detailed explanation of the methodology and its application to various models. The experiments are thorough, and the results are presented in a clear and concise manner. The authors also provide a detailed comparison with related work, including the diagnosis of Markov chain convergence and the automatic configuration of MCMC algorithms.
The strengths of the paper include:
* The introduction of a novel method for evaluating the accuracy of approximate posterior inference, which is a significant contribution to the field of probabilistic programming.
* The provision of a rigorous upper bound on the Jeffreys divergence, which is a well-established measure of the difference between two distributions.
* The demonstration of the effectiveness of BREAD on several models and datasets, which shows the practical applicability of the method.
* The comparison with related work, which provides a clear understanding of the context and the contributions of the paper.
The weaknesses of the paper include:
* The limitation of BREAD to cases where an exact sample from the posterior distribution is available, which may not always be the case in practice.
* The requirement of a significant amount of computation to obtain accurate bounds, which may be a limitation for large-scale models.
* The lack of a clear discussion on the choice of hyperparameters and their impact on the results, which is an important aspect of any machine learning method.
Overall, the paper is well-written, and the authors provide a significant contribution to the field of probabilistic programming. The method is rigorous, and the experiments are thorough, making it a strong paper. However, the limitations of the method and the lack of discussion on hyperparameters are areas that need to be addressed in future work.
Arguments pro acceptance:
* The paper introduces a novel method for evaluating the accuracy of approximate posterior inference, which is a significant contribution to the field.
* The method is rigorous, and the experiments are thorough, demonstrating the effectiveness of BREAD on several models and datasets.
* The paper provides a clear and detailed explanation of the methodology and its application to various models.
Arguments con acceptance:
* The method is limited to cases where an exact sample from the posterior distribution is available, which may not always be the case in practice.
* The requirement of a significant amount of computation to obtain accurate bounds may be a limitation for large-scale models.
* The lack of discussion on the choice of hyperparameters and their impact on the results is an area that needs to be addressed in future work.