This paper proposes a novel approach to training unitary recurrent neural networks (uRNNs) by optimizing the recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. The authors provide a theoretical argument to determine if a unitary parameterization has restricted capacity and show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed method. The introduction provides a good background on the vanishing and exploding gradient problems in recurrent neural networks and motivates the use of unitary recurrent matrices. The authors also provide a clear explanation of the proposed method, including the theoretical argument and the optimization procedure.
The experimental results are impressive, demonstrating the effectiveness of the proposed full-capacity uRNNs on various tasks, including synthetic system identification, long-term memorization, frame-to-frame prediction of speech spectra, and pixel-by-pixel classification of handwritten digits. The results show that full-capacity uRNNs outperform restricted-capacity uRNNs and LSTMs on most tasks, especially those requiring long-term memory.
However, there are some areas that could be improved. The paper could benefit from a more detailed comparison to simpler methods, such as LSTMs and gated recurrent units, to further demonstrate the advantages of the proposed approach. Additionally, the authors could provide more insight into the trade-off between hidden state dimension and capacity of unitary parameterizations, as suggested by the results on the permuted pixel-by-pixel MNIST task.
Overall, the paper makes a significant contribution to the field of recurrent neural networks and provides a novel approach to addressing the vanishing and exploding gradient problems. The results are promising, and the proposed method has the potential to be widely adopted in the field.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to training uRNNs.
* The theoretical argument is sound and provides a clear understanding of the restricted capacity of unitary parameterizations.
* The experimental results are impressive and demonstrate the effectiveness of the proposed method.
* The paper is well-organized and clearly written.
Arguments con acceptance:
* The paper could benefit from a more detailed comparison to simpler methods.
* The authors could provide more insight into the trade-off between hidden state dimension and capacity of unitary parameterizations.
* The paper assumes a good understanding of recurrent neural networks and unitary matrices, which may make it difficult for non-experts to follow.