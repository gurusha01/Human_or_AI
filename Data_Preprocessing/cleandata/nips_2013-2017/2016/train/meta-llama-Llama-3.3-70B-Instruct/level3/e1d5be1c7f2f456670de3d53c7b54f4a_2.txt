This paper presents a significant contribution to the field of contextual semibandits, a variant of bandit learning where the learner receives partial feedback in the form of a scalar value for each individual item, and a reward that is linearly related to this feedback. The authors develop two oracle-based algorithms, VCEE and EELS, for the cases where the linear transformation from feedback to reward is known and unknown, respectively.
The paper is well-written, and the authors provide a clear and concise explanation of the problem, the algorithms, and the theoretical guarantees. The VCEE algorithm, which applies to the case with known weights, achieves a regret bound of ˜O( p KLT logN), which is the best known bound for computationally efficient procedures. The EELS algorithm, which applies to the case with unknown weights, achieves a regret bound of ˜O(T 2/3(K logN) 1/3), which is the first bound for this setting that does not depend on the size of the composite action space.
The experimental evaluation of VCEE on two large-scale learning-to-rank datasets demonstrates its effectiveness in practice, outperforming existing alternatives such as ε-GREEDY and LINUCB. The authors also provide a detailed analysis of the results, highlighting the importance of adaptive exploration and the benefits of using a rich policy class.
The strengths of the paper include:
* The development of two novel algorithms, VCEE and EELS, which achieve state-of-the-art regret bounds for contextual semibandits.
* A thorough theoretical analysis of the algorithms, including regret bounds and computational complexity.
* A detailed experimental evaluation of VCEE on two large-scale datasets, demonstrating its effectiveness in practice.
* A clear and concise writing style, making the paper easy to follow and understand.
The weaknesses of the paper include:
* The assumption of a linear transformation from feedback to reward, which may not always hold in practice.
* The requirement of a supervised learning oracle, which may not be available in all settings.
* The lack of a clear comparison with other existing algorithms for contextual semibandits, such as those using non-linear transformations or different exploration strategies.
Overall, the paper presents a significant contribution to the field of contextual semibandits, and the authors demonstrate a deep understanding of the problem and the algorithms. The results are well-supported by theoretical analysis and experimental evaluation, and the paper is well-written and easy to follow.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of contextual semibandits.
* The algorithms developed in the paper achieve state-of-the-art regret bounds and are computationally efficient.
* The experimental evaluation demonstrates the effectiveness of VCEE in practice.
* The paper is well-written and easy to follow, making it accessible to a wide range of readers.
Arguments against acceptance:
* The assumption of a linear transformation from feedback to reward may not always hold in practice.
* The requirement of a supervised learning oracle may not be available in all settings.
* The paper could benefit from a more detailed comparison with other existing algorithms for contextual semibandits.