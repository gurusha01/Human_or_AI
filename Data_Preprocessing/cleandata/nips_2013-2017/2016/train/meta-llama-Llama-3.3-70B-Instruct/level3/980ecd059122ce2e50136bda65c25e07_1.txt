This paper proposes a novel approach to measuring the robustness of neural networks to small input perturbations, which is a critical aspect of their reliability and security. The authors introduce two new metrics, adversarial frequency and adversarial severity, to quantify the robustness of neural networks. They also develop an efficient algorithm to approximate these metrics using a linear programming approach.
The paper is well-structured and clearly written, making it easy to follow the authors' arguments and understand their contributions. The technical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed approach. The authors also provide a thorough review of related work, highlighting the limitations of existing approaches and the advantages of their own method.
One of the strengths of the paper is its ability to provide a more accurate estimate of robustness metrics compared to existing algorithms. The authors demonstrate that their approach can identify more adversarial examples and provide a better understanding of the robustness of neural networks. The paper also highlights the importance of using impartial measures to compare robustness, rather than relying on the same algorithm used to find adversarial examples.
However, there are some limitations to the paper. The technical analysis is restricted to the local linear regime of a neural network using ReLU units, which may not be representative of all neural network architectures. Additionally, the experimental results on CIFAR-10 demonstrate that the proposed approach can improve robustness, but the improvements are relatively small. The authors acknowledge that new techniques are required to significantly improve robustness on large neural networks.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a novel contribution to the field. The authors demonstrate a clear understanding of the related work and provide a thorough analysis of their approach. The paper also has the potential to make a significant impact on the field, as it provides a new perspective on measuring and improving the robustness of neural networks.
Arguments for acceptance:
* The paper provides a novel approach to measuring the robustness of neural networks, which is a critical aspect of their reliability and security.
* The authors develop an efficient algorithm to approximate robustness metrics using a linear programming approach.
* The paper demonstrates the effectiveness of the proposed approach through experimental results on MNIST and CIFAR-10.
* The authors provide a thorough review of related work and highlight the limitations of existing approaches.
Arguments against acceptance:
* The technical analysis is restricted to the local linear regime of a neural network using ReLU units, which may not be representative of all neural network architectures.
* The experimental results on CIFAR-10 demonstrate that the proposed approach can improve robustness, but the improvements are relatively small.
* The paper may benefit from additional experimental results on larger neural networks and more complex datasets.