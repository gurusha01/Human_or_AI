This paper proposes a novel approach to learning conditional distributions using conditional generative moment-matching networks (CGMMN). The authors introduce a conditional maximum mean discrepancy (CMMD) criterion, which measures the difference between the kernel mean embeddings of two conditional distributions. The CMMD objective is optimized using stochastic gradient descent, allowing for efficient training of the CGMMN model.
The paper is well-written and clearly motivated, with a thorough review of related work in the field of deep generative models. The authors provide a detailed explanation of the theoretical background, including the concept of kernel mean embeddings and the definition of CMMD. The experimental results demonstrate the effectiveness of CGMMN in various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge.
One of the strengths of the paper is its ability to handle complex conditional distributions, which is a challenging problem in many real-world applications. The authors show that CGMMN can learn flexible conditional distributions, including those with multiple modes and non-linear relationships between variables. The results on the MNIST and SVHN datasets demonstrate the competitive performance of CGMMN compared to state-of-the-art models, such as variational autoencoders and generative adversarial networks.
However, there are some limitations to the paper. One potential weakness is the requirement for a large amount of training data to estimate the CMMD objective. The authors mention that the computation cost for kernel gram matrix grows cubically with the sample size, which may limit the applicability of CGMMN to large-scale datasets. Additionally, the choice of kernel function and hyperparameters may require careful tuning, which can be time-consuming and require significant expertise.
In terms of originality, the paper makes a significant contribution to the field of deep generative models by introducing a novel approach to learning conditional distributions. The concept of CMMD is new and provides a useful tool for evaluating the difference between conditional distributions. The authors also demonstrate the effectiveness of CGMMN in various tasks, which showcases the versatility of the model.
Overall, I would recommend accepting this paper for publication. The paper is well-written, clearly motivated, and makes a significant contribution to the field of deep generative models. The experimental results demonstrate the effectiveness of CGMMN, and the authors provide a thorough discussion of the limitations and potential future directions.
Arguments for acceptance:
* The paper introduces a novel approach to learning conditional distributions using CMMD.
* The experimental results demonstrate the competitive performance of CGMMN compared to state-of-the-art models.
* The paper provides a thorough review of related work and a clear explanation of the theoretical background.
* The authors demonstrate the effectiveness of CGMMN in various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge.
Arguments against acceptance:
* The requirement for a large amount of training data to estimate the CMMD objective may limit the applicability of CGMMN to large-scale datasets.
* The choice of kernel function and hyperparameters may require careful tuning, which can be time-consuming and require significant expertise.
* The paper could benefit from additional experiments and analysis to further demonstrate the effectiveness of CGMMN in various tasks and datasets.