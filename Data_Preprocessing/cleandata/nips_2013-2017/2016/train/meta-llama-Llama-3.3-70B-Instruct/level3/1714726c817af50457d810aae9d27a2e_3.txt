This paper explores the problem of fast learning rates for heavy-tailed losses in the context of empirical risk minimization. The authors introduce two new conditions, the multi-scale Bernstein's condition and the integrability of the envelope function, which enable the derivation of fast learning rates for unbounded losses. The paper provides a rigorous analysis of the new averaging scheme in the continuous setting and presents new contributions and results.
The paper is well-written, and the authors provide a clear and detailed explanation of the problem, the methodology, and the results. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors also provide a thorough discussion of the related work and the significance of the results.
The strengths of the paper include:
* The introduction of new conditions that enable the derivation of fast learning rates for unbounded losses.
* The provision of a rigorous analysis of the new averaging scheme in the continuous setting.
* The presentation of new contributions and results that extend and complement existing results in the literature.
* The clear and detailed explanation of the problem, the methodology, and the results.
The weaknesses of the paper include:
* The paper assumes that the hypothesis class has a finite VC-dimension, which may not be the case in practice.
* The paper does not provide a thorough analysis of the discrete version of the algorithm.
* The paper does not discuss the computational complexity of the algorithm.
Overall, the paper is a significant contribution to the field of machine learning, and the results have the potential to impact the development of robust algorithms that can automatically adapt to data with hidden structures and achieve faster rates whenever possible.
Arguments pro acceptance:
* The paper introduces new conditions that enable the derivation of fast learning rates for unbounded losses.
* The paper provides a rigorous analysis of the new averaging scheme in the continuous setting.
* The paper presents new contributions and results that extend and complement existing results in the literature.
Arguments con acceptance:
* The paper assumes that the hypothesis class has a finite VC-dimension, which may not be the case in practice.
* The paper does not provide a thorough analysis of the discrete version of the algorithm.
* The paper does not discuss the computational complexity of the algorithm.
Recommendation: Accept with minor revisions. The authors should address the weaknesses of the paper, including the assumption of finite VC-dimension, the lack of analysis of the discrete version of the algorithm, and the computational complexity of the algorithm. Additionally, the authors should provide more details on the experimental results and the comparison with existing methods.