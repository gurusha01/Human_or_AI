This paper proposes a novel stochastic gradient descent algorithm, Stochastic Multiple Choice Learning (sMCL), for training ensemble models to produce multiple highly likely hypotheses. The authors build upon existing work in Multiple Choice Learning (MCL) and introduce a new approach that enables the use of deep neural networks. The key contribution of this paper is the use of stochastic gradient descent for training, which makes the algorithm more useful and efficient.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of producing multiple outputs in perception systems. They also provide a thorough review of related work in ensemble learning and generating multiple solutions. The technical contribution of the paper is sound, and the authors provide a detailed explanation of the sMCL algorithm and its advantages over existing methods.
The experiments demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles on a range of tasks, including image classification, semantic segmentation, and image captioning. The results show that sMCL significantly outperforms classical ensembles and other strong baselines, including the 5x slower MCL procedure.
One of the strengths of the paper is its clarity and organization. The authors provide a clear and concise introduction to the problem and related work, and the technical contribution is well-explained. The experiments are also well-designed and demonstrate the effectiveness of the proposed algorithm.
However, one potential weakness of the paper is that the novelty of the algorithm may be considered incremental. The authors build upon existing work in MCL and introduce a new approach that enables the use of deep neural networks. While the authors argue that this change makes the algorithm more useful, some reviewers may consider it an incremental improvement rather than a significant breakthrough.
To further improve the paper, it would be interesting to see a comparison of the sMCL algorithm with batch usage rather than the entire dataset. This could provide more insight into the effectiveness of the algorithm in different scenarios. Additionally, more analysis on the "diversity" effect of the proposed algorithm would be beneficial. The authors provide some examples of how the algorithm can produce diverse solutions, but more quantitative analysis would be helpful in understanding the strengths and limitations of the algorithm.
Overall, the paper is well-written, and the authors provide a clear and concise introduction to the problem and related work. The technical contribution is sound, and the experiments demonstrate the effectiveness of the proposed algorithm. With some additional analysis and comparison, this paper has the potential to make a significant contribution to the field of ensemble learning and multiple output production.
Arguments pro acceptance:
* The paper proposes a novel stochastic gradient descent algorithm for training ensemble models.
* The authors provide a clear and concise introduction to the problem and related work.
* The technical contribution is sound, and the experiments demonstrate the effectiveness of the proposed algorithm.
* The paper is well-written and well-organized.
Arguments con acceptance:
* The novelty of the algorithm may be considered incremental.
* More analysis on the "diversity" effect of the proposed algorithm would be beneficial.
* A comparison of the sMCL algorithm with batch usage rather than the entire dataset would be interesting.