This paper introduces a novel approach to training deep neural networks, called Stochastic Multiple Choice Learning (sMCL), which enables the production of multiple diverse solutions for a given input. The authors formulate the task of training an ensemble of deep networks as a stochastic block gradient descent optimization problem, where each ensemble member is trained to minimize the oracle loss. The proposed method is simple to implement, agnostic to both architecture and loss function, and parameter-free.
The paper is well-written and clearly presented, with a sufficient degree of novelty in the new cell structure and proven usefulness. The authors provide a thorough analysis of the training and output behaviors of the resulting ensembles, demonstrating how ensemble member specialization and expertise emerge automatically when trained using sMCL. The experiments demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles, outperforming classical ensembles and other strong baselines, including the 5x slower MCL procedure.
The strengths of the paper include its ability to handle complex tasks such as image captioning and semantic segmentation, and its capacity to produce diverse and interpretable solutions. The authors also provide a detailed comparison with existing methods, including MCL and Dey et al., and demonstrate the advantages of sMCL in terms of performance and efficiency.
One potential weakness of the paper is the lack of a clear explanation of how the oracle loss is defined and calculated. Additionally, the authors could provide more insight into the choice of hyperparameters, such as the number of ensemble members and the learning rate. However, these are minor concerns, and overall, the paper presents a significant contribution to the field of deep learning.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The authors provide a clear and concise summary of the main ideas and relate them to previous work, demonstrating a good understanding of the context and relevance of their contribution. The paper is well-organized, and the writing is clear and concise, making it easy to follow and understand.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of deep learning.
* The proposed method, sMCL, is simple to implement and agnostic to both architecture and loss function.
* The experiments demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles.
* The paper provides a thorough analysis of the training and output behaviors of the resulting ensembles.
Arguments against acceptance:
* The paper could benefit from a clearer explanation of the oracle loss and its calculation.
* The authors could provide more insight into the choice of hyperparameters.
* The paper may not be suitable for readers without a strong background in deep learning and optimization techniques.
Overall, I recommend accepting this paper for publication, as it presents a significant contribution to the field of deep learning and demonstrates a good understanding of the context and relevance of the proposed method.