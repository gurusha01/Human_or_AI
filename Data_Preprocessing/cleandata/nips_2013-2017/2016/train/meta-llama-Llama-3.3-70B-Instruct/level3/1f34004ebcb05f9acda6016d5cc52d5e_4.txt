This paper proposes a novel batch Bayesian optimization algorithm, the parallel knowledge gradient method (q-KG), which is designed to optimize expensive-to-evaluate functions in a parallel setting. The algorithm is derived from a decision-theoretical perspective and is shown to outperform or be competitive with state-of-the-art benchmark algorithms on several synthetic functions and in tuning practical machine learning algorithms.
The paper is well-written and clearly organized, with a thorough review of related work and a detailed description of the proposed algorithm. The authors provide a clear explanation of the mathematical formulation of the q-KG algorithm and its computational implementation. The experimental results demonstrate the effectiveness of the q-KG algorithm in both noise-free and noisy settings.
One of the strengths of the paper is its ability to handle noisy function evaluations, which is a common challenge in real-world optimization problems. The authors demonstrate that the q-KG algorithm can learn the noise level from data and adapt to it, resulting in improved performance compared to benchmark algorithms.
However, one potential weakness of the paper is the lack of discussion on the computational cost of the q-KG algorithm. While the authors provide an efficient strategy for computing the q-KG factor and its gradient, they do not report the time taken by the proposed optimization method. This information would be useful for evaluating the practicality of the algorithm in real-world applications.
In terms of originality, the paper proposes a novel batch Bayesian optimization algorithm that is derived from a decision-theoretical perspective. The algorithm is a significant contribution to the field of Bayesian optimization, and its ability to handle noisy function evaluations makes it a valuable tool for practitioners.
The significance of the paper lies in its ability to provide a robust and efficient optimization algorithm for expensive-to-evaluate functions. The q-KG algorithm has the potential to be widely adopted in various fields, including machine learning, engineering, and finance, where optimization problems are common.
Overall, I would recommend accepting this paper for publication. The paper is well-written, and the proposed algorithm is a significant contribution to the field of Bayesian optimization. The experimental results demonstrate the effectiveness of the algorithm, and the discussion on the computational implementation provides a clear understanding of the algorithm's practicality.
Arguments pro acceptance:
* The paper proposes a novel batch Bayesian optimization algorithm that is derived from a decision-theoretical perspective.
* The algorithm is shown to outperform or be competitive with state-of-the-art benchmark algorithms on several synthetic functions and in tuning practical machine learning algorithms.
* The paper provides a clear explanation of the mathematical formulation of the q-KG algorithm and its computational implementation.
* The experimental results demonstrate the effectiveness of the q-KG algorithm in both noise-free and noisy settings.
Arguments con acceptance:
* The paper lacks discussion on the computational cost of the q-KG algorithm.
* The authors do not report the time taken by the proposed optimization method, which would be useful for evaluating the practicality of the algorithm in real-world applications.