This paper presents a framework for cooperative interaction between humans and robots, focusing on settings like inverse reinforcement learning and optimal teaching. The authors propose a general framework that can be cast as a Partially Observable Markov Decision Process (POMDP) problem. The key insight is that humans may be better off demonstrating a "best response" to the robot's strategy rather than an optimal policy if they know the robot uses Inverse Reinforcement Learning (IRL).
The paper provides an approximation scheme for cases with linear combinations of state features and single demonstrated trajectories. Experimental results support the insight in a navigation problem. However, the reviewer finds the paper's exposition to be generally clear but lacking in precision and rigor, with some technical explanations and proofs being "hand wavy".
The use of terminology from non-cooperative games, such as Nash equilibrium, in a cooperative framework is questioned. The assumption that both agents know the probability distribution over θ is found to be strange, and the reviewer suggests that the model could be interpreted from the robot's viewpoint with P0 as its prior belief.
The reviewer requests a formal definition of the coordination POMDP to clarify inconsistencies between the paper and supplementary material. Additionally, clarification is sought on whether the IRL approach demonstrates only one trajectory in Sec.4.2 and if this could explain the experimental results.
The paper has several typos and minor errors throughout, including inconsistencies in notation and formatting. Despite these issues, the paper presents an interesting framework for cooperative interaction between humans and robots, and the experimental results are promising.
Arguments for acceptance:
* The paper presents a novel framework for cooperative interaction between humans and robots.
* The experimental results are promising and support the insight in a navigation problem.
* The paper has the potential to contribute to the field of human-robot interaction and IRL.
Arguments against acceptance:
* The paper lacks precision and rigor in its technical explanations and proofs.
* The use of terminology from non-cooperative games in a cooperative framework is questionable.
* The assumption that both agents know the probability distribution over θ is strange and needs clarification.
* The paper has several typos and minor errors throughout.
Overall, the paper has potential, but it requires significant revisions to address the issues mentioned above. With proper revisions, the paper could be a valuable contribution to the field of human-robot interaction and IRL. 
Quality: 6
The paper is technically sound, but it lacks precision and rigor in its technical explanations and proofs. The claims are well-supported by experimental results, but the theoretical analysis could be improved.
Clarity: 7
The paper is generally well-written, but it could be improved with more precise and rigorous technical explanations. The notation and formatting are sometimes inconsistent, which can make it difficult to follow.
Originality: 8
The paper presents a novel framework for cooperative interaction between humans and robots, which is a significant contribution to the field.
Significance: 8
The paper has the potential to contribute significantly to the field of human-robot interaction and IRL. The experimental results are promising, and the framework could be applied to a wide range of applications.