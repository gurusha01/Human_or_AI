This paper presents Conditional Generative Moment-Matching Networks (CGMMN), a novel framework for learning conditional distributions. The authors propose a conditional maximum mean discrepancy (CMMD) criterion, which measures the difference between the kernel mean embeddings of two conditional distributions. The CMMD objective is optimized using stochastic gradient descent, allowing for efficient training of the model.
The paper is well-written, and the authors provide a clear and concise introduction to the concept of CGMMN and its applications. The theoretical foundations of the method are well-established, and the authors provide a thorough analysis of the CMMD criterion and its relationship to the maximum mean discrepancy (MMD) criterion.
The experimental results demonstrate the effectiveness of CGMMN in various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge. The authors compare their method to several state-of-the-art models, including Variational Autoencoders (VAE), Generative Adversarial Networks (GAN), and Max-Margin Deep Generative Models (MMVA). The results show that CGMMN is competitive with these models in terms of performance, while also providing a more flexible and efficient framework for learning conditional distributions.
One of the strengths of the paper is its clarity and organization. The authors provide a clear introduction to the concept of CGMMN, followed by a detailed analysis of the theoretical foundations and experimental results. The paper is well-structured, making it easy to follow and understand.
However, there are some areas where the paper could be improved. For example, the authors could provide more intuition behind the CMMD criterion and its relationship to the MMD criterion. Additionally, the paper could benefit from more detailed analysis of the experimental results, including a more thorough comparison to other state-of-the-art models.
Overall, this paper presents a significant contribution to the field of deep learning and conditional modeling. The proposed CGMMN framework provides a flexible and efficient way to learn conditional distributions, and the experimental results demonstrate its effectiveness in various tasks. With some minor improvements, this paper has the potential to be a high-quality publication.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of deep learning and conditional modeling.
* The authors provide a clear and concise introduction to the concept of CGMMN and its applications.
* The theoretical foundations of the method are well-established, and the authors provide a thorough analysis of the CMMD criterion and its relationship to the MMD criterion.
* The experimental results demonstrate the effectiveness of CGMMN in various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge.
Arguments against acceptance:
* The paper could benefit from more intuition behind the CMMD criterion and its relationship to the MMD criterion.
* The paper could benefit from more detailed analysis of the experimental results, including a more thorough comparison to other state-of-the-art models.
* Some of the experimental results could be improved, such as the generation of more realistic images in the contextual generation task.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions. The authors should provide more intuition behind the CMMD criterion and its relationship to the MMD criterion, and perform more detailed analysis of the experimental results. Additionally, the authors should consider improving some of the experimental results, such as the generation of more realistic images in the contextual generation task.