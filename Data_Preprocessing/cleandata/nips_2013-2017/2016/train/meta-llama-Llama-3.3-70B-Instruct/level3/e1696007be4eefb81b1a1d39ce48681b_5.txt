This paper presents a novel approach to estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations is much larger than the number of predictors. The authors propose an algorithm, called Scaled Least Squares (SLS), which achieves the same accuracy as the maximum likelihood estimator (MLE) but with a significantly reduced computational cost.
The paper is well-structured and well-written, with clear explanations of the methodology and theoretical results. The authors provide a thorough analysis of the performance of the SLS algorithm, including theoretical guarantees and extensive numerical studies on large-scale real and synthetic datasets.
The key contribution of the paper is the observation that GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients, which allows for a significant reduction in computational cost. The authors show that this proportionality holds in the general random design setting, regardless of the predictor distribution.
The SLS algorithm is composed of two steps: first, estimating the OLS coefficients, and then estimating the proportionality constant through iterations that can attain quadratic or cubic convergence rate, with only O(n) per-iteration cost. The authors provide theoretical results characterizing the finite sample behavior of the SLS estimator, including bounds on the estimation error.
The numerical experiments demonstrate that the SLS algorithm outperforms standard methods, including Newton-Raphson, quasi-Newton methods, and gradient descent, in terms of computational cost and accuracy. The authors also provide a comparison with other optimization algorithms, including Newton-Stein and limited memory BFGS, and show that the SLS algorithm achieves the minimum achievable test error substantially faster.
The paper has several strengths, including:
* The proposed algorithm is computationally efficient and scalable to large datasets.
* The theoretical results provide a clear understanding of the performance of the SLS algorithm.
* The numerical experiments demonstrate the effectiveness of the SLS algorithm in practice.
However, there are some potential weaknesses and areas for future research:
* The assumption of random design may not always hold in practice, and it would be interesting to explore the performance of the SLS algorithm in other settings.
* The authors mention that the proportionality between the coefficients holds even when there is regularization, but this is not fully explored in the paper.
* The SLS algorithm may not be suitable for very large datasets, and it would be interesting to explore distributed or parallel versions of the algorithm.
Overall, the paper presents a significant contribution to the field of large-scale optimization and GLM estimation, and the proposed SLS algorithm has the potential to be widely adopted in practice.
Arguments pro acceptance:
* The paper presents a novel and efficient algorithm for GLM estimation.
* The theoretical results provide a clear understanding of the performance of the SLS algorithm.
* The numerical experiments demonstrate the effectiveness of the SLS algorithm in practice.
Arguments con acceptance:
* The assumption of random design may not always hold in practice.
* The paper could benefit from a more detailed exploration of the performance of the SLS algorithm in other settings.
* The SLS algorithm may not be suitable for very large datasets.