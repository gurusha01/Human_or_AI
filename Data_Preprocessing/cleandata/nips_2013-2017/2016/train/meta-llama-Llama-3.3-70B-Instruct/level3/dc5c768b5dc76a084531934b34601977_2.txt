This paper proposes a novel approach to learning conditional distributions using a conditional generative moment-matching network (CGMMN). The authors provide a thorough theoretical treatment of the problem and conduct extensive experiments to demonstrate the effectiveness of their approach. The paper is well-written, and the ideas are clearly presented.
The strengths of the paper include:
* The authors provide a clear and concise introduction to the problem of learning conditional distributions and the limitations of existing approaches.
* The proposed CGMMN framework is well-motivated and theoretically sound, building on the concept of maximum mean discrepancy (MMD) and kernel mean embedding.
* The experiments are thorough and demonstrate the competitive performance of CGMMN on various tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge.
However, there are some weaknesses and limitations to the paper:
* The paper assumes that the conditional variables are given, which may not always be the case in practice. It would be interesting to explore how CGMMN can be extended to handle cases where the conditional variables are not observed.
* The authors mention that the computation cost for kernel gram matrix grows cubically with the sample size, which can be a limitation for large-scale datasets. It would be useful to explore more efficient algorithms or approximations to mitigate this issue.
* The paper could benefit from more discussion on the relationship between CGMMN and other existing approaches to conditional modeling, such as conditional GANs and conditional variational autoencoders.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and presents a novel approach to learning conditional distributions. The experiments are thorough and demonstrate the competitive performance of CGMMN.
Arguments for acceptance:
* The paper presents a novel and theoretically sound approach to learning conditional distributions.
* The experiments demonstrate the competitive performance of CGMMN on various tasks.
* The paper is well-written and clearly presents the ideas and contributions.
Arguments for rejection:
* The paper assumes that the conditional variables are given, which may not always be the case in practice.
* The computation cost for kernel gram matrix can be a limitation for large-scale datasets.
* The paper could benefit from more discussion on the relationship between CGMMN and other existing approaches to conditional modeling.
Overall, I recommend accepting the paper, as it presents a novel and theoretically sound approach to learning conditional distributions, and the experiments demonstrate its competitive performance on various tasks. However, the authors should address the limitations and weaknesses mentioned above in the final version of the paper.