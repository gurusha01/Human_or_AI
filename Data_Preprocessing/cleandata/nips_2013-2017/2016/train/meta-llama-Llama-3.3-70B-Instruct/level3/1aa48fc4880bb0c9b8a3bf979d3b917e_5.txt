This paper proposes a novel approach to dropout, a widely used technique in deep learning to prevent overfitting. The authors introduce a multinomial dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features or neurons. The key contribution of this paper is the analysis of the risk bound for stochastic optimization with multinomial dropout, which leads to a distribution-dependent dropout that achieves faster convergence and smaller generalization error.
The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work. The related work section is thorough, and the authors provide a good overview of the existing literature on dropout and optimization algorithms for deep learning.
The technical contributions of the paper are significant, and the authors provide a rigorous analysis of the risk bound for stochastic optimization with multinomial dropout. The proposed distribution-dependent dropout is shown to achieve faster convergence and smaller generalization error than the standard dropout. The authors also propose an efficient evolutional dropout for deep learning, which adapts the sampling probabilities to the evolving distributions of layers' outputs.
The experimental results are impressive, and the authors demonstrate the effectiveness of the proposed dropouts on several benchmark datasets. The results show that the proposed dropouts can dramatically improve the convergence and reduce the testing error.
The paper has several strengths, including:
* The proposed distribution-dependent dropout is theoretically grounded, and the authors provide a rigorous analysis of the risk bound for stochastic optimization with multinomial dropout.
* The experimental results are impressive, and the authors demonstrate the effectiveness of the proposed dropouts on several benchmark datasets.
* The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work.
However, the paper also has some weaknesses, including:
* The paper could benefit from more discussion on the relationship between the proposed dropouts and other regularization techniques, such as batch normalization.
* The authors could provide more insight into the choice of hyperparameters, such as the value of k and the step size.
Overall, this paper makes a significant contribution to the field of deep learning, and the proposed distribution-dependent dropout has the potential to improve the performance of deep neural networks. The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work.
Arguments for acceptance:
* The paper proposes a novel approach to dropout, which is theoretically grounded and has the potential to improve the performance of deep neural networks.
* The experimental results are impressive, and the authors demonstrate the effectiveness of the proposed dropouts on several benchmark datasets.
* The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work.
Arguments against acceptance:
* The paper could benefit from more discussion on the relationship between the proposed dropouts and other regularization techniques, such as batch normalization.
* The authors could provide more insight into the choice of hyperparameters, such as the value of k and the step size.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above and provide more discussion on the relationship between the proposed dropouts and other regularization techniques. Additionally, the authors could provide more insight into the choice of hyperparameters.