This paper proposes a novel memory allocation strategy using dynamic programming to reduce GPU memory consumption for the backpropagation through time (BPTT) algorithm in recurrent neural networks (RNNs). The approach aims to balance the trade-off between caching intermediate results and recomputation, achieving a significant reduction in memory usage compared to previous methods. The authors provide a good theoretical analysis and bounds, demonstrating the effectiveness of their approach in reducing memory consumption while maintaining a reasonable computational cost.
The paper's strengths include its ability to tightly fit within almost any user-defined memory budget, making it a valuable contribution to the field of RNNs. The authors also provide a thorough analysis of the computational cost and memory usage of their approach, comparing it to existing methods. The use of dynamic programming to find an optimal memory usage policy is a key innovation of the paper.
However, the paper has some weaknesses. The experimental results show insignificant speedup for very long sequences with feasible memory consumption, which may limit the impact of the approach in certain applications. Additionally, the idea of dynamic programming is considered incremental compared to existing methods, which may reduce the paper's originality. The paper's focus on saving memory consumption may have limited impact on training deep neural networks, as other factors like convergence speed and computational cost also play a crucial role.
The paper requires minor corrections, including typos in the supplementary materials and a grammatical error on page 7. Overall, the paper is well-written and clearly organized, making it easy to follow and understand.
Arguments for acceptance:
* The paper proposes a novel approach to reducing memory consumption in RNNs, which is a significant contribution to the field.
* The authors provide a thorough theoretical analysis and bounds, demonstrating the effectiveness of their approach.
* The paper is well-written and clearly organized, making it easy to follow and understand.
Arguments against acceptance:
* The experimental results show insignificant speedup for very long sequences with feasible memory consumption, which may limit the impact of the approach.
* The idea of dynamic programming is considered incremental compared to existing methods, which may reduce the paper's originality.
* The paper's focus on saving memory consumption may have limited impact on training deep neural networks, as other factors like convergence speed and computational cost also play a crucial role.
Overall, I would recommend accepting the paper, as its contributions to the field of RNNs and its thorough analysis and bounds make it a valuable addition to the conference. However, the authors should address the minor corrections and consider the limitations of their approach in their future work.