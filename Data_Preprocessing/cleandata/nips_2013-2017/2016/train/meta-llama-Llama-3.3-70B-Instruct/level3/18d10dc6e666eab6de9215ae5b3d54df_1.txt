This paper introduces a novel batch-sequential Bayesian optimization algorithm, the parallel knowledge gradient (q-KG) method, which is derived from a decision-theoretical perspective. The q-KG method is shown to outperform state-of-the-art batch-sequential Bayesian optimization algorithms in numerical experiments on both synthetic test functions and practical machine learning algorithms, especially in the presence of noisy function evaluations.
The paper is well-written and clearly organized, with a thorough review of related work and a detailed description of the proposed method. The authors provide a clear motivation for the development of the q-KG method and demonstrate its effectiveness through extensive numerical experiments.
The strengths of the paper include:
* The introduction of a novel batch-sequential Bayesian optimization algorithm that is derived from a decision-theoretical perspective.
* The demonstration of the effectiveness of the q-KG method in numerical experiments on both synthetic test functions and practical machine learning algorithms.
* The provision of a clear and detailed description of the proposed method, including the derivation of the q-KG factor and the computation of its gradient.
However, there are also some weaknesses and areas for improvement:
* The paper lacks a study on the speed-ups from sequential to batch-sequential Bayesian optimization, which would be useful in understanding the benefits of the proposed method.
* There are some imprecise statements in the literature review, which could be clarified or removed.
* The paper raises several technical questions and remarks, including the compactness and continuity of the function A, the restriction of A to be an LHS, and the hyperparameter re-estimation in Algorithm 1, which could be addressed in future work.
* The paper contains some minor mistakes, including an incorrect specification of the Mat√©rn parameter and a potential issue with the reproducibility of the experiments using different software.
Overall, the paper is well-written and makes a significant contribution to the field of Bayesian optimization. The proposed q-KG method has the potential to be widely used in practice, especially in the presence of noisy function evaluations.
Arguments pro acceptance:
* The paper introduces a novel batch-sequential Bayesian optimization algorithm that is derived from a decision-theoretical perspective.
* The paper demonstrates the effectiveness of the q-KG method in numerical experiments on both synthetic test functions and practical machine learning algorithms.
* The paper provides a clear and detailed description of the proposed method, including the derivation of the q-KG factor and the computation of its gradient.
Arguments con acceptance:
* The paper lacks a study on the speed-ups from sequential to batch-sequential Bayesian optimization.
* There are some imprecise statements in the literature review.
* The paper raises several technical questions and remarks that could be addressed in future work.
* The paper contains some minor mistakes that could be corrected in a revised version.
In conclusion, the paper is well-written and makes a significant contribution to the field of Bayesian optimization. While there are some weaknesses and areas for improvement, the strengths of the paper outweigh the weaknesses, and the paper is recommended for acceptance.