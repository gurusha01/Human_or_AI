This paper addresses the expectation-maximization (EM) algorithm for estimating mean parameters in a mixture of Gaussian variables model with known and fixed parameters. The authors prove three negative results for the EM algorithm, including the existence of local maxima for the likelihood function and convergence to critical points with low likelihood values. Specifically, they show that the EM algorithm, even with random initiation, converges with high probability to a critical point with a likelihood value much smaller than the maximal likelihood value. The same negative result holds for the gradient EM algorithm.
The paper's simplifications, such as a known number of components and uniform weights, are not considered a problem due to the technical nature of the proofs and the informative results obtained. However, I raise concerns about the proof of Theorem 1, suggesting that it is too short and requires more details, particularly in the computation of limits of supremums of likelihood functions. Additionally, I find the part of the paper addressing the Gradient EM algorithm to be difficult to follow and suggest that more explanation is needed, particularly for Equation (5).
The paper's quality is good, with well-supported claims and a clear structure. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The clarity of the paper is also good, with sufficient information provided for the expert reader to reproduce the results. However, some parts of the paper, such as the proof of Theorem 1 and the discussion of the Gradient EM algorithm, could be improved with more detailed explanations.
The originality of the paper is high, as it provides new and significant results on the population likelihood function of Gaussian mixture models. The paper's significance is also high, as it highlights the necessity of careful initialization when using the EM algorithm in practice, even in highly favorable settings. The results have important implications for the use of EM algorithms in statistical and machine learning applications.
Arguments pro acceptance:
* The paper provides new and significant results on the population likelihood function of Gaussian mixture models.
* The authors prove three negative results for the EM algorithm, including the existence of local maxima for the likelihood function and convergence to critical points with low likelihood values.
* The paper highlights the necessity of careful initialization when using the EM algorithm in practice, even in highly favorable settings.
Arguments con acceptance:
* The proof of Theorem 1 is too short and requires more details.
* The discussion of the Gradient EM algorithm is difficult to follow and requires more explanation.
* Some parts of the paper could be improved with more detailed explanations and clearer notation.
Overall, I recommend acceptance of the paper, but with revisions to address the concerns mentioned above. The paper's contributions to the field of statistical and machine learning are significant, and the results have important implications for the use of EM algorithms in practice.