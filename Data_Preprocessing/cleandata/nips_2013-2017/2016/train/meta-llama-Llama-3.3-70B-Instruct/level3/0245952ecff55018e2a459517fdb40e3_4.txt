This paper proposes two novel optimization methods, a gradient-free and a gradient-based approach, to solve the non-convex loss-minimization problem of learning Supervised PageRank models. The authors demonstrate the technical soundness of their proposed model by showcasing its competitive performance on predictive modeling, contextual generation, and distillation of predictions from a Bayesian model.
The paper offers novelty and usefulness through its mechanism and derivation of the empirical estimator, making the conditional extension to the generative moment matching framework non-obvious. The derived framework is easy to apply, as shown in Algorithm 1, and is likely to be useful for those interested in using deep models for generative purposes or distilling statistical processes.
The paper is well-written, clear, and easy to read, with a useful appendix providing additional information on methods and experiments. The authors provide a thorough analysis of the trade-off between the accuracy of the lower-level algorithm and the computational complexity of the two-level algorithm as a whole.
The strengths of the paper include its ability to provide theoretical guarantees on convergence rate and complexity bounds, outperforming the state-of-the-art method in experiments. The main advantages of the first gradient-based algorithm are its guarantees of convergence without requiring convexity and having fewer input parameters than the gradient-free algorithm. The main advantage of the second gradient-free algorithm is that it avoids calculating the derivative for each element of a large matrix.
However, a potential weakness of the paper is the assumption that the set Φ is a small vicinity of the local minimum ϕ∗, in which f(ϕ) is convex. This assumption may not always hold in practice, and the authors could provide more discussion on the robustness of their method to non-convexity.
In terms of originality, the paper combines the ideas of random gradient-free and gradient optimization methods with the concept of an inexact oracle, which is a novel contribution to the field. The authors also provide a thorough comparison with the state-of-the-art method, demonstrating the superiority of their approach.
Overall, the paper is well-structured, and the authors provide a clear and concise presentation of their ideas. The paper is a significant contribution to the field of optimization and machine learning, and its results have the potential to impact the development of more efficient and reliable algorithms for web-page ranking and other applications.
Arguments pro acceptance:
* The paper proposes novel optimization methods with theoretical guarantees on convergence rate and complexity bounds.
* The methods outperform the state-of-the-art approach in experiments.
* The paper provides a thorough analysis of the trade-off between accuracy and computational complexity.
* The authors combine the ideas of random gradient-free and gradient optimization methods with the concept of an inexact oracle, which is a novel contribution to the field.
Arguments con acceptance:
* The assumption of convexity in the set Φ may not always hold in practice.
* The paper could provide more discussion on the robustness of the method to non-convexity.
* The authors could provide more experimental results to demonstrate the scalability and applicability of their approach to other problems.