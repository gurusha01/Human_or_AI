This paper proposes a novel stochastic gradient descent-based approach, called Stochastic Multiple Choice Learning (sMCL), for training diverse deep ensembles that minimize the oracle loss. The authors formulate the task of training an ensemble of deep networks as a stochastic block gradient descent optimization problem, where each ensemble member is trained concurrently using a "winner-take-gradient" strategy. The proposed method is simple to implement, agnostic to both architecture and loss function, and parameter-free.
The paper is well-written and clearly explains the motivation and technical details of the proposed approach. The authors provide a thorough review of related work and demonstrate the broad applicability and efficacy of sMCL on various computer vision tasks, including image classification, semantic segmentation, and image captioning. The experimental results show that sMCL significantly outperforms classical ensembles and other strong baselines, including the 5x slower MCL procedure.
However, the paper has some limitations. The proposed method builds upon existing work with minor adjustments, and the technical section lacks clarity on the differences between the proposed framework and alternative training methods. The experiments are weakened by the lack of strong baselines and comprehensive comparisons. Additionally, the paper could benefit from more detailed analysis of the training and output behaviors of the resulting ensembles.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is well-organized, and the writing is clear and concise. The authors provide enough information for the expert reader to reproduce the results.
The originality of the paper is limited, as the proposed method is an extension of existing work. However, the paper provides a novel combination of familiar techniques and demonstrates the efficacy of the proposed approach on various tasks. The related work is adequately referenced, and the authors clearly explain how their work differs from previous contributions.
The significance of the paper is moderate, as the results are important and demonstrate the potential of the proposed approach. The paper addresses a difficult problem in a better way than previous research and provides unique insights into the training and output behaviors of deep ensembles. However, the impact of the paper may be limited by the lack of strong baselines and comprehensive comparisons.
Overall, I would recommend accepting this paper, but with some revisions to address the limitations mentioned above. The authors should provide more detailed analysis of the training and output behaviors of the resulting ensembles and include stronger baselines and comprehensive comparisons to demonstrate the efficacy of the proposed approach.
Arguments pro acceptance:
* The paper proposes a novel stochastic gradient descent-based approach for training diverse deep ensembles.
* The experimental results demonstrate the broad applicability and efficacy of the proposed approach on various computer vision tasks.
* The paper is well-written and clearly explains the motivation and technical details of the proposed approach.
Arguments con acceptance:
* The proposed method builds upon existing work with minor adjustments.
* The technical section lacks clarity on the differences between the proposed framework and alternative training methods.
* The experiments are weakened by the lack of strong baselines and comprehensive comparisons.