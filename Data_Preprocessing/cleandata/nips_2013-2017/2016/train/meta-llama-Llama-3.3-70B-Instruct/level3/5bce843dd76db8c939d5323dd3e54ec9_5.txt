This paper proposes a novel approach to training deep neural networks, called Stochastic Multiple Choice Learning (sMCL), which enables the learning of multiple diverse solutions to a problem. The authors formulate the task of training an ensemble of deep networks as a stochastic block gradient descent optimization problem, where each ensemble member is trained to minimize the oracle loss. The proposed approach is simple to implement, agnostic to both architecture and loss function, and parameter-free.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of multiple choice learning. The related work section is thorough, and the authors provide a good overview of existing methods for training ensembles and generating multiple solutions. The technical contribution of the paper is significant, and the authors provide a detailed description of the sMCL algorithm and its implementation.
The experiments section is extensive, and the authors demonstrate the efficacy of sMCL on a range of tasks, including image classification, semantic segmentation, and image captioning. The results show that sMCL significantly outperforms classical ensembles and other strong baselines, including the 5x slower MCL procedure. The authors also provide a detailed analysis of the training and output behaviors of the resulting ensembles, demonstrating how ensemble member specialization and expertise emerge automatically when trained using sMCL.
The strengths of the paper include its technical soundness, clarity, and significance. The authors provide a clear and concise description of the sMCL algorithm and its implementation, and the experiments section is thorough and well-designed. The results demonstrate the efficacy of sMCL on a range of tasks, and the authors provide a detailed analysis of the training and output behaviors of the resulting ensembles.
The weaknesses of the paper are minor. One potential area for improvement is the provision of more details on the experimental setup, such as the number of oscillation periods used. Additionally, the authors could provide more discussion on the potential applications of sMCL in real-world scenarios.
Overall, I believe that this paper is a strong candidate for acceptance at NIPS. The technical contribution is significant, and the authors provide a clear and concise description of the sMCL algorithm and its implementation. The experiments section is thorough, and the results demonstrate the efficacy of sMCL on a range of tasks.
Arguments for acceptance:
* The paper proposes a novel approach to training deep neural networks, which enables the learning of multiple diverse solutions to a problem.
* The technical contribution is significant, and the authors provide a clear and concise description of the sMCL algorithm and its implementation.
* The experiments section is thorough, and the results demonstrate the efficacy of sMCL on a range of tasks.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem of multiple choice learning.
Arguments against acceptance:
* The paper could benefit from more details on the experimental setup, such as the number of oscillation periods used.
* The authors could provide more discussion on the potential applications of sMCL in real-world scenarios.
Rating: 9/10
Recommendation: Accept