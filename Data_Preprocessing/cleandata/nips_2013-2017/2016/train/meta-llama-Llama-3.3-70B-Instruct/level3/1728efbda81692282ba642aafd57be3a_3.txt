This paper presents a significant contribution to the field of recurrent neural networks (RNNs) by introducing full-capacity unitary RNNs (uRNNs) that optimize their recurrence matrix over all unitary matrices. The authors provide a theoretical argument to determine if a unitary parameterization has restricted capacity and show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. They also propose a method for stochastic gradient descent to train the unitary recurrence matrix, which constrains the gradient to lie on the differentiable manifold of unitary matrices.
The paper is well-written, organized, and technically solid, with clear presentation of theoretical concepts and sufficient numerical experiments to illustrate the method's usefulness. The authors demonstrate the effectiveness of their approach on various tasks, including synthetic system identification, long-term memorization, frame-to-frame prediction of speech spectra, and pixel-by-pixel classification of handwritten digits. The results show that full-capacity uRNNs generally achieve equivalent or superior performance compared to both LSTMs and the original restricted-capacity uRNNs.
However, there are some technical issues that need to be addressed. The proof of Theorem 3.2 relies on Sard's theorem, which may not be familiar to all readers. Additionally, the application of the operator K_H in the proof of Theorem 3.2 is not entirely clear. Furthermore, there are a few typos in the paper, including errors in lines 55 and 153.
The significance of this work lies in its ability to address the vanishing and exploding gradient problems in RNNs. The use of unitary recurrence matrices provides a novel approach to stabilizing the gradients, and the introduction of full-capacity uRNNs offers a more flexible and powerful framework for modeling complex sequential data. The results demonstrate the potential of this approach to outperform existing methods, including LSTMs, on various tasks.
In terms of originality, the paper presents a new combination of familiar techniques, including unitary matrices and stochastic gradient descent. The authors also provide a unique theoretical argument to quantify the capacity of constrained unitary matrices. The related work is adequately referenced, and the paper provides a clear overview of the current state of the field.
Overall, this paper makes a valuable contribution to the field of RNNs and has the potential to impact the development of more effective and efficient models for sequential data. The strengths of the paper include its technical solidity, clear presentation, and significant experimental results. The weaknesses include some technical issues and typos, which can be addressed in a revised version.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of RNNs.
* The authors provide a clear and well-organized presentation of their work.
* The experimental results demonstrate the effectiveness of the proposed approach.
* The paper has the potential to impact the development of more effective and efficient models for sequential data.
Arguments con acceptance:
* The proof of Theorem 3.2 relies on Sard's theorem, which may not be familiar to all readers.
* The application of the operator K_H in the proof of Theorem 3.2 is not entirely clear.
* There are a few typos in the paper, including errors in lines 55 and 153.
Recommendation: Accept with minor revisions to address the technical issues and typos.