This paper proposes two novel optimization methods, a gradient-free method and a gradient-based method, for solving a non-convex loss-minimization problem in the context of learning Supervised PageRank models. The problem is formulated as a constrained optimization problem, where the goal is to minimize the discrepancy between the ranking scores and the assigned labels. The authors provide a thorough analysis of the proposed methods, including convergence rate guarantees and complexity bounds.
The paper is well-structured and easy to follow, with clear explanations of the proposed methods and their theoretical analysis. The experimental results demonstrate the effectiveness of the proposed methods, outperforming the state-of-the-art gradient-based method on a web page ranking problem.
The strengths of the paper include:
* The proposal of two novel optimization methods, which can be applied to a wide range of problems beyond the specific context of Supervised PageRank models.
* The provision of convergence rate guarantees and complexity bounds for the proposed methods, which is essential for understanding their theoretical properties.
* The thorough experimental evaluation of the proposed methods, which demonstrates their effectiveness in practice.
However, there are some weaknesses and areas for improvement:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution, which may not always be the case in practice.
* The choice of hyperparameters, such as the Lipschitz constant and the accuracy, can significantly affect the performance of the proposed methods.
* The paper could benefit from a more detailed comparison with other optimization methods, such as stochastic gradient descent and its variants.
Overall, the paper makes a significant contribution to the field of optimization and machine learning, and the proposed methods have the potential to be applied to a wide range of problems. However, further research is needed to fully understand the properties and limitations of the proposed methods.
Arguments pro acceptance:
* The paper proposes two novel optimization methods with convergence rate guarantees and complexity bounds.
* The experimental results demonstrate the effectiveness of the proposed methods in practice.
* The paper is well-structured and easy to follow, with clear explanations of the proposed methods and their theoretical analysis.
Arguments con acceptance:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution, which may not always be the case in practice.
* The choice of hyperparameters can significantly affect the performance of the proposed methods.
* The paper could benefit from a more detailed comparison with other optimization methods.