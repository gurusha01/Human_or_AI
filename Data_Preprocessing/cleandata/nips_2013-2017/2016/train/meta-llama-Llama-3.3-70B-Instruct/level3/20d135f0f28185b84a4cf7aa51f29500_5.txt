This paper proposes a novel extension of the encoder-decoder framework, called the review network, which performs multiple review steps with attention on the encoder hidden states to generate a set of thought vectors. The review network is generic and can be integrated into existing encoder-decoder models, and the authors demonstrate its effectiveness on image captioning and source code captioning tasks.
The paper is well-written and clearly explains the motivation, architecture, and training procedure of the review network. The authors provide a thorough analysis of the model's performance on two different tasks and compare it with various baseline models, including attentive encoder-decoders. The results show that the review network consistently outperforms the baseline models, demonstrating its ability to learn a more compact and effective representation of the input data.
One of the strengths of the paper is its clarity and organization. The authors provide a clear overview of the review network architecture and its components, and the experimental results are well-presented and easy to follow. The paper also provides a good discussion of related work and how the review network differs from existing models.
However, there are some areas that could be improved. For example, the authors could provide more analysis on the importance of initialization and how it affects the results. Additionally, the comparison settings could be more clearly explained, including whether the same pre-trained network was used for baseline methods. It would also be useful to evaluate the diversity of ensemble members, not just oracle accuracy, for a more comprehensive analysis.
Another potential area for improvement is the comparison with other similar works, such as the work by Stefan Lee et al. on training a diverse ensemble of deep networks. This comparison could provide more insight into the strengths and weaknesses of the review network and how it relates to other existing models.
Overall, the paper presents a novel and effective architecture for improving the encoder-decoder framework, and the results demonstrate its potential for a wide range of applications. With some additional analysis and comparison with other works, the paper could be even stronger.
Arguments pro acceptance:
* The paper proposes a novel and effective architecture for improving the encoder-decoder framework.
* The results demonstrate the review network's ability to learn a more compact and effective representation of the input data.
* The paper is well-written and clearly explains the motivation, architecture, and training procedure of the review network.
Arguments con acceptance:
* The paper could benefit from more analysis on the importance of initialization and its effect on the results.
* The comparison settings could be more clearly explained, including whether the same pre-trained network was used for baseline methods.
* The paper could benefit from a more comprehensive analysis of the diversity of ensemble members, not just oracle accuracy.