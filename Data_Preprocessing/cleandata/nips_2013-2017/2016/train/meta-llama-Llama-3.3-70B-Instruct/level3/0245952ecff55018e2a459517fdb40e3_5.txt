This paper proposes two novel optimization methods, a gradient-free and a gradient-based approach, to solve the non-convex loss-minimization problem of learning Supervised PageRank models. The authors extend existing frameworks to accommodate inexact oracles, which is a crucial aspect of the problem. The gradient-free method is based on a randomization technique, while the gradient-based method utilizes an adaptive projected gradient algorithm.
The paper is well-structured, and the authors provide a clear explanation of the problem, the proposed methods, and the theoretical guarantees. The experimental results demonstrate the effectiveness of the proposed methods, outperforming the state-of-the-art gradient-based method. The authors also provide a thorough analysis of the trade-off between the accuracy of the lower-level algorithm and the computational complexity of the two-level algorithm.
The strengths of the paper include:
* The proposal of two novel optimization methods that can handle inexact oracles, which is a significant contribution to the field.
* The provision of theoretical guarantees for the convergence rate and complexity bounds of the proposed methods.
* The experimental results demonstrate the effectiveness of the proposed methods in practice.
However, there are some weaknesses:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution, which might not always be the case.
* The choice of hyperparameters, such as the Lipschitz constant and the accuracy, can significantly affect the performance of the algorithms.
* The paper could benefit from a more detailed comparison with other existing methods, including those that do not use inexact oracles.
Overall, the paper presents a significant contribution to the field of optimization and machine learning, and the proposed methods have the potential to be applied to a wide range of problems. The authors have addressed the reviewer's concerns in their rebuttal, and the revised paper is well-written and clear.
Arguments pro acceptance:
* The paper proposes novel optimization methods that can handle inexact oracles.
* The authors provide theoretical guarantees for the convergence rate and complexity bounds of the proposed methods.
* The experimental results demonstrate the effectiveness of the proposed methods in practice.
Arguments con acceptance:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution.
* The choice of hyperparameters can significantly affect the performance of the algorithms.
* The paper could benefit from a more detailed comparison with other existing methods.
In conclusion, the paper is well-written, and the proposed methods are significant contributions to the field. While there are some weaknesses, the authors have addressed the reviewer's concerns, and the revised paper is clear and well-structured. I recommend accepting the paper.