This paper proposes a novel extension of the encoder-decoder framework, called the review network, which enhances the existing framework by performing multiple review steps with attention on the encoder hidden states. The review network outputs a set of thought vectors that capture global properties of the input, which are then used by the decoder to generate the output. The authors demonstrate the effectiveness of the review network on two tasks: image captioning and source code captioning, achieving state-of-the-art performance on the MSCOCO benchmark dataset.
The paper is well-motivated and clearly explained, with numerous worked examples demonstrating the potential of the review network. The authors provide a thorough comparison to existing work, including attentive encoder-decoders and memory networks. The review network is shown to be a generalization of conventional encoder-decoders, and the authors demonstrate its ability to learn more compact and effective representations.
One of the strengths of the paper is its clarity and organization. The authors provide a clear overview of the review network architecture and its components, including the encoder, reviewer, and decoder. The paper is well-structured, with each section building on the previous one to provide a comprehensive understanding of the review network.
The experimental results are also impressive, with the review network achieving state-of-the-art performance on the MSCOCO benchmark dataset. The authors provide a detailed analysis of the results, including a comparison to existing work and an ablation study to evaluate the effectiveness of different components of the review network.
However, there are some areas for improvement. One concern is the lack of explicit description of the term "sufficiently flexible" in Proposition 1, which should be clarified to provide a clearer understanding of the review network's capabilities. Additionally, the comparison to existing work in Section 4 could be improved by noting earlier work on kernel-based estimators of the likelihood, such as Diggle & Gratton (1984).
Overall, the paper presents a significant contribution to the field of natural language processing and computer vision, and the review network has the potential to be outstanding and impactful, especially for problems with costly mock data simulations. The paper is well-written, and the authors provide a clear and comprehensive overview of the review network architecture and its applications.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of natural language processing and computer vision.
* The review network architecture is well-motivated and clearly explained, with numerous worked examples demonstrating its potential.
* The experimental results are impressive, with the review network achieving state-of-the-art performance on the MSCOCO benchmark dataset.
* The paper is well-organized and clearly written, making it easy to follow and understand.
Arguments con acceptance:
* The lack of explicit description of the term "sufficiently flexible" in Proposition 1 may raise concerns about the review network's capabilities.
* The comparison to existing work in Section 4 could be improved by noting earlier work on kernel-based estimators of the likelihood.
* The paper may benefit from additional experiments and analysis to further evaluate the effectiveness of the review network and its components.