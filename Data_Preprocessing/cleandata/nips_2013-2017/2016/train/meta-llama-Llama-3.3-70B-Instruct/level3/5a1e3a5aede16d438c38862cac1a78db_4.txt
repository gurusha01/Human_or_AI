This paper proposes a novel approach to dropout, a widely used technique for preventing overfitting in deep neural networks. The authors introduce a multinomial dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features or neurons. The paper provides a rigorous analysis of the risk bound for stochastic optimization with multinomial dropout and demonstrates that a distribution-dependent dropout leads to a smaller expected risk and faster convergence.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed approach. The theoretical analysis is thorough, and the experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts. The authors also provide a comparison with batch normalization, which shows that the evolutional dropout achieves comparable performance.
The strengths of the paper include:
* The proposal of a novel approach to dropout that adapts to the evolving distribution of layers' outputs
* A rigorous theoretical analysis of the risk bound for stochastic optimization with multinomial dropout
* Experimental results on several benchmark datasets that demonstrate the effectiveness of the proposed dropouts
* A comparison with batch normalization that shows the evolutional dropout achieves comparable performance
The weaknesses of the paper include:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The paper does not provide a clear explanation of how to choose the hyperparameters, such as the value of k
* The paper does not provide a comparison with other dropout techniques, such as adaptive dropout
Overall, the paper is well-written, and the proposed approach is novel and effective. The theoretical analysis is thorough, and the experimental results demonstrate the effectiveness of the proposed dropouts. However, the paper could be improved by providing a clearer explanation of how to choose the hyperparameters and by comparing the proposed approach with other dropout techniques.
Arguments for acceptance:
* The paper proposes a novel approach to dropout that adapts to the evolving distribution of layers' outputs
* The paper provides a rigorous theoretical analysis of the risk bound for stochastic optimization with multinomial dropout
* The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts
Arguments against acceptance:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The paper does not provide a clear explanation of how to choose the hyperparameters, such as the value of k
* The paper does not provide a comparison with other dropout techniques, such as adaptive dropout
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 8/10
Overall, I would recommend accepting the paper with minor revisions to address the weaknesses mentioned above.