This paper introduces a novel algorithm, Riemannian Relaxation (RR), for improving low-dimensional embeddings of data using distortion loss functions based on the push-forward Riemannian metric. The proposed method outperforms existing techniques such as Laplacian Eigenmaps, Maximum Variance Unfolding (MVU), Isomap, and Hessian Locally Linear Embedding (HLLE) on synthetic datasets with lower distortion.
The strengths of this paper include its clear organization, well-motivated introduction, and thorough experimental evaluation. The authors provide a detailed explanation of the algorithm, including the construction of the loss function and the optimization procedure. The experiments demonstrate the effectiveness of RR in obtaining low-distortion embeddings, particularly in cases where the intrinsic dimension of the data is not equal to the embedding dimension.
However, there are some weaknesses and areas for improvement. The loss function is non-convex, which may lead to difficulties in achieving global optimality and sensitivity to initialization. The authors should provide clearer explanations on how the loss functions measure deviation from isometry and quantify the deviation when the loss is not zero. Additionally, the computational expense of RR is higher than other methods, and its practical usefulness needs to be justified, such as through improved classification or regression results.
The paper also contains some typos and unclear notations, such as Eq (7)'s $\mathcal{L}_k$. The authors should ensure that the notation is consistent throughout the paper and provide clear explanations for any technical terms or concepts.
In terms of originality, the paper proposes a new approach to manifold learning that departs from existing methods. The use of the push-forward Riemannian metric and the iterative optimization procedure are novel contributions. However, the paper could benefit from a more detailed discussion of related work and how RR differs from existing methods.
The significance of the paper lies in its potential to improve the quality of low-dimensional embeddings, which is a crucial step in many machine learning and data analysis tasks. The authors demonstrate the effectiveness of RR in various experiments, including the visualization of the main SDSS galaxy sample in spectra space. However, further work is needed to fully explore the potential of RR and its applications in real-world problems.
Overall, I would recommend accepting this paper, but with some revisions to address the issues mentioned above. The authors should provide clearer explanations, improve the notation, and discuss the limitations and potential applications of RR in more detail.
Arguments pro acceptance:
* The paper proposes a novel approach to manifold learning that outperforms existing methods.
* The experiments demonstrate the effectiveness of RR in obtaining low-distortion embeddings.
* The paper has a clear organization and well-motivated introduction.
Arguments con acceptance:
* The loss function is non-convex, which may lead to difficulties in achieving global optimality.
* The computational expense of RR is higher than other methods.
* The paper contains some typos and unclear notations.
* The discussion of related work and the significance of the paper could be improved.