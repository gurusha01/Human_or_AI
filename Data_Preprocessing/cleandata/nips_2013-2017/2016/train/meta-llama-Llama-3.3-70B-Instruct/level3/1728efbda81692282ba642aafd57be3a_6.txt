This paper proposes a novel approach to training unitary recurrent neural networks (uRNNs) by optimizing the recurrence matrix over the Stiefel manifold of unitary matrices. The authors provide a theoretical argument to determine when a unitary parameterization has restricted capacity, and they show that a previously proposed parameterization has restricted capacity for hidden state dimensions greater than 7. The paper also presents a method for directly optimizing a full-capacity unitary matrix, which leads to significantly improved performance over restricted-capacity uRNNs.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The theoretical derivations are sound, and the experimental results demonstrate the effectiveness of the proposed method. The paper also provides a thorough comparison with other methods, including LSTMs and restricted-capacity uRNNs.
One of the strengths of the paper is its ability to address the vanishing and exploding gradient problems that are common in recurrent neural networks. The use of unitary recurrence matrices provides a stable and efficient way to train recurrent networks, and the proposed method for optimizing these matrices is both simple and effective.
However, there are some limitations to the paper. The authors note that the restricted-capacity parameterization may still be effective for certain tasks, and that further exploration of the trade-off between hidden state dimension and capacity of unitary parameterizations is necessary. Additionally, the paper could benefit from more detailed analysis of the computational complexity of the proposed method and its scalability to larger models and datasets.
In terms of originality, the paper presents a novel approach to training uRNNs, and the theoretical argument for determining restricted capacity is a significant contribution. The paper also provides a thorough review of related work and clearly demonstrates the advantages of the proposed method over existing approaches.
The significance of the paper lies in its ability to provide a stable and efficient way to train recurrent neural networks, which has the potential to impact a wide range of applications, including speech recognition, natural language processing, and time-series prediction. The paper also provides a foundation for further research into the use of unitary recurrence matrices and the optimization of these matrices over the Stiefel manifold.
Overall, I would recommend accepting this paper for publication. The paper is well-written, and the authors provide a clear and concise explanation of their approach. The theoretical derivations are sound, and the experimental results demonstrate the effectiveness of the proposed method. The paper has the potential to make a significant impact in the field of recurrent neural networks and is a valuable contribution to the literature.
Arguments pro acceptance:
* The paper presents a novel approach to training uRNNs that addresses the vanishing and exploding gradient problems.
* The theoretical argument for determining restricted capacity is a significant contribution.
* The paper provides a thorough comparison with other methods, including LSTMs and restricted-capacity uRNNs.
* The experimental results demonstrate the effectiveness of the proposed method.
Arguments con acceptance:
* The paper could benefit from more detailed analysis of the computational complexity of the proposed method and its scalability to larger models and datasets.
* The authors note that the restricted-capacity parameterization may still be effective for certain tasks, and that further exploration of the trade-off between hidden state dimension and capacity of unitary parameterizations is necessary.