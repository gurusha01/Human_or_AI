This paper proposes a novel method called Sparse Access Memory (SAM), which combines the strengths of neural networks and external memory to efficiently learn algorithmic solutions to complex tasks. The authors introduce a sparse read and write scheme that allows the model to access and modify a large external memory in a computationally efficient manner. The key innovation of SAM is the use of a sparse memory management scheme that tracks memory usage and finds unused blocks of memory for recording new information.
The paper presents a thorough evaluation of SAM on a range of tasks, including synthetic tasks, question answering, and one-shot learning on the Omniglot dataset. The results demonstrate that SAM is able to learn with comparable efficiency to dense models, and in some cases, even outperforms them. The authors also show that SAM is able to scale to tasks that require storing a large amount of information over long sequences, and that it can generalize to sequences that far exceed those observed during training.
One of the strengths of the paper is its clarity and organization. The authors provide a clear and concise introduction to the background and motivation of the work, and the technical sections are well-organized and easy to follow. The paper also includes a thorough discussion of the related work and the potential applications of SAM.
However, there are some areas where the paper could be improved. One concern is the lack of quantitative evaluation to judge the similarity between generated shapes and nearest neighbors from the training set. The authors provide some visualizations of the results, but it would be helpful to have a more rigorous evaluation of the model's performance. Additionally, the paper could benefit from more discussion of the potential limitations and challenges of the approach, as well as potential avenues for future work.
In terms of originality, the paper presents a novel combination of existing techniques, including neural networks, external memory, and sparse memory management. While the individual components of the model are not new, the way they are combined and applied to a range of tasks is innovative and contributes to the state of the art in the field.
Overall, the paper is well-written, clearly organized, and presents a significant contribution to the field of neural networks and external memory. The results are impressive, and the authors provide a thorough evaluation of the model's performance. With some minor revisions to address the concerns mentioned above, the paper has the potential to be a strong contribution to the conference.
Arguments for acceptance:
* The paper presents a novel and innovative approach to neural networks and external memory.
* The results are impressive and demonstrate the effectiveness of the model on a range of tasks.
* The paper is well-written and clearly organized, making it easy to follow and understand.
Arguments against acceptance:
* The paper could benefit from more quantitative evaluation of the model's performance.
* The discussion of potential limitations and challenges of the approach is limited.
* The paper could benefit from more discussion of potential avenues for future work. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10