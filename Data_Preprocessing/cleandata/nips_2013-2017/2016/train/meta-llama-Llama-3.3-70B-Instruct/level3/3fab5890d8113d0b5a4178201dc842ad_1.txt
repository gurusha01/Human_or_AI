This paper proposes a novel neural memory architecture called Sparse Access Memory (SAM), which enables efficient training of neural networks with large external memories. The key innovation of SAM is the use of fast approximate-nearest-neighbors structures and arg-top-K operations to compute attention weights, allowing for training on larger problems. While the novelty of the work is considered thin, as similar methods have been used in memory network papers, its application to writeable memories is unique and worth reporting.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a good background on the limitations of traditional recurrent neural networks and the need for external memory mechanisms. The authors also provide a thorough review of related work, including Neural Turing Machines and Memory Networks.
The proposed SAM architecture is described in detail, including the sparse read and write operations, the use of approximate nearest neighbor indexes, and the efficient backpropagation through time algorithm. The authors also provide a thorough analysis of the time and space complexity of SAM, showing that it is asymptotically optimal in both respects.
The experimental results are impressive, demonstrating that SAM can learn with comparable efficiency to dense models on a range of synthetic and natural tasks, including one-shot Omniglot character recognition and question answering on the Babi tasks. The authors also show that SAM can scale to tasks requiring 100,000s of time steps and memories, making it a promising approach for applications that require large amounts of memory.
However, there are some limitations and potential failure cases of the model that are not fully addressed in the paper. For example, the authors do not provide a detailed analysis of the argmax operation and the choice of K, which could have a significant impact on the performance of the model. Additionally, the authors do not discuss the potential limitations of using approximate nearest neighbor indexes, such as the trade-off between accuracy and efficiency.
To improve the paper, I suggest that the authors provide more detailed descriptions of the tasks and commit to releasing the code for the experiments. This would allow other researchers to reproduce the results and build on the work. I also suggest that the authors analyze the limitations and potential failure cases of the model, including the argmax operation and the choice of K, to add value to the paper. Finally, I recommend removing section 3.6 and rephrasing the description of the Omniglot data to avoid calling it "non-synthetic" or "real-world", as this terminology is not accurate.
Overall, I recommend accepting the paper, as it presents a significant contribution to the field of neural memory architectures. The paper is well-written, clearly organized, and provides a thorough analysis of the proposed architecture and its experimental results. With some minor revisions to address the limitations and potential failure cases of the model, the paper has the potential to make a significant impact in the field. 
Arguments pro acceptance:
- The paper presents a novel and efficient neural memory architecture that can be used for a wide range of applications.
- The experimental results are impressive, demonstrating that SAM can learn with comparable efficiency to dense models on a range of synthetic and natural tasks.
- The paper is well-written and clearly organized, making it easy to follow the authors' arguments.
Arguments con acceptance:
- The novelty of the work is considered thin, as similar methods have been used in memory network papers.
- The paper does not provide a detailed analysis of the argmax operation and the choice of K, which could have a significant impact on the performance of the model.
- The authors do not discuss the potential limitations of using approximate nearest neighbor indexes, such as the trade-off between accuracy and efficiency.