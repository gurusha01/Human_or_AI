This paper proposes a novel batch Bayesian optimization algorithm, the parallel knowledge gradient method (q-KG), which is derived from a decision-theoretical perspective. The algorithm is designed to optimize the expected incremental value of a measurement in the parallel setting, where multiple points can be evaluated simultaneously. The authors provide a computationally efficient approach to maximize the q-KG acquisition function using infinitesimal perturbation analysis (IPA) and demonstrate the effectiveness of the algorithm on both synthetic functions and practical machine learning problems.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed method. The introduction provides a good background on Bayesian optimization and motivates the need for a parallel batch method. The related work section is comprehensive, covering various existing batch Bayesian optimization algorithms and highlighting the differences between them and the proposed q-KG method.
The technical contributions of the paper are significant, as the authors develop a new acquisition function that considers the batch of points collectively and provide an efficient way to compute its gradient using IPA. The experimental results demonstrate the effectiveness of the q-KG algorithm in both noise-free and noisy settings, outperforming or being competitive with state-of-the-art benchmark algorithms.
However, there are some areas that could be improved. The paper assumes that the feasible domain A is finite or can be discretized, which may not always be the case in practice. The authors could provide more discussion on how to handle infinite or continuous domains. Additionally, the computational complexity of the q-KG algorithm could be further analyzed, as the authors only provide a brief discussion on the computational efficiency of the IPA method.
In terms of the conference guidelines, the paper meets most of the criteria. The paper is technically sound, with a clear and well-organized presentation of the proposed method and its theoretical foundations. The experimental results are convincing, demonstrating the effectiveness of the q-KG algorithm in various settings. The paper is also well-written, making it easy to understand the authors' arguments and follow their presentation.
However, the paper could benefit from more discussion on the significance and impact of the proposed method. While the authors demonstrate the effectiveness of the q-KG algorithm in various settings, they could provide more context on how this method can be used in practice and what kind of problems it can solve. Additionally, the paper could benefit from more comparison with other existing methods, particularly in the noisy setting, where the q-KG algorithm seems to have a significant advantage.
Overall, the paper is well-written and presents a significant technical contribution to the field of Bayesian optimization. With some minor revisions to address the areas mentioned above, the paper has the potential to make a strong impact in the field.
Arguments pro acceptance:
* The paper proposes a novel batch Bayesian optimization algorithm that is derived from a decision-theoretical perspective.
* The algorithm is computationally efficient and can handle both noise-free and noisy settings.
* The experimental results demonstrate the effectiveness of the q-KG algorithm in various settings.
* The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed method.
Arguments con acceptance:
* The paper assumes that the feasible domain A is finite or can be discretized, which may not always be the case in practice.
* The computational complexity of the q-KG algorithm could be further analyzed.
* The paper could benefit from more discussion on the significance and impact of the proposed method.
* The paper could benefit from more comparison with other existing methods, particularly in the noisy setting.