This paper presents a novel approach to dropout, a widely used technique for preventing overfitting in deep neural networks. The authors propose a multinomial dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features or neurons. The paper provides a rigorous analysis of the risk bound for stochastic optimization with multinomial dropout and demonstrates that a distribution-dependent dropout leads to a smaller expected risk and faster convergence.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a good background on dropout and its applications, and the related work section gives a comprehensive overview of existing research on dropout. The authors' contribution is significant, as they provide a new perspective on dropout and demonstrate its effectiveness in both shallow and deep learning.
The theoretical analysis is sound, and the authors provide a clear explanation of the risk bound and its dependence on the sampling probabilities. The experimental results are also convincing, showing that the proposed dropouts achieve faster convergence and smaller testing errors compared to the standard dropout.
However, there are some areas that could be improved. The paper could benefit from additional diagrams and explanations to improve readability, such as a small diagram explaining the regression problem in 1D or 2D. The transitions between general dual pairs and specialized losses could be made clearer, with more intuitive explanations and statements of the required intuitions. The introduction of notations and symbols could be improved, with definitions and explanations provided before they are used, to make the first reading of the paper easier.
Additionally, the paper could benefit from more explicit connections to previous contributions, such as the work of Tibshirani, and more intuition on the Lagrange multipliers and their geometric interpretation. Minor errors and typos, such as punctuation and formatting issues, should be corrected to improve the overall quality of the paper.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, and the organization is logical and easy to follow. The approach is novel, and the paper provides a new perspective on dropout. The results are significant, and the paper demonstrates the effectiveness of the proposed dropouts in improving convergence and reducing testing errors.
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of deep learning and dropout. With some minor revisions to address the areas mentioned above, the paper has the potential to be a high-quality contribution to the conference. 
Arguments pro acceptance:
- The paper provides a novel approach to dropout, which is a significant contribution to the field of deep learning.
- The theoretical analysis is sound, and the authors provide a clear explanation of the risk bound and its dependence on the sampling probabilities.
- The experimental results are convincing, showing that the proposed dropouts achieve faster convergence and smaller testing errors compared to the standard dropout.
- The paper is well-written and clearly organized, making it easy to follow the authors' arguments.
Arguments con acceptance:
- The paper could benefit from additional diagrams and explanations to improve readability.
- The transitions between general dual pairs and specialized losses could be made clearer, with more intuitive explanations and statements of the required intuitions.
- The introduction of notations and symbols could be improved, with definitions and explanations provided before they are used, to make the first reading of the paper easier.
- Minor errors and typos, such as punctuation and formatting issues, should be corrected to improve the overall quality of the paper.