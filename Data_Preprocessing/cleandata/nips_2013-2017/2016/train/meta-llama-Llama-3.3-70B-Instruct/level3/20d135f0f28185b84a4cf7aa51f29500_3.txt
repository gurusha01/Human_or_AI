This paper proposes a novel approach to the multiple choice learning problem for deep networks, introducing a winner-takes-the-gradient layer in back propagation. The method, called Stochastic Multiple Choice Learning (sMCL), is shown to perform well across a wide range of problems, including image classification, image segmentation, and captioning, through experimental results. The paper's strength lies in its simplicity and the importance of the problem of Multiple Choice Learning (MCL) for deep learning to the NIPS community.
The authors demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles, outperforming classical ensembles and other strong baselines, including the 5x slower MCL procedure. The analysis shows that exactly the same algorithm (sMCL) automatically generates specializations among ensemble members along different task-specific dimensions. sMCL is simple to implement, agnostic to both architecture and loss function, parameter-free, and simply involves introducing one new sMCL layer into existing ensemble architectures.
However, a significant weakness of the paper is that it motivates "diversity" but does not enforce it explicitly in the model. The paper's proposed solution is considered an incremental step, building upon the relaxation proposed by Guzman et al. The reviewer suggests several minor improvements, including re-writing the first sentence of the abstract, toning down the emphasis on diversity, and correcting grammatical errors.
In terms of quality, the paper is technically sound, with claims well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader. The originality of the paper lies in its novel combination of familiar techniques, and it is clear how this work differs from previous contributions. The significance of the paper is evident, as it addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.
Arguments for acceptance include:
* The paper proposes a novel and effective approach to the multiple choice learning problem for deep networks.
* The method is simple to implement and agnostic to both architecture and loss function.
* The experimental results demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles.
* The paper is well-written, well-organized, and adequately informs the reader.
Arguments against acceptance include:
* The paper does not enforce diversity explicitly in the model.
* The proposed solution is considered an incremental step, building upon previous work.
* The paper could benefit from minor improvements, such as re-writing the first sentence of the abstract and toning down the emphasis on diversity.
Overall, the paper is a good scientific contribution to the field, and its strengths outweigh its weaknesses. With some minor revisions, the paper has the potential to make a significant impact on the NIPS community.