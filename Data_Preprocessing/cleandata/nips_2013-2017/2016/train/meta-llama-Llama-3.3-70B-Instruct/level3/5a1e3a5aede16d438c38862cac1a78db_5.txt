This paper proposes a novel dropout technique, called multinomial dropout, which samples features or neurons according to a multinomial distribution with different probabilities for different features/neurons. The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout and demonstrate that a distribution-dependent dropout leads to a faster convergence and a smaller generalization error. They also propose an efficient evolutional dropout for deep learning, which adapts the sampling probabilities to the evolving distributions of layers' outputs.
The paper is well-written and clearly explains the problem setup, main results, and experiments. The authors provide a thorough analysis of the proposed dropout techniques and compare them with standard dropout and batch normalization. The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts in improving convergence and reducing testing error.
The strengths of the paper include:
* A novel and well-motivated dropout technique that takes into account the importance of different features/neurons
* A rigorous analysis of the risk bound of shallow learning with multinomial dropout
* An efficient evolutional dropout for deep learning that adapts to the evolving distributions of layers' outputs
* Experimental results that demonstrate the effectiveness of the proposed dropouts
The weaknesses of the paper include:
* Minor typos, such as "explicitely" on line 62
* Lack of clarity on the noise model used in simulations, making it unclear how the three methods perform differently under various noise models
* Theoretical support recovery guarantees are questioned due to simulations showing l2 loss superiority for support recovery, with no practically interesting cases provided
* Theorem 1 is limited to small noise settings, failing to provide insights for larger noise levels
Overall, the paper is well-written and provides a significant contribution to the field of deep learning. The proposed dropout techniques have the potential to improve the convergence and generalization of deep neural networks. However, some minor issues and limitations need to be addressed to further strengthen the paper.
Arguments pro acceptance:
* The paper proposes a novel and well-motivated dropout technique that takes into account the importance of different features/neurons
* The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout
* The experimental results demonstrate the effectiveness of the proposed dropouts in improving convergence and reducing testing error
Arguments con acceptance:
* Minor typos and lack of clarity on the noise model used in simulations
* Theoretical support recovery guarantees are questioned due to simulations showing l2 loss superiority for support recovery
* Theorem 1 is limited to small noise settings, failing to provide insights for larger noise levels
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 8/10
Recommendation: Accept with minor revisions to address the minor issues and limitations mentioned above.