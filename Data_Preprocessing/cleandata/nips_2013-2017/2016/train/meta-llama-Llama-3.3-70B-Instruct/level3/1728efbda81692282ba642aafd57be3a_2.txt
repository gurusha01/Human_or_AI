This paper proposes a novel approach to training unitary recurrent neural networks (uRNNs) by optimizing the recurrence matrix over the Stiefel manifold of unitary matrices. The authors provide a theoretical argument to determine when a unitary parameterization has restricted capacity, and they show that a previously proposed parameterization has restricted capacity for hidden state dimensions greater than 7. The paper then introduces a method for directly optimizing a full-capacity unitary matrix, which is demonstrated to achieve superior performance to restricted-capacity uRNNs and LSTMs on several tasks, including system identification, long-term memorization, speech prediction, and image classification.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and contributions. The theoretical analysis is sound, and the experimental results are convincing, demonstrating the effectiveness of the proposed approach. The paper also provides a thorough review of related work, including previous attempts to address the vanishing and exploding gradient problems in recurrent neural networks.
One of the strengths of the paper is its ability to balance theoretical rigor with practical applications. The authors provide a clear and concise explanation of the theoretical background, and they demonstrate the practical implications of their approach through a range of experiments. The paper also raises interesting questions about the trade-off between hidden state dimension and capacity of unitary parameterizations, which could be explored in future work.
However, there are some limitations to the paper. The authors acknowledge that the restricted-capacity uRNNs can still achieve impressive performance, even if they do not represent all unitary matrices. This suggests that further exploration of the potential trade-off between hidden state dimension and capacity of unitary parameterizations is necessary. Additionally, the paper could benefit from a more detailed comparison with other approaches to addressing the vanishing and exploding gradient problems, such as gradient clipping and orthogonal initialization.
Overall, I would argue in favor of accepting this paper, as it makes a significant contribution to the field of recurrent neural networks and provides a novel approach to training uRNNs. The paper is well-written, clearly organized, and demonstrates the effectiveness of the proposed approach through a range of experiments.
Arguments pro acceptance:
* The paper proposes a novel approach to training uRNNs, which is demonstrated to achieve superior performance to restricted-capacity uRNNs and LSTMs.
* The paper provides a thorough theoretical analysis, including a clear and concise explanation of the theoretical background.
* The experimental results are convincing, demonstrating the effectiveness of the proposed approach on several tasks.
* The paper raises interesting questions about the trade-off between hidden state dimension and capacity of unitary parameterizations, which could be explored in future work.
Arguments con acceptance:
* The paper could benefit from a more detailed comparison with other approaches to addressing the vanishing and exploding gradient problems.
* The authors acknowledge that the restricted-capacity uRNNs can still achieve impressive performance, even if they do not represent all unitary matrices, which suggests that further exploration of the potential trade-off between hidden state dimension and capacity of unitary parameterizations is necessary.