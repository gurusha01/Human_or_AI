This paper proposes a novel dropout technique, called multinomial dropout, which samples features or neurons according to a multinomial distribution with different probabilities for different features/neurons. The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout and demonstrate that a distribution-dependent dropout leads to a smaller expected risk, resulting in faster convergence and smaller generalization error.
The paper is well-written, and the presentation is excellent. The authors provide a clear and concise introduction to the background and motivation of the work, and the related work section is thorough and well-organized. The main results are presented in a clear and easy-to-follow manner, and the experimental results are convincing and well-presented.
The strengths of the paper include the novelty of the proposed dropout technique, the rigorous analysis of the risk bound, and the convincing experimental results. The authors also provide a clear and concise explanation of the advantages of the proposed technique over standard dropout and batch normalization.
The weaknesses of the paper are minor. There are some minor typos and formatting issues, such as line 118, equation names in section 2, line 139, line 186, and Figure captions, which need to be corrected. Additionally, the authors could provide more discussion on the choice of hyperparameters, such as the value of k, and how it affects the performance of the proposed technique.
Overall, the paper is well-written, and the proposed technique is novel and promising. The experimental results are convincing, and the authors provide a clear and concise explanation of the advantages of the proposed technique. I would recommend accepting this paper for publication.
Arguments pro acceptance:
* The paper proposes a novel dropout technique that is theoretically grounded and experimentally verified.
* The authors provide a rigorous analysis of the risk bound of shallow learning with multinomial dropout.
* The experimental results are convincing and demonstrate the effectiveness of the proposed technique.
* The paper is well-written, and the presentation is excellent.
Arguments con acceptance:
* Minor typos and formatting issues need to be corrected.
* More discussion on the choice of hyperparameters could be provided.
* The paper could benefit from more comparisons with other related work.