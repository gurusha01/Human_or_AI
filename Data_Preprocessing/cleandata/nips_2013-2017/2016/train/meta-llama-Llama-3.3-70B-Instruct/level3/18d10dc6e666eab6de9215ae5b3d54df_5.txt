This paper proposes a novel batch Bayesian optimization algorithm, the parallel knowledge gradient method (q-KG), which is derived from a decision-theoretical perspective. The method is designed to optimize the expected incremental value of a measurement in the parallel setting, where multiple points can be evaluated simultaneously. The authors demonstrate the effectiveness of q-KG on both synthetic test functions and practical machine learning algorithms, showing that it outperforms or is competitive with state-of-the-art benchmark algorithms, especially in noisy settings.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed method. The introduction provides a good background on Bayesian optimization and motivates the need for a parallel batch method. The related work section is comprehensive, covering various batch Bayesian optimization algorithms and their limitations.
The technical contributions of the paper are significant, as the authors develop a novel acquisition function, q-KG, and provide an efficient strategy for computing its gradient using infinitesimal perturbation analysis (IPA). The experimental results demonstrate the effectiveness of q-KG in various settings, including noise-free and noisy problems.
However, there are some areas that require clarification or improvement. Firstly, the paper's contribution to the existing batch Knowledge Gradient (KG) method needs to be clarified, particularly in relation to previous papers. The authors should provide more discussion on how their method differs from previous KG methods and why it is more effective. Additionally, the comparison of q-KG with Expected Improvement (EI) in a noisy setting may be unfair, as EI assumes noiseless observations. The authors should address this issue and provide a more comprehensive comparison.
Furthermore, Section 5.2 of the paper requires more elaboration, as it is a key point of the proposed method. The authors should provide more details on how the gradient of q-KG is estimated using IPA and how this estimation is used in the optimization process. Finally, the abbreviation "IPA" is redefined in line 162, which may cause confusion and should be corrected.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is clearly written, well-organized, and adequately informs the reader. The originality of the paper is high, as it proposes a novel batch Bayesian optimization algorithm and provides a comprehensive evaluation of its effectiveness. The significance of the paper is also high, as it addresses a difficult problem in Bayesian optimization and provides a more effective solution than existing methods.
Overall, this paper is a significant contribution to the field of Bayesian optimization, and its results have the potential to impact various applications, including machine learning and optimization. With some minor revisions to address the issues mentioned above, this paper has the potential to be a strong candidate for acceptance.
Arguments pro acceptance:
* The paper proposes a novel batch Bayesian optimization algorithm, q-KG, which is derived from a decision-theoretical perspective.
* The method is shown to be effective in various settings, including noise-free and noisy problems.
* The paper provides a comprehensive evaluation of q-KG, including a comparison with state-of-the-art benchmark algorithms.
* The authors demonstrate the significance of their work, showing that q-KG can outperform existing methods in various applications.
Arguments con acceptance:
* The paper's contribution to the existing batch Knowledge Gradient (KG) method needs to be clarified.
* The comparison of q-KG with Expected Improvement (EI) in a noisy setting may be unfair.
* Section 5.2 of the paper requires more elaboration.
* The abbreviation "IPA" is redefined in line 162, which may cause confusion.