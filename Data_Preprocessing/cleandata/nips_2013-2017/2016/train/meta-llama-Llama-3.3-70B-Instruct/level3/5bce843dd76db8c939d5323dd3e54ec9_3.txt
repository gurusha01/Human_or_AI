This paper proposes a novel approach to training deep neural networks, called Stochastic Multiple Choice Learning (sMCL), which enables the network to produce multiple diverse solutions for a given input. The authors formulate the problem as a stochastic block gradient descent optimization, where the network is trained to minimize the oracle loss, which is the minimum loss among all possible solutions. The proposed approach is simple to implement, agnostic to both architecture and loss function, and parameter-free.
The paper presents a thorough analysis of the proposed method, including its strengths and weaknesses. The authors demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles on a wide range of tasks, including image classification, semantic segmentation, and image captioning. The results show that sMCL significantly outperforms classical ensembles and other strong baselines, including the 5x slower MCL procedure.
One of the key strengths of the paper is its ability to automatically generate specializations among ensemble members along different task-specific dimensions. The authors provide detailed analysis of the training and output behaviors of the resulting ensembles, demonstrating how ensemble member specialization and expertise emerge automatically when trained using sMCL.
However, there are some areas that could be improved. For example, the paper could benefit from a more detailed analysis of the computational efficiency of the proposed method, particularly in comparison to other ensemble learning methods. Additionally, the authors could provide more insight into the solution the model is learning on simple tasks, and how the time-sampling was performed in cases where sampling was "asynchronous".
The paper also mentions the potential for reduced computational load due to the gating alleviating the need to compute updates to the cell when k_t = 0, but this claim could be supported with empirical analysis in one of the experiments. Furthermore, the authors could explore the connection between the proposed method and reservoir computing, and cite some of the relevant work in this area.
Overall, the paper presents a novel and promising approach to training deep neural networks, and the results demonstrate its potential for improving the performance of deep ensembles on a wide range of tasks. With some additional analysis and experimentation, the paper could be even stronger.
Arguments pro acceptance:
* The paper proposes a novel and promising approach to training deep neural networks.
* The results demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles.
* The paper provides a thorough analysis of the proposed method, including its strengths and weaknesses.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the computational efficiency of the proposed method.
* The authors could provide more insight into the solution the model is learning on simple tasks.
* The paper could explore the connection between the proposed method and reservoir computing, and cite some of the relevant work in this area.