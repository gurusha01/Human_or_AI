This paper studies fast learning rates for heavy-tailed losses in machine learning. The authors introduce two new conditions: the existence and Lr-integrability of the envelope function, and the multi-scale Bernstein's condition, which generalizes the standard Bernstein's condition for unbounded losses. Under these assumptions, they prove that learning rates faster than O(n−1/2) can be obtained, and depending on the parameters, can be arbitrarily close to O(n−1). The authors then verify these assumptions for the problem of vector quantization by k-means clustering with heavy-tailed distributions and derive novel learning rates.
The paper is well-written and provides a clear overview of the main ideas and contributions. The authors relate their work to previous studies at NIPS and elsewhere, demonstrating a good understanding of the existing literature. The technical proofs are thorough and well-organized, making it easier to follow the arguments.
The strengths of the paper include:
* The introduction of the multi-scale Bernstein's condition, which provides a more general framework for studying fast learning rates with unbounded losses.
* The derivation of novel learning rates for k-means clustering with heavy-tailed distributions, which extends and complements existing results.
* The provision of a clear path to verify the multi-scale Bernstein's condition in practice.
The weaknesses of the paper include:
* The assumption of independent and identically distributed observations, which may not always hold in practice.
* The requirement of an integrable envelope function, which may be restrictive for some hypothesis classes.
* The lack of comparison with other robust estimators, such as those proposed in Brownlees et al. (2015) and Hsu and Sabato (2016).
Arguments for acceptance:
* The paper provides a significant contribution to the understanding of fast learning rates with heavy-tailed losses.
* The introduction of the multi-scale Bernstein's condition provides a new framework for studying fast learning rates with unbounded losses.
* The derivation of novel learning rates for k-means clustering with heavy-tailed distributions extends and complements existing results.
Arguments against acceptance:
* The assumptions may be too restrictive for some practical applications.
* The paper could benefit from a more detailed comparison with other robust estimators.
* The results may not be directly applicable to more complex hypothesis classes or non-iid observations.
Overall, I recommend accepting the paper, as it provides a significant contribution to the field and the strengths outweigh the weaknesses. However, the authors should consider addressing the weaknesses and providing more comparisons with other robust estimators in future work.