This paper presents a novel approach to efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations is much larger than the number of predictors. The authors show that in GLMs with random design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, they design an algorithm, called Scaled Least Squares (SLS), that achieves the same accuracy as the maximum likelihood estimator (MLE) through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of O(p).
The paper is well-written, and the authors provide a clear and concise introduction to the problem, related work, and their contributions. The theoretical results are sound, and the experiments demonstrate the effectiveness of the proposed algorithm. The authors also provide a thorough comparison with other existing methods, which shows that their algorithm outperforms them in terms of computation time.
The strengths of the paper include:
* The authors provide a novel and efficient algorithm for estimating GLM coefficients, which is particularly useful in large-scale settings.
* The theoretical results are sound and provide a clear understanding of the algorithm's performance.
* The experiments are well-designed and demonstrate the effectiveness of the proposed algorithm.
The weaknesses of the paper include:
* The algorithm's performance may depend on the choice of initialization, which is not thoroughly discussed in the paper.
* The authors assume that the predictors are sub-Gaussian, which may not always be the case in practice.
* The paper could benefit from more discussion on the potential applications and limitations of the proposed algorithm.
Overall, the paper is well-written, and the authors provide a significant contribution to the field of large-scale optimization. The proposed algorithm has the potential to be widely used in practice, and the theoretical results provide a clear understanding of its performance.
Arguments pro acceptance:
* The paper presents a novel and efficient algorithm for estimating GLM coefficients.
* The theoretical results are sound and provide a clear understanding of the algorithm's performance.
* The experiments demonstrate the effectiveness of the proposed algorithm.
Arguments con acceptance:
* The algorithm's performance may depend on the choice of initialization.
* The authors assume that the predictors are sub-Gaussian, which may not always be the case in practice.
* The paper could benefit from more discussion on the potential applications and limitations of the proposed algorithm.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, particularly the discussion on initialization and the assumption of sub-Gaussian predictors. Additionally, they should provide more discussion on the potential applications and limitations of the proposed algorithm.