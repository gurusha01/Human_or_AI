This paper presents a novel approach to training neural networks, called the nonlinear spectral method, which guarantees global optimality under certain conditions. The authors provide a thorough analysis of the method, including its convergence properties and experimental results on several UCI datasets.
The paper's main strength is its theoretical contribution, providing a novel and efficient method for training neural networks with a guarantee of global optimality. The authors demonstrate that their method outperforms stochastic gradient descent and is competitive with other state-of-the-art methods, such as kernel SVM and ReLU networks.
The paper is well-organized, and the authors provide a clear and concise explanation of their method, including the mathematical derivations and proofs. The experimental results are also well-presented, with a clear comparison to other methods and a discussion of the limitations and potential improvements.
One potential weakness of the paper is the restriction to non-negative weights and the requirement of a specific activation function. While the authors provide some justification for these restrictions, it is not clear how limiting they are in practice. Additionally, the authors note that their bounds on the spectral radius of the matrix A are likely not tight, which could limit the applicability of their method to larger networks.
In terms of the review criteria, the paper scores well on quality, clarity, and originality. The authors provide a thorough and well-motivated analysis of their method, and the experimental results are well-presented and convincing. The paper's significance is also high, as it provides a novel and efficient method for training neural networks with a guarantee of global optimality.
Arguments pro acceptance:
* The paper presents a novel and efficient method for training neural networks with a guarantee of global optimality.
* The authors provide a thorough and well-motivated analysis of their method, including mathematical derivations and proofs.
* The experimental results are well-presented and convincing, demonstrating the effectiveness of the method.
Arguments con acceptance:
* The method is restricted to non-negative weights and a specific activation function, which may limit its applicability in practice.
* The bounds on the spectral radius of the matrix A are likely not tight, which could limit the applicability of the method to larger networks.
Overall, I recommend accepting this paper, as it presents a significant and novel contribution to the field of neural networks, with a thorough and well-motivated analysis and convincing experimental results.