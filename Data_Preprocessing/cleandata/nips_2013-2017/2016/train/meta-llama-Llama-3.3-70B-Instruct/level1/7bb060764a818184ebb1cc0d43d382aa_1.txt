This paper proposes a novel approach to dropout, a widely used technique for preventing overfitting in deep neural networks. The authors introduce multinomial sampling for dropout, where features or neurons are sampled according to a multinomial distribution with different probabilities for different features/neurons. The paper provides a rigorous analysis of the risk bound for stochastic optimization with multinomial dropout and demonstrates that a distribution-dependent dropout leads to faster convergence and smaller generalization error.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed approach. The theoretical analysis is sound, and the experimental results on several benchmark datasets demonstrate the effectiveness of the proposed dropouts. The authors also provide a comparison with batch normalization, which shows that the evolutional dropout achieves comparable performance.
The strengths of the paper include:
* The proposal of a novel approach to dropout that adapts to the evolving distribution of layers' outputs
* A rigorous theoretical analysis of the risk bound for stochastic optimization with multinomial dropout
* Experimental results on several benchmark datasets that demonstrate the effectiveness of the proposed dropouts
* A comparison with batch normalization that shows the evolutional dropout achieves comparable performance
The weaknesses of the paper include:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The paper does not provide a clear explanation of how to choose the hyperparameters for the proposed dropouts
* The paper could benefit from more experimental results on larger datasets and more complex models
Overall, the paper is well-written, and the proposed approach is novel and effective. The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed dropouts. The paper has the potential to make a significant contribution to the field of deep learning.
Arguments pro acceptance:
* The paper proposes a novel approach to dropout that adapts to the evolving distribution of layers' outputs
* The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed dropouts
* The paper provides a comparison with batch normalization that shows the evolutional dropout achieves comparable performance
Arguments con acceptance:
* The paper assumes that the sampling probabilities are known, which may not be the case in practice
* The paper does not provide a clear explanation of how to choose the hyperparameters for the proposed dropouts
* The paper could benefit from more experimental results on larger datasets and more complex models
Rating: 8/10
Recommendation: Accept with minor revisions. The paper is well-written, and the proposed approach is novel and effective. The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed dropouts. However, the paper could benefit from more experimental results on larger datasets and more complex models, and a clear explanation of how to choose the hyperparameters for the proposed dropouts.