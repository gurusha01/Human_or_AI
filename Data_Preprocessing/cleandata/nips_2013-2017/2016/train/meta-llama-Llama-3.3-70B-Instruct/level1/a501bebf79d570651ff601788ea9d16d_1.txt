This paper proposes a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). The authors introduce a dynamic programming approach to balance the trade-off between caching of intermediate results and recomputation, allowing the algorithm to fit within almost any user-set memory budget while minimizing computational cost. The paper is well-written, and the authors provide a clear explanation of the problem, related work, and their proposed approach.
The paper's main contribution is the introduction of three different strategies for memory-efficient backpropagation: BPTT-HSM, BPTT-ISM, and BPTT-MSM. The authors provide a detailed analysis of each strategy, including their computational costs and memory usage. The results show that BPTT-MSM outperforms the other two strategies, and the authors demonstrate that their approach can save up to 95% of memory usage while using only 33% more time per iteration than the standard BPTT.
The paper is technically sound, and the authors provide a thorough analysis of their approach. The use of dynamic programming to find an optimal memory usage policy is a novel contribution, and the authors demonstrate its effectiveness through numerical experiments. The paper also provides a comparison with existing approaches, including Chen's √t algorithm, and shows that their approach can achieve significant memory savings while maintaining similar computational performance.
The paper's strengths include its clear explanation of the problem and proposed approach, its thorough analysis of the computational costs and memory usage of each strategy, and its comparison with existing approaches. The paper's weaknesses include the complexity of the dynamic programming approach, which may be difficult to implement in practice, and the lack of experimental results on real-world datasets.
Arguments for acceptance:
* The paper proposes a novel approach to reducing memory consumption of BPTT, which is a significant problem in training RNNs.
* The authors provide a thorough analysis of their approach, including computational costs and memory usage.
* The results show that the proposed approach can achieve significant memory savings while maintaining similar computational performance.
* The paper provides a comparison with existing approaches, including Chen's √t algorithm.
Arguments against acceptance:
* The dynamic programming approach may be complex to implement in practice.
* The paper lacks experimental results on real-world datasets.
* The authors could provide more discussion on the potential applications of their approach and its limitations.
Overall, I recommend accepting this paper, as it proposes a novel approach to reducing memory consumption of BPTT and provides a thorough analysis of its computational costs and memory usage. However, the authors should address the weaknesses mentioned above, including providing more experimental results and discussing the potential applications and limitations of their approach.