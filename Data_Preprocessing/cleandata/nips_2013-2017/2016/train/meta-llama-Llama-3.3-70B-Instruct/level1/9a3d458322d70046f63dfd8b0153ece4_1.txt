This paper presents a novel approach to learning safe policies in sequential decision-making under uncertainty. The authors propose a model-based approach that computes a safe policy by minimizing the robust regret with respect to a given baseline strategy. The key insight is to interleave the baseline policy with an improved policy based on the error at each state, allowing for a more systematic performance comparison.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach. The technical sections are thorough and well-organized, with a good balance between theoretical analysis and empirical evaluation. The authors provide a detailed analysis of the optimization problem, including its NP-hardness and the development of approximate algorithms.
The experimental evaluation is comprehensive, covering various domains, including a grid problem and an energy arbitrage problem. The results demonstrate the effectiveness of the proposed approach in computing safe policies that outperform the baseline policy, especially when the model error is large.
The paper's strengths include:
1. Originality: The paper presents a novel approach to learning safe policies, which differs from existing methods that either adopt a newly learned policy or do not make any improvement.
2. Technical soundness: The authors provide a thorough analysis of the optimization problem, including its NP-hardness and the development of approximate algorithms.
3. Empirical evaluation: The experimental evaluation is comprehensive, covering various domains and demonstrating the effectiveness of the proposed approach.
The paper's weaknesses include:
1. Complexity: The paper assumes a good understanding of Markov decision processes, robust optimization, and machine learning, which may make it challenging for non-experts to follow.
2. Scalability: The proposed approach may not be scalable to very large problems due to the NP-hardness of the optimization problem.
Arguments for acceptance:
1. The paper presents a novel and original approach to learning safe policies.
2. The technical analysis is thorough and well-organized.
3. The experimental evaluation is comprehensive and demonstrates the effectiveness of the proposed approach.
Arguments against acceptance:
1. The paper may be challenging for non-experts to follow due to its technical complexity.
2. The proposed approach may not be scalable to very large problems.
Overall, I recommend accepting this paper due to its originality, technical soundness, and comprehensive empirical evaluation. The paper makes a significant contribution to the field of sequential decision-making under uncertainty and has the potential to impact various applications, including online marketing, inventory control, and energy management.