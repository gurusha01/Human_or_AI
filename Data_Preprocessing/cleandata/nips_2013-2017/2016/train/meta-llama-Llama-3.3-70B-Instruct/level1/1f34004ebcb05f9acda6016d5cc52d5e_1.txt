This paper proposes two optimization methods, a gradient-free and a gradient-based method, to solve the non-convex loss-minimization problem of learning Supervised PageRank models. The authors provide theoretical guarantees on the convergence rate and complexity bounds for both methods. The gradient-free method is based on a random gradient-free optimization framework with inexact oracle, while the gradient-based method is based on an adaptive projected gradient algorithm with inexact first-order oracle.
The paper is well-written and provides a clear overview of the problem, the proposed methods, and the theoretical analysis. The authors also provide experimental results comparing their methods with the state-of-the-art gradient-based method, showing that their methods outperform the existing method in terms of the loss function.
The strengths of the paper include:
* The proposal of two new optimization methods for the non-convex loss-minimization problem of learning Supervised PageRank models.
* The provision of theoretical guarantees on the convergence rate and complexity bounds for both methods.
* The experimental results showing that the proposed methods outperform the state-of-the-art method.
The weaknesses of the paper include:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution, which may not always be the case.
* The paper does not provide a comparison with other optimization methods, such as stochastic gradient descent or quasi-Newton methods.
* The experimental results are limited to a specific dataset and may not generalize to other datasets.
Overall, the paper is well-written and provides a significant contribution to the field of optimization and machine learning. The proposed methods have the potential to improve the performance of search engines and other applications that rely on Supervised PageRank models.
Arguments pro acceptance:
* The paper proposes two new optimization methods with theoretical guarantees on convergence rate and complexity bounds.
* The experimental results show that the proposed methods outperform the state-of-the-art method.
* The paper is well-written and provides a clear overview of the problem and the proposed methods.
Arguments con acceptance:
* The paper assumes that the loss function is convex in a small vicinity of the optimal solution, which may not always be the case.
* The paper does not provide a comparison with other optimization methods.
* The experimental results are limited to a specific dataset and may not generalize to other datasets.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Overall score: 8.2/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses of the paper, such as providing a comparison with other optimization methods and discussing the limitations of the experimental results. Additionally, the authors should consider providing more details on the implementation of the proposed methods and the computational resources required.