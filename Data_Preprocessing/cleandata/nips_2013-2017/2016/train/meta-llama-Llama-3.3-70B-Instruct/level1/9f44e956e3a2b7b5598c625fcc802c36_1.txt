This paper proposes a novel approach to improving the performance of Recurrent Neural Networks (RNNs) by introducing a mechanism called "fast weights" that allows each new state of the hidden units to be attracted towards recent hidden states in proportion to their scalar products with the current state. The authors argue that this form of attention to the recent past is biologically plausible and can be used to implement an associative memory for the recent past, freeing up the states of neurons to apply knowledge recursively.
The paper relates to previous work on RNNs, Long Short-Term Memory (LSTM) networks, and attention mechanisms. The authors cite several papers on these topics, including [Hochreiter and Schmidhuber, 1997], [Graves et al., 2014], and [Bahdanau et al., 2015]. The paper also draws on ideas from computational neuroscience and cognitive science, citing papers on short-term synaptic plasticity and working memory.
The strengths of the paper include its clear and well-organized presentation, its thorough experimental evaluation, and its interesting implications for computational neuroscience and cognitive science. The authors provide a detailed analysis of the benefits and limitations of their approach, and they demonstrate its effectiveness on a range of tasks, including associative retrieval, visual attention, and reinforcement learning.
The weaknesses of the paper include its reliance on a simple outer-product storage rule, which may not be optimal for all tasks, and its lack of comparison to other attention mechanisms, such as those used in neural machine translation models. Additionally, the paper could benefit from a more detailed analysis of the computational costs and scalability of the fast weights approach.
Arguments pro acceptance:
* The paper proposes a novel and biologically plausible approach to improving RNN performance.
* The authors provide a thorough experimental evaluation of their approach on a range of tasks.
* The paper has interesting implications for computational neuroscience and cognitive science.
Arguments con acceptance:
* The paper relies on a simple outer-product storage rule, which may not be optimal for all tasks.
* The paper lacks comparison to other attention mechanisms, such as those used in neural machine translation models.
* The paper could benefit from a more detailed analysis of the computational costs and scalability of the fast weights approach.
Overall, I believe that this paper is a good scientific contribution to the field of machine learning, and I recommend acceptance. The paper's strengths outweigh its weaknesses, and its novel approach and thorough experimental evaluation make it a valuable contribution to the field.