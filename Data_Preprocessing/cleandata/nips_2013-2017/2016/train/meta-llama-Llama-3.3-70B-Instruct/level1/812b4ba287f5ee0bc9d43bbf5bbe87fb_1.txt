This paper proposes a novel Tree-structured Reinforcement Learning (Tree-RL) approach for object localization, which incorporates global interdependency between objects into the search process. The approach learns multiple searching policies through maximizing the long-term reward that reflects localization accuracies over all objects. The Tree-RL method uses a tree-structured search scheme, allowing the agent to travel along multiple near-optimal paths to discover multiple objects.
The paper is well-written and clearly explains the proposed approach, including the Markov decision process, reward function, and deep Q-learning algorithm. The experiments on PASCAL VOC 2007 and 2012 demonstrate the effectiveness of Tree-RL, achieving comparable recall to state-of-the-art object proposal algorithms with fewer proposals and higher localization accuracy.
The strengths of the paper include:
* The proposed Tree-RL approach is novel and addresses the limitation of existing object proposal algorithms that ignore global interdependency between objects.
* The paper provides a clear and detailed explanation of the approach, including the mathematical formulation and algorithmic details.
* The experiments demonstrate the effectiveness of Tree-RL, and the results are compared to state-of-the-art methods.
The weaknesses of the paper include:
* The paper assumes that the objects are non-overlapping, which may not be the case in real-world scenarios.
* The reward function is designed to encourage the agent to localize objects with high IoU, but it may not be optimal for objects with low IoU.
* The paper does not provide a detailed analysis of the computational complexity of the proposed approach.
Arguments for acceptance:
* The paper proposes a novel and effective approach for object localization that addresses the limitation of existing methods.
* The experiments demonstrate the effectiveness of Tree-RL, and the results are comparable to state-of-the-art methods.
* The paper provides a clear and detailed explanation of the approach, making it easy to understand and implement.
Arguments against acceptance:
* The paper assumes that the objects are non-overlapping, which may not be the case in real-world scenarios.
* The reward function may not be optimal for objects with low IoU.
* The paper does not provide a detailed analysis of the computational complexity of the proposed approach.
Overall, the paper is well-written and proposes a novel and effective approach for object localization. The experiments demonstrate the effectiveness of Tree-RL, and the results are comparable to state-of-the-art methods. However, the paper has some limitations, such as assuming non-overlapping objects and not providing a detailed analysis of computational complexity. Therefore, I would recommend accepting the paper with minor revisions to address these limitations.