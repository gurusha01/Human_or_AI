This paper presents a significant contribution to the field of online learning and game theory, improving upon recent work by Syrgkanis et al. [28] in several ways. The authors introduce the concept of Low Approximate Regret, a property that is ubiquitous among learning algorithms, including the vanilla Hedge algorithm. This property allows for fast convergence to approximate optimality in a large class of repeated games, including smooth games and dynamic population games.
The paper's main strengths are:
1. Improved convergence rate: The authors achieve a faster convergence rate of O(n/T) compared to the O(n2/T) rate in [28], with only an arbitrarily small loss in the approximation.
2. Realized feedback: The authors show that convergence occurs with high probability, even when players only receive realized feedback, which is more realistic than the expected feedback assumption in [28].
3. Bandit feedback: The authors extend their results to the bandit setting, where players only observe the cost of their own realized actions, and propose a new algorithm that achieves fast convergence.
4. Dynamic population games: The authors apply their framework to dynamic population games, where players enter and leave the game over time, and show that their results hold even in this setting.
The paper's weaknesses are:
1. Technical complexity: The paper assumes a high level of technical expertise in online learning and game theory, which may make it challenging for non-experts to follow.
2. Limited experimental evaluation: The paper lacks experimental evaluations to demonstrate the practical effectiveness of the proposed algorithms.
Arguments for acceptance:
1. Significant improvement over previous work: The paper improves upon recent work in several ways, making it a significant contribution to the field.
2. Strong theoretical foundations: The paper provides a solid theoretical foundation for the proposed algorithms, including proofs and analyses.
3. Broad applicability: The paper's results apply to a wide range of settings, including smooth games, dynamic population games, and bandit feedback.
Arguments against acceptance:
1. Technical complexity: The paper's technical complexity may limit its accessibility to a broader audience.
2. Lack of experimental evaluation: The paper's lack of experimental evaluations may raise concerns about the practical effectiveness of the proposed algorithms.
Overall, I recommend accepting this paper due to its significant contributions to the field, strong theoretical foundations, and broad applicability. However, I suggest that the authors consider adding experimental evaluations to demonstrate the practical effectiveness of their algorithms and make the paper more accessible to a broader audience.