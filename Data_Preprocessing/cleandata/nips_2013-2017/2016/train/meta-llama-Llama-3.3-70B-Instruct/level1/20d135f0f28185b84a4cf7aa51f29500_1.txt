This paper proposes a novel approach to training deep neural networks, called Stochastic Multiple Choice Learning (sMCL), which enables the production of multiple diverse solutions for a given input. The authors formulate the task of training an ensemble of deep networks as a stochastic block gradient descent optimization problem, where the goal is to minimize the oracle loss, which is the loss achieved by the best predictor in the ensemble.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of multiple choice learning and its importance in various applications. The related work section is thorough, and the authors provide a good overview of existing methods for training ensembles and generating multiple solutions.
The technical contribution of the paper is significant, and the authors propose a novel stochastic gradient descent-based approach to minimize the oracle loss. The approach is simple to implement, agnostic to both architecture and loss function, and parameter-free. The authors also provide a detailed analysis of the training and output behaviors of the resulting ensembles, demonstrating how ensemble member specialization and expertise emerge automatically when trained using sMCL.
The experimental results are impressive, and the authors demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles on a wide range of tasks, including image classification, semantic segmentation, and image captioning. The results show that sMCL significantly outperforms classical ensembles and other strong baselines, including the 5x slower MCL procedure.
The strengths of the paper include:
* The proposal of a novel and effective approach to training deep neural networks for multiple choice learning
* The thorough analysis of the training and output behaviors of the resulting ensembles
* The impressive experimental results demonstrating the broad applicability and efficacy of sMCL
The weaknesses of the paper include:
* The lack of a detailed comparison with other existing methods for generating multiple solutions, such as probabilistic structured-output models
* The limited analysis of the computational complexity of the proposed approach, which may be important for large-scale applications
Overall, the paper is well-written, and the authors provide a significant technical contribution to the field of deep learning. The proposed approach has the potential to be widely applicable and effective in various applications, and the experimental results demonstrate its efficacy.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to training deep neural networks for multiple choice learning
* The experimental results demonstrate the broad applicability and efficacy of sMCL
* The approach is simple to implement, agnostic to both architecture and loss function, and parameter-free
Arguments con acceptance:
* The lack of a detailed comparison with other existing methods for generating multiple solutions
* The limited analysis of the computational complexity of the proposed approach
Recommendation: Accept. The paper provides a significant technical contribution to the field of deep learning, and the proposed approach has the potential to be widely applicable and effective in various applications. The experimental results demonstrate its efficacy, and the approach is simple to implement and parameter-free.