The paper introduces a "multi-fidelity" version of the multi-armed bandit problem in which there are K arms, each of which can be played at M different levels of "fidelity". Lower fidelity is less costly, but leads to potentially greater bias in the estimate of the arm's reward. (Pulling an arm at fidelity m draws a sample from a distribution whose expected value deviates from the arm's true expected reward by an amount whose upper bound depends on the fidelity.) The paper analyzes the natural UCB-based algorithm for this problem, where the width of the confidential is increased to compensate for the uncertainty stemming from the bias of sampling at low fidelity. The analysis of the algorithm is summarized by theorems containing messy regret bounds that are hard to interpret; the paper argues that this regret bound will often be significantly better than the regret bound attained by always playing arms at the highest fidelity and using the standard UCB algorithm to select which arm to play. A brief section near the end of the paper presents simulation results to substantiate this claim. I was initially puzzled as to why the paper chooses to model lower fidelity as introducing more bias, rather than introducing more variance. (The latter interpretation of "low fidelity" also seems well motivated to me.) The motivating examples given in the paper (e.g. algorithm selection for machine learning problems) convinced me that modeling low fidelity as greater bias is well motivated, although I'm still not convinced by one of the motivations (namely, the contention that displaying an ad for a shorter time interval yields a biased estimate of the ad's effectiveness over longer intervals) and in the example of algorithm selection for machine learning, it's not clear why minimizing regret would be the goal of an algorithm designer in that application. So to sum up, I thought that there was a disappointingly weak connection between the model in this paper and the motivating applications, but not so weak as to constitute a disqualifying flaw. The main innovation in the paper seems to be the formulation of the multi-fidelity multi-armed bandit problem itself. The algorithm is a fairly typical application of the "optimism in the face of uncertainty" principle. The performance guarantee, as noted earlier in this review, strikes me as disappointingly hard to interpret. The simulation results on synthetic problems are hard for me to assess: Appendix C spells out in greater detail the set of parameter values that were used in the simulations, but does not attempt to justify these choices of parameter values. The regret of MF-UCB seems to improve that of UCB by a factor of 3 to 4 in many of the simulations; should we expect this to be typical of the relative performance of the two algorithms in practice, or is it an artifact of the way parameters (e.g. values of \zeta^{(m)}) were chosen?