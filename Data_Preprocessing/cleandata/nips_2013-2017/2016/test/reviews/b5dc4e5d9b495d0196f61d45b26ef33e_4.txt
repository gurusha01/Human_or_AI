This paper shows how to run Nesterov's smooth minimization of non-smooth functions technique in phases, with increasingly better strongly convex regularizers for the dual, in order to obtain faster convergence for a class of optimization problems. Improvement follows from the fact that the iterate obtained at the end of a phase can be used to warm start the next phase. This is guaranteed via a simple lemma bounding norm distance to sublevel sets in terms of value distance. Together with a local error bound condition, better convergence follows for a set of problems. I find this idea really cute. It is a very intuitive thing to do; however, for it seems that we lacked rigorous analyses for such a "homotopy" method. The paper is a pleasure to read, and it introduces the reader very gently to the topic, by first presenting Nesterov's smoothing trick. It would be interesting to write down what is the precise iteration bound for optimizing Hoffman's bound and for cone programming (linear convergence for these problems sounds very exciting; I don't know if it is new or not, and it would be nice to mention whether anything similar results via first order methods were previously known). update I still like the paper, so I maintain my scores. A quick Google search shows that this is not quite the first claim of a first order method for LP with linear convergence. It's not clear to me how various parameters of the problem affect the iteration count, so I suggest carefully analyzing those instances before claiming something in that direction.