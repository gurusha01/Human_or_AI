In this paper the authors train an LSTM to act as a diagonal gradient based optimizer. They demonstrate the systems capability to optimize quadratic functions, small MLPs and images for style-transfer faster than mainstream optimizers. The topic of the paper is really nice and important, but the paper itself feels rushed.  weight of the individual timesteps: The chosen scheme of putting a weight of 1 to each timestep implies that the system tries to minimize the integral over the error, instead of the final error. This can be an advantage in the beginning since it rewards quick convergence. On the other hand it might lead the system to focus too much on the beginning of the optimization where most of the improvement made. So this issue is at the core of the question "What makes a good optimizer" raised earlier in the paper. It should therefore be discussed more explicitly.  How many optimization runs did it take to train the optimizer?  Optimizers are run for a very short time: Only 1/5 of an epoch on MNIST! What is the reason for this particularly short time? Figure 2(right) gives the impression that the method generalizes to longer horizons, so why not show that? And if it doesn't that would be important to show too!  Discuss limitations! How does the runtime of this model compare to SGD/ADAM? I assume the main limitiation is memory consumption, since during (meta-)training the LSTM optimizer needs to keep about 160 states per parameter per step.  The most important aspect to investigate should be how well the trained optimizers generalize different architectures and datasets! But the paper only investigates three small changes to the architecture, with no error-bars. Having, for example, a systematic investigation about how hidden layer size affects optimizer performance should be easy to do and is painfully missing.  I don't see what the NTM-BFGS optimizer adds to the paper. It is only briefly mentioned and only evaluated on the cubic functions, where it performs the same as the LSTM-GAC. I therefore suggest to remove it. Minor:  in Line 97 it should be be the gradient of f  The sentence "... was optimized for 100 steps and the trained optimizer was unrolled for x steps." occurs several times and is confusing: I assume you mean that for training the optimizer was unrolled for x steps.  Line 193: Averaged over how many functions?  Plots are sloppy: No axis labels. Short titles would have helped.  Pre- and Postprocessing are handcrafted and tuned for each problem, which in my opinion collides with the basic tenet of the paper: remove handcrafting from the optimizer. * I would really like to see also other optimizer architectures. How important is LSTM? Two layers? 20 Units?