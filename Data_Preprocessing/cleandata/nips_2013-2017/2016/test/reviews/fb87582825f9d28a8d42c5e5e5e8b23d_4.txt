This paper proposes to simplify the design of the optimizer used to minimize a function by using a meta-optimizer whose input is the gradient of the function to be optimized and whose output is the update to be applied. The idea is definitely interesting and I can see how this would benefit the practitioner. I do think, however, that the paper has difficulty living to its claims and I am afraid that, despite the obvious interest of the idea, there are still many hurdles before this idea or a similar one will be adopted. The optimization community is vast and numerous papers are published every year about new optimization techniques. Knowing which technique is the most suited to a particular problem requires lots of experience and, once a particular technique has been chosen, there are usually optimizer hyperparameters to be tuned. Starting from this observation, the paper attempts at providing a meta-algorithm whose output would be optimizers (or, rather, gradient transforms) suited to a wide range of problems. The idea is definitely appealing and, should this method work, be of great significance to the community. The paper in general is well written. The literature review is thorough and the presentation of the various ideas very clear. Though I am not too familiar with LSTMs, I could easily follow the gist of the meta-algorithm as well as the claims made. However, I would like to comment on the actual claims made and how the paper actually delivers on them. First, the paper claims that the proposed architecture could technically model famous algorithms such as L-BFGS. While this is true from a parametric point of view, in that it can model a low-rank linear transformation of the gradient using a history of past updates, it is very far-fetched to claim that "a memory could allow the optimizer to learn algorithms similar to [...] L-BFGS". The presence of "if appropriately designed" does not change the fact that L-BFGS relies on very specific updates and that there is a huge gap between implementing history-based transforms and implementing something similar to L-BFGS. Second, while it is absolutely true that there is a wide variety of optimizers "tailored to specific classes of problems" and that that is indeed a problem, the proposed architecture does nothing to cater to these specific classes of problems. In particular, the LSTM is not adapted to problems involving sparsity (using proximal methods and sparsity-inducing norms), convex problems with finite training sets (tackled with methods such as SAGA or SVRG), strongly-convex problems (L-BFGS) or optimization over a compact set (Frank-Wolfe). In fact, all the comparisons are done with diagonal scalings of SGD. As such, the paper falls short of its claim that "the design of an optimization algorithm can be cast as a learning problem". The conclusion is much more accurate, stating that "learned neural optimizers compare favorably against state-of-the-art optimization methods used in deep learning". Third, the use of an LSTM actually increases the number of parameters the practitioner needs to tune. My lack of knowledge on LSTMs unfortunately prevents me from commenting on the sensitivity of the LSTM to the specific optimizer used but I would have liked an analysis of this issue. Further, unless I am mistaken (and that is entirely possible, I am sure the authors will correct me if that is the case), momentum or other tricks (batch normalization, for instance) were not used for the optimizers the LSTM is compared to. As the difference in performance, albeit significant, seems of the same order as the gain of such tricks, this comparison would greatly benefit the paper. In summary, the idea of designing a model that takes local information about the function as input and outputs an update, using a hidden state to summarize history, is very appealing and could unify a lot of the current techniques using stochastic gradient, or variants thereof. However, while the added complexity of an LSTM would be negligible if the rest lived up to its expectations, it is not the case anymore when the impact is a speedup over a few, well-established methods. Thus, the paper needs to be more thorough about its analysis of the optimization of the LSTM and the impact on a wider range of problems. UPDATE AFTER AUTHORS' FEEDBACK AND DISCUSSION I carefully read the authors' feedback and I think they sidestepped the main issue I was mentioning. I fully agree that designing an optimizer through examples rather than through mathematical properties is appealing but this is not what the paper shows. In particular, it does not show that the work needed to learn that optimizer is smaller than the work needed to tune a first-order optimizer for deep nets. Also, it offers generalization at a very narrow scale, which means that I cannot conclude from reading the paper that the amount of designing work for someone who wishes to solve a broad range of problems is smaller than if they had to tune each optimizer individually. That is why I do not change my score and still think that this paper needs a major rewriting to be fully honest about what it achieves.