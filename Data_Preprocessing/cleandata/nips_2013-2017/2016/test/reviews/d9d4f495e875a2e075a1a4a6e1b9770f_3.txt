The paper proposes a framework for making long-range predictions in videos by predicting motion. This is different from the standard approaches that reconstruct the next frame(s) explicitly from some internal state of the model. A weakness of such approaches is that the internal state needs to explicitly handel appearance information. However, this is actually not necessary, as this kind of information is readily available in the current input frame. The proposed method instead can focus learning the necessary physical concepts that concern motion. The motion prediction is implemented by a deep network that outputs either convolutional filters or affine transformation matrices that are applied to the current input frame. Additionally, the method allows to integrate conditional input in the form of actions/states of a robot interacting with the provided scenery. Overall comment: The paper is well motivated and well written, nearly every presented aspect is understandable. (I do not completely understand how this 'tiling' operation of the action/input state works in detail). It might have been conceptually better to separate out the fact that the model is itself action-conditioned. While the motivation for introducing the model has a clear 'interaction' aspect, focusing only on predicting motion first would have made the two contributing aspects (prediction, action-conditioning) better visible (well, a personal opinion). Of course, without the action-part, the presented model has some similarities to the recently (NIPS-submit) Dynamic Filter Networks from Luc van Gools group. Another very recent suitable reference/related work should be Forecasting from Static Images using Variational Autoencoders out of CMU. I would have loved a more detailed analysis of things that actually do not work too well yet. E.g. it seems that mostly predicting the robot arm movement works, though the moved objects are still quite blurry. This seems to be particularly true for non-rigid objects (if my interpretation of the videos is correct)? This might be due to the fact that the training loss still is the reconstruction loss in pixel space? A more GAN-type loss could have a positive effect here? A very different question is related to the overall approach of producing predictions in the actual pixel space. Apart from the fact that the results are interpretable to humans, there does not seem to be a reason for that. Instead, predicting some (compact?) code for the next frames should be much more reasonable, e.g. with respect to the available capacaty of the model or with respect to the subsequent use of this predictive framework?