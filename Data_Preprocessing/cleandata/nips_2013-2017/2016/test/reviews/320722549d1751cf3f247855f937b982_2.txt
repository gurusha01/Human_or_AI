The present paper addresses the online convex optimization setting with a decision set equal to either (i) a Hilbert space or (ii) the probability simplex. The authors present a novel reduction from this problem to a coin betting problem for which optimal and simple algorithms are well known. In particular they show that the Krichevsky-Trofimov strategy (or any generalization of it), which is optimal for coin betting, leads to parameter-free algorithms in (i) and (ii). "Parameter-free" means that the learning algorithm is not tuned as a function of the unknown norm of the competitive vector (for (i)) or the unknown KL divergence of the a good weight vector w.r.t. the the prior at hand (for (ii)). This general framework helps generalize previous works on parameter-free algorithms in either (i) or (ii). My opinion about the paper is very good. It is technically solid, very well written, and it helps generalize existing algorithms for Online Linear Optimization (OCO) or Learning with Expert Advice (LEA) through a novel reduction to a coin betting problem. It should be of great interest to people in the online learning community. I must admit that I do not find the coin betting reduction intuitive (especially for LEA, cf. Section 4.2). However, the generality that this approach provides and the simplicity of the resulting algorithms makes it quite attractive. "Parameter-free": you assume that all gains are bounded by 1 (in Euclidean norm for OLO or in sup norm for LEA). Imagine for a second that all gains were bounded by b instead, with an unknown range b. Can you please answer the following? 1. Of course we could rescale all gains by b and apply your machinery to the rescaled gains to get regret bounds that would be proportional to b. However I have the impression that the resulting outputs wt would depend on the unknown value of b. Is it correct? 2. Fortunately, for the particular case of the Krichevsky-Trofimov potential, the wt are proportional to the gains. Therefore, in the LEA framework, the resulting weight vectors pt defined by (12) would not depend on the value of b. Am I correct? 3. Unfortunately it seems that this invariance property does not hold in the OLO framework. Is it true? 4. What happens if we play with losses instead of gains? Again, if all losses are bounded by b, I have the impression that the transformation gain <- 1-loss/b works for LEA (i.e., the algorithm does not depend on b), but we may have a problem for OLO. Is it indeed the case? Please add a discussion of all the four points above in the paper. In particular if I am not wrong for 3. or 4., then the authors should be careful when using the word "parameter-free". Other (minor) comments: - l.16: the Hilbert space --> a Hilbert space - Lemma 1: is equivalent to: implies? I do not understand the equivalence. - Definition 2: can you give examples of potentials at this point? (As such, the reader has to wait a little bit.) - l.193, Gamma function: t^{x-1} instead of t^{-x} - end of l.394: the cases ui=0 or \pi_i=0 should be treated separately (but the result is ok).