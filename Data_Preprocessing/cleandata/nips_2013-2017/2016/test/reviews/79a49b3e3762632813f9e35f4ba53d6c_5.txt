This paper proposed to solve the problem of estimating the class prior probabilities when one only has access to positive and unlabeled examples, albeit with some label noise. Though their experimental results show that they indeed perform better than competing methods, the paper lacked a clear motivation for why this is an important problem to solve. Thus, even though I do not have any particular gripe against the paper, it is difficult for me to recommend this for acceptance. I do, however, fully acknowledge my lack of visibility on this topic and I would be perfectly happy with the authors or the other reviewers convincing me otherwise. This paper tackles the issue of identifying the prior probability of an example being positive given that one has only access to positively labeled and unlabeled examples, with the further assumption that some examples have been mislabeled. The algorithm works by assuming each of the two distributions to be a mixture of two based distributions (the one for positive and the one for negative examples) with different mixture weights. It then jointly learns all the parameters in the mixture, either parametrically using GMMs or nonparametrically using AlphaMax (which I am not familiar with). Further, the paper proves sufficient and necessary conditions for identifiability of the class priors. Finally, they show on a wide range of datasets that the introduced models perform significantly better than competing methods. I am torn with this paper. On the one hand, there is nothing wrong with it. The previous work is mentioned, the theorems are clear and so are the proposed methods. The experiments are also plentiful. However, I have a few gripes which make me lean towards rejection. As said in the summary, I would be happy to be proven wrong and change my rating. First, and this is the major one, there is very little justification for the importance of the problem this paper is trying to solve. Though molecular biology and social networks are mentioned as domains where such data could occur, there is no compelling argument as to which problems are solved by a better approximation of the class priors. This is problematic as, in its current state, I fail to see how this paper could open the way to new practical results. Second, the theorems are weak. In particular, the condition for identifiability seems extremely strong and I do not see it ever occurring in practice: there needs to exist an x for which one of the densities is 0 and the other is positive. This is even more the case when dealing with empirical distributions where either this always happens (when using the empirical distributions) or almost never happens (when using smoothed versions). Thus, as it is, I fail to see how those theorems bring an understanding to the problem that is being solved. Finally, the proposed method relies on density estimation, which is notoriously difficult in high dimension, as mentioned in the abstract. In fact, all the datasets in the experiments are low-dimensional (d < 128), which is in stark contrast with molecular biology where d is typically very large. UPDATE AFTER AUTHORS' FEEDBACK AND DISCUSSION I read the paper again after reading the authors' feedback. I had indeed missed a critical component, which was the specific way to go from the original dimension to the unidimensional problem. I apologize to the authors for that and raised my score accordingly. I still fail to see specific real-world problems where this algorithm would bring value, which keeps my "potential impact" score low. As this is my core issue, I am still leaning towards rejection but will not fight for it because of my low confidence.