The paper presents a comprehensive method to sparsify deep neural networks across channel, filter, filter shape and layer. The proposed method looks very practical. First, the proposed method needs to be compared with the state-of-the-art method, group-wise brain damage (GBD) [Lebedev, 2015]. Although GDB belongs to the filter shape according to the categories of this paper, the performance improvement (in terms of the amount of floating point operations) is larger than what this paper presents, especially, in the case of Conv1 in AlexNet. Second, the example networks, LeNet, AlexNet for CIFAR-10, etc. used in the experiments are mostly over-parameterized, i.e., relatively easy to compress. It is recommended that more optimized networks such as GoogLeNet, SqueezeNet and ResNet-152 need to be utilized to show the effectiveness of the proposed method.