This paper provides an analysis of the Recurrent Neural Net (RNN) architecture by defining several measurements: 1). Recurrent depth, dr; 2). Feedforward depth, df; 3). Skip coefficient s. In the experiment, the paper compares several RNN variants with different dr, df, and s on five standard datasets and problems. Based on these experiments, this paper offers some guidelines about how to design an RNN. The goal of the paper is to provide a theoretical and quantitative guideline for designing an RNN for a specific problem. This is an important and hard topic that worth investigating. The measurements of dr (Recurrent depth), df (Feedforward depth) and s (Skip coefficient) seem to be reasonable. The authors also conduct comprehensive experiments to compare several RNN variants. However, I find the paper hard to understand. The idea behind the proposed measurements is actually very simple, but the paper uses too much symbols and definitions to explain it. In many cases, the symbols just appear without being explained or defined before (e.g. "s" in the caption of Figure 1 in Page 3 is defined in Page 5.). The reader sometimes needs to manually search for a specific symbol. I strongly recommend the authors to remove unnecessary symbols and rewrite the paper using plain language as best as they can. It would be also very helpful if the author could summarize the meaning of the important symbols in a table at the beginning of the paper. Several definitions and lemmas can also be moved to the supplementary material. This will significantly improve the clearance and impact of the paper. I also have several questions about the experiments: 1. In Table 1, the performance of different models seems to be too close to each other. E.g. (1.84 v.s. 1.83). What is the performance variance of the same model with different random initializations? If the experiment is conducted again, will the conclusion be the opposite to the current one? Also, although BPC in the caption has been explained before in text, it would be better if you write Bit-Per-Character at least once in the caption since the reader might not be very familiar with this term. 2. In Table 2, in the top-left table the best performance of MNIST is 87.8. But when you try to compare your model with other papers, the best performance is changed to 98.1 shown in the bottom-left table. It is really confusing to me. I guess the models are different. In the top left table the model is RNN(tanh). In the bottom left table the model is RNN(stanh)? But why not just compare RNN(stanh) models with different s in the top left table? Please explain. 3. Minor: Please add a vertical line in Figure 2 to separate the right and left sub-figures. In summary, this is an interesting paper studying an important problem. However, the current form of the paper is unclear and sometimes confusing. It needs more polishing in order to be accepted in NIPS.