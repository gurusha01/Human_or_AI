This paper proposes several definitions of measures of complexity of a recurrent neural network. They measure 1) recurrent depth (degree of multi-layeredness as a function of time of recursive connections) 2) feedforward depth (degree of multi-layeredness as a function of input -> output connections) 3) recurrent skip coefficient (degree of directness, like the inverse of multilayeredness, of connections) In addition to the actual definitions, there are two main contributions: - The authors show that the measures (which are limits as the number of time steps -> infinity) are well defined. - The authors correlate the measures with empirical performance in various ways, showing that all measure of depth can lead to improved performance. The measures proposed by the paper seem intuitively sensible and useful. One could argue importance of these measures is not clear if they are simply demonstrated (non entirely conclusively as it turns out) by comparison with empirical results on real data. In other words, if some formal guarantees could be provided as a function of these measures, rather than empirical evaluations, the usefulness of the measures would be undeniable. At present, one could ask why these measures are more relevant than other possible measures of complexity. It is also a little unfortunate that so much complexity is left in the appendix, but this is a common trend in nips and the authors can't be blamed for that.