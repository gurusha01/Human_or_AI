The paper considers a sequential decision problem with stochastic rewards bounded in [0,1], both in the full information and bandit information settings. The mean reward of each arm is allowed to change with time, and thus the instantaneous regret at time t is defined with respect to the best action at this time. The cumulative regret is thus defined against a strategy that potentially selects a different arm at each time step. The rewards are assumed to be independent across arms and time. Three quantities are considered that capture the complexity of the environment: the number of times the mean reward vector changes (Gamma-1) the cumulative drift measured in infinite norm (V), and the cumulative variance of the reward distributions (Lambda). In this non-stationary setting, the paper studies several algorithms. Algorithm 1 has perfect knowledge of Gamma, Lambda, V and of the time horizon and works in the bandit setting. Algorithm 2 has similar knowledge and works on the full-information setting. Algorithm 3 is parameter free, and works in the full information setting. It is directly inspired from the work of [3] and [4]. Its regret is controlled in Theorem 3.5; its static regret against any fixed arm is controlled in Theorem 3.4. Section 4 provides lower bounds on the regret, in terms of the quantities Gamma, Lambda, V. I have read the feedback from the authors. Overall, this is an interesting contribution. Algorithms 1 and 2 are arguably not interesting; at least it provides some warming-up. Algorithm 3 is more interesting. It uses a simple, yet effective combination of two ideas coming from [3] and [4]. The regret performance is also appealing, especially in view of the lower bounds. However, it could be useful to provide some explicit constants in the main material of the paper. I disagree with the claim that the regret is "constant" with respect to time, since the quantity Lambda typically scales with T, even when Gamma and V are constants. Thus, I think this claim should be nuanced. Any idea how to extend this work to the parameter-free bandit setting? Gamma is defined in terms of the number of changes between the mean vectors. Another related quantity of interest is the number of changes of the optimal arm, which can be arbitrarily smaller than Gamma. Do you think such a quantity could be used as well or do you see a strong reason why not do use it?