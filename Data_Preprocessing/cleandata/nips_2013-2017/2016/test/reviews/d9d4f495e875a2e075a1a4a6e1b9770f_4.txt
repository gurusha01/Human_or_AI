This paper presents three different approaches to predict future video frames conditional of an agent action in an unsupervised way. More precisely, the approach predicts how objects move in a video without explicitly learning the optical flow. The method relies on a deep network combining LSTM, dynamic neural advection and spatial transformers. The method is extensively evaluated on a new dataset of robots manipulating objects (to be released soon according to the authors) and on the Human3.6SM dataset. The results are compared with the state of the art, showing excellent performance. This paper is well written, the method well evaluated and the quantitative results convincing. Learning physical interaction from video is indeed a challenging topic, which could greatly impact autonomous systems and beyond. Unfortunately, it is difficult to me to really assess the strength of the approach, despite the excellent quantitative results: 1) First, the still images are not convincing. The object to predict becomes quickly blurry. A prediction of 1s is quite limited, and if the image becomes already blurry at that stage, I wonder what would be the utility of the approach. That said, I could not look at the videos as the link was not working. Maybe the results are more convincing in the videos. 2) The use of LSTM for prediction is not really new. How did the author came up with such an architecture. Would adding more LSTM layer improve the prediction length? What about using differentiable long-term memory (like in NTM)? 3) For the dynamic neural advection, it is not clear to me what is M exactly. Is is a scalar value for each pixel? How is the motion parametrized? 4) It would have been great to provide more mathematical details on each part of the architecture, as this could greatly help the reader in grasping what were the modeling assumptions and limitations but also help reproduce the results. Unfortunately, the link with videos and all the appendices were not available at the time of this review.