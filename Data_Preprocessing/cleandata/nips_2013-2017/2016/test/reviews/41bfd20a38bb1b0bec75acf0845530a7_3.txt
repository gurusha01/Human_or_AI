The paper proposes the use of Graph Lasso to prune weights in a way which respects the 4d tensor structure. So, e.g. we would like to prune a whole channel or a whole filter, or even prune an entire layer of weights using the "shortcut" trick present in e.g. residual nets. The justification for doing this is two fold: it makes more sense semantically, and it allows us to avoid the computational penalty incurred with truly sparse weight matrices (especially in convolutional layers), as the sparsity can be quite random. The idea of using graph lasso to prune weights is simply not novel enough an idea to be publishable at NIPS. I would be more open to it if the experimental results were amazing, but they are quite lackluster.