This paper develops novel deep network architectures that make pixel-level predictions of video frames, and introduces a new large scale dataset containing video of real-world object manipulation by robotic arms. The architectures all predict the motion of pixels in each frame, rather than directly predicting pixel values. This approach enables more accurate predictions on unseen objects. Two of the architectures also contain a 'compositing' scheme, whereby different transformations are applied to different objects, resulting in more or less unsupervised segmentation of objects. The large scale dataset enables end to end training of the system, and rigorous evaluation of competing approaches. Moreover, beyond the video input, the dataset contains the pose of the robot arm as well as a representation of the goal pose, making it suitable for learning about the effects of actions in the environment. Together these advances may help develop robots which can learn from unstructured interaction with their environment. This paper introduces a very intriguing dataset of real world physical interactions between a robot and a variety of objects. This is a strong contribution which is likely to generate significant future work. The models proposed in the paper outperform competitor methods, though it should be noted that the absolute quality of predictions is still rather poor. The paper understandably focuses on making use of the real-world dataset it introduces. However to justify some of its claims that the neural network architectures it proposes are capable of learning the physics of the environment, it would be very helpful to test these architectures on, eg, the block world dataset of Battaglia et al. This would enable controlled examination of the degree to which the models learn genuine physical concepts and constraints. Said another way, learning physics (i.e. underlying causal structure) may be different from learning predictions of motion. There is very recent work which has made pixel-level predictions multiple time steps into the future, with results that seem at least comparable to what is achieved in this paper: Lotter, Kreiman, & Cox. Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning. ArXiv, 2016. Ideally this method would be part of the quantitative comparisons in the paper, but given how recent this work is, that may not be possible. It should at a minimum be discussed, though.