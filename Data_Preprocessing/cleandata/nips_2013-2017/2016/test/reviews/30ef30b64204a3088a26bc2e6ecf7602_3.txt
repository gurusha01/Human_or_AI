The paper proposes and investigates a very simple idea for deep semi-supervised learning. The idea is to impose additional constraints derived from unlabelled data enforcing that deep representation of a data sample should be as constant as possible under nuisance transformations. The idea is thoroughly validated and is shown to achieve state-of-the-art on a number of datasets. The idea is very simple and takes half a page (top half of page 3), while the bulk of the paper is devoted to its validation (which is the right thing to do). The validation brings spectacularly good results for semi-supervised learning across a range of standard datasets. The high absolute numbers are also explained by the use of sophisticated state-of-the-art architectures, however the authors are careful to show the advantage over the baseline that does not use the proposed augmentation or uses a competing idea (mutual-exclusivity) only. In terms of novelty, perhaps, the authors should have paid more attention to the link with [31], as their approach seems to be quite related (note that [31] have tried up to 32000 surrogate classes in their experiments, which is essentially very similar to what is proposed here). The approach is also very much related to tangent-distance methods [Simard et al.98]. Another obvious connection is "slow feature analysis" that has been investigated and tried with ConvNets many times. Finally, very similar trick has been used by [Kulkarni et al. NIPS15] (see section 3.2 there). Thus, the idea is not as novel as the submission tries to present. This, however, is compensated by the in-depth experimental validation and comparison.