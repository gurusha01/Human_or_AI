This paper experiments with structured sparsity (group lasso) for reducing the size of convolutional neural networks in a way that is more regular, and hence more amenable to vector computation on GPUs, than the simple l1 regularization that has been recently explored by Han et al. I find this to be an important innovation. The experimental results seem promising. I find the depth regularization is particularly interesting. This is a nice paper with a nice set of experimental results. Making DNNS run more efficiently is a core problem of deep learning. But sparsification is also useful for improved performance (improved regularization) and also for improved interpretability. The novelty is not so great as structured sparsity is a well established idea.