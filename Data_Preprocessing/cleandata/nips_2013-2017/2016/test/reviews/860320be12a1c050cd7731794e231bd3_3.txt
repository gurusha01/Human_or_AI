This is a good theory paper! The authors first present a rigorous graph-theoretic framework that describes the connecting architectures of RNNs in general, with which the authors easily explain how we can unfold an RNN. The authors then go on and propose tree architecture complexity measures of RNNs, namely the recurrent depth, the feedforward depth and the recurrent skip coefficient. Experiments on various tasks show the importance of certain measures on certain tasks, which indicates that those three complexity measures might be good guidelines when designing a recurrent neural network for certain tasks. 1. On line 59, shouldn't it be $i \in {0, 1, 2, ..., m-1}$? 2. In the caption of Figure 1, it will be better if you add brief description of how you compute $dr$, $df$ and $s$ for networks in (b). 3. On line 155, I think it should be $df = sup{i, n \in \mathbb{Z}}(\mathfrak(D)i^{\ast}(n) - n \cdot dr)$, i.e., you have to add "()" for sup? 4. On line 167, same as above, do you have to add "()" for sup and max? 5. I notice you conduct experiments on NLP tasks, image classification tasks. I think it will be a good idea if you add experiments for speech recognition tasks as well, as recurrent neural networks are extensively used in that field. This will also bring the paper to broader potential readers. Adding speech recognition results should be easy nowadays. E.g., you can search for Kaldi, Librispeech, etc. They have existing recipes for different neural networks and they even have results. You just have to do the analysis.