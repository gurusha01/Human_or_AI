The authors present a semi-supervised procedure that uses the internal consistency of the data to drive the adaptation process of a learning machine. While traditional learning procedures only consider the difference between a required output and the actual output, this work also uses the fact that in classification problems the output should be the same despite the variations in the inputs and the learning machine. In other words, the machine's final goal is to find those invariances that maximize the recognition success. The paper is very focused and well explained. The core idea makes a great deal of sense, and exploits information present in the data in ways that has been overlooked before. This allowed the authors to obtain impressive results with small labelled sets, and large unlabeled ones. This work is one step more towards understanding machine learning as finding invariances as pointed out by Gens and Domingos in their "Deep Symmetry Networks" paper. Minor comment: 1. Define the index i in eq. (1).