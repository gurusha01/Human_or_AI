The paper considers the problem of online learning with K actions under both full and bandit information, and studies the notion of dynamic regret (regret measured against the strongest comparator that can switch actions in each round). The main novelty in the paper is studying the impact of the total variance of the loss vectors on the dynamic regret: the authors show several new upper and lower bounds in terms of this quantity. The paper is well-written and the results are, to my knowledge, novel. While many of the technical tools used in the proofs are more-or-less standard, their combination offers some non-trivial challenges and the execution of the analysis appears to be excellent. My only complaint is that the results are of a somewhat limited interest: none of the proposed algorithms is very practical and the bounds don't give any drastically new insight about the complexity of learning in the considered settings. Besides, I also have a feeling that I'm missing something about the results. Namely, it appears that the newly introduced quantity \Lambda may scale linearly with the number of arms/experts K, so the new bounds may have a worse scaling with this parameter than existing bounds (K^{1/3} instead of log K in the full-information setting). At first glance, the lower bounds of Theorem 4.3 appear to contradict known upper bounds in these terms, but I believe that this is not the case: the problem seems to be that the statement of the theorem is phrased a bit poorly. It should be made clear that the statement of the theorem does not hold in a problem-dependent sense, but only in the sense that "for any fixed choice of \Lambda, T and V, there exist a sequence on which any algorithm is going to suffer a regret of \Omega(...)". It would be nice if the authors added a discussion about this issue, as it may be puzzling for other readers too. Overall, while I think that the results are certainly not ground-breaking, I don't have problems with the paper being published. Detailed comments ================= 050: Describing the notion of "dynamic regret" after stating the bounds is a bit clumsy; consider rewriting this section so that it begins with the regret definition. 061: Same as above: informal definition of dynamic regret repeated. 089: Technically, your bound does not recover the previous results, as you have a worse scaling in V (sqrt(V) vs. V^{1/3}). To resolve this apparent conflict, you should mention that the asymptotics of V are tied together with the asymptotics of T. 111: "choose an action at and then suffer a corresponding loss \ell{t,i}"---maybe \ell{t,at}? 146: Add reference for vUCB (Audibert, Munos and Szepesvari, 2009?). Theorem 3.1: Since you consider both full-information and bandit settings, it would be useful if the theorems explicitly stated which setting they are about. Eq. (5): Note that this doesn't exactly match the algorithm of [4]; it is more closely related to the Bernstein Online Aggregation algorithm of Wintenberger (arxiv 2014) or to the Variation MW algorithm of Hazan and Kale (COLT 2008). Theorem 3.4: Note that it is possible to get better bounds by using the algorithm of Steinhardt and Liang [10], which depend on the path length of the best expert rather than the L1 path length of the loss vectors. (Also note however that this algorithm is not parameter-free.)