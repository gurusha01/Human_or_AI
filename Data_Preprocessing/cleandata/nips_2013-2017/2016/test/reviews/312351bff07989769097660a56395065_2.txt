The authors propose a neural network method for learning to map input sequences to output sequences that is able to operate in an online fashion. Like similar models of this type, the input is processed by an encoder and a decoder produces an output sequence using the information provided by the encoder and conditioned on its own previous predictions. The method is evaluated on a toy problem and the TIMIT phoneme recognition task. The authors also propose some smaller ideas like two different attention mechanism variations. I find the subject matter of the paper interesting as sequence/structured prediction with neural networks is still an open problem and I agree with the authors that there's a need for methods that can do online sequence processing and prediction. The biggest issue I have with the paper is that the term 'transducer' has been used multiple times before in the context of neural sequence prediction and the authors don't cite this work or discuss how it relates to their own method. This is somewhat surprising, because the authors do cite a paper in which both CTC and the transducer from Graves (2012) are evaluated and the transducer actually outperforms CTC. The transducer by Graves also takes into account previous predictions while still allowing for dynamic programming style inference as in CTC. This method differs from the method proposed by the authors and doesn't include the feature to process the input data in blocks but should be an important baseline to compare with. The existence of this prior work (and for example the work by Boulanger-Lewandowski et al., 2013) limits the novelty of the ideas presented in the paper and the extend to which the current title of the paper is appropriate. Perhaps something like "An Online Neural Transducer" or "An Incremental Neural Transducer" would shift the emphasis in the right direction. I find the empirical work somewhat limited. The results on TIMIT are decent, but don't really show the benefits of the new method in comparison to existing algorithms. I don't find the argument that it was beyond the scope of the paper to use regularization methods to get the best results possible convincing when the related work they compare with also uses an attention mechanism and windowing. That said, I did like the comparison with a model in which the RNN state is reset between different blocks and the plot about the relation between the window size and the use of attention mechanisms. All in all, it would have been interesting to see results on a dataset with longer sequences (like Wall Street Journal or Switchboard), where the new method may actually have a significant advantage.