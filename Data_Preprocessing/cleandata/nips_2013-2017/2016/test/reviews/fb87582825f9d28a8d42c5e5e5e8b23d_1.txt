The authors propose a method for learning update functions in gradient-based optimizers. By jointly training an optimizer and optimizee, parameters of an LSTM-based DNN can be learned that accumulate information from multiple gradients, similar to momentum. Once trained, an optimizer can be reused to optimize similar tasks, and can partially generalize to new network architectures. While no theoretical properties of the resulting algorithms are known, several small empirical experiments demonstrate that the resulting optimizers are competitive with state-of-the-art alternatives. I really enjoyed this paper - it's creative, timely, interesting, and could have significant impact. I think the idea of "learning algorithms" is an important next step in deep learning; this seems like a reasonable step in that direction. The experimental results seemed fine, but I guess I wish they were done on larger tests. I think everyone would like to know if this idea could help train, say, the next AlexNet. I wish that the post-hoc analysis told us something more about the strategy the optimization nets are learning. It would be interesting to figure out what signal they're capitalizing on, and see if that suggests new theoretical directions for the optimization community. While I was pleasantly surprised to see that the resulting algorithms were better than state-of-the-art alternatives, I was more surprised to see that they weren't better by all that much. I suppose this suggests that our current algorithms are pretty good; a testament to the hard work done by the optimization community. I wish that this demonstrated learning on a problem that simply wasn't solvable before.