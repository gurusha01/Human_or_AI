The paper presents an approach for learning kernels from random features. The authors propose a two-stage approach, where first set of random features are drawn and then their weights are optimized to maximize the alignment to the target kernel. The authors show how to optimize the resulting model efficiently and provide theorems for consistency and generalization performance. The method is evaluated on some benchmark problems The proposed method seems to provide a useful addition for kernel learning literature. In particular the capability to learn a better kernel when the original data is not well-aligned with the target (section 4.1) is potentially very useful. I am perhaps less enthusiastic about the feature selection experiments, since the compared method is selecting features completely randomly - we have better feature selection methods that that. The speedup over the joint optimization method is also notable. The write-up is slightly more convoluted than one would like and requires several reads to understand. Details: - line 64: you should state what assumptions you make about the feature function \phi - line 76 onwards: the notation could be described better, e.g. showing the dimensionalities of the objects e.g. W here - eq (6): I could not figure out why is the square root appearing here - line 169: It is not clear why all this follows from choosing the Gaussian kernel e.g. the exact form of the feature function probably is chosen independently?