The main idea behind this paper is that for any training sample, a good model should make the same prediction under any random transformation to the data or perturbation to the model. They propose an unsupervised loss function that they called the transformation/stability loss which explicitly tries to minimize the sum of squared differences between each pair of predictions resulting from different passes of the same sample through the model. They combine this with another unsupervised loss function, the mutual-exclusivity loss function, and show in several experiments that training in a semi-supervised fashion using these unsupervised losses in conjunction with a supervised loss function allows for better models to be trained when labeled data is not abundant. The datasets used to demonstrate this approach include MNIST, CIFAR10, CIFAR100, SVHN, NORB, and ILSVRC. This paper presents an intuitive and straightforward idea about how to use unlabeled data when training deep convolutional networks. They demonstrate impressive results on benchmark datasets, including state-of-the-art performance on CIFAR10 and CIFAR100. However, their results on ImageNet lag far behind the current state-of-the-art, which makes me wonder if their approach can help in settings where training data is abundant, or if it is only relevant for specific settings where labeled data is limited. The paper is well-written and easy to follow for the most part. Here are some specific suggestions for improvements: - At the bottom of page 5 in Section 4.2 SVHN, it's unclear to me what exactly was done in the experiment using sparse convolutional networks. It says "we create five sets of labeled data" and that "for each set, we randomly pick a different 1% subset of training samples." Does this mean that the 1% is used as labeled training examples while the rest is used as unlabeled training examples? Are the results reported in Table 2 averaged over these five sets? - Why is data augmentation used sometimes and not others?