This paper proposes to model the observation space with a hand-crafted Riemannian metric, and define a curved normal distribution based on the associated Riemannian geometry, so that the data manifold can be better captured. The paper addresses a fundamental problem on how to define a Riemannian metric based on a set of high-dimensional real-valued observations, and then how to define and estimate a curved Gaussian distribution based on this geometry. This presents a useful tool and an interesting application of Riemannian geometry in statistical learning. The first part of the proposed method defines the underlying Riemannian geometry in section 3.1. The title suggests that it "learns a metric". However the metric is actually hand-crafted in equation 7 based on two hyper-parameters rho and sigma. These hyper-parameters are fixed empirically in advance. I am guessing that sigma is a sensitive parameter. The paper should discuss in more detail what is the intuitive effect when varying sigma, and how to select this sigma. With this part missing the paper is incomplete. Is it possible to merge the metric learning stage with the LAND learning stage, so that rho and sigma can be learned? A beauty of the geometric approach is to be invariant to coordinate transformations. However the metric in equation 7 is not invariant. For example, if the data forms a line in a 2D observation space. We will have completely different metrics in a coordinate system with its x-axis parallel to this line, and in a coordinate system that forms an angle, e.g. 45 degrees, to this line. It is useful to discuss this in more detail, including whether the corresponding full (rather than diagonal) metric is invariant, and give more intuitions of the metric in equation 7. There is a rich literature in metric learning and manifold learning with connection to Riemannian geometry that can be related to this work (see google "Riemannian" "metric learning" "manifold learning"). As a main criticism, in the first experiment, the comparison is not so meaningful because that obviously the more flexible LAND can describe the data better with a smaller number of component. However GMM can describe the data better with a large number of components when LAND will overfit. The author(s) are suggested to redesign the experiment, to let each compared method to choose its favourable number of components based on certain criterion (e.g. BIC), then compare the corresponding performance. LAND is still expected to perform better because it describe better the manifold structure, and this comparison is more meaningful. It is nice to see how LAND with a large number of components will overfit. LAND is obviously a more expensive procedure than estimating a Gaussian. Is there any quantitative measurements or experiments to make it clear how expensive it is? What is the limit of the scale that LAND can handle? Is it possible to learn LAND on MNIST and compare it with Gaussian mixture learning? ====== After reading the authors' response LAND is a more flexible model than Gaussian distribution. If you increase the number of components, each local LAND will be more vulnerable to local noises as compared to a local Gaussian. "The geodesics will almost be straight lines", true, but it is still curves, and these curves will lead to overfiting. I highly suggest that the authors to revise this experiment in the revision.