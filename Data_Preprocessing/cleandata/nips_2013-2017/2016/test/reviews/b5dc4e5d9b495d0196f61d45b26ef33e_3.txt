The paper addresses the composite optimization problem: minimize F = f + g, where neither g nor f need be smooth. The current paper assumes that g has an efficient prox operator, and focuses on f with the rather general structure: f(x) = maxu(< Ax,u > - \phi(u)) The paper proposes to repeatedly approximately minimize versions of f smoothed by amounts that decrease exponentially. The main result Thm 5 shows that when F fulfills a \theta Local Error Bound (LEB) condition (for theta in (0,1]), the number of iterations needed to optimize smoothed versions is O(eps^(theta-1)). For theta = 1 (absolute value, hinge loss...), this leads to convergence linear in total iterations; for theta = 0.5 (e.g., strongly convex functions), this leads to 1/t^2 rates. The paper discusses some different conditions that result in LEB conditions, including KL property, and hinge loss ERM as in SVM. The paper shows some experimental results comparing to other first order methods. Technical quality: - The experiments are interesting and well done, except for a significant point reported below. - The proof Thm 5 is explained clearly in the supp. mat., and the aspects in the paper itself (Thm, definitions, examples) are sound well written. Novelty: I am not sufficiently familiar with the exact literature, so am assuming the authors claims hold. In this case, the new rates under the proposed setting are new and non-trivial. It is worth noting that there exist previous work on gradual smoothing for faster rates, such as "Optimal Black-Box Reductions Between Optimization Objectives" despite it being quite different in assumptions and results. Impact: The paper shows how LEB and KL can enable faster than typical rates for problems where f,g are both non-smooth, which has potential to spur further research into methods adaptive to such data dependent parameters. It would be even stronger: - If it considered in detail the computational complexity for ERM problems, e.g., by replacing APG by an SVRG variant. - If it focused on representative performance. The experiments report the best HOPS performance among several arbitrary seeming settings of t, but this is not a practical way to use the algorithm. If PD-HOPS fully avoids this, it should be the focus, and we should see its downsides as well (in the paper, not just the supp. mat.). Clarity and presentation: The sketch of Thm 5 is not clear enough to be useful. The most salient point is the use of x^daggers-1,eps and its interaction with Lemma 1, but the induction is neither clear there nor that interesting. l.231: Typo: min_{x\in\RR^}F(x) is a number, does not have an epigraph, remove the min.