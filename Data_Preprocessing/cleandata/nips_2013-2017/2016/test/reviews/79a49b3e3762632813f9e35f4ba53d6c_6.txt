Suppose we are given samples from two mixtures of a pair of (unknown) distributions, with different mixing coefficients: \mu = \alpha \mu1 + (1-\alpha) \mu0 \nu = \beta \mu1 + (1-\beta) \mu0 This paper presents methods of estimating \alpha and \beta. The suggested application is for PU-learning where \mu1 is the density of positive samples, \mu0 is the density of negative samples, \mu is the density of the unlabeled samples and \nu of the positive-labeled samples with 1-\beta probability of error. (e.g. if \beta=0.9 then 10% of the positive-labeled samples are actually mislabeled negative samples). One nonparametric method and one parametric method are presented and compared via simulations. First, the good parts: this paper addresses a problem which seems to be important and it provides a solution to it. The paper is readable and the work seems solid. My biggest issue is that this work appears to be a rather minor extension of [1] (obviously, written by the same authors):  The theoretical results seem to have some serious overlap.  The main algorithmic contribution of this paper is the AlphaMax-N algorithm, which is a small variation of AlphaMax from [1]. The authors present another parametric algorithm MSGMM-T, but it is not competitive in most cases. * The simulations use the same UCI data sets, but with added noise. I must admit I did not have the time to perform a thorough comparison with [1], so I might be wrong in my assessment. Could the authors list which parts in this work are brand new and which parts are closely related or reformulations of results from [1]? Another, more minor gripe, is that several real-world motivations are discussed, such as facebook likes and learning certain protein interactions. But no such data set is used in the simulations. Rather, standard UCI data sets are used. If this problem really has many important applications, it should be possible to obtain such a data set and it would make the paper stronger. My personal opinion is that this is a decent paper, but it does not contain enough interesting new material to warrant publication in a top conference such as NIPS. Other suggestions for improvement: 1. Section 3 has a lot of notation and technical results that make it somewhat unpleasant to decipher. But after deciphering the results are quite intuitive and appealing. I think a couple of figures with mixtures of some simple distributions (e.g. discrete multinomial) could make the ideas much clearer. 2. It would be interesting to obtain some more theoretical results regarding the AlphaMax-N. e.g. How does the error in estimating the mixing proportions decrease as the sample size goes to infinity (perhaps under some strong conditions). 3. The authors should certainly make their code available for others to use. [1] "Nonparametric semi-supervised learning of class proportions" by Jain, White, Trosset and Radivojac (2016)