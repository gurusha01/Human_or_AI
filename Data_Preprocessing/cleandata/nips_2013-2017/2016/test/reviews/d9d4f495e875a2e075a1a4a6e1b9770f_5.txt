The paper is about learning to predict the outcomes from the physical interaction in an unsupervised manner. The main idea is to predict the motion of the pixel (as opposed to predicting the actual pixels values). Building upon convolutional LSTM, the authors present three predictive models to achieve this goal (DNA, CDNA, and STP). These methods are evaluated on a new large-scale physical interaction dataset as well as the Human3.6M datasets. Quantitative comparisons against [14,17] are presented. Clarity of exposition: - The paper is well-written and easy to understand. - The discussion of the literature is comprehensive, clear and well-organized. - The paper only shows the CDNA architecture. The other two architectures are described in the texts. I wonder if it is also possible to visualize the other two architectures in Figure 1 (or in the supplementary material). - In the supplementary site, the authors show video results as well as the masks. Method: The presented method achieves excellent results compared with other state-of-the-art algorithms. The predicted frame, however, suffers from blur artifacts and missing details. For examples, - It seems to me that the masks do not have temporal coherence. I wonder why this is the case. Does the frame prediction in the next frame depends on the current frame? - The collection of motion transformation M can be viewed as the backward optical flow field. It will be interesting to impose priors on the term (e.g., penalizing large gradients of the flow fields) to encourage spatial smoothness. In sum, this is well-written paper with solid results. The presented methods provide a new way to predict future frames from physical interaction. I think the paper makes solid contributions to the community.