This paper proposes a new optimization method to replace traditional methods such as SGD and RMSProp, which are hand-crafted parameters update rules. The proposed method is through an LSTM that learns the optimal parameter update at each step. The proposed method replaced hand-designed update rules with a learned update rule. Evaluated on quadratic functions, MNIST and Neural Art, the proposed LSTM optimizer achieved better performance than traditional optimizer. This is a really novel optimizing technique that replace hand-crafted parameters update rules with learnable update rule. The learning curve achieved much better result than traditional methods. It's a well written paper enjoyable to read and very easy to follow. There are a few questions hopefully the authors could address: 1. What is the time cost of running LSTM optimizer? Apparently the LSTM optimizer is not free lunch, the proposed update rule requires much more complicated computation than SGD (computer a LSTM v.s. just do ax+y) how much slower will the training process be? 2. In the result section, the authors provided the loss value over interations, what about the final accuracy? 3. The authors provided experiment results on simple tasks such as quadratic functions, MNIST and Neural Art, how is this method generalizable to modern deep networks such as ResNet on large scale datasets? will it be hard to train?