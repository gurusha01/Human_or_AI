In this work, the authors study semi-supervised learning with convolutional neural networks. They propose an unsupervised loss function that minimizes the difference between predictions of multiple passes of a training sample through a deep network. The new method is evaluated on some image recognition tasks and experimental results are shown. Overall, the work is interesting and the authors do a nice job of setting up and reporting on multiple benchmark image recognition datasets. However, technical contributions of the paper could be summarized as adding a simple technique to reduce variation in predictions for the same training example due to techniques such as randomization, dropout, max-pooling, etc. A lot of the effort seems to have been spent on running experiments to demonstrate the effect of this extension to the loss function. One thing was unclear --- in practice, how did you optimize the new loss function? Do you replicate training samples in each mini-batch and keep copies of the predictions per sample from previous iterations in order to optimize the transformation function? What effect does this have on the speed and convergence of the network? Accuracies for the new method on the ImageNet ILSVR task seem significantly lower than current state-of-the-art methods.