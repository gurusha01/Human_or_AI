This paper applies structured sparsity learning to simplify the parameters of a deep neural network in order for fast computation without large loss of accuracy. This paper propose a structured sparsity learning approach to simplify or speed-up a learned deep network in order for applications in platforms with limited computation resources. The idea of learning a compact and hardware-friendly structure is very interesting. However, some aspects are not clear enough. 1. How the approach is related to hardware? How to get CPU, GPU, or FPGA friendly structures, respectively? Or how to design the optimization goal to achieve this? 2. Could the approach to be used to transform the network to a version with only integer operations? 3. How to balance between computational complexity and error of the simplified network? 4. What is the computation complexity of the proposed approach itself?