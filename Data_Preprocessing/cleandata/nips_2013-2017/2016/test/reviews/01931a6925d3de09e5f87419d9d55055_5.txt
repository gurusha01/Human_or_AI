Summary: the paper proposes a locally adaptive normal distribution based on a geodesic manifold traversal distance, learned adaptively from the data. The distance replaces the standard Mahalanobis distance used in the Gaussian distribution. The extension to mixture of LANDs is also provided. The performance is evaluated on a synthetic dataset as well as a real-world dataset of EEG measurements. Major comments: the proposed idea of using a gedesic distance, similar to Isomaps, is very interesting. However, there are few weak spots in the clarity of the paper. First, the definition of the inverse of the metric tensor in Eq. (7) seems ad hoc. Can authors provide more intution on the idea of using a Gaussian kernel for the weights in a weighted sum of local covariances? Also, I got lost between the definition of the local covarance matrix in Eq. (7) and later, the MAP estimation of the same quantity in Eq. (11). Do these refer to the same quantity? Next, the experimental evaluations are rather poor. But my main concern is the scalibility and computational efficiency of the method. For example, what is computational compleixity and the running time compared to standard EM? In my opinion, the paper has an interesting theoretical contribution, but might not be very applicable in practice. Minor comments: Algorithm 1, lines 4 and 5, d -> \nabla