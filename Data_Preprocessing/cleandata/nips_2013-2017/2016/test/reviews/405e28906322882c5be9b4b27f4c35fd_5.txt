The paper provides improved lower and upper bounds on the regret in the bandit and the full information setting. The bounds are formulated in terms of four parameters:  the number \Gamma the index of the optimal arm changes,  the sum V of the changes of the mean reward between subsequent rounds,  the sum \Lambda of the variances of the rewards in the various rounds, which has not been considered before in this context,  and the time horizon T. Previous results only combined T with \Gamma or V, and can be obtained from the results in this paper as a kind of worst case version. The paper is well-motivated, easy to read, and all the details are clearly described. Although the derived results are not likely the ultimate version of the regret bounds (meaning that the parameters do not capture all the seemingly important features of the problem complexity), they do provide interesting additions to existing results, and clearly contribute to the deeper understanding of the topic. A downside is that most of the algorithms require some knowledge of the parameters which is rather unrealistic. Nevertheless, the paper provides enough additional novelties that make it a valuable contribution. - Line 192: adding the remark "mt is determined below" could slightly improve the reading experience. - Lines 186-187: do you choose a random arm generated by using distribution pt? - Line 205: is \eta{t,k} correct there? Shouldn't it be \eta{t-1,k}? - Lines 304-309: i is already preserved for the index of the optimal arm. Please use a different index here. - Lines 318-320: Does not \sqrt{VT} \el \sqrt[3]{\Lambda VT}, plugged into Theorem 4.1 immediately imply the claim of the theorem? Is the argument in the sentence between lines 318 and 320 really needed? - Table 1 in Appendix A is very nice and really helpful. However, extending it with highlighting the upper bounds that are based on algorithm that require extra knowledge of the parameters could make it even more helpful. - An overall question: the regret in this paper is computed against the best dynamic learner. What could be the regret against the best fixed arm? That is, could these parameters lead to some improvement of the classical results too?