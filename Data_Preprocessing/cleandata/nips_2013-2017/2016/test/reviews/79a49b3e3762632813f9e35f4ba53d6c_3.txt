The paper introduces a method to estimate class priors in a positive and unlabeled setting. The authors account for the possibility of false (known) positives, which are very common in practice but rarely addressed in existing work. To avoid the curse of dimensionality, the proposed approach first transforms data to a single dimension using a non-traditional classifier and then proceeds with density estimation. The benchmark shows that the proposed approach works well. Overall, the manuscript is well written and easy to follow. The proofs are involved and somewhat heavy on notation, but the fact the authors summarize the main practical impact of each proof greatly improves readability and flow of the text. The insights of theorem 1 and 2 are very useful. I have a few comments on the current text: - The authors mention that this is the first approach they are aware of for noisy positive and unlabeled data (lines 57-58 and 312-314). However, prior work exists that explicitly addresses this variant of learning from positive and unlabeled data [1], though it is understandable that some prior work is missed. Additionally, learning from noisy positives and unlabeled data can also be considered in the broad setting of learning with label noise (i.e., noisy positives vs noisy negatives), for which a plethora of methods is available (e.g., reviewed in [2]). Hence, I must disagree with the claims that the proposed method is the first that addresses this specific task. - What happens if the non-traditional classifier is poor (e.g., overfitting, underfitting, trivial)? How exactly is the non-traditional classifier used to transform to a single dimension, i.e., is it done in a cross-validation setup? How do you optimize hyperparameters of the non-traditional classifier (specifically, which score function do you suggest using)? Overall, the practical details of using the non-traditional classifier should be explained better. - The use of regression data sets in the benchmark seems awkward and unnecessary. I suggest replacing them with other classification problems. [1] M. Claesen, F. De Smet, J. Suykens, B. De Moor, A robust ensemble approach to learn from positive and unlabeled data using SVM base models, Neurocomputing (2015) 73-84. http://dx.doi.org/10.1016/j.neucom.2014.10.081 [2] B. Frenay, M. Verleysen, Classification in the presence of label noise: a survey, IEEE Trans. Neural Netw. Learn. Syst. 25 (May (5)) (2014) 845â€“869. http://dx.doi.org/10.1109/TNNLS.2013.2292894, ISSN 2162-237X.