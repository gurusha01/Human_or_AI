The authors present a new sequence-to-sequence model architecture that processes fixed-size blocks of inputs using an encoder RNN and generates outputs block-by-block using a transducer RNN; the latter receives its inputs from the encoder RNN and maintains its state across blocks using recurrent connections. This class of models is aimed at online tasks like speech recognition that require predictions to be made incrementally as more data comes in. This paper is well written and the idea of using a blocked transducer is novel. On the TIMIT core test set, the proposed online model performs quite comparably (19.8% PER) to the best-reported offline sequence-to-sequence model (17.6% PER), without using very carefully trained models. Since this model is designed to alleviate attention issues that particularly affect long utterances in sequence-to-sequence models, it would be interesting to see how the performance of the proposed model varies as a function of utterance length. One suggestion to make room for this experiment in the paper would be to drop the addition toy task which can be omitted without losing the flow of the narrative. There are a couple of missing implementation-specific details that'll be useful for the reader to know:  In the comparison model (with a PER of 18.7%) that used a bidirectional encoder, were bidirectional features computed after seeing the end of an input block or after seeing the entire utterance? The latter would mean that the model is no longer offline. This should be clarified in the write-up.  What value of M was used in the experiments? (M appears in the DP algorithm used to compute alignments during training.) What beam search width was used during inference? As the authors have mentioned, the results reported in Table 2 could be improved by using better regularization techniques. Another way to potentially boost the performance of these models is to use better acoustic features (with speaker-dependent transforms). Lu et al., "Segmental Recurrent Neural Networks for End-to-end Speech Recognition", Interspeech 2016 shows significant PER improvements on TIMIT obtained by using better acoustic features. When the authors say that the alignments from the proposed model are similar to the GMM-HMM alignments, do they mean this in terms of per-frame phone error rates? Clearly, the models benefited from using the GMM-HMM alignments as evidenced by the final PER numbers (19.8% vs 20.8%). What could this be attributed to, if the alignments were very similar? Some minor edits: — pg.3, typo in "compute the probability of 1 compute" — pg.4, "is in computed in" —> "is computed in" — pg.8, "showed that is can" —> "showed that it can" — argmax should be a single operator and not "arg max" in Eqn 12. — use \mathrm for the softmax function in Eqn 8.