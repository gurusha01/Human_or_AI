The paper proposes LAND, a new parametric metric defined over a nonparametric space, by bridging concepts from metric learning/manifold learning and Riemannian statistics. The parametric metric learned is a Riemannian normal distribution as defined in Pennec [15]. The authors propose a maximum likelihood algorithm to estimate the mean and covariance of the Riemannian normal distribution, i.e. a Mahalanobis distance for Riemannian manifolds. Distances between points are defined as geodesics along the Riemannian manifold as in Hauberg et al. [9], and they rely on learning a local metric. Here, the authors propose to learn the metric by using the inverses of the local (diagonal) covariances. The authors also consider mixtures of LANDs. The paper brings new insights into the connection between metric learning and Riemannian statistics. Having parametric and generative models that can go beyond the training data is very useful in many scenarios. However, what I am mainly concerned is its applicability to real world data which is both large scale and high-dimensional. LAND requires the computation of a (here, diagonal) covariance matrix for each data point and of its inverse, which in high dimensions are known to be hard problems. The authors do mention high-dimensionality as a possible issue (lines 265-266), but the paper would need an analysis of the complexity and limitations of the metric introduced. How high is here high-dimensional? 3, 5, 10, 100 dimensions? The literature on metric learning is fairly vast and I find that the paper needs to be better fitted in this context for future readers. Hauberg et al. [9] already discuss some of the literature on metric learning, e.g., Frome et al. [1,2], Malisiewicz and Efros [6], who all learn a diagonal metric tensor for every point similar to LAND. Local covariances have also been used in methods like local PCA (Kambhatla â€“ Dimension reduction by local principal component analysis) even if not for every point. Lines 101-103: For learning the local metric, the authors use a Gaussian kernel to define locality. How does the parameter sigma influence the results? The choice of sigma is only discussed for the sleep data in Sect. 4.2., but not for the synthetic data in Sect. 4.1. The paper would benefit from robustness tests for sigma. Is sigma the same for all points, whether in low or high-density regions? Fig. 4 compares LAND also against intrinsic estimators and the results are fairly similar, but the authors never show visual results with these intrinsic estimators in Figs. 5 and 6. Instead, visualization results are shown against GMM which are already shown in Fig. 4 to perform badly. This is therefore less insightful. Some visual results using the intrinsic model are shown in the supplementary material (Fig. 2 and 4) and they outperform GMM. Therefore, these results should probably replace the GMM results in the initial paper or make room for both. It would be interesting to do an analysis of the computational complexity of LAND vs. intrinsic estimators to see which method should be favored over the other, and for which data structures. The authors also consider mixtures of LANDs. I think this is one of the most interesting applications of LAND, as data is rarely generated by one component. This aspect would be worth expanding. The paper is very well written, it is a pleasure to read.