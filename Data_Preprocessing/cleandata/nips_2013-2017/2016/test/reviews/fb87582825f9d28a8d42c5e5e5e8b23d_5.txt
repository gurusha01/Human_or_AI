The authors investigate the use of recurrent neural networks for learning optimal update strategies in the context of first-order optimization (i.e., they replace hand-crafted update strategies such as steepest descent with the output of recurrent networks). They use one LSTM per coordinate, with two variations that do not obtain significant gains in the experiments with respect to the basic version (using global averaging of some LSTM cells, and a read/write mechanism to an external memory). In three examples, they show that the networks can beat a few selected first-order strategies (e.g. ADAM), and possess a small amount of generalization to problems in similar classes. The benchmarks they use are (i) optimization of unregularized linear least-square; (ii) fitting a neural network to the MNIST dataset; and (iii) solving the optimization problems in the context of the 'artistic style transfer', which is used to fuse together two different images. From a conceptual point of view, the idea proposed in the paper is very interesting. It is simple to understand, but it opens for many possibilities, so in my opinion it is a perfect paper for the NIPS conference. Of course, from a practical point of view, the work in its current form is not easy to justify, and in this sense this is the major drawback. First-order optimization algorithms have two main advantages: (i) they can be applied to almost any (sub)differentiable problem; (ii) they are very cheap to implement once the gradients are known. The proposed approach looses both of these advantages: the network must be retrained even for very simple changes in the problem to be optimized (e.g., a change of activation function in the network); training is costly; and the memory/time overhead during the execution of many LSTM networks can be significant. In fact, if one has available all this computing power and time to spend, what is the rationale behind choosing a simple first-order procedure compared to second-order techniques? The method appears to be promising particularly for situations in which there is the need for solving the same optimization problem thousands of times over. In this sense, I guess the most interesting applications can come from outside the realm of machine learning. In fact, the only experiment actually involving the training of a neural network is (in my opinion) the least convincing of the three.