The authors propose a novel sequence-to-sequence model architecture that processes fixed-size input blocks using an encoder RNN and generates outputs incrementally, block-by-block, through a transducer RNN. The transducer RNN receives input from the encoder RNN and maintains its state across blocks via recurrent connections. This model class is particularly suited for online tasks, such as speech recognition, where predictions must be made incrementally as data arrives. The paper is well-written, and the concept of employing a blocked transducer is innovative. On the TIMIT core test set, the proposed online model achieves a performance of 19.8% PER, which is competitive with the best-reported offline sequence-to-sequence model (17.6% PER), despite not relying on extensively fine-tuned models. Given that the model is designed to address attention-related challenges in sequence-to-sequence models, especially for long utterances, it would be insightful to analyze how its performance varies with utterance length. To accommodate this experiment, the authors could consider omitting the addition toy task, which does not significantly contribute to the narrative.
There are a few missing implementation details that would benefit readers:  
* For the comparison model with a PER of 18.7% that uses a bidirectional encoder, were the bidirectional features computed after processing the entire input block or after processing the entire utterance? The latter scenario would imply that the model is no longer operating in an online manner, and this distinction should be clarified.  
* What value of M was used in the experiments? (M is a parameter in the DP algorithm used for computing alignments during training.) Additionally, what beam search width was employed during inference?  
The authors note that the results in Table 2 could be improved with better regularization techniques. Another avenue to enhance model performance could involve using improved acoustic features, such as those incorporating speaker-dependent transforms. For instance, Lu et al., "Segmental Recurrent Neural Networks for End-to-end Speech Recognition," Interspeech 2016, demonstrated significant PER improvements on TIMIT by leveraging better acoustic features.  
When the authors state that the alignments from the proposed model are similar to GMM-HMM alignments, are they referring to similarity in terms of per-frame phone error rates? The models appear to benefit from using GMM-HMM alignments, as evidenced by the final PER results (19.8% vs. 20.8%). If the alignments are indeed very similar, what might explain this performance improvement?  
Minor edits:  
— pg.3: Typo in "compute the probability of 1 compute."  
— pg.4: "is in computed in" → "is computed in."  
— pg.8: "showed that is can" → "showed that it can."  
— In Eqn 12, "argmax" should be written as a single operator rather than "arg max."  
— Use \mathrm for the softmax function in Eqn 8.