This paper introduces a method to accelerate CNN execution by applying group sparsity to various CNN parameters. The approach involves pre-training the CNN using a standard baseline, followed by retraining with group sparsity constraints applied to the pre-trained parameters. Inactive groups are subsequently pruned, and the final network undergoes fine-tuning without the sparsity constraints. This straightforward method achieves notable speed-ups with minimal accuracy loss and, in some cases, slight accuracy improvements. The use of group sparsity to deactivate redundant components of a CNN and enhance its efficiency appears to be a promising idea. The experimental results demonstrate significant speed-ups across diverse scenarios, with negligible accuracy degradation and occasional improvements. 
The authors apply group sparsity along several dimensions, such as the number of filters and channels, the filter shapes (though the process of efficiently deactivating specific filter sites is unclear and requires clarification), and the number of layers (leveraging shortcuts to maintain network connectivity). While the idea presented in the paper is relatively simple, it is both practical and potentially impactful. However, several critical details seem to be missing, unless I overlooked them. Specifically, how is the group sparsity optimization integrated into the CNN training process? Equally important, how are the regularization weights determined? Are they selected through cross-validation? For instance, Table 2 presents results for "different strengths of structured sparsity regularization," but the methodology behind these choices is not explained. These details are crucial, as achieving an effective trade-off between speed and accuracy is essential for practical deployment. Simply replicating baseline results on test data is insufficient for real-world applications. The authors should address these points to strengthen the paper.