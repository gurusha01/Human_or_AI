The authors present a neural network approach for learning to map input sequences to output sequences in an online manner. Similar to other models of this type, the proposed method employs an encoder to process the input and a decoder to generate the output sequence, leveraging the information encoded by the encoder and conditioned on its own prior predictions. The method is tested on a toy problem and the TIMIT phoneme recognition task. Additionally, the authors introduce smaller contributions, such as two variations of attention mechanisms. I find the topic of the paper compelling, as sequence/structured prediction using neural networks remains an open challenge, and I agree with the authors that there is a need for methods capable of online sequence processing and prediction. 
However, my primary concern lies in the use of the term "transducer," which has been extensively used in prior work on neural sequence prediction. The authors fail to cite or discuss this prior work, which is surprising given that they reference a paper where both CTC and the transducer from Graves (2012) are evaluated, with the transducer outperforming CTC. Graves' transducer also incorporates prior predictions while enabling dynamic programming-style inference, similar to CTC. While this prior method differs from the authors' approach in that it does not process input data in blocks, it should serve as an important baseline for comparison. The omission of this prior work (as well as related work, such as Boulanger-Lewandowski et al., 2013) diminishes the novelty of the proposed ideas and raises questions about the appropriateness of the paper's current title. A title like "An Online Neural Transducer" or "An Incremental Neural Transducer" might better reflect the contributions of the work.
I also find the empirical evaluation somewhat limited. While the results on TIMIT are reasonable, they do not clearly demonstrate the advantages of the proposed method over existing algorithms. The authors' argument that incorporating regularization methods to achieve optimal results was beyond the scope of the paper is unconvincing, especially since the related work they compare against also employs attention mechanisms and windowing. That said, I appreciated the comparison with a model that resets the RNN state between blocks and the analysis of the relationship between window size and attention mechanisms. Nevertheless, it would have been more compelling to include results on datasets with longer sequences, such as Wall Street Journal or Switchboard, where the proposed method might exhibit a more pronounced advantage.