This paper utilizes structured sparsity learning to reduce the parameters of a deep neural network, enabling faster computation with minimal accuracy loss. The authors propose a structured sparsity learning method to simplify or accelerate a trained deep network, making it suitable for deployment on platforms with constrained computational resources. The concept of learning a compact and hardware-efficient structure is intriguing. However, several aspects require further clarification: 1. What is the connection between the proposed approach and hardware? How can it generate CPU-, GPU-, or FPGA-friendly structures, respectively? Alternatively, how can the optimization objective be designed to achieve this? 2. Can the proposed method be applied to convert the network into a version that relies solely on integer operations? 3. How does the approach manage the trade-off between computational complexity and the accuracy of the simplified network? 4. What is the computational complexity of the proposed method itself?