This paper introduces three distinct approaches for predicting future video frames conditioned on an agent's actions in an unsupervised manner. Specifically, the proposed method forecasts object movements in videos without explicitly learning optical flow. The approach leverages a deep network architecture that integrates LSTM, dynamic neural advection, and spatial transformers. The authors conduct extensive evaluations on a novel dataset featuring robots manipulating objects (which they indicate will be released soon) and the Human3.6SM dataset. The results are benchmarked against state-of-the-art methods, demonstrating excellent performance. The paper is well-written, the methodology is thoroughly evaluated, and the quantitative results are compelling. Learning physical interactions from video is undoubtedly a challenging and impactful research area with significant implications for autonomous systems and beyond. However, despite the strong quantitative results, I find it challenging to fully assess the robustness of the approach due to the following concerns: 1) The still images presented in the paper are not particularly convincing. The predicted objects become blurry relatively quickly. A prediction horizon of 1 second is quite limited, and if the images already lose clarity at this stage, it raises questions about the practical utility of the method. That said, I was unable to review the videos as the provided link was not functional. It is possible that the videos present more compelling results. 2) The use of LSTM for prediction is not novel. How did the authors arrive at this specific architecture? Would increasing the number of LSTM layers extend the prediction horizon? Additionally, have the authors considered alternative architectures, such as differentiable long-term memory models (e.g., Neural Turing Machines)? 3) Regarding the dynamic neural advection component, it is unclear what exactly "M" represents. Is it a scalar value for each pixel? How is the motion parameterized in this context? 4) More mathematical details about each component of the architecture would have been highly beneficial. Such details would not only help readers better understand the modeling assumptions and limitations but also facilitate reproducibility of the results. Unfortunately, at the time of this review, the links to the videos and supplementary materials were unavailable.