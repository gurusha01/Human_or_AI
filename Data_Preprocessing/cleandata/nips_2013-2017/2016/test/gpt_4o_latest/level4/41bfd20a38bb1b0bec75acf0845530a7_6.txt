The paper introduces a thorough approach for sparsifying deep neural networks at the levels of channel, filter, filter shape, and layer. The proposed method appears highly practical. However, there are a few concerns that need to be addressed. First, the method should be compared against the state-of-the-art approach, group-wise brain damage (GBD) [Lebedev, 2015]. While GBD falls under the filter shape category as defined in this paper, it demonstrates greater performance gains (in terms of reducing floating point operations) than the results presented here, particularly for Conv1 in AlexNet. Second, the networks used in the experiments, such as LeNet and AlexNet for CIFAR-10, are predominantly over-parameterized and thus relatively easier to compress. It is suggested that more optimized architectures, such as GoogLeNet, SqueezeNet, and ResNet-152, be included in the experiments to better demonstrate the effectiveness of the proposed method.