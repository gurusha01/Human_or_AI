This paper addresses a common issue in CNN training: redundancy in filters, channels, depth, and even filter shapes within CNN structures. To mitigate this redundancy, the authors propose a structured sparsity learning approach that employs group lasso regularization to directly derive a compressed CNN structure. The grouping strategy considers channel-wise, filter-wise, shape-wise, and depth-wise dimensions. Experimental results demonstrate a significant speed-up compared to the baseline CNN model. However, the details of the training methodology are not sufficiently clear. The authors primarily present the optimization objective but do not elaborate on the parameter settings. Additionally, it is unclear why the model is trained using weights initialized from the baseline rather than from scratch, which suggests it might be more of a fine-tuning approach. While the authors explore different regularization methods by grouping channels, filters, and filter shapes, it raises the question of whether these methods could be combined for further acceleration or if the optimization process itself poses the main challenge. Another concern is that modern CNN architectures, such as VGG-16 and ResNet, predominantly use 3x3 filters, which are already highly localized. This brings into question the relevance of shape regularization in such cases.