This paper introduces several definitions for quantifying the complexity of a recurrent neural network. These measures include: 1) Recurrent depth: the degree of multi-layeredness over time in recursive connections, 2) Feedforward depth: the degree of multi-layeredness in input-to-output connections, and 3) Recurrent skip coefficient: the degree of directness (essentially the inverse of multi-layeredness) in connections.  
Beyond the definitions themselves, the paper makes two primary contributions:  
- The authors demonstrate that these measures (defined as limits as the number of time steps approaches infinity) are well-defined.  
- They establish correlations between these measures and empirical performance, showing that all proposed depth measures can contribute to improved performance.  
The proposed measures appear intuitively reasonable and potentially valuable. However, one could question their significance given that their utility is primarily demonstrated through empirical comparisons on real-world data, which are not entirely conclusive. In other words, the impact of these measures would be more compelling if formal guarantees could be provided as a function of these definitions, rather than relying solely on empirical evaluations. This raises the question of why these particular measures are more relevant than other possible complexity metrics.  
Lastly, it is somewhat regrettable that much of the technical detail is relegated to the appendix. However, this is a common practice in NeurIPS submissions, and the authors cannot be faulted for adhering to this trend.