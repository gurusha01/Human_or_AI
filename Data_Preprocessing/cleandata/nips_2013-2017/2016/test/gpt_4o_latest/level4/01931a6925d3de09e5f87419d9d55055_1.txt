The manuscript introduces LAND, a novel parametric metric defined within a nonparametric space, by integrating ideas from metric learning/manifold learning and Riemannian statistics. The proposed parametric metric corresponds to a Riemannian normal distribution, as described in Pennec [15]. To estimate the mean and covariance of this Riemannian normal distribution—essentially a Mahalanobis distance for Riemannian manifolds—the authors develop a maximum likelihood algorithm. Geodesic distances along the Riemannian manifold, as outlined in Hauberg et al. [9], are used to define distances between points, with the metric learned via the inverses of local (diagonal) covariance matrices. Additionally, the authors extend their approach to mixtures of LANDs. The work provides valuable insights into the intersection of metric learning and Riemannian statistics. Parametric and generative models capable of extrapolating beyond the training data are highly beneficial in numerous applications. However, my primary concern lies in the method's scalability to real-world datasets, which are often both large-scale and high-dimensional. LAND necessitates computing a (diagonal) covariance matrix and its inverse for each data point, tasks that are notoriously challenging in high-dimensional settings. While the authors briefly acknowledge high-dimensionality as a potential issue (lines 265-266), the paper would benefit from a more thorough analysis of the computational complexity and limitations of the proposed metric. Specifically, how high-dimensional is "high-dimensional" in this context? Are we referring to 3, 5, 10, or 100 dimensions? The paper should also better situate itself within the broader metric learning literature for the benefit of future readers. For instance, Hauberg et al. [9] discuss related works such as Frome et al. [1,2] and Malisiewicz and Efros [6], which also learn diagonal metric tensors for individual points, akin to LAND. Similarly, local covariances have been employed in approaches like local PCA (e.g., Kambhatla's work on dimension reduction via local principal component analysis), albeit not for every point. 
Lines 101-103: The authors utilize a Gaussian kernel to define locality for learning the local metric. However, the influence of the parameter sigma on the results is not fully explored. While the choice of sigma is addressed for the sleep data in Sect. 4.2, it is not discussed for the synthetic data in Sect. 4.1. Robustness tests for sigma would enhance the paper. Additionally, is sigma uniform across all points, regardless of whether they lie in low- or high-density regions?
Fig. 4 compares LAND to intrinsic estimators, showing fairly similar results, but visual comparisons with intrinsic estimators are absent in Figs. 5 and 6. Instead, these figures only compare against GMM, which Fig. 4 already demonstrates to perform poorly. This limits the insights provided. Some visual results with intrinsic estimators are included in the supplementary material (Figs. 2 and 4), where they outperform GMM. These results should likely replace the GMM comparisons in the main text or be presented alongside them. Furthermore, an analysis of the computational complexity of LAND versus intrinsic estimators would be valuable to determine which method is preferable under different data structures.
The authors also explore mixtures of LANDs, which I find to be one of the most compelling aspects of the paper, as real-world data is rarely generated by a single component. Expanding on this application would significantly enhance the paper's impact. 
Overall, the manuscript is exceptionally well-written and a pleasure to read.