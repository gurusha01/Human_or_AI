The paper addresses the problem of online learning with \( K \) actions under both full and bandit feedback, focusing on the concept of dynamic regret—regret measured against the best comparator that can switch actions in each round. The primary contribution lies in analyzing the influence of the total variance of the loss vectors on dynamic regret, with the authors deriving several novel upper and lower bounds in terms of this quantity. The paper is well-structured, and the results appear to be original. While many of the technical tools employed in the proofs are relatively standard, their combination presents non-trivial challenges, and the analysis is executed with precision.
However, I find the results to have somewhat limited practical appeal. None of the proposed algorithms seem particularly implementable in real-world scenarios, and the bounds do not provide fundamentally new insights into the complexity of learning in the given settings. Additionally, I have some concerns about the results. Specifically, the newly introduced quantity \( \Lambda \) seems to scale linearly with the number of arms/experts \( K \), which could result in worse scaling compared to existing bounds (e.g., \( K^{1/3} \) instead of \( \log K \) in the full-information setting). Furthermore, the lower bounds in Theorem 4.3 initially appear to conflict with known upper bounds, but this seems to be a matter of imprecise phrasing. The theorem should clarify that the statement does not hold in a problem-dependent sense but rather in the sense that "for any fixed choice of \( \Lambda \), \( T \), and \( V \), there exists a sequence on which any algorithm incurs a regret of \( \Omega(...) \)." Including a discussion on this point would help avoid confusion for other readers.
In summary, while the results are not groundbreaking, I have no objection to the paper being accepted for publication.
---
Detailed Comments
- Line 050: Introducing the notion of "dynamic regret" after presenting the bounds feels awkward. Consider reordering this section to define regret first.
- Line 061: The informal definition of dynamic regret is repeated here. Consider consolidating these explanations.
- Line 089: Your bound does not fully recover prior results, as it exhibits worse scaling in \( V \) (\( \sqrt{V} \) vs. \( V^{1/3} \)). To address this, clarify that the asymptotics of \( V \) are tied to those of \( T \).
- Line 111: In "choose an action \( at \) and then suffer a corresponding loss \( \ell{t,i} \)," consider replacing \( \ell{t,i} \) with \( \ell{t,a_t} \) for clarity.
- Line 146: Add a citation for vUCB (Audibert, Munos, and Szepesvári, 2009?).
- Theorem 3.1: Since both full-information and bandit settings are considered, explicitly state which setting each theorem pertains to.
- Equation (5): This algorithm does not exactly match the one in [4]; it is more closely related to the Bernstein Online Aggregation algorithm (Wintenberger, arXiv 2014) or the Variation MW algorithm (Hazan and Kale, COLT 2008).
- Theorem 3.4: Note that improved bounds can be achieved using the algorithm of Steinhardt and Liang [10], which depends on the path length of the best expert rather than the \( L_1 \) path length of the loss vectors. However, this algorithm is not parameter-free.