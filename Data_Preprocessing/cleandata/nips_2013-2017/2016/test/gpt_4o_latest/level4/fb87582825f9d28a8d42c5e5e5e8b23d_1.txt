The authors present a method for learning update functions in gradient-based optimizers. By simultaneously training an optimizer and an optimizee, the parameters of an LSTM-based deep neural network can be learned to aggregate information from multiple gradients, akin to momentum. Once trained, the optimizer can be reused for similar tasks and exhibits partial generalization to new network architectures. Although the theoretical properties of the resulting algorithms remain unexplored, several small-scale empirical experiments indicate that the proposed optimizers are competitive with state-of-the-art methods. I found this paper to be highly creative, timely, and engaging, with the potential for significant impact. The concept of "learning algorithms" represents an important frontier in deep learning, and this work appears to be a promising step in that direction. While the experimental results were adequate, I would have preferred to see evaluations on larger-scale benchmarks. For instance, it would be compelling to know whether this approach could contribute to training a model on the scale of AlexNet. Additionally, the post-hoc analysis could have been more insightful, shedding light on the strategies the learned optimization networks are employing. Understanding the signals they leverage might inspire new theoretical avenues for the optimization community. Although I was pleasantly surprised that the proposed algorithms outperformed state-of-the-art alternatives, the margin of improvement was smaller than expected. This outcome may suggest that existing optimization algorithms are already highly effective, reflecting the substantial progress made by the optimization community. Finally, I would have liked to see this method applied to a problem that was previously unsolvable, demonstrating its unique capabilities.