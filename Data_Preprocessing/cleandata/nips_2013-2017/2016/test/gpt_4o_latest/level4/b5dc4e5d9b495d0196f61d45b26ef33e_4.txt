This paper presents a phased approach to implementing Nesterov's smooth minimization technique for non-smooth functions, utilizing progressively improved strongly convex regularizers for the dual to achieve faster convergence for a specific class of optimization problems. The improvement arises from the ability to use the iterate obtained at the end of one phase to warm start the next phase. This is supported by a straightforward lemma that bounds the norm distance to sublevel sets in terms of value distance. Combined with a local error bound condition, this leads to improved convergence for certain problems. I find this idea quite elegant. It is an intuitive approach, yet rigorous analyses for such a "homotopy" method have been lacking until now. The paper is well-written and provides a gentle introduction to the topic, beginning with a clear exposition of Nesterov's smoothing technique. It would be valuable to explicitly state the precise iteration bounds for optimizing Hoffman's bound and for cone programming. Linear convergence for these problems is very exciting, though I am unsure if it is novel; it would be helpful to clarify whether similar results have been achieved using first-order methods in prior work.  
update I continue to appreciate the paper and maintain my scores. A brief Google search indicates that this is not the first claim of a first-order method for LP with linear convergence. Additionally, it remains unclear how various problem parameters influence the iteration count. I recommend a careful analysis of such instances before making definitive claims in this regard.