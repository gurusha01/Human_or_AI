This paper introduces a method to model the observation space using a hand-crafted Riemannian metric and defines a curved normal distribution grounded in the associated Riemannian geometry, aiming to better capture the data manifold. The work tackles a fundamental challenge: how to construct a Riemannian metric from high-dimensional real-valued observations and subsequently define and estimate a curved Gaussian distribution based on this geometry. This approach provides a valuable tool and an intriguing application of Riemannian geometry in statistical learning. The first component of the proposed method, outlined in Section 3.1, establishes the underlying Riemannian geometry. While the title implies that the metric is "learned," it is, in fact, hand-crafted in Equation 7 using two hyperparameters, ρ and σ, which are empirically fixed beforehand. It appears that σ is a particularly sensitive parameter. The paper would benefit from a more detailed discussion of the intuitive effects of varying σ and guidance on how to select its value. Without this discussion, the paper feels incomplete. Could the authors consider integrating the metric definition stage with the LAND learning stage to allow ρ and σ to be learned jointly? 
One of the appealing aspects of the geometric approach is its invariance to coordinate transformations. However, the metric in Equation 7 lacks this invariance. For instance, if the data forms a line in a 2D observation space, the resulting metrics would differ significantly between a coordinate system where the x-axis is aligned with the line and one where the coordinate axes are rotated, say by 45 degrees. A deeper discussion on this point would be valuable, including whether the full (non-diagonal) metric would exhibit invariance and providing more intuition about the metric in Equation 7. Additionally, there is a rich body of literature on metric learning and manifold learning in the context of Riemannian geometry that could be more explicitly connected to this work (e.g., searching for "Riemannian," "metric learning," and "manifold learning" in the literature).
A primary criticism lies in the first experiment, where the comparison is not particularly meaningful. It is evident that the more flexible LAND can better describe the data with fewer components. However, Gaussian Mixture Models (GMM) can achieve better performance with a larger number of components, where LAND might overfit. The authors are encouraged to redesign the experiment to allow each method to select its optimal number of components based on a criterion such as BIC, and then compare their respective performances. While LAND is still expected to outperform due to its ability to better capture manifold structures, this would result in a more meaningful comparison. It would also be insightful to observe how LAND behaves with a large number of components and whether it overfits. 
Additionally, LAND is evidently more computationally expensive than estimating a Gaussian. Are there any quantitative measurements or experiments to clarify the computational cost of LAND? What is the largest scale of data that LAND can handle? For instance, could LAND be applied to MNIST, and how would its performance compare to Gaussian mixture learning?
=====
After reviewing the authors' response, it is clear that LAND is a more flexible model than the Gaussian distribution. However, increasing the number of components makes each local LAND more susceptible to local noise compared to a local Gaussian. While the authors argue that "the geodesics will almost be straight lines," they remain curves, which can lead to overfitting. I strongly recommend that the authors revise this experiment in their next submission.