Summary: The paper introduces a locally adaptive normal distribution (LAND) that leverages a geodesic manifold traversal distance, which is adaptively learned from the data. This geodesic distance serves as a replacement for the conventional Mahalanobis distance in the Gaussian distribution. Additionally, the authors extend the approach to a mixture of LANDs. The method's performance is demonstrated on both a synthetic dataset and a real-world EEG dataset.
Major comments: The concept of employing a geodesic distance, akin to Isomaps, is quite intriguing. However, there are certain areas where the paper lacks clarity. First, the definition of the inverse of the metric tensor in Eq. (7) appears somewhat arbitrary. Could the authors provide more intuition behind the use of a Gaussian kernel to compute the weights in the weighted sum of local covariances? Furthermore, I found the transition between the definition of the local covariance matrix in Eq. (7) and its MAP estimation in Eq. (11) confusing. Are these equations referring to the same quantity? Additionally, the experimental evaluation is relatively weak. My primary concern, however, lies in the scalability and computational efficiency of the proposed method. For instance, how does the computational complexity and runtime compare to the standard EM algorithm? While the paper presents an interesting theoretical contribution, its practical applicability seems limited.
Minor comments: In Algorithm 1, lines 4 and 5, the symbol "d" should be replaced with "\nabla".