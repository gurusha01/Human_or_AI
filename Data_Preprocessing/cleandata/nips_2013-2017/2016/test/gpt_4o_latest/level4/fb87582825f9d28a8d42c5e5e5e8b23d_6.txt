In this paper, the authors propose training an LSTM to function as a diagonal gradient-based optimizer. They demonstrate the system's ability to optimize quadratic functions, small MLPs, and perform style-transfer on images more quickly than mainstream optimizers. The topic is both interesting and significant, but the paper itself feels underdeveloped.
Weight of Individual Timesteps: The authors assign a weight of 1 to each timestep, which effectively means the system minimizes the integral of the error rather than the final error. While this approach can promote rapid convergence early on, it may also cause the system to overly prioritize initial optimization phases, where most of the improvement occurs. This issue is central to the broader question raised in the paper, "What makes a good optimizer?" and warrants a more explicit discussion.
Key Questions and Concerns:
- Training Details: How many optimization runs were required to train the LSTM-based optimizer?
- Short Optimization Time: The optimizers are only run for 1/5 of an epoch on MNIST. Why was such a short time chosen? Figure 2 (right) suggests that the method generalizes to longer optimization horizons. If this is the case, why not demonstrate it? Conversely, if it does not generalize, this limitation should be clearly shown.
- Runtime and Memory: The paper does not discuss runtime comparisons with standard optimizers like SGD or ADAM. I assume memory consumption is a major limitation, as the LSTM optimizer must store approximately 160 states per parameter per step during (meta-)training. This should be explicitly addressed.
- Generalization: The most critical question is how well the trained optimizer generalizes to different architectures and datasets. However, the paper only explores three minor architectural changes without providing error bars. A systematic investigation, such as how hidden layer size impacts optimizer performance, seems straightforward to conduct and is notably absent.
- NTM-BFGS Optimizer: The inclusion of the NTM-BFGS optimizer adds little value. It is only briefly mentioned and evaluated on cubic functions, where its performance is identical to the LSTM-GAC. I recommend removing it.
Minor Issues:
- Line 97: The gradient of f should be corrected.
- The sentence "... was optimized for 100 steps and the trained optimizer was unrolled for x steps." appears multiple times and is unclear. I assume this refers to the optimizer being unrolled for x steps during training.
- Line 193: Over how many functions were the results averaged?
- Figures lack axis labels, and short titles would improve clarity.
- The pre- and postprocessing steps are handcrafted and tuned for each problem, which seems to contradict the paper's core premise of reducing handcrafting in optimization.
- The paper does not explore alternative optimizer architectures. How critical is the use of LSTM? What about using two layers or 20 units?