The paper addresses a sequential decision-making problem with stochastic rewards constrained within [0,1], considering both the full-information and bandit-information frameworks. The mean reward of each arm is allowed to vary over time, leading to the definition of instantaneous regret at time \( t \) relative to the best action at that moment. Consequently, the cumulative regret is defined with respect to a strategy that may select a different arm at each time step. The rewards are assumed to be independent across both arms and time. The paper introduces three quantities that characterize the complexity of the environment: the number of times the mean reward vector changes (\( \Gamma \)), the cumulative drift measured in infinity norm (\( V \)), and the cumulative variance of the reward distributions (\( \Lambda \)). In this non-stationary setting, the study examines multiple algorithms. Algorithm 1 operates in the bandit setting and assumes perfect knowledge of \( \Gamma \), \( \Lambda \), \( V \), and the time horizon. Algorithm 2, with similar assumptions, is designed for the full-information setting. Algorithm 3, on the other hand, is parameter-free and functions in the full-information setting. It draws direct inspiration from the works of [3] and [4]. The regret of Algorithm 3 is analyzed in Theorem 3.5, while its static regret against any fixed arm is addressed in Theorem 3.4. Section 4 establishes lower bounds on regret in terms of \( \Gamma \), \( \Lambda \), and \( V \).
After reviewing the authors' feedback, I find this paper to be a valuable contribution overall. While Algorithms 1 and 2 are less compelling, they serve as a useful warm-up. Algorithm 3, however, is more intriguing. It employs a straightforward yet effective combination of ideas from [3] and [4], and its regret performance is particularly noteworthy in light of the established lower bounds. That said, it would be beneficial to include explicit constants in the main body of the paper for clarity. I also disagree with the assertion that the regret is "constant" with respect to time, as the quantity \( \Lambda \) typically scales with \( T \), even when \( \Gamma \) and \( V \) are constants. This claim should be refined to reflect this nuance. Additionally, it would be interesting to explore extensions of this work to the parameter-free bandit setting. While \( \Gamma \) is defined as the number of changes in the mean reward vectors, another potentially relevant quantity is the number of changes in the optimal arm, which can be significantly smaller than \( \Gamma \). Could this alternative measure be incorporated into the analysis, or is there a compelling reason to avoid its use?