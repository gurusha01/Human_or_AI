The manuscript introduces three deep learning approaches for predicting future frames of a scene. The authors propose learning a distribution of pixel values in the prior scene for each pixel. The first approach explicitly predicts pixel movements, while the other two implicitly estimate object movements. The network incorporates multiple masks, including one for the background. The methods are evaluated on pushing data collected using multiple robots as well as the Human3.6M dataset. While the paper references appendices and provides a link to videos, the link is non-functional, and the appendices are neither included in the main document nor the supplementary material. My review is based solely on the provided content. 
The descriptions of the three methods require further elaboration. The explanations rely heavily on Fig. 1, assuming that the network structure clarifies most details. However, I found this assumption to be unfounded. For instance, what is the structure of the m/M? Do they adapt to local image patches, or are they uniform across the entire image and dependent on the masks? Including a visualization of an m/M would be beneficial. Additionally, can these components be interpreted? 
The dataset collected using the robot setup is substantial, and the proposed methods outperform the state of the art. Nonetheless, the paper lacks a discussion of the methods' limitations. For example, why is the image resolution restricted to 64x64 pixels? Could higher resolutions be utilized? The pushing task primarily involves planar movements parallel to the image plane. Could the methods handle more complex 3D motions, such as a can tipping over? For Fig. 7, the authors should consider including an example of a larger rotational motion from the Human3.6M dataset. 
The qualitative results also leave room for improvement. The predicted images exhibit significant blurring. While video prediction is undoubtedly a challenging task, and perfect results are not expected, the authors should discuss the sources of these errors and propose potential strategies for addressing them in future work. Additionally, the authors could better leverage the robot setup for evaluations. For instance, can the robot grasp a displaced object based solely on the predicted image? Is the prediction quality sufficient for such tasks?
The MSE criterion is based on optical flow computed between the ground truth and predicted data. Including an example image illustrating the optical flow for both would be helpful. How does the optical flow handle the observed blurring? Are there scenarios in which one of the proposed methods is preferable to the others? The authors could also consolidate some of the plots to create space for additional discussions. For example, how does performance change when the background masks are omitted?
Overall, the paper presents an intriguing approach to a highly challenging task. The authors also introduce a large new dataset for video prediction. The network design is impressive and appears well-conceived. I commend the authors for their efforts. However, the quality of the work is not fully reflected in the paper. Providing more detailed descriptions and a thorough discussion of the methods' limitations would enhance the paper's accessibility and significantly increase its long-term impact.