The authors present an encoder-decoder architecture that incorporates attention over input blocks and employs a variable-length decoder structure. The encoder is implemented as a multi-layer LSTM RNN, while the decoder is an RNN conditioned on weighted sums of the encoder's final layer and its own prior output. The attention mechanism varies, being conditioned either on hidden states or prior attention vectors. The decoder generates a sequence of symbols until a special end character "e" is produced, signaling a transition to the next block. Alternative mechanisms were also explored, such as omitting the end-of-block symbol or separately predicting block termination based on the attention vector. Subsequently, the decoder processes the weighted sum of the next block of encoder states. The resulting symbol sequence determines an alignment of target symbols over input blocks, with each block potentially assigned a variable number of characters. Training involves fixing an alignment that approximates the optimal alignment, which is computed using a beam-search-like procedure with a beam size of M (line 169) and a restricted symbol set conditioned on the last symbol of each hypothesis (since the target sequence is known). Alignments are updated less frequently than model updates, typically every 100 to 300 sequences. During inference, an unconstrained beam-search is performed with thresholds on sequence length and beam size.
Overall, the paper is well-written and clear. The primary contribution appears to be the introduction of a novel model architecture, as suggested by the title. Prior work has applied attention mechanisms to speech recognition ([2], [5]), and sequence transduction was introduced by Graves in "Sequence Transduction with Recurrent Neural Networks" (2012). The follow-up paper, cited as [7], extends this transduction method. Graves describes his transducer as "extending CTC by defining a distribution over output sequences of all lengths and jointly modeling both input-output and output-output dependencies." The novelty of this paper seems to lie in applying the transducer on a block-by-block basis and defining an RNN conditioned on its last definite output. While other methods integrate over alignments using an RNN's output, the authors propose a distinct and complex network structure for transduction. However, Graves and Hinton also employ a separate prediction network in their transduction framework. A potential drawback of the proposed method is its complicated training procedure, as exact constrained inference becomes intractable. Additionally, I am uncertain about the novelty of this approach, as I may not be fully aware of all relevant literature in this rapidly evolving field. The method appears quite similar to Graves and Hinton [7], particularly since they also use a separate prediction network, with the main distinction being the attention mechanism. However, experimental results suggest that the attention mechanism does not consistently provide significant benefits unless larger window sizes are used, which might offer advantages such as speed.
The experimental evaluation includes a toy addition task and the TIMIT dataset. For the addition task, the authors focus on adding two three-digit numbers. It would be interesting to evaluate how well the model generalizes to longer numbersâ€”for instance, whether a model trained on three-digit additions can handle four-digit additions. On TIMIT, the authors compare models with and without recurrent state across blocks. If the transducer lacks recurrent state, it might enable more efficient inference using Graves' transduction method, though I am uncertain about this point. Figure 3 indicates that for certain block sizes, the no-attention model performs comparably. In this case, no-attention likely means that encoder hidden states are averaged per block and fed into the transducer, resembling a convolution with window size W and stride W. It might be worth exploring how the encoder performs if W states are combined and passed to a simple RNN that produces a softmax over symbols, trained using CTC or Graves' Transducer. The best reported performance of Graves' Transducer on a unidirectional LSTM RNN is 19.6%, which is close to the 19.8% achieved by the proposed model. However, the authors note that their model requires further tuning, which can be a time-intensive process. Their best-performing network uses a three-layer LSTM RNN transducer, and adding a fourth layer could lead to overfitting on TIMIT. It would be valuable to assess how much these models overfit.
In summary, while the model is competitive on TIMIT, its complexity and similarity to existing methods raise questions about its overall impact. Simpler methods achieve comparable performance, and it is unclear whether the additional capacity and context provided by the proposed model are necessary for this dataset. Training on larger datasets might clarify its scalability, but the training time required for this model is also a concern. I would appreciate a more detailed comparison with Graves' and Hinton's Transducer network to better understand the distinctions. Overall, I am uncertain about the novelty and broader significance of this approach.