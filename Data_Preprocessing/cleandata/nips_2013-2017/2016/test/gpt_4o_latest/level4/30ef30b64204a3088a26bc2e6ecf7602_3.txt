The paper introduces and explores a straightforward approach for deep semi-supervised learning. The core idea involves imposing additional constraints derived from unlabeled data, ensuring that the deep representation of a data sample remains as invariant as possible under nuisance transformations. This concept is rigorously validated and demonstrated to achieve state-of-the-art performance across multiple datasets. The proposed idea is remarkably simple and is concisely explained in half a page (top half of page 3), while the majority of the paper focuses on its validation, which is an appropriate emphasis. The experimental results are exceptionally strong, showcasing significant improvements in semi-supervised learning on a variety of standard datasets. The impressive absolute performance is partly attributed to the use of advanced state-of-the-art architectures. However, the authors carefully highlight the gains achieved over a baseline that either omits the proposed augmentation or employs a competing method (mutual exclusivity). 
In terms of novelty, the authors could have better acknowledged the connection to [31], as their approach appears closely related (notably, [31] experimented with up to 32,000 surrogate classes, which is conceptually similar to the method proposed here). The approach also shares similarities with tangent-distance methods [Simard et al., 1998]. Another clear link is to "slow feature analysis," which has been explored and applied to ConvNets in prior work. Additionally, a comparable technique was employed by [Kulkarni et al., NIPS 2015] (see Section 3.2 of their paper). Consequently, the idea is not as novel as the submission suggests. Nonetheless, this is offset by the thorough experimental validation and extensive comparisons provided in the paper.