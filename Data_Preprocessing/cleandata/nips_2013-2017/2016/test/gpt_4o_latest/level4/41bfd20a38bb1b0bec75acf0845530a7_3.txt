The paper introduces the application of Graph Lasso for weight pruning while preserving the 4D tensor structure. Specifically, the method focuses on pruning entire channels, filters, or even entire layers of weights, leveraging techniques like the "shortcut" mechanism commonly used in architectures such as residual networks. The rationale behind this approach is twofold: it aligns better with the semantic structure of the model and mitigates the computational overhead associated with truly sparse weight matrices, particularly in convolutional layers, where sparsity patterns can be highly irregular. However, the use of Graph Lasso for weight pruning lacks sufficient novelty to warrant acceptance at NIPS. While stronger experimental results could have compensated for this, the reported outcomes are underwhelming.