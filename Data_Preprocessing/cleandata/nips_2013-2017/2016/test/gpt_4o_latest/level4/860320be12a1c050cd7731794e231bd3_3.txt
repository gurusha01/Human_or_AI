This paper presents a solid theoretical contribution. The authors initially introduce a rigorous graph-theoretic framework that characterizes the connectivity architectures of RNNs in general, which they effectively use to explain the process of unfolding an RNN. Subsequently, the authors propose three tree architecture complexity measures for RNNs: the recurrent depth, the feedforward depth, and the recurrent skip coefficient. Experimental results across various tasks demonstrate the relevance of specific measures to specific tasks, suggesting that these three complexity measures could serve as valuable guidelines for designing recurrent neural networks tailored to particular applications. 
1. On line 59, should it instead be $i \in {0, 1, 2, ..., m-1}$?  
2. In the caption of Figure 1, it would be helpful to include a brief explanation of how $dr$, $df$, and $s$ are computed for the networks shown in (b).  
3. On line 155, I believe it should be $df = sup{i, n \in \mathbb{Z}}(\mathfrak(D)i^{\ast}(n) - n \cdot dr)$, i.e., you need to add parentheses "()" for the sup?  
4. On line 167, similarly, do you also need to add parentheses "()" for sup and max?  
5. I noticed that you conducted experiments on NLP and image classification tasks. It might be beneficial to include experiments on speech recognition tasks as well, given the extensive use of recurrent neural networks in that domain. This addition could broaden the paper's appeal to a wider audience. Incorporating speech recognition results should be relatively straightforward nowadays. For instance, you could explore resources such as Kaldi or Librispeech, which provide existing recipes for various neural networks and even include results. You would primarily need to perform the analysis.