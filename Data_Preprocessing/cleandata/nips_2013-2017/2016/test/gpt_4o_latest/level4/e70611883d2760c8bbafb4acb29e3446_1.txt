The paper introduces a method for learning kernels using random features. The authors propose a two-step approach: first, a set of random features is sampled, and then their weights are optimized to maximize alignment with the target kernel. They demonstrate how to efficiently optimize the resulting model and provide theoretical guarantees for consistency and generalization performance. The approach is evaluated on several benchmark problems. The proposed method appears to be a valuable contribution to the kernel learning literature. Notably, its ability to learn an improved kernel when the original data is poorly aligned with the target kernel (Section 4.1) is particularly promising. However, I am less impressed with the feature selection experiments, as the comparison involves a baseline that selects features entirely at random—more advanced feature selection methods are available. The speedup achieved compared to the joint optimization method is also noteworthy. The paper's presentation, however, could be clearer, as the write-up is somewhat convoluted and requires multiple readings to fully grasp.  
Details:  
- Line 64: Please clarify the assumptions made about the feature function \(\phi\).  
- Line 76 onward: The notation could be improved, for example, by explicitly indicating the dimensionalities of objects such as \(W\).  
- Equation (6): The appearance of the square root here is unclear—please elaborate.  
- Line 169: It is not evident why all of this follows from selecting the Gaussian kernel. For instance, is the exact form of the feature function chosen independently?