Review: Assume we have samples from two mixtures of a pair of (unknown) distributions with differing mixing coefficients:  
\(\mu = \alpha \mu1 + (1-\alpha) \mu0\)  
\(\nu = \beta \mu1 + (1-\beta) \mu0\).  
This paper proposes methods to estimate \(\alpha\) and \(\beta\). The primary application discussed is PU-learning, where \(\mu1\) represents the density of positive samples, \(\mu0\) the density of negative samples, \(\mu\) the density of unlabeled samples, and \(\nu\) the density of positive-labeled samples, with a mislabeling probability of \(1-\beta\) (e.g., if \(\beta=0.9\), 10% of the positive-labeled samples are actually mislabeled negative samples). The paper introduces one nonparametric method and one parametric method, both of which are evaluated through simulations.
First, the positive aspects: this paper tackles an important problem and offers a solution. It is well-written, and the work appears to be technically sound. However, my primary concern is that this work seems to be a relatively minor extension of [1] (which is evidently authored by the same group):  
- The theoretical contributions appear to overlap significantly with [1].  
- The main algorithmic contribution, AlphaMax-N, is a slight modification of AlphaMax from [1]. While the authors also propose a parametric algorithm, MSGMM-T, it is generally not competitive.  
- The simulations rely on the same UCI datasets as [1], albeit with added noise.  
I must acknowledge that I did not have the opportunity to conduct a detailed comparison with [1], so my assessment may be incorrect. I suggest that the authors explicitly clarify which aspects of this work are novel and which are closely related to or reformulations of results from [1].
A secondary, less significant concern is that the paper discusses several real-world applications, such as Facebook likes and protein interaction learning, but none of these datasets are utilized in the experiments. If the problem indeed has numerous practical applications, obtaining and using such datasets would strengthen the paper.
In my opinion, while this is a solid paper, it lacks sufficient novel contributions to merit acceptance at a top-tier conference like NIPS.  
Suggestions for improvement:  
1. Section 3 is dense with notation and technical results, making it somewhat challenging to follow. However, the results themselves are intuitive and appealing once understood. Including a few illustrative figures with mixtures of simple distributions (e.g., discrete multinomial) could significantly enhance clarity.  
2. It would be valuable to derive additional theoretical results for AlphaMax-N, such as how the error in estimating mixing proportions scales with increasing sample size (potentially under strong assumptions).  
3. The authors should make their code publicly available to facilitate its use by others.  
[1] "Nonparametric semi-supervised learning of class proportions" by Jain, White, Trosset, and Radivojac (2016).