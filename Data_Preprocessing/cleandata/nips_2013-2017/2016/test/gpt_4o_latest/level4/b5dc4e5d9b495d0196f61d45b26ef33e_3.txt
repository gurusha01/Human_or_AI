The paper investigates the composite optimization problem of minimizing \( F = f + g \), where neither \( f \) nor \( g \) is required to be smooth. The study assumes that \( g \) has an efficient proximal operator and examines \( f \) with the general structure \( f(x) = \max_u (\langle Ax, u \rangle - \phi(u)) \). The proposed approach involves iteratively minimizing smoothed versions of \( f \), where the smoothing parameter decreases exponentially. The primary contribution, Theorem 5, demonstrates that under a \(\theta\)-Local Error Bound (LEB) condition (with \(\theta \in (0,1]\)), the number of iterations required to optimize the smoothed versions is \( O(\epsilon^{\theta-1}) \). For \(\theta = 1\) (e.g., absolute value, hinge loss), this results in linear convergence in total iterations, while for \(\theta = 0.5\) (e.g., strongly convex functions), the rate improves to \( 1/t^2 \). The paper also explores various conditions that lead to the LEB property, such as the KL property and hinge loss ERM (as in SVM). Experimental results are provided to compare the proposed method with other first-order optimization techniques.
Technical Quality:  
- The experiments are well-executed and provide interesting insights, though there is a notable limitation discussed below.  
- The proof of Theorem 5 is clearly detailed in the supplementary material, and the corresponding elements in the main paper (theorem, definitions, examples) are sound and well-presented.
Novelty:  
While I am not deeply familiar with the exact literature, I assume the authors' claims regarding novelty are accurate. If so, the derived rates under the proposed setting are both new and non-trivial. It is worth mentioning that prior work, such as "Optimal Black-Box Reductions Between Optimization Objectives," has explored gradual smoothing for faster rates, though the assumptions and results differ significantly.
Impact:  
The paper demonstrates how LEB and KL properties can enable faster convergence rates for problems where both \( f \) and \( g \) are non-smooth. This has the potential to inspire further research into methods that adapt to such data-dependent parameters. The impact could be enhanced:  
- By analyzing the computational complexity for ERM problems in greater depth, for instance, by replacing APG with an SVRG variant.  
- By focusing on representative performance. The experiments currently report the best HOPS performance across several seemingly arbitrary parameter settings for \( t \), which is not practical for real-world use. If PD-HOPS avoids this issue entirely, it should be emphasized, and its limitations should also be discussed in the main paper (not just in the supplementary material).
Clarity and Presentation:  
The sketch of Theorem 5 is insufficiently clear to be useful. The key point appears to be the role of \( x^\dagger_{s-1,\epsilon} \) and its interaction with Lemma 1, but the induction is neither well-explained nor particularly compelling.  
- Line 231: Typoâ€”\( \min_{x \in \mathbb{R}^d} F(x) \) is a scalar and does not have an epigraph. The "min" should be removed.