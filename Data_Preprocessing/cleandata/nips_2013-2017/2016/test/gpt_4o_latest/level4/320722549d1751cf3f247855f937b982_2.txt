The submitted paper investigates the online convex optimization (OCO) framework with a decision set defined as either (i) a Hilbert space or (ii) the probability simplex. The authors propose an innovative reduction of this problem to a coin betting framework, for which optimal and straightforward algorithms are well-established. Specifically, they demonstrate that the Krichevsky-Trofimov strategy (or its generalizations), which is optimal for coin betting, yields parameter-free algorithms in both (i) and (ii). By "parameter-free," the authors mean that the learning algorithm does not require tuning based on the unknown norm of the competitive vector (in case (i)) or the unknown KL divergence of a good weight vector relative to the prior (in case (ii)). This generalized framework extends prior work on parameter-free algorithms in either (i) or (ii).
I hold a very positive opinion of the paper. It is technically robust, exceptionally well-written, and contributes to the generalization of existing algorithms for Online Linear Optimization (OLO) and Learning with Expert Advice (LEA) through a novel reduction to a coin betting problem. This work is likely to attract significant interest within the online learning community. That said, I find the coin betting reduction somewhat unintuitive, particularly in the context of LEA (see Section 4.2). Nevertheless, the generality and simplicity of the resulting algorithms make this approach compelling.
Regarding the term "parameter-free," the authors assume that all gains are bounded by 1 (in Euclidean norm for OLO or sup norm for LEA). Now, consider the scenario where all gains are instead bounded by an unknown value \( b \). I would like the authors to address the following questions:
1. It seems plausible to rescale all gains by \( b \) and apply the proposed framework to the rescaled gains, thereby obtaining regret bounds proportional to \( b \). However, I suspect that the resulting outputs \( w_t \) might depend on the unknown value of \( b \). Is this correct?
2. For the specific case of the Krichevsky-Trofimov potential, the outputs \( wt \) appear to be proportional to the gains. Consequently, in the LEA framework, the weight vectors \( pt \) defined by Equation (12) would not depend on \( b \). Am I correct in this interpretation?
3. Unfortunately, this invariance property does not seem to hold in the OLO framework. Is this observation accurate?
4. What happens if the algorithm operates on losses instead of gains? If all losses are bounded by \( b \), I suspect that the transformation \( \text{gain} \leftarrow 1 - \text{loss}/b \) would work for LEA (i.e., the algorithm would remain independent of \( b \)), but there might be issues for OLO. Is this indeed the case?
I recommend that the authors include a discussion of these four points in the paper. If my observations regarding points 3 and 4 are correct, the authors should exercise caution when using the term "parameter-free."
Additional minor comments:
- Line 16: Replace "the Hilbert space" with "a Hilbert space."
- Lemma 1: Does "is equivalent to" mean "implies"? The equivalence is unclear to me.
- Definition 2: Could you provide examples of potentials at this point? As it stands, the reader must wait for clarification.
- Line 193: The Gamma function should use \( t^{x-1} \) instead of \( t^{-x} \).
- End of line 394: The cases \( ui = 0 \) or \( \pii = 0 \) should be addressed separately (though the result appears correct).