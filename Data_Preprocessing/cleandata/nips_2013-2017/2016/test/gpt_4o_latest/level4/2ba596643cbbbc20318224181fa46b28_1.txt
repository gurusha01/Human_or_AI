The paper addresses multi-armed bandit problems where the learner must select a 'fidelity' level (from a finite set of fidelities) for the arm being pulled. Pulling an arm at a specific fidelity yields a stochastic iid reward with a mean that is within zetam of the true mean, where the zetam parameters (errors) are known beforehand, and the highest fidelity corresponds to zeta = 0. Additionally, lower fidelity pulls incur lower costs compared to higher fidelity pulls. The authors formulate the problem of sequentially pulling arms with fidelities to maximize a notion of net reward, propose a novel algorithm for this setting, and demonstrate that it can outperform UCB when operating exclusively at high fidelity levels. The paper also establishes a lower bound for the regret of the proposed algorithm. 
In my view, the paper explores an interesting and timely problem—namely, the tradeoff between information, cost, and reward (deciding whether to opt for low-cost, low-information fidelity or high-cost, high-information fidelity) in the context of online learning, specifically stochastic bandits. As such, it has potential value as a benchmark for future improvements. While the paper appears technically sound, a significant limitation lies in the insufficient explanation of its results and underlying assumptions. 
The regret definition adopted by the authors seems somewhat unconventional—why penalize resource consumption (or 'cost') multiplicatively rather than additively, as is done here? While the authors justify their definition with an example from ad-display, this may not generalize well to other applications. It would be insightful to explore what happens when the cost of fidelity observations is incorporated additively into the total reward (or regret), as this might represent a more natural formulation of the problem. 
The introduction of Assumption 1 also feels somewhat artificial or opaque, as it imposes constraints on the fidelity parameters available to the learner. It is unclear why the authors expect this assumption to hold in general learning problems. More critically, if the zeta_m values do not decay as required by Assumption 1, is there a systematic way to select a subsequence of fidelities that satisfies the assumption and preserves the results? Addressing this question would significantly bolster the paper's contributions. Although the authors claim that the assumption is not essential, it appears to play a role in the proofs. My primary concern is whether this assumption has a substantial impact on the algorithm's design and performance. The algorithm seems tailored to perform well for specific zeta configurations, but the rationale for why these configurations are the most appropriate is not adequately justified. 
Additionally, the gap in the lower bound warrants further investigation and may necessitate a rethinking of the proposed strategy. Interpreting the main result (Theorem 2, the regret bound for MF-UCB) and comparing it to UCB (presumably at the highest fidelity, though this is not explicitly clarified) is challenging due to the technical density of the presentation. The relative performance of MF-UCB appears to depend on Delta and [[k]], which are themselves tied to the complex sequence of sets \cal{K}^(m). A clearer and more detailed explanation would significantly strengthen the authors' argument. 
Overall, while the paper begins with an intriguing learning model, it quickly becomes difficult to follow when presenting its results. 
Minor typos:
- Line 56: "near-optimal"
- Line 126: Remove the period before "for all m < M"
- Line 179: Use curly braces for set notation when defining [[k]]