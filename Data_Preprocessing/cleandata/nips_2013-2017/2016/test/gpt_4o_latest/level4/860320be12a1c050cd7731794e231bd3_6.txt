In this paper, the authors introduce three complexity measures for RNNs: recurrent depth, feedforward depth, and the recurrent skip coefficient. These measures are defined within a precise graph-theoretic framework. The authors then conduct experiments on various sequential tasks using RNN models of differing complexity based on these measures. A significant portion of the paper is devoted to rigorously defining these three complexity measures, which, in my view, are relatively straightforward and do not necessitate the extensive graph-theoretic formalism employed. For instance, the intuitive definitions of recurrent depth and skip coefficient (as the average maximum number of non-linear transformations per time step and the shortest path length from time step i to time step (i+n), respectively) are clear and well-illustrated in Figure 1. The formal graph-theoretic definitions do not seem to provide any substantial additional insight. While formalization can be valuable when proving novel and impactful theorems, the theorems presented in this work are limited to demonstrating the computability of the proposed measures, which appears to be more of a technical detail. Furthermore, the experimental results lack clear conclusions. Although the exploration of different architectures in Figures 2a and 2b is a promising direction, the experiments are not systematically conducted, and there are methodological concerns.
Questions/comments:  
- Why are the architectures in Figure 2a evaluated on PennTree and Text8, while those in Figure 2b are evaluated on Text8 and MNIST? Would it not be more insightful to evaluate the same set of architectures across all datasets to determine if certain architectures consistently perform better for specific problem types?  
- Are the models in Table 1 (right) the same size as those in Table 1 (left) for Text8? It is unclear whether the performance differences arise from architectural differences or model size variations.  
- Are the improvements in recurrent depth reported in Table 1 (right) statistically significant? Including error bars over multiple initializations would clarify this.  
- In Section 4.3, the MNIST results should be moved from the appendix to the main paper and presented in a table similar to Table 1 (right).  
- In Section 4.4, the results for the adding and copying problems should also be presented in a table rather than in the text.  
- In Table 2 (top left), it is surprising that the results for permuted MNIST outperform those for unpermuted MNIST, as permuting the data presumably increases the timescale of dependencies. Can the authors provide an explanation for this?  
- Sentences such as lines 294â€“296 ("We varied...found that this kind of improvement persists") should reference specific results. The insights from the experiments are unclear.  
The conclusion lacks clarity and alignment with the experimental findings. For example, the authors claim, "we find empirical evidence that increasing recurrent depth might yield performance improvements," but this does not hold for sequential MNIST, and recurrent depth is not tested for the copy or addition tasks. Similarly, the statement "increasing feedforward depth might not help on long-term dependency tasks" is unsurprising, as feedforward depth is unlikely to significantly affect vanishing or exploding gradients. Lastly, the claim that "increasing recurrent skip coefficient can largely improve performance on long-term dependency tasks" reiterates a known result (e.g., the Clockwork RNN).