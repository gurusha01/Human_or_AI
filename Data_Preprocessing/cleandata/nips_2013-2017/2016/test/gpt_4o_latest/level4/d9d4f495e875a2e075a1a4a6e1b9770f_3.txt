The paper introduces a framework for making long-range predictions in videos by focusing on motion prediction. Unlike conventional methods that explicitly reconstruct future frames from the model's internal state, this approach avoids the need for the internal state to encode appearance information, which is already present in the current input frame. By doing so, the proposed method can concentrate on learning the physical principles underlying motion. The motion prediction mechanism is implemented using a deep network that generates either convolutional filters or affine transformation matrices, which are then applied to the current input frame. Furthermore, the framework accommodates conditional inputs, such as the actions or states of a robot interacting with the scene.
Overall Comment: The paper is well-motivated and clearly written, with most aspects being easy to understand. (However, I found the details of the 'tiling' operation for the action/input state somewhat unclear.) Conceptually, it might have been better to disentangle the action-conditioning aspect from the motion prediction aspect. While the motivation for the model is grounded in interaction, focusing initially on motion prediction alone could have made the contributions of prediction and action-conditioning more distinct (this is, of course, a subjective opinion). Without the action-conditioning component, the model bears some resemblance to the recently proposed Dynamic Filter Networks from Luc van Gool's group (NIPS submission). Another relevant recent work to cite would be Forecasting from Static Images using Variational Autoencoders from CMU.
I would have appreciated a more detailed analysis of the model's limitations. For instance, it appears that the framework performs well in predicting the robot arm's movement, but objects being moved remain blurry, particularly non-rigid ones (assuming my interpretation of the videos is correct). This might stem from the use of a reconstruction loss in pixel space during training. Incorporating a GAN-based loss could potentially address this issue. Another broader question pertains to the choice of predicting in pixel space. While this makes the results interpretable to humans, there seems to be no inherent reason for this choice. Predicting a more compact representation of future frames might be more efficient, both in terms of the model's capacity and the downstream utility of the predictive framework.