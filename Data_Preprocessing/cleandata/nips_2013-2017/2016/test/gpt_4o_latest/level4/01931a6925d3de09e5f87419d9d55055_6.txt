In this paper, the authors introduce a locally adaptive normal distribution. Rather than relying on Euclidean distances, they define a Riemannian metric \( M(x) \) that varies smoothly across the dataset. Using this Riemannian metric, geodesic distances along the manifold are computed via the exponential and logarithmic maps. The authors employ the Euler-Lagrange equations, which yield a set of ordinary differential equations, to estimate the geodesic distances. The logarithmic map projects points from the manifold (\( y \)) into the tangent plane at \( x \), such that the length of the resulting vector corresponds to the geodesic distance from \( x \) to \( y \) along the manifold. Subsequently, the authors define a normal distribution by computing a Mahalanobis distance on points mapped via the logarithmic map. This inner product is then used to measure distances from a point \( x \) to the mean, replacing Euclidean distances. They derive estimators for the mean and covariance using maximum likelihood estimation. Since closed-form solutions are unavailable, the authors propose a steepest descent algorithm for parameter estimation. Additionally, because the normalization constant cannot be computed in closed form, they present a Monte Carlo approximation method. The proposed approach is applied to synthetic data generated on a manifold and to a clustering task, with comparisons made to the standard Gaussian mixture model (GMM). Overall, I find the paper to be an interesting and valuable extension of the normal distribution, tailored to adapt to local manifold structures. The authors effectively integrate techniques for metric estimation, geodesic distance computation, and distribution learning. I believe this work merits publication. Below are my detailed comments on the scores:
Technical Quality Comments:  
The manifold techniques, including the use of Euler-Lagrange equations and logarithmic map estimation, are robust and well-articulated. The Monte Carlo approximation for the normalization constant and the steepest descent algorithm are innovative and clearly described. However, the analysis of the applications is somewhat limited. For instance, the choice of the sigma parameter appears to be based on optimization over the training data, which raises concerns. Additionally, the practical utility of the learned distribution could be better elaborated.
Novelty Comments:  
The primary contribution of this paper lies in combining existing methods—using kernels to define the metric \( M(x) \), estimating geodesic distances via Euler-Lagrange equations, and applying a normal distribution—and proposing an algorithm to estimate the parameters (mean, covariance, and normalization constant). While the integration is novel, the individual components are largely based on prior work.
Potential Impact Comments:  
The evaluation of the proposed locally adaptive normal distribution (LAND) against GMMs on synthetic data is fair, as it involves comparing log-likelihoods on data generated by the learned distributions. However, the choice of the sigma parameter is unclear. The authors demonstrate that LAND performs well on manifold-based synthetic data but do not evaluate its performance when data is generated from a single normal distribution or a GMM. In the real-world clustering task, the sigma parameter for LAND appears to have been selected by optimizing the F-measure, which is then reported on the same dataset. To ensure fairness, the authors should have evaluated the F-measure on a held-out test set or clarified if this was done. This omission makes it difficult to assess potential overfitting. Additionally, it would have been beneficial to test the method on very high-dimensional data. One promising application suggested by the authors is using LAND as a generative model for manifold data. However, the assumption that the observed data \( X \) lies in \( D \)-dimensional space and that the manifold dimension is also \( D \) is unusual. Typically, in manifold learning, the data lies on a \( d \)-dimensional manifold embedded in a \( D \)-dimensional Euclidean space, where \( D \) is much larger than \( d \).
Clarity Comments:  
One aspect of the notation is confusing: the overloading of the sigma (\( \Sigma \)) symbol. For example, the authors state \( M = \Sigma^{-1} \), but then estimate it using a (local) diagonal kernel matrix. Is this \( \Sigma \) the same as the fitted covariance matrix? If so, is the kernel merely an initialization? This would be unusual, as \( M(x) \) should vary with \( x \), whereas the fitted covariance matrix is a single parameter. In the proof of the steepest descent algorithm, the authors explicitly state \( M = \Sigma^{-1} \) and use this substitution, which adds to the confusion. Clarification on this point would be helpful. In Section 4.2, the authors do not explicitly state the method for determining cluster membership, but I assume it involves selecting two clusters and computing the closest (geodesic?) distance to the cluster means. Overall, the introduction to manifolds is well-explained, particularly for readers familiar with Riemannian manifolds.