This paper introduces a method to accelerate kernel learning by leveraging random features (RF). Compared to existing techniques, such as structured kernel composition guided by an alignment metric and joint kernel optimization via empirical risk minimization, the proposed approach achieves greater efficiency by optimizing kernel compositions through explicit RF mappings instead of relying on full kernel matrices. The method is structured into two main steps: First, the kernel is efficiently learned using RF. Subsequently, the optimized features corresponding to the learned kernel are employed in a standard supervised learning framework to compute an estimator. The authors establish theoretical guarantees for the method, including kernel consistency and generalization bounds for the resulting estimator. 
The empirical evaluation of the method includes:  
- A toy example demonstrating the learned features when starting from a poor initial kernel guess.  
- An experiment on a high-dimensional dataset, illustrating that the method achieves superior predictive performance and induces a sparse representation, which could enhance interpretability.  
- A performance comparison on benchmark datasets, showing that the method achieves comparable test accuracies at a significantly reduced computational cost relative to the standard RF approach.  
The proposed method is both innovative and compelling. The use of random features to expedite kernel alignment is particularly noteworthy. The paper is well-written and systematically organized. The authors provide a rigorous theoretical analysis, ensuring the consistency of the learned kernel and the generalization properties of the resulting estimator. The experimental results effectively illustrate the method's utility, demonstrating its ability to identify sparse features in high-dimensional settings and comparing its accuracy and computational efficiency on three benchmark datasets. These benchmarking results underscore the method's potential for practical applications. 
As a potential improvement, I recommend evaluating the method on additional benchmark datasets, such as those mentioned in [18].  
Minor issues:  
- Line 37 (?): "optimization" → "optimization problem"  
- Lines 76, 83, 89: Clearly define or revise the notation for $W$ and $W^k$, and clarify their relationship to the lower-case $w^i$.  
- Line 87: "need only store" → "only needs to store"  
- Line 98: Missing closing parenthesis in "the last element of $z^i$".  
- Line 189: "one-hot" → "one-shot"  
- Algorithm 1: Add "end" at the conclusion of each while loop.  
- Figure 1: It is unclear why the optimized random features $w^k$, clustered around (-1, -1) and (1, 1) and depicted in yellow, signify a good solution.