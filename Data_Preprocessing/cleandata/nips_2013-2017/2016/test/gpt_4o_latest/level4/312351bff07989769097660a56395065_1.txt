The paper introduces a "neural transducer" model for sequence-to-sequence tasks that operates in an online, left-to-right manner. Unlike traditional sequence-to-sequence models that wait for the entire input sequence before producing output, this model generates output incrementally as input is received. The key innovations enabling the model include a recurrent attention mechanism, the introduction of an end-of-block symbol in the output alphabet to signal transitions between input blocks, and approximate algorithms based on dynamic programming and beam search for training and inference. Experiments conducted on the TIMIT speech recognition task demonstrate the model's effectiveness and investigate various design parameters. 
This is a well-executed paper that addresses an important problem: designing and training a sequence-to-sequence model capable of operating online rather than requiring complete input sequences. The paper provides a clear description of the proposed architecture and systematically explains the design considerations for its components, including next-step prediction, the attention mechanism, and end-of-block modeling. It also effectively outlines the challenges associated with training and inference for the model and proposes practical approximate solutions. The experimental results on TIMIT are appropriate for showcasing the model's utility and exploring design choices, such as recurrent state handling across block boundaries, block size, attention mechanism design, and model depth.
However, there are several areas where the paper could be improved. 
- Page 3, line 93: The phrase "We first compute the probability of l compute the probability of seeing output sequence" contains an editing error that needs correction.  
- Section 3.4: The paper does not discuss the comparative performance of the three different approaches to end-of-block modeling. While it is implied that the use of the `<e>` symbol was the most effective, the performance differences between the methods are not quantified. Including such an analysis would strengthen the paper.  
- Section 3.5: The approach of computing alignments less frequently than model updates is reminiscent of lattice-based techniques in sequence-discriminative training for acoustic models in speech recognition. Relevant references include D. Povey and P. C. Woodland's work on large-scale discriminative training (ASR Workshop, 2000) and B. Kingsbury's lattice-based optimization for neural network acoustic modeling (ICASSP, 2009). Future work on the transducer model could explore the use of lattices to represent alignment information. Additionally, methods for rescoring lattices with recurrent neural network language models, such as those described by X. Liu et al. (ICASSP, 2014), might be applicable to address the challenge of conditioning the transducer on all input and output history up to the current moment.  
- Page 7, lines 202-203: Replace "Log Mel filterbanks" with "Log Mel spectra" for accuracy.  
- Section 4.2: The TIMIT experiments deviate from standard practices. Typically, TIMIT phones are collapsed more than was done in this paper, and specific subsets of the data are excluded from training, with results reported only on the core test set. For example, see T. N. Sainath et al. (IEEE Transactions on Speech and Audio Processing, 2011) or K. F. Lee and H. W. Hon (IEEE Transactions on Acoustics, Speech, and Signal Processing, 1989). Adhering to the standard experimental framework would make the results more comparable to prior work and should be explicitly stated in the paper.  
- Section 4.2: The authors could have explored initializing the transducer by training on HMM-GMM alignments before transitioning to alignments inferred by the transducer itself. This approach might have yielded improved performance on TIMIT.
Overall, the paper is a strong contribution to the field, but addressing the above issues would enhance its clarity, rigor, and impact.