The paper investigates online learning within a non-stationary stochastic environment, focusing on the achievable regret as a function of the changes in the mean loss vector (quantified by the number of switches or the magnitude of shifts) and the cumulative variance of the random losses. The authors present algorithms with regret upper bounds and provide matching lower bounds.  
ADDED AFTER THE REBUTTAL:  
- The proof of Theorem 4.1 is indeed correct, and the constructed environment is oblivious; I acknowledge that this was my mistake—apologies for the confusion.  
- The generalization in the analysis of UCB-V appears relatively straightforward, as the applied concentration inequalities, to the best of my recollection, do not require the random variables to share the same distribution (neither in your case nor in theirs).  
- Regarding the comparison to Jadbabaie et al.: If one selects \(M\) in their work to be the previous gradient, their \(D_T\) essentially aligns with your \(\Lambda\) (up to constant factors), with the primary distinction being that you take the expectation inside the norms in \(V\). While I may be mistaken, it seems plausible that a slight modification of their proof could allow for moving the expectation inside. However, this requires careful verification on your part, and the connection to related work must be described rigorously, clarifying what follows directly and what does not.  
- You should still provide justification for why the proposed complexity parameters are meaningful, such as offering an example where \(\Lambda\) is neither zero nor linear in \(T\).  
While the paper offers potentially interesting contributions, it does not adequately engage with the existing literature and fails to cite key references properly. This omission makes it challenging to assess the novelty of the results, as several of them appear to follow directly from prior work. For instance, the paper by Jadbabaie, Rakhlin, Shahrampour, and Sridharan—whose arXiv version is "sort of" cited in the references but not discussed in detail—was published in AISTATS 2015 and proposes an algorithm similar to Algorithm 2, achieving comparable bounds in the more challenging adversarial setting.  
The main contribution of the paper, as acknowledged by the authors, is the incorporation of the variance of random losses into the analysis, building on techniques for deriving second-order and shifting bounds (e.g., blocking in simple cases). While this could be an interesting addition, it appears that similar ideas were already explored in the work of Jadbabaie et al. (2015). Consequently, a thorough discussion of the paper's novelty relative to prior work is essential.  
Moreover, the writing requires significant improvement. For example, the UCB-V algorithm of Audibert et al. (2009), which is used almost identically in the paper, is not even cited.  
Regarding motivation, the paper should explain why the proposed complexity measures are of interest. For instance, in cases where there is a clearly suboptimal arm, why does it matter if its mean changes or if it exhibits high variance (assuming that even its smallest loss exceeds the losses of other arms)? Additionally, what are the meaningful scenarios where \(\Lambda\) is neither constant nor linear in time?  
The connection to tracking/shifting bounds (Herbster and Warmuth, 1998, 2001) also requires clarification. In standard online learning literature, tracking and shifting bounds typically pertain to properties of the comparator sequence rather than the loss functions, so this distinction should be made explicit to avoid confusion.  
The switching results should also be linked to the extensive body of work in information theory on coding piecewise stationary sources. While these works often focus on log-loss, the underlying techniques are quite similar, with entropy or entropy-rate replacing variance. Additionally, generic reductions for sleeping experts are available in the literature (e.g., Gyorgy et al., 2012), and these should be acknowledged.  
In summary, while the paper has potential, substantial revisions are necessary before it can be considered for publication.  
References:  
- J-Y. Audibert, R. Munos, and Cs Szepesvari. "Exploration-exploitation trade-off using variance estimates in multi-armed bandits." Theoretical Computer Science, 410:1876-1902, 2009.  
- A. Gyorgy, T. Linder, and G. Lugosi. "Efficient Tracking of Large Classes of Experts." IEEE Transactions on Information Theory, vol. 58, pp. 6709-6725, November 2012.  
- Herbster, M. and Warmuth, M.K. (1998). "Tracking the Best Expert." Journal of Machine Learning, Vol. 32(2), pp. 151-178, August 1998.  
- Herbster, M. and Warmuth, M.K. (2001). "Tracking the Best Linear Predictor." Journal of Machine Learning Research, Vol. 1, pp. 281-309, September 2001.