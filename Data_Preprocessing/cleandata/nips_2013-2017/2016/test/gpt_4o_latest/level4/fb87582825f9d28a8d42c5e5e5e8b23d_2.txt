Review - Paper 1982: Learning to Learn by Gradient Descent by Gradient Descent
This paper presents an approach where an LSTM learns entire (gradient-based) learning algorithms for specific classes of functions, building on related work from the 1990s and early 2000s. Below are my comments, primarily focused on related work:
---
Comments on Related Work:
The authors state: "The idea of using learning to learn or meta-learning to acquire knowledge or inductive biases has a long history [Thrun and Pratt, 1998]." However, the introduction to this reference conflates meta-learning (which involves learning the learning algorithm itself) with transfer learning, grouping nearly everything under "meta-learning." This includes standard backpropagation, which can be applied to a dataset and subsequently used to learn new data points more efficiently—a process that is better categorized as standard transfer learning. To my knowledge, the earliest work on learning general learning algorithms expressed in a universal programming language dates back to 1987:  
J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook. Diploma thesis, Institut für Informatik, Technische Universität München, 1987.
The authors also state: "This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network acts as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies. Alternatively, Schmidhuber [1992, 1993] considers networks that are able to modify their own behavior and act as an alternative to recurrent networks in meta-learning. Note, however, that these earlier works do not directly address the transfer of a learned training procedure to novel problem instances and instead focus on adaptivity in the online setting."
This characterization does not fully capture the contributions of prior work. Schmidhuber's 1993 meta-RNN was not an alternative to RNNs but rather an RNN capable of executing arbitrary computable weight update algorithms on itself. It could sequentially access and modify its own weights (similar to a Neural Turing Machine, where storage cells are implemented as fast weights). The entire system was differentiable end-to-end, enabling gradient descent to search for learning algorithms, meta-learning algorithms, meta-meta-learning algorithms, and so on—without restrictions. Furthermore, Hochreiter's meta-LSTM did indeed "directly address the transfer of a learned training procedure to novel problem instances" (as described by the authors). Specifically, it was trained on certain quadratic functions to learn a learning algorithm for such functions. When presented with a new quadratic function via input/target pairs, the meta-LSTM generalized from prior instances, learning the new function 30 times faster than standard backpropagation or gradient descent. In essence, it learned by gradient descent to outperform gradient descent in terms of speed. While the exact nature of the resulting learning algorithm (e.g., whether it resembled a fast version of gradient descent or something more sophisticated) was not analyzed, the setup was sufficiently general to enable any computable learning algorithm. 
It is crucial for the authors to clearly articulate the differences between their approach and prior work, assuming such differences exist. The primary distinction appears to be the explicit bias in their meta-learning framework toward gradient-based learning algorithms, whereas earlier work lacked such a bias. This explicit bias may be advantageous, at least for the experimental tasks studied, and should be highlighted.
The authors further state: "Finally, Daniel et al. [2016] considers using reinforcement learning to train a controller for selecting step-sizes; however, this work is much more constrained than ours and still requires hand-tuned features." However, a much earlier and more general reinforcement learning system capable of learning learning algorithms (expressed in a general programming language without restrictions) was introduced in the success-story algorithm (SSA), also known as EIRA in earlier work:  
M. Wiering and J. Schmidhuber. Solving POMDPs using Levin search and EIRA. In L. Saitta, ed., Machine Learning: Proceedings of the 13th International Conference (ICML 1996), pages 534-542, Morgan Kaufmann Publishers, San Francisco, CA, 1996.  
Based on:  
J. Schmidhuber. On learning how to learn learning strategies. TR FKI-198-94, TUM, 1994.  
J. Schmidhuber, J. Zhao, and M. Wiering. Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning 28:105-130, 1997.  
This system could execute arbitrary algorithms, including learning and meta-learning algorithms, and used SSA to favor "good" learning algorithms that optimized reward intake over time. The authors should identify the main differences between their approach and this earlier work.
---
Technical Clarifications:
The authors state: "We implement the update rule for each coordinate using a two-layer Long Short Term Memory (LSTM) network." Did they use the original 1997 LSTM or the version with forget gates, which has become more widely adopted?  
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):2451-2471, 2000.
The authors also mention: "an NTM-BFGS optimizer, because its use of external memory is similar to the Neural Turing Machine [Graves et al., 2014]." Does this resemble the fast weight system of 1992, which separates storage and control?
Regarding the experiment with quadratic functions: "In this experiment, we consider training an optimizer on a simple class of synthetic 10-dimensional quadratic functions." This is reminiscent of the experiments with quadratic functions in Hochreiter et al.'s 2001 paper. Did the authors perform a direct comparison with that system?
---
Conclusion:
The authors conclude: "We have shown how to cast the design of optimization algorithms as a learning problem, which enables us to train optimizers that are specialized to particular classes of functions." However, this was already demonstrated in prior work up to 2001, as discussed earlier. The conclusion should instead emphasize the novel contributions of this paper. The primary novelty appears to lie in the explicit bias toward gradient-based learning algorithms, which was absent in earlier work. This bias is likely beneficial for the experiments presented and should be underscored. Additionally, the supplementary experiments (beyond those involving quadratic functions) are compelling, and the results are intriguing.
I hope this paper helps reignite interest in gradient-based meta-learning. It should be published, provided the authors address the comments above. I look forward to reviewing the revised version.