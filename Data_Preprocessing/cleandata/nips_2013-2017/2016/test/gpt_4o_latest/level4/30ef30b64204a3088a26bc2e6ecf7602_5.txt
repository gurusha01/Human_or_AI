The authors propose a semi-supervised approach that leverages the internal consistency of the data to guide the adaptation process of a learning machine. Unlike traditional learning methods, which primarily focus on minimizing the discrepancy between the desired output and the actual output, this work also incorporates the principle that, in classification tasks, the output should remain consistent despite variations in the inputs and the learning machine itself. Essentially, the ultimate objective of the machine is to identify those invariances that optimize recognition performance. The paper is well-structured and clearly articulated. The central concept is both compelling and innovative, utilizing information inherent in the data in a manner that has been largely overlooked in prior work. This approach enables the authors to achieve remarkable results using small labeled datasets in conjunction with large unlabeled ones. The study represents a meaningful step forward in framing machine learning as the discovery of invariances, as highlighted by Gens and Domingos in their "Deep Symmetry Networks" paper. Minor comment: 1. Clarify the definition of the index i in Eq. (1).