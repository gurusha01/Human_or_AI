This paper addresses the problem of learning from positive-unlabeled (PU) data, proposing two classification algorithms that are robust to noise in the positive labels and effective for high-dimensional data. The authors build on prior theoretical work in PU learning, which has often struggled with practical applicability in noisy, high-dimensional settings. Their key contributions include the explicit modeling of label noise and the use of univariate transforms built on discriminative classifiers. A notable theoretical result is the proof that these univariate transforms preserve the class prior, enabling estimation in the univariate space and circumventing the challenges of kernel density estimation in high-dimensional data. The authors provide both parametric and nonparametric implementations of their methods, positioning their work as a step toward practical, robust PU classification.
Strengths:
1. Relevance and Significance: The paper addresses a critical gap in PU learning by tackling both noise robustness and scalability to high-dimensional data, which are significant challenges in real-world applications. The proposed methods have the potential to advance the state of the art and be widely adopted.
2. Theoretical Rigor: The proof that the univariate transforms preserve the class prior is a strong theoretical contribution, as it underpins the feasibility of the proposed approach for high-dimensional data.
3. Practical Focus: Unlike many prior works that remain theoretical, this paper emphasizes practical applicability by proposing algorithms that are implementable and scalable.
4. Clarity of Contributions: The paper clearly delineates its contributions, including the explicit modeling of label noise and the novel use of univariate transforms.
Weaknesses:
1. Experimental Evaluation: While the paper claims robustness and effectiveness, the experimental results are not detailed in the abstract. It is unclear how the proposed methods compare quantitatively to existing PU learning algorithms on diverse real-world datasets.
2. Related Work: The abstract does not provide sufficient context on how the proposed methods differ from or improve upon prior PU learning approaches. A more thorough discussion of related work would strengthen the paper.
3. Clarity of Presentation: The abstract is dense and could benefit from clearer explanations of key terms (e.g., "univariate transforms" and "discriminative classifiers") for accessibility to a broader audience.
Pro/Con Arguments for Acceptance:
Pro:
- The paper addresses an important and underexplored problem in PU learning with a strong theoretical foundation.
- The proposed methods are practical and scalable, addressing real-world challenges of noise and high dimensionality.
Con:
- The experimental evaluation and comparison to prior work are not sufficiently detailed (based on the abstract).
- The paper could improve in clarity and accessibility, particularly for readers less familiar with the technical details of PU learning.
Recommendation:
Overall, this paper makes a meaningful contribution to PU learning by addressing noise robustness and scalability. However, the lack of detailed experimental results and discussion of related work limits its immediate impact. I recommend acceptance with minor revisions, contingent on the authors providing a more comprehensive evaluation and clearer exposition in the full paper.