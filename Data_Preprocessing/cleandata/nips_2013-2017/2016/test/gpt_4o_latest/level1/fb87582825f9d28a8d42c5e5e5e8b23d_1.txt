This paper proposes a novel approach to optimization by replacing traditional hand-designed algorithms with learned optimizers implemented using Long Short-Term Memory (LSTM) networks. The authors frame the design of optimization algorithms as a learning problem, enabling the optimizer to exploit problem-specific structures. The learned optimizers outperform standard methods like SGD, ADAM, and RMSprop on tasks for which they are trained and generalize well to related tasks. The paper demonstrates the effectiveness of this approach across diverse applications, including quadratic optimization, training neural networks on MNIST and CIFAR-10, and neural art style transfer. The authors highlight the potential for transfer learning, showing that the learned optimizers generalize to tasks with different architectures, datasets, and problem scales.
Strengths:
1. Novelty and Originality: The paper introduces a fresh perspective on optimization by leveraging meta-learning to design optimizers, which is a significant departure from traditional hand-crafted methods. The use of LSTMs to parameterize the optimizer is innovative and well-motivated.
2. Technical Soundness: The methodology is rigorously developed, with clear mathematical formulations and justifications. The use of truncated backpropagation through time (BPTT) to train the optimizers is appropriate for the problem setting.
3. Comprehensive Experiments: The authors evaluate their approach on a wide range of tasks, including synthetic quadratic functions, neural network training, and neural art. The experiments are thorough, with comparisons to state-of-the-art optimizers and analyses of generalization.
4. Generalization and Transfer Learning: The paper convincingly demonstrates that the learned optimizers generalize well to new tasks, architectures, and datasets, which is a critical contribution to the field of meta-learning and optimization.
5. Clarity: The paper is well-written and organized, with detailed explanations of the methodology and experiments. Figures and results are presented clearly, aiding comprehension.
Weaknesses:
1. Scalability: While the proposed method performs well on small-scale tasks, its scalability to very large-scale problems (e.g., modern deep learning models with millions of parameters) is not fully explored. The coordinate-wise decomposition is a step in this direction, but further discussion on computational overhead and memory requirements would strengthen the paper.
2. Limited Diversity in Training Tasks: The optimizers are trained on relatively narrow distributions of tasks (e.g., a single style image in neural art). While generalization to related tasks is demonstrated, it is unclear how well the method would perform on entirely unrelated tasks.
3. Comparison to Meta-Learning Baselines: While the paper compares its approach to standard optimizers, it does not benchmark against other meta-learning methods for optimization, such as reinforcement learning-based approaches or hyperparameter optimization techniques.
4. Interpretability: The learned update rules are parameterized by LSTMs, which makes them less interpretable compared to traditional optimization algorithms. A deeper analysis of the learned dynamics would enhance the paper.
Arguments for Acceptance:
- The paper addresses a fundamental problem in machine learning and optimization with an innovative and well-executed approach.
- The results are compelling, demonstrating significant improvements over standard optimizers and strong generalization capabilities.
- The work is likely to inspire further research in meta-learning and learned optimization.
Arguments Against Acceptance:
- The scalability of the approach to large-scale problems remains uncertain.
- The lack of comparisons to other meta-learning-based optimization methods limits the contextualization of the contribution.
Recommendation:
I recommend acceptance of this paper. While there are areas for improvement, the paper makes a strong scientific contribution by introducing a novel framework for learned optimization and demonstrating its effectiveness across diverse tasks. It is likely to be of significant interest to the NeurIPS community.