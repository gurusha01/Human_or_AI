This paper introduces a novel method called Structured Sparsity Learning (SSL) to regularize the structure of Deep Neural Networks (DNNs) by leveraging group Lasso regularization. SSL targets multiple structural aspects of DNNs—filters, channels, filter shapes, and layer depth—to achieve compactness, computational efficiency, and improved classification accuracy. The authors demonstrate that SSL can dynamically optimize DNN structures during training, achieving significant speedups (5.1× on CPU and 3.1× on GPU for AlexNet) while maintaining or even improving accuracy. For instance, SSL reduces a 20-layer ResNet to 18 layers, improving accuracy on CIFAR-10 from 91.25% to 92.60%. The paper also highlights SSL's ability to outperform non-structured sparsity methods by ensuring hardware-friendly memory access patterns, making it more practical for real-world deployment.
Strengths:
1. Technical Soundness: The paper provides a rigorous mathematical formulation of SSL, supported by theoretical insights and experimental results. The use of group Lasso for structured sparsity is well-motivated and effectively applied to various DNN components.
2. Significant Results: The experimental results are compelling, showing substantial speedups and accuracy improvements across diverse datasets (MNIST, CIFAR-10, ImageNet) and architectures (LeNet, ResNet, AlexNet). The comparison with non-structured sparsity methods convincingly demonstrates SSL's advantages.
3. Practical Relevance: By focusing on hardware-friendly sparsity, the paper addresses a critical bottleneck in deploying large-scale DNNs on resource-constrained devices. This makes the work highly relevant for both academia and industry.
4. Clarity: The paper is well-organized, with detailed explanations of the methodology, experiments, and results. Figures and tables effectively illustrate key findings.
Weaknesses:
1. Limited Discussion of Limitations: While the paper highlights SSL's strengths, it does not sufficiently discuss potential limitations, such as the computational overhead of applying group Lasso during training or the scalability of SSL to extremely deep networks.
2. Comparative Analysis: Although the paper compares SSL with non-structured sparsity methods and low-rank approximation, it could benefit from a broader comparison with other recent model compression techniques, such as neural architecture search or quantization.
3. Reproducibility: While the authors provide a link to the source code, the paper could include more details on hyperparameter settings and training procedures to facilitate reproducibility.
Arguments for Acceptance:
- The paper addresses a critical problem in DNN deployment and proposes a novel, effective solution.
- The results are significant and demonstrate clear improvements over existing methods.
- The methodology is well-founded and generalizable to various architectures and tasks.
Arguments Against Acceptance:
- The lack of a detailed discussion on limitations and broader comparisons with other compression techniques slightly weakens the paper's positioning.
- The computational cost of applying SSL during training is not thoroughly analyzed.
Recommendation:
Overall, this paper makes a strong contribution to the field of model compression and structured sparsity in DNNs. Its combination of theoretical rigor, practical relevance, and significant experimental results makes it a valuable addition to the conference. I recommend acceptance, with minor revisions to address the weaknesses noted above.