Review of the Paper: "Learning Kernels via Randomized Features"
Summary
This paper presents a novel method for supervised kernel learning by extending the randomized feature approach. Unlike traditional kernel methods that require a pre-defined kernel, the authors propose an efficient optimization framework to learn kernels directly via their associated random features. The method involves solving a convex optimization problem to select a subset of random features, which are then used to train a model. The authors provide theoretical guarantees for the consistency of the learned kernel and its generalization performance. Empirical evaluations demonstrate the method's scalability and competitive performance on benchmark datasets, often at a fraction of the computational cost of other techniques.
The paper builds on the randomized feature approach introduced by Rahimi and Recht (2007, 2008) and addresses the challenge of kernel selection, a topic that has been explored extensively in the literature (e.g., Gönen and Alpaydın, 2011). The proposed method combines kernel alignment with randomization, offering a computationally efficient alternative to traditional kernel learning methods that rely on eigendecomposition or semidefinite programming.
Strengths
1. Technical Soundness: The paper is technically rigorous, with well-supported claims. The authors provide consistency guarantees for their method and derive generalization bounds using Rademacher complexity.
2. Scalability: The proposed method is computationally efficient, with near-linear time complexity for the kernel optimization step. This makes it highly scalable to large datasets.
3. Empirical Validation: The authors conduct extensive experiments on synthetic and real-world datasets, demonstrating the method's effectiveness and efficiency compared to both unoptimized random features and joint kernel-classifier optimization.
4. Theoretical Contributions: The paper provides novel theoretical insights, including consistency results for the optimization procedure and generalization guarantees for the learned kernel.
5. Practical Relevance: The method is practical and addresses a significant limitation of existing randomized feature approaches—namely, the reliance on user-defined kernels.
Weaknesses
1. Clarity: While the paper is generally well-written, some sections, particularly the optimization derivations, are dense and may be challenging for readers unfamiliar with the mathematical details. Additional explanations or visual aids could improve accessibility.
2. Comparative Analysis: Although the method is compared to unoptimized random features and joint optimization, the paper does not benchmark against other recent kernel learning methods (e.g., deep kernel learning). This limits the scope of the empirical evaluation.
3. Assumptions on Base Kernels: The method assumes the availability of a base kernel distribution \(P0\), which may still require domain knowledge. The paper does not discuss how sensitive the results are to the choice of \(P0\).
4. Limited Exploration of Divergences: The paper focuses primarily on the \(f\)-divergence \(f(t) = t^k - 1\) (e.g., \(k=2\)), leaving the exploration of other divergences as future work. This limits the generality of the presented results.
Arguments for Acceptance
- The paper addresses an important problem in kernel learning and provides a novel, efficient solution.
- The theoretical contributions are solid, and the empirical results are compelling.
- The method is scalable and has practical relevance for large-scale machine learning tasks.
Arguments Against Acceptance
- The paper could benefit from improved clarity and a broader comparative analysis.
- The reliance on a user-defined base kernel distribution \(P_0\) may limit the method's applicability in some scenarios.
Recommendation
Overall, this paper makes a significant contribution to the field of kernel learning and randomized features. Its strengths in scalability, theoretical rigor, and practical relevance outweigh its minor weaknesses in clarity and comparative analysis. I recommend acceptance, provided the authors address the clarity issues and include a discussion on the sensitivity to the choice of \(P_0\). 
Rating: 8/10