The paper proposes a novel Structured Sparsity Learning (SSL) method to regularize the structures of Deep Neural Networks (DNNs), including filters, channels, filter shapes, and layer depth. The authors claim that SSL achieves three main contributions: (1) learning compact and hardware-friendly structures to reduce computation costs, (2) improving classification accuracy through structural regularization, and (3) achieving significant speedups in convolutional layer computations compared to non-structured sparsity methods. Experimental results demonstrate up to 5.1× and 3.1× speedups for AlexNet on CPU and GPU, respectively, while maintaining or improving accuracy on datasets like CIFAR-10 and ImageNet.
Strengths:
1. Practical Significance: The paper addresses a critical challenge in deploying large-scale DNNs on resource-constrained devices by proposing a method that balances computational efficiency and accuracy. The hardware-friendly nature of SSL is particularly valuable for real-world applications.
2. Comprehensive Evaluation: The authors provide extensive experimental results across multiple datasets (MNIST, CIFAR-10, ImageNet) and architectures (LeNet, AlexNet, ResNet). The results convincingly show the effectiveness of SSL in reducing computational costs while maintaining or improving accuracy.
3. Novelty: The paper extends previous work on sparsity by introducing structured sparsity regularization, which avoids the irregular memory access issues of non-structured sparsity. The dynamic optimization of DNN structures, including layer depth, is a notable innovation.
4. Clarity of Results: The paper provides detailed comparisons with existing methods like `l1`-norm regularization and low-rank approximation, highlighting SSL's advantages in terms of speedups and accuracy retention.
Weaknesses:
1. Limited Discussion of Limitations: While the authors acknowledge that hardware and program optimizations could further boost performance, they do not explore the potential trade-offs or limitations of SSL, such as its applicability to non-convolutional architectures or its scalability to extremely deep networks.
2. Reproducibility: Although the authors provide a link to the source code, the paper lacks sufficient implementation details (e.g., hyperparameter settings, training configurations) to ensure reproducibility without additional effort.
3. Comparative Depth: While the paper compares SSL to non-structured sparsity and low-rank approximation, it does not benchmark against other recent structured pruning techniques, which could strengthen its claims of novelty and superiority.
Suggestions for Improvement:
1. Provide a more detailed discussion of SSL's limitations and potential areas for future work, such as its applicability to other types of networks (e.g., transformers) or its performance on edge devices.
2. Include more implementation details, such as exact hyperparameter values and training schedules, to enhance reproducibility.
3. Compare SSL with other state-of-the-art structured pruning methods to provide a more comprehensive evaluation of its advantages.
Recommendation:
I recommend acceptance of this paper, as it presents a significant and practical contribution to the field of DNN optimization. While there are minor areas for improvement, the strengths of the paper, particularly its novelty, practical relevance, and thorough experimental validation, outweigh its weaknesses. This work is likely to inspire further research in structured sparsity and efficient DNN deployment.