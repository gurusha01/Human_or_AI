The paper proposes a novel Homotopy Smoothing (HOPS) algorithm for solving structured non-smooth optimization problems, achieving a significant improvement in iteration complexity compared to existing methods. The authors claim that HOPS achieves an iteration complexity of \( \tilde{O}(1/\epsilon^{1-\theta}) \), where \( \theta \in (0, 1] \) captures the local sharpness of the objective function. This is the lowest complexity reported so far for the considered class of problems without assuming strong convexity. The paper also introduces a primal-dual variant (PD-HOPS) to further enhance practical performance. Experimental results demonstrate the superior performance of HOPS and PD-HOPS over state-of-the-art methods.
Strengths:
1. Novelty and Theoretical Contribution: The paper introduces a homotopy-based smoothing approach that systematically reduces the smoothing parameter in stages, leveraging local error bounds to achieve improved iteration complexity. This approach is novel and well-supported by rigorous theoretical analysis.
2. Practical Relevance: The proposed algorithm is applicable to a wide range of non-smooth optimization problems, including machine learning tasks (e.g., regularized empirical loss minimization), image processing, and cone programming. The paper demonstrates the generality of the method by connecting it to well-known problems and error bound conditions.
3. Experimental Validation: The experimental results convincingly show that HOPS and PD-HOPS outperform existing methods such as Nesterov's smoothing algorithm and primal-dual methods in terms of both iteration count and runtime, particularly for small \( \epsilon \).
4. Clarity of Presentation: The paper is well-organized, with clear explanations of the algorithm, its theoretical guarantees, and its relationship to existing work. The inclusion of detailed experimental comparisons further strengthens the paper.
Weaknesses:
1. Assumptions and Generality: While the local error bound condition is well-motivated and applies to many practical problems, the paper does not extensively discuss cases where this condition might not hold, limiting the generality of the results.
2. Complexity of Analysis: The theoretical analysis, while thorough, is dense and may be challenging for readers unfamiliar with advanced optimization techniques. Simplifying or summarizing key insights could improve accessibility.
3. Practical Implementation Details: Although the paper discusses the tuning of parameters (e.g., the smoothing parameter \( \mu \)), it does not provide sufficient guidance on how to set these parameters in practice for new problems. This could limit adoption by practitioners.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by improving the state-of-the-art iteration complexity for a broad class of non-smooth problems.
- The experimental results are robust and demonstrate clear practical advantages of the proposed methods.
- The work is relevant to the NIPS community, addressing challenging optimization problems that arise in machine learning and related fields.
Arguments Against Acceptance:
- The reliance on the local error bound condition may limit the applicability of the method to certain problem classes.
- The dense theoretical presentation could be a barrier for non-expert readers.
Recommendation:
I recommend acceptance of this paper. The proposed HOPS algorithm is a meaningful advancement in non-smooth optimization, with strong theoretical and empirical support. While there are some limitations in terms of generality and accessibility, the contributions are substantial and relevant to the field. The authors are encouraged to provide additional practical guidance and simplify the theoretical exposition in the final version.