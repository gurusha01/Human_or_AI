The paper addresses the problem of learning Bayesian networks (BNs) optimally under ancestral constraints, a significant challenge due to the non-decomposable nature of these constraints. The authors build upon the EC tree framework introduced by Chen et al. (2015, 2016) and propose a novel method to incorporate ancestral constraints by pruning the search space and inferring decomposable constraints to empower the oracle used in structure learning. Empirical results demonstrate that the proposed approach achieves orders-of-magnitude efficiency improvements over integer linear programming (ILP)-based methods, such as GOBNILP, particularly for larger networks and datasets.
Strengths:
1. Novelty and Contribution: The paper makes a clear and significant contribution by extending the EC tree framework to handle non-decomposable ancestral constraints. The proposed method is innovative in its ability to infer decomposable constraints from ancestral ones, which improves the efficiency of the oracle.
2. Empirical Validation: The authors provide comprehensive experiments across multiple benchmarks, varying dataset sizes, network sizes, and proportions of ancestral constraints. The results convincingly show the scalability and efficiency of the proposed approach compared to ILP-based methods.
3. Theoretical Rigor: The paper provides sound theoretical foundations for pruning the EC tree and inferring constraints, supported by formal theorems and proofs. This ensures the correctness of the proposed method.
4. Practical Significance: The work has practical implications for real-world applications where background knowledge (ancestral constraints) is available, as it demonstrates how such knowledge can significantly improve learning efficiency and accuracy.
Weaknesses:
1. Clarity: While the paper is technically sound, it is dense and difficult to follow in places, particularly for readers unfamiliar with the EC tree framework or Bayesian network structure learning. The inclusion of more intuitive examples and diagrams could improve accessibility.
2. Comparative Analysis: Although the paper compares its approach to ILP-based methods, it does not benchmark against other heuristic or dynamic programming methods for structure learning. This limits the scope of the evaluation.
3. Scalability Limits: While the approach scales well to networks with up to 20 variables, it is unclear how it performs for larger networks (e.g., 50+ variables), which are common in real-world applications. A discussion of potential bottlenecks and future scalability improvements would strengthen the paper.
4. Reproducibility: The paper does not provide sufficient implementation details for reproducing the results, such as parameter settings or the exact datasets used. Sharing code or pseudocode for key algorithms would enhance reproducibility.
Arguments for Acceptance:
- The paper addresses a challenging and important problem in Bayesian network structure learning, making a significant theoretical and empirical contribution.
- The proposed method demonstrates substantial efficiency improvements over existing approaches, which is critical for scaling to larger datasets and networks.
- The work is well-grounded in theory and provides a solid foundation for further research in this area.
Arguments Against Acceptance:
- The paper's clarity and accessibility could be improved, particularly for readers outside the immediate subfield.
- The evaluation could be broadened to include comparisons with other state-of-the-art methods beyond ILP-based approaches.
Recommendation:
I recommend acceptance of this paper, as it makes a meaningful contribution to the field of Bayesian network structure learning and demonstrates significant improvements in efficiency. However, the authors should address clarity issues and provide additional comparative analysis in the final version to strengthen the paper further.