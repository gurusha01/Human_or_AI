This paper addresses the challenging problem of estimating class priors and posterior distributions from positive-unlabeled (PU) data, extending the work of Jain et al., 2016 by incorporating noisy positives into the framework. The authors propose two classification algorithms that explicitly model noise in positive labels and leverage univariate transforms to handle high-dimensional data. By proving that these transforms preserve the class prior, the authors avoid computationally expensive kernel density estimation, which is a notable contribution. Simulations demonstrate improvements over Jain et al., 2016, suggesting the practical utility of the proposed methods.
Strengths:
1. Practical Relevance: The focus on noisy positive labels and high-dimensional data is highly relevant, as real-world datasets often exhibit these characteristics. The proposed algorithms address a significant gap in existing PU learning methods.
2. Theoretical Contributions: The proof that univariate transforms preserve class priors is a valuable theoretical insight, enabling efficient estimation in high-dimensional settings.
3. Empirical Validation: The paper provides simulation results that show improvements over Jain et al., 2016, supporting the efficacy of the proposed methods.
4. Clarity: The paper is well-organized and clearly written, making the methodology and results accessible to the reader.
Weaknesses:
1. Lack of Comparison with Recent Work: The paper does not cite or compare its methods with recent advancements (e.g., [1*]) that reportedly improve upon Jain et al., 2016. This omission weakens the paper's positioning within the current state of the art and raises questions about whether the proposed methods are truly competitive.
2. Limited Novelty: While the paper builds on Jain et al., 2016, the novelty and technical depth of the proofs appear incremental rather than groundbreaking. The reliance on univariate transforms, though practical, may not constitute a substantial theoretical advance.
3. Empirical Scope: The evaluation is limited to simulations, and it is unclear how the proposed methods perform on real-world datasets. Including experiments on diverse, noisy, high-dimensional datasets would strengthen the empirical claims.
4. Significance: While the methods are practical, the lack of a clear demonstration of their superiority over recent work diminishes their potential impact.
Recommendation:
While the paper makes a meaningful contribution to PU learning by addressing noise and high-dimensionality, the lack of engagement with recent work and limited novelty reduce its overall significance. I recommend conditional acceptance if the authors address the missing citations, provide comparisons with recent methods, and include experiments on real-world datasets. Otherwise, the paper may be more suitable for a specialized venue focused on incremental advances in PU learning.
Pro:
- Addresses a practical and relevant problem.
- Provides theoretical insights and empirical improvements over Jain et al., 2016.
Con:
- Limited novelty and lack of comparison with recent work.
- Evaluation restricted to simulations, reducing generalizability.