The paper presents a novel Structured Sparsity Learning (SSL) approach to compress Convolutional Neural Networks (CNNs) by applying group sparsity constraints, targeting filters, channels, filter shapes, and layer depth. The method achieves significant computational speedups while maintaining or even improving accuracy, as demonstrated across datasets like MNIST, CIFAR-10, and ImageNet. The proposed SSL framework involves pre-training a network, applying group sparsity during retraining, pruning inactive groups, and fine-tuning. The results are promising, with reported speedups of up to 5.1× on CPUs and 3.1× on GPUs for AlexNet, alongside minor or no accuracy loss.
Strengths:
1. Significant Practical Impact: The method addresses a critical bottleneck in deploying large-scale DNNs on resource-constrained devices by achieving structured sparsity, which is more hardware-friendly than non-structured sparsity.
2. Comprehensive Evaluation: The authors validate SSL across diverse datasets and architectures, including LeNet, ResNet, and AlexNet, demonstrating its generalizability and effectiveness.
3. Improved Accuracy: Beyond compression, SSL occasionally improves classification performance, such as increasing ResNet accuracy on CIFAR-10 while reducing its depth.
4. Conceptual Simplicity: The approach is straightforward and leverages well-established techniques like group Lasso, making it accessible for adoption and further exploration.
5. Hardware Efficiency: By focusing on structured sparsity, the method achieves practical speedups using standard libraries, avoiding the need for specialized hardware optimizations.
Weaknesses:
1. Lack of Implementation Details: Critical aspects such as the optimization process for group sparsity and the selection of regularization weights are not adequately detailed. This omission hinders reproducibility and practical adoption.
2. Ambiguity in Filter Deactivation: While the paper discusses sparsity at various levels, the mechanism for deactivating specific filter sites remains unclear, particularly for complex architectures.
3. Limited Real-World Applicability: Although SSL demonstrates strong results on benchmark datasets, the paper does not explore deployment scenarios or constraints in real-world applications, such as edge devices or latency-critical systems.
4. Baseline Comparisons: While comparisons with non-structured sparsity methods are included, the paper could benefit from more extensive benchmarking against state-of-the-art compression techniques like low-rank approximations or neural architecture search.
Suggestions for Improvement:
- Provide detailed explanations of the group sparsity optimization process and regularization weight selection, including hyperparameter tuning and validation strategies.
- Clarify the deactivation mechanism for specific filter sites and its implications for network architecture.
- Include experiments or discussions on SSL's performance in real-world deployment scenarios, such as mobile or embedded devices.
- Expand comparisons to include other compression methods to better contextualize SSL's advantages and trade-offs.
Recommendation:
This paper makes a strong contribution to the field of model compression and acceleration, particularly in its focus on structured sparsity for practical hardware efficiency. However, the lack of implementation details and real-world applicability limits its immediate impact. I recommend acceptance with minor revisions, contingent on the authors addressing the noted gaps in clarity and reproducibility.