The paper introduces a novel Structured Sparsity Learning (SSL) method to sparsify deep neural networks (DNNs) across multiple dimensions, including filters, channels, filter shapes, and layer depth. The authors aim to address the limitations of existing sparsity regularization and low-rank approximation techniques, which often lead to irregular memory access or require costly fine-tuning iterations. By leveraging group Lasso regularization, SSL dynamically learns compact, hardware-friendly DNN structures that reduce computation costs while maintaining or even improving classification accuracy. The experimental results demonstrate significant speedups (e.g., 5.1× on CPU and 3.1× on GPU for AlexNet) and improved accuracy in certain cases (e.g., ResNet-20 reduced to 18 layers with higher accuracy on CIFAR-10).
Strengths:
1. Practical Contribution: The SSL method addresses a critical challenge in deploying DNNs on resource-constrained devices by achieving structured sparsity that is hardware-friendly, unlike non-structured sparsity methods.
2. Comprehensive Sparsification: The paper extends sparsity regularization to multiple dimensions (filters, channels, shapes, and depth), offering a holistic approach to DNN compression.
3. Experimental Results: The method demonstrates substantial speedups and accuracy improvements across various architectures (e.g., LeNet, AlexNet, ResNet) and datasets (MNIST, CIFAR-10, ImageNet).
4. Theoretical Rigor: The use of group Lasso regularization is well-motivated and effectively applied to achieve structured sparsity.
5. Reproducibility: The authors provide source code, which enhances the transparency and usability of the proposed method.
Weaknesses:
1. Limited Comparisons: While the paper mentions Group-wise Brain Damage (GBD) as a related method, it does not provide a direct experimental comparison. GBD has shown better performance in specific cases (e.g., Conv1 in AlexNet), and a head-to-head evaluation would strengthen the claims.
2. Choice of Networks: The experiments primarily focus on over-parameterized networks like LeNet and AlexNet, which are easier to compress. Testing on more optimized architectures such as GoogLeNet, SqueezeNet, or ResNet-152 would better demonstrate the generalizability and effectiveness of SSL.
3. Clarity of Presentation: While the paper is generally well-written, some sections (e.g., the mathematical formulations) could benefit from clearer explanations and more intuitive visualizations to aid understanding.
4. Hardware-Specific Optimizations: Although the paper claims hardware-friendliness, it does not explore how SSL interacts with specialized hardware accelerators (e.g., TPUs or FPGAs), which could provide additional insights.
Arguments for Acceptance:
- The method is novel, practical, and addresses a significant challenge in DNN deployment.
- The results demonstrate meaningful improvements in both computational efficiency and accuracy.
- The approach is theoretically sound and well-supported by experiments.
Arguments Against Acceptance:
- The lack of direct comparison with GBD leaves a gap in evaluating the method's relative performance.
- The focus on over-parameterized networks limits the scope of the findings.
- The paper could benefit from more clarity in its presentation and additional experiments on modern, optimized architectures.
Recommendation:
Overall, the paper makes a valuable contribution to the field of DNN compression and sparsification. However, addressing the weaknesses—particularly the comparison with GBD and testing on more optimized networks—would significantly strengthen the work. I recommend acceptance with minor revisions, contingent on the inclusion of additional experiments and comparisons to better contextualize the results.