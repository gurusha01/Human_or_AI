The paper presents a Structured Sparsity Learning (SSL) method that leverages group Lasso regularization to prune weights in deep neural networks (DNNs) while maintaining structured sparsity. The proposed approach targets filters, channels, filter shapes, and layer depth, aiming to achieve hardware-friendly sparsity that improves computational efficiency without sacrificing accuracy. The authors claim that SSL achieves significant speedups (5.1× on CPU and 3.1× on GPU for AlexNet) and even improves classification accuracy in some cases, such as reducing ResNet-20 to 18 layers while increasing accuracy on CIFAR-10. The method is compared against non-structured sparsity and low-rank approximation, with SSL purportedly offering better trade-offs between speedup and accuracy.
Strengths:
1. Preservation of Structure: The focus on structured sparsity is a practical contribution, addressing the inefficiencies of non-structured sparsity in hardware implementations.
2. Comprehensive Evaluation: The paper evaluates SSL across multiple datasets (MNIST, CIFAR-10, ImageNet) and architectures (LeNet, AlexNet, ResNet), demonstrating its applicability.
3. Hardware Efficiency: The use of structured sparsity to achieve speedups with off-the-shelf libraries is a notable strength, as it avoids reliance on custom hardware.
4. Improved Accuracy: The ability to improve accuracy while reducing network depth (e.g., ResNet-20 to ResNet-18) is an interesting finding.
Weaknesses:
1. Lack of Novelty: The use of group Lasso for structured pruning is not novel. Similar techniques have been explored in prior works, such as Group-wise Brain Damage and other group sparsity methods.
2. Underwhelming Results: While the reported speedups are significant, the improvements in accuracy are marginal (e.g., ~1% for AlexNet) and may not justify the method's complexity. Additionally, the speedups are context-dependent and primarily evaluated in GEMM-based settings.
3. Limited Theoretical Insights: The paper lacks a strong theoretical foundation to explain why SSL outperforms other structured pruning methods or how it generalizes across architectures.
4. Clarity Issues: The paper is dense and could benefit from clearer explanations of the methodology, particularly in the mathematical formulations and their practical implications.
Arguments for Acceptance:
- The focus on structured sparsity is relevant for real-world applications where hardware efficiency is critical.
- The experimental results demonstrate the method's applicability across diverse architectures and datasets.
Arguments Against Acceptance:
- The lack of novelty in using group Lasso for structured pruning makes the contribution incremental rather than groundbreaking.
- The experimental results, while comprehensive, are not compelling enough to offset the lack of originality.
- The paper does not sufficiently differentiate itself from prior work, both in terms of methodology and results.
Recommendation: Reject. While the paper addresses an important problem and provides a practical solution, its lack of novelty and underwhelming experimental results make it unsuitable for publication at a top-tier venue like NIPS. The authors are encouraged to explore more innovative approaches or provide deeper theoretical insights to strengthen their contribution.