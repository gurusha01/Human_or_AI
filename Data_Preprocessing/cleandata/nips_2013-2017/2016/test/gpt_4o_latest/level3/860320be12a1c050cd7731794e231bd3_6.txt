Review of the Paper
This paper introduces a graph-theoretic framework to analyze the architectural complexity of recurrent neural networks (RNNs) and defines three complexity measures: recurrent depth, feedforward depth, and recurrent skip coefficient. These measures aim to capture the over-time nonlinearity, input-output nonlinearity, and information propagation speed in RNNs, respectively. While the authors provide a rigorous theoretical foundation for these measures and demonstrate their computability, the practical value of the graph-theoretic framework beyond proving computability remains limited. The paper also presents empirical evaluations to validate the proposed measures, but the experimental design and presentation leave room for improvement.
Strengths:
1. Novelty and Formalization: The paper addresses an underexplored aspect of RNNs—connecting architectures—and provides a formal graph-theoretic framework. The definitions of recurrent depth, feedforward depth, and recurrent skip coefficient are intuitive and theoretically sound.
2. Computability Proofs: The authors rigorously prove the existence and computability of the proposed measures, which is a significant theoretical contribution.
3. Relevance to Long-Term Dependencies: The recurrent skip coefficient is a meaningful addition, as it directly relates to the challenges of learning long-term dependencies, a critical issue in RNN research.
4. Empirical Evidence: The experiments suggest that increasing recurrent depth and skip coefficients can improve performance on certain tasks, offering insights into architectural design.
Weaknesses:
1. Limited Value of the Graph-Theoretic Framework: While the formal framework is mathematically rigorous, its practical utility beyond proving computability is unclear. The insights provided by the measures are either unsurprising or already known in the field.
2. Experimental Design Issues: The experiments lack systematic evaluation. For instance, the choice of datasets (e.g., PennTreebank, text8, MNIST) is inconsistent, making it difficult to draw meaningful comparisons across architectures. Additionally, the influence of model size on performance differences (Table 1) is not adequately controlled or clarified.
3. Presentation of Results: Key results, such as those for MNIST and the adding/copying problems, are buried in the text and should be presented in table format for clarity. The surprising results in Table 2, such as permuted MNIST outperforming unpermuted MNIST, are not explained and require further investigation.
4. Lack of Statistical Rigor: The paper does not report error bars or statistical significance for the experimental results, which undermines the reliability of the conclusions.
5. Vague Insights: The experimental conclusions about the proposed measures are not particularly novel or actionable. For example, the finding that recurrent depth should not be too large or too small is already well-understood in the literature.
Arguments for Acceptance:
- The paper provides a rigorous theoretical framework and formalizes intuitive concepts like recurrent depth and skip coefficients.
- The proposed measures are relevant for understanding RNN architectures and could inspire future research on architectural design.
Arguments Against Acceptance:
- The practical impact of the graph-theoretic framework is limited, and the insights derived from the measures are not groundbreaking.
- The experimental design and presentation are flawed, with inconsistent datasets, unclear controls, and a lack of statistical rigor.
- The conclusions are vague and fail to provide actionable guidance for practitioners.
Recommendation: Weak Reject. While the theoretical contributions are solid, the paper falls short in experimental rigor and practical significance. Addressing the methodological issues and providing clearer insights would significantly strengthen the paper.