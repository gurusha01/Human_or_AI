Review of "Learning to Learn with Recurrent Neural Networks"
This paper proposes a novel approach to optimization by training an LSTM-based optimizer to learn update rules, replacing traditional hand-designed optimizers. The authors demonstrate the effectiveness of their method on tasks such as quadratic optimization, training small MLPs on MNIST, and style transfer tasks, showing faster convergence compared to standard optimizers like SGD, RMSprop, and ADAM. The paper also highlights the generalization capabilities of the learned optimizer to tasks with similar structures, such as different architectures and datasets.
Strengths:
1. Core Contribution: The idea of casting optimizer design as a learning problem is compelling and aligns with the broader trend of automating hand-designed components in machine learning. The use of LSTMs to model dynamic update rules is novel and well-motivated, leveraging their ability to process sequential data.
2. Empirical Results: The experiments demonstrate that the LSTM optimizer outperforms traditional optimizers on the tasks it was trained for and generalizes reasonably well to related tasks. The results on style transfer and CIFAR-10 tasks are particularly promising, showcasing the method's potential for practical applications.
3. Transfer Learning: The paper makes a strong case for the generalization capabilities of the learned optimizer, which is a critical property for real-world usability. The ability to adapt to different architectures and datasets is a significant advantage.
Weaknesses:
1. Paper Quality: While the topic is important, the paper feels rushed and lacks depth in key areas. For instance, the discussion of the weighting scheme (equal weights for all timesteps) is superficial and does not address potential trade-offs between early and late-stage optimization.
2. Training Details: The paper omits critical details about the training process, such as the number of optimization runs required to train the LSTM optimizer. This omission makes it difficult to assess the computational cost of the proposed approach.
3. Short Optimization Runs: The experiments on MNIST involve optimizers running for only 1/5 of an epoch, which raises concerns about whether the method is truly competitive over longer horizons. The lack of exploration of longer optimization runs is a missed opportunity to demonstrate robustness.
4. Runtime and Memory: The paper does not discuss the runtime and memory overhead of the LSTM optimizer compared to traditional methods. Given the high state requirements of LSTMs, this omission is significant, as practical adoption would depend on these factors.
5. Generalization Analysis: The generalization experiments are limited in scope, with minimal architectural changes and no error bars provided. A more systematic investigation across diverse tasks and datasets would strengthen the claims.
6. Alternative Architectures: The paper exclusively focuses on LSTMs without exploring alternative architectures (e.g., GRUs or Transformers). This leaves open the question of whether LSTMs are uniquely suited for this task or if other architectures could perform better.
7. Minor Issues: The paper suffers from unclear phrasing, missing details (e.g., number of functions averaged in experiments), sloppy plots without axis labels, and reliance on handcrafted pre/post-processing, which contradicts the stated goal of removing handcrafting.
Arguments for Acceptance:
- The paper tackles an important problem and presents a novel approach with promising results.
- The empirical results demonstrate the potential of learned optimizers to outperform traditional methods in specific settings.
- The idea of automating optimizer design could inspire further research in meta-learning and optimization.
Arguments Against Acceptance:
- The paper lacks depth in its analysis and omits critical details, such as training costs and runtime comparisons.
- The experiments are limited in scope, with short optimization runs and insufficient exploration of generalization.
- The reliance on handcrafted pre/post-processing undermines the claim of automation.
Recommendation:
While the paper introduces an interesting and potentially impactful idea, its execution falls short in several areas, particularly in terms of depth, clarity, and experimental rigor. I recommend rejection in its current form, but I encourage the authors to address the identified weaknesses and resubmit. With more comprehensive experiments, clearer presentation, and additional analysis, this work could make a valuable contribution to the field.