The paper introduces a neural network-based method for online sequence-to-sequence mapping, termed the Neural Transducer, which builds upon the encoder-decoder architecture. The model is designed to address the limitations of traditional sequence-to-sequence models, particularly their inability to generate outputs incrementally as input data arrives. The authors propose a dynamic programming-based training approach to handle the discrete decision-making process inherent in the model and evaluate its performance on a toy addition task and the TIMIT phoneme recognition dataset.
Strengths:
1. Relevance and Novelty: The topic of online sequence prediction is highly relevant, as it addresses practical challenges in real-time applications like speech recognition and translation. The proposed Neural Transducer offers a novel approach by conditioning predictions on both the partially observed input and the partially generated output.
2. Attention Mechanism Variants: The introduction of two attention mechanism variants (MLP-attention and LSTM-attention) is a valuable contribution, particularly the LSTM-attention, which addresses limitations of existing attention models.
3. Empirical Analysis: The paper provides a detailed analysis of the impact of block size, attention mechanisms, and model architecture on performance. The comparison with a sequence-to-sequence model that resets RNN states is insightful.
4. Incremental Output Capability: The model's ability to produce outputs incrementally as data arrives is a significant advancement over traditional sequence-to-sequence models, which require the entire input sequence for decoding.
Weaknesses:
1. Lack of Baseline Comparisons: The paper does not compare its method against Graves' sequence transducer (2012), which is a critical baseline for this work. This omission limits the contextualization and novelty of the proposed approach.
2. Empirical Evaluation Scope: While the TIMIT results are decent, the evaluation is limited to relatively short sequences. Testing on datasets with longer sequences, such as Wall Street Journal or Switchboard, would better demonstrate the model's advantages.
3. Regularization Argument: The authors' decision not to use regularization methods is unconvincing, especially since related work (e.g., attention and windowing) has shown the benefits of regularization in improving performance.
4. Clarity in Prior Work Citation: The term "transducer" is used without proper citation of prior work, particularly Graves (2012). This oversight detracts from the paper's clarity and acknowledgment of foundational contributions.
5. Title Precision: The title could better reflect the paper's focus, such as "An Online Neural Transducer" or "An Incremental Neural Transducer."
Recommendation:
While the paper addresses a relevant and challenging problem, the lack of comparison with Graves' transducer and limited empirical evaluation weaken its overall contribution. The strengths, including the novel attention mechanisms and incremental output capability, are notable, but the paper requires additional experiments and stronger baselines to substantiate its claims. I recommend conditional acceptance provided the authors address the baseline comparison and expand the evaluation to longer sequence datasets. 
Arguments for Acceptance:
- Tackles an important and open problem in neural networks.
- Proposes novel attention mechanisms and a dynamic programming-based training approach.
- Provides incremental output capability, which is critical for real-time applications.
Arguments Against Acceptance:
- Lacks comparison with key prior work (Graves' transducer).
- Limited empirical evaluation scope.
- Insufficient justification for excluding regularization techniques.