This paper introduces a novel framework for solving the Learning with Expert Advice (LEA) and Online Linear Optimization (OLO) problems by reducing them to the coin-betting problem. The authors propose parameter-free algorithms for both LEA and OLO, achieving improved regret bounds for LEA and matching state-of-the-art regret bounds for OLO. The key innovation lies in leveraging the Krichevsky-Trofimov estimator within a potential-based framework for coin betting, which leads to simple algorithms that eliminate the need for parameter tuning. This approach establishes an intriguing connection between online learning algorithms and coin-betting strategies, contributing to the originality of the work.
The paper is well-written and provides a clear exposition of the proposed methods, including detailed theoretical analysis and empirical validation. The reductions from coin betting to LEA and OLO are elegant, and the resulting algorithms are computationally efficient, with optimal worst-case regret bounds. The authors also demonstrate the practical relevance of their approach through experiments, showing that the proposed algorithms perform competitively with existing methods, even under oracle tuning.
However, there are some concerns and limitations. First, the proof of Lemma 14 appears to contain a potential error, which raises doubts about the correctness of Corollaries 5 and 6. While the lemma itself may hold with a revised proof, this issue needs to be addressed to ensure the validity of the results. Second, the potential function used in the framework is complex and unintuitive, which may hinder its broader applicability and adoption. Additionally, the proposed algorithm for OLO focuses on the unconstrained version, leaving its applicability to the more common constrained version unclear.
Strengths:
1. Novelty: The reduction of LEA and OLO to coin betting is an original and insightful contribution.
2. Parameter-free algorithms: The elimination of parameter tuning is a significant practical advantage.
3. Theoretical rigor: The paper provides strong regret guarantees and connects the proposed methods to existing work.
4. Clarity: The paper is well-organized and easy to follow, with detailed explanations of the algorithms and proofs.
Weaknesses:
1. Proof error: The potential issue in Lemma 14 undermines confidence in some of the results.
2. Complexity of potential function: The unintuitive nature of the potential function may limit the framework's accessibility.
3. Limited scope: The focus on unconstrained OLO leaves questions about the applicability to constrained settings.
Recommendation:
Overall, this paper makes a valuable contribution to the field of online learning by introducing a novel and elegant framework for parameter-free algorithms. However, the authors should address the proof error in Lemma 14 and clarify the applicability of their methods to constrained OLO. With these revisions, the paper would be a strong candidate for acceptance.