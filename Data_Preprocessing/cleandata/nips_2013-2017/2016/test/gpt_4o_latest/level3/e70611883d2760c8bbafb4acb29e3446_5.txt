This paper extends the work of Rahimi and Recht on random features by introducing an efficient optimization method for learning kernels in a supervised manner. The authors propose a two-phase procedure: first, they optimize a distribution over random features to align the kernel with the target labels, and then use these optimized features in a standard supervised learning framework. The paper provides theoretical guarantees for the consistency of the learned kernel and its generalization performance, and it demonstrates the method's scalability and effectiveness through empirical evaluations on synthetic and benchmark datasets.
Strengths:
1. Novel Contribution: The paper addresses a critical limitation of existing random feature methods by enabling kernel learning rather than relying on user-defined kernels. This is a meaningful extension of Rahimi and Recht's work and has the potential to reduce sensitivity to the choice of base kernels.
2. Efficiency: The proposed optimization problem is efficiently solvable in near-linear time, making the method scalable to large datasets. The sparsity induced in the learned kernel further reduces computational costs.
3. Theoretical Guarantees: The authors provide rigorous consistency and generalization bounds, which strengthen the paper's technical foundation and ensure the reliability of the proposed method.
4. Empirical Validation: The method is empirically validated on synthetic and real-world datasets, demonstrating competitive performance with significantly reduced training costs compared to baseline methods.
Weaknesses:
1. Clarity of Mathematical Steps: While the key concepts are sound, some mathematical derivations, particularly in the optimization problem and its solution, are not entirely clear. Additional explanations or examples would help improve accessibility for readers.
2. Limited Comparisons: The empirical evaluations focus primarily on comparisons with Rahimi and Recht's random features and a joint optimization method. Broader comparisons with other efficient kernel learning methods are missing, despite claims of general superiority.
3. Heuristic Aspect: The heuristic nature of the optimization procedure impacts its efficiency, but the paper does not evaluate the trade-offs introduced by these heuristics. For instance, the choice of divergence measures and their effect on performance is not thoroughly explored.
4. Scope of Kernels: The experiments are limited to three kernels (Gaussian, linear, and arc-cosine). A wider range of kernel types would better demonstrate the generality of the proposed approach.
Arguments for Acceptance:
- The paper makes a significant and novel contribution to kernel learning, addressing a key limitation of existing random feature methods.
- The proposed method is efficient, scalable, and supported by strong theoretical guarantees.
- Empirical results demonstrate competitive performance with reduced computational costs.
Arguments Against Acceptance:
- The clarity of the mathematical exposition could be improved, which may hinder reproducibility.
- The lack of broader comparisons with other kernel learning methods weakens the empirical validation.
- The heuristic aspects of the approach are not sufficiently evaluated, leaving questions about robustness and general applicability.
Recommendation:
Overall, this paper makes a valuable contribution to the field of kernel learning and random features. While there are areas for improvement, particularly in clarity and broader empirical comparisons, the strengths of the work outweigh its weaknesses. I recommend acceptance, with the suggestion that the authors address the noted concerns in the final version.