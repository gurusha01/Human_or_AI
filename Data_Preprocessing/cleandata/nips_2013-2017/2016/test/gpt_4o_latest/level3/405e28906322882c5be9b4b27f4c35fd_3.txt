This paper investigates the dynamic regret of online learning with \(K\) actions in nonstationary stochastic environments under both full-information and bandit settings. The authors introduce a novel parameter, \(\Lambda\), which measures the total variance of loss distributions, and analyze its interplay with other parameters like \(\Gamma\) (number of distribution switches) and \(V\) (total drift of distributions). The primary contribution is the derivation of new upper and lower bounds on dynamic regret, highlighting the role of \(\Lambda\) in determining regret performance. Notably, the paper shows that in the full-information setting, constant regret is achievable under certain conditions, while in the bandit setting, regret inevitably grows with \(T\).
Strengths:
1. Novelty and Theoretical Contribution: The introduction of \(\Lambda\) as a parameter to measure variance and its integration into dynamic regret analysis is a meaningful addition to the literature. The derived bounds provide a nuanced understanding of how variance impacts regret in nonstationary environments.
2. Clarity of Writing and Analysis: The paper is well-written and demonstrates a strong grasp of technical tools. The theoretical results are supported by rigorous proofs, and the interplay between \(\Lambda\), \(\Gamma\), and \(V\) is explored in depth.
3. Comprehensive Framework: By bridging the stochastic and adversarial settings, the paper provides a unified perspective on regret analysis in nonstationary environments, which could inspire future research.
Weaknesses:
1. Practical Applicability: While the theoretical results are robust, the proposed algorithms are parameter-dependent and lack practical applicability. The parameter-free algorithm achieves weaker regret bounds, limiting its utility in real-world scenarios.
2. Scaling Issues: The scaling of \(\Lambda\) with \(K\) introduces worse regret bounds (\(K^{1/3}\)) compared to existing results (\(\log K\)) in the full-information setting. This raises concerns about the practicality of the proposed framework for large action spaces.
3. Clarity and Organization: Theorem 4.3 requires clarification regarding the problem dependency of the lower bounds. Additionally, the repeated and clumsy placement of the dynamic regret definition detracts from the paper's readability.
4. Incomplete Context: Theorems do not explicitly state whether they pertain to the full-information or bandit setting, which could confuse readers. Furthermore, a missing reference to vUCB (Audibert et al., 2009) and an unclear connection between Equation (5) and the cited algorithms suggest gaps in related work coverage.
5. Improved Bounds: Theorem 3.4 could achieve better bounds using Steinhardt and Liang's algorithm, which depends on the path length of the best expert. This omission limits the paper's contribution to advancing state-of-the-art bounds.
Recommendation:
While the paper makes significant theoretical contributions, its practical limitations and scaling issues temper its impact. I recommend acceptance with minor revisions, contingent on addressing clarity issues (e.g., Theorem 4.3 and dynamic regret definition), improving the contextualization of results, and acknowledging the limitations of the proposed algorithms more explicitly. The paper provides valuable insights into the role of variance in dynamic regret but would benefit from a stronger emphasis on practical relevance and connections to existing algorithms.