The paper presents a novel Homotopy Smoothing (HOPS) algorithm for solving a class of structured non-smooth optimization problems, achieving a lower iteration complexity of \( \tilde{O}(1/\epsilon^{1-\theta}) \), where \( \theta \in (0, 1] \) captures the local sharpness of the objective function. By leveraging Nesterov's smoothing and acceleration techniques, combined with insights from Yang and Lin's local error bound results, the authors demonstrate significant improvements over existing methods, particularly for problems without strong convexity assumptions. The paper also introduces a primal-dual variant (PD-HOPS) to automate parameter tuning, further enhancing practical applicability.
Strengths:
1. Technical Contributions: The proposed HOPS algorithm achieves faster convergence rates than the standard \( O(1/\epsilon) \) for non-smooth problems, under mild local error bound conditions. This is a significant theoretical advancement, particularly for applications like empirical risk minimization, cone programming, and image processing.
2. Novelty: The homotopy strategy for progressively reducing the smoothing parameter is well-motivated and rigorously analyzed. The connection to local error bounds and the Kurdyka-≈Åojasiewicz property broadens the applicability to a wide range of non-smooth functions.
3. Clarity and Presentation: The paper is well-written, with clear explanations of the algorithm, its theoretical guarantees, and its relationship to prior work. Familiar optimization techniques are elegantly combined to achieve novel results.
4. Experimental Validation: The experimental results convincingly demonstrate the effectiveness of HOPS and PD-HOPS, showing significant improvements over state-of-the-art methods like Nesterov's smoothing and primal-dual approaches.
Weaknesses:
1. Dependence on Constants: The algorithm relies on prior knowledge of constants \( c \) and \( \theta \), which may not be readily available in practice. While PD-HOPS partially addresses this, a more detailed discussion on estimating these constants or making practical assumptions would strengthen the paper.
2. Comparisons to Related Work: While the paper references related works, a deeper discussion and empirical comparisons with methods like [1,2] would provide a more comprehensive evaluation of the contributions.
3. Ambiguities and Notational Issues: Specific statements, such as the one on line 231 regarding the polyhedral epigraph, are flagged as nonsensical and require clarification. Additionally, the notation for the dual norm and Equation (11) could be refined for better readability.
4. Primal-Dual Variant: The discussion of PD-HOPS (Section 4.4) is underdeveloped. A more detailed theoretical analysis and practical insights would enhance its impact.
5. Dimension Independence: The paper does not adequately address whether the constant \( c \) is independent of the problem dimension in applications. This is a critical point that should be clarified.
Suggestions for Improvement:
1. Expand the discussion on estimating \( c \) and \( \theta \), and provide practical guidelines for their use.
2. Clarify ambiguous statements (e.g., line 231) and refine the notation for better clarity.
3. Provide a more detailed comparison with related works, both theoretically and experimentally.
4. Strengthen the discussion of PD-HOPS, including its theoretical guarantees and practical implementation details.
5. Discuss the dimension dependence of \( c \) explicitly, especially in the context of large-scale applications.
Recommendation:
The paper makes a significant theoretical and practical contribution to non-smooth optimization, with a novel algorithm that improves convergence rates under mild assumptions. However, the reliance on prior knowledge of constants, limited discussion of related works, and underdeveloped sections (e.g., PD-HOPS) leave room for improvement. With revisions addressing these concerns, the paper would be a strong candidate for acceptance. Recommendation: Accept with minor revisions.