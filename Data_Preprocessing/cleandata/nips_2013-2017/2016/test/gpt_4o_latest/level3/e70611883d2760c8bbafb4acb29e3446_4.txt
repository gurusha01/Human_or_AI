This paper presents an extension of Rahimi and Recht's randomized feature approach for kernel approximation by introducing a supervised kernel learning method. The authors propose a two-phase procedure that optimizes the distribution of random features to align with the labels, improving generalization performance while maintaining computational efficiency. The key contribution lies in solving an optimization problem (equation (4)) to derive a weighted distribution \(\hat{q}\), which is then used to construct the kernel in the binary classification setting. The authors provide theoretical guarantees for the consistency of the learned kernel and its generalization performance, and they demonstrate the method's scalability and effectiveness through empirical evaluations.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with well-supported claims through theoretical analysis (e.g., consistency guarantees in Theorem 1) and empirical results. The derivation of equation (4) and its efficient solution using divergence measures is a notable contribution.
2. Improved Generalization: The proposed method improves generalization error compared to Rahimi and Recht's original work, as evidenced by both theoretical bounds and experimental results.
3. Computational Efficiency: The optimization problem (4) is efficiently solvable in near-linear time, and the sparsity of \(\hat{q}\) reduces the dimensionality of subsequent learning tasks. This makes the method scalable to large datasets.
4. Practical Utility: The method achieves competitive performance on benchmark datasets with significantly reduced training time compared to joint kernel and classifier optimization approaches. The feature selection application in high-dimensional biological data is particularly compelling.
5. Clarity and Organization: The paper is well-organized, with clear explanations of the methodology, theoretical results, and experimental setup. The inclusion of visualizations and detailed comparisons enhances readability.
Weaknesses:
1. Incremental Contribution: While the extension to Rahimi and Recht's framework is valuable, the differences from the original work are relatively minimal. The novelty lies more in the efficient optimization and practical implementation rather than a fundamentally new approach.
2. Limited Scope of Divergences: The theoretical results focus on specific \(f\)-divergences (e.g., \(\chi^2\)-divergence), and it is unclear how well the method generalizes to other divergence measures or kernel types.
3. Empirical Comparisons: Although the method is compared to baseline randomized feature approaches and joint optimization, additional comparisons with other state-of-the-art kernel learning methods (e.g., deep kernel learning) would strengthen the evaluation.
4. Dependence on Base Kernel: The method still requires a user-defined base kernel, and while it improves upon poor initial kernels, its performance is sensitive to the choice of the base distribution \(P_0\).
Arguments for Acceptance:
- The paper addresses a relevant and challenging problem in kernel learning, offering a computationally efficient and theoretically grounded solution.
- The method demonstrates strong empirical performance and practical utility in diverse applications.
- The theoretical contributions, particularly the consistency and generalization guarantees, are significant and well-executed.
Arguments Against Acceptance:
- The incremental nature of the contribution may limit its impact, as it builds closely on existing work.
- The empirical evaluation could be expanded to include more diverse baselines and datasets.
Recommendation:
Overall, this paper makes a solid contribution to the field of randomized feature methods and kernel learning. While the novelty is somewhat incremental, the combination of theoretical rigor, practical efficiency, and empirical validation makes it a valuable addition to the conference. I recommend acceptance, with minor revisions to address the scope of comparisons and clarify the generalizability of the approach.