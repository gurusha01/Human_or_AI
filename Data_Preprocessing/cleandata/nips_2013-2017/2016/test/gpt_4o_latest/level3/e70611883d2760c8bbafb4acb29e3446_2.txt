The paper proposes a supervised kernel learning method leveraging random features, addressing the challenge of selecting an appropriate kernel for a given problem. The authors formulate the method as a kernel alignment problem and solve it efficiently using a convex optimization procedure. Theoretical contributions include consistency guarantees for the learned kernel and generalization bounds for the induced estimators. Empirical evaluations demonstrate the method's scalability and competitive performance on benchmark datasets, with reduced computational costs compared to traditional kernel learning approaches.
Strengths:
1. Technical Contribution: The paper makes a strong technical contribution by combining kernel learning with randomized features, resulting in a scalable and computationally efficient method. The use of f-divergences to define the space of kernels is innovative and well-motivated.
2. Theoretical Rigor: The authors provide consistency guarantees and generalization bounds, which are critical for establishing the robustness of the proposed method. These results are clearly derived and add significant value to the work.
3. Clarity: The paper is well-written and organized. The theoretical and empirical sections are presented with sufficient detail, making the methodology accessible to readers with a background in kernel methods and random features.
4. Empirical Evaluation: The experiments are comprehensive, covering synthetic data, feature selection tasks, and benchmark datasets. The results convincingly demonstrate the method's efficiency and competitive performance, particularly in scenarios where the base kernel is poorly suited to the data.
Weaknesses:
1. Missing Comparisons: The paper does not include comparisons with standard multiple kernel learning (MKL) methods, such as those using linear combinations of Gaussian kernels or other kernel learning techniques. This omission limits the ability to contextualize the proposed method's performance relative to established approaches.
2. Baseline Optimization: The empirical evaluation could be strengthened by including a baseline using a Gaussian kernel optimized via model selection (e.g., cross-validation). This would provide a more direct comparison to standard practices in kernel-based learning.
3. Empirical Depth: While the experiments are broad, some analyses, such as the sensitivity of the method to hyperparameters (e.g., œÅ in the f-divergence), are not explored in depth. This could help practitioners better understand the method's practical applicability.
Recommendation:
I recommend acceptance of this paper, with the suggestion to address the missing comparisons and include a Gaussian kernel baseline in the final version. The paper makes a significant contribution to the field by advancing kernel learning in a computationally efficient manner, supported by strong theoretical and empirical results. Its relevance to the NIPS community is clear, as it addresses scalability challenges in kernel methods, a topic of ongoing interest.
Arguments for Acceptance:
- Strong technical and theoretical contributions.
- Clear and well-organized presentation.
- Demonstrated scalability and competitive performance in empirical evaluations.
Arguments Against Acceptance:
- Missing comparisons with standard MKL methods.
- Lack of a Gaussian kernel baseline optimized via model selection.
Overall, the paper advances the state of the art in kernel learning and is a valuable addition to the conference.