The paper introduces a novel Homotopy Smoothing (HOPS) algorithm for solving structured non-smooth optimization problems, improving upon the iteration complexity of existing methods. By employing a phased approach to Nesterov's smoothing technique, the authors gradually decrease the smoothing parameter while leveraging warm-starting between phases. This strategy, supported by a rigorous analysis of local error bounds, achieves a lower iteration complexity of \(Õ(1/ε^{1-θ})\), where \(θ \in (0, 1]\) captures the local sharpness of the objective function. The results are particularly significant for problems satisfying local error bounds, such as polyhedral feasibility, cone programming, and certain machine learning tasks. The paper also extends the method to a primal-dual version (PD-HOPS), which automates parameter tuning and further accelerates convergence.
Strengths:
1. Technical Contribution: The paper rigorously analyzes a homotopy smoothing approach, filling a gap in the literature where such methods lacked theoretical guarantees. The use of local error bounds to improve iteration complexity is both novel and impactful.
2. Clarity and Organization: The paper is well-written, with a clear exposition of the problem, related work, and the proposed method. The introduction of Nesterov's smoothing technique and its limitations provides a strong foundation for the proposed approach.
3. Experimental Validation: The experiments convincingly demonstrate the superiority of HOPS and PD-HOPS over state-of-the-art methods, particularly in terms of runtime and scalability for small \(ε\).
4. Generality: The method applies to a broad class of problems, including those with non-smooth loss functions and regularizers, making it relevant to various domains like machine learning, image processing, and optimization.
Weaknesses:
1. Novelty of Linear Convergence: While the paper claims linear convergence for certain problems, such as polyhedral feasibility and cone programming, this result is not entirely novel. Similar guarantees have been shown in prior work, such as Hoffman's bound and related studies. The authors should clarify how their approach differs in practice and theory.
2. Parameter Sensitivity: The iteration complexity depends on constants like the local error bound parameter \(c\) and sharpness \(θ\), which are problem-specific and may not always be easy to estimate or satisfy. While PD-HOPS mitigates this issue, further discussion on practical parameter tuning would strengthen the paper.
3. Broader Claims: The claim of achieving the "lowest iteration complexity" for non-smooth problems without strong convexity could benefit from a more nuanced discussion. A Google search reveals prior work on first-order methods with linear convergence for LPs, suggesting the need for a more careful comparison.
4. Limited Novelty in Applications: While the theoretical framework is robust, the applications (e.g., hinge loss, TV denoising) are standard benchmarks. Exploring more challenging or novel applications could enhance the paper's impact.
Recommendation:
The paper makes a strong theoretical and practical contribution to optimization, particularly in its rigorous analysis of homotopy smoothing. However, the novelty of some claims (e.g., linear convergence) and the practical implications of parameter sensitivity warrant further clarification. I recommend acceptance, provided the authors address the above concerns, particularly by refining their claims and expanding the discussion on parameter effects and iteration bounds.
Arguments for Acceptance:
- Rigorous theoretical analysis of a novel homotopy smoothing method.
- Significant improvement in iteration complexity for a broad class of problems.
- Strong experimental results demonstrating practical benefits.
Arguments Against Acceptance:
- Some claims (e.g., linear convergence) overlap with prior work.
- Practical challenges in parameter tuning are not fully addressed.
- Broader claims about iteration complexity need more careful justification. 
Overall, the paper is a valuable contribution to the field and aligns well with the conference's focus on advancing optimization techniques.