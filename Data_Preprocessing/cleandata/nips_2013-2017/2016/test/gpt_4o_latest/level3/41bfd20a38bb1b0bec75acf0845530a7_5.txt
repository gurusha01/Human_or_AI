The paper presents a novel Structured Sparsity Learning (SSL) method to simplify deep neural network (DNN) parameters by regularizing their structures, such as filters, channels, filter shapes, and layer depth. The goal is to achieve hardware-friendly compactness for faster computation with minimal accuracy loss, which is particularly relevant for resource-constrained platforms. The authors demonstrate significant speedups (5.1× on CPU and 3.1× on GPU for AlexNet) and even accuracy improvements in some cases (e.g., CIFAR-10 ResNet reduced from 20 to 18 layers with a 1.35% accuracy gain). The method builds on prior work in sparsity regularization and low-rank approximation but extends these ideas by targeting structured sparsity, which better aligns with hardware acceleration.
Strengths:
1. Technical Contribution: The paper introduces a comprehensive approach to structured sparsity, addressing multiple dimensions of DNN structure (filters, channels, shapes, and depth). This is a significant improvement over non-structured sparsity, which often results in irregular memory access and limited hardware acceleration.
2. Experimental Results: The authors provide extensive experimental validation across multiple datasets (MNIST, CIFAR-10, ImageNet) and architectures (LeNet, ConvNet, ResNet, AlexNet). The results convincingly demonstrate the method's ability to reduce computational costs while maintaining or improving accuracy.
3. Practical Relevance: The focus on hardware-friendly sparsity makes the work highly relevant for deployment on resource-constrained devices, addressing a critical bottleneck in DNN applications.
Weaknesses:
1. Hardware-Specific Optimization: The paper does not adequately address how the proposed method interacts with specific hardware platforms (e.g., CPU, GPU, FPGA). While speedups are reported, the lack of detailed discussion on hardware-specific optimization goals limits the practical applicability of the method.
2. Integer Operations: It is unclear whether the proposed SSL method can transform the network to use only integer operations, which is crucial for efficient deployment on certain hardware (e.g., edge devices with limited floating-point capabilities).
3. Computational Complexity: The computational overhead of applying SSL during training is not discussed. This omission raises concerns about the scalability of the method for very large models or datasets.
4. Trade-Off Analysis: While the paper demonstrates accuracy improvements in some cases, it does not provide a systematic analysis of how to balance computational complexity and accuracy loss in the simplified network.
Recommendation:
The paper makes a strong contribution to the field by addressing the critical problem of hardware-friendly DNN compression. However, the lack of clarity on hardware-specific optimization, integer operation compatibility, and computational overhead limits its practical impact. I recommend acceptance with minor revisions, provided the authors address these concerns, particularly by elaborating on the relationship between SSL and specific hardware platforms and discussing the computational cost of their method.
Arguments for Acceptance:
- Novel and well-motivated approach to structured sparsity.
- Strong experimental results demonstrating practical benefits.
- Relevance to real-world deployment scenarios.
Arguments against Acceptance:
- Insufficient discussion of hardware-specific considerations.
- Lack of clarity on integer operation compatibility.
- Missing analysis of computational overhead during training.
In summary, the paper is a valuable contribution to the field, but addressing the highlighted weaknesses would significantly enhance its impact.