This paper introduces the Homotopy Smoothing (HOPS) algorithm, a novel approach for solving structured non-smooth optimization problems, with applications in machine learning, image processing, and cone programming. The authors build on Nesterov's smoothing technique and accelerated proximal gradient descent, achieving improved iteration complexity of \(O(\tilde{1}/\epsilon^{1-\theta})\), where \(\theta \in (0,1]\) captures the local sharpness of the objective function. The work leverages the Local Error Bound (LEB) condition to dynamically adjust the smoothing parameter, significantly enhancing convergence rates. A primal-dual variant (PD-HOPS) is also proposed, which eliminates the need for manual parameter tuning and demonstrates competitive wall-time efficiency.
Strengths:
1. Theoretical Contributions: The paper provides rigorous theoretical guarantees, including the lowest known iteration complexity for the considered class of problems without assuming strong convexity. The integration of the LEB condition is a key innovation, enabling exponential improvements in specific cases (e.g., \(O(\log(\epsilon_0/\epsilon))\) for L1-regularized problems).
2. Algorithmic Design: The homotopy strategy for gradually decreasing the smoothing parameter, combined with warm-starting, is well-motivated and effectively balances accuracy and computational cost.
3. Practical Efficiency: The primal-dual variant (PD-HOPS) is particularly noteworthy for its ability to adaptively determine the number of iterations per stage, avoiding manual tuning and achieving the best wall-time performance in experiments.
4. Experimental Validation: The experiments span diverse application domains, demonstrating consistent improvements in iteration complexity and runtime over baseline methods, particularly for small \(\epsilon\).
Weaknesses:
1. Experimental Comparisons: While the results are promising, it is unclear if the baseline methods represent the state-of-the-art for all tasks. A more comprehensive comparison with recent advancements in non-smooth optimization would strengthen the claims.
2. Backtracking Clarification: The necessity and impact of the backtracking trick in experiments remain ambiguous and should be explicitly addressed.
3. Practical Recommendations: The suggestion to combine HOPS with other methods for larger \(\epsilon\) is insightful but not explored experimentally. This limits the practical guidance provided to practitioners.
4. Wall-Time Accounting: The reported wall-time efficiency of PD-HOPS is impressive, but it is unclear if the dual update times were fully accounted for, which could affect the validity of the runtime comparisons.
Pro vs. Con Arguments for Acceptance:
- Pro: The paper offers significant theoretical advancements, a novel algorithmic framework, and strong empirical results, making it a valuable contribution to the field of non-smooth optimization.
- Con: The experimental evaluation could be more comprehensive, and some practical aspects (e.g., backtracking and wall-time accounting) require clarification.
Recommendation: Overall, this paper is a high-quality submission that advances the state of the art in non-smooth optimization. While there are minor gaps in experimental rigor and practical considerations, the strengths outweigh the weaknesses. I recommend acceptance, with the expectation that the authors address the raised concerns in the final version.