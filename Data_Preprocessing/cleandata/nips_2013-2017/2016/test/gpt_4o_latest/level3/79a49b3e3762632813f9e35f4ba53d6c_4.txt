This paper proposes a Bayesian classification routine for estimating posterior distributions from positive-unlabeled (PU) data, with a focus on robustness to noise and applicability to high-dimensional datasets. The authors address a critical gap in PU learning, where existing methods often falter in noisy, high-dimensional settings. The proposed approach builds on prior work, introducing two practical algorithms that explicitly model label noise and leverage univariate transformations to simplify the estimation process. Theoretical results are provided to justify the approach, including proofs that the univariate transforms preserve the class prior, which is a significant contribution.
Strengths:  
The paper is technically sound, with theoretical claims that are well-supported and convincing, even though I did not verify the proofs line-by-line. The authors provide a tailored analysis of their problem setup, which is commendable. The robustness of the proposed method to noise and high-dimensional data is both theoretically justified and empirically demonstrated. Transparency is another strong point, as the authors include both strong and weak empirical results. The method outperforms baselines in 19 out of 36 cases, which is promising, though it is significantly outperformed in 11 cases. Comparisons to prior work are thorough and well-presented, situating the contribution clearly within the existing literature. 
Weaknesses:  
The univariate transformation section lacks innovation and may struggle with scalability for large datasets, which is a key limitation given the focus on high-dimensional data. While the theoretical robustness to the curse of dimensionality is acknowledged, a detailed analysis of this susceptibility is missing, leaving a gap in understanding the method's practical limitations. Additionally, the paper does not adequately assess the scalability of the proposed algorithms as data size increases, which is crucial for real-world applicability. The mixed empirical performance also raises questions about the consistency of the method across diverse datasets.
Clarity:  
The paper is well-written and organized, making the theoretical and empirical contributions accessible. However, the discussion on univariate transformations could benefit from more detail, particularly regarding their practical limitations.
Originality and Significance:  
While the problem of PU learning is not new, the explicit modeling of label noise and the use of univariate transformations are novel contributions. The work advances the state of the art by addressing a practical challenge in PU learning, though the lack of scalability analysis limits its broader impact.
Recommendation:  
I recommend acceptance with minor revisions. The paper makes a valuable contribution to PU learning, but the authors should address the scalability concerns and provide a more detailed analysis of the curse of dimensionality to strengthen the practical applicability of their approach.