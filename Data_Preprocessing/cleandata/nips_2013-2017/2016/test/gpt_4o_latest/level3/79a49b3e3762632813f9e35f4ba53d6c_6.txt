This paper addresses the important problem of estimating mixing coefficients (\(\alpha\) and \(\beta\)) in mixtures of unknown distributions, with applications in PU-learning. The authors propose two algorithms: a nonparametric method, AlphaMax-N, and a parametric method, MSGMM-T. The paper claims to improve robustness to noise in positive labels and effectiveness for high-dimensional data, which are critical challenges in PU-learning. While the problem is significant and the proposed methods are relevant, the paper falls short in several key areas.
Strengths:  
The problem tackled is both timely and important, particularly for applications involving noisy, high-dimensional data. The paper is generally readable, and the authors provide intuitive explanations for their methods. The theoretical insight that univariate transforms preserve the class prior is a valuable contribution, as it avoids the computational challenges of kernel density estimation in high-dimensional spaces. The nonparametric method, AlphaMax-N, appears to be a solid extension of prior work, and the simulations demonstrate its utility in certain scenarios.
Weaknesses:  
The paper has significant overlap with prior work, particularly with AlphaMax from [1], making the novelty of AlphaMax-N questionable. The parametric method, MSGMM-T, is not competitive in most cases, which undermines its practical value. Additionally, the simulations rely on UCI datasets with artificial noise rather than real-world datasets relevant to PU-learning applications, such as Facebook likes or protein interactions. This limits the practical relevance and impact of the work. Section 3 is dense with notation and technical details, which hampers clarity, especially for non-expert readers. Furthermore, the paper lacks theoretical analysis of AlphaMax-N's error behavior with increasing sample size, which would strengthen its contributions. The absence of publicly available code also detracts from reproducibility and usability.
Suggestions for Improvement:  
1. Clarify Section 3 by adding figures or examples with simple distribution mixtures to illustrate key concepts.  
2. Include real-world datasets relevant to PU-learning applications to demonstrate practical utility.  
3. Provide theoretical results on AlphaMax-N's error behavior as sample size increases.  
4. Release the code to enhance reproducibility.  
Overall Assessment:  
While the paper addresses an important problem and provides some interesting insights, it lacks sufficient novelty and practical validation to merit acceptance at a top-tier conference like NIPS. The overlap with prior work, limited competitiveness of MSGMM-T, and lack of real-world datasets weaken its contributions. I recommend rejection but encourage the authors to address these issues for future submissions.