The paper proposes a novel two-stage approach to learning kernels from random features by optimizing weights for better alignment to the target kernel. This method addresses a critical limitation in kernel methods, which typically require a user-defined kernel, by enabling the learning of a kernel in a supervised manner. The authors leverage randomized features to approximate kernel embeddings, thereby maintaining computational efficiency while optimizing kernel compositions. The theoretical contributions include consistency guarantees for the optimization procedure and generalization bounds for the resulting models. Empirical evaluations demonstrate the method's utility, particularly in cases where the base kernel is poorly aligned with the data, and highlight its scalability and efficiency compared to joint optimization approaches.
Strengths:
1. Novelty and Contribution: The paper introduces a computationally efficient method for supervised kernel learning, combining kernel alignment with random feature optimization. This is a meaningful contribution to the field of kernel methods and randomized features.
2. Theoretical Guarantees: The authors provide rigorous consistency and generalization bounds, which enhance the credibility of the proposed approach. These guarantees are critical for establishing the method's reliability.
3. Scalability: The method achieves significant speedups over joint optimization approaches, making it practical for large-scale datasets. The use of sparse weights further reduces computational costs.
4. Empirical Validation: The experiments on benchmark datasets and synthetic data demonstrate the method's effectiveness, particularly for misaligned data, as shown in Section 4.1. The feature selection application is a promising direction for high-dimensional data analysis.
Weaknesses:
1. Feature Selection Experiments: The feature selection experiments are less compelling due to the limited comparison with completely random feature selection methods. A broader evaluation against state-of-the-art feature selection techniques would strengthen the claims.
2. Clarity and Readability: The write-up is convoluted and requires multiple reads for comprehension. Specific issues include unclear assumptions about the feature function (line 64), insufficiently described notation (line 76), and unexplained elements such as the square root in Equation (6). The reasoning for the choice of the Gaussian kernel (line 169) is also inadequately justified.
3. Limited Benchmarking: While the method is compared to unoptimized random features and joint optimization, additional comparisons with other kernel learning approaches would provide a more comprehensive evaluation.
Arguments for Acceptance:
- The paper addresses a relevant and challenging problem in kernel learning with a novel and efficient approach.
- Theoretical guarantees and empirical results support the validity and utility of the method.
- The scalability of the approach makes it a valuable addition to the toolkit for large-scale machine learning.
Arguments Against Acceptance:
- The clarity of the paper is a significant concern, as it hinders accessibility for readers unfamiliar with the topic.
- The feature selection experiments and benchmarking could be more robust to strengthen the empirical claims.
Suggestions for Improvement:
1. Revise the manuscript for clarity, ensuring that assumptions, notation, and equations are well-explained.
2. Expand the feature selection experiments to include comparisons with state-of-the-art methods.
3. Provide additional justification for design choices, such as the Gaussian kernel in Section 4.1.
4. Include comparisons with other kernel learning methods to contextualize the contributions better.
Overall, the paper makes a valuable contribution to kernel learning and randomized features, but improvements in clarity and experimental rigor would significantly enhance its impact.