This paper proposes a Structured Sparsity Learning (SSL) method to regularize the structures of Deep Neural Networks (DNNs), including filters, channels, filter shapes, and layer depth. The main claim of the paper is that SSL can learn a compact structure from a larger DNN, reducing computation cost while improving classification accuracy. The authors support this claim through extensive experiments on various datasets, including MNIST, CIFAR-10, and ImageNet, using different DNN architectures, such as LeNet, ConvNet, ResNet, and AlexNet.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed method, including the mathematical formulations and the experimental setup. The results are impressive, with SSL achieving significant speedups (up to 5.1× on CPU and 3.1× on GPU) and improving classification accuracy (e.g., reducing the error of AlexNet by ∼1% on ImageNet).
The strengths of the paper include:
* The proposed SSL method is novel and addresses the limitations of existing sparsity regularization techniques, which often produce non-structured random connectivity and poor data locality.
* The authors provide a comprehensive evaluation of the proposed method, including experiments on various datasets and DNN architectures.
* The results demonstrate the effectiveness of SSL in reducing computation cost and improving classification accuracy.
The weaknesses of the paper include:
* The paper could benefit from a more detailed analysis of the computational complexity of the proposed method and its scalability to larger DNNs.
* The authors could provide more insights into the trade-offs between the different structures regularized by SSL (e.g., filters, channels, filter shapes, and layer depth).
Overall, the paper is well-written, and the proposed SSL method is novel and effective. The results demonstrate the potential of SSL to improve the efficiency and accuracy of DNNs, making it a valuable contribution to the field.
Arguments pro acceptance:
* The paper proposes a novel and effective method for regularizing the structures of DNNs.
* The results demonstrate significant speedups and improvements in classification accuracy.
* The paper is well-written, and the authors provide a clear and concise explanation of the proposed method.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the computational complexity of the proposed method.
* The authors could provide more insights into the trade-offs between the different structures regularized by SSL.