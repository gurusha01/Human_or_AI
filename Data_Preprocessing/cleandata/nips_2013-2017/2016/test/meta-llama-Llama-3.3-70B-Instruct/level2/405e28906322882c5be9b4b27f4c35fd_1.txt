This paper presents a comprehensive study of the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions over T rounds of the process, and investigate its interaction with other parameters, including Γ, which counts the number of times the distributions change, and V, which measures how far the distributions deviate over time.
The paper provides several key contributions. Firstly, it establishes a regret lower bound of Ω(√ΓT) for bandit algorithms, even when there is no variance in distributions and the distributions only change once. This result highlights the fundamental difficulty of the bandit setting in non-stationary environments. In contrast, the authors show that in the full-information setting, a constant regret becomes achievable with constant Γ and Λ.
The paper also presents several algorithms with upper bound guarantees, including a bandit algorithm that achieves a regret of about 3√ΛV T + √V T, and a full-information algorithm that achieves a regret of about √ΓΛ + Γ. The authors also propose a parameter-free full-information algorithm, which achieves a regret of about √ΓΛ lnK + Γ lnK for switching distributions and 3√V ΛT lnK + √V T lnK for drifting distributions.
The strengths of the paper include its thorough analysis of the regret landscape in non-stationary environments, its introduction of a new parameter Λ, and its presentation of several algorithms with upper bound guarantees. The paper also provides a clear and well-organized exposition of the results, making it easy to follow.
However, there are some limitations to the paper. Firstly, the algorithms presented require knowledge of the related parameters, which may not be available in practice. Secondly, the paper focuses primarily on the theoretical analysis of the regret bounds, without providing extensive experimental evaluations of the algorithms.
In terms of novelty, the paper presents a significant improvement over existing approaches, particularly in the non-stationary stochastic setting. The introduction of the new parameter Λ and the analysis of its interaction with other parameters provide new insights into the regret landscape of multi-armed bandit and experts problems.
The paper is well-written, and the authors provide clear explanations of the technical details. The notation is consistent, and the paper includes a comprehensive list of references.
Overall, I would recommend accepting this paper for publication. The paper presents a significant contribution to the field of online learning and multi-armed bandit problems, and its results have the potential to impact the design of algorithms for non-stationary environments.
Arguments pro acceptance:
* The paper presents a comprehensive study of the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments.
* The introduction of the new parameter Λ provides new insights into the regret landscape of multi-armed bandit and experts problems.
* The paper presents several algorithms with upper bound guarantees, including a parameter-free full-information algorithm.
* The paper provides a clear and well-organized exposition of the results.
Arguments con acceptance:
* The algorithms presented require knowledge of the related parameters, which may not be available in practice.
* The paper focuses primarily on the theoretical analysis of the regret bounds, without providing extensive experimental evaluations of the algorithms.