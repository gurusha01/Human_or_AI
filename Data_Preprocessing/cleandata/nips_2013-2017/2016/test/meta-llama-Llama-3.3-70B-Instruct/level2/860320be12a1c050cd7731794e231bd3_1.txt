This paper presents a rigorous graph-theoretic framework for analyzing the connecting architectures of recurrent neural networks (RNNs). The authors propose three measures of architectural complexity: recurrent depth, feedforward depth, and recurrent skip coefficient. These measures capture the nonlinearity of RNNs in both the time and feedforward directions, as well as the ability of the network to "skip" across multiple time steps.
The paper is well-written and provides a clear explanation of the proposed framework and measures. The authors also provide empirical evidence to support the usefulness of these measures, demonstrating that increasing recurrent depth and feedforward depth can yield performance improvements, while increasing the recurrent skip coefficient can improve performance on long-term dependency tasks.
The strengths of the paper include:
* A clear and rigorous framework for analyzing RNN architectures
* Well-defined and computable measures of architectural complexity
* Empirical evidence to support the usefulness of the proposed measures
* A thorough analysis of the relationships between the proposed measures and RNN performance
The weaknesses of the paper include:
* The paper assumes a homogeneous RNN, which may not be the case in practice
* The authors do not provide a clear explanation of how to choose the optimal values for the proposed measures
* The paper could benefit from more extensive experiments and comparisons with other RNN architectures
Overall, the paper presents a significant contribution to the field of RNNs and provides a useful framework for analyzing and designing RNN architectures. The proposed measures of architectural complexity have the potential to guide the development of new RNN architectures and improve their performance on a variety of tasks.
Arguments for acceptance:
* The paper presents a clear and rigorous framework for analyzing RNN architectures
* The proposed measures of architectural complexity are well-defined and computable
* The empirical evidence provided supports the usefulness of the proposed measures
Arguments against acceptance:
* The paper assumes a homogeneous RNN, which may not be the case in practice
* The authors do not provide a clear explanation of how to choose the optimal values for the proposed measures
* The paper could benefit from more extensive experiments and comparisons with other RNN architectures
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, including providing more extensive experiments and comparisons with other RNN architectures, and clarifying how to choose the optimal values for the proposed measures.