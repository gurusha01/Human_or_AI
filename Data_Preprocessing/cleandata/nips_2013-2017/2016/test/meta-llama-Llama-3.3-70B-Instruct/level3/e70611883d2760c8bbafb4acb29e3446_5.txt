This paper presents a comprehensive study on the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions over T rounds of the process, and investigate its impact on the regret. The paper provides a thorough analysis of the tradeoff between Λ and other parameters, including Γ, V, and T, and presents several algorithms with upper bound guarantees.
The paper is well-organized, and the authors provide a clear and concise introduction to the problem and related works. The mathematical steps in the paper can be difficult to follow at times, but the key parts appear to be logical and make sense. The authors also provide a detailed analysis of the algorithms and their regret bounds, which is helpful in understanding the strengths and weaknesses of each approach.
One of the strengths of the paper is its ability to provide a big picture of the regret landscape in terms of the parameters Λ, Γ, V, and T, in both full-information and bandit settings. The authors present several algorithms, including a parameter-dependent bandit algorithm, a parameter-dependent full-information algorithm, and a parameter-free full-information algorithm, each with its own strengths and weaknesses. The paper also provides a thorough analysis of the lower bounds, which helps to understand the limitations of each approach.
However, one of the weaknesses of the paper is that the heuristic part of the method affects efficiency, but its impact has not been evaluated in the paper. Additionally, the performance comparison is limited, only involving the previous method with three different kernels, and lacks comparison with other known efficient methods.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is also well-written, and the authors provide enough information for the expert reader to reproduce the results.
In terms of clarity, the paper is well-organized, and the authors provide a clear and concise introduction to the problem and related works. The notation is consistent, and the authors provide a detailed analysis of the algorithms and their regret bounds.
In terms of originality, the paper presents a novel combination of familiar techniques, and the introduction of the new parameter Λ provides a new perspective on the problem. The paper also provides a thorough analysis of the tradeoff between Λ and other parameters, which is a significant contribution to the field.
In terms of significance, the paper addresses a difficult problem in a better way than previous research, and the results have the potential to impact the field of online learning and decision-making. The paper provides a comprehensive study of the dynamic regret of multi-armed bandit and experts problem, which is a fundamental problem in online learning.
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of online learning and decision-making. The paper is well-written, technically sound, and provides a comprehensive study of the dynamic regret of multi-armed bandit and experts problem.
Arguments pro acceptance:
* The paper provides a comprehensive study of the dynamic regret of multi-armed bandit and experts problem.
* The introduction of the new parameter Λ provides a new perspective on the problem.
* The paper presents several algorithms with upper bound guarantees, each with its own strengths and weaknesses.
* The paper provides a thorough analysis of the lower bounds, which helps to understand the limitations of each approach.
Arguments con acceptance:
* The heuristic part of the method affects efficiency, but its impact has not been evaluated in the paper.
* The performance comparison is limited, only involving the previous method with three different kernels, and lacks comparison with other known efficient methods.