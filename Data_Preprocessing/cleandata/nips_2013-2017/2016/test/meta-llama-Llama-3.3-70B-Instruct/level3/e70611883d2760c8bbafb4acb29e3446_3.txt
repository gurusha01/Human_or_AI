This paper presents a comprehensive study on the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions, and investigate its interaction with other parameters such as Γ, which counts the number of times the distributions change, and V, which measures the distance the distributions deviate over time.
The paper provides a thorough analysis of the problem, including a clear definition of the dynamic regret, a description of the algorithms used, and a detailed proof of the theoretical guarantees. The authors also provide empirical evaluations on various datasets, which demonstrate the effectiveness of their approach.
One of the strengths of the paper is its ability to provide a big picture of the regret landscape in terms of the parameters Λ, Γ, V, and T, in both full-information and bandit settings. The authors also propose a parameter-free algorithm, which achieves a good regret bound and may have independent interest of its own.
However, there are some minor issues with notation and formatting that could be improved. Additionally, the paper could benefit from a more detailed comparison with existing works, particularly in the context of non-stationary stochastic environments.
In terms of the conference guidelines, the paper meets the criteria of quality, clarity, originality, and significance. The paper is technically sound, well-written, and well-organized, making it easy to follow and understand. The authors provide a clear motivation for their work, and the results are significant and relevant to the field.
Here is a list of arguments pro and con acceptance:
Pro:
* The paper provides a comprehensive study on the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments.
* The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions, and investigate its interaction with other parameters.
* The paper provides a thorough analysis of the problem, including a clear definition of the dynamic regret, a description of the algorithms used, and a detailed proof of the theoretical guarantees.
* The authors provide empirical evaluations on various datasets, which demonstrate the effectiveness of their approach.
Con:
* There are some minor issues with notation and formatting that could be improved.
* The paper could benefit from a more detailed comparison with existing works, particularly in the context of non-stationary stochastic environments.
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of online learning and multi-armed bandit problems. The paper is well-written, well-organized, and provides a thorough analysis of the problem, making it a valuable addition to the conference program.