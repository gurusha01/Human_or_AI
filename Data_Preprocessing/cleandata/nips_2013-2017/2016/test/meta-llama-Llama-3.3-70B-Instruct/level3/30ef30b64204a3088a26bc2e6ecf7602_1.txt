This paper proposes a novel unsupervised loss function to improve the generalization properties of convolutional neural networks (ConvNets) in semi-supervised learning settings. The loss function minimizes the difference in predictions of multiple passes of a training sample through the network, taking advantage of the stochastic nature of techniques such as randomized data augmentation, dropout, and random max-pooling. The authors evaluate the proposed method on several benchmark datasets, including MNIST, SVHN, NORB, CIFAR10, CIFAR100, and ImageNet, using two different ConvNet implementations.
The paper presents extensive experimental evaluations, demonstrating that the proposed unsupervised loss function consistently improves the performance of ConvNets, especially in the low-sample regime. The results show that the combination of the proposed loss function with mutual-exclusivity loss leads to state-of-the-art performance on CIFAR10 and CIFAR100. The authors also demonstrate that the proposed method can be used with different ConvNet architectures and implementations, and that it can improve the accuracy of ConvNets regardless of the architecture and implementation.
The strengths of the paper include the novelty of the proposed loss function, the thorough experimental evaluation, and the demonstration of state-of-the-art performance on several benchmark datasets. However, the paper lacks theoretical justification, derivation, or discussion of the proposed loss function, which is a significant weakness. Additionally, the baselines could be further improved by including supervised training on augmented labeled data, and the paper could benefit from more discussion on the role of the number of transformations in the augmented set.
Arguments for acceptance:
* The paper proposes a novel and effective unsupervised loss function for semi-supervised learning with ConvNets.
* The experimental evaluations are thorough and demonstrate state-of-the-art performance on several benchmark datasets.
* The proposed method can be used with different ConvNet architectures and implementations.
Arguments against acceptance:
* The paper lacks theoretical justification, derivation, or discussion of the proposed loss function.
* The baselines could be further improved by including supervised training on augmented labeled data.
* The paper could benefit from more discussion on the role of the number of transformations in the augmented set.
Overall, the paper presents a significant contribution to the field of semi-supervised learning with ConvNets, and the proposed loss function has the potential to improve the performance of ConvNets in a variety of applications. However, the paper could be improved by addressing the weaknesses mentioned above. 
Quality: 8/10
The paper is well-written, and the experimental evaluations are thorough. However, the lack of theoretical justification and derivation of the proposed loss function is a significant weakness.
Clarity: 8/10
The paper is well-organized, and the writing is clear. However, some sections, such as the related work section, could be improved for better clarity.
Originality: 9/10
The proposed loss function is novel and effective, and the paper presents a significant contribution to the field of semi-supervised learning with ConvNets.
Significance: 9/10
The paper demonstrates state-of-the-art performance on several benchmark datasets, and the proposed method has the potential to improve the performance of ConvNets in a variety of applications.