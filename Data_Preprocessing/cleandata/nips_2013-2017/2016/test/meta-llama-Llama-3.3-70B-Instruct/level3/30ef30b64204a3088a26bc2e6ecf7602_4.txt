This paper proposes a novel approach to studying the dynamic regret of multi-armed bandit and experts problems in non-stationary stochastic environments. The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions over time, and investigate its interaction with other parameters such as Γ, which counts the number of times the distributions change, and V, which measures the distance the distributions deviate over time.
The paper is well-written and provides a clear overview of the problem and the proposed approach. The authors demonstrate the effectiveness of their approach on various datasets, including MNIST, CIFAR10, and ImageNet, with impressive results on some benchmarks. However, the results on ImageNet are lagging behind the current state-of-the-art, which raises questions about the approach's effectiveness in settings with abundant training data.
One of the strengths of the paper is its ability to provide a big picture of the regret landscape in terms of the parameters Λ, Γ, V, and T, in both full-information and bandit settings. The authors propose several algorithms, including a parameter-dependent bandit algorithm, a full-information algorithm, and a parameter-free full-information algorithm, which achieve regret bounds that are close to optimal.
However, there are some weaknesses in the paper. Some sections, such as the experiment using sparse convolutional networks on SVHN, require clarification on the methodology and results. Additionally, the use of data augmentation is inconsistent throughout the paper, and it is unclear why it is used in some cases and not others.
In terms of originality, the paper proposes a novel approach to studying dynamic regret in non-stationary stochastic environments, which is a significant contribution to the field. The authors also provide a thorough analysis of the regret bounds and demonstrate the effectiveness of their approach on various datasets.
The significance of the paper lies in its ability to provide a framework for understanding the dynamic regret of multi-armed bandit and experts problems in non-stationary stochastic environments. The results have important implications for practitioners and researchers in the field, as they provide a better understanding of the trade-offs between exploration and exploitation in non-stationary environments.
Arguments for acceptance:
* The paper proposes a novel approach to studying dynamic regret in non-stationary stochastic environments.
* The authors provide a thorough analysis of the regret bounds and demonstrate the effectiveness of their approach on various datasets.
* The paper has significant implications for practitioners and researchers in the field.
Arguments against acceptance:
* The results on ImageNet are lagging behind the current state-of-the-art.
* Some sections require clarification on the methodology and results.
* The use of data augmentation is inconsistent throughout the paper.
Overall, I would recommend accepting the paper, as it provides a significant contribution to the field and has important implications for practitioners and researchers. However, the authors should address the weaknesses in the paper, such as clarifying the methodology and results in some sections and providing a consistent use of data augmentation.