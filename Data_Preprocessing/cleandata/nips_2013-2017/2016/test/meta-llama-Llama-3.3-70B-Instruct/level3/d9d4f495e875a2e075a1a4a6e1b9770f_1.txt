This paper introduces a novel approach to video prediction by predicting pixel motion rather than direct pixel values, enabling more accurate predictions on unseen objects and allowing for unsupervised segmentation of objects. The authors propose three motion prediction modules: Dynamic Neural Advection (DNA), Convolutional Dynamic Neural Advection (CDNA), and Spatial Transformer Predictors (STP). They also introduce a large-scale dataset of real-world object manipulation by robotic arms, which enables end-to-end training and rigorous evaluation of competing approaches.
The paper's strengths include its novel approach to video prediction, its ability to generalize to unseen objects, and its large-scale dataset. The experiments demonstrate that the proposed models outperform competitor methods, and the qualitative results show that the models can produce plausible video sequences more than 10 time steps into the future.
However, the paper also has some weaknesses. The absolute quality of predictions is still poor, and further testing on other datasets is needed to justify claims of learning physics. Additionally, the paper's focus on real-world dataset is commendable, but comparison with recent work such as Deep Predictive Coding Networks and testing on other datasets like block world dataset is necessary for comprehensive evaluation.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader.
The paper's originality lies in its novel approach to video prediction, which is a significant departure from previous methods. The authors provide a thorough review of related work and demonstrate how their approach differs from previous contributions.
The significance of the paper lies in its potential to enable interactive agents, such as robots, to imagine different futures based on available actions. The paper's results have important implications for model-based planning, reinforcement learning, and robotics.
Arguments for acceptance:
* The paper introduces a novel approach to video prediction that has the potential to enable interactive agents to imagine different futures based on available actions.
* The paper's experiments demonstrate that the proposed models outperform competitor methods, and the qualitative results show that the models can produce plausible video sequences more than 10 time steps into the future.
* The paper's large-scale dataset of real-world object manipulation by robotic arms is a significant contribution to the field.
Arguments against acceptance:
* The absolute quality of predictions is still poor, and further testing on other datasets is needed to justify claims of learning physics.
* The paper's focus on real-world dataset is commendable, but comparison with recent work such as Deep Predictive Coding Networks and testing on other datasets like block world dataset is necessary for comprehensive evaluation.
Overall, I recommend accepting this paper due to its novel approach, significant contributions, and potential impact on the field. However, I suggest that the authors address the weaknesses mentioned above in their future work.