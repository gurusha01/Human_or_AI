This paper presents a novel approach to video prediction, focusing on predicting pixel motion rather than reconstructing frames from scratch. The authors introduce three motion prediction modules: Dynamic Neural Advection (DNA), Convolutional Dynamic Neural Advection (CDNA), and Spatial Transformer Predictors (STP). These models are designed to be partially invariant to object appearance, allowing them to generalize to unseen objects. The paper also introduces a new dataset of 59,000 robot interactions, which is used to evaluate the performance of the proposed models.
The paper is well-written and clearly explains the technical details of the proposed models. The experiments demonstrate the effectiveness of the approach, showing that the proposed models outperform prior methods in terms of PSNR and SSIM metrics. The paper also provides a thorough analysis of the results, including ablation studies and qualitative evaluations.
One of the strengths of the paper is its ability to predict long-range video sequences, up to 18 time steps into the future. This is a significant improvement over prior methods, which have largely focused on single-frame prediction. The paper also demonstrates the potential of the approach for action-conditioned prediction, where the model is conditioned on the actions taken by a robot.
However, there are some limitations to the paper. The proposed models are not groundbreaking, and the results, while impressive, are not significantly better than prior methods. The paper could benefit from more discussion on the potential applications of the approach, beyond the specific task of video prediction. Additionally, some of the technical details, such as the architecture of the models, could be clarified for better understanding.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is also well-organized and clearly written, making it easy to follow. The originality of the paper lies in its novel approach to video prediction, which combines appearance information from previous frames with motion predicted by the model. The significance of the paper is in its potential to enable interactive agents, such as robots, to imagine different futures based on available actions.
Overall, I recommend minor revisions to address the mentioned issues. The paper is a good scientific contribution to the field, and with some revisions, it has the potential to be a strong publication.
Arguments pro acceptance:
* The paper presents a novel approach to video prediction, which is well-motivated and clearly explained.
* The experiments demonstrate the effectiveness of the approach, showing significant improvements over prior methods.
* The paper provides a thorough analysis of the results, including ablation studies and qualitative evaluations.
Arguments con acceptance:
* The proposed models are not groundbreaking, and the results, while impressive, are not significantly better than prior methods.
* The paper could benefit from more discussion on the potential applications of the approach, beyond the specific task of video prediction.
* Some of the technical details, such as the architecture of the models, could be clarified for better understanding.