This paper presents a novel approach to video prediction, focusing on predicting pixel motion rather than reconstructing frames from scratch. The authors propose three motion prediction modules: Dynamic Neural Advection (DNA), Convolutional Dynamic Neural Advection (CDNA), and Spatial Transformer Predictors (STP). These models are designed to be partially invariant to object appearance, allowing them to generalize to unseen objects. The paper also introduces a large dataset of robot interactions, consisting of 59,000 sequences with 1.5 million video frames, which is used to evaluate the proposed models.
The paper is well-written, and the authors provide a clear explanation of their approach and the motivations behind it. The experiments are thorough, and the results demonstrate the effectiveness of the proposed models in predicting future video frames. The use of a robotic pushing dataset and a human motion dataset provides a good testbed for evaluating the models in different scenarios.
One of the strengths of the paper is its ability to predict long-range video sequences, which is a challenging task in video prediction. The authors' approach to predicting pixel motion rather than reconstructing frames from scratch allows for more accurate predictions, especially in scenarios where objects are moving or interacting with each other.
The paper also provides a good discussion of related work, highlighting the differences between the proposed approach and existing methods. The authors' use of convolutional LSTMs and their integration of action information into the model are notable contributions.
However, there are some areas where the paper could be improved. For example, the authors could provide more details on the training process, such as the hyperparameters used and the convergence criteria. Additionally, the paper could benefit from a more detailed analysis of the results, including a discussion of the limitations of the proposed approach and potential avenues for future work.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is well-organized, and the authors provide a clear explanation of their approach and the results. The paper also presents a novel approach to video prediction, which is a significant contribution to the field.
Arguments for acceptance:
* The paper presents a novel approach to video prediction, which is a significant contribution to the field.
* The authors provide a clear explanation of their approach and the results, making it easy to understand and replicate.
* The paper demonstrates the effectiveness of the proposed models in predicting future video frames, especially in scenarios where objects are moving or interacting with each other.
* The use of a robotic pushing dataset and a human motion dataset provides a good testbed for evaluating the models in different scenarios.
Arguments against acceptance:
* The paper could benefit from more details on the training process and the convergence criteria.
* The paper could provide a more detailed analysis of the results, including a discussion of the limitations of the proposed approach and potential avenues for future work.
* The paper may not be suitable for a general audience, as it assumes a significant amount of background knowledge in computer vision and machine learning.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of video prediction and demonstrates the effectiveness of the proposed approach in predicting future video frames. However, the authors could benefit from providing more details on the training process and the convergence criteria, as well as a more detailed analysis of the results.