This paper presents a novel approach to predicting future images of a scene by explicitly modeling pixel motion, allowing for the prediction of object movements without requiring labeled object information. The authors propose three deep learning methods, including Dynamic Neural Advection (DNA), Convolutional Dynamic Neural Advection (CDNA), and Spatial Transformer Predictors (STP), which are evaluated on a robotic pushing dataset and the Human3.6M dataset. The results show that these methods outperform state-of-the-art approaches, demonstrating the effectiveness of predicting pixel motion for video prediction tasks.
The paper is well-structured and clearly written, with a thorough introduction to the problem and related work. The authors provide a detailed description of their methods and experiments, making it easy to follow and understand their approach. The use of figures and diagrams helps to illustrate the concepts and results, making the paper more engaging and easier to comprehend.
One of the strengths of the paper is its ability to predict future images of a scene without requiring labeled object information, making it a significant contribution to the field of computer vision. The authors also provide a large dataset of robotic interactions, which can be useful for future research in this area.
However, there are some areas that could be improved. The paper lacks detailed descriptions of the three methods, relying heavily on figures and assuming clarity that is not present. For example, the structure of the m/M and their adaptability to local image patches are not clearly explained. Additionally, the paper is missing a discussion of the methods' weaknesses and limitations, such as the choice of 64x64 pixel image resolution and the potential for applying the method to more 3D movements.
The qualitative results show considerable blurring in the images, and the authors need to discuss the sources of these errors and potential future improvements. Furthermore, the authors could exploit the robot setup more for evaluations, such as testing the robot's ability to grasp a moved object using only the predicted image.
To increase the paper's long-term impact, the authors could provide additional discussions, visualizations, and examples, such as an example image showing the optical flow computed for both ground truth and predicted data. The network structure is impressive, but the paper's presentation and discussion of the methods' limitations need improvement.
Overall, the paper presents a significant contribution to the field of computer vision, and with some improvements, it has the potential to be a highly influential work. The authors demonstrate a good understanding of the problem and related work, and their approach shows promise for predicting future images of a scene. With further refinement and discussion of the methods' limitations, this paper could be even more effective in conveying its message and contributing to the field.
Arguments pro acceptance:
- The paper presents a novel approach to predicting future images of a scene.
- The authors propose three deep learning methods that outperform state-of-the-art approaches.
- The paper provides a large dataset of robotic interactions, which can be useful for future research.
- The authors demonstrate a good understanding of the problem and related work.
Arguments con acceptance:
- The paper lacks detailed descriptions of the three methods.
- The paper is missing a discussion of the methods' weaknesses and limitations.
- The qualitative results show considerable blurring in the images, and the authors need to discuss the sources of these errors and potential future improvements.
- The authors could exploit the robot setup more for evaluations.