This paper explores the development of online learning algorithms that can adapt to new data without requiring a predetermined learning rate. Many existing algorithms fix this rate in advance, while others adjust it as they progress. The primary challenge addressed in this paper is designing an algorithm that can perform comparably to one that has access to all data upfront, rather than receiving it incrementally. To achieve this, the authors propose a strategy for a coin betting game, where the gambler must decide how much to bet and on which outcome at each turn. This strategy is grounded in carefully chosen potential functions, with each family of functions yielding a betting strategy that also provides an upper bound on the regret for the game. The regret refers to the difference in potential winnings between knowing all coin tosses in advance and using the proposed strategy. The authors then apply this coin betting strategy to tackle other online learning problems, including Online Learning Optimization and Learning with Expert Advice. This approach appears to generalize existing parameter-free algorithms. The coin betting game, as well as the online learning optimization and learning with expert advice problems, are intriguing issues in online learning, with the choice of learning rate being crucial to many learning algorithms. Therefore, any framework that offers a smart way to select this rate is highly interesting and has numerous potential applications. However, a significant concern with the paper is the definition of the Coin Betting Potential, which underlies the choice of the learning rate. Although the formula following line 137 clarifies the usefulness of the condition in line 130, and lines 138-140 explain the origin of the expression for beta_t (albeit with a somewhat confusing explanation), it remains unclear how to construct such a function. Later, it is demonstrated that known online learning algorithms have corresponding potential functions. It would be fascinating if the authors could provide insight into how to generate more such functions, thereby showing that their algorithm not only generalizes known results but can also produce new ones. Additionally, several minor comments can be made: at line 16, "the Hilbert space" should be "a Hilbert space" unless this space has been previously defined. Lines 24-29 could benefit from an explanation of what a learning rate is, beyond its usage in the examples of OGD, to avoid confusing readers unfamiliar with the concept. It might also be helpful to clarify that "parameter-free" means not setting the learning rate in advance. Furthermore, at line 49, the meaning of "dom(f) not empty" should be explicitly stated, especially if the domain is not all of V. The inequality in (5) at line 109 does not seem trivial and could be elaborated upon. The names of Krichevsky and Trofimov are inexplicably in blue at line 114. At line 141, the coin betting seems to be one-dimensional, making the reference to "infinite dimensional" unclear. Lastly, the significance of the "peculiar property" mentioned at line 196 is not apparent.