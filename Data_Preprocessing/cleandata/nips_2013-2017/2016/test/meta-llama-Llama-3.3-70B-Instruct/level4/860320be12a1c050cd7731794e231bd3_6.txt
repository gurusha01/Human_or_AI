This work introduces three complexity measures for Recurrent Neural Networks (RNNs): recurrent depth, feedforward depth, and the recurrent skip coefficient, all defined within a precise graph-theoretic framework. The authors then conduct experiments on various sequential tasks using RNN models with differing complexities according to these measures. However, a substantial portion of the paper is dedicated to rigorously defining these measures, which, in my opinion, are fairly intuitive and do not necessitate the extensive graph-theoretic machinery employed. For instance, the definitions of recurrent depth and skip coefficient, illustrated in Figure 1, are clear and do not fundamentally benefit from the formal graph-theoretic definitions provided. The formalization could be beneficial if it led to the proof of novel and useful theorems, but the theorems presented are limited to proving the computability of the defined measures, which seems more of a technical detail. The conclusions drawn from the experiments are not clear. While the investigation of different architectures in Figure 2a and 2b appears to be an interesting direction, the experiments lack systematic approach and have methodological issues. Several questions and comments arise: 
- The evaluation of architectures in Figure 2a on PennTree and Text8, and those in Figure 2b on Text8 and MNIST, seems inconsistent; wouldn't it be more sensible to evaluate the same set of architectures across all datasets to identify if certain architectures consistently perform better on specific problem types?
- It is unclear if the models in Table 1 (right) are the same size as those in Table 1 (left) for Text8; thus, it's ambiguous whether the performance difference stems from architectural differences or model size.
- The improvements in recurrent depth reported in Table 1 (right) lack statistical significance analysis, which could be addressed by including error bars over multiple initializations.
- The results for MNIST in Section 4.3 should be included in the main paper, presented in a table format similar to Table 1 (right), rather than being relegated to the appendix.
- In Section 4.4, the results on the adding and copying problem should be presented in a table for clarity, rather than being described in the text.
- The top-left entry in Table 2 shows surprising results where permuted MNIST performs better than the unpermuted version, contrary to expectations that increasing the timescale of dependencies would hinder performance; an explanation for this phenomenon would be beneficial.
- Statements like those in lines 294-296, which mention varying a parameter and finding persistent improvement, should reference specific results to provide clear insights. The conclusion states that empirical evidence suggests increasing recurrent depth may yield performance improvements, yet this does not hold for sequential MNIST and was not tested for the copy or addition tasks. It also mentions that increasing feedforward depth may not aid long-term dependency tasks, which is unsurprising, and that increasing the recurrent skip coefficient can significantly improve performance on long-term dependency tasks, a point already established in previous work, such as the Clockwork RNN.