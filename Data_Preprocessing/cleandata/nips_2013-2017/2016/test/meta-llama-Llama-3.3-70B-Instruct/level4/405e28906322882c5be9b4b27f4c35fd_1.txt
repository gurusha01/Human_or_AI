This paper explores online learning in a non-stationary stochastic environment, examining the achievable regret as a function of the change in the mean loss vector and the cumulative variance of the random losses. The authors provide algorithms with regret upper bounds, along with matching lower bounds. 
Upon reevaluation after the rebuttal, it is acknowledged that the proof of Theorem 4.1 is indeed correct, with the constructed environment being oblivious, thus rectifying an earlier mistake. Furthermore, the generalization in the analysis of UCB-V appears straightforward, as the applied concentration inequalities do not necessitate that the random variables have the same distribution. 
A comparison with the work of Jadbabaie et al. reveals that their algorithm, with a specific choice of parameters, closely resembles the one presented in this paper, differing primarily in how the expectation is handled within the norms. It is suggested that a slight modification of their proof could potentially align the two approaches, though this requires careful verification and description of the connection to related work.
The paper's proposed complexity parameters, such as Lambda, need justification for their interest, ideally supported by examples where Lambda is not zero or linear in T. However, the paper's failure to adequately engage with the existing literature and properly cite references hinders the assessment of the novelty of its results. The algorithm and bounds presented bear a resemblance to those in previous works, such as Jadbabaie et al. (2015), which achieved similar results in a stronger adversarial setting. The consideration of the variance of random losses, while interesting, seems to build upon techniques from earlier studies without a thorough discussion of its novelty.
The writing of the paper requires significant improvement, including the proper referencing of algorithms like UCB-V from Audibert et al. (2009). The motivation behind the complexity measures used and their interest needs clarification. For instance, the impact of mean changes or high variance in clearly suboptimal arms, and the scenarios where Lambda is not constant or linear in time, should be explained. Additionally, the connection to tracking and shifting bounds, as well as the distinction between properties of the comparator sequence and loss functions, needs to be clarified to avoid confusion.
The switching results could also benefit from a relation to the body of work in information theory on coding piecewise stationary sources, albeit with a focus on log-loss and entropy/entropy-rate considerations. The availability of sleeping expert-type reductions in generic forms, as seen in Gyorgy et al. (2012), further underscores the need for a comprehensive review of related literature.
In conclusion, while the paper shows potential, substantial improvements are necessary before publication. These include a thorough discussion of novelty in relation to existing work, enhancement of the paper's writing, and clarification of the motivation and connections to broader research areas. References to key works such as Audibert et al. (2009), Gyorgy et al. (2012), and Herbster and Warmuth (1998, 2001) should be properly integrated to contextualize the paper's contributions accurately.