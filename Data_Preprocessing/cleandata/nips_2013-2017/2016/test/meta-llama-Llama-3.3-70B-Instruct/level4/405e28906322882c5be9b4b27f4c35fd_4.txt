This paper examines a sequential decision problem characterized by stochastic rewards bounded between 0 and 1, under both full information and bandit information settings. The mean reward of each arm is permitted to vary over time, leading to the definition of instantaneous regret at time t with respect to the optimal action at that time. Consequently, cumulative regret is defined in relation to a strategy that may select a different arm at each time step. The rewards are assumed to be independent across arms and time. Three key quantities are considered to capture the complexity of the environment: the number of changes in the mean reward vector (Γ), the cumulative drift measured in infinite norm (V), and the cumulative variance of the reward distributions (Λ). 
In this non-stationary context, the paper investigates several algorithms. Algorithm 1 possesses perfect knowledge of Γ, Λ, V, and the time horizon, operating in the bandit setting. Algorithm 2 has similar knowledge but functions in the full-information setting. Algorithm 3 is parameter-free and operates in the full information setting, drawing inspiration from the works of [3] and [4]. Its regret is bounded as per Theorem 3.5, and its static regret against any fixed arm is controlled as per Theorem 3.4. Section 4 provides lower bounds on the regret in terms of Γ, Λ, and V.
Upon reviewing the authors' feedback, it is clear that this contribution is interesting. While Algorithms 1 and 2 may not be particularly noteworthy, they serve as a useful precursor. Algorithm 3, however, is more compelling, leveraging a simple yet effective combination of ideas from [3] and [4]. The regret performance is also noteworthy, especially when considered alongside the lower bounds. Nonetheless, including explicit constants in the main body of the paper could enhance clarity. The claim that the regret is "constant" with respect to time is misleading, as Λ typically scales with T even when Γ and V are constant. Therefore, this claim requires nuance.
A potential direction for future research could involve extending this work to the parameter-free bandit setting. Γ is defined by the number of changes between mean vectors, but another relevant quantity is the number of changes in the optimal arm, which can be significantly smaller than Γ. It would be valuable to explore whether this quantity could also be utilized, or if there are compelling reasons to exclude it.