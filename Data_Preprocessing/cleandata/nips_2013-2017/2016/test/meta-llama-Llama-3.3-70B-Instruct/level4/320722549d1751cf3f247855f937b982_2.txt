The present paper explores the online convex optimization setting, where the decision set is either a Hilbert space or the probability simplex. The authors introduce a novel reduction of this problem to a coin betting problem, for which optimal and simple algorithms are well-established. Specifically, they demonstrate that the Krichevsky-Trofimov strategy, or any of its generalizations, which is optimal for coin betting, yields parameter-free algorithms in both settings. The term "parameter-free" signifies that the learning algorithm does not require tuning based on the unknown norm of the competitive vector in the Hilbert space or the unknown KL divergence of a good weight vector with respect to the prior in the probability simplex. This framework effectively generalizes previous works on parameter-free algorithms in these settings. 
In my opinion, the paper is of very high quality, being technically sound and well-written. It makes a significant contribution by generalizing existing algorithms for Online Linear Optimization (OCO) and Learning with Expert Advice (LEA) through a novel reduction to a coin betting problem, which will be of great interest to the online learning community. Although I find the coin betting reduction, particularly for LEA as discussed in Section 4.2, not immediately intuitive, the generality and simplicity it offers make it an attractive approach.
However, a crucial aspect to consider is the assumption that all gains are bounded by 1. If instead, gains are bounded by an unknown value b, several questions arise:
1. Rescaling all gains by b and applying the proposed machinery would indeed yield regret bounds proportional to b, but the resulting outputs w_t would depend on the unknown value of b, which seems to undermine the "parameter-free" claim.
2. Fortunately, for the Krichevsky-Trofimov potential, wt is proportional to the gains, implying that in the LEA framework, the weight vectors pt defined by equation (12) would not depend on b, maintaining the "parameter-free" property.
3. Unfortunately, this invariance property does not seem to hold in the OLO framework, suggesting that the "parameter-free" claim may not be universally applicable.
4. When considering losses instead of gains, if all losses are bounded by b, the transformation gain <- 1-loss/b appears to work for LEA without dependence on b, but a similar approach may encounter issues in the OLO framework.
A detailed discussion of these points should be included in the paper, particularly to address the implications for the "parameter-free" designation. If the concerns regarding points 3 and 4 are valid, the authors should exercise caution when using the term "parameter-free" to avoid any potential misunderstanding.
Additionally, several minor comments are worth noting:
- On line 16, it should read "a Hilbert space" instead of "the Hilbert space."
- In Lemma 1, the phrase "is equivalent to" could be clarified or replaced with "implies" for better understanding, as the equivalence is not immediately clear.
- Following Definition 2, providing examples of potentials would enhance readability by giving immediate context to the definition.
- There is a typo in the Gamma function on line 193, where it should be t^{x-1} instead of t^{-x}.
- At the end of line 394, the cases where ui=0 or \pii=0 should be treated separately to ensure clarity and accuracy, although the result appears to be correct.