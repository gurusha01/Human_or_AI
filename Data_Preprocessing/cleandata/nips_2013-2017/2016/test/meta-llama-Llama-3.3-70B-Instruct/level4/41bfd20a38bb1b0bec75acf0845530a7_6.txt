This paper introduces a thorough approach to sparsifying deep neural networks across multiple dimensions, including channel, filter, filter shape, and layer, presenting a highly practical methodology. To further validate its efficacy, a comparison with the state-of-the-art group-wise brain damage (GBD) method [Lebedev, 2015] is necessary. Notably, GBD, categorized under filter shape in this context, demonstrates a more significant reduction in floating-point operations, particularly evident in the Conv1 layer of AlexNet. Furthermore, the experimental evaluation relies heavily on example networks such as LeNet and AlexNet for CIFAR-10, which are generally over-parameterized and thus more amenable to compression. To comprehensively demonstrate the proposed method's effectiveness, it is suggested that more efficient and optimized network architectures like GoogLeNet, SqueezeNet, and ResNet-152 be incorporated into the analysis.