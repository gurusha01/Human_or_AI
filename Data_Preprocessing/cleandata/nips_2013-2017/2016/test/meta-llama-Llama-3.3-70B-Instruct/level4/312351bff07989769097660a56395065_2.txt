The authors present a neural network approach for mapping input sequences to output sequences in an online manner, utilizing an encoder-decoder framework where the decoder generates output sequences based on the encoder's information and its own previous predictions. This method is applied to a toy problem and the TIMIT phoneme recognition task, with additional proposals for attention mechanism variations. The subject matter is intriguing, as sequence prediction with neural networks remains an open problem, and the need for online sequence processing and prediction methods is acknowledged. However, a significant concern is the lack of citation and discussion of prior work on "transducers" in neural sequence prediction, despite the authors referencing a paper that evaluates both CTC and the transducer from Graves (2012), which outperforms CTC and considers previous predictions while allowing dynamic programming-style inference. This omission limits the novelty of the presented ideas and renders the current title somewhat misleading; a more accurate title, such as "An Online Neural Transducer" or "An Incremental Neural Transducer," might be more fitting. The empirical evaluation is also found to be limited, with decent but not particularly impressive results on TIMIT that fail to demonstrate the benefits of the new method over existing algorithms. The decision not to employ regularization methods to optimize results is unconvincing, especially given the comparison to related work that utilizes attention mechanisms and windowing. Notable aspects of the empirical work include the comparison to a model with reset RNN states between blocks and the analysis of window size and attention mechanism interaction. Ultimately, evaluating the method on datasets with longer sequences, such as Wall Street Journal or Switchboard, could potentially reveal significant advantages of the proposed approach.