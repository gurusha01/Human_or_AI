This manuscript presents innovative deep network architectures designed to predict video frames at the pixel level and introduces a large-scale dataset of real-world object manipulation by robotic arms. The proposed architectures predict pixel motion in each frame rather than directly predicting pixel values, allowing for more accurate predictions on unseen objects. Additionally, some architectures incorporate a 'compositing' scheme, applying different transformations to distinct objects, which enables unsupervised object segmentation to a certain extent. The large-scale dataset facilitates end-to-end system training and rigorous evaluation of competing approaches. Notably, the dataset includes the robot arm's pose and a representation of the goal pose, making it suitable for learning about environmental action effects. These advancements have the potential to contribute to the development of robots that can learn from unstructured environmental interactions. The introduction of a real-world physical interaction dataset between a robot and various objects is a significant contribution, likely to inspire substantial future research. Although the proposed models outperform competing methods, the absolute prediction quality remains relatively poor. While the paper focuses on utilizing the introduced real-world dataset, testing the proposed neural network architectures on datasets like the block world dataset by Battaglia et al. would be beneficial to validate their ability to learn environmental physics. This would allow for a controlled examination of the models' capacity to learn genuine physical concepts and constraints, as learning physics (i.e., underlying causal structure) may differ from learning motion predictions. Recent work, such as Lotter, Kreiman, & Cox's Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning (ArXiv, 2016), has achieved comparable results in pixel-level predictions multiple time steps into the future. Ideally, this method should be included in the paper's quantitative comparisons, but given its recency, a discussion of the work would be a suitable alternative.