This paper proposes an LSTM-based diagonal gradient optimizer, demonstrating its capability to optimize quadratic functions, small MLPs, and images for style-transfer more efficiently than conventional optimizers. Although the topic is intriguing and significant, the paper appears to be rushed in its presentation. 
A key aspect of the paper is the weighting scheme, where each timestep is assigned a weight of 1, implying that the system aims to minimize the integral of the error over time rather than the final error. This approach can be beneficial in the initial stages by rewarding rapid convergence but may lead to an overemphasis on the beginning of the optimization process, where most improvements are made. This issue is central to the question of "What makes a good optimizer" posed earlier in the paper and warrants more explicit discussion.
Several questions and concerns arise from the methodology: the number of optimization runs required to train the optimizer is unclear, and the optimizers are run for a remarkably short duration, with only 1/5 of an epoch on MNIST. The rationale behind this brief timeframe is not provided, despite Figure 2 (right) suggesting generalizability to longer horizons. It would be valuable to demonstrate this or, conversely, to show limitations if present.
A discussion on the limitations of the approach, including a comparison of the runtime with established optimizers like SGD and ADAM, is necessary. The primary limitation is presumed to be memory consumption, given that the LSTM optimizer must maintain approximately 160 states per parameter per step during meta-training.
The generalizability of the trained optimizers across different architectures and datasets is a critical aspect that requires thorough investigation. However, the paper only explores minor architectural changes without error bars, missing a systematic analysis of how factors like hidden layer size impact optimizer performance.
The inclusion of the NTM-BFGS optimizer seems unnecessary, as it is briefly mentioned and only evaluated on cubic functions, where its performance is comparable to the LSTM-GAC. Its removal is suggested to streamline the paper.
Minor issues include the need for clarification in Line 97 regarding the gradient of f, and the repetitive sentence structure regarding optimizer unrolling, which assumes a specific meaning but could be confusing. Line 193 lacks specificity on the averaging process, and plots are presented without axis labels or concise titles. The handcrafted and tuned pre- and postprocessing for each problem seem to contradict the paper's core idea of minimizing handcrafting in optimizers. Finally, exploring the importance of LSTM specifics, such as the number of layers and units, and comparing different optimizer architectures would provide valuable insights.