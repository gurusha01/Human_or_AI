This manuscript presents three complexity metrics for Recurrent Neural Networks (RNNs) and demonstrates their validity through experimental results, which suggest a positive correlation between complexity and performance. 
Update: On line 71, a subscript appears to be missing from the variable V, which should likely be denoted as V_c.
Overall, the paper is well-structured, but some figures require more detailed explanations to enhance clarity. Figure 1(a) effectively corresponds to definitions 2.1 and 2.2, making it easily understandable. However, Figures 1(b) and 2 employ a distinct notation system, necessitating a significant amount of time to decipher. The caption for Figure 2, "We only plot the hidden states within 1 time step (which also have a period of 1)," is somewhat ambiguous and could be improved. Specifically, the use of arrows to denote nonlinear transformations, including implicit input/output nodes, is confusing, particularly in Figure 2a, where the representation appears cyclic rather than unrolled due to the limited number of time slices.
Furthermore, the definitions and explanations provided become increasingly complex to accommodate m > 1, without clear justification for this added complexity. It would be beneficial to discuss whether restricting RNNs to m = 1 would significantly alter the set of RNNs or if allowing m > 1 simply makes explicit a particular structural type that RNNs can have. If the latter, it is essential to clarify the significance of the period number and why it warrants explicit notation.
Minor observations include:
- In the definition of unfolding, specifically in the definition of V_{un}, it seems a subscript 'c' should be added to V for consistency.
- In the definitions, it might be necessary to constrain the weight sigma to non-negative values to maintain consistency and validity.