This paper tackles the composite optimization problem of minimizing the function F = f + g, where neither f nor g is required to be smooth. It assumes that g has an efficient proximal operator and focuses on f with a general structure given by f(x) = max_u(<Ax, u> - φ(u)). The proposed approach involves iteratively approximating the minimization of smoothed versions of f, with the smoothing amounts decreasing exponentially. The key result, Theorem 5, demonstrates that under a θ-Local Error Bound (LEB) condition, the number of iterations required to optimize these smoothed versions is O(ε^(θ-1)). This leads to linear convergence for θ = 1, applicable to functions like absolute value or hinge loss, and 1/t^2 rates for θ = 0.5, relevant to strongly convex functions. The paper explores various conditions leading to LEB, including the KL property and hinge loss empirical risk minimization as seen in Support Vector Machines (SVM). Experimental comparisons with other first-order methods are also provided. 
From a technical standpoint, the experiments are well-conducted, with a notable exception. The proof of Theorem 5 is clearly explained in the supplementary material, and the aspects presented in the paper, such as theorems, definitions, and examples, are sound and well-written. Assuming the authors' claims about novelty hold, the proposed rates under the given setting are new and non-trivial, contributing to the existing body of work on gradual smoothing for achieving faster rates, such as "Optimal Black-Box Reductions Between Optimization Objectives," albeit with different assumptions and outcomes.
The paper's impact lies in demonstrating how LEB and KL properties can facilitate faster-than-typical rates for problems involving non-smooth f and g, potentially inspiring further research into adaptive methods based on data-dependent parameters. The impact could be enhanced by a detailed analysis of computational complexity for empirical risk minimization problems, possibly by substituting the Accelerated Proximal Gradient (APG) with a Stochastic Variance Reduced Gradient (SVRG) variant. Additionally, focusing on representative performance rather than reporting the best performance among various settings of t would strengthen the paper. The current presentation of experiments, highlighting the best HOPS performance without a clear, practical guideline for setting t, falls short of demonstrating practical usability. If PD-HOPS indeed avoids these issues, it should be the primary focus, including a discussion of its potential downsides within the main paper, not just the supplementary material.
Regarding clarity and presentation, the sketch of Theorem 5 lacks sufficient detail to be useful, particularly concerning the role of x^daggers-1,eps and its interaction with Lemma 1. The induction process is neither clearly explained nor particularly engaging. Furthermore, there is a typo on line 231, where min{x∈ℝ^n}F(x) is referred to as having an epigraph, which is incorrect since it represents a numerical value. This should be corrected by removing the reference to the epigraph.