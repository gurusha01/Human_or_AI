This paper presents a straightforward concept for deep semi-supervised learning, where it introduces additional constraints from unlabelled data to ensure that the deep representation of a data sample remains as invariant as possible under nuisance transformations. The concept is rigorously validated, yielding state-of-the-art results on multiple datasets. Notably, the concept itself is concise, occupying only half a page, while the majority of the paper focuses on its thorough validation, which is a commendable approach. The validation demonstrates exceptional performance in semi-supervised learning across various standard datasets, with the high results also attributable to the utilization of advanced state-of-the-art architectures. The authors diligently compare their approach to baselines, including those that do not employ the proposed method or utilize alternative concepts like mutual-exclusivity. However, in terms of originality, the authors could have explored the connection to [31] more thoroughly, as their method bears a strong resemblance, particularly given that [31] experimented with up to 32,000 surrogate classes, akin to the proposal here. The approach also draws parallels with tangent-distance methods [Simard et al.98] and "slow feature analysis," which has been previously investigated in conjunction with ConvNets. Furthermore, a similar technique was employed by [Kulkarni et al. NIPS15], as described in section 3.2 of their work. Consequently, while the idea may not be as innovative as claimed, its value is substantiated by comprehensive experimental validation and comparison, offsetting the lack of novelty to some extent.