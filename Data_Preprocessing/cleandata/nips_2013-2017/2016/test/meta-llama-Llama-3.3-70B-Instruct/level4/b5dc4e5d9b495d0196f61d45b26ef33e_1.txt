This submission explores algorithms for solving a specific class of optimization problems, namely minimizing $F(x) = \max{u \in \Omega2} \langle Ax, u \rangle - \phi(u) + g(x)$ over $x \in \Omega1$, where $g$ is convex, $\Omega1$ is closed and convex, $\Omega2$ is closed, convex, and bounded, and the set of optimal solutions $\Omega* \subset \Omega_1$ is convex, compact, and non-empty. The authors assume that a proximal mapping for $g$ can be computed efficiently. This framework is general enough to capture various applications, including regularized empirical loss minimization problems in machine learning. 
Building on Nesterov's work, which combined a smooth approximation technique with accelerated proximal gradient descent to achieve $O(1/\epsilon)$ iterations for an $\epsilon$-accurate solution, the submission introduces an improvement by leveraging a "Local Error Bound" (LEB) condition on $F$. The LEB condition ensures that the distance of any $x$ from an optimal solution decreases at a rate proportional to a constant power of the distance of $F(x)$ from the optimal value $F(x^*)$. 
The key innovation is to start with a large smoothing parameter $\mu$ and gradually decrease it, allowing for a better smooth approximation as the algorithm approaches an optimal solution without compromising the convergence rate. A primal-dual version of the algorithm, which does not require manual parameter tuning, is also presented. The submission demonstrates the efficacy of the proposed algorithm through applications to specific problem classes and experimental results from three different domains, showing significantly improved iteration complexity for small $\epsilon$ values compared to basic Accelerated Proximal Gradient Descent and a first-order primal-dual method.
The theoretical analysis appears clean and intuitive, offering the first such analysis for this kind of smoothing algorithm. The experiments suggest that the algorithm provides a significant improvement, especially for small values of $\epsilon$. However, it is noted that the advantage of the proposed algorithm seems most pronounced at small $\epsilon$ values, raising the question of whether combining this algorithm with others for an initial coarse solution could be beneficial. 
For certain applications, such as regularized empirical loss minimization with $L1$ or $L{\infty}$ norms and non-smooth loss functions, the submission's algorithm achieves $O(\log(\epsilon_0/\epsilon))$ iterations, potentially offering an exponential improvement over prior work that requires $O(1/\epsilon)$ iterations. It would be beneficial to highlight this point more prominently and to explore the algorithm's performance against prior art for a broader range of $\epsilon$ values.
Minor questions include the necessity of the "backtracking trick" for computing $L_\mu$ in the experimental applications and the details of the wall time measurement for the primal-dual method, specifically whether it accounts for both primal and dual updates. Overall, the submission presents a valuable contribution in terms of both analysis and experimental efficiency, with potential for significant impact on solving a class of important optimization problems.