This study examines semi-supervised learning using convolutional neural networks, introducing an unsupervised loss function designed to minimize the discrepancy between multiple passes of a training sample through a deep network. The proposed method is assessed on various image recognition tasks, with experimental results provided. The work is notable, and the authors effectively establish and report on multiple benchmark image recognition datasets. Nevertheless, the technical contributions can be distilled to the application of a straightforward technique aimed at reducing predictive variation for identical training examples, stemming from methods like randomization, dropout, and max-pooling. A significant portion of the effort appears to focus on experimental demonstrations of the extended loss function's impact. One aspect that requires clarification is the optimization process for the new loss function: did the authors replicate training samples within each mini-batch and retain predictions per sample from preceding iterations to optimize the transformation function, and what implications does this have on network speed and convergence? Notably, the reported accuracies for the new method on the ImageNet ILSVRC task fall substantially short of current state-of-the-art methodologies.