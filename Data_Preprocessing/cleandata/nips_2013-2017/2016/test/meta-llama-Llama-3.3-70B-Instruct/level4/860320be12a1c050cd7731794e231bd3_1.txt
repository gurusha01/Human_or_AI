This paper introduces multiple definitions for quantifying the complexity of a recurrent neural network, specifically: 1) recurrent depth, which assesses the degree of multi-layeredness over time due to recursive connections, 2) feedforward depth, measuring the degree of multi-layeredness from input to output connections, and 3) the recurrent skip coefficient, which evaluates the directness of connections, akin to the inverse of multi-layeredness. Beyond these definitions, the paper makes two significant contributions: it demonstrates that these measures, defined as limits as the number of time steps approaches infinity, are well-defined, and it correlates these measures with empirical performance, showing that increased depth, as measured by these metrics, can lead to improved performance. The proposed measures appear intuitively reasonable and useful. However, the importance of these measures might be questioned if their significance is primarily supported by empirical comparisons rather than formal guarantees. In essence, if the usefulness of these measures could be formally established, their value would be unequivocal. Currently, it is plausible to inquire about the superiority of these measures over other potential complexity metrics. Furthermore, while the extensive use of appendices for detailed complexity analyses follows a common trend in NIPS submissions and does not reflect poorly on the authors, it somewhat detracts from the overall readability of the paper.