This manuscript proposes three deep learning-based approaches for predicting future images of a scene, where each pixel's distribution is learned from the previous scene. The methods encompass predicting pixel movements and implicitly estimating object movements, utilizing a network with multiple masks, including one for the background. Evaluation was conducted on a pushing dataset collected using multiple robots and the Human3.6M dataset. However, the paper's presentation is hindered by the absence of appendices and a non-functional link to supplementary videos, necessitating a review based solely on the provided content.
The descriptions of the three methods are concise but rely heavily on Figure 1, assuming clarity from the network structure that is not entirely evident. Specifically, the structure of m/M and whether it adapts to local image patches or remains constant across the image, relying on masks, requires clarification. Including a visualization of m/M could enhance interpretability. The impressive dataset collected via the robot setup and the methods' outperformance of the state of the art are notable strengths.
However, a critical discussion on the methods' weaknesses and limitations is lacking. For instance, the rationale behind the 64x64 pixel image resolution and the potential for using higher resolutions should be addressed. The applicability of the method to more complex 3D movements, beyond planar movements, also warrants exploration. An example from the Human3.6M dataset demonstrating a larger turning motion in Figure 7 could provide valuable insight.
The qualitative results show considerable blurring, which, while expected in such a challenging task, necessitates a discussion on error sources and potential future improvements. Leveraging the robot setup for more comprehensive evaluations, such as assessing the robot's ability to grasp a moved object based on predicted images, could offer significant insights into the prediction quality. The Mean Squared Error (MSE) criterion, based on optical flow computations, would benefit from an illustrative example to clarify how well the optical flow handles blurring.
Furthermore, scenarios where one proposed method might be preferred over the others should be outlined. Consolidating some plots could create space for more in-depth discussions, such as the impact of removing background masks on performance. The paper presents an intriguing approach to a challenging task and introduces a substantial new dataset, with an impressive network structure. Yet, the lack of detailed method descriptions and thorough limitation discussions hinders the paper's accessibility and potential long-term impact. Enhancing these aspects would significantly improve the manuscript.