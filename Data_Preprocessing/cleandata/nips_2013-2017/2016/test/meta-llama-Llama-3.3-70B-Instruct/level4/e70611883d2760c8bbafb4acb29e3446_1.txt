This manuscript introduces a novel methodology for kernel learning based on random features. The proposed approach involves a two-stage process, initially generating a set of random features, followed by the optimization of their weights to achieve maximum alignment with the target kernel. The authors demonstrate an efficient optimization strategy for the resulting model and provide theoretical guarantees regarding its consistency and generalization capabilities. Experimental evaluations are conducted on several benchmark problems, showcasing the potential of the proposed method as a valuable contribution to the kernel learning literature. A notable strength of this approach is its ability to learn an effective kernel even when the original data does not align well with the target, as highlighted in section 4.1. However, the feature selection experiments could be improved by comparing against more sophisticated feature selection methods, as the current comparison to completely random feature selection may not fully demonstrate the method's capabilities. The observed speedup over joint optimization methods is also a significant advantage. One area for improvement is the clarity of the manuscript, as the writing is somewhat convoluted and requires multiple readings to fully comprehend. Specific suggestions for improvement include: 
- At line 64, it would be beneficial to explicitly state the assumptions made regarding the feature function \phi.
- Starting from line 76, enhancing the notation by including dimensionalities of variables, such as W, would improve readability.
- Equation (6) could be clarified by explaining the rationale behind the inclusion of the square root.
- Line 169 lacks clear justification for the consequences of choosing the Gaussian kernel, particularly how the exact form of the feature function is determined and its independence from other choices.