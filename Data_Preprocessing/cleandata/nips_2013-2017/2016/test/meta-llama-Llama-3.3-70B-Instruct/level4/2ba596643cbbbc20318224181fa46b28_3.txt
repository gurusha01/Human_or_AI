This paper presents a novel "multi-fidelity" extension of the multi-armed bandit problem, where each of the K arms can be played at one of M distinct fidelity levels, with lower fidelity resulting in reduced costs but potentially increased bias in reward estimation. The algorithm proposed, a UCB-based approach with adjusted confidence intervals to account for the bias introduced by low-fidelity sampling, is analyzed and its performance guarantee is provided in the form of complex regret bounds that are difficult to decipher. The authors argue that this approach can significantly outperform the standard UCB algorithm, which always plays arms at the highest fidelity, and support this claim with simulation results. Initially, the choice to model low fidelity as increased bias rather than variance seemed counterintuitive, but the provided motivating examples, such as algorithm selection for machine learning, helped to justify this decision, although some motivations, like the effectiveness of ad display time, remain less convincing. Furthermore, the application of regret minimization in algorithm selection for machine learning is not clearly justified. Overall, while the connection between the proposed model and its motivating applications is somewhat tenuous, it does not constitute a fatal flaw. The primary contribution of the paper lies in formulating the multi-fidelity multi-armed bandit problem, with the proposed algorithm representing a fairly standard application of the "optimism in the face of uncertainty" principle. However, the performance guarantee is cumbersome and hard to interpret, and the simulation results, which show an improvement of 3 to 4 times over the standard UCB algorithm, are difficult to evaluate due to a lack of justification for the chosen parameter values, leaving uncertainty as to whether this performance boost is representative of real-world scenarios or an artifact of the simulation setup.