This paper suggests utilizing Graph Lasso for weight pruning in a manner that preserves the 4D tensor structure, enabling the removal of entire channels, filters, or layers, including the application of the "shortcut" technique found in residual networks. The rationale behind this approach is twofold: it offers semantic coherence and mitigates the computational overhead associated with genuinely sparse weight matrices, particularly in convolutional layers, where sparsity can be highly irregular. However, the concept of employing Graph Lasso for weight pruning lacks sufficient novelty to warrant publication at NIPS. Only exceptionally strong experimental results could potentially justify its consideration, but unfortunately, the presented results are underwhelming.