The manuscript explores online learning with K actions under both full and bandit information settings, focusing on dynamic regret, which is defined as the regret measured against the strongest comparator that can switch actions in each round. A key contribution of the paper is the examination of how the total variance of the loss vectors impacts dynamic regret, yielding novel upper and lower bounds in terms of this quantity. The paper is well-structured, and its findings appear to be new. Although the technical tools employed in the proofs are largely standard, their application presents non-trivial challenges, and the analysis is executed skillfully. However, the results may be of limited practical interest, as the proposed algorithms are not highly practical, and the bounds do not offer drastically new insights into the complexity of learning in these settings. Furthermore, there is a concern regarding the scalability of the newly introduced quantity Λ with the number of arms/experts K, potentially leading to worse scaling compared to existing bounds. The lower bounds presented in Theorem 4.3 seem to contradict known upper bounds at first glance, but this discrepancy may stem from the theorem's wording, which should clarify that its statement applies only in the sense that for any fixed choice of Λ, T, and V, there exists a sequence where any algorithm will suffer a regret of Ω(...). It would be beneficial for the authors to address this issue to avoid confusion. While the results may not be groundbreaking, they are worthy of publication. 
Detailed comments:
- Line 050: Consider rephrasing the section to introduce the notion of dynamic regret before stating the bounds for better clarity.
- Line 061: Similar to the suggestion above, the informal definition of dynamic regret could be presented earlier for smoother reading.
- Line 089: The bound technically does not recover previous results due to a worse scaling in V; this could be resolved by discussing the asymptotics of V in relation to T.
- Line 111: For precision, consider changing "suffer a corresponding loss \ell{t,i}" to "suffer a corresponding loss \ell{t,a_t}".
- Line 146: Adding a reference for vUCB, such as Audibert, Munos, and Szepesvari (2009), would be helpful.
- Theorem 3.1: Given that the paper covers both full-information and bandit settings, explicitly stating which setting each theorem pertains to would enhance clarity.
- Equation (5): Note that this equation more closely resembles the Bernstein Online Aggregation algorithm by Wintenberger (arxiv 2014) or the Variation MW algorithm by Hazan and Kale (COLT 2008) rather than the algorithm in [4].
- Theorem 3.4: It's worth noting that better bounds could be achieved using the algorithm by Steinhardt and Liang [10], which depends on the path length of the best expert, although this algorithm requires parameter tuning.