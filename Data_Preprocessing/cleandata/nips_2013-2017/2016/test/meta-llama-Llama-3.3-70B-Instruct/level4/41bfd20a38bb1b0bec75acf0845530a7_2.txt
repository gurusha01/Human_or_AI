This manuscript presents a methodology for accelerating CNN execution by leveraging group sparsity across various parameters. The approach involves pre-training a CNN using a traditional baseline, followed by the application of group sparsity constraints during retraining, starting from the baseline parameters. Subsequently, inactive groups are eliminated, and the resulting network is fine-tuned without sparsity constraints, yielding notable speed improvements with minimal accuracy loss, and occasionally, slight accuracy gains. The concept of utilizing group sparsity to deactivate redundant CNN components and enhance efficiency appears promising. The experiments demonstrate substantial speed-ups across a wide range of scenarios, accompanied by negligible accuracy degradation and occasional minor improvements. The authors apply group sparsity along multiple dimensions, including the number of filters and channels, filter shapes (although the method for efficiently deactivating specific filter sites is unclear and warrants clarification), and the number of layers (with shortcuts employed to maintain network connectivity). While the idea explored in the paper is relatively straightforward, it is also practical and potentially useful. However, several crucial details are lacking, such as the optimization procedure for group sparsity within CNN training and the selection process for regularization weights, including whether cross-validation is used. For instance, Table 2 mentions results obtained with varying strengths of structured sparsity regularization, but the underlying methodology is not specified. These omissions are significant because achieving a balance between speed and accuracy is essential. To ensure practical applicability, simply replicating baseline results on test data is insufficient; the authors must provide clarification on these points to strengthen their contribution.