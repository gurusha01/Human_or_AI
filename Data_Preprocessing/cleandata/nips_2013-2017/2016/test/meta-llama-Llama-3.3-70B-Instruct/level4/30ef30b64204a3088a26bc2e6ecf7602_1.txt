This paper investigates the potential of leveraging unlabeled data, specifically transformed versions of the training set, to enhance the generalization capabilities of convolutional neural networks (CNNs). It proposes an unsupervised loss function that can be used in conjunction with standard supervised training to minimize the variability of multiple passes of the same point through the network due to stochastic regularization techniques, such as dropout and stochastic pooling, or various augmentation transformations. The loss function aims to minimize the prediction difference among all combinations of transformed or perturbed points without using label information, while also enforcing mutual exclusivity in the values of the prediction output vector. 
The majority of the work is experimental, with extensive benchmarks using two different network architectures. The results show that the unsupervised loss consistently improves performance, particularly in the low sample regime, and achieves state-of-the-art performance on CIFAR10 and CIFAR100. This work contributes to the field of semi-supervised learning by proposing the use of an unsupervised loss term to improve the regularization capacity of CNNs. The idea is conceptually simple, enforcing stability explicitly by minimizing the difference between predictions corresponding to the same input data point.
The paper's focus on experimental results is its stronger aspect, with the largest part devoted to presenting results when adding the new loss to standard supervised CNNs. However, the weaker aspect is the lack of baselines and theoretical justification, derivation, or discussion. The novelty of the work lies in the application of the unsupervised loss term for controlling the stability of predictions under transformations or stochastic variability, although the mutual exclusivity term is from previous work.
The technical quality of the paper is high, with extensive experimental evaluations on well-known vision datasets using two different models and two different perturbation types. Comparisons with one other relevant method are included, but baselines could be further improved by including supervised training on augmented labeled data using the same transformations as in the unsupervised case. The potential impact of the work is significant, as it makes the case for using unlabeled data to complement supervised training and enforcing stability through a simple loss on the predictions.
However, several questions arise from the analysis. The proposed loss is not studied theoretically or discussed in the context of the underlying perturbations, which could be seen as additional regularization or data augmentation. Some discussion would be useful to clarify the equivalence of dropout to model averaging. Additionally, the efficiency of the proposed loss in terms of scaling up with the number of transformations in the augmented set is unclear, and an empirical study on the role of the number of transformations and the trade-off between performance and efficiency might be insightful.
The implementation of the loss computation during training is also unclear, and it is uncertain whether creating mini-batches with replicates or transformations of the same data points would create gradient bias or affect convergence. Furthermore, the comparison with other methods, such as Ladder networks, might not be exactly fair, and a relative error decrease with respect to the supervised network used as a baseline for each model might be more meaningful.
In terms of clarity and presentation, some revisions could be made to improve the focus on semi-supervised learning methods in CNNs and stochastic regularization techniques. The network parameter description could be made more compact and descriptive, and some sections, such as Sec 4.2 and 4.3, could be merged or shortened. The comments on experiments highlight the need for additional baselines, such as training on the entire augmented labeled set, and more detailed comparisons with other methods.
Minor comments and typos include missing entries or only dates in many references, and the need to update the ImageNet reference. Some lines, such as line 69, could be revised for clarity, and minor typos, such as "more stable generalization," could be corrected to "stability is generalization." Overall, the paper presents a significant contribution to the field of semi-supervised learning, but could benefit from additional theoretical justification, improved baselines, and clearer presentation.