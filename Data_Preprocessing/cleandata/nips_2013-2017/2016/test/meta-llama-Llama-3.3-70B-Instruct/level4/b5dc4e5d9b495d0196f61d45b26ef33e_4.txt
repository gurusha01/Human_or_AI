This manuscript presents a phased implementation of Nesterov's smooth minimization technique for non-smooth functions, utilizing increasingly refined strongly convex regularizers for the dual problem to achieve accelerated convergence for a specific class of optimization problems. The key insight is that the final iterate of each phase can be leveraged as a warm start for the subsequent phase, as supported by a straightforward lemma that relates norm distance to sublevel sets with value distance. When combined with a local error bound condition, this approach yields improved convergence for certain problems. The concept is elegant and intuitive, yet it appears that a rigorous analysis of this "homotopy" method was previously lacking. The paper is well-structured and introduces the topic gently, starting with an explanation of Nesterov's smoothing technique. To further strengthen the results, it would be beneficial to derive the exact iteration bound for optimizing Hoffman's bound and cone programming, as the claimed linear convergence for these problems is intriguing. Although a preliminary search suggests that first-order methods with linear convergence for LP are not entirely novel, a more detailed analysis of how problem parameters influence the iteration count is necessary to support claims in this direction. Overall, the paper remains engaging, and my initial assessment is unchanged.