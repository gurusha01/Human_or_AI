This paper presents enhanced lower and upper bounds on regret in both bandit and full information settings, expressed in terms of four key parameters: the frequency of optimal arm changes (∆), the cumulative difference in mean rewards between rounds (V), the sum of reward variances across rounds (∧), and the time horizon (T). Notably, previous studies only combined T with ∆ or V, and can be seen as a special case of the results presented here. The paper is well-structured, readable, and provides clear descriptions of all details. Although the derived bounds may not be the definitive version, as they do not capture all aspects of problem complexity, they offer valuable additions to existing literature and contribute to a deeper understanding of the topic. One limitation is that most algorithms require prior knowledge of parameters, which is unrealistic. Nonetheless, the paper's novelty and contributions make it a significant addition to the field. 
Minor suggestions for improvement include adding a remark on Line 192 to clarify that "mt is determined below" for better readability. Additionally, clarification is needed on Lines 186-187 regarding the selection of a random arm using distribution pt, and on Line 205, where "\eta{t,k}" should possibly be "\eta{t-1,k}". Furthermore, to avoid index duplication, a different index should be used instead of "i" on Lines 304-309. The argument between Lines 318-320 seems redundant, as plugging "\sqrt{VT}" into Theorem 4.1 directly implies the claim. 
The inclusion of Table 1 in Appendix A is commendable, but it could be further enhanced by highlighting upper bounds based on algorithms that require additional parameter knowledge. A broader question arises regarding the computation of regret against the best dynamic learner; it would be intriguing to explore the potential for these parameters to improve classical results by considering regret against the best fixed arm.