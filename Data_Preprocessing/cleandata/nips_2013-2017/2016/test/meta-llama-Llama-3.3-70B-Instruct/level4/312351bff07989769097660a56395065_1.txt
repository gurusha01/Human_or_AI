The proposed "neural transducer" model is designed for sequence-to-sequence tasks, operating in a left-to-right and online manner, where output is generated as input is received, rather than waiting for the entire input. This approach is facilitated by key components including a recurrent attention mechanism, an end-of-block symbol in the output alphabet to signal progression to the next input block, and approximate algorithms based on dynamic programming and beam search for training and inference. Experimental results on the TIMIT speech task demonstrate the model's effectiveness and explore various design parameters. 
This paper is well-structured, tackling the worthwhile problem of constructing and training a sequence-to-sequence model for online operation. It clearly outlines an architecture for solving this problem and provides a detailed walkthrough of the design issues for each component, including next-step prediction, the attention mechanism, and block end modeling. The challenges in training the model and performing inference are also clearly explained, along with reasonable approximate algorithms proposed for these tasks. The speech recognition experiments are reasonable, demonstrating the utility of the transducer model and exploring design issues such as recurrent state maintenance, block size, attention mechanism design, and model depth.
However, there are a few areas that require improvement. On page 3, line 93, an editing error is noted in the sentence describing the computation of output sequence probability. In Section 3.4, a discussion on the comparative performance of the different approaches to end-of-block modeling is lacking. It would be beneficial to understand how the use of the <e> symbol compares to the other two methods in terms of performance. 
Section 3.5's approach to computing alignments less frequently than model updates bears resemblance to the use of lattices in sequence-discriminative training of acoustic models, as seen in previous works such as those by D. Povey and P. C. Woodland, and B. Kingsbury. Future work on the transducer model could explore the use of lattices to represent alignment information, potentially leveraging methods developed for rescoring lattices using RNN language models to address the model's conditioning on all input and output up to the present moment.
Minor corrections are also suggested: on page 7, lines 202-203, "Log Mel filterbanks" should be corrected to "Log Mel spectra." Furthermore, the TIMIT experiments deviate from standard practices, such as phone collapsing and specific data exclusion from the training set. Following the standard framework, as outlined in works by T. N. Sainath et al. and K. F. Lee and H. W. Hon, and clearly stating any deviations, would enhance comparability with other studies. 
Additionally, initializing the transducer with HMM-GMM alignments and then continuing training with alignments inferred by the transducer could potentially lead to improved TIMIT performance and is suggested as a future direction. Overall, the paper presents a significant contribution to the field, with minor adjustments and additional explorations proposed to further enhance its impact.