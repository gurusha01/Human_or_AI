The authors propose an encoder-decoder architecture that utilizes attention mechanisms over blocks of input data and a variable-length decoder structure. The encoder consists of a multi-layer LSTM RNN, while the decoder is an RNN model conditioned on weighted sums of the encoder's last layer and its previous output. The attention mechanism allows for varying weighting schemes, which can be conditioned on hidden states or previous attention vectors. The decoder generates a sequence of symbols until it produces a special end character, at which point it moves on to the next block of input data. The system is trained by fixing an alignment that approximates the optimal alignment, which is computed using a beam-search procedure with a restricted set of symbols. 
The paper is well-written and clear, and the main contribution appears to be the novel application of the transducer architecture on a block-by-block basis, with a separate RNN conditional on its last definite output. However, this approach bears similarities to previous work, such as Graves' sequence transduction method, which also uses a separate prediction network. The training procedure is complex, and exact constrained inference becomes intractable. The experimental evaluation is performed on an addition task and the TIMIT dataset, with results showing competitive performance. 
However, the novelty of this approach is unclear, and it seems to share similarities with existing methods, particularly Graves and Hinton's Transducer network. The use of attention mechanisms does not necessarily provide significant improvements, unless larger window sizes are used. The model's ability to generalize to longer numbers or more complex tasks is also unclear. Additionally, the computational efficiency of the model, particularly in terms of training time, is not discussed. 
The comparison to Graves' Transducer on a unidirectional LSTM RNN is notable, with the best reported performance being 19.6%, which is close to the 19.8% reported in this paper. However, the authors argue that their model requires further tuning, which can be a significant undertaking. The use of a 3-layer LSTM RNN transducer achieves the best results, but it is unclear how much the model overfits, particularly with deeper architectures. 
Overall, while the model's performance is competitive, its complexity and similarities to existing methods raise questions about its novelty and interest. Further clarification on how this model differs from Graves' and Hinton's Transducer network would be beneficial, as well as an analysis of its scalability and computational efficiency. The model's ability to generalize to more complex tasks and its potential for overfitting also require further investigation.