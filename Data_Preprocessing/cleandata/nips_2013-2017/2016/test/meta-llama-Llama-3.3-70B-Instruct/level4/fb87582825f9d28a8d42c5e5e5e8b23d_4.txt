This manuscript presents a novel approach to simplifying the design of optimizers by utilizing a meta-optimizer that takes the gradient of the function to be optimized as input and produces the update to be applied as output. The concept is intriguing and has the potential to benefit practitioners. However, upon closer examination, it appears that the paper struggles to fully deliver on its promises, and significant hurdles must be overcome before this idea or a similar one can be widely adopted. The optimization community is vast, with numerous papers published annually on new techniques, making it challenging to determine the most suitable method for a particular problem. Typically, this requires extensive experience, and even after selecting a technique, optimizer hyperparameters must be tuned. 
The paper attempts to address this issue by proposing a meta-algorithm that outputs optimizers, or more specifically, gradient transforms, suited to a wide range of problems. This idea is appealing, and if successful, it could be of great significance to the community. The manuscript is generally well-written, with a thorough literature review and clear presentation of ideas. Although I am not extensively familiar with Long Short-Term Memory (LSTM) networks, I was able to follow the gist of the meta-algorithm and the claims made. 
However, I would like to comment on the actual claims made and how the paper delivers on them. Firstly, the paper claims that the proposed architecture can model famous algorithms such as L-BFGS. While it is technically possible from a parametric standpoint, as it can model a low-rank linear transformation of the gradient using a history of past updates, it is an overstatement to claim that "a memory could allow the optimizer to learn algorithms similar to [...] L-BFGS." The presence of "if appropriately designed" does not mitigate the fact that L-BFGS relies on very specific updates, and there is a substantial gap between implementing history-based transforms and implementing something similar to L-BFGS. 
Secondly, while it is true that there is a wide variety of optimizers tailored to specific classes of problems, which is indeed a problem, the proposed architecture does not cater to these specific classes of problems. For instance, the LSTM is not adapted to problems involving sparsity, convex problems with finite training sets, strongly-convex problems, or optimization over a compact set. In fact, all comparisons are made with diagonal scalings of Stochastic Gradient Descent (SGD). As such, the paper falls short of its claim that "the design of an optimization algorithm can be cast as a learning problem." The conclusion is more accurate, stating that "learned neural optimizers compare favorably against state-of-the-art optimization methods used in deep learning." 
Thirdly, the use of an LSTM actually increases the number of parameters the practitioner needs to tune. I would have liked an analysis of the sensitivity of the LSTM to the specific optimizer used, but my lack of knowledge on LSTMs prevents me from commenting further on this issue. Additionally, unless I am mistaken, momentum or other tricks such as batch normalization were not used for the optimizers the LSTM is compared to. As the difference in performance seems to be of the same order as the gain of such tricks, this comparison would greatly benefit the paper. 
In summary, the idea of designing a model that takes local information about the function as input and outputs an update, using a hidden state to summarize history, is very appealing and could unify many current techniques using stochastic gradient or variants thereof. However, while the added complexity of an LSTM would be negligible if the rest lived up to its expectations, it is not the case when the impact is a speedup over a few well-established methods. Thus, the paper needs to be more thorough in its analysis of the optimization of the LSTM and its impact on a wider range of problems. 
After carefully considering the authors' feedback and discussion, I believe they sidestepped the main issue I was raising. I fully agree that designing an optimizer through examples rather than through mathematical properties is appealing, but this is not what the paper demonstrates. Specifically, it does not show that the work needed to learn that optimizer is smaller than the work needed to tune a first-order optimizer for deep nets. Also, it offers generalization at a very narrow scale, which means I cannot conclude from reading the paper that the amount of designing work for someone who wishes to solve a broad range of problems is smaller than if they had to tune each optimizer individually. That is why I do not change my score and still think that this paper needs a major rewriting to be fully honest about what it achieves.