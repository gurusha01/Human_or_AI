This manuscript proposes three distinct unsupervised approaches for predicting future video frames based on an agent's action, specifically forecasting object movement without explicit optical flow estimation. The methodology employs a deep neural network architecture that integrates LSTM, dynamic neural advection, and spatial transformers. Extensive evaluations are conducted on a newly introduced dataset of robots interacting with objects (scheduled for release) and the Human3.6M dataset, with comparisons to state-of-the-art methods yielding impressive performance. The paper is well-structured, the method is thoroughly evaluated, and the quantitative results are compelling. The topic of learning physical interactions from video is indeed challenging and has significant implications for autonomous systems and beyond. However, several aspects of the approach are difficult to assess thoroughly: 
1. The still images provided, which quickly become blurry, raise concerns about the utility of the method, especially given the limited 1-second prediction horizon. Although the link to videos was not functional, potentially providing more convincing results.
2. The utilization of LSTM for predictive purposes is not novel; the authors' rationale for selecting this architecture and the potential benefits of adding more LSTM layers or exploring alternatives like differentiable long-term memory (as in Neural Turing Machines) would be beneficial.
3. The dynamic neural advection component lacks clarity, particularly regarding the definition and parametrization of 'M' - whether it represents a scalar value per pixel and how motion is parameterized.
4. More detailed mathematical explanations of each architectural component would significantly enhance the reader's understanding of the modeling assumptions, limitations, and facilitate result reproduction. Unfortunately, access to videos and appendices was not available during this review, which would have provided valuable additional context.