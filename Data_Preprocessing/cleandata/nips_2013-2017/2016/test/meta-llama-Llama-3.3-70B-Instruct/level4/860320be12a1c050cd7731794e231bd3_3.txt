This paper presents a compelling theoretical framework, leveraging graph theory to describe the architectural connections of Recurrent Neural Networks (RNNs) in a general sense, which facilitates a straightforward explanation of how to unfold an RNN. The authors further propose three complexity measures for tree architectures of RNNs: recurrent depth, feedforward depth, and the recurrent skip coefficient. Experimental results across various tasks underscore the significance of these measures for specific tasks, suggesting their potential as guidelines for designing RNNs tailored to particular applications. 
Several minor adjustments are recommended for clarity and accuracy: 
1. On line 59, the index set should be specified as $i \in {0, 1, 2, ..., m-1}$ for precision.
2. Enhancing the caption of Figure 1 with a brief methodology for computing $dr$, $df$, and $s$ for the networks depicted in (b) would improve readability.
3. The equation on line 155 might require a slight correction to $df = \sup{i, n \in \mathbb{Z}}(\mathfrak{D}i^{\ast}(n) - n \cdot dr)$, adding parentheses for clarity around the $\sup$ operation.
4. Similarly, on line 167, adding parentheses around the $\sup$ and $\max$ operations could enhance readability and precision.
5. Considering the extensive application of RNNs in speech recognition, incorporating experimental results from this domain could broaden the paper's appeal and relevance. Utilizing existing resources like Kaldi or Librispeech could facilitate this expansion, offering a straightforward path to analyze and present findings in speech recognition tasks.