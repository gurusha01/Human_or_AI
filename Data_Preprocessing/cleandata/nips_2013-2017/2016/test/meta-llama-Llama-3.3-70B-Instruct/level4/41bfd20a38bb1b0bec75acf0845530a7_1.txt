This manuscript investigates the application of structured sparsity, specifically group lasso, to downsize convolutional neural networks in a manner that enhances regularity, thereby facilitating more efficient vector computation on GPUs compared to the straightforward l1 regularization approach recently examined by Han et al. This innovation is deemed significant. The experimental outcomes appear encouraging, with depth regularization being a notable highlight. Overall, the paper presents a compelling set of experimental findings. Optimizing the efficiency of deep neural networks (DNNs) is a fundamental challenge in deep learning, and sparsification not only contributes to improved performance through enhanced regularization but also to better interpretability. Although the concept of structured sparsity is well-established, the novelty of this work lies in its application to DNNs, even if the overall innovation may not be groundbreaking.