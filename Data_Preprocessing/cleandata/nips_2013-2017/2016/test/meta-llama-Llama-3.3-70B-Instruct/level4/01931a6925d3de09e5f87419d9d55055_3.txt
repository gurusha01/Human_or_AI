This manuscript presents a novel approach to modeling the observation space by introducing a hand-crafted Riemannian metric and defining a curved normal distribution based on the associated Riemannian geometry, thereby enhancing the capture of the data manifold. The work tackles a fundamental issue in defining a Riemannian metric from a set of high-dimensional real-valued observations and subsequently defining and estimating a curved Gaussian distribution based on this geometry, offering a valuable tool and an intriguing application of Riemannian geometry in statistical learning. 
The methodology's initial stage, outlined in section 3.1, establishes the underlying Riemannian geometry. However, despite the title's suggestion of "learning a metric," the metric is actually predefined in equation 7 using two hyper-parameters, rho and sigma, which are empirically fixed in advance. It is presumed that sigma is a sensitive parameter, and thus, the paper should provide a more detailed discussion on the intuitive effects of varying sigma and the process of selecting this parameter. The absence of this discussion renders the paper incomplete. Furthermore, it would be beneficial to explore the possibility of integrating the metric learning stage with the LAND learning stage, allowing rho and sigma to be learned.
A key aspect of the geometric approach is its invariance to coordinate transformations. Nonetheless, the metric defined in equation 7 lacks this invariance. For instance, if the data forms a line in a 2D observation space, the metrics obtained in different coordinate systems (e.g., one with its x-axis parallel to the line and another at a 45-degree angle) would be significantly different. A more in-depth discussion on this issue, including the invariance of the full metric and the intuition behind the metric in equation 7, would be valuable.
The literature is rich in metric learning and manifold learning connected to Riemannian geometry, which could be related to this work. A major criticism of the paper is the first experiment's comparison, which is not particularly meaningful since LAND, being more flexible, can describe the data better with fewer components, whereas GMM can perform better with a larger number of components, potentially leading to overfitting by LAND. The authors are advised to redesign the experiment, allowing each method to choose its optimal number of components based on a specific criterion (e.g., BIC) and then comparing their performances. It is expected that LAND would still outperform due to its better description of the manifold structure, making the comparison more meaningful. Additionally, it would be interesting to observe how LAND overfits with a large number of components.
Given that LAND is a more expensive procedure than estimating a Gaussian, it would be beneficial to include quantitative measurements or experiments to clarify the extent of this expense and the scale limit that LAND can handle. An experiment involving learning LAND on MNIST and comparing it with Gaussian mixture learning could provide valuable insights.
After considering the authors' response, it is clear that LAND is a more flexible model than the Gaussian distribution. However, increasing the number of components makes each local LAND more susceptible to local noises compared to a local Gaussian. Although "the geodesics will almost be straight lines," they are still curves that can lead to overfitting. Therefore, revising this experiment in the revision is highly recommended.