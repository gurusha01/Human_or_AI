The authors present a novel approach to learning update functions in gradient-based optimizers by concurrently training an optimizer and optimizee, enabling the learning of LSTM-based DNN parameters that aggregate information from multiple gradients, analogous to momentum. Upon training, the optimizer can be repurposed for similar tasks and exhibits partial generalizability to new network architectures. Although the theoretical underpinnings of the resulting algorithms remain unexplored, empirical experiments on a small scale demonstrate their competitiveness with current state-of-the-art methods. This paper is noteworthy for its creativity, timeliness, and potential impact, as it represents a significant step towards the concept of "learning algorithms" in deep learning. The experimental results appear satisfactory, but larger-scale tests would be beneficial to fully assess the method's capabilities, such as its potential to train complex models like future iterations of AlexNet. Furthermore, a more in-depth post-hoc analysis could provide valuable insights into the optimization strategy employed by the learned algorithms, potentially revealing the underlying signals they exploit and informing new theoretical directions for the optimization community. While the results show that the learned algorithms surpass state-of-the-art alternatives, the margin of improvement is modest, suggesting that current optimization algorithms are highly effective, a testament to the advancements made in the field. A more significant demonstration of the method's capabilities would be its ability to solve previously intractable problems, highlighting its true potential.