This manuscript introduces LAND, a novel parametric metric defined on a nonparametric space, which integrates concepts from metric learning, manifold learning, and Riemannian statistics. Specifically, the proposed metric is a Riemannian normal distribution, as defined by Pennec [15], with a maximum likelihood algorithm employed to estimate its mean and covariance, effectively yielding a Mahalanobis distance for Riemannian manifolds. The distances between points are determined by geodesics along the Riemannian manifold, similar to Hauberg et al. [9], with a local metric learned using the inverses of local diagonal covariances. The authors also explore mixtures of LANDs, providing new insights into the interplay between metric learning and Riemannian statistics. The development of parametric and generative models that can generalize beyond training data is highly valuable in various scenarios. However, a primary concern is the applicability of LAND to real-world data, which is often large-scale and high-dimensional. The computation of a covariance matrix and its inverse for each data point, as required by LAND, is known to be challenging in high dimensions. Although the authors acknowledge the potential issue of high-dimensionality (lines 265-266), a thorough analysis of the complexity and limitations of the proposed metric is necessary. It would be beneficial to clarify what is considered high-dimensional, as this can vary significantly (e.g., 3, 5, 10, or 100 dimensions). Furthermore, the paper would benefit from a more comprehensive discussion of its position within the vast literature on metric learning, as Hauberg et al. [9] have already discussed relevant works, such as those by Frome et al. [1,2] and Malisiewicz and Efros [6], which learn diagonal metric tensors for every point, similar to LAND. The use of local covariances is also reminiscent of methods like local PCA (Kambhatla â€“ Dimension reduction by local principal component analysis). The choice of the Gaussian kernel parameter sigma for defining locality (lines 101-103) is only discussed for the sleep data in Sect. 4.2, but not for the synthetic data in Sect. 4.1, and its influence on the results is unclear. Robustness tests for sigma would be valuable, particularly in determining whether sigma remains constant across all points, regardless of density. The comparison of LAND to intrinsic estimators in Fig. 4 yields similar results, but the visualization results in Figs. 5 and 6 only show comparisons to GMM, which is already demonstrated to perform poorly in Fig. 4. The inclusion of visual results using intrinsic estimators, as shown in the supplementary material (Figs. 2 and 4), would provide more insightful comparisons. An analysis of the computational complexity of LAND versus intrinsic estimators would be interesting, as it could inform the choice of method for specific data structures. The consideration of mixtures of LANDs is a notable aspect of the paper, as real-world data is often generated by multiple components, making this application worthy of further exploration. Overall, the manuscript is well-written and a pleasure to read.