This manuscript explores the application of structured sparsity learning to reduce the complexity of deep neural network parameters, enabling rapid computation with minimal accuracy loss. The authors propose a structured sparsity learning methodology to streamline or accelerate a trained deep network, facilitating deployment on platforms with constrained computational resources. The concept of deriving a compact, hardware-amenable architecture is intriguing. Nevertheless, several aspects require clarification: 
1. What is the relationship between the proposed approach and hardware considerations, and how can CPU, GPU, or FPGA-friendly structures be obtained, or what optimization objectives should be designed to achieve this?
2. Is it possible to utilize this approach to convert the network into a version that exclusively employs integer operations?
3. How can the trade-off between computational complexity and error in the simplified network be optimized?
4. What are the computational complexity and overhead associated with the proposed methodology itself?