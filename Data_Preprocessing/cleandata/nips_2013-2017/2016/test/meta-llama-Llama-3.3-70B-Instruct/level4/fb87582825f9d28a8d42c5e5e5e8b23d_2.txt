This paper, "Learning to learn by gradient descent by gradient descent," proposes an LSTM that learns entire gradient-based learning algorithms for specific classes of functions, building upon similar work from the 1990s and early 2000s. The authors cite the idea of using learning to learn or meta-learning to acquire knowledge or inductive biases, referencing Thrun and Pratt (1998). However, the introduction to this reference conflates meta-learning, which involves learning the learning algorithm itself, with transfer learning, where a model learns to adapt to new data more quickly. 
To my knowledge, the first work on learning general learning algorithms written in a universal programming language was published by J. Schmidhuber in 1987, in the diploma thesis "Evolutionary principles in self-referential learning, or on learning how to learn: The meta-meta-... hook." The authors also mention that their work was built upon by Younger et al. (2001) and Hochreiter et al. (2001), where a higher-level network acts as a gradient descent procedure, with both levels trained during learning. Additionally, they reference Runarsson and Jonsson (2000), who trained similar feed-forward meta-learning rules using evolutionary strategies, and Schmidhuber (1992, 1993), who considered networks that could modify their own behavior and act as an alternative to recurrent networks in meta-learning.
However, this description does not fully acknowledge the previous work. Schmidhuber's 1993 meta-RNN was not an alternative to RNNs but an RNN capable of running arbitrary computable weight change algorithms on itself, allowing it to sequentially address and modify its own weights. This system was differentiable end-to-end, enabling gradient descent to search for a learning algorithm, a meta-learning algorithm, and so on, without restrictions. Furthermore, Hochreiter's meta-LSTM did directly address the transfer of a learned training procedure to novel problem instances, as it was trained on certain quadratic functions to learn a learning algorithm for quadratic functions and then generalized to new quadratic functions.
The authors' approach explicitly biases their meta-learning setup towards the learning of gradient-based learning algorithms, which may be a good thing in the context of the studied experimental tasks and should be emphasized. They also reference Daniel et al. (2016), who used reinforcement learning to train a controller for selecting step-sizes, but there was an earlier, more general reinforcement learning system that learned to learn learning algorithms, namely the success-story algorithm (SSA) or EIRA, published by Wiering and Schmidhuber (1996).
The authors implement the update rule for each coordinate using a two-layer Long Short Term Memory (LSTM) network, but it is unclear whether they used the original LSTM or the LSTM with forget gates. They also mention an NTM-BFGS optimizer, which uses external memory similar to the Neural Turing Machine. The experiment with quadratic functions is reminiscent of the experiments with quadratic functions in Hochreiter et al.'s 2001 paper, and a direct comparison to the 2001 system would be beneficial.
In conclusion, the authors have shown how to cast the design of optimization algorithms as a learning problem, enabling the training of optimizers specialized to particular classes of functions. However, this was already demonstrated by previous work up to 2001. The novelty of this paper lies in the explicit bias towards gradient-based learning algorithms, which is probably a good bias for the experiments presented and should be emphasized. The additional experiments are intriguing, and the results are promising. This paper should be published, provided the comments above are addressed, and I would like to see the revised version again.