This paper studies the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. The authors introduce a new parameter Λ, which measures the total statistical variance of the loss distributions over T rounds of the process, and investigate how this amount affects the regret. They also examine the interaction between Λ and other parameters, including Γ, which counts the number of times the distributions change, and V, which measures how far the distributions deviate over time.
The paper provides a comprehensive analysis of the regret landscape in terms of the parameters Λ, Γ, V, and T, in both full-information and bandit settings. The authors propose several algorithms with upper bound guarantees and prove their matching lower bounds. The results show that even when Γ, V, and Λ are all restricted to constant, the regret lower bound in the bandit setting still grows with T, while a constant regret becomes achievable with constant Γ and Λ in the full-information setting.
The strengths of the paper include its thorough analysis of the problem, the introduction of a new parameter Λ, and the provision of both upper and lower bounds for the regret. The authors also propose a parameter-free algorithm, which may have independent interest of its own. The paper is well-organized, and the writing is clear and concise.
However, there are some weaknesses in the paper. The analysis is quite technical, and some of the proofs are lengthy and involved. The authors could have provided more intuition and explanations for the results, especially for the non-expert readers. Additionally, the paper could have benefited from more discussions on the implications of the results and potential future directions.
Here is a list of arguments pro and con acceptance:
Pro:
* The paper provides a comprehensive analysis of the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments.
* The introduction of the new parameter Λ is a significant contribution to the field.
* The authors provide both upper and lower bounds for the regret, which gives a complete picture of the problem.
* The paper is well-organized, and the writing is clear and concise.
Con:
* The analysis is quite technical, and some of the proofs are lengthy and involved.
* The authors could have provided more intuition and explanations for the results.
* The paper could have benefited from more discussions on the implications of the results and potential future directions.
* Some of the notation and terminology may be unfamiliar to non-expert readers.
Overall, I would recommend accepting the paper, as it provides a significant contribution to the field and a comprehensive analysis of the problem. However, the authors could have improved the paper by providing more intuition and explanations for the results and discussing the implications of the results and potential future directions. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, the analysis is quite technical, and some of the proofs are lengthy and involved.
Clarity: 8/10
The paper is well-organized, and the writing is clear and concise. However, some of the notation and terminology may be unfamiliar to non-expert readers, and the authors could have provided more intuition and explanations for the results.
Originality: 9/10
The introduction of the new parameter Λ is a significant contribution to the field, and the authors provide a comprehensive analysis of the problem.
Significance: 9/10
The paper provides a complete picture of the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments, which is a significant contribution to the field. The results have important implications for the design of online learning algorithms in non-stationary environments.