This paper proposes a novel method called Structured Sparsity Learning (SSL) to regularize the structures of Deep Neural Networks (DNNs), including filters, channels, filter shapes, and layer depth. The authors aim to reduce the computation cost of DNNs while maintaining or even improving their accuracy. The proposed method is based on group Lasso regularization, which can adaptively adjust multiple structures in DNNs.
The paper is well-written and clearly explains the motivation, methodology, and experimental results. The authors provide a thorough review of related work, including connection pruning, weight sparsifying, low rank approximation, and model structure learning. They also discuss the limitations of existing methods and highlight the advantages of their proposed approach.
The experimental results demonstrate the effectiveness of SSL in reducing computation cost and improving accuracy. The authors evaluate their method on several benchmark datasets, including MNIST, CIFAR-10, and ImageNet, and compare it with other state-of-the-art methods. The results show that SSL can achieve significant speedups on both CPU and GPU platforms, with an average of 5.1× and 3.1× layer-wise acceleration, respectively.
The strengths of this paper include:
* The proposed method is novel and well-motivated, addressing a significant problem in deep learning.
* The experimental results are thorough and well-presented, demonstrating the effectiveness of SSL.
* The authors provide a clear and concise explanation of the methodology and related work.
The weaknesses of this paper include:
* The paper could benefit from more detailed analysis of the computational complexity and memory requirements of SSL.
* The authors could provide more insights into the trade-offs between computation cost, accuracy, and model complexity.
* Some of the figures and tables could be improved for better readability and clarity.
Overall, this paper is well-written and presents a significant contribution to the field of deep learning. The proposed method has the potential to improve the efficiency and accuracy of DNNs, making it a valuable contribution to the community.
Arguments pro acceptance:
* The paper proposes a novel and well-motivated method for reducing computation cost and improving accuracy in DNNs.
* The experimental results demonstrate the effectiveness of SSL on several benchmark datasets.
* The authors provide a clear and concise explanation of the methodology and related work.
Arguments con acceptance:
* The paper could benefit from more detailed analysis of the computational complexity and memory requirements of SSL.
* The authors could provide more insights into the trade-offs between computation cost, accuracy, and model complexity.
* Some of the figures and tables could be improved for better readability and clarity.
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above and provide more detailed analysis and insights into the proposed method. Additionally, they should improve the readability and clarity of the figures and tables.