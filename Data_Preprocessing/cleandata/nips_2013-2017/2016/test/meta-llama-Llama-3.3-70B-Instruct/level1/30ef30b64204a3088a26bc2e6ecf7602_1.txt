This paper proposes a novel unsupervised loss function that takes advantage of the stochastic nature of techniques such as randomized data augmentation, dropout, and random max-pooling in convolutional neural networks (ConvNets). The loss function, called transformation/stability loss, minimizes the difference between the predictions of multiple passes of a training sample through the network. The authors also combine this loss function with a mutual-exclusivity loss function, which forces the classifier's prediction vector to have only one non-zero element.
The paper is well-written and clearly explains the motivation behind the proposed loss function. The authors provide a thorough review of related work in semi-supervised learning, including techniques such as self-training, co-training, and generative models. They also discuss recent works in semi-supervised deep learning, including ladder networks and predictive sparse decomposition.
The experiments are extensive and well-designed, covering multiple benchmark datasets such as MNIST, SVHN, NORB, CIFAR10, CIFAR100, and ImageNet. The results show that the proposed loss function can significantly improve the accuracy of ConvNets, especially when the number of labeled samples is small. The authors also demonstrate that the combination of transformation/stability loss and mutual-exclusivity loss can lead to further improvements in accuracy.
The strengths of the paper include:
* The proposed loss function is novel and takes advantage of the stochastic nature of techniques such as dropout and randomized data augmentation.
* The experiments are extensive and well-designed, covering multiple benchmark datasets.
* The results show significant improvements in accuracy, especially when the number of labeled samples is small.
The weaknesses of the paper include:
* The paper could benefit from a more detailed analysis of the computational cost of the proposed loss function, especially when used with large datasets.
* The authors could provide more insights into why the combination of transformation/stability loss and mutual-exclusivity loss leads to further improvements in accuracy.
Overall, the paper is well-written and makes a significant contribution to the field of semi-supervised learning. The proposed loss function is novel and has the potential to improve the accuracy of ConvNets, especially in scenarios where labeled data is scarce.
Arguments pro acceptance:
* The paper proposes a novel and effective loss function that can improve the accuracy of ConvNets.
* The experiments are extensive and well-designed, covering multiple benchmark datasets.
* The results show significant improvements in accuracy, especially when the number of labeled samples is small.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the computational cost of the proposed loss function.
* The authors could provide more insights into why the combination of transformation/stability loss and mutual-exclusivity loss leads to further improvements in accuracy.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Overall score: 8.5/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, including providing a more detailed analysis of the computational cost and insights into why the combination of loss functions leads to further improvements in accuracy.