This paper presents a systematic analysis of the connecting architectures of recurrent neural networks (RNNs) and proposes three architecture complexity measures: recurrent depth, feedforward depth, and recurrent skip coefficient. The authors introduce a general formulation of RNN architectures using a graph-theoretic framework, which provides a solid foundation for analyzing the architectural complexity of RNNs.
The paper's main strengths are:
1. Rigorous framework: The authors provide a rigorous graph-theoretic framework for analyzing RNN architectures, which is a significant contribution to the field.
2. Novel complexity measures: The proposed complexity measures (recurrent depth, feedforward depth, and recurrent skip coefficient) provide new insights into the architectural complexity of RNNs and can guide the design of new recurrent architectures.
3. Empirical evaluations: The authors conduct extensive experiments to validate the usefulness of the proposed measures and demonstrate their effectiveness in improving model performance on various tasks.
However, the paper also has some weaknesses:
1. Complexity of the framework: The graph-theoretic framework, while rigorous, may be challenging for some readers to follow, particularly those without a strong background in graph theory.
2. Limited analysis of optimization issues: The authors mention optimization issues as a potential limitation of increasing recurrent depth or feedforward depth, but a more detailed analysis of these issues would be beneficial.
3. Comparisons to existing work: While the authors compare their results to some existing work, a more comprehensive comparison to other relevant studies would strengthen the paper.
Arguments for acceptance:
* The paper presents a novel and rigorous framework for analyzing RNN architectures.
* The proposed complexity measures provide new insights into the architectural complexity of RNNs.
* The empirical evaluations demonstrate the effectiveness of the proposed measures in improving model performance.
Arguments against acceptance:
* The paper's framework may be challenging for some readers to follow.
* The analysis of optimization issues is limited.
* The comparisons to existing work could be more comprehensive.
Overall, I believe that the paper's strengths outweigh its weaknesses, and it makes a significant contribution to the field of RNNs. With some revisions to address the weaknesses, the paper has the potential to be a high-quality contribution to the conference.