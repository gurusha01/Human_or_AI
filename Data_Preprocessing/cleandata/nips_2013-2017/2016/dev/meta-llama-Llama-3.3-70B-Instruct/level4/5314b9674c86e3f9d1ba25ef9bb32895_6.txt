This manuscript presents an innovative algorithm for image classification, leveraging tensor networks to optimize supervised learning tasks through matrix product states. The approach involves mapping large vectors X to a higher-dimensional space using a feature map Φ(x), followed by classification using a decision function f(x) = W × Φ(x). To address the potential exponential size or infinity of feature vector Φ(x) and weight vector W, the authors introduce a novel method distinct from the kernel trick (reference 9), utilizing a tensor network to approximate the optimal weight vector W (Eq. 5). By extracting hidden information from the trained model and exploiting the structure of W, a "sweeping" optimization algorithm (Eq. 7) is developed to minimize a quadratic cost function defining the classification task. The paper effectively demonstrates the adaptation of tensor networks optimization algorithms to supervised learning tasks. Initially, large vectors X are transformed into a higher-dimensional space via the feature map Φ(x), and then classified using the decision function. Since Φ(x) and W can be extremely large, the authors propose an alternative approach to the kernel trick, employing a tensor network to approximate W and developing an optimization algorithm to minimize the classification cost function. Overall, the paper is well-structured and clearly written, offering a novel contribution to machine learning by bridging tensor networks and feature selection. However, a significant limitation lies in the experimental section, where the approach is only tested on the MNIST dataset with a less than 1% test set classification error. It would be beneficial for the authors to apply their method to other widely used datasets and compare it with state-of-the-art learning algorithms and feature selection methods to assess its performance comprehensively. Clarification of these concerns during the rebuttal process would be appreciated. In my opinion, the paper warrants a weak accept rating.