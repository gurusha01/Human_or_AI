This paper proposes a novel approach to identifying subsets of causal sources by introducing a measure to quantify submodularity, enabling the application of greedy algorithms to this problem. The authors derive the submodularity index (SmI), which provides bounds for a random greedy algorithm as a function of the SmI, thus generalizing the established approach of submodular function maximization. The work explores source detection and causal covariate selection using the information-theoretic measure directed information (DI), demonstrating that the first problem is submodular but not monotonic, while the second is nearly submodular according to the SmI. Theoretical results are validated using datasets generated by DBN's Murphy's Bayes net toolbox, revealing the underlying causal structure. 
The paper covers a range of fundamental and relevant topics, yielding potentially interesting results for each. However, its broad scope diminishes the overall impact, as the contribution of the work as a whole appears incoherent. The notion of SmI, particularly the implications of Theorem 3, has significant relevance to a broad class of NP-hard problems but is restricted to causality analysis. Although causality analysis is a substantial field, there is insufficient examination of the fundamental properties of SmI before its application, making the paper seem focused on causal subset selection instead. 
The broad scope of the paper affects its depth, making it challenging to understand based solely on the material presented. The first five pages introduce fundamental results on optimizing (nearly) submodular set functions, but the proofs are not included in the body of the paper. These proofs are crucial to the paper's results and should be more prominent. It might be beneficial to condense the analysis of the causal subset selection component to accommodate this; there doesn't seem to be room for both within a single conference paper.
Minor comments and questions include considering transfer entropy as a measure of predictability, for which similar results for the SmI are anticipated. Directed information was originally intended to capture feedback structures, according to Massey, and reference [2] provides a good comparison between transfer entropy and directed information in certain situations. References for directed information and causally conditioned entropy in Section 2 are needed. Additionally, several minor errors and inconsistencies in formatting, grammar, and references were noted, including the need for a definition of monotonicity, potentially in the supplementary materials. The conditions under which the maximum DI finds the underlying graph structure should be clarified, assumed to be for fully observed DBNs.