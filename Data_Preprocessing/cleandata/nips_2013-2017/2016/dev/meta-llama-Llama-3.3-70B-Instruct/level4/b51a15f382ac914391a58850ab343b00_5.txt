This paper introduces two algorithms for the emerging field of combinatorial partial monitoring (CPM), which integrates finite partial monitoring and combinatorial multi-armed bandit. It presents two algorithms that operate against a stochastic adversary, providing both distribution-dependent and distribution-independent theoretical regret bounds for each. The first algorithm, PEGE, effectively combines forced exploration from multi-armed bandit (MAB) problems with a globally observable set to address CPM. The second algorithm, PEGE2, initially estimates the gap Î” between the expected rewards of the best and second-best actions, then utilizes this estimate in PEGE. The paper also explores the application of this setting to online ranking with feedback. A significant contribution of the paper is the development of an algorithm that eliminates the need for a unique optimal action assumption while maintaining state-of-the-art regret bounds up to logarithmic factors. The setting and algorithms presented are noteworthy, with the PEGE algorithm being particularly commendable for removing the assumption of a unique optimal action. Two points warrant further clarification: firstly, it would be beneficial to intuitively understand why setting T_0 to O(T^{2/3}) in PEGE2 yields a problem-dependent bound of O(log T); secondly, incorporating a discussion on this intuition within the paper would enhance its clarity and readability.