This paper presents an algorithm for learning behavior based on long-term goals, with a specific application to predicting player movement in basketball games. The core concept involves training a hierarchical model that combines a "micro-planner" (utilizing only the current state as input) with a "macro-planner" (considering the estimated long-term goal) to predict low-level actions. The estimation of long-term goals relies on initial seeding with heuristically-defined targets, such as stationary points in basketball where players stop moving. The primary experimental evaluation compares the quality of completed basketball player traces generated by the proposed model with those produced by baseline methods, as assessed by human judges. The idea of learning higher-level goal abstractions to drive agent behavior is highly relevant, and the proposed approach appears to be novel. However, the "Related Work" section is underdeveloped, lacking sufficient connections to previous research in hierarchical reinforcement learning, goal recognition, and trajectory planning.
A significant issue with this submission is its overall lack of clarity. Upon initial reading, the paper was difficult to follow, and certain points only became clear towards the end, necessitating a re-read to fully understand the content. For instance, it was not until equations 6-7 (page 4) that it became apparent that the method employed supervised learning, as the mentions of planning and reinforcement learning had caused confusion. The terminology used, such as referring to the basic state-to-action predictor as a micro-"planner," seems inappropriate given that it does not involve any look-ahead into the future.
Several aspects of the paper require clarification, including:
- Notations: The distinction between scalars, vectors, and matrices is not clearly defined. The difference between bold and non-bold notations (e.g., st vs. st on line 83 and the confusing explanation on line 160) needs explanation. The role of mg as a probability on "m" in equation 2 but as a function of "a" in equation 4 is unclear. The presence of at in two probabilities in equation 3 is confusing, and the equation's usage in the model is not well-explained. The index i on line 133 (presumably referring to a player, but lacking clarity) and its sudden appearance on line 142 without prior definition in the equations are points of confusion. The multiplication by Ã¢_t in equation 7 is also confusing, although its meaning became clearer upon further reading.
- The assumption that macro-goals are relatively static is not clearly justified.
- The relevance of equation 6 is questionable, as fine-tuning is reportedly done on equation 7 only.
Beyond these clarity issues, there are concerns regarding the methodology. The predictor for the attention mask m seems to only consider the predicted macro-goal g as input, which could lead to the same prediction for a given g regardless of the player's position. This oversight is particularly significant for goals that may be approached from different sides, requiring distinct movements. During extrapolation, the generated player trajectory may not align with the original seeding sequence, potentially resulting in unrealistic game scenarios that were not encountered during training. This limitation should be acknowledged, and it would be beneficial to explore how input data beyond the considered player's trace affects the model's output.
Minor points and typos include:
- Line 96: "should simultaneously optimizes" should be corrected.
- Line 176: "we used an simultaneously predicted the next 4 micro-actions" contains a typo.
- Figure 5 caption: The suggestion that additional game state information could fix the depicted situation is unclear.
- Figure 6 caption: "bottom-right" should be corrected to "bottom-left."
- Line 222: "We now inspect this our macro-planner" contains a typo.
- The Conclusion section is brief and could potentially be merged with Section 6 for greater impact.
- The statement "sub-sampling temporally at 4 Hz" implies keeping one frame out of four, which would result in a 25/4 Hz sampling rate.
Following the author's feedback, it is acknowledged that conditioning the mask on the state may not be beneficial in the specific case of this basketball dataset. However, in general, this approach could be valuable. The overall impression, after considering other reviews and the author's response, is that the algorithm needs to be presented more clearly and motivated, with a more thorough evaluation against simpler or previously proposed methods. The willingness to make the data public is appreciated, but this also underscores the importance of establishing clear, quantitative benchmarks for comparison.