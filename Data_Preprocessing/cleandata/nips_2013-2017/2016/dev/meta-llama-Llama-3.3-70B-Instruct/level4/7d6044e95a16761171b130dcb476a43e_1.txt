The authors propose a novel class of probabilistic models that leverage the complementary strengths of graphical models and neural networks, integrating structured prior distributions from graphical models with nonlinear observation models based on neural networks. This integration aims to merge the benefits of graphical models, including interpretability and efficient inference, with the powerful representation learning capabilities of neural networks. A key contribution of this paper is the development of an efficient stochastic variational inference algorithm designed for training these hybrid models. This algorithm utilizes a neural network-based recognition model to address the non-conjugate observation model, facilitating efficient mean-field updates for the inference of local latent variables. Although the experimental section is limited, presenting qualitative outcomes on a synthetic dataset and a small, low-resolution video dataset, it introduces an intriguing concept - employing a neural recognition model to induce conjugacy in a surrogate objective for inferring local latent variables. This approach, while powerful, is less direct compared to methods like variational autoencoders (VAEs), where the recognition model directly generates parameters of the variational posterior. A comparison of these approaches, discussing their strengths and weaknesses, would have been beneficial. The paper is well-written but dense, with some sections lacking detail, making the supplementary material crucial for understanding certain aspects. Despite the experimental section's brevity and lack of quantitative results, the paper's substantial conceptual contribution outweighs this limitation. However, the related work section omits several recent sequence modeling studies within the VAE framework and notably misses the work by Titsias and Lazaro-Gredilla on Doubly Stochastic Variational Bayes for non-Conjugate Inference.