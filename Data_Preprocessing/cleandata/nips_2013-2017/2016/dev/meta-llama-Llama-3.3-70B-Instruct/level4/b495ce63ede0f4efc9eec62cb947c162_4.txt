This paper presents a novel approach to discovering visual correspondences, deviating from traditional patch similarity-based methods. The authors introduce a new model that leverages dense feature vectors, a correspondence contrastive loss function combining mean squared loss and margin-based ranking loss, and a CNN Spatial Transformer. This methodology effectively captures both geometric and semantic correspondences. The proposal of a universal correspondence network utilizing CNN and pair-wise ranking loss is a notable contribution. The paper is well-structured and clear. 
The following points require further clarification:
1. The loss function defined in Equation (1) utilizes $si/(1-si)$ as an identity function to select the applicable part of the loss function. The authors denote $s = 0$ for positive pairs and $s = 1$ for negative pairs in Line 121, implying that positive pairs are trained using margin-based ranking loss, while negative pairs are trained using mean squared loss. However, this seems to contradict the statement in Line 132, which suggests that mean squared loss minimizes the distance between positive pairs, and ranking loss ensures negative pairs are at least $m$ distance apart.
2. Given that key points are pre-selected in this work, it is worth exploring whether the algorithm can function without predefined key points, allowing the model to autonomously decide. Essentially, this would involve ranking all vectors in the right image based on a dense feature vector from the left image. It is also important to consider how training with hard negative pairs might impact performance in such a scenario, as many vectors may not be updated during training.
3. The incorporation of a spatial transformer to mimic path normalization is shown to enhance model performance. It would be beneficial to understand how this differs from using shortcuts and what advantages the spatial transformer offers over shortcuts.
4. Figure 5 displays the accuracy of PCK performance but lacks precision and recall metrics. If accuracy is calculated as the number of actual correspondent pixels divided by a certain pixel threshold $k$, it would be insightful to examine the recall curve to gain a more comprehensive understanding of the model's performance.