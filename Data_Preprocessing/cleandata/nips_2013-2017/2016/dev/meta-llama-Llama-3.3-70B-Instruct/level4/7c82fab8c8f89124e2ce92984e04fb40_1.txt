The manuscript presents a novel approach for constructing confidence intervals and p-values for the selection of groups of covariates, which is applicable to a wide range of methods including forward selection, iterative hard-thresholding, and group lasso. A key challenge addressed in the paper is accounting for the fact that the same data are used for both group selection and statistical assessment, which necessitates a nuanced approach to evaluating the importance of these groups. The paper's originality stems from its ability to derive confidence intervals, whereas previous methods were limited to calculating p-values for groups of covariates. This achievement is rooted in a crucial lemma that describes the distribution of truncated projections of Gaussians. 
The manuscript includes two small-scale experiments, one on synthetic data and the other on real-world data, to demonstrate the approach's efficacy. Overall, the paper is well-written, clearly structured, and provides a comprehensive introduction to the problem, effectively highlighting the new challenges and resulting contributions. However, there are a few areas where additional details would enhance clarity and understanding. 
From a technical standpoint, the paper appears sound, with Lemma 1 and Theorem 1 representing significant contributions. Nonetheless, certain aspects of the proofs require clarification to ensure completeness and accuracy. The experimental section, while a good start, feels somewhat underwhelming. For instance, the purpose and takeaway of the second experiment in Section 4.2 are not immediately clear. Furthermore, comparing the proposed method with other approaches, such as the naive strategy of splitting the data, would provide valuable insights into its performance and efficiency, especially considering the main challenge of working with the same data for group selection and inference.
Several detailed comments are noteworthy: 
- Early discussion on whether the groups are assumed to be non-overlapping (forming a partition) would be beneficial.
- Exploring the possibility of extending the approach beyond the quadratic loss or Gaussian model could offer interesting avenues for future research.
- Condition (1) warrants a more detailed explanation to justify its consideration.
- Theorem 1 would benefit from additional details regarding the conditioned quantities and the random variables with respect to which probabilities are considered.
- The numerical solution to the equation \(\hat{fY}(L\alpha) = \alpha\) (line 252) needs clarification.
- A discussion on the computational complexity of the approach across different scenarios (e.g., IHT, group lasso) would be informative.
- The second experiment's objective and conclusions are obscure and require further elucidation.
- The potential application of the proposed methodology to techniques like those in [a] and its comparison with a simple bootstrap operation, as in [b] but for group-sparse estimators, are intriguing questions.
- In the supplementary material, the proof of Theorem 1 should also consider the continuity of \(t \rightarrow fy(t)\) to ensure the existence of \(L\alpha\), and more details on the proofs of \(f_y\)'s properties would be appreciated.
References:
[a] Ndiaye, E.; Fercoq, O.; Gramfort, A. & Salmon, J. GAP Safe screening rules for sparse multi-task and multi-class models. Advances in Neural Information Processing Systems, 2015, 811-819.
[b] Bach, F. Bolasso: model consistent Lasso estimation through the bootstrap. Proceedings of the International Conference on Machine Learning (ICML), 2008.