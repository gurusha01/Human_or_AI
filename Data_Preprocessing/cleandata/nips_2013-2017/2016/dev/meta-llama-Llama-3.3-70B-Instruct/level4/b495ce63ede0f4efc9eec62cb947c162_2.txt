This manuscript introduces a deep learning-based approach for establishing dense correspondences between pairs of natural images. The proposed framework learns correspondence maps directly from the data, enabling its application to both geometric correspondences, such as those resulting from pose changes, and semantic correspondences, like matching body regions across different species of birds. Key innovations over existing methods include the implementation of a correspondence contrastive loss for efficient training, the utilization of a convolutional spatial transformer for local patch normalization, and the incorporation of a K-nearest neighbor layer for efficient correspondence search between feature vectors extracted from two images. The paper is well-structured, and the experiments, which yield state-of-the-art results on several relevant datasets, are thoroughly conducted. Although there are aspects that could be explored further, the manuscript contains sufficient substance to warrant consideration for an oral presentation at NIPS. 
Several points merit further consideration: 
[1] The argument regarding the computational complexity, specifically the comparison between O(n) and O(n^2) forward passes, requires clarification. The distinction lies in whether these passes are over image patches, as in related works, or over the entire image, as in this paper. Additionally, the KNN layer performs O(mn) comparisons, suggesting a need for a more nuanced analysis of computational efficiency. Even in related works, features can be computed once per patch, and decision layers can be applied to all pairs, potentially mitigating the complexity difference. While the proposed framework may offer practical computational advantages due to its fully convolutional nature and implicit bookkeeping, the theoretical complexity benefit is not entirely clear.
[2] The paper would benefit from a discussion on how the framework handles occlusions. Specifically, how the existing loss function addresses situations where a point in one image has no correspondence in the other due to occlusion. The current evaluation metric, PCK curves, only penalizes mis-registrations, potentially overlooking occlusion issues that could be addressed by selecting an appropriate threshold.
[3] A relevant reference that could enhance the discussion is "Learning Dense Correspondence via 3D-guided Cycle Consistency" from CVPR 2016, available on arXiv.