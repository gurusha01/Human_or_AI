This paper proposes a multimodal learning approach based on residual learning to tackle the visual question answering problem, yielding state-of-the-art results on VQA benchmarks. A notable aspect of this work is the utilization of backpropagation to derive attention representations without explicitly defining attention parameters, effectively enabling the visualization of VQA attention mechanisms. To enhance the manuscript's accessibility for readers unfamiliar with VQA, incorporating background references would be beneficial. Given the growing popularity of VQA, the detailed information in section 4.1 could be condensed, as it occupies a significant amount of space. Additional clarification on the TrimZero operation, mentioned in lines 136-137, would also be helpful. Furthermore, the update rule v = v + 1 in section 4.2, related to postprocessing, requires more explanation for clarity. Nonetheless, the paper presents impressive results, demonstrating its contribution to the field.