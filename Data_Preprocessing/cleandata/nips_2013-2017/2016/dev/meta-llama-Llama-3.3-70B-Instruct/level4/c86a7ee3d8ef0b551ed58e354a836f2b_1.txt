This manuscript explores the adaptation of step-size in Stochastic Gradient Descent (SGD) and its variants, proposing the utilization of the Barzilai-Borwein (BB) method for automatic step-size computation in SGD and stochastic variance reduced gradient (SVRG), as an alternative to fixed or predefined decreasing schemes. The approach also incorporates a smoothing technique for SGD. The topic addressed is significant for SGD-type algorithms. Initially, the BB method is applied within SVRG, with simulations demonstrating the learning of optimal step-size following an adaptation phase. However, the notable overshoot to excessively small step-sizes in the initial iterations, as seen in Figure 1, appears suboptimal and warrants clarification. For SGD implementation, a smoothing technique is introduced, but it raises concerns as it seems to reintroduce a deterministic, non-adaptive decrease, specifically a decrease in 1/(k+1), which may undermine the adaptability of the proposed scheme, mirroring the drawbacks of predefined schemes. Additionally, a technical note: in Lemma 1, the expectation should be conditional to maintain accuracy.