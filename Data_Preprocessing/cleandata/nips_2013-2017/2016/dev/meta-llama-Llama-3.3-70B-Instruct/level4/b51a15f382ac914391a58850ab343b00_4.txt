This paper provides an in-depth examination of combinatorial partial monitoring games, where a learner chooses actions from a vast set and an adversary selects moves from a continuous, bounded space. The authors propose two novel algorithms, Phased Exploration with Greedy Exploitation (PEGE) and its variant PEGE2, drawing inspiration from forced exploration techniques commonly employed in bandit literature. Notably, PEGE does not require prior knowledge of the time horizon, and the authors derive both distribution-dependent and independent upper bounds for this algorithm. PEGE2 aims to estimate the gap between the optimal and second-best actions, yielding improved distribution-dependent bounds. The authors demonstrate that PEGE2 achieves the same upper bound as the state-of-the-art algorithm, GCB, for combinatorial partial monitoring, effectively removing the dependence on the learner's action space size. Furthermore, the paper explores potential applications of the algorithmic framework to online ranking problems with limited feedback. The writing is clear and concise, making the paper easy to follow, with a well-defined model and significant technical contributions, including the PEGE and PEGE2 algorithms and their regret upper bound analyses. The authors provide thorough comparisons with GCB, highlighting the advantages and disadvantages of their proposed algorithms. However, the absence of experimental results to demonstrate the practical performance of PEGE and PEGE2 relative to GCB is a notable omission. While the theoretical benefits of computing a single argmax are well-motivated, experimental evidence showing the regret behavior of PEGE and PEGE2 compared to GCB would have been valuable. The paper concludes with a discussion on applying the algorithms to real-world problems, such as online ranking with top feedback, which is motivated by user privacy concerns, although this connection could be more clearly elucidated. After considering the rebuttal, my assessment remains unchanged.