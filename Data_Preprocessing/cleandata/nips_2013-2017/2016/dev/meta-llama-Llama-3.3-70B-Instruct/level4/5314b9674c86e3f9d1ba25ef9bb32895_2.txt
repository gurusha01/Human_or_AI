This manuscript explores the application of tensor networks to supervised learning tasks, demonstrating how they can be leveraged to generate informative data representations and tackle classification problems. The core concept involves constructing data feature maps as higher-order tensors through the tensor product of local features. By representing model parameters as a tensor network, classification problems can be solved using a matrix-product state (MPS) decomposition, with an accompanying optimization algorithm described for finding this decomposition. An experimental validation is provided using the MNIST digit dataset, followed by a discussion on the class of functions within which the solution is sought. The idea of utilizing tensor networks for machine learning is intriguing, but several critical issues hinder the manuscript's publication in its current form. Key concerns include: 
1. Certain sections of the paper are challenging to comprehend, partly due to the adaptation of tensor network concepts from the physics community to machine learning problems. While the authors effectively illustrate the idea with numerous figures, providing more context on tensor networks, MPS decomposition algorithms, and their application in physics (including a brief overview of relevant physics journals) would enhance readability and accessibility for a machine learning audience. A dedicated section for this background information would be beneficial.
2. The notation is dense and obstructs understanding. For instance, the relationship between x and xj in equation 2 is unclear, as is the definition of N (whether it represents the number of attributes). In equation 3, the indexing of \phi by sj despite its dependence solely on xj is confusing, and it is unclear if \phi remains constant across different sj. Furthermore, A in equation 5 lacks definition, and there are typographical errors, such as the reference to 'labeled data' in line 68, which should be 'unlabeled data' from a given training set.
3. The experimental section is underwhelming, relying solely on the MNIST digit dataset. A more comprehensive assessment of the method's performance would require the inclusion of additional datasets. Moreover, crucial details about the experimental setup, such as the division of the dataset into training and testing sets, are missing.
4. The selection of the local feature map appears pivotal, yet the criteria for choosing this map are not clearly outlined. The feature map presented in equation 3 seems tailored for image data, raising questions about its applicability to other data types. Similarly, the parameters d and m lack clear guidelines for selection.
5. The uniqueness of the MPS decomposition is a concern; if it is not unique, how does this impact the learning process?
6. Previous work, such as [4], has utilized MPS decomposition for feature extraction and classification. The novel contributions of the current manuscript compared to [4] need to be explicitly stated, as the brief mention in line 210 is insufficient for discerning the differences between the two works.
7. Finally, exploring the connection between the function space obtained through ANOVA kernels (which are based on the product of local kernels) and the tensor network representation proposed in this work could yield interesting insights, given that both involve constructions based on tensor products.