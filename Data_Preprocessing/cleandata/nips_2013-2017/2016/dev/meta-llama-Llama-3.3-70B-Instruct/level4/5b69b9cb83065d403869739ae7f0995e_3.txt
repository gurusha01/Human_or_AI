This paper presents a novel approach to jointly training two machine translation systems using monolingual data, framed as a communication game that bears resemblance to the concept of autoencoders. The method involves training models of p(X|Y) and p(Y|X) jointly on independent samples from X (or Y), effectively learning to predict a sample X from itself through the composition of p(X|Y) and p(Y|X), where Y can be viewed as a representation of X. Although the technical concepts are straightforward, the idea is innovative and potentially useful. The experimental results indicate a promising direction, yet they appear preliminary and leave several questions unanswered. The technical ideas are sufficiently simple, making their repeated explanation in the abstract and on pages 2 and 3 somewhat redundant. Instead of reiterating the basic concept, the paper could be strengthened by providing a more comprehensive empirical analysis, including learning curves, the impact of initialization with varying amounts of data, and a detailed examination of the results. Specifically, it would be beneficial to investigate the aspects in which systems trained using this method demonstrate improvement, such as word choice, word order, or other factors, given that vocabulary is limited to 30K words.