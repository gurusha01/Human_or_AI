The authors present a novel architecture for deep generative models, designed to facilitate the learning of deeper models without compromising their trainability. The incorporation of lateral, shortcut, and residual connections within the network is thought to be the primary factor contributing to this improvement in trainability. The proposed architecture achieves state-of-the-art results on multiple benchmark datasets, demonstrating its efficacy. The authors provide a well-reasoned justification for their architecture, drawing on key findings from recent studies on deep recurrent networks and semi-supervised ladder networks. The reported results are noteworthy and impressive. However, while the paper's central focus is on the trainability of the proposed networks, the evidence supporting this claim is largely indirect, relying on performance metrics from various benchmarks. To further substantiate the trainability of these networks as a primary contribution, additional analysis or targeted experiments explicitly highlighting their trainability would be beneficial.