This paper explores the concept of causal subset selection using directed information, which can be viewed as maximizing a submodular or approximately submodular function for selecting features to use in prediction. A notable aspect of this work is the introduction of a novel definition of approximate submodularity and the resulting approximation outcomes. However, there are two primary concerns with the manuscript. Firstly, the discussion on utilizing directed information for causality is problematic due to the dual meaning of causality. The term "causality" can refer to either the prediction of a time series using other time series, respecting time and thus termed "causal," or it can pertain to counter-factuals and structural equations, which relate to understanding the underlying data generation mechanisms. The paper builds upon the former concept, akin to generalized Granger causality or prediction causality, which has been explored in recent works such as [20]. It is essential to clarify that the best time-series prediction does not provide insight into how modifying one time series affects another, highlighting the distinction between prediction and true causality. 
The most interesting results in the paper concern submodularity, particularly the approximation of near-submodular functions, with Granger causality serving as an application. The authors may benefit from emphasizing this aspect and potentially referencing relevant literature, such as the work by Pearl, Imbens, Rubin, and Richardson, for non-Granger causality. Theorem 2 presents an interesting finding, with a correct proof, while Lemma 1 (referred to as Lemma 2 in the appendix) is crucial in demonstrating the relaxation of submodularity, with a clear proof. Theorem 3 and its corollaries are the most compelling results, introducing a new relaxation of submodularity independent of causality or information metrics, which should be highlighted.
The second issue pertains to the work by Das and Kempe in [3], which defines a relaxation of submodularity known as the submodularity ratio. The current paper inaccurately states that [3] defined this concept solely for the R^2 score. In reality, [3] defines the submodularity ratio for an arbitrary set function and then specializes it for the R^2 score. Furthermore, the performance guarantee generalizing the 1-1/e result in Theorem 3.2 of [3] applies to any set function with a bounded submodularity ratio, although this is not adequately emphasized in [3]. A key question arises as to whether some of the results in this paper are direct corollaries of Theorem 3.2 in [3], given the similarity between the Submodularity index defined in this paper and the submodularity ratio in [3]. Both concepts essentially capture the same relationship between adding elements one at a time versus all at once, with submodularity implying a difference greater than zero (or a ratio greater than or equal to 1). The relaxation in [3] involves a submodularity ratio greater than or equal to a constant gamma, similar to how this paper's SMI difference is negative but close to zero, and the results seem to normalize SMI/f(S_greedy), suggesting a possible direct mapping from [3]'s results for general set functions. This analysis is complex, especially since [3] only addresses monotone functions, whereas this paper covers both monotone and non-monotone functions, potentially generalizing [3] beyond monotonicity.
Minor comments include correcting the author order in [3], clarifying the proofs of Propositions 2 and 3, and addressing numerous typos and grammatical errors throughout the manuscript, such as "Consider two random process X^n (processes)" which should be "Consider two random processes X^n," "directed information maximization-> maximization problems," "address in details-> in detail," "any polynomial algorithm-> polynomial-time algorithm," and the consistent use of "monotone" instead of "monotonic" for submodular functions. Despite these issues, the combinatorial optimization results are intriguing, and with the necessary revisions, the paper is recommended for acceptance.