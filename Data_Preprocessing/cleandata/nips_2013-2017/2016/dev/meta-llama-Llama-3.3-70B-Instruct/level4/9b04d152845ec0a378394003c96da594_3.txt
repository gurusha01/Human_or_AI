This manuscript presents a multimodal approach based on residual networks for visual question answering, enabling the learning of joint representations from both visual and linguistic data. The proposed method achieves state-of-the-art results on the dataset, validating its effectiveness. Additionally, the introduced visualization technique for attention effects in the joint representation is noteworthy. The paper is well-structured and readable, with the proposed concept supported by relevant experimental results. To further enhance the manuscript, it would be beneficial to compare the proposed method with recent publications in the field of Visual Question Answering, such as the ICML 2016 paper "Dynamic Memory Networks for Visual and Textual Question Answering", to provide a more comprehensive understanding of its contributions and advancements over existing techniques.