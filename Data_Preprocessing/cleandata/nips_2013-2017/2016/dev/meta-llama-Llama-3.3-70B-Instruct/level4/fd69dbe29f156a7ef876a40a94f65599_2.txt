This manuscript applies the "neural reasoning" framework to visual question answering, tackling the challenge posed by inherently ambiguous or difficult questions that lack context without visual information. The approach involves segmenting the image into candidate regions, effectively converting it into a sequence of "facts" analogous to the bAbI dataset setup, thereby enabling the application of the "neural reasoning" model. The evaluation on the COCO-QA dataset yields promising results, slightly surpassing those of current state-of-the-art models. However, on the VQA dataset, the performance is comparable to, yet slightly inferior to, other systems. The fundamental concept presented in this work is intriguing, and the authors appear to have successfully adapted a model originally designed for the somewhat artificial bAbI task to achieve robust performance on the more realistic VQA task. While the overall narrative is coherent, the empirical results do not entirely convince me of the model's purported advantages. For example, certain example questions exhibit significant ambiguity, raising questions about how many such questions are incorrectly answered by other models but correctly addressed by the proposed method. Furthermore, Figure 3 and its associated questions seem counterintuitive, given the inherent ambiguity of the question. The results on the newer VQA dataset, although less impressive than those on COCO-QA, warrant a more in-depth discussion comparing the proposed model to other state-of-the-art methods, particularly in the context of the VQA experiments, to elucidate the pros and cons of each approach.