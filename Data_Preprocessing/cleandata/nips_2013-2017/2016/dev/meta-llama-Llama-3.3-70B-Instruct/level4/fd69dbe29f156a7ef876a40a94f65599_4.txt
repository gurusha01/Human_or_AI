This manuscript explores the task of visual question answering (VQA) by leveraging a model derived from neural reasoners. The approach utilizes object detection, including spatial positions, as supporting facts to answer questions about images. Although the results on standard VQA test datasets show promise, they fall short of the performance achieved by current state-of-the-art methods. The paper is well-structured, and the experiments appear to be sound. The motivation behind using more directed and detailed questions is noteworthy, and the method's foundation in neural reasoning systems previously developed for text-based QA is clear. However, the authors do not introduce significant improvements or modifications to the reasoning system, instead applying objects in images as facts, similar to textual facts in traditional neural reasoning systems, which limits the novelty of the proposed method.
Furthermore, the reliance on object detection boxes as the primary source of facts may not be sufficient for answering questions in current datasets. This approach struggles with counting questions, as acknowledged by the authors, and may only be beneficial for questions directly related to objects. The contention that most questions are object-related is not entirely accurate for the VQA dataset, as many questions seek information about the surroundings of objects. The use of automated 'top-ranked' regions to generate object proposals, without consideration of the entities referred to in the question, restricts the knowledge source. The inclusion of the whole image as part of the 'facts' set makes it challenging to assess the impact of edge boxes. The absence of ablation studies to demonstrate the effectiveness of including facts in the reasoning system is notable, and such studies could be conducted by training a system without 'overall' image features, relying solely on object proposals.
The attention map, generated on top-ranked image regions without question knowledge, may not provide the necessary granularity for attending to specific parts of the image. The resolution offered by including the whole image is insufficient for detailed attention. The examples of question representation updates, while showing increased detail, do not convincingly demonstrate successful representation update. The use of a nearest neighbor approach to find matching questions in the dataset may not ensure that the updated representation asks about the same entity as the original question. The meaning of the question may change, despite the updated representation containing more details. The lack of analysis on failure cases and the exclusive use of COCO-QA dataset questions, which have notable limitations in terms of grammar and composition, further raises concerns about the method's efficacy.
In summary, while the paper presents an interesting application of reasoning engines to VQA, it lacks significant innovation in neural reasoning and may not provide adequate image-based facts for comprehensive reasoning. The general applicability of the work, in its current form, is limited, particularly considering the superior performance of similar methods in VQA. To strengthen the case, the visual facts should be more detailed and question-specific, and the efficacy of the proposed work should be demonstrated through ablation studies.