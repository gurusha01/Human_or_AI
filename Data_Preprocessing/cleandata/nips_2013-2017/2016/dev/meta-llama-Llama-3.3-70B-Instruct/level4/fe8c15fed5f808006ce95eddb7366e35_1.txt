This manuscript proposes a novel approach for learning hierarchical policy models from demonstrations, with a specific application to generating player behavior in basketball. The authors leverage deep neural networks in conjunction with an attention mechanism to integrate high-level objectives and micro-actions. The research goal is intriguing, but in its current state, the paper is challenging to comprehend and evaluate, particularly for reviewers like myself who are familiar with reinforcement learning (RL) but not experts in deep learning. The methodology section, specifically section 4, is difficult to follow due to undefined terms and notation, such as the variable L in Equation 5, which appears to represent loss but lacks a clear definition. Furthermore, the work lacks sufficient context within the existing literature and comparison to alternative approaches. Notably, relevant studies by George Konidaris and Sanjoy Krishnan on learning skills or macro-actions from demonstrations are not cited or compared. To strengthen the manuscript, I strongly encourage the authors to revise and resubmit, incorporating more detailed explanations, situating their work within the broader literature, and addressing related research, such as Konidaris et al.'s work on constructing skill trees from demonstrations and Niekum et al.'s research on learning complex tasks from unstructured demonstrations. Additionally, the authors should clarify the practical source of weak labels, as this aspect is not clearly addressed in the current version. References to relevant studies, including Han et al.'s work on learning compound multi-step controllers and Murali et al.'s research on unsupervised trajectory segmentation of multi-modal surgical demonstrations, could further enhance the manuscript's context and validity.