This paper presents an integration of the Barzilai-Borwein (BB) step-size into stochastic gradient descent variants, including SGD, SVRG, and SAG (as detailed in the appendix). The authors demonstrate that SVRG with the BB step-size achieves linear convergence in expectation for mu-strongly convex functions with Lipschitz continuous gradients, provided that the integer m, which determines the frequency of full gradient computations, is sufficiently large. Experimental findings indicate that the proposed SVRG with BB step-size tends to converge towards an optimally tuned step-size, resulting in a slightly lower but comparable convergence rate across three datasets. Similarly, other SGD variants with BB step-size exhibit promising results. Notably, SVRG-I, purportedly more efficient than SVRG-II in practice, requires tuning of two parameters (m and step-size eta) to attain linear convergence. The paper's key contribution lies in simplifying SVRG by introducing the BB step-size, thereby eliminating the need for step-size tuning as long as m is sufficiently large. However, further empirical analysis of m's sensitivity would enhance the paper. Theoretical results show a linear convergence rate of 1 - theta in terms of epochs, but fixing the convergence rate per epoch would linearly increase time complexity with respect to m, potentially making m's tuning as critical as step-size tuning, which may not be ideal.