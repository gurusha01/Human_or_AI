This manuscript integrates a residual-like network with multimodal fusion of linguistic and visual features to tackle the Visual QA task, yielding notable advancements on established benchmarks. Although the novelty of the approach is somewhat limited, its acceptance would facilitate the development of future research by building upon the current state-of-the-art outcomes. The authors propose a novel combination of residual-like architecture and multimodal fusion for visual question answering, with a key innovation being the utilization of multiplicative interactions to integrate visual features with word embeddings. The reported results are impressive, surpassing the current state-of-the-art by a significant margin. However, the extensive reliance on pre-trained models and embeddings warrants a more in-depth analysis of their effects on the overall performance. To enhance the manuscript, it would be beneficial to reproduce equations from referenced papers, rather than simply citing them, as seen in section 3.1. Furthermore, a question arises in section 5.2: what would be the outcome if sigmoid(W_q*q) were employed as the attentional mask for visualization purposes?