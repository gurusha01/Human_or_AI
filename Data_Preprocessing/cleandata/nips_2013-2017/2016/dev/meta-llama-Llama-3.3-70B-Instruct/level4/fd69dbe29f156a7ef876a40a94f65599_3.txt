This manuscript presents a neural network architecture for visual question answering, building upon the framework established in [21]. The core concept involves iteratively refining the question representation by selectively focusing on relevant image regions. Each input image is first encoded using a VGG model, from which features are extracted for 20 candidate regions per image. These image features are then transformed into a shared latent space with the question features. The question sequence is embedded into word vectors using an embedding matrix, and these vectors are fed into a GRU at each time step, with the final hidden state serving as the question representation, also transformed into the common latent space. The question representation is updated through multiple reasoning layers, utilizing a multilayer perceptron and weighted pooling, which incorporates an attention mechanism. The final answer is predicted via a softmax layer. The model demonstrates superior performance to state-of-the-art models on the COCO-QA dataset and comparable results on the VQA dataset. The paper is well-structured and clearly written, although it would benefit from a more detailed description of baseline models and other state-of-the-art approaches for context. The employment of the attention mechanism in weighted pooling likely contributes to the model's performance improvement, which is a reasonable approach. A valuable contribution of this work is the comparison of different question representation pooling mechanisms, an aspect not explored in [21]. Further clarification on the choice of using a single layer in the MLP and the third example question in Figure 3 would be beneficial, as the question representation appears unchanged after updating with one reasoning layer. A thorough analysis of the results would also enhance the paper. Despite being derived from [21], this study appears to be the first to adapt the model for the VQA task, showcasing the effective application of a text question answering model to visual question answering. Both this paper and [21] are noteworthy, as they open up new avenues for research in QA and VQA.