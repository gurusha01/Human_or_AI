This paper examines the drift-diffusion model (DDM), a widely-used approach for modeling two-alternative forced choice experiments in psychophysics, with a focus on determining optimal thresholds. The DDM is characterized by a random walk representing evidence accumulation, which terminates when it reaches an upper or lower threshold corresponding to the respective choices. The authors propose that these thresholds are set by solving a meta-optimality problem aimed at minimizing the "Bayes risk", a convex objective function derived from a loss function that linearly combines expected decision time and type I and II errors. This meta-optimization is framed as a continuous-armed bandit problem, with two strategies explored: one based on REINFORCE and the other on Bayesian optimization using a custom acquisition function. These strategies are compared empirically. The strengths of the paper include its intriguing research question, which has significant implications for psychophysics, and its clear mathematical and empirical presentations. However, several weaknesses are noted: the conceptual basis for using meta-reasoning to optimize DDM thresholds is not justified, and there are mathematical errors, such as inconsistencies in notation (e.g., equation (4) uses both "t" and "T"). Additionally, certain design choices lack justification, including the selection of the loss function for formulating the Bayes risk and the use of a linear combination of binary units in the bandit strategy. Furthermore, the findings are neither substantial nor surprising from either a machine learning or psychophysics perspective, as the models are relatively straightforward and lack comparison to empirical data from humans or monkeys. While the paper shows promise, its current state appears premature for publication.