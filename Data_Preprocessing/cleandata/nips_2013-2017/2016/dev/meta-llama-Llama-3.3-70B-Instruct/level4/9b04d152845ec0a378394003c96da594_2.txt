The authors have introduced a novel network architecture that adapts deep residual learning to accommodate multi-modal inputs, with a specific application to visual question-answering tasks. In contrast to prior research that employed explicit attention network mechanisms, this proposed method builds upon earlier work by utilizing element-wise multiplication between question and visual feature vectors to represent the joint residual function, effectively implementing an implicit attention mechanism. Through experimentation with various architectural variants, the authors ultimately settled on a relatively straightforward learning block configuration within a 3-block layered network. The synergy between deep residual learning and implicit attention yielded impressive results on the Visual QA dataset, surpassing existing state-of-the-art outcomes. To illustrate the attention effects of multi-modal residual learning, the authors devised a visualization technique that displays the disparity between visual inputs and joint residual mappings using back-propagation for each learning block. Examples from the visual QA task provided intuitive evidence of the implicit attention effect. By successfully integrating two potent concepts - deep residual learning and element-wise multiplication for implicit attention - the authors have created a versatile solution for general multi-modal tasks. The experiments were meticulously designed to optimize the architecture and hyper-parameters for the targeted Visual QA task, resulting in outstanding performance compared to previous studies leveraging diverse deep learning techniques. However, it would be beneficial for the authors to provide additional comparisons with existing techniques in terms of model parameter size and data requirements. Furthermore, a separate evaluation of the contributions of residual learning and implicit attention to the Visual QA task would be insightful, as it would help elucidate which aspect is most critical. The proposed visualization method was intuitively appealing, and the examples effectively demonstrated its utility in explaining the implicit attention mechanism. Additional clarification on the differences between the three images at each block layer would be helpful, as would an intuitive explanation for why a three-layer configuration appears optimal for this task.