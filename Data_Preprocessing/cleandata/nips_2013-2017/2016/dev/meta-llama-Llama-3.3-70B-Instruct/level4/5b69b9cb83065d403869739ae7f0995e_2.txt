This manuscript introduces a framework called "communication-based machine translation" (CMT), which enhances translation models for both translation directions by iteratively forth- and back-translating monolingual data and gathering feedback after each translation iteration. The primary objective is to effectively utilize monolingual data to improve machine translation systems based on bilingual corpora. The proposed model is evaluated on two language pairs and compared to a standard neural machine translation system as well as a system augmented with pseudo-bilingual data training. Similar goals have been explored in previous studies, such as incorporating a target-side language model for translation re-scoring or integrating it into the machine translation system through deep or shallow fusion. However, the manuscript falls short in comprehensively reviewing prior work in this area, primarily focusing on recent research by Sennrich et al. Given the extensive attention devoted to leveraging monolingual data for machine translation, more rigorous empirical comparisons are necessary to demonstrate the superiority of the proposed system. A straightforward comparison could have been facilitated by utilizing the same dataset as in previous studies. Furthermore, relevant work on unsupervised training of noisy-channel models should be acknowledged. The manuscript also lacks comparisons to other communication-based learning scenarios in reinforcement learning, instead repetitively describing the fundamental concept of two players communicating via translation models. The authors suggest that this model offers novel directions for learning translation models from scratch using monolingual data, but this claim is questionable given that previous studies, such as those by Ranzato et al. and Shen et al., have found that a warm-start model is required due to weak feedback. The assumption of initiating translations with "weak" models, as stated in line 116, contradicts the use of "well-trained" models, as mentioned in line 152. The only experiment that supports significant improvement with monolingual data and a "weak" model is the French-to-English translation task. Additionally, extending the model to multiple languages in a translation chain is not as straightforward as described, as the source of translation errors becomes more challenging to detect, potentially leading to noisy updates. Although the two-player game metaphor is intriguing, it does not align well with the actual algorithm, as the communication reward requires knowledge of the translation model, which is not available to the player in the described game. To validate the experiments and support the proposed model, more detailed information about the models, including the number of training instances (both monolingual and bilingual), should be provided, and more comprehensive comparisons should be made using original data from referenced work and state-of-the-art systems as upper bounds. In summary, the manuscript presents an interesting idea, but it lacks sufficient experimental comparisons to closely related work, making it difficult to accept. Relevant studies, such as those by Holger Schwenk, Gulcehre et al., Mylonakis et al., and Shen et al., should be more thoroughly reviewed and compared to the proposed model.