The authors present an automated step size adjustment scheme utilizing the Barzilai-Borwein method for Stochastic Gradient Descent (SGD) and Stochastic Variance Reduced Gradient (SVRG) algorithms. They provide a linear convergence analysis for SVRG-BB and SVRG-I, where the latter refers to option 1 of SVRG using the current iterate as the mark iterate in the next epoch. However, for SGD-BB to converge, a smoothing technique is necessary in conjunction with the Barzilai-Borwein step size, although no convergence analysis is offered for SGD-BB. The comparison of SVRG-BB to SVRG with a constant step size and SGD-BB to SGD with a decreasing step size scheme reveals that the Barzilai-Borwein step sizes achieve a comparable convergence rate to the best-tuned step size for both SVRG and SGD.
This paper tackles the crucial issue of automatically determining step size sequences for stochastic methods like SGD by leveraging the well-established Barzilai-Borwein method. While the results are promising, two significant limitations are noted: 
1. The absence of convergence rate analysis for SGD-BB is a notable omission. Although the authors acknowledge the necessity of a decreasing step size for SGD's convergence, a theoretical examination of how the Barzilai-Borwein step size influences SGD's convergence rate is essential. Furthermore, the optimization function employed for smoothing SGD step sizes appears heuristic and lacks theoretical justification.
2. The numerical experiments are somewhat limited, focusing solely on binary classification models. It would be beneficial to investigate the performance of the step sizes on regression problems and non-convex problems like neural networks. Additionally, a comparison of SGD-BB to other automatic step size methods for SGD is missing, including discussions on aspects such as computational speed and memory usage relative to existing methods.
The convergence analysis for SVRG-BB and SVRG-I, although straightforward, is interesting. However, applying a Barzilai-Borwein step size to SVRG may not be considered a substantial contribution, given that SVRG calculates true gradients after each epoch, making the integration of the Barzilai-Borwein step size relatively straightforward. 
In conclusion, while the paper demonstrates good results and addresses a critical problem in SGD, further work is necessary. Specifically, a convergence rate proof for SGD-BB and more extensive experiments would enhance the paper. Minor suggestions include:
- Correcting the typo in line 13 of the supplementary material (\eta -> \eta_k).
- Improving the bounds in line 9 of the supplementary material using techniques from the original SVRG paper.
- Investigating the sensitivity of SGD-BB to the parameter m.
- Exploring the initial low step sizes in SVRG-BB that lead to slightly slower convergence and determining if this can be mitigated to match the performance of the best-tuned step size for SVRG. 
References to related work, such as Mahsereci and Hennig (2015) and Schaul et al. (2013), highlight the existence of other methods for automatic step size adjustment, underscoring the need for a more comprehensive comparison and discussion of the Barzilai-Borwein method's advantages and limitations in the context of SGD and SVRG.