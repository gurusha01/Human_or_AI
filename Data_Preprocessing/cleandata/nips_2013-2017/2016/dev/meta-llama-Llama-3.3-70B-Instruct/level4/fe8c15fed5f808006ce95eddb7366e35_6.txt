The paper presents a specific implementation of a deep memory network with an attention mechanism, designed for hierarchical, long-term action planning in complex problems. This network is trained using behavioral cloning of expert trajectories and evaluated on a novel task: predicting basketball player behavior from real play data. The authors conduct a thorough study of different model architecture choices and evaluate them extensively. The paper is well-written, addressing an important problem: learning hierarchical behavioral policies from data. The chosen benchmark is interesting, offering a welcome change from standard benchmarks used in control tasks and imitation learning. The experiments are thorough, and the authors attribute performance improvements to specific modeling decisions, which is a significant advantage. The model formulation appears reasonable, and the connection between hierarchical policies and attention mechanisms for language modeling is a sensible idea.
However, the paper also has some issues. Firstly, the related work section lacks citations from imitation learning, inverse reinforcement learning (IRL), and apprenticeship learning, which could be used to solve the presented problem. The algorithm can be seen as an instance of imitation learning, specifically "behavioral cloning." The terms "policy" and "planner" are used interchangeably, which may seem odd to someone from the RL community. Additionally, referring to DQN as a "shallow planner" might be considered unfortunate terminology.
Regarding notation issues and loss functions, the auxiliary transfer function in Equations (2) and (3) seems to be conditioned on the state, but this is not explicitly stated. It is unclear whether the actual posterior derived in Equation (3) is used during training, as each part of the model is trained in a supervised manner, and fine-tuning only occurs on the micro-action level without computing the integral. The dependency on history disappears between Equations (3) and (7), which appears to be an error. The main issue with the loss function is that it seems symmetrical in the output of the micro and macro networks, which could lead to a trivial local minimum where one of the networks outputs a uniform distribution.
In terms of experimental details, it is unclear what method is used for evaluation, whether sampling from the posterior defined by Equation (3) or using the maximum action from the deterministic network output. The task evaluated is interesting, but a comparison to a standard task where hierarchical behavior is expected to help would be beneficial for facilitating comparison to existing work. The visualizations could be improved by depicting the closest trajectory in the training data to the one produced for the test cases and including more visualizations for baseline methods.
The human preference study has some issues, such as the statement that "During extrapolation, the other nine players do not animate," which seems odd and might restrict the capabilities of experts to tell apart a real strategy from a generated one. The evaluation of the macro-planner shows that the agent rarely visits the actual macro goals, which is surprising and might be a result of the fine-tuning procedure.
The benchmark analysis has some problems, such as evaluating classification accuracy, which might not be the best measure given the discretization of the action space. A distance-based measure would be more suitable. The table could be improved by including evaluations of stochastic action selection, splitting the test data into folds, and reporting mean and variances.
Minor issues include poor-quality figures, incorrect citation style, and some minor text mistakes. The figures should be converted to vector graphics or high-resolution versions, and the citation style should be corrected according to the author guidelines. The text mistakes, such as missing articles and minor errors, should be corrected.
Overall, the paper could be accepted if the issues with notation, loss function formulation, and experimental details are addressed, and the minor issues are corrected. The paper has some positive aspects, such as the thorough evaluation of model choices and the interesting benchmark, but it requires some revisions to improve its clarity and technical soundness.