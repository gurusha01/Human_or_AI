This paper presents a valuable methodology for simultaneously segmenting and transcribing handwritten paragraphs into text. The approach leverages MDLSTMs to learn two key components: 1) the encoding of input data and 2) an attention mechanism that effectively segments each line of text. A Bidirectional-LSTM decoder is then trained to generate text corresponding to the concatenated encoded sequences, with a CTC loss function minimized across the entire paragraph. The work builds upon previous research, notably extending beyond models that require pre-segmented lines, and demonstrates practical applicability. To further strengthen the paper, considering a baseline model utilizing simpler Convolutional or Affine layers [5] would have been beneficial, as these are less complex and easier to train than MDLSTMs. Additionally, incorporating another layer of MDLSTMs with CTCs to identify paragraph boundaries could increase model complexity. Although the authors report improvements over their baseline and enhanced performance on the Rimes benchmark, the results on the IAM benchmark did not surpass previous outcomes, suggesting room for further optimization.