The authors present a novel analysis of SVRG, enabling the use of "Option I" which involves taking the final iterate of the inner iteration, a common practice. Additionally, they propose utilizing a scaled version of the Barzilai-Borwein method to determine the step-size for SVRG, with a heuristic argument suggesting its potential applicability to traditional stochastic gradient methods as well. The experimental results demonstrate that this adaptive step-size approach is competitive with fixed step-sizes. Following the author response, I have increased my score due to the experiments presented. Initially, I reviewed this paper for ICML, and some of my previous comments remain relevant, which I will include below. First, I will address the changes and lack of changes since the previous review round: 
1. The authors have removed several misleading statements, added references, and included further discussion, significantly improving the paper.
2. A reviewer noted that the quadratic dependence on certain problem constants is inferior to existing results. I find this acceptable, given the advantage of having an automatic step-size, but the paper should explicitly acknowledge that the bound is looser. The same reviewer pointed out that achieving a "bad" rate under Option I is straightforward to establish, although I agree with the authors that this contribution is novel.
3. The paper still lacks an empirical comparison with existing heuristics used for step-size tuning. While the SAG line-search is now mentioned in the introduction, noting its potential to outperform the "best tuned step size" by adapting the step-size as it approaches the solution, there is no experimental comparison. Furthermore, the MISO line-search heuristic remains unaddressed. This omission is puzzling, as including such comparisons would only strengthen the paper, regardless of whether the proposed method surpasses or matches existing methods. The absence of these comparisons may lead the reader to suspect that there are undisclosed issues.
4. A previous reviewer highlighted a severe restriction on the condition number, which has been addressed in the current submission.
--- Comments from the previous review ---
Summary: The authors provide a new analysis of SVRG, facilitating the use of "Option I" and proposing a scaled Barzilai-Borwein method for step-size determination in SVRG, with potential applications to classic stochastic gradient methods. Their experiments show that the adaptive step-size is competitive with fixed step-sizes. 
Clarity: The paper is well-written and easy to understand, despite numerous grammar issues.
Significance: This work presents the first theoretically justified adaptive step-size method, although it still relies on unknown constants. It represents a step towards developing black-box stochastic gradient methods.
Details: The authors offer a novel approach to bounding the step-size for the Barzilai-Borwein method. While this method typically leads to faster rates than constant step-sizes, the theory and experiments in the SVRG context only demonstrate equivalence to the best fixed step-size. To strengthen the paper, comparisons with existing practical strategies, such as the line-search methods of Le Roux et al. and Mairal, would be beneficial. However, I do not expect the proposed approach to outperform these methods, although it provides a theoretical foundation lacking in the older approaches.