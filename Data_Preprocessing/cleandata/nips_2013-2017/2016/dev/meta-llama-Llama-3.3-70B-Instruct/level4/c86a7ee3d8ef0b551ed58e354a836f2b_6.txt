This study presents an approach utilizing the Barzilai-Borwein method for calculating the learning rate in stochastic gradient methods and their variants. Notably, it offers a linear convergence proof for the SVRG-BB method, although it lacks a convergence analysis for the SGD-BB method. The proposal of a step size learning technique for SGD and its variants is significant, as similar research is scarce for SVRG, whereas numerous studies have explored step size learning for SGD. Several concerns arise: 
1. Figures 1(d, e, f) intriguingly demonstrate the convergence of the BB step size to a constant value. However, a comparison with the findings of Allen-Zhu and Hazan in "Variance reduction for faster non-convex optimization" (Fig 5) reveals that an optimally tuned decreasing learning rate yields a faster convergence rate for SVRG. An explanation for the constant convergence observed in this work would be beneficial.
2. Given the plethora of step size learning methods for SGD, the experimental section on SGD-BB would be strengthened by the inclusion of additional comparative methods, such as Adam and Adagrad, to provide a more comprehensive evaluation.