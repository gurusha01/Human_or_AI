This manuscript presents a noteworthy application of two established learning algorithms, namely Williams' reinforce algorithm and Bayesian optimization, to estimate the decision threshold in a 2-AFC task. The threshold estimation is achieved by minimizing the difference between the reward sum of the optimal decision policy and the sum of collected rewards. The author also conducts a comparative analysis of the two algorithms' performance and maps the SPRT decision performance across the primary parameter space. From a technical standpoint, the paper appears to be sound. The core idea of estimating decision thresholds resonates with me, as it addresses a significant problem in decision science. 
In terms of technical quality, this paper focuses on estimating a decision threshold, a problem that has not received sufficient attention within this community. The use of two existing algorithms to estimate the threshold and the comparison of their performances against each other are notable aspects. The paper is interesting and technically sound, exploring a relatively unexamined issue. 
Regarding novelty, the concept of estimating the threshold offers some novel contributions to the computational psychology community. However, the algorithms employed in this paper are not novel, as they are used with minimal modifications based on previous works [5], [6], and [7]. Furthermore, the estimation of the optimal threshold requires other parameters to be fixed, which could be addressed through a naive approach of searching the parameter space of the threshold to minimize the cost function. This cost function could be computed by averaging trials simulated with the SPRT method, a complexity similar to other parameter estimation problems with fixed thresholds. The primary novel contribution of this paper lies in its attempt to estimate the decision threshold, an area that has not been sufficiently studied.
The potential impact of this paper is significant, as it estimates a constant decision threshold in a 2-AFC task with an infinite horizon, likely attracting attention from researchers in psychology and cognitive science. However, most psychology experiments require the 2-AFC task to be completed within a specific deadline, which would necessitate a time-dependent threshold within each trial. A more sophisticated estimation algorithm capable of estimating time-dependent thresholds under finite horizons would be beneficial. Additionally, if the algorithm could jointly learn the threshold along with other parameters, the paper's impact would be enhanced.
The clarity and presentation of the paper could be improved. The method section lacks crucial information, making it challenging for readers to replicate the work. For instance, in Section 3.2, "REINFORCE METHOD," an illustrative network figure would provide readers with a clearer understanding of the inputs and network training process. The information in Section 3.3 is also vague, and while references are provided for more details, the main framework of these algorithms should be clearly explained within the paper. I recommend including more algorithmic details and condensing the lengthy discussion section. Furthermore, correcting typographical errors, such as those in equation (6) and line 114, is necessary.
Upon further discussion, I realized that the optimal threshold has been studied by other researchers using different approaches, which led me to lower the impact score due to the unclear advantage of the methodology used in this paper compared to others. However, after re-reading the paper, I increased the clarity score.