This paper investigates the problem of vector recovery from quadratic measurements, also known as phase retrieval, by formulating a non-convex optimization problem and solving it using a gradient-descent-like method with modified initialization and refinement stages. The initialization is obtained as an eigenvector of a newly proposed matrix, which differs from existing methods and demonstrates improved sample complexity in numerical experiments (Figs. 3 and 4). The refinement stage employs a gradient-descent approach that ignores undesirable data points at each iteration, with a new objective function and truncation rule that contribute to improved sample complexity, as shown in Fig. 1. The overall advantages of the new method in terms of sample complexity are illustrated in Fig. 5, although the computational complexity remains comparable to existing methods. 
Several aspects of the paper warrant further exploration: 
1) A comparison of algorithms using the truncated spectral initialization (Fig. 1) reveals a significant difference in success rates between TWF and TGGF. It would be insightful to investigate how replacing the initialization with the new proposal affects this comparison, potentially attributing the improvement to either the initialization or the refinement stage. 
2) The performance of TGGF with structured sensing vectors, such as coded diffraction patterns, is unknown and deserves examination. 
3) While empirical results for the noisy case are promising, theoretical guarantees are lacking. Although not a major concern given the existing results, exploring theoretical foundations for the noisy case could strengthen the paper. 
4) The measurement model assumes Gaussian noise and a specific form of noise addition. Numerical experiments with Poisson noise, relevant to imaging applications, could provide valuable insights, especially in the absence of theoretical results for the noisy case. 
5) The comparison between TGGF and TWF primarily focuses on sample complexity. Investigating the convergence rates of both methods for a fixed problem size (e.g., n=1000 and m=8000) could reveal whether the improvements extend beyond sample complexity to the rate of convergence.