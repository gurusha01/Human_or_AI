The proposed method presents a finite mixture model, wherein each component is a Gaussian latent variable model, with neural networks parameterizing the likelihood means and covariances. At its core, the paper's concept aligns with existing deep generative model (DGM) formulations that utilize a finite mixture of Gaussians as the postulated likelihood. The distinction lies in the formulation of the latent variable posteriors, where the authors diverge from the common approach of using Gaussian posteriors parameterized by deep neural networks. Instead, they adopt a conventional approach, imposing Normal-Wishart hyper-priors on the Gaussian posterior mean and precision matrix. It is unclear why the authors opted for this simpler method over variational posterior amortization, which is currently the state-of-the-art approach and has been shown to yield superior modeling and inferential performance. A more detailed justification for this choice would be necessary. Furthermore, the experimental results lack substantial empirical evidence to demonstrate the effectiveness and usefulness of the proposed approach, as they are confined to simulated datasets and do not provide an exhaustive comparison to existing state-of-the-art methods.