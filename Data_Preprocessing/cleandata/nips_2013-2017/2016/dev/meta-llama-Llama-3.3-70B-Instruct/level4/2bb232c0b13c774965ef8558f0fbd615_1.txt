The authors have proposed a comprehensive system for parsing handwritten text from images containing multiple horizontal lines, building upon the single-line parser introduced by Graves et al. [18] with the incorporation of an attention mechanism. The text decoding process utilizes Bidirectional-LSTMs, diverging from the original softmax approach employed by Graves et al. The system's performance is evaluated on the Rimes and IAM datasets, yielding results comparable to state-of-the-art methods on the Rimes dataset but falling short on the IAM dataset. This work represents a notable application of guided attention for identifying sequences of handwritten text lines within an image, extending the region attention approach pioneered by Xu et al [38] and others. 
To enhance the paper's clarity and self-containment, it would be beneficial to provide a more detailed explanation of the MD-LSTM model, akin to the description in [18], particularly in section 3. Several aspects of the results and methodology warrant further elucidation by the authors: 
- The superior performance of the proposed approach over methods utilizing ground truth line labeling, as observed in table 2 and section 5.3, could be attributed to the differences in decoder RNN re-initialization between the two approaches. A more comprehensive description of the authors' implementation of Graves et al.'s method would facilitate a clearer understanding of this disparity.
- The claim of a 20-30x speed improvement over the character decoder method of Bluche et al. [6], as stated on line 142, lacks specific timing comparisons to substantiate this assertion.
- The notable discrepancy in performance between the Rimes and IAM datasets, with the system excelling on the former but underperforming on the latter, possibly due to insufficient training data, could be addressed through an additional experiment. Training the model on the combined test and training data of the other dataset, followed by fine-tuning on the IAM training data, would provide valuable insights into the system's adaptability and data requirements.
- The attention mechanism's failure to recognize punctuation, as mentioned on line 213, requires a detailed explanation to understand the underlying causes and potential avenues for improvement.