This paper proposes Multimodal Residual Networks (MRN) for visual question-answering tasks, extending the idea of deep residual learning to multimodal inputs. The authors introduce a novel method to visualize the attention effect of joint residual mappings using back-propagation, which allows for implicit attention modeling without explicit attention parameters. The paper is well-structured, and the authors provide a clear overview of the related work, including deep residual learning and stacked attention networks.
The strengths of this paper include its ability to achieve state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. The authors also provide a thorough analysis of alternative models and explore various options, such as the number of learning blocks, visual features, and target answers. The visualization method introduced in the paper is also a significant contribution, as it provides a novel way to interpret the attention effect in multimodal models.
However, there are some weaknesses to the paper. The authors could provide more detailed explanations of the mathematical formulations, particularly in Section 3.2, where the joint residual function is defined. Additionally, the paper could benefit from more comparisons with other state-of-the-art methods, particularly in the area of attentional models. The authors also mention that the performance of Number and Other types is still not satisfactory compared to Human performance, which suggests that there is still room for improvement in these areas.
Arguments pro acceptance:
* The paper achieves state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks.
* The authors introduce a novel method to visualize the attention effect of joint residual mappings using back-propagation.
* The paper provides a thorough analysis of alternative models and explores various options.
Arguments con acceptance:
* The paper could benefit from more detailed explanations of the mathematical formulations.
* The authors could provide more comparisons with other state-of-the-art methods, particularly in the area of attentional models.
* The performance of Number and Other types is still not satisfactory compared to Human performance, which suggests that there is still room for improvement in these areas.
Overall, I believe that this paper is a strong contribution to the field of multimodal learning and visual question-answering. The authors provide a well-structured paper with a clear overview of the related work, and their proposed method achieves state-of-the-art results on a challenging dataset. With some minor revisions to address the weaknesses mentioned above, I would recommend accepting this paper for publication.