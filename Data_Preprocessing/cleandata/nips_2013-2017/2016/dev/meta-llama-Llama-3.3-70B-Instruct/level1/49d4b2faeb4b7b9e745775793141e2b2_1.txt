This paper presents a novel architecture for training deep, directed generative models with many layers of latent variables, called Matryoshka Networks (MatNets). The authors combine the benefits of DRAW-like models and Ladder Networks to create a class of models that can learn hierarchically-deep generative models with jointly-trained inference and generation. The MatNet architecture includes deterministic paths between all latent variables and the generated output, and provides a richer set of connections between computations for inference and generation.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the technical details of the proposed architecture. The authors provide a thorough review of related work, including LapGANs, Diffusion Nets, and Probabilistic Ladder Networks, and demonstrate how their approach addresses the limitations of these previous methods.
The strengths of the paper include:
* The proposed MatNet architecture is novel and well-motivated, addressing a significant challenge in training deep generative models.
* The authors provide a clear and detailed explanation of the technical details of the architecture, including the use of lateral connections, shortcut connections, and residual connections.
* The experimental results demonstrate the effectiveness of the MatNet architecture on several benchmark datasets, including MNIST, Omniglot, and CIFAR 10.
* The authors provide a thorough analysis of the results, including a discussion of the limitations of the approach and potential avenues for future work.
The weaknesses of the paper include:
* The paper is quite long and dense, making it challenging to read and understand in a single pass.
* Some of the technical details, such as the use of stochastic gradient variational Bayes and the reparametrization trick, may be unfamiliar to readers without a strong background in deep learning and variational inference.
* The authors could provide more discussion of the potential applications of the MatNet architecture beyond the benchmark datasets used in the experiments.
Overall, I believe that this paper makes a significant contribution to the field of deep learning and generative models, and demonstrates the potential of the MatNet architecture for solving challenging problems in computer vision and other areas. I would recommend accepting this paper for publication at NIPS.
Arguments pro acceptance:
* The paper presents a novel and well-motivated architecture for training deep generative models.
* The experimental results demonstrate the effectiveness of the MatNet architecture on several benchmark datasets.
* The authors provide a thorough analysis of the results and discuss potential avenues for future work.
Arguments con acceptance:
* The paper is quite long and dense, making it challenging to read and understand.
* Some of the technical details may be unfamiliar to readers without a strong background in deep learning and variational inference.
* The authors could provide more discussion of the potential applications of the MatNet architecture beyond the benchmark datasets used in the experiments.