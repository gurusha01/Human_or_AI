This paper presents a novel approach to learning decision thresholds in the drift-diffusion model of perceptual decision making. The authors propose two distinct methods to optimize the decision thresholds: a REINFORCE method derived from Williams' REINFORCE algorithm for training neural networks, and a Bayesian optimization method that fits a Gaussian process to the reward function. Both methods are shown to converge to near-optimal decision thresholds, as validated against an exhaustive optimization of the reward function.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and methodology. The introduction provides a thorough background on the drift-diffusion model and the sequential probability ratio test, and the methods section clearly explains the two approaches used to optimize the decision thresholds. The results section presents a comprehensive comparison of the two methods, including their convergence rates, variance, and computational cost.
The strengths of the paper include its novel approach to learning decision thresholds, its thorough comparison of the two methods, and its validation against experimental data from animal learning studies. The authors also provide a clear discussion of the limitations and potential improvements of their methods, which demonstrates a thorough understanding of the subject matter.
One potential weakness of the paper is that the Bayesian optimization method is found to be more variable in its threshold estimates and slower in computation time compared to the REINFORCE method. However, the authors acknowledge this limitation and suggest potential improvements to the Bayesian optimization method.
In terms of the review criteria, the paper scores well on quality, clarity, and originality. The paper is technically sound, with a clear and well-organized presentation of the methods and results. The writing is clear and concise, making it easy to understand the authors' arguments and methodology. The paper also presents a novel approach to learning decision thresholds, which demonstrates originality and significance.
The significance of the paper lies in its potential to advance our understanding of perceptual decision making and the neural mechanisms underlying it. The authors' finding that the REINFORCE method is a plausible model of animal learning supports the idea that reward-driven learning plays a key role in perceptual decision making. The paper also has implications for the development of more efficient and effective methods for optimizing decision thresholds in a variety of applications.
Arguments pro acceptance:
* The paper presents a novel approach to learning decision thresholds in the drift-diffusion model.
* The authors provide a thorough comparison of the two methods, including their convergence rates, variance, and computational cost.
* The paper is well-written and clearly organized, making it easy to follow the authors' arguments and methodology.
* The authors validate their methods against experimental data from animal learning studies, which supports the idea that reward-driven learning plays a key role in perceptual decision making.
Arguments con acceptance:
* The Bayesian optimization method is found to be more variable in its threshold estimates and slower in computation time compared to the REINFORCE method.
* The paper could benefit from a more detailed discussion of the potential limitations and biases of the methods used.
* The authors could provide more context on the significance of their findings and their implications for the field of perceptual decision making.