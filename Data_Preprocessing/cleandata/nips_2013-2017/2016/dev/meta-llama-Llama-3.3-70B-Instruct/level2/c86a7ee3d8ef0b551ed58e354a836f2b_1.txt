This paper proposes two new stochastic gradient descent (SGD) methods, namely SGD-BB and SVRG-BB, which incorporate the Barzilai-Borwein (BB) method to automatically compute step sizes. The authors prove the linear convergence of SVRG-BB for strongly convex objective functions and demonstrate its effectiveness through numerical experiments on standard data sets. The paper also proposes a smoothing technique to stabilize the step size in SGD-BB.
The main claims of the paper are: (1) the proposed SGD-BB and SVRG-BB methods can automatically compute step sizes without requiring any parameters, (2) SVRG-BB converges linearly for strongly convex objective functions, and (3) the numerical experiments demonstrate the effectiveness of the proposed methods.
The support for these claims is provided through theoretical analysis and numerical experiments. The authors provide a detailed proof of the linear convergence of SVRG-BB and demonstrate its effectiveness through numerical experiments on standard data sets. The numerical results show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes.
The usefulness of the proposed methods is evident from the fact that they can automatically compute step sizes without requiring any parameters, which can be time-consuming in practice. The methods are also shown to be effective in solving standard machine learning problems such as logistic regression and SVM.
The paper reflects common knowledge in the field, and the authors provide a clear and concise introduction to the background and related work. The use of terms and notation is consistent with the field, and the authors provide adequate references to relevant literature.
The novelty of the paper lies in the proposal of the BB method for computing step sizes in SGD and its variants. The authors demonstrate the effectiveness of the proposed methods through numerical experiments and provide a detailed proof of the linear convergence of SVRG-BB.
The completeness of the paper is evident from the fact that the authors provide a detailed introduction, theoretical analysis, and numerical experiments to support their claims. The paper is well-organized, and the authors provide clear and concise explanations of the proposed methods and their analysis.
The limitations of the paper are acknowledged by the authors, who note that the performance of the proposed methods may depend on the choice of parameters such as the update frequency and the initial step size. However, the authors demonstrate that the proposed methods are not sensitive to these choices and can perform well in practice.
Overall, the paper is well-written, and the authors provide a clear and concise presentation of their proposed methods and analysis. The paper demonstrates the effectiveness of the proposed methods and provides a significant contribution to the field of stochastic gradient descent.
Arguments pro acceptance:
* The paper proposes a novel method for computing step sizes in SGD and its variants.
* The authors provide a detailed proof of the linear convergence of SVRG-BB.
* The numerical experiments demonstrate the effectiveness of the proposed methods.
* The paper is well-organized, and the authors provide clear and concise explanations of the proposed methods and their analysis.
Arguments con acceptance:
* The paper may benefit from additional numerical experiments to further demonstrate the effectiveness of the proposed methods.
* The authors may need to provide more discussion on the choice of parameters such as the update frequency and the initial step size.
* The paper may benefit from additional analysis on the theoretical properties of the proposed methods.