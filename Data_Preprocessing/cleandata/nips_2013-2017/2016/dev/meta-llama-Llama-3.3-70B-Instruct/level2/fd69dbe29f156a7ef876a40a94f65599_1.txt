This paper proposes a novel neural network-based reasoning model for visual question answering (VQA) tasks. The model iteratively updates the question representation by selecting image regions relevant to the query and learns to give the correct answer. The authors claim that their model achieves state-of-the-art performance on two challenging VQA datasets, COCO-QA and VQA.
The paper is well-written, and the authors provide a clear explanation of their model and its components. The use of object proposals to obtain candidate image regions and the attention mechanism to determine the relevance between question and image regions are notable contributions. The experimental results demonstrate the effectiveness of the proposed model, particularly in answering questions that require spatial reasoning and object recognition.
The strengths of the paper include:
* The proposal of a novel neural network-based reasoning model for VQA tasks
* The use of object proposals and attention mechanisms to improve question representation and image region selection
* The achievement of state-of-the-art performance on two challenging VQA datasets
* The provision of qualitative analysis and visualization of attention masks to demonstrate the model's ability to focus on relevant image regions
However, there are some weaknesses and limitations:
* The model's performance on questions that require counting and word-region relation is not satisfactory
* The use of a limited number of object proposals (20) may not be sufficient for complex scenes
* The model's ability to generalize to other datasets and tasks is not evaluated
Arguments for acceptance:
* The paper proposes a novel and effective model for VQA tasks
* The experimental results demonstrate the model's state-of-the-art performance on two challenging datasets
* The qualitative analysis and visualization of attention masks provide insights into the model's decision-making process
Arguments against acceptance:
* The model's limitations and weaknesses, such as its performance on counting and word-region relation tasks, need to be addressed
* The use of a limited number of object proposals may not be sufficient for complex scenes
* The model's ability to generalize to other datasets and tasks is not evaluated
Overall, the paper is well-written, and the proposed model is novel and effective. However, the limitations and weaknesses need to be addressed, and the model's ability to generalize to other datasets and tasks should be evaluated. With some revisions and additional experiments, the paper has the potential to be a strong contribution to the field of VQA.