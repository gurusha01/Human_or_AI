This paper proposes using the Barzilai-Borwein (BB) method to automatically compute step-sizes in Stochastic Gradient Descent (SGD) and stochastic variance reduced gradient (SVRG) algorithms. The authors implement the BB method within SVRG and SGD, with a smoothing technique used for the latter, to adaptively learn the optimal step-size. The simulation results are convincing, showing that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes.
The paper has several strengths. Firstly, the proposed method is able to automatically compute the step-size, which is a significant advantage over traditional methods that require manual tuning. Secondly, the method is shown to be effective in practice, with numerical experiments demonstrating its superiority over other methods. Thirdly, the paper provides a thorough analysis of the method, including a proof of linear convergence for SVRG-BB.
However, there are also some weaknesses. Firstly, the paper notes that the BB method can result in a strong overshoot towards small step-sizes in the initial iterations, which may be suboptimal. Secondly, the smoothing formula used in SGD reintroduces a deterministic non-adaptive decrease, similar to predefined schemes, which may limit the adaptability of the method. Thirdly, the paper contains a technical error, with the expectation in Lemma 1 needing to be a conditional expectation for accuracy.
In terms of quality, the paper is technically sound, with a clear and well-organized presentation. The claims are well-supported by theoretical analysis and experimental results. However, the paper could benefit from a more detailed discussion of the limitations of the method and potential avenues for future research.
In terms of clarity, the paper is well-written and easy to follow. The notation is clear and consistent, and the authors provide a thorough explanation of the methodology and results.
In terms of originality, the paper proposes a novel combination of the BB method with SGD and SVRG algorithms. The idea of using the BB method to compute the step-size is new and innovative, and the paper demonstrates its effectiveness in practice.
In terms of significance, the paper addresses a difficult problem in stochastic optimization, namely the choice of step-size. The results have the potential to impact a wide range of applications, including machine learning and data science.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of stochastic optimization. However, I would suggest that the authors address the technical error and provide a more detailed discussion of the limitations of the method.
Arguments pro acceptance:
* The paper proposes a novel and innovative method for computing the step-size in SGD and SVRG algorithms.
* The method is shown to be effective in practice, with numerical experiments demonstrating its superiority over other methods.
* The paper provides a thorough analysis of the method, including a proof of linear convergence for SVRG-BB.
Arguments con acceptance:
* The paper notes that the BB method can result in a strong overshoot towards small step-sizes in the initial iterations, which may be suboptimal.
* The smoothing formula used in SGD reintroduces a deterministic non-adaptive decrease, similar to predefined schemes, which may limit the adaptability of the method.
* The paper contains a technical error, with the expectation in Lemma 1 needing to be a conditional expectation for accuracy.