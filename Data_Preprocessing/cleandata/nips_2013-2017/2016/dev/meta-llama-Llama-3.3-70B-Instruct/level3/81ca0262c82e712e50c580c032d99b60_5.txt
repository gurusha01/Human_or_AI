This paper introduces a novel approach to visual question answering (VQA) by iteratively updating the question representation through interactions with image regions. The proposed model, called QRU, consists of four major components: image understanding, question encoding, reasoning, and answering layers. The authors evaluate their model on two challenging VQA datasets, COCO-QA and VQA, and achieve state-of-the-art performance.
The strengths of this paper include its ability to focus on relevant image regions through the attention mechanism, which is demonstrated by the visualization of attention masks. The model's performance is also impressive, outperforming other state-of-the-art models on both datasets. Additionally, the authors provide a thorough analysis of their model's strengths and weaknesses, including an ablation study that highlights the importance of each component.
However, there are some weaknesses to the paper. The model's complexity and the number of hyperparameters may make it difficult to train and tune. The authors also rely heavily on pre-trained models, such as the VGG network and the skip-thought vectors model, which may limit the model's ability to generalize to new datasets. Furthermore, the model's performance on certain question types, such as "Number", is weaker than other models, which may indicate a need for further improvement.
In terms of quality, the paper is technically sound, and the authors provide a clear and well-organized presentation of their model and results. The claims are well-supported by experimental results, and the authors are careful to evaluate both the strengths and weaknesses of their model. However, the paper could benefit from more depth and clarity in certain sections, such as the description of the reasoning layer and the attention mechanism.
The originality of the paper lies in its novel approach to VQA, which combines the strengths of both computer vision and natural language processing. The authors draw on previous work in text question answering and image captioning, but their model is distinct and innovative. The significance of the paper is also high, as it addresses a difficult problem in a better way than previous research and provides a unique approach to VQA.
Overall, I would recommend accepting this paper, as it presents a novel and effective approach to VQA and achieves state-of-the-art performance on two challenging datasets. However, the authors should address the minor errors and inconsistencies in notation, formatting, and references, and provide more clarity and depth in certain sections of the paper.
Arguments pro acceptance:
* Novel and effective approach to VQA
* State-of-the-art performance on two challenging datasets
* Clear and well-organized presentation
* Thorough analysis of strengths and weaknesses
Arguments con acceptance:
* Model complexity and number of hyperparameters
* Reliance on pre-trained models
* Weaker performance on certain question types
* Minor errors and inconsistencies in notation, formatting, and references.