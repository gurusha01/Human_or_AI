This paper proposes a novel approach to learning decision thresholds in the drift-diffusion model of perceptual decision making. The authors introduce two distinct methods to optimize the decision thresholds: a REINFORCE method derived from Williams' REINFORCE algorithm for training neural networks, and a Bayesian optimization method that samples and builds a probabilistic model of the reward function. Both methods converge to nearby the optimum decision thresholds, as validated against an exhaustive optimization.
The paper is well-written and clearly structured, making it easy to follow the authors' arguments. The technical contribution of the paper relies on a key insight into the distribution of rewards, which are highly stochastic and challenging to optimize. The authors address this challenge by proposing two distinct methods, each with its strengths and weaknesses. The REINFORCE method is shown to be a plausible model of animal learning, with results resembling acquisition data from behavioral experiments in rodent and bat.
The paper's strengths include its clear and concise writing style, its well-structured organization, and its thorough evaluation of the proposed methods. The authors provide a detailed comparison of the two methods, highlighting their pros and cons, and discuss potential extensions and improvements. The paper also provides a comprehensive review of related work, situating the proposed methods within the broader context of decision making and reinforcement learning.
However, the paper could benefit from additional details in some places, such as clarifying the proofs and providing more comparisons to other approaches. The experimental section is somewhat disappointing, with unclear take-home messages and a lack of comparisons to other approaches, such as naive data-splitting strategies. Additionally, the authors raise several technical questions and suggestions, including discussing the assumption of non-overlapping groups, going beyond quadratic loss, and providing more details on computational complexity and numerical solutions.
Overall, the paper makes a significant contribution to the field of decision making and reinforcement learning, and its results have important implications for our understanding of how animals learn to make decisions under uncertainty. The proposed methods have the potential to be applied to a wide range of problems, from psychology and neuroscience to machine learning and artificial intelligence.
Arguments pro acceptance:
* The paper proposes a novel approach to learning decision thresholds in the drift-diffusion model.
* The authors introduce two distinct methods to optimize the decision thresholds, each with its strengths and weaknesses.
* The paper provides a thorough evaluation of the proposed methods, including a comparison of their pros and cons.
* The results have important implications for our understanding of how animals learn to make decisions under uncertainty.
Arguments con acceptance:
* The paper could benefit from additional details in some places, such as clarifying the proofs and providing more comparisons to other approaches.
* The experimental section is somewhat disappointing, with unclear take-home messages and a lack of comparisons to other approaches.
* The authors raise several technical questions and suggestions, which may require further investigation and clarification.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Recommendation: Accept with minor revisions. The authors should address the minor issues raised in the review, including providing additional details and clarifying the proofs. Additionally, the authors should consider adding more comparisons to other approaches and providing a clearer take-home message from the experimental section.