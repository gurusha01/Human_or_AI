This paper proposes two distinct methods for learning decision thresholds in the drift-diffusion model of perceptual decision making: a REINFORCE method derived from Williams' REINFORCE algorithm for training neural networks, and a Bayesian optimization method that samples and builds a probabilistic model of the reward function to guide further sampling. The authors demonstrate that both methods can converge to nearby the optimum decision thresholds, as validated against an exhaustive optimization. However, the Bayesian optimization method converges much faster (∼500 trials) than the REINFORCE method (∼5000 trials), but is three-times as variable in the threshold estimates and 40-times slower in computation time.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the proposed methods. The use of figures and tables to illustrate the results is also helpful. The authors provide a thorough discussion of the strengths and weaknesses of each method, as well as potential improvements and broader applicability.
One of the key strengths of the paper is its ability to relate the proposed methods to previous work in the field, including the drift-diffusion model and the sequential probability ratio test. The authors also provide a clear explanation of the reward function and how it is derived from Wald's trial-averaged cost function.
However, there are some areas where the paper could be improved. For example, the authors could provide more detail on how the REINFORCE method is implemented, including the specific policy used and how the weights are updated. Additionally, the authors could provide more comparison to other methods, such as the method from reference 6, to demonstrate the effectiveness of the proposed approaches.
In terms of originality, the paper proposes a novel combination of familiar techniques, including the use of REINFORCE and Bayesian optimization for learning decision thresholds. The authors also provide a unique perspective on the problem of learning decision thresholds, highlighting the importance of considering the stochastic nature of the rewards.
The significance of the paper lies in its potential to advance our understanding of how animals learn decision thresholds in perceptual decision making tasks. The proposed methods could also have broader applicability to other areas of decision making, such as tasks with deadlines or multiple choices.
Overall, I would argue in favor of accepting this paper, as it provides a clear and well-organized presentation of two distinct methods for learning decision thresholds, and demonstrates their effectiveness through simulation and comparison to previous work. However, I would suggest that the authors address some of the areas for improvement mentioned above, such as providing more detail on the implementation of the REINFORCE method and comparing to other methods.
Arguments pro acceptance:
* The paper proposes two distinct methods for learning decision thresholds, which could be useful for understanding how animals learn decision thresholds in perceptual decision making tasks.
* The authors provide a clear and well-organized presentation of the proposed methods, making it easy to follow their arguments and understand the results.
* The paper demonstrates the effectiveness of the proposed methods through simulation and comparison to previous work.
Arguments con acceptance:
* The paper could benefit from more detail on the implementation of the REINFORCE method, including the specific policy used and how the weights are updated.
* The authors could provide more comparison to other methods, such as the method from reference 6, to demonstrate the effectiveness of the proposed approaches.
* The paper could be improved by providing more discussion of the broader applicability of the proposed methods to other areas of decision making.