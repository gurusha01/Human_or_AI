This paper proposes the use of the Barzilai-Borwein (BB) method to automatically tune the step size for Stochastic Gradient Descent (SGD) and Stochastic Variance Reduced Gradient (SVRG) methods, achieving linear convergence. The authors provide a missing proof in the literature for the linear convergence of SVRG with Option-I, filling a gap in existing research. The experimental results show that SVRG-BB achieves competitive performance compared to the best-tuned SVRG, regardless of the initial step size.
The paper is well-written and provides a useful contribution to the community, despite being incremental, by complementing the proof of SVRG-I and offering practical methods. The proposed algorithms, SGD-BB and SVRG-BB, are easy to implement and do not require any additional parameters, making them attractive for practical applications.
The strengths of the paper include:
* The proposal of a novel method for automatically tuning the step size for SGD and SVRG, which can be useful in practice.
* The provision of a missing proof in the literature for the linear convergence of SVRG with Option-I.
* The experimental results demonstrating the competitive performance of SVRG-BB compared to the best-tuned SVRG.
The weaknesses of the paper include:
* The paper is incremental, building on existing work, and may not be considered as a major breakthrough.
* The theoretical analysis is limited to strongly convex objective functions, and it would be interesting to see if the results can be extended to more general cases.
* The experimental results are limited to a few datasets and problems, and it would be useful to see more extensive experiments to confirm the effectiveness of the proposed methods.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
In terms of originality, the paper proposes a novel combination of familiar techniques, and the use of the BB method for automatically tuning the step size for SGD and SVRG is new. The related work is adequately referenced, and the paper clearly explains how it differs from previous contributions.
In terms of significance, the paper addresses an important problem in stochastic optimization, and the results have the potential to be useful in practice. The paper advances the state of the art in a demonstrable way, providing a new method for automatically tuning the step size for SGD and SVRG.
Overall, I would recommend accepting this paper, as it provides a useful contribution to the community, and the results have the potential to be useful in practice. However, I would suggest that the authors consider addressing the weaknesses mentioned above, such as providing more extensive experiments and exploring the extension of the results to more general cases.
Arguments pro acceptance:
* The paper proposes a novel method for automatically tuning the step size for SGD and SVRG.
* The paper provides a missing proof in the literature for the linear convergence of SVRG with Option-I.
* The experimental results demonstrate the competitive performance of SVRG-BB compared to the best-tuned SVRG.
Arguments con acceptance:
* The paper is incremental, building on existing work.
* The theoretical analysis is limited to strongly convex objective functions.
* The experimental results are limited to a few datasets and problems.
Future directions:
* Extending the results to more general cases, such as non-convex objective functions.
* Providing more extensive experiments to confirm the effectiveness of the proposed methods.
* Exploring the use of the BB method for automatically tuning the step size for other stochastic optimization algorithms.