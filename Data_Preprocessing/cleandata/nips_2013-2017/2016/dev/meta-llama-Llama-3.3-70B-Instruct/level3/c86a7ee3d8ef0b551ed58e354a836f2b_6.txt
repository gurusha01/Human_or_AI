This paper proposes a novel approach to automatically compute step sizes for stochastic gradient descent (SGD) and its variant, stochastic variance reduced gradient (SVRG), using the Barzilai-Borwein (BB) method. The resulting algorithms, SGD-BB and SVRG-BB, are shown to have linear convergence for strongly convex objective functions. The paper fills a gap in step size learning for SVRG, as existing research primarily focuses on SGD.
The strengths of the paper include its technical soundness, clarity, and originality. The authors provide a thorough analysis of the convergence of SVRG-BB and SGD-BB, and the numerical experiments demonstrate the efficacy of the proposed algorithms. The paper is well-organized, and the authors provide sufficient background information and references to related work.
However, there are some weaknesses and areas for improvement. The convergence of the BB step size to a constant value is questionable, as conflicting results from another paper suggest that a decreasing learning rate yields faster convergence for SVRG. Additionally, the experimental comparison for SGD-BB should include more methods, such as Adam and Adagrad, to provide a more comprehensive evaluation.
The significance of the paper lies in its potential to advance the state of the art in stochastic optimization. The proposed algorithms have the potential to be widely used in practice, as they can automatically compute step sizes without requiring manual tuning. The paper also provides new insights into the convergence of SVRG and SGD, which can inform future research in this area.
Arguments pro acceptance:
* The paper proposes a novel and technically sound approach to step size learning for SVRG and SGD.
* The numerical experiments demonstrate the efficacy of the proposed algorithms.
* The paper fills a gap in the literature and has the potential to advance the state of the art in stochastic optimization.
Arguments con acceptance:
* The convergence of the BB step size to a constant value is questionable.
* The experimental comparison for SGD-BB is limited and should include more methods.
* The paper could benefit from more discussion on the potential limitations and future directions of the proposed algorithms.
Overall, the paper is well-written, and the authors provide a clear and concise presentation of their work. With some revisions to address the weaknesses and areas for improvement, the paper has the potential to make a significant contribution to the field of stochastic optimization.