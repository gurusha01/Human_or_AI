This paper addresses the problem of finding optimal thresholds for the drift-diffusion model (DDM) in two-alternative forced choice experiments in psychophysics. The authors propose a meta-optimality problem to minimize the "Bayes risk" and introduce two bandit strategies, REINFORCE and Bayesian optimization, to solve the meta-optimization problem. The paper presents empirical tests comparing the two methods and discusses their strengths and weaknesses.
The paper's main contribution is the formulation of the threshold learning problem as a reinforcement learning problem, which is a novel and interesting approach. The authors provide a clear and well-structured presentation of the problem, the proposed methods, and the empirical results. The use of REINFORCE and Bayesian optimization methods is well-motivated, and the comparison between the two methods is thorough.
However, there are some potential conceptual pitfalls in addressing the question of optimally setting parameters. The paper assumes that the optimal thresholds can be learned through reinforcement learning, but it is not clear if this assumption is justified. Additionally, the paper lacks a comparison to real-world data, which makes the findings insufficient and unsurprising.
The paper has some shortcomings, including typos in the math, unjustified design choices, and a lack of comparison to real-world data. The Bayesian optimization method is found to converge faster than REINFORCE but is more computationally expensive and has greater variance in the threshold estimates. The REINFORCE method is considered a better model of acquisition behavior in animals, but the paper's findings are not surprising, and the methods are not significantly different from previous contributions.
The paper's potential is acknowledged, but its current state is considered premature due to the aforementioned shortcomings and lack of surprising or sufficient findings from both machine learning and psychophysics perspectives. To improve the paper, the authors should address the conceptual pitfalls, provide a more thorough comparison to real-world data, and justify their design choices.
Arguments pro acceptance:
* The paper addresses an interesting and novel problem in psychophysics.
* The authors propose two distinct methods to solve the meta-optimization problem.
* The empirical results are well-presented and provide a thorough comparison between the two methods.
Arguments con acceptance:
* The paper lacks a comparison to real-world data.
* The methods are not significantly different from previous contributions.
* The paper has some shortcomings, including typos in the math and unjustified design choices.
* The findings are not surprising, and the paper's potential is considered premature. 
Overall, the paper is well-written, and the authors provide a clear presentation of the problem and the proposed methods. However, the paper's shortcomings and lack of surprising or sufficient findings make it premature for acceptance. With further improvements, the paper has the potential to make a significant contribution to the field of psychophysics and machine learning.