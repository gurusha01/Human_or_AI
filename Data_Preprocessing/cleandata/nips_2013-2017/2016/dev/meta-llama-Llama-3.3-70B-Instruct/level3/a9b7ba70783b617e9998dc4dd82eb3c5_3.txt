This paper proposes a method for statistical inference on cluster trees, which are used to represent the hierarchy of high-density clusters in a density function. The authors introduce a partial ordering on cluster trees and use it to prune statistically insignificant features from the empirical cluster tree, resulting in a more interpretable and parsimonious representation of the data.
The paper is well-written and clearly explains the concepts and methods used. The authors provide a thorough review of related work and demonstrate the effectiveness of their approach on several synthetic examples and a real dataset. The use of a partial ordering to prune the cluster tree is a novel and useful contribution to the field.
However, there are some limitations and areas for improvement. The numerical example provided does not effectively demonstrate the potential of the proposed method, particularly in showcasing the richness of the confidence sets through pruning. Additionally, the partial ordering defined for comparing trees may not be useful for comparing different trees within the confidence sets, as it requires the main split to occur at exactly the same location.
The metrics defined in Section 3 can be applied to any functions on the same domain, not just density functions representing the same underlying population. This raises questions about the suitability of these metrics for inference on cluster trees. Furthermore, the definition of $p_{inf}$ and $a$ may not be meaningful for density functions without compact support, and the statement "more generally" in Lemma 1 is unclear in this context.
The equivalence of $d\infty$ and $dM$ is expected when considering the distance with respect to the truth, but it is unclear why this principle may not apply when comparing two different functions. The heights of the main part and two small clusters in the "mickey mouse data" example are expected to be different due to differing probabilities, but the figure shows them to be the same, causing confusion.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader.
The originality of the paper lies in the introduction of a partial ordering on cluster trees and the use of pruning to simplify the empirical cluster tree. The paper addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.
The significance of the paper is high, as it provides a new approach to statistical inference on cluster trees, which is an important problem in data analysis. The paper has the potential to impact the field of data science and statistics, and the methods proposed can be used in a variety of applications.
Overall, I would recommend accepting this paper, but with some revisions to address the limitations and areas for improvement mentioned above. The paper has the potential to make a significant contribution to the field, and with some revisions, it can be even stronger.
Arguments pro acceptance:
* The paper proposes a novel and useful method for statistical inference on cluster trees.
* The authors provide a thorough review of related work and demonstrate the effectiveness of their approach on several examples.
* The paper is well-written and clearly explains the concepts and methods used.
* The introduction of a partial ordering on cluster trees is a significant contribution to the field.
Arguments con acceptance:
* The numerical example provided does not effectively demonstrate the potential of the proposed method.
* The partial ordering defined for comparing trees may not be useful for comparing different trees within the confidence sets.
* The metrics defined in Section 3 may not be suitable for inference on cluster trees.
* The definition of $p_{inf}$ and $a$ may not be meaningful for density functions without compact support.