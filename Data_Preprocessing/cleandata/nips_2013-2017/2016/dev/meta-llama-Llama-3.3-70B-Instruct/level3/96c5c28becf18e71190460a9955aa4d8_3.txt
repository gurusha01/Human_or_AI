This paper presents a valuable contribution to the theoretical literature on stochastic gradient descent methods, specifically proposing the use of the Barzilai-Borwein (BB) method to automatically compute step sizes for stochastic gradient descent (SGD) and its variant, stochastic variance reduced gradient (SVRG) method. The resulting algorithms, SGD-BB and SVRG-BB, are shown to have comparable or even better performance than SGD and SVRG with best-tuned step sizes.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed methods. The theoretical analysis of the linear convergence of SVRG-BB is thorough and well-supported by experimental results. The authors also provide a smoothing technique to stabilize the step size in SGD-BB, which is a useful addition to the algorithm.
However, I am disappointed to see a lack of application to neuroscience or psychology, as the paper primarily targets a neuroscience/psychology audience. The authors could have explored the cognitive or biological plausibility of the algorithms and provided empirical evidence to support their claims.
To further improve the paper, I suggest plotting learning curves as a function of computation time to determine the optimal algorithm choice for a particular computational budget. Additionally, the authors should address why Bayesian optimization has a larger variance, as this is not clearly explained in the paper.
Furthermore, I recommend discussing and comparing the new model with existing neural network models, such as Simen, Holmes & Cohen (2006) and Frank (2006), to establish its validity as a psychological/neural theory. This would provide a more comprehensive understanding of the proposed algorithms and their potential applications in neuroscience and psychology.
In terms of the conference guidelines, I evaluate the paper based on the four criteria: quality, clarity, originality, and significance. The paper scores high on quality, as the theoretical analysis is sound and well-supported by experimental results. The paper is also clear and well-organized, making it easy to follow. The originality of the paper is moderate, as the authors propose a new application of the BB method to SGD and SVRG. The significance of the paper is high, as it addresses an important problem in stochastic gradient descent methods and provides a valuable contribution to the field.
Overall, I recommend accepting the paper, but with revisions to address the aforementioned suggestions and improve the paper's clarity and impact. 
Arguments pro acceptance:
- The paper presents a valuable contribution to the theoretical literature on stochastic gradient descent methods.
- The proposed algorithms, SGD-BB and SVRG-BB, are shown to have comparable or even better performance than SGD and SVRG with best-tuned step sizes.
- The paper is well-organized and clearly written.
Arguments con acceptance:
- The paper lacks application to neuroscience or psychology, despite targeting a neuroscience/psychology audience.
- The authors do not provide empirical evidence to support their claims.
- The paper does not address why Bayesian optimization has a larger variance.
- The paper could benefit from a comparison with existing neural network models to establish its validity as a psychological/neural theory.