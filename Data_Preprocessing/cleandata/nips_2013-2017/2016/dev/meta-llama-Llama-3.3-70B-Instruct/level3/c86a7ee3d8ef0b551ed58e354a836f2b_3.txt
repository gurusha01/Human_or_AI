This paper proposes the incorporation of the Barzilai-Borwein (BB) step-size into variants of stochastic gradient descent, including SGD, SVRG, and SAG, to improve convergence. The authors prove that SVRG with BB step size converges linearly in expectation on mu-strongly convex functions with Lipschitz continuous gradient when m is sufficiently large. The experimental results show that the proposed SVRG with BB step size converges towards the optimally tuned step size, resulting in a similar convergence rate on three data sets.
The paper's contribution is significant, as it eliminates the need to tune the step size, making SVRG easier to use. However, the sensitivity of the parameter m requires further empirical study to understand its impact on convergence. The time complexity of the algorithm increases linearly with the value of m, which may make tuning of m as sensitive as tuning of the step size.
The strengths of the paper include its technical soundness, clarity, and originality. The authors provide a clear and well-organized presentation of their work, making it easy to follow and understand. The use of the BB method to compute the step size is a novel approach that differs from previous contributions. The paper also provides a thorough analysis of the convergence of SVRG-BB and SGD-BB, including theoretical proofs and numerical experiments.
The weaknesses of the paper include the limited scope of the experiments and the sensitivity of the parameter m. The authors only test their algorithms on three data sets, which may not be representative of all possible scenarios. Additionally, the sensitivity of m requires further study to understand its impact on convergence.
Overall, the paper is well-written, and the authors provide a clear and concise presentation of their work. The use of the BB method to compute the step size is a significant contribution to the field, and the experimental results demonstrate the efficacy of the proposed algorithms.
Arguments pro acceptance:
* The paper proposes a novel approach to computing the step size in stochastic gradient descent variants.
* The authors provide a clear and well-organized presentation of their work.
* The paper includes a thorough analysis of the convergence of SVRG-BB and SGD-BB, including theoretical proofs and numerical experiments.
* The experimental results demonstrate the efficacy of the proposed algorithms.
Arguments con acceptance:
* The scope of the experiments is limited to three data sets.
* The sensitivity of the parameter m requires further empirical study to understand its impact on convergence.
* The time complexity of the algorithm increases linearly with the value of m, which may make tuning of m as sensitive as tuning of the step size.
In conclusion, the paper is a significant contribution to the field of stochastic gradient descent, and the proposed algorithms have the potential to improve convergence rates. However, further study is needed to fully understand the impact of the parameter m on convergence and to explore the applicability of the algorithms to a wider range of scenarios.