This paper proposes the use of the Barzilai-Borwein (BB) method to automatically compute step sizes for stochastic gradient descent (SGD) and its variant, stochastic variance reduced gradient (SVRG) method. The resulting algorithms, SGD-BB and SVRG-BB, are shown to have comparable or even better performance than SGD and SVRG with best-tuned step sizes.
The paper is well-written and clearly organized, with a thorough introduction to the background and motivation of the research. The authors provide a detailed description of the proposed algorithms and their convergence analysis, which is a significant contribution to the field. The numerical experiments demonstrate the efficacy of the proposed algorithms on standard testing problems in machine learning.
However, there are a few areas that could be improved. Firstly, the paper could benefit from a more detailed comparison with existing methods, particularly in terms of computational complexity and convergence rates. Secondly, the authors could provide more insight into the choice of parameters, such as the update frequency m and the weighting parameter Î², and how they affect the performance of the algorithms.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
The originality of the paper lies in the application of the BB method to SGD and SVRG, which is a novel combination of familiar techniques. The paper provides a unique perspective on the problem of choosing step sizes in stochastic gradient methods and offers a new approach that can be used in practice.
The significance of the paper is that it addresses a difficult problem in a better way than previous research. The proposed algorithms have the potential to be widely used in machine learning and other fields where stochastic gradient methods are commonly employed. The paper advances the state of the art in a demonstrable way and provides a unique theoretical and pragmatic approach to the problem.
Overall, I would recommend accepting this paper for publication, as it makes a significant contribution to the field and has the potential to be widely used in practice. 
Arguments pro acceptance:
- The paper proposes a novel approach to choosing step sizes in stochastic gradient methods.
- The proposed algorithms, SGD-BB and SVRG-BB, are shown to have comparable or even better performance than SGD and SVRG with best-tuned step sizes.
- The paper provides a thorough convergence analysis and numerical experiments to demonstrate the efficacy of the proposed algorithms.
- The paper addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.
Arguments con acceptance:
- The paper could benefit from a more detailed comparison with existing methods.
- The authors could provide more insight into the choice of parameters and how they affect the performance of the algorithms.
- The paper may not be suitable for readers who are not familiar with stochastic gradient methods and the BB method.