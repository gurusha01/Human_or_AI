This paper proposes Multimodal Residual Networks (MRN) for visual question-answering tasks, extending the idea of deep residual learning to multimodal inputs. The authors introduce a novel method to visualize the attention effect of joint residual mappings using back-propagation, which allows for implicit attention modeling without explicit attention parameters. The paper presents a thorough evaluation of the proposed model on the Visual QA dataset, achieving state-of-the-art results for both Open-Ended and Multiple-Choice tasks.
The strengths of the paper include its innovative approach to multimodal learning, the introduction of a novel visualization method, and the thorough evaluation of the proposed model. The authors provide a clear and well-structured presentation of their work, making it easy to follow and understand.
However, there are some weaknesses and areas for improvement. The paper lacks a thorough theoretical evaluation and embedding in existing literature, particularly in the context of tensor networks and optimization algorithms. The empirical evaluation is limited to the MNIST dataset, and a more comprehensive comparison with other approaches is needed. Additionally, the paper could benefit from more precise definitions and notation, as some terms, such as "exponentially large" and "delta^ellLn", are unclear or undefined.
In terms of quality, the paper is well-written and easy to follow, but it could be improved with more rigorous theoretical analysis and a more comprehensive evaluation of the proposed model. The originality of the paper is high, as it introduces a novel approach to multimodal learning and a new visualization method. The significance of the paper is also high, as it achieves state-of-the-art results on a challenging task and has the potential to impact the field of visual question-answering.
Arguments pro acceptance:
* Innovative approach to multimodal learning
* Introduction of a novel visualization method
* Thorough evaluation of the proposed model
* State-of-the-art results on the Visual QA dataset
Arguments con acceptance:
* Lack of thorough theoretical evaluation and embedding in existing literature
* Limited empirical evaluation
* Unclear or undefined terms and notation
* Need for more comprehensive comparison with other approaches
Overall, I recommend accepting this paper, but with revisions to address the weaknesses and areas for improvement mentioned above. With more rigorous theoretical analysis, a more comprehensive evaluation, and clearer definitions and notation, this paper has the potential to make a significant contribution to the field of visual question-answering.