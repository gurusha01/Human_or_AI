This paper adapts the "neural reasoning" framework for the problem of visual question answering. In particular, it addresses the issue that some questions themselves are inherently ambiguous/difficult, without seeing the image to know the context. By breaking the picture into candidate image regions, the whole image becomes a sequence of "facts" (similar to the setting in the bAbI dataset), the "neural reasoning" model can then be applied. Evaluation on COCO-QA shows decent results that are slightly higher than the state-of-the-art models. On the VQA dataset, the results are comparable to other systems and slightly worse. I like the basic idea of this work and it seems that the authors have successfully modified the model designed for a somewhat artificial bAbI task and managed to show strong performance on the more real VQA task. The general story makes sense to me, but I'm not fully convinced that the strong empirical results are indeed due to the claimed advantages of the model. For instance, some of the example questions seem very ambiguous. How many such questions are answered incorrectly by other models, but instead correctly answered by the proposed method? In addition, Figure 3 and the associated questions don't seem intuitive to me, as the question is ambiguous anyway. The results on the newer VQA dataset was not as impressive as those on the COCO-QA dataset, but there is little discussion on the pros and cons when comparing the proposed models with other state-of-the-art methods, especially on the VQA experiments.