The drift-diffusion model (DDM) is a standard approach for modeling the result of two-alternative forced choice experiments in psychophysics. In its simplest version, it consists of a random walk representing the evidence accumulation process, which evolves until hitting either an upper or lower threshold associated with the choices respectively. This paper discusses the problem of finding optimal thresholds for DDM. Prior studies suggest that thresholds are set in order to maximize the reward rate; however, the mechanics for doing so are not well understood. In a nutshell, this paper proposes that thresholds are determined by solving a meta-optimality problem that attempts to minimize the "Bayes risk" (a convex objective function which is assumed to arise from a loss function that is linear in the expected decision time and the type I and II errors). The resulting meta-optimization is then treated as a continuous-armed bandit problem. Two bandit strategies are discussed: one based on REINFORCE and another based on Bayesian optimization (with a custom acquisition function). These are compared against each other in empirical tests. Pros: - The question addressed is very interesting. Understanding the mechanisms by which the DDM thresholds are chosen might have farreaching implications for psychophysics. - The paper is well written: the mathematics is sound and the empirical results well presented. Cons: - There are potential conceptual pitfalls in addressing the question of "optimally setting the very parameters of optimal choice". Why is it obvious that this can be solved using meta-reasoning? This problem is not addressed at all. - There's quite a bit of typos in the math. For instance, equation (4) uses t and T. - Some design choices are unjustified. What loss function was used to formulate the Bayes risk? Why is the bandit strategy a linear combination of binary units? Isn't the exponential spread of coefficient values s_j not just a-ary coding? - IMHO however, the most important shortcoming of the paper is that its findings are neither sufficient nor surprising. From a ML point of view, the models are straightforward and thus less interesting; and from a psychophysics point of view, it is hard to evaluate the relevance of the proposed models without a careful comparison to monkey or human choice data. I believe that this paper has potential, but at this stage it seems premature.