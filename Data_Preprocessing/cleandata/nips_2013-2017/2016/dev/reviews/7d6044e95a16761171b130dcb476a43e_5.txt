The authors provide an approach to learn models that combine nonlinear likelihoods from neural networks with structured latent variables. They bring together several pre-existing tools including stochastic variational inference, message passing, and backpropagation using the reparameterization trick. Efficiency of optimization is improved where possible by using conjugate exponential families and natural gradients. The paper is well written with helpful examples and comments on related work. The paper is clearly written and neatly combines several earlier methods. To their credit, recent work that takes a similar approach [7,19,20,21,22] is cited and briefly described in Section 5. It would be worth adding a brief comment on Belanger and McCallum ICML 2016, Structured Prediction Energy Networks. Good examples are provided demonstrating scalability, and the videos are a nice bonus. Please could you clarify the strengths of new contributions here? My main concern is if bringing together this collection of earlier tools is a sufficient novel contribution for this conference. I do not feel strongly and am not an expert in this area.