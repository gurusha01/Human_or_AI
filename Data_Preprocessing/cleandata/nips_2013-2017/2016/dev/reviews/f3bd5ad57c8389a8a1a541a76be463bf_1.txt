This paper presents a method for monocular depth estimation from a single image. A set of derivative filters is convolved with the image producing an overcomplete feature representation. The statistics of this representation is gathered to build a probabilistic model which a neural net attempts to predict at each image location. This allows for an explicit form of uncertainty in the output of the net. After that, all point wise estimated need to combine to form a coherent output map - this is done by using Half Quadratic Optimization. Results on NYU2 are demonstrated and are quite favorable. This is a very nice paper. While not groundbreaking, it certainly has a lot of merits. I like the probabilistic nature of the network output, while not an explicit probabilistic model, it still allows to take uncertainty into account. Furthermore, the consensus step where all partial observations are combined is nice as well as it allows for an elegant way of resolving ambiguities. I have several concerns and questions though: 1. Using a fixed set of derivative feature estimates is a nice idea, but I wonder - can these be learned within the same framework? possibly with an iterative algorithm? 2. Section 3.1 - why learn these parameters with K-means? wouldn't EM be a more natural choice with better results? 3. What are typical running times at test time? seems to me this would be quite slow. 4. Looking at Table 2 it seems that the most important features are the order filters - differences between their performance and the full model seem negligible. Any comments about this?