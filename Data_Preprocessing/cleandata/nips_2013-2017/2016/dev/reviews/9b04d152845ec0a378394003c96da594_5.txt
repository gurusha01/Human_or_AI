The paper presents a multimodal learning scheme in lines of residual learning for addressing visual question answering problem. It achieves state of the art results on VQA. The most interesting part is the attention representation using back propagation without having attention parameters. If I understand correctly, the most interesting part is visualizing VQA attention without having attention parameters. Some background references on VQA will make the manuscript better for a non-VQA reader. With VQA becoming popular, information in section 4.1 seems redundant and taking lot of space. Some explanation on TrimZero in lines 136-137 may be helpful. In section 4.2- postprocessing, the update v= v+1 (line 154) is not clear. The results are impressive.