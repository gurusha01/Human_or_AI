This work introduces a modification to the collapse layer of standard MD-LSTMS used to do handwriting recognition introduced in [18]. The work is also closely related to [6] that also performs the same task -- decoding at a character level per time step. The modification is to convert the global average pooling into a weighted average pooling (eq. 2). Further, these weights are learned using a recurrent attention layer (MD-LSTM) that takes in both the previous weights and the encoded feature vector. A CTC loss is used to directly compute loss for the transcription without needing character or line-level alignment. Moreover, the attention mechanism can be interpreted as a "soft" line segmentation. The work provides comparison with other methods including [6] and obtains better / comparable performance. From my understanding, the major change from [6] to this paper is that at each time step an entire line is decoded as against a single character. Although this results in improved speeds and better performance, I am not completely convinced that there is enough technical novelty. (Please correct me in the rebuttal if I am missing something and if so, I will be happy to increase my rating) Also, it would have been interesting to visualize attentions when number of lines in input image is greater than the number of time steps the algorithm is set to run -- as it is mentioned that the method still fares better in such a scenario.