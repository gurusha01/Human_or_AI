It's a well written paper with interesting ideas: considering machine translation as a communication game between two parties who only know their own respective language. The following points were raised: 1. The method doesn't seem fully developed from a theoretical perspective. Would be great to see more theoretical analysis or discussion. 2. There are many missing details. For example, what is the exact one-way translation model used and how is its architecture/parameters determined? What is the exact model for evaluating the quality of pseudo-translations? Details like those are important. 3. In Algorithm 1 there is a scheduling that is alternating between A to B and B to A, one sentence by another. How effective is this scheduling and can it be made better? E.g. when sampling sentence from B one can use one that show some overlap with the pseudo-translation of the sentence from A in the last round. 4. The experiment is somewhat disappointing. Because it is monolingual translation, the proposed method could be applied to much larger datasets but the authors only tested on conventional small ones. How about train on the entire Wikipedia of English and French? 5. Another interesting question is how the method compare to training on parallel data. For example, how will the proposed method reduce the amount of parallel data needed to match the performance of fully parallel data training?