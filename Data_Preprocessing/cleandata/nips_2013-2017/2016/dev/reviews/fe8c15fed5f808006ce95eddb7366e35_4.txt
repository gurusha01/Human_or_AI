This paper proposes an algorithm to learn behavior based on long-term goals, with the prediction of player movement in basketball games as application. The core idea is to train a hierarchical model that predicts low-level actions by combining a so-called "micro-planner" (that uses only the current state as input) with a "macro-planner" (that also takes into account the estimated long-term goal). The estimation of the long-term goal requires initial seeding with heuristically-defined targets: for instance in basketball the authors use stationary points (when players stop moving) as such goals. The main experimental evaluation consists in comparing (with human judges) the quality of the completion of basketball player traces, vs. those generated by baselines. The idea of learning higher-level goal abstractions to drive agent behavior is definitely a very relevant one, and as far as I am aware the proposed approach to tackling this problem is new. I found however the "Related Work" section to be under-developed, with too few links to previous work in the areas of hierarchical reinforcement learning, goal recognition and trajectory planning. To me, the biggest issue with this submission is an overall lack of clarity. I was really lost my first time reading through, and some points only made sense to me near the end, so I had to re-read it again to actually understand what was going on. I did not realize until eq. 6-7 (p. 4) that it was actually supervised learning: the mentions of planning and reinforcement learning had thrown me off. I guess it can be argued that predicting long-term goals is a form of "planning", however calling the basic state-to-action predictor a micro-"planner" seems inappropriate to me, considering it is only trying to imitate expert trajectories with absolutely no look-ahead into the future. Here are additional details I found unclear: - Notations in general. Which quantities are scalars / vectors / matrices? What is the difference between bold st and non-bold st l.83? (l.160 really confuses me). Why is mg a probability on "m" in eq. 2 but a function of "a" in eq. 4? Why is at found in two probabilities in eq. 3? (I failed to understand eq. 3 and how it is used in the model) What is the index i l.133? (I guess a player, but please make it clear -- also i appears out of nowhere l.142 although it is not used in the equations above) What does the multiplication by \hat{a}_t in eq. 7 mean? (I believe I understand now but it is confusing) - Why is it important to "assume that macro-goals are relatively static"? - Is eq. 6 used at all? (it is said fine-tuning is done on eq. 7 only) Besides the above clarity issues, I also have some concerns on the overall methodology, that hopefully authors may address in their reply. As far as I can understand the predictor for the attention mask m only takes the predicted macro-goal g as input: it means the prediction will be the same for a given g, regardless of the position of the player! This seems like a significant issue to me, in particular for goals the player may approach from different sides (requiring moving in completely different directions). Also, during extrapolation, since the generated player trajectory is not the same as the one from the seeding sequence, the movements of the ball and other players may make no sense: as a result, the model may eventually be making predictions from very "unrealistic" situations never seen during training, and it is hard to tell how it would behave (in theory it would be possible for a model generating great-looking trajectories on realistic data to completely break in this experimental setup). I realize it is a hard problem and I have no magic answer to it, but I feel like this limitation should at least be mentioned. It would have also been interesting to investigate to which extent the input data beyond the considered player's trace impact the model's output. Other minor points / typos: - l.96 - "should simultaneously optimizes" - l.176 "we used an simultaneously predicted the next 4 micro-actions" - Fig. 5 caption: "which may be fixable by using additional game state information": it is not clear to me why additional game state information would fix this specific situation - Fig. 6 caption: "bottom-right" on the last line should be "bottom-left" - l.222: "We now inspect this our macro-planner" - The 4-line Conclusion is a bit weak, maybe consider merging it with Section 6? - "sub-sampling temporally at 4 Hz": does that mean keeping one frame out of four? That would make it 25/4 Hz Update after author feedback: Thanks for the response. I can indeed see why in the specific case of this basketball dataset, conditioning the mask on the state may not help (but in general I think it would make sense to do it). My overall feeling after reading other reviews and the author feedback is that the algorithm needs to presented in a clearer way, and be better motivated / evaluated against simpler or previously proposed methods. I appreciate the willingness to make the data public (but note that it makes it even more important to have clear quantitative benchmarks other people can compare against).