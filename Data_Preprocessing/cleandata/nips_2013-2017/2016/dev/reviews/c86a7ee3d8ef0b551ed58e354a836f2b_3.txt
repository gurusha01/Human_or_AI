In this paper the authors incorporate the Barzilai-Borwein (BB) step-size for variants of stochastic gradient descent such as SGD and SVRG. It is also introduced to SAG in the appendix. It is proven that the SVRG with BB step size converges linearly in expectation on mu-strongly convex function with Lipschitz continuous gradient as long as m is chosen sufficiently large, where m is an integer such that the full gradient is computed every m iterations. Experimental results show the proposed SVRG with BB step size tends to converge the step size towards the optimally tuned step size, resulting in a slightly low but similar convergence rate on three data sets. The results of the other variants of SGD with BB step size looks also promising. In the SVRG-I, which is claimed in this paper to be more efficient in practice than SVRG-II, two parameters, an integer m and the step size eta, need to be tuned to achieve linear convergence. The contribution of the paper is that it makes SVRG easier to use by introducing BB step size in the sense that we do not need to tune the step size anymore as long as m is taken sufficiently large. This paper will be improved if the sensitivity of m is studied empirically. The theorem tells that one can achieve the linear convergence with the rate of convergence of 1 - theta in terms of the number of epochs. If the convergence rate in terms of epoch is fixed, the time complexity will increase linearly in terms of m. If we set m twice, we expect twice slow convergence. If this is the case, tuning of m is as sensitive as tuning of the step size, and is not very nice.