This paper proposes a method for visual question answering, by updating the representation of the question iteratively such that it better discriminates the actual content in the image. This is a very interesting idea, reminiscent of the 20 questions game, where uncertainty is reduced iteratively. The idea is inspired by the neural reasoner [21] paper, but now applied to visual images. The neural network model relies on scertain pre-trained networks (CNN for images, GRU for text), which are augmented by a "reasoning" layer. A softmax layer gives the final answer. The model is evaluated on challenging real world datasets with good results. The paper is explained well, with several qualitative results in addition to quantitative evaluation. The implementation details give additional information on how to adjust learning rates etc. In order to improve the expository quality of the paper, some limitations and failure cases can be illustrated through figures. In this sense, the Figure 5 can be complemented by another figure showing limitations. This will provide guidance to future work. Specific ambiguities in the parsing of text or confusion between categories can be illustrated. Although the method is evaluated experimentally, it is still unclear why the "question update" iterations should converge. In fact, I feel they can very well oscillate between two conflicting interpretations of the image content. It will be nice if the authors comment on this case.