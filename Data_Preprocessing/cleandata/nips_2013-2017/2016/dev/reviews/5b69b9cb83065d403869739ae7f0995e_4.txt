This paper formulates machine translation as a bidirectional reinforcement learning problem using RNN translation models and two corpora (which do not have to be aligned). The objective is to minimize the linear combination of two losses from 1) directly the language corpora and 2) how consistent the recovered message is (communication). This is a new idea that allows for the possibility of translation without aligned parallel corpora. I think that this could also open up new ideas in other fields as well. Therefore, the novelty factor is there. I would like to see some more data on the selection of $\alpha$, which is the weighting factor of the two losses. For example, why was alpha = 0.01? Was it because the strength of the communication loss was naturally much higher? Or was communication consistency more of an impact?