This paper proposed Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering. The framework utilized CNN models for extracting visual features and RNN models for language information processing. Then a MRN model can effectively learns the joint representation from visual and language information. They achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. They also introduce a novel method to visualize the attention effect of the joint representations. The MRN is effective for Visual QA dataset or visual question-answering task. However, contributions are little simple. As far as I know, in order to learn the joint representation, the using of DNN is very common. This paper did not analyze which component of the framework contributes the most. Is CNN or RNN models that they have used or the MRN? If there is any proof or discussion, this will be an impressive paper.