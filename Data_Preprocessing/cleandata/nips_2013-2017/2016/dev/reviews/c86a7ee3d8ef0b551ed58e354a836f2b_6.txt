This work proposes to use Barzilai-Borwein method to compute learning rate for stochastic gradient method and its variants. It provides the linear convergence proof of SVRG-BB method, however, there is not any convergence analysis about SGD-BB method. This work proposes a step size learning method for SGD and its variants. This kind of work is missing for SVRG, while there are many works about step size learning for SGD. Here is a list of my concerns: 1. In Fig 1 (d,e,f), it is interesting to see that the BB step size converges to a constant value. However, in the paper: Allen-Zhu, Zeyuan, and Elad Hazan. "Variance reduction for faster non-convex optimization.", Fig 5 shows that a best tuned decreasing learning rate has faster convergence rate for SVRG. Can you explain why it converges to a constant? 2. There are many step size learning works for SGD. In the experiment of SGD-BB section, I think there should be more comparing methods, like Adam, adagrad.