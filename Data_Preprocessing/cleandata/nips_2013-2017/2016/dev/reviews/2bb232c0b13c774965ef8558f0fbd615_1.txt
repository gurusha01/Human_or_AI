The authors have developed an end-to-end system for parsing hand written text from paragraph (multiple horizontal lines) images. They achieve this by adding an attention mechanism to the single-line parser of Graves et al. [18] and then decoding the text using Bidirectional-LSTMs (instead of softmax as used originally by Graves et al). They evaluate recognition performance on the Rimes and IAM datasets. The performance is comparable to the state-of-the-art methods on Rimes, but lags behind on IAM. This is a very nice application of guided attention over an image for identifying a sequence of handwritten text lines (as the authors correctly point out, this builds on the approach of Xu et al [38] and others, who have used region attention over an image). On the exposition, the paper could be written to be more self-contained. It is difficult to understand the model as presented in the paper without first referring to Graves et al. [18]. It would help to improve section 3 if the MD-LSTM model is explained in more detail (as in [18]). There are a few peculiar results and statements that should be explained by the authors: - why does the proposed approach give better performance than the methods that use ground truth line labelling (in table 2 & section 5.3)? Is it because the line breaks in the ground truth are used to re-initialize the decoder RNN, but this re-initialization does not happen in the character sequence of the attention mechanism? More details on the authors' implementation of Graves et al should be provided to answer this question. - line 142, the authors say their method is 20-30x faster than the character decoder method of Bluche et al. [6], but do not provide the actual time for comparison. - In terms of results, the performance is superior to the state of the art (for one setting) for CER% on the Rimes dataset, but substantially worse on the IAM dataset, probably due to lack of training data. A simple experiment to test this would be to first train the model on the test+training data of the other dataset and then finetune on the IAM training data. I suggest the authors add this experiment. - line 213, why does the attention mechanism miss punctuation?