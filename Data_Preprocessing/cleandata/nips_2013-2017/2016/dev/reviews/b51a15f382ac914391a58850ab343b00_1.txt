The authors deal with the stochastic combinatorial partial monitoring problem. In this setting, at each round an unknown vector theta is generated from an unknown fixed distribution, the operator chooses an arm among possibly infinitely many and receives a reward which is a known function of the arm and theta. Instead of receiving the reward as feedback or the full vector theta, the operator receives a known arm-dependent linear transformation of theta and tries to minimize its regret. The authors apply the Phased Exploration and Greedy Exploitation strategy to produce a first algorithm. This strategy consist of√í alternating between full exploration and greedy exploitation phases of predetermined length. They also present a version of this algorithm that first tries to estimate the gap. They then provide analysis for both algorithms and prove both problem-dependent and problem-independent regret bounds. These regret bounds enjoy the same complexity over T than previous work but are independent of the number of arms and do not require the assumption of a unique optimal arm in the problem-independent-bound case. They finally present an example of application to online ranking. The paper is clear and well presented. The ideas to use the PEGE framework to completely get rid of dependency on the number of arms and to allocate samples to the gap estimation to get rid of the square in the problem dependent regret bound are interesting. I suggest to accept this paper. I have a few question for the authors. Q: You do not mention lower bounds for the regret. Do you know such bounds for the stochastic CPM problem and if so, how do they compare to your bound ? Q: You do not elaborate on the choice of the global observable set. Do you know a way to do a smart choice of the global observable set sigma so that \beta_\sigma is low ? AFTER I have read the rebuttal and kept my scores