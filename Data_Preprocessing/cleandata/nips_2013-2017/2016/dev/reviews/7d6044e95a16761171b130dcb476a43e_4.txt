The paper presents a method for generalizing variational autoencoders (VAEs) to structured graphical models with general, non-linear observation models. The main idea of the approach is to use the VAE framework to predict potential functions rather than the variational parameters themselves to facilitate inference on structured latent representations. This is a good idea, and as far as I can ascertain, all claims (bounds, etc.) hold. However the manuscript in it's present form is a little hard to follow. For example The main objective, L{SVAE}, is never even introduced in the main body of the text, and important proofs are scattered throughout the lengthy supplementary material---I think the authors can definitely improve the exposition substantially with some effort. A tighter correspondence around the algorithm description and an expanded exposition in the appendix might be useful where appropriate. Perhaps Figure 6 could go in the appendix to make some room for further explanations. The experimental results are proof of concept I suppose, but after getting excited about the new machinery, it was a let-down to see the model exercised only on toy data and mouse depth data, and compared to other methods only on synthetic spiral data. I hope the authors will be able to demonstrate their model on a more involved and well-known task if the paper gets accepted. Notes: - Appendix, eq (28) constant terms left in posterior.u - Sum-product networks represent discrete sub-structures and are efficient... line 86- zn should be z_{n+1} according to Figure 2...