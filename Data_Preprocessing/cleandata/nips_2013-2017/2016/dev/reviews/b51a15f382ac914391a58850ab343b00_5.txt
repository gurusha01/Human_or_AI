The paper present two algorithms for a relatively new settings called combinatorial partial monitoring CPM. This setting is a combination of finite partial monitoring and combinatorial multi-armed bandit. The paper presents two algorithm against a stochastic adversary and provide theoretical regret bound (both the distribution dependent and independent one) for each of them. The first algorithm called PEGE combines the classical forced-exploration in MAB together with globally observable set to solve CPM. The second algorithm (PEGE2) first try to estimate the gap \Delta between the expected reward of the first best action (assumed unique) and that of the second best action. Then, this estimate is fed to PEGE. Finally, the paper discusses how this setting can be used in online ranking with feedback. The main improvement in the paper is to provide an algorithm that removes the assumption of the existence of a unique optimal action while still achieving the state-of-the art regret up to logarithmic factors. The settings considered in the paper is nice and the provided algorithms are simple and well designed. I like the PEGE algorithm as it removed the non-reasonable assumption of a unique optimal action. 1- My only question is: Intuitively why would we get a problem dependent bound of O(log T) for PEGE2 if we set T_0 to O(T^{2/3})? 2- It will also be good to discuss this intuition in the paper