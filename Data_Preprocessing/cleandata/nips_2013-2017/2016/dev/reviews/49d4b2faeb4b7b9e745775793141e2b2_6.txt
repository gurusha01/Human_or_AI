This paper proposes a new class of architectures, named Matryoshka Networks (MatNets), which combine the benefits of DRAW-like models and Ladder Networks to learn hierarchically deep generative models with jointly-trained inference/generation. The paper shows the quantitative performance of MatNets on MNIST, Omniglot, and CIFAR10. The paper proposes an interesting framework to learn deep generative models. The authors show nice results on imputing missing regions of images. However, it would be more convincing if the paper includes classification results (supervised/semi-supervised/unsupervised) on benchmarks such as MNIST and CIFAR10. The paper is well-written. The claims, explanation, and derivation in the paper are clear and easy to follow.