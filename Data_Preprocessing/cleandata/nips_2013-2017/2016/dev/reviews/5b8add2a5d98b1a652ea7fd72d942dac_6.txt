The authors provide an algorithm for solving a random system of quadratic equations using truncated gradient updates. The method minimizes the least squares criterion based on the amplitude based empirical loss. The truncation is different than Chen, Candes 2015 in the sense that large-sized gradients are not thrown away. Authors theoretically analyze the noiseless case and support their analysis via simulations. Definition 1 (generalized gradient) is not referred to in main text. Where is this definition used, why is this needed? In line 145, do you mean "sufficient" instead of necessary? Equation 9 can be presented much better so that definition of h is clear to the reader without too much effort. This is the first time h appears, and it is not explicitly defined anywhere. In line 154, authors invoke strong law of large numbers, but both fixed points x and z^* depend on the data, hence there is dependence among the summands. Also, why is h a good direction? Please elaborate on this. In line 164, how much does the statement rely on the gaussian data assumption? Authors do not mention about the noisy case in Section 3. Can you generalize the main theorem to the noisy case? Or is there a technical challenge in this case? In any case, a comment about this would be helpful for future research. Does the proposed algorithm work on datasets that have distributions far from Gaussian? A comment is necessary. The authors should test the algorithm on real datasets to validate its performance, i.e. image recovery example in TWF paper.