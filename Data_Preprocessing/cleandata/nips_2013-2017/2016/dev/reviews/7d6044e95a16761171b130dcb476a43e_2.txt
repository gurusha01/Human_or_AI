The proposed approach essentially introduces a finite mixture model, where each component distribution is a Gaussian latent variable model, the likelihood means and covariances of which are parameterized via neural networks. The main concept of the paper is not different from existing deep generative model (DGM) formulations where the postulated likelihood is a finite mixture of Gaussians. What differentiates this work from existing DGMs concerns the postulated latent variable posteriors: Instead of considering a Gaussian posterior (conditional on the mixture component) that is parameterized via deep neural networks, the authors consider here a conventional formulation, where a set of Normal-Wishart hyper-priors is further imposed over the Gaussian posterior mean and precision matrix. I have to admit that I've failed to understand why the authors drop variational posterior amortization to opt for this more simplistic solution. Using amortized variational posteriors is now the state-of-the-art approach in the literature, and is proven to allow for better modeling and inferential performance. The authors should have motivated this selection a lot more extensively. The presented experiments fail to provide substantial empirical evidence to support the efficacy and the usefulness of the approach. 
They are both limited to some simulated datasets and not exhaustively compared to the state of the art.