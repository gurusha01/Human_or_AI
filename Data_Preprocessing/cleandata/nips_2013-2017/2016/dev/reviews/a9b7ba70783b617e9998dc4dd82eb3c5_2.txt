The cluster tree of a probability density function provides a hierarchical representation of its high density regions. Given an iid sample from an unknown density p0, the paper discusses statistical inference for the cluster tree associated with p0. Specifically, a bootstrap-based confidence set for the cluster tree (of a closely related object to p0) is proposed. The shape of the confidence set is governed by a metric defined on cluster trees, which is amenable for the task. While the confidence set is given in some sense implicitly, the authors propose methods for obtaining "informative" representatives in the confidence set, that retain "statistically significant" features of the cluster tree. I found the paper original and interesting. I am wondering what relationships exist to nonparametric inference for density functions: I assume that the task of constructing a confidence set for the cluster tree of p0 is quite different from constructing a confidence set for p0 itself, which justifies addressing the former problem directly instead of going through the latter (and taking {Tp: p \in C\alpha} where C\alpha is a confidence set for p0). For example, the focus on ph (rather than p0) is justifiable as far as T{ph} compares with T{p0} but not necessarily as far as ph compares with p0. A few comments: 1. Since the confidence set is implicit, the main concrete contribution I see in this work, is that you have a way to prune the tree while maintaining statistical guarantees; maybe this is worth emphasizing. 2. If I wanted to test H0: T{ph} = T0, I could in principle use an arbitrary statistic instead of d{\infty}, and could use this statistic to form a confidence set by inverting the tests (as you propose). The choice of d_{\infty} (and, in fact, of any other metric you discussed) seems a little arbitrary to me. I guess senses of "optimality" of this statistics are beyond the scope of the paper, but are there further justifications of using it?