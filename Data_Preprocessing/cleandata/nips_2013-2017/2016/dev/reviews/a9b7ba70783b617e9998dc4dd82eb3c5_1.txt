The authors estimate the cluster tree of the unknown density of an observed sample. The estimator is the cluster tree of a kernel density estimator. The accuracy is measured in terms of the sup-norm of the underlying density, which is shown to be equal to a natural metric on the class of clustering trees. Confidence sets are proposed, based on a bootstrap procedure. Then, a procedure is defined in order to exhibit a tree in that confidence set, which has a simpler structure than the cluster tree of the kernel density estimator, yet still shows important topological features of that estimator. Experiments results on both simulated and real data are provided. 1. Reading this work is pleasant, and the reader can learn many things. However, the main result is a little deceiving: although the authors write, in the introduction, that estimating the cluster tree instead of the density itself overcomes the curse of the dimension, they actually have no guarantee for the accuracy of their estimator. Actually, they do not estimate T{p0} but, instead, T{ph} for a small but arbitrary bandwidth h. In order to estimate T{p0} accurately, the bandwidth h should be chosen small enough, i.e., smaller than some h0, but the value of h0 is unknown (this is mentioned in line 192). Hence, the curse of dimension is actually not avoided at all. 2. In Theorem 3, which is not proven, I am surprised to find no dependency in B. 3. To my opinion, all mentions to d_{MM} should be omitted.