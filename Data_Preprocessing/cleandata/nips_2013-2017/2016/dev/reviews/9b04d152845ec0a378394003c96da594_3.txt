This paper proposed a Mutimodal method based residual network for visual question-answering. As a multimodal method, this work is able to learn the joint representation from visual and language information. The state-of-art results on the data set demonstrate the effectiveness of this proposed work. More than that, the introduced method to visualize attention effects of the joint representation is interesting. Overall, this paper is well-written and easy to read. Proposed idea has been validated using relevant experiments. There are some latest papers on VQA topic, if possible, please make some comparisons with the latest published papers. For example, ICML 2016 "Dynamic Memory Networks for Visual and Textual Question Answering" This paper proposed a Mutimodal method based residual network for visual question-answering. As a multimodal method, this work is able to learn the joint representation from visual and language information. The state-of-art results on the data set demonstrate the effectiveness of this proposed work. More than that, the introduced method to visualize attention effects of the joint representation is interesting and seems effective. There are some latest papers working on Visual Question-Asnwering, for example, ICML 2016 "Dynamic Memory Networks for Visual and Textual Question Answering". It is suggested to make comparisons with these latest methods.