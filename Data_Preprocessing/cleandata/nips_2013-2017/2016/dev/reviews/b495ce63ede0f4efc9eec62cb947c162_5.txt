This paper presents a novel architecture for learning the visual component correspondence between objects of the same category in different pictures. The fully convolution architecture makes use of a contrastive loss designed to efficiently reuse computation through the convolutional layers and a modified spatial transformer module to adaptively transform each input patch. Both features are novel and are crucial for the efficiency and effectiveness of the model. The paper is well-written with clear explanations. The use of a fully convolutional architecture is well-justified as it greatly improves training efficiency compared to the triplet loss and the contrastive loss. The hard negative mining technique is essential in deep metric learning. There are some minor typos on lines 49 and in the caption for Figure 2. The authors used the term semantic correspondence numerous times. However, there is no evidence that the model learns any semantic meaning of parts of visual objects, but rather learns to extract abstract visual features that are shared across multiple instances of the same object part. The term geometric correspondence is more suitable. Experimental results are qualitatively unimpressive. In Figure 7, the HN-ST model fails to match the dog, boat and bus. This is further evidence that the model does not learn semantic correspondences since these three example pairs all have very different visual perspectives between the query and the test samples.