The authors propose an automatic step size scheme using the Barzilai-Borwein method for SGD and SVRG. Linear convergence analysis is provided for SVRG-BB and SVRG-I (option 1 of SVRG where the current iterate is used as the mark iterate on the next epoch). A smoothing technique is required on top of the BB step size for SGD-BB to converge, but no convergence analysis is provided for SGD-BB. The authors compare SVRG-BB to SVRG with a constant step size, and SGD-BB to SGD with a decreasing step size scheme, and find that the BB step sizes have a comparable rate of convergence to the best tuned step size for both SVRG and SGD. EDIT: I have increased my ratings based on the author's response. This paper addresses an important problem: that of automatically determining a step size sequence for stochastic methods like SGD. The authors use a well-known method, the Barzilai-Borwein method, to automatically determine step sizes for SGD and SVRG. While the results are promising, there are two major shortcomings of this paper: 1. No convergence rate analysis for SGD-BB is provided. While the authors rightly mention that a decreasing step size is required for convergence of the SGD method, it is important to theoretically study how the BB step size affects the convergence rate of SGD. Can the authors comment on this? Moreover, the optimization function used for smoothing the SGD step sizes (equation 13) seem quite heuristic, with no theoretical justification. 2. The numerical experiments performed are not very extensive. The authors only perform experiments on binary classification models. It would be interesting to see how the step sizes perform on regression problems, and non-convex problems like neural networks. Moreover, the authors do not compare SGD-BB to other automatic step size methods for SGD (see for references [1] and [2]). There is no discussion in the paper about how the BB step sizes compare to existing automatic step size methods. For example, is this method faster than the other proposed methods, or does it use lesser memory? The convergence analysis for SVRG-BB and SVRG-I is simple, but interesting. However, the application of a BB step size on SVRG is not a big contribution since SVRG calculates true gradients after each epoch. Thus, the BB step size can be trivially applied on SVRG, as demonstrated in the paper. Overall, while the paper shows good results and is addressing a very important problem in methods like SGD, I think it requires a bit more work. In particular, the paper would benefit from a convergence rate proof for SGD-BB, as well as slightly more extensive experiments. Some minor comments: 1. Typo in line 13 of supplementary: \eta -> \eta_k. 2. Bounds in line 9 of supplementary can be improved using the same technique as in the original SVRG paper 3. How sensitive is SGD-BB to m? 4. In SVRG-BB, the step sizes initially seem to be quite low. This results in slightly slower convergence at first. Do the authors have any intuition about why this might be happening? If the initial slowdown can be avoided, SVRG-BB might be able to match the performance of the best-tuned step size for SVRG. [1] Mahsereci, Maren, and Philipp Hennig. "Probabilistic line searches for stochastic optimization." Advances In Neural Information Processing Systems. 2015. [2] Schaul, Tom, Sixin Zhang, and Yann LeCun. "No more pesky learning rates." ICML (3) 28 (2013): 343-351.