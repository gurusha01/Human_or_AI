The authors propose a an architecture for deep generative models that enable deeper models to be learned without forfeiting trainability. The improvement in trainability is believed to arise from the inclusion of lateral, shortcut, and residual connections within the network. The proposed architecture yields state-of-the-art performance on several benchmark datasets. The authors provide reasonable justification for the architecture proposed, taking key insights from successes in recent work on deep recurrent networks and semi supervised ladder networks. The results reported are indeed impressive. While the premise of the paper is centered on the trainability of the proposed networks, evidence for their trainability comes only in the form of performance metrics on various benchmarks. If the trainability of these networks is a primary contribution of this work, further analysis thereof, or experiments specifically highlighting trainability would be desirable.