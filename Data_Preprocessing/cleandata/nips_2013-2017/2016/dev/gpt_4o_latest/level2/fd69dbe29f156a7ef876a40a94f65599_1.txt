The paper presents a novel neural network-based approach for visual question answering (VQA) that iteratively updates question representations by interacting with image regions through reasoning layers. The authors claim that their model achieves state-of-the-art performance on the COCO-QA and VQA datasets by leveraging object proposals, attention mechanisms, and end-to-end training. The main contributions include a reasoning network for iterative question representation updates, the integration of object proposals to focus on relevant image regions, and the use of attention mechanisms to enhance reasoning.
Strengths:
1. Technical Soundness: The paper is technically robust, with a well-defined architecture that builds on the neural reasoner framework and incorporates attention mechanisms effectively. The use of object proposals and spatial coordinates is a thoughtful design choice that enhances the model's ability to reason about image content.
2. State-of-the-Art Results: The model demonstrates superior performance on both COCO-QA and VQA datasets, outperforming prior approaches like the stacked attention network (SAN) in several categories. The ablation studies and comparisons with state-of-the-art methods provide strong empirical support for the claims.
3. Clarity and Organization: The paper is well-written and organized, with clear explanations of the architecture, experimental setup, and results. The inclusion of qualitative analyses, such as attention visualizations, adds interpretability and strengthens the paper's contributions.
4. Novelty: The iterative updating of question representations through reasoning layers and the use of object proposals for VQA are innovative contributions. The approach addresses limitations of prior methods that rely on global image features or coarse attention mechanisms.
5. Practical Relevance: The proposed method has practical implications for applications like human-computer interaction and assistive technologies, as it improves the ability to answer complex visual questions.
Weaknesses:
1. Counting Ability: The paper acknowledges that the model struggles with counting tasks, which is a significant limitation given the prevalence of such questions in VQA datasets. While this is noted, no concrete solutions are proposed to address this issue.
2. Dataset Bias: The evaluation is limited to COCO-QA and VQA datasets, which may not fully represent the diversity of real-world VQA scenarios. Testing on additional datasets could strengthen the generalizability of the results.
3. Limited Discussion of Failure Cases: While the paper highlights its strengths, it provides limited analysis of failure cases or scenarios where the model underperforms. A deeper exploration of these cases could offer insights for future improvements.
4. Computational Complexity: The use of multiple reasoning layers and object proposals may introduce computational overhead, but the paper does not discuss the efficiency or scalability of the approach.
Recommendation:
I recommend acceptance of this paper, as it makes significant contributions to the VQA field with a novel and effective approach. While there are areas for improvement, such as addressing counting tasks and providing a broader evaluation, the strengths of the paper outweigh its weaknesses. The proposed method advances the state of the art and offers a solid foundation for future research in VQA. 
Pro/Con Summary:
Pros: 
- Technically sound and innovative approach.
- State-of-the-art results on benchmark datasets.
- Clear and well-organized presentation.
- Practical relevance and interpretability through attention visualizations.
Cons: 
- Limited ability to handle counting tasks.
- Evaluation restricted to two datasets.
- Lack of detailed analysis of failure cases.
- Potential computational overhead not addressed.
Overall, this paper is a valuable contribution to the field and aligns well with the goals of the conference.