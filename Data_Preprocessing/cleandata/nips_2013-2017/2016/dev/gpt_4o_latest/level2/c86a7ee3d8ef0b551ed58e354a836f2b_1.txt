The paper proposes a novel approach to automatically compute step sizes for stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) methods using the Barzilai-Borwein (BB) method, leading to two algorithms: SGD-BB and SVRG-BB. The authors claim three primary contributions: (1) introducing BB step sizes to SGD and SVRG, (2) proving the linear convergence of SVRG-BB for strongly convex functions (and, as a by-product, proving the linear convergence of SVRG-I, which was previously unestablished), and (3) demonstrating the superior or comparable performance of SGD-BB and SVRG-BB to their counterparts with best-tuned step sizes through numerical experiments.
Strengths:
1. Novelty and Practicality: The use of the BB method to dynamically compute step sizes for SGD and SVRG is innovative and addresses a critical challenge in stochastic optimizationâ€”manual tuning of step sizes. This is particularly valuable for practitioners who often struggle with hyperparameter tuning.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis, including proving the linear convergence of SVRG-BB and SVRG-I. This fills a gap in the literature, as the convergence of SVRG-I was previously unproven.
3. Experimental Validation: The numerical experiments on standard datasets (logistic regression and SVM tasks) convincingly demonstrate that SGD-BB and SVRG-BB achieve comparable or better performance than SGD and SVRG with best-tuned step sizes. The results also show that the algorithms are robust to the choice of initial step sizes.
4. Generality: The proposed BB step size computation can be incorporated into other SGD variants, enhancing its applicability across a broad range of optimization problems.
Weaknesses:
1. Limited Scope of Experiments: While the experiments are well-executed, they are limited to a few datasets and problem types (logistic regression and SVM). It would be beneficial to test the algorithms on more diverse tasks, such as deep learning models, to validate their generalizability.
2. Computational Overhead: Although the authors claim that the per-iteration cost of SGD-BB and SVRG-BB is comparable to their base methods, the additional computations for BB step size and smoothing might introduce overhead in large-scale applications. This aspect is not thoroughly analyzed.
3. Lack of Discussion on Limitations: The paper does not explicitly discuss the potential limitations of the proposed methods, such as scenarios where the BB step size might fail or perform suboptimally (e.g., non-convex settings).
4. Clarity of Presentation: While the paper is generally well-organized, some sections (e.g., the smoothing technique for SGD-BB) are dense and could benefit from clearer explanations or visual aids.
Recommendation:
The paper makes a significant contribution to stochastic optimization by introducing a practical and theoretically grounded method for automatic step size computation. The novelty, theoretical rigor, and experimental results make it a strong candidate for acceptance. However, the authors should address the computational overhead, expand the experimental scope, and explicitly discuss limitations in a revised version.
Pro Arguments:
- Novel and practical approach to step size computation.
- Strong theoretical contributions, including filling a gap in the literature.
- Promising experimental results demonstrating robustness and efficacy.
Con Arguments:
- Limited experimental scope.
- Potential computational overhead not analyzed.
- Lack of explicit discussion on limitations.
Overall Rating: Accept with minor revisions.