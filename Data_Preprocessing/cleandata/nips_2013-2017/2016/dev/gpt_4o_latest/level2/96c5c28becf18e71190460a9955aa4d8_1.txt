This paper investigates the problem of learning decision thresholds in the drift-diffusion model (DDM) of perceptual decision-making, proposing two distinct methods: a reward-modulated reinforcement learning rule based on Williams' REINFORCE algorithm and Bayesian optimization using Gaussian processes. The authors aim to address the challenge of optimizing highly stochastic single-trial rewards derived from Wald's cost function, a problem that standard methods fail to solve effectively. The paper's main claims are that both methods can successfully learn near-optimal decision thresholds, with Bayesian optimization converging faster but exhibiting higher variance and computational cost, while the REINFORCE method aligns better with animal learning behavior.
Strengths:
1. Novelty and Contribution: The paper makes a significant contribution by addressing the underexplored problem of learning decision thresholds in DDMs, combining insights from neuroscience and machine learning. The use of REINFORCE as a biologically plausible learning rule and Bayesian optimization as a machine learning approach is innovative.
2. Experimental Validation: The authors validate their methods against exhaustive optimization and demonstrate their scalability to two thresholds. The comparison of REINFORCE with animal learning behavior is particularly compelling, as it bridges computational modeling with experimental neuroscience.
3. Clarity of Results: The results are well-presented, with clear comparisons between the two methods in terms of convergence speed, variance, and computational cost. The inclusion of both single and multiple learning episodes strengthens the analysis.
4. Relevance: The work is highly relevant to both the neuroscience and machine learning communities, addressing fundamental questions about decision-making and reinforcement learning.
Weaknesses:
1. Computational Cost of Bayesian Optimization: While the authors acknowledge the high computational cost of Bayesian optimization, they do not explore potential optimizations, such as incremental updates to the Gaussian process.
2. Limited Exploration of REINFORCE Variants: The paper mentions potential improvements to the REINFORCE method, such as incorporating context signals or retaining past trial performance, but does not implement or test these ideas.
3. Simplified Experimental Setup: The validation primarily focuses on symmetric decision thresholds and does not explore more complex scenarios, such as dynamic thresholds or multi-choice decision-making.
4. Animal Learning Comparison: While the REINFORCE method aligns qualitatively with animal learning data, the discrepancy in the initial learning phase (random choices in animals vs. accurate choices in the model) is not fully addressed.
Arguments for Acceptance:
- The paper addresses a novel and challenging problem with clear implications for both neuroscience and machine learning.
- The methods are well-validated and provide actionable insights into the trade-offs between speed, accuracy, and computational cost.
- The REINFORCE method's alignment with animal learning behavior adds significant value and interdisciplinary relevance.
Arguments Against Acceptance:
- The computational inefficiency of Bayesian optimization and the lack of exploration of REINFORCE variants limit the practical applicability of the methods.
- The experimental setup could be extended to more complex and realistic scenarios.
Recommendation:
Overall, this paper is a strong contribution to the field, with clear novelty, rigorous validation, and interdisciplinary relevance. While there are areas for improvement, particularly in optimizing the methods and extending the experimental setup, the paper's strengths outweigh its weaknesses. I recommend acceptance, with minor revisions to address the computational inefficiencies and expand the discussion of potential improvements to the REINFORCE method.