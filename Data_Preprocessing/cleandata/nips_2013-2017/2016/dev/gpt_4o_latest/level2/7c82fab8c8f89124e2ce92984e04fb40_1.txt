The paper presents a novel framework for selective inference in the group-sparse regression setting, enabling the construction of confidence intervals and p-values for selected groups of variables. The authors extend existing selective inference techniques, such as the polyhedral lemma of Lee et al. (2016), to the group-sparse setting, introducing the "truncated projection lemma" as a key technical contribution. This result characterizes the distribution of the magnitude of the projection of data onto a subspace, conditioned on the selection event, and allows for inference on group-sparse models such as the group lasso, iterative hard thresholding (IHT), and forward stepwise regression. The paper also demonstrates the practical utility of the proposed methods through experiments on simulated data and real-world health data.
Strengths:
1. Technical Novelty: The truncated projection lemma is a significant theoretical advancement, extending selective inference techniques to the group-sparse setting. This fills an important gap in the literature, as previous methods were limited to non-grouped sparse regression.
2. Broad Applicability: The framework is applied to multiple group-sparse model selection methods, demonstrating its versatility. The authors provide detailed derivations for forward stepwise regression, IHT, and the group lasso, which are widely used in practice.
3. Practical Relevance: The experiments on simulated and real-world data illustrate the utility of the proposed methods. The results are compelling, showing that the framework can provide valid p-values and confidence intervals while accounting for the data-dependent nature of model selection.
4. Clarity of Presentation: The paper is well-organized and provides sufficient theoretical and empirical details to reproduce the results. The authors clearly explain the challenges of selective inference in the group-sparse setting and how their approach addresses these challenges.
Weaknesses:
1. Computational Complexity: The proposed method for the group lasso relies on numerical approximations and importance sampling, which may be computationally expensive for large datasets or high-dimensional problems. This could limit its scalability.
2. Limited Real-World Validation: While the experiments on California health data are interesting, the paper would benefit from additional real-world applications to demonstrate the robustness and generalizability of the approach.
3. Assumptions on Known Variance: The framework assumes that the noise variance is known, which may not always hold in practical settings. The authors could discuss potential extensions to handle unknown variance.
4. Comparison to Alternatives: While the paper references prior work, it does not provide a direct empirical comparison to alternative selective inference methods, such as those by Loftus and Taylor (2015), to highlight the advantages of the proposed approach.
Recommendation:
This paper makes a significant contribution to the field of selective inference and group-sparse regression. The theoretical advancements and practical tools it provides are likely to be of interest to researchers and practitioners working on high-dimensional regression problems. However, the computational challenges and limited real-world validation should be addressed in future work. Overall, I recommend acceptance, with the expectation that the authors will refine their discussion of limitations and explore additional applications in subsequent iterations.
Pro Arguments for Acceptance:
- Significant theoretical contribution (truncated projection lemma).
- Broad applicability to multiple group-sparse methods.
- Practical utility demonstrated through experiments.
Con Arguments for Acceptance:
- Computational limitations for the group lasso.
- Limited real-world validation and lack of empirical comparisons.
Rating: 7/10 (Good paper with minor weaknesses).