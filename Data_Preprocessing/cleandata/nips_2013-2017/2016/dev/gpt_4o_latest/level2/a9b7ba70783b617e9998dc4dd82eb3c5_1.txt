The paper presents a novel approach to statistical inference for cluster trees, which are hierarchical representations of high-density clusters in a dataset. The authors focus on quantifying uncertainty in empirical cluster trees by constructing confidence sets for the true cluster tree and pruning statistically insignificant features. They propose metrics to compare cluster trees, analyze their properties, and develop methods for visualizing simpler, interpretable trees. The paper demonstrates the utility of these methods on synthetic datasets and a real-world Graft-versus-Host Disease (GvHD) dataset.
Strengths:
1. Novelty and Contribution: The paper addresses a significant gap in clustering literature by introducing statistical inference methods for cluster trees. While prior work has focused on estimation, this paper is the first to propose confidence sets and pruning methods for cluster trees, making it a valuable contribution.
2. Theoretical Rigor: The authors provide a detailed theoretical foundation for their methods, including the use of metrics like the `d∞` metric for statistical inference and the introduction of a partial ordering to simplify trees. The proofs and guarantees (e.g., asymptotic validity of confidence sets) are well-grounded.
3. Practical Relevance: The pruning methods enhance interpretability by removing noise-induced features, which is crucial for real-world applications. The experiments on synthetic datasets and the GvHD dataset effectively demonstrate the practical utility of the proposed methods.
4. Clarity of Results: The visualizations of pruned trees and the comparison between control and positive samples in the GvHD dataset highlight the interpretability and applicability of the proposed techniques.
Weaknesses:
1. Metric Limitations: While the `d∞` metric is computationally tractable and statistically robust, the authors acknowledge that it may not fully capture the nuances of tree structures compared to the modified merge distortion metric (`dMM`). The inability to construct confidence sets using `dMM` limits the scope of the work.
2. Bandwidth Selection: The reliance on the Silverman reference rule for bandwidth selection is a limitation, as this rule is not tailored for tree inference. A more targeted bandwidth selection method would enhance the robustness of the results.
3. Complexity of Confidence Sets: The confidence sets are infinite and contain highly complex trees, which may hinder interpretability. While pruning simplifies the trees, the authors do not provide a method to identify all minimal trees in the confidence set, leaving room for improvement in summarizing results.
4. Scalability: The computational feasibility of the proposed methods for large-scale, high-dimensional datasets is not thoroughly discussed. This could be a concern for practical adoption.
Recommendation:
The paper is a strong contribution to the field of clustering and statistical inference, addressing a critical gap with innovative methods. However, the limitations in metric selection, bandwidth optimization, and scalability warrant further exploration. I recommend acceptance, contingent on addressing these concerns in future work.
Pro and Con Arguments for Acceptance:
Pro:
- Introduces a novel and impactful approach to statistical inference for cluster trees.
- Provides rigorous theoretical guarantees and practical pruning methods.
- Demonstrates utility on both synthetic and real-world datasets.
Con:
- Limited use of advanced metrics like `dMM` for inference.
- Bandwidth selection and scalability issues remain unresolved.
- Confidence sets are complex and not fully summarized.
Overall, the paper advances the state of the art in density-based clustering and statistical inference, making it a valuable addition to the conference.