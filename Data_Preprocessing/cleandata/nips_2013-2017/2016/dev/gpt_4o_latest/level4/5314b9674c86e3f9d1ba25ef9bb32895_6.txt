This paper presents an efficient algorithm for image classification. The authors illustrate how tensor network optimization algorithms can be adapted to supervised learning tasks through the use of matrix product states. Initially, the authors map the high-dimensional vectors \( X \) into an even higher-dimensional space using a feature map \( \Phi(x) \), and then employ a decision function \( f(x) = W \times \Phi(x) \) to classify these vectors. Since both the feature vector \( \Phi(x) \) and the weight vector \( W \) can grow exponentially large or even become infinite, the authors propose a novel approach that diverges from the kernel trick (reference 9). Specifically, this method leverages a tensor network to approximate the optimal weight vector \( W \) (Eq. 5). By extracting latent information from the trained model and exploiting the structural properties of \( W \), the authors develop a "sweeping" optimization algorithm (Eq. 7) to minimize a quadratic cost function associated with the classification task.
The paper is well-structured and clearly written, and the proposed method is innovative, offering a significant contribution to the field of machine learning. Establishing a connection between tensor networks and feature selection is particularly noteworthy. However, the experimental evaluation is a major limitation of this work. To validate the proposed method, the authors rely solely on a single dataset, MNIST, achieving a test set classification error of less than 1%. It would strengthen the paper if the authors applied their approach to additional, widely-used datasets in the field and presented the corresponding results. Furthermore, the lack of comparison with state-of-the-art learning algorithms and feature selection techniques makes it challenging to assess the relative performance of the proposed method. I encourage the authors to address these concerns during the rebuttal process.
In conclusion, my overall recommendation for this paper is a weak accept.