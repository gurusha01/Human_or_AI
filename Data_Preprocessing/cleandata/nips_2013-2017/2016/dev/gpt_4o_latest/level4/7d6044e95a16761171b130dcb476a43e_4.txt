The paper introduces a method to extend variational autoencoders (VAEs) to structured graphical models with general, non-linear observation models. The core idea is to leverage the VAE framework to predict potential functions instead of directly predicting the variational parameters, thereby enabling inference on structured latent representations. This is a promising approach, and based on my assessment, the theoretical claims (e.g., bounds) appear to be valid. However, the current manuscript is somewhat difficult to follow. For instance, the primary objective, \( L_{\text{SVAE}} \), is not explicitly introduced in the main text, and key proofs are dispersed across the extensive supplementary material. I believe the authors could significantly enhance the clarity of the paper with some effort. Improving the alignment between the algorithm description and the main text, as well as expanding the exposition in the appendix where necessary, would be beneficial. For example, relocating Figure 6 to the appendix could free up space for additional explanations. 
Regarding the experimental results, while they serve as a proof of concept, they are underwhelming given the potential of the proposed method. After the excitement generated by the novel framework, it is disappointing to see the model evaluated only on toy datasets and mouse depth data, with comparisons to other methods limited to synthetic spiral data. I encourage the authors to apply their model to a more complex and widely recognized task if the paper is accepted.
Notes:
- In the appendix, Eq. (28) includes constant terms in the posterior \( u \).
- Sum-product networks, which efficiently represent discrete sub-structures, are mentioned on line 86.
- On line 86, \( zn \) should be \( z{n+1} \), as indicated in Figure 2.