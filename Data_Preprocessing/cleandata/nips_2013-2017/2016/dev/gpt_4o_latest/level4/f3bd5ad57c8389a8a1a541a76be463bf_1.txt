This paper introduces a method for monocular depth estimation from a single image. The approach involves convolving the image with a set of derivative filters to generate an overcomplete feature representation. The statistics of this representation are then used to construct a probabilistic model, which a neural network predicts at each image location. This framework enables an explicit representation of uncertainty in the network's output. Subsequently, the pointwise estimates are combined to produce a coherent depth map, achieved through Half Quadratic Optimization. The results on the NYU2 dataset are presented and are quite promising. Overall, this is a strong paper. While it does not introduce groundbreaking innovations, it has significant merits. I particularly appreciate the probabilistic aspect of the network's output. Although it is not a fully explicit probabilistic model, it effectively incorporates uncertainty into the predictions. Additionally, the consensus step, where partial observations are integrated, is an elegant solution for resolving ambiguities. However, I have several concerns and questions: 1. The use of a fixed set of derivative feature estimates is an interesting idea, but could these features be learned within the same framework, potentially using an iterative algorithm? 2. In Section 3.1, why were the parameters learned using K-means? Wouldn't Expectation-Maximization (EM) be a more natural choice that might yield better results? 3. What are the typical test-time running times? It seems that this method might be computationally slow. 4. From Table 2, it appears that the order filters are the most critical features, as the performance differences between them and the full model are minimal. Could you provide any insights or comments on this observation?