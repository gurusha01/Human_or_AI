This paper presents an intriguing application of two established learning algorithms, Williams' REINFORCE algorithm and Bayesian optimization, to estimate the decision threshold in a 2-AFC task. The threshold is determined by minimizing the discrepancy between the cumulative reward of the optimal decision policy and the total collected reward. The author also evaluates the performance of the two algorithms and maps the SPRT decision performance across the primary parameter space. The paper appears technically sound. I appreciate the central idea of this work, as estimating decision thresholds is a critical problem in decision science.
Technical Quality: This paper addresses the estimation of a decision threshold, a problem that has not been extensively explored within this community. Two existing algorithms are employed to estimate the threshold, and their performances are compared. The work is both interesting and technically robust.
Novelty: While the concept of estimating the decision threshold offers a novel contribution to the computational psychology community, the algorithms themselves are not novel, as they are applied with minimal modifications based on prior work [5], [6], and [7]. Additionally, the estimation process requires fixing other parameters, which raises the possibility of using a straightforward approach to search the parameter space of the threshold. This could involve minimizing the cost function by averaging trials simulated using the SPRT method. The complexity of this problem is comparable to other parameter estimation problems with fixed thresholds. The primary novel contribution of this paper lies in its focus on estimating the decision threshold, a topic that has not been sufficiently addressed in the literature.
Potential Impact: The paper estimates a constant decision threshold for a 2-AFC task with an infinite horizon. I believe this work will garner interest from researchers in psychology and cognitive science. However, in many psychological experiments, the 2-AFC task must be completed within a finite timeframe, causing the threshold to vary over time within each trial. A more sophisticated algorithm capable of estimating a time-dependent threshold under a finite horizon would significantly enhance the impact of this work. Furthermore, the paper would be more impactful if the algorithm could jointly estimate the threshold along with other parameters.
Clarity and Presentation: The paper is not particularly well-written, and the methods section lacks critical details. This omission makes it challenging for readers to replicate the work based solely on the paper. For instance, in Section 3.2 (REINFORCE Method), the author should include an illustrative network diagram to clarify the inputs and the training process of the network. Similarly, the information in Section 3.3 is vague. While the author references additional sources for further details, the core framework of the algorithms should be clearly explained within the paper itself. I recommend adding more details about the algorithms and condensing the lengthy discussion section (by about half). Additionally, a few typographical errors should be corrected, such as:  
1. In Equation (6), it seems you intended to write "-NC_risk/c."  
2. Line 114: "work work well."
Revised Assessment: After further discussion, I realized that the optimal threshold has already been studied by other researchers using different approaches. As a result, I have lowered the impact score, as the advantages of the methodology used in this paper are not clearly demonstrated in comparison to existing approaches. However, after re-reading the paper, I have increased the clarity score.