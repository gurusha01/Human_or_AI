This paper presents a multimodal method based on a residual network for visual question answering. By leveraging a multimodal approach, the proposed method effectively learns a joint representation from both visual and linguistic information. The state-of-the-art results achieved on the dataset highlight the efficacy of the proposed approach. Additionally, the method introduced for visualizing the attention effects of the joint representation is both intriguing and promising. Overall, the paper is well-written, clear, and supported by relevant experimental validations. However, there are some recently published works on the topic of visual question answering, such as the ICML 2016 paper "Dynamic Memory Networks for Visual and Textual Question Answering." It is recommended to include comparisons with these more recent methods to strengthen the evaluation.