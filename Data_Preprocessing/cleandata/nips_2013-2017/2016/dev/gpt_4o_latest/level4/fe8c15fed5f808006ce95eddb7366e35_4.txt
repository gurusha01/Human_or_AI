This paper introduces an algorithm designed to learn behavior based on long-term goals, with the prediction of player movement in basketball games as its primary application. The central concept involves training a hierarchical model that predicts low-level actions by integrating a "micro-planner" (which relies solely on the current state as input) with a "macro-planner" (which also incorporates the estimated long-term goal). The estimation of long-term goals requires an initial seeding process using heuristically-defined targets; for example, in basketball, the authors identify stationary points (moments when players stop moving) as such goals. The primary experimental evaluation compares the quality of basketball player trajectory completions (as judged by human evaluators) against those generated by baseline methods. 
The idea of leveraging higher-level goal abstractions to guide agent behavior is undoubtedly relevant, and to the best of my knowledge, the proposed approach to addressing this challenge is novel. However, I found the "Related Work" section to be underdeveloped, with insufficient connections to prior research in hierarchical reinforcement learning, goal recognition, and trajectory planning. My main concern with this submission is its overall lack of clarity. My initial reading left me quite confused, and some aspects only became clear near the end, requiring me to re-read the paper to fully grasp the methodology. For instance, it was not apparent until equations 6-7 (p. 4) that the approach is based on supervised learning; earlier mentions of planning and reinforcement learning were misleading. While it could be argued that predicting long-term goals constitutes a form of "planning," referring to the basic state-to-action predictor as a micro-"planner" seems inappropriate, as it merely imitates expert trajectories without any forward-looking component. 
Here are specific points I found unclear:  
- Notations: The distinction between scalars, vectors, and matrices is ambiguous. For example, what differentiates bold \( \mathbf{s}t \) from non-bold \( st \) on l.83? (l.160 is particularly confusing.) Why is \( mg \) a probability over "m" in eq. 2 but a function of "a" in eq. 4? Why does \( at \) appear in two probabilities in eq. 3? (I struggled to understand eq. 3 and its role in the model.) What does the index \( i \) represent on l.133? (I assume it refers to a player, but this should be clarified—also, \( i \) appears suddenly on l.142 without prior introduction in the equations.) What does the multiplication by \( \hat{a}_t \) in eq. 7 signify? (I believe I understand now, but it was initially confusing.)  
- Assumptions: Why is it critical to "assume that macro-goals are relatively static"?  
- Equation 6: Is this equation used at all? (The paper states that fine-tuning is performed solely on eq. 7.)  
Beyond these clarity issues, I have concerns regarding the overall methodology, which I hope the authors can address in their response. As far as I understand, the predictor for the attention mask \( m \) relies solely on the predicted macro-goal \( g \) as input. This implies that the prediction remains the same for a given \( g \), regardless of the player's position. This seems problematic, particularly for goals that players might approach from different directions, requiring entirely different movements. Additionally, during extrapolation, the generated player trajectory may diverge from the seeding sequence, potentially leading to unrealistic ball and player movements. Consequently, the model might encounter "unrealistic" situations not observed during training, making its behavior in such scenarios unpredictable. While I acknowledge the difficulty of this problem and do not have a straightforward solution, this limitation should at least be discussed. It would also have been valuable to investigate how input data beyond the target player's trajectory influence the model's predictions.  
Minor points and typos:  
- l.96: "should simultaneously optimizes"  
- l.176: "we used an simultaneously predicted the next 4 micro-actions"  
- Fig. 5 caption: "which may be fixable by using additional game state information"—it is unclear why additional game state information would resolve this specific issue.  
- Fig. 6 caption: "bottom-right" should be "bottom-left" on the last line.  
- l.222: "We now inspect this our macro-planner"  
- The 4-line Conclusion feels weak; consider merging it with Section 6 for a more cohesive ending.  
- "sub-sampling temporally at 4 Hz": Does this mean retaining one frame out of every four? If so, this would correspond to 25/4 Hz.  
Update after author feedback:  
Thank you for the response. I now understand why, in the specific case of this basketball dataset, conditioning the attention mask on the state may not provide additional benefits (though I believe it would generally make sense to do so). My overall impression, after considering the other reviews and the author feedback, is that the algorithm needs to be presented more clearly and better justified and evaluated against simpler or previously proposed methods. I appreciate the authors' willingness to make the data public, but this increases the importance of providing clear, quantitative benchmarks for future comparisons.