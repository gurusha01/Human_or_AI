Unlike prior methods based on patch similarity, this work introduces a novel model for discovering visual correspondences by leveraging dense feature vectors, a correspondence contrastive loss function (combining mean squared loss and margin-based ranking loss), and a CNN-based Spatial Transformer. The proposed approach effectively captures both geometric and semantic correspondences. The authors present a universal correspondence network using CNNs and pair-wise ranking loss. Overall, the paper is well-written and easy to follow. Below is the detailed review:
1. Equation (1) defines the loss function for the proposed model, where \( si / (1 - si) \) functions as an identity selector to determine which component of the loss function is applied. In Line 121, the authors state that \( s = 0 \) is used for positive pairs and \( s = 1 \) for negative pairs, such that positive pairs are trained with margin-based ranking loss and negative pairs with mean squared loss. However, this contradicts Line 132, which suggests that mean squared loss minimizes the distance between positive pairs, while ranking loss ensures negative pairs are separated by at least a margin \( m \).
2. In this work, all key points are pre-selected. Is it feasible to run the algorithm without predefined key points and instead allow the model to determine them? Essentially, this would involve ranking all vectors in the right image based on a dense feature vector from the left image. Would training with hard negative pairs impact performance in such a scenario, given that many vectors might remain unupdated during training?
3. The authors employ a spatial transformer to emulate path normalization, demonstrating that its inclusion improves model performance. How does this differ from shortcut connections? Additionally, what advantages does the spatial transformer offer over shortcuts?
4. Figure 5 illustrates the Accuracy of PCK performance but omits precision and recall metrics. If the interpretation is correct, for a given pixel threshold \( k \), accuracy corresponds to \( \(\text{actual correspondent pixels}) / k \). It would be insightful to observe the recall curve under these conditions.