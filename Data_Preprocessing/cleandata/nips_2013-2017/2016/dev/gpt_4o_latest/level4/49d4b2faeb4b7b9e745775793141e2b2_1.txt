The authors argue that the success of DRAW-like models stems from their incremental approach to data generation, coupled with deterministic connections between each latent variable and the observation. Building on these principles, they extend the design to hierarchical latent variable models, introducing Matryoshka Networks (MatNets). The proposed architecture comprises a bottom-up (BU) network, a top-down (TD) network, and merge modules that facilitate information flow between the two. MatNets bear similarities to Probabilistic Ladder Networks (PLNs) [28], featuring a deterministic pathway from the observation to latent layers in the inference model and a top-down factorization of the variational posterior. The key distinction lies in the deterministic connections between the latents and the observation in MatNets, which are absent in PLNs. The authors evaluate MatNets on density modeling tasks using MNIST, Omniglot, and CIFAR, achieving results that are either highly competitive or state-of-the-art. 
This is an interesting and generally well-executed paper. Its primary contribution is a hierarchical VAE architecture that incorporates deterministic connections in both the inference and generative models, enabling incremental generation of observations. While DRAW-like models also adopt an incremental approach, their latent variables do not form a hierarchy and are instead situated at the same level. Probabilistic Ladder Networks share most of the proposed features, except for the deterministic connections in the generative model. Consequently, the main novelty of this work lies in introducing deterministic connections within a hierarchical generative model. However, the experimental section does not include any analysis specifically demonstrating the importance of these deterministic connections in either the inference or generative models. Instead, the experiments focus on showing that the architecture performs well across several datasets. While this is noteworthy, it does not directly validate the central claim of the paper.
The paper is reasonably well-written, but the presentation of the proposed architecture could be improved for better accessibility. Specifically, a high-level probabilistic description of the model should precede the detailed procedural computations outlined in Section 2.1. Additionally, the lack of citations in certain sections may unintentionally suggest claims of novelty. For instance, the discussion of convolutional GRU modules does not reference prior use in generative models, such as in [24]. Furthermore, despite the introduction explaining the relationship between PLNs and DRAW, the related work section omits any mention of PLNs, which should be addressed. It is also unclear whether the inference regularization technique described in Section 2.3 was applied in any of the experiments. 
Typos: Equations 11 and 12 should include a log in front of p(x|z).