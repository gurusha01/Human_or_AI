In this paper, the authors integrate the Barzilai-Borwein (BB) step size into variants of stochastic gradient descent, including SGD and SVRG, with an additional application to SAG discussed in the appendix. They establish that SVRG with the BB step size achieves linear convergence in expectation for μ-strongly convex functions with Lipschitz continuous gradients, provided that the parameter m—representing the frequency of full gradient computation—is chosen sufficiently large. Experimental results demonstrate that the proposed SVRG with the BB step size tends to adaptively converge toward the optimally tuned step size, yielding a slightly slower but comparable convergence rate across three datasets. The performance of other SGD variants with the BB step size also appears promising. In the case of SVRG-I, which the authors argue is more practical than SVRG-II, two parameters—m and the step size η—must be tuned to ensure linear convergence. The paper's primary contribution lies in simplifying the usability of SVRG by introducing the BB step size, thereby eliminating the need to tune the step size, provided that m is sufficiently large. However, the paper could be strengthened by empirically analyzing the sensitivity of m. The theoretical results indicate that linear convergence can be achieved with a rate of 1 - θ in terms of the number of epochs. If the convergence rate per epoch is fixed, the time complexity scales linearly with m. Doubling m would therefore result in twice as slow convergence. If this holds true, the sensitivity of m would be comparable to that of the step size, which would undermine the claimed advantage of the approach.