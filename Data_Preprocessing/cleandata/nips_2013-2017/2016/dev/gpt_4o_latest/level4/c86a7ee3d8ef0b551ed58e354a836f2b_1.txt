This paper investigates the problem of step-size adaptation in Stochastic Gradient Descent (SGD) and some of its variants. It introduces the use of the Barzilai-Borwein (BB) method to automatically compute step-sizes for SGD and Stochastic Variance Reduced Gradient (SVRG), replacing the need for predefined fixed or decreasing step-size schedules. For SGD, a smoothing technique is also incorporated. The paper tackles a significant issue for SGD-type algorithms. The BB method is initially applied within the SVRG framework. The simulations are compelling, demonstrating that the optimal step-size is learned after an adaptation phase. However, I am curious about the pronounced overshoot towards excessively small step-sizes during the initial iterations in Figure 1. This behavior appears suboptimal. For the implementation in SGD, the authors introduce a smoothing technique, but my concern lies in the fact that the smoothing formula reintroduces a deterministic, non-adaptive decrease. Specifically, the decrease follows a 1/(k+1) pattern, which effectively reinstates the limitations of predefined step-size schedules. Additional comments: In Lemma 1, the expectation should be expressed as a conditional expectation.