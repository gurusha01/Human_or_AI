The authors introduce an automatic step size scheme for SGD and SVRG based on the Barzilai-Borwein (BB) method. They provide a linear convergence analysis for SVRG-BB and SVRG-I (option 1 of SVRG, where the current iterate serves as the mark iterate for the next epoch). For SGD-BB, a smoothing technique is necessary for convergence, but no theoretical convergence analysis is presented. The authors compare SVRG-BB to SVRG with a constant step size and SGD-BB to SGD with a decreasing step size, demonstrating that BB step sizes achieve a convergence rate comparable to the best-tuned step size for both SVRG and SGD. EDIT: Following the authors' response, I have improved my ratings. This paper tackles an important challenge: automatically determining step size sequences for stochastic optimization methods such as SGD. The authors leverage the well-established Barzilai-Borwein method to automate step size selection for SGD and SVRG. While the results are promising, the paper has two significant limitations: 
1. The paper does not provide a convergence rate analysis for SGD-BB. Although the authors correctly note that a decreasing step size is necessary for SGD's convergence, it is crucial to theoretically analyze how the BB step size influences SGD's convergence rate. Could the authors elaborate on this? Additionally, the smoothing optimization function used for SGD step sizes (Equation 13) appears heuristic and lacks theoretical justification. 
2. The experimental evaluation is limited. The authors focus solely on binary classification tasks. It would be valuable to assess the performance of the proposed step sizes on regression problems and non-convex tasks, such as training neural networks. Furthermore, the authors do not compare SGD-BB to other existing automatic step size methods for SGD (e.g., [1] and [2]). The paper does not discuss how BB step sizes compare to these methods in terms of speed, memory usage, or other metrics. For instance, is this approach faster or more memory-efficient than alternative methods?
The convergence analysis for SVRG-BB and SVRG-I is straightforward yet interesting. However, applying BB step sizes to SVRG is not a particularly novel contribution, as SVRG computes true gradients at the end of each epoch, making the application of BB step sizes relatively straightforward, as shown in the paper. 
Overall, while the paper addresses an important problem and presents promising results, it requires further development. Specifically, a convergence rate proof for SGD-BB and more comprehensive experiments would strengthen the paper. 
Minor comments:  
1. Typo in line 13 of the supplementary material: \(\eta \to \eta_k\).  
2. The bounds in line 9 of the supplementary material could be improved using techniques from the original SVRG paper.  
3. How sensitive is SGD-BB to the choice of \(m\)?  
4. In SVRG-BB, the step sizes appear to be quite small initially, leading to slower convergence at the start. Do the authors have any insights into why this occurs? Addressing this initial slowdown could help SVRG-BB match the performance of the best-tuned step size for SVRG.  
References:  
[1] Mahsereci, Maren, and Philipp Hennig. "Probabilistic line searches for stochastic optimization." Advances in Neural Information Processing Systems. 2015.  
[2] Schaul, Tom, Sixin Zhang, and Yann LeCun. "No more pesky learning rates." ICML (3) 28 (2013): 343-351.