This paper explores the application of tensor networks to supervised learning tasks. Specifically, the authors demonstrate how tensor networks can be adapted to generate meaningful data representations and address classification problems. The central idea involves constructing data feature maps through the tensor product of local features, which are then represented as higher-order tensors. The classification task is framed by treating the model parameters as a tensor network and optimizing them using a matrix-product state (MPS) decomposition. The paper describes an optimization algorithm for obtaining the MPS decomposition and provides an experimental evaluation on the MNIST digit dataset. Additionally, the authors discuss the class of functions within which the solution is sought. While the concept of employing tensor networks for machine learning is intriguing, there are several significant issues that must be addressed before the manuscript can be considered for publication. My primary concerns are as follows:
1. Clarity and Accessibility: Certain sections of the paper are challenging to follow. The manuscript adapts concepts from tensor networks, which are well-established in the Physics community, to machine learning problems. Although the authors have made commendable efforts to illustrate their ideas and intuition through numerous figures, the paper would benefit greatly from additional details on tensor networks, MPS decomposition algorithms, and their applications in Physics. Providing a dedicated section that elaborates on the foundational concepts and references cited from Physics literature would enhance the paper's readability and accessibility for a machine learning audience.
2. Complex Notation: The notation used in the paper is cumbersome and detracts from understanding. For instance:
   - In Equation 2, the relationship between \(x\) and \(xj\) is unclear. Is \(xj\) a component of the vector \(x\)? Additionally, \(N\) needs clarificationâ€”does it represent the number of attributes?
   - In Equation 3, \(\phi\) is indexed by \(sj\) but depends only on \(xj\). Is the same \(\phi\) used for all \(s_j\)?
   - The term \(A\) in Equation 5 requires definition.
   - Line 68 contains a typographical error: "in classifying labeled data" should be revised to "unlabeled data (from a given training set)."
   - Additional examples of unclear notation and phrasing exist throughout the manuscript.
3. Experimental Weaknesses: The experimental evaluation is insufficient. To robustly assess the method's performance, additional datasets beyond the MNIST digit dataset should be utilized. Furthermore, critical details about the experimental setup are missing. For example, how was the dataset divided into training and testing subsets?
4. Feature Map Selection: The choice of the local feature map is a key aspect of the proposed method, yet it is not adequately addressed. It remains unclear how this feature map should be selected. The feature map provided in Equation 3 appears to be tailored specifically for image data. Similarly, the parameters \(d\) and \(m\) are not well-explained. How should these parameters be chosen in practice?
5. Uniqueness of MPS Decomposition: The manuscript does not address whether the MPS decomposition is unique. If it is not unique, how does this affect the learning process and the resulting model?
6. Novelty Compared to Prior Work: The use of MPS decomposition for feature extraction and classification has been explored in [4]. The manuscript does not clearly articulate its novel contributions relative to [4]. The brief mention in Line 210 is insufficient and lacks the detail necessary to distinguish this work from prior research.
7. Connection to ANOVA Kernels: ANOVA kernels are constructed from the product of a set of "local" kernels, enabling solutions in a Hilbert space formed by the tensor product of the reproducing Hilbert spaces associated with each local kernel. It would be valuable to investigate the relationship between the function space derived from such kernels and the tensor network representation proposed in this paper.