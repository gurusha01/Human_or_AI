This paper investigates the task of visual question answering (VQA). To address the challenge of answering questions about images, the authors derive their model from neural reasoners. Neural reasoners operate by taking a question and supporting facts required to answer it, refining the query based on these facts, and producing an updated representation. In this work, object detection (along with spatial positions of objects) is proposed as the source of supporting facts necessary for answering questions. While the results on standard VQA datasets show some promise, they fall short of the performance achieved by current state-of-the-art methods. The paper is well-written, and the experiments appear methodologically sound. The motivation to generate more directed and detailed questions, as opposed to vague ones, is also a valuable contribution to the VQA domain. 
The proposed method builds on a neural reasoning system originally developed for text-based question answering. However, the authors do not introduce any novel improvements or modifications to the reasoning framework itself. Instead, they adapt the system by replacing textual facts with object-based facts derived from images. Consequently, the overall novelty of the method is limited. Furthermore, I am not entirely convinced that object detection boxes alone provide sufficient facts for answering questions in existing VQA datasets. First, this approach tends to perform poorly for counting-related questions, as acknowledged by the authors (line 220). Second, object locations and features are primarily useful for questions directly tied to objects. While the authors argue that "most of the questions are related to objects" (lines 97â€“99), this claim does not hold true for the VQA dataset. Many questions refer to objects but actually seek contextual information about their surroundings (e.g., "What is next to the couch?" or "What is to the left of the dog?"). Moreover, the object proposals are generated using automated "top-ranked" regions rather than being tailored to the entities mentioned in the question. As a result, even when combined with spatial information, the object-based facts provide only a limited knowledge base. Since the entire image is also included as a source of information, it is unclear how much the object proposals contribute to the reasoning process. The authors do not conduct ablation studies to demonstrate the specific impact of including object-based facts. Such studies could involve training a controlled system that excludes overall image features and relies solely on object proposals. 
Similarly, the attention mechanism in the proposed method operates on top-ranked image regions that are generated independently of the question. This design choice excludes certain image regions that are not covered by the top-ranked proposals. Unlike grid-based layouts (commonly derived from convolutional neural network features before pooling [1][2]), this approach lacks the granularity needed to attend to specific parts of the image. While the inclusion of the entire image provides some additional context, it does not offer the resolution necessary for precise reasoning. 
The authors also present examples of updated question representations, which sometimes exhibit greater detail compared to the original questions. However, I am not convinced that this necessarily indicates successful representation updates. There are several concerns: (1) The authors use a nearest-neighbor approach to find the closest question in the dataset that matches the updated representation. Given that the reasoning system explicitly incorporates object locations and features, it is unsurprising that the updated representations reference objects present in the image. However, it remains unclear whether the updated representation preserves the original question's intent. While the updated questions may include more detail, it is uncertain if their meaning remains consistent with the original. (2) The paper does not provide examples of failure cases, which would help clarify the limitations of the approach. (3) The examples are drawn exclusively from the COCO-QA dataset, which is derived from image captions using natural language processing algorithms. This dataset is known for its poor grammar, composition, and phrasing, which may affect the quality of the updated representations. These concerns do not imply that the updated representations are inherently ineffective but rather that the nearest-neighbor evaluation method used to assess their quality is questionable.
In summary, this paper is well-written and introduces an interesting application of neural reasoning systems to VQA. However, the work does not present any significant innovations in neural reasoning, and the proposed method for generating visual facts may be insufficient for comprehensive reasoning about images. The general applicability of the method in its current form is limited, especially given that similar approaches achieve better performance on VQA tasks. To strengthen the contribution, the visual facts should be more detailed and tailored to the specific question, providing a stronger foundation for reasoning. Additionally, the authors should conduct ablation studies to explicitly demonstrate the efficacy of their proposed approach. 
----------- References -----------  
1. Kevin J. Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual question answering. In CVPR, 2016.  
2. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In CVPR, 2016.