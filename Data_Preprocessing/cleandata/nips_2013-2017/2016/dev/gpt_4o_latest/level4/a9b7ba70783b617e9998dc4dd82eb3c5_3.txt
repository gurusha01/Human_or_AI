The paper addresses an inference problem for cluster trees derived from density estimation, proposing a bootstrap-based method for constructing confidence sets. Additionally, the authors explore a pruning method to enhance visualization. Their approach is grounded in topological considerations for comparing trees, introducing an $\ell_\infty$ metric-based pruning strategy. The primary contribution lies in developing a theoretical framework to justify the method, supplemented by some numerical examples. Overall, the problem is both interesting and significant, and the theoretical rigor is commendable. The proposed solution appears reasonable and practical. 
1. While the theoretical development is valuable, the numerical example does not convincingly showcase the potential of the proposed method. Specifically, since the primary focus is on confidence sets, it would be helpful to demonstrate the richness of these sets more effectively, enabling readers to better appreciate the visualization facilitated by pruning.
2. Related to the first point, I initially assumed that partial ordering was used to compare different trees. However, the definition of partial ordering seems to require the primary split to occur at exactly the same location (as shown in Fig. 2), which is unlikely for two different estimates. Even if the branching points vary minimally, they would still allow for an approximation. While the partial ordering is well-defined for pruning a given estimate, it does not seem applicable for comparing various trees within the confidence sets. Am I misunderstanding this aspect?
3. The metric definitions in Section 3 appear to allow $p$ and $q$ to be any functions defined on the same domain, without necessarily assuming they are density functions representing the same underlying population (e.g., KDE estimates from different realizations of the population). Is this interpretation correct?
4. Similarly, regarding $p{inf}$ and the definition of $a$, if we consider density functions without compact support, $p{inf}$ and $a$ would be zero. In Lemma 1, does "more generally" refer to whether the density functions have compact support or not?
5. When considering the distance relative to the truth, it is intuitive that KDE bias, being proportional to the second derivatives, would lead to errors at peaks and valleys, which are directly tied to merging heights. This suggests that $d\infty$ and $dM$ should be equivalent. However, this equivalence does not seem to hold when comparing two different functions, as their relative differences may not be amplified at these landmarks. Nonetheless, I feel that the same principle should apply. Could you clarify the source of my confusion?
6. In Figure 4, for the second example (mickey mouse data), I would expect the heights of the main cluster and the two smaller clusters to differ, given that their probabilities are distinct. Why is this not reflected in the figure?