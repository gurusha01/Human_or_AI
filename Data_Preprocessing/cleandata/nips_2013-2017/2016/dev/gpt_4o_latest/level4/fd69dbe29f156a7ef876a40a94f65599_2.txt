This paper applies the "neural reasoning" framework to tackle the problem of visual question answering. Specifically, it focuses on addressing the challenge posed by inherently ambiguous or difficult questions that cannot be fully understood without considering the context provided by the image. By segmenting the image into candidate regions and treating the entire image as a sequence of "facts" (analogous to the structure of the bAbI dataset), the "neural reasoning" model is employed. The evaluation on COCO-QA demonstrates respectable performance, slightly surpassing state-of-the-art models, while results on the VQA dataset are comparable to other systems but slightly weaker. I appreciate the core idea of this work, and it appears that the authors have effectively adapted a model originally designed for the more synthetic bAbI task to achieve strong performance on the more realistic VQA task. While the overall approach is reasonable, I am not entirely convinced that the reported empirical success can be fully attributed to the claimed advantages of the model. For example, some of the provided questions seem highly ambiguous. How many of these ambiguous questions are incorrectly answered by other models but correctly handled by the proposed approach? Furthermore, Figure 3 and its associated questions are not particularly intuitive to me, as the ambiguity of the question remains unresolved. The performance on the newer VQA dataset is less impressive compared to COCO-QA, yet the paper offers limited discussion on the strengths and weaknesses of the proposed model in comparison to other state-of-the-art methods, particularly in the VQA experiments.