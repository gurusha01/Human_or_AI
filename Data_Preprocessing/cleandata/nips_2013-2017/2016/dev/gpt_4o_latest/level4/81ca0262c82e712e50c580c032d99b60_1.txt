This paper addresses the problem of causal subset selection using directed information, framing it as the task of maximizing a submodular (or approximately submodular) function to determine which features to select for prediction. A particularly notable contribution is the novel definition of approximate submodularity and the approximation results derived from it. However, there are two main concerns with the paper. 
The first issue pertains to the discussion of how directed information is used to infer causality. Unfortunately, the term "causality" is ambiguous, as it can refer to two distinct concepts that should be clearly delineated (and this distinction should be explicitly discussed in the manuscript). The first concept of causality is essentially the prediction of a time series based on other time series, where the temporal structure is respected, leading to its characterization as causal. For example, given a set of time series, one might ask which series can best predict another in the next time step, using this as a measure of causality. This approach aligns with the paper's focus on identifying the k-best predictors and performing "causal" feature selection. The connection between this Granger-like causality and directed information is intriguing and has been explored in prior work, such as [20], which is cited in this paper. The current paper builds on this type of causality, which might be more appropriately termed generalized Granger causality or prediction causality. 
The second concept of causality, which is widely regarded as the more rigorous definition (as supported by much of the literature), relates to counterfactual reasoning and structural equations. In this framework, causality involves understanding what WOULD happen to Y if X were changed, focusing on the mechanisms or functions that generate the data rather than on prediction. It is crucial to clarify in the paper that optimal time-series prediction does not necessarily provide insight into how modifying one time series would influence another. In summary, while the paper presents compelling results, these are primarily about the prediction of processes rather than causality in the counterfactual sense. The authors should consider emphasizing their contributions to submodularity and its approximations, with Granger causality as a specific application. For the broader causality literature, the authors might also reference works by Pearl, Imbens, Rubin, and Richardson. 
Theorem 2 is interesting, and its proof appears correct. Lemma 1 is central to demonstrating how submodularity is relaxed in this paper (though it is referred to as Lemma 2 in the appendix), and the proof is clear. Theorem 3 and its corollaries are the most significant results in the paper, focusing on a new relaxation of submodularity that is independent of causality or any specific information metric. This distinction should be explicitly highlighted in the manuscript.
The second issue concerns the relationship between this paper's results and the submodularity ratio introduced by Das and Kempe in [3]. The paper incorrectly claims that [3] defines the submodularity ratio only for the R² score, whereas [3] actually introduces the submodularity ratio for any arbitrary set function (Definition 2.3) and then specializes it to the R² score as an example. Furthermore, the performance guarantee in [3] (Theorem 3.2) generalizes the classic 1-1/e result for any set function with a bounded submodularity ratio. While [3] does not emphasize this general result in its abstract or introduction, it is clearly stated before Theorem 3.2. 
A key question is whether some of the results in this paper are direct corollaries of Theorem 3.2 in [3]. Specifically, the Submodularity Index (SMI) defined in Equation (6) of this paper measures the difference between adding elements one at a time versus adding them all at once. In contrast, the submodularity ratio in [3] is the ratio of these quantities. Submodularity implies that the difference is non-negative (as shown in Lemma 1 of this paper), which corresponds to a ratio of at least 1 (as stated in [3]). 
- Submodularity ratio (from [3]): sum fx(A) / fS(A) ≥ 1  
- SMI (from this paper): sum fx(A) - fS(A) ≥ 0  
The relaxation in [3] involves a submodularity ratio ≥ a constant γ, while this paper considers cases where the SMI difference is negative but close to zero. Since the results in this paper normalize SMI by f(S_greedy), there may be a direct mapping between the two frameworks for general set functions. However, this connection is non-trivial and would require additional analysis from the authors. I recommend that the authors discuss the potential relationship between their results and the submodularity ratio framework in [3]. The analysis is further complicated by the fact that [3] only addresses monotone functions, whereas this paper extends its results to both monotone and non-monotone functions. If this paper indeed generalizes [3] beyond monotonicity, it would represent a significant contribution.
Minor Comments:
- In [3], the author order should be reversed. 
- The proofs of Propositions 2 and 3 are unclear (or could not be located). 
- The paper contains numerous typos and grammatical errors, as well as missing symbols (e.g., ??) in the appendix. Examples of typos include:  
  - "Consider two random process X^n" → "Consider two random processes X^n"  
  - "directed information maximization" → "maximization problems" (page 3)  
  - "address in details" → "address in detail"  
  - "any polynomial algorithm" → "polynomial-time algorithm"  
  - "Monotonic submodular functions" → "Monotone submodular functions"  
  - "we mainly analysis" → "we mainly analyze"  
  - "we can justify its submodularity," → (rephrase for clarity)  
  - "that in literature" → "that in the literature" (page 4)  
  - "several effort" → "several efforts"  
Recommendation:
Overall, I recommend accepting this paper after these issues are addressed, as the combinatorial optimization results, particularly the novel relaxation of submodularity, are highly interesting.