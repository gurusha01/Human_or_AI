The paper presents a method for jointly training two machine translation systems using monolingual data, framed as a type of communication game. However, this approach can be viewed as a variation of the autoencoder concept (interpreted broadly). Specifically, the method involves models for p(X|Y) and p(Y|X), which are trained together on independent samples from X (or Y), enabling the prediction of a sample X from itself via p(X|Y)p(Y|X). In this context, Y can be interpreted as a "representation" of X, aligning with the general idea of autoencoders. While the technical concepts are relatively straightforward, the approach is innovative and has the potential to be practically impactful. The experimental results indicate that this is a promising research direction, though the findings appear preliminary and leave several natural questions unanswered. The simplicity of the technical ideas makes them easy to grasp, but their explanation in the paper feels overly repetitive: the core concept is reiterated in the abstract, on p. 2 (line 47), and p. 3 (line 124). Instead of restating this straightforward idea multiple times, the paper would have been more compelling with a deeper empirical analysis. For instance, it could include learning curves, the impact of initializing with varying amounts of data, and a detailed examination of the results. Specifically, it would be valuable to understand how systems trained in this manner improve—whether in terms of word choice, word order, or other aspects—given that vocabulary size is capped at 30K words.