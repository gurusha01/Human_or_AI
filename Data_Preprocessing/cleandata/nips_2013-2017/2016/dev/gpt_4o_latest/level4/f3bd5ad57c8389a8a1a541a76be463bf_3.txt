Given a single RGB image, the proposed method aims to estimate its depth map by leveraging visual cues. The approach involves using a neural network to predict a distribution for higher-order derivatives of the depth for each image patch. These parameterized distributions (Gaussian Mixture Models, GMMs) are subsequently utilized in a global optimization step to reconstruct the depth map. While the paper is well-organized and remains comprehensible up to Section 3.1, there are several unresolved questions regarding the technical aspects and the rationale behind the proposed methodology.
Motivation  
The authors do not provide a clear justification for the sequential design of their approach, despite presenting experimental results. Why is the intermediate GMM representation necessary, rather than allowing the neural network to directly learn effective intermediate representations? Fixing the derivative filters in the initial network appears to impose an unnecessary constraint—why not allow these filters to be learned? Furthermore, the proposed method bears some resemblance to approaches such as VLAD/Fisher Vector features or the more recent NetVLAD neural network architecture. What distinguishes the proposed method from these existing techniques? Additionally, if the intermediate representation involves distributions, one might expect to see some form of variance estimation for the depth map. Is this feasible? Including such an estimate would significantly strengthen the motivation for using the intermediate distribution representation.
Technical Points  
- Line 123: What happens to the other clusters? Did the authors employ standard techniques, such as splitting highly populated clusters, to address negligible clusters? How many clusters are effectively utilized—are all 64 clusters used?  
- How does the number of clusters impact the results?  
- Line 116: Isn't this essentially linear regression, given that everything except \(\hat{p}\) is fixed? This step seems to function primarily as a normalization. What would happen if the weights \(w_i\) were used directly?  
- Line 129: Why was this specific architecture chosen?  
- What is the total runtime during the inference phase, given that the initial stage processes patches individually?  
- The paper lacks visual comparisons with other methods, making it difficult to assess the areas where the proposed algorithm demonstrates greater robustness.  
- Line 227: Referring readers to other papers to understand the notations used is highly discouraged.  
- The paper does not discuss any failure cases. Are the authors aware of specific scenarios where their method fails?  
Conclusion  
Thank you for addressing the above concerns in the rebuttal. After reviewing the authors' responses, I revised my evaluation and increased my score.