The authors propose an algorithm for solving random systems of quadratic equations using truncated gradient updates. The approach minimizes the least squares criterion based on an amplitude-based empirical loss. Unlike the truncation method in Chen and Candes (2015), this method does not discard large-sized gradients. The authors provide a theoretical analysis for the noiseless case and validate their findings through simulations. However, Definition 1 (generalized gradient) is not referenced in the main text. Where is this definition applied, and why is it necessary? In line 145, do you mean "sufficient" instead of "necessary"? Equation 9 could be presented more clearly to ensure the definition of h is immediately apparent to the reader without requiring additional effort. This is the first instance where h appears, but it is not explicitly defined anywhere. In line 154, the authors invoke the strong law of large numbers, but both fixed points x and z^* depend on the data, introducing dependence among the summands. Additionally, why is h considered a good direction? Please provide further clarification. In line 164, how strongly does the statement depend on the Gaussian data assumption? The authors do not address the noisy case in Section 3. Can the main theorem be extended to the noisy case, or does this present a technical challenge? A comment on this would be valuable for guiding future research. Furthermore, does the proposed algorithm perform well on datasets with distributions that deviate significantly from Gaussian? A discussion on this point is necessary. Finally, the authors should evaluate the algorithm on real datasets to validate its practical performance, such as an image recovery example similar to the one in the TWF paper.