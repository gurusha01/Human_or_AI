The authors present an architecture for deep generative models that facilitates the training of deeper networks without compromising their trainability. The enhanced trainability is attributed to the integration of lateral, shortcut, and residual connections within the network. The proposed architecture achieves state-of-the-art results across multiple benchmark datasets. The authors offer a well-reasoned justification for their design, drawing on key insights from recent advancements in deep recurrent networks and semi-supervised ladder networks. The reported results are undeniably impressive. However, while the paper emphasizes the trainability of the proposed networks as a central contribution, the evidence provided for this claim is limited to performance metrics on various benchmarks. If trainability is a core focus of this work, additional analysis or experiments explicitly demonstrating this aspect would strengthen the contribution.