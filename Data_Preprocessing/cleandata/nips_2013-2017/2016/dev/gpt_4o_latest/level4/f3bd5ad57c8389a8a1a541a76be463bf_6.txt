This paper introduces a novel algorithm for monocular depth estimation. Rather than directly predicting the depth map, the proposed approach estimates a set of filter-bank response coefficients for the depth map. During the prediction phase, the method models the distribution of filter responses and reconstructs the depth map via a global energy minimization process. Overall, the proposed method appears technically sound, and the general concept of leveraging high-level statistics is reasonable. The approach also seems novel. However, I have several concerns regarding the justification of its significance and the rationale behind the chosen methodology.
The general idea bears some resemblance to recent works that incorporate loss functions based on high-order statistics for various low-level vision tasks, such as optical flow, monocular depth recovery, and image generation (e.g., "DisparityNet" by Mayer et al., 2016; "Image Generation with Perceptual Loss" by Ridgeway et al., 2015; "DeePSiM Loss" by Dosovitskiy et al., 2015; and "Depth Estimation" by Eigen et al., 2015). These methods utilize loss functions that capture high-level statistics, often through handcrafted or learnable filter responses. Motivated by these works, I believe a baseline comparison is necessary to strengthen the paper's claims. Specifically, instead of predicting filter-bank responses followed by an energy minimization step to recover depth, one could directly fit the depth map using a loss function that measures the similarity of filter responses between the ground truth and predicted depth maps. This alternative approach would avoid the computationally expensive alternating inference step while still capturing high-order statistics through handcrafted filters. I am curious about the performance of such a baseline and why the authors favor their proposed method over it.
The authors emphasize the importance of predicting filter-bank response coefficients but do not provide sufficient experimental evidence to support this claim. Does this approach produce visually superior results or better preserve 3D geometry compared to directly fitting the depth map? Alternatively, does modeling uncertainty lead to more diverse predictions? Section 4.2 does not convincingly explain why this approach is impactful. From the comparison studies, the proposed method seems to contribute less to performance improvement than pretraining a VGG model on ImageNet or incorporating a semantic labeling loss.
Additionally, the paper lacks a strong theoretical foundation for directly modeling high-order depth map statistics instead of working in the 3D geometry space. It is also unclear why the authors chose to use a specific set of handcrafted Gaussian filters. In fact, it seems feasible to integrate the inference stage described in Equation 6 into the network and learn the filters in an end-to-end manner. This would allow the depth reconstruction objective to be directly optimized. I also question whether the indirect regression approach might degrade performance in terms of RMSE, as the model is not explicitly optimized to minimize this metric.
Overall, I find the paper to be borderline in its current form. I look forward to the authors' rebuttal to address these concerns.