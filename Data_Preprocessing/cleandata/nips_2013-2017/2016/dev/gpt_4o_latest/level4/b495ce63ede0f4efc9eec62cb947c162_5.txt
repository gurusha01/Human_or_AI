This paper introduces a novel architecture for learning visual component correspondences between objects of the same category across different images. The fully convolutional design leverages a contrastive loss that efficiently reuses computations through convolutional layers and incorporates a modified spatial transformer module to adaptively transform input patches. Both of these contributions are innovative and play a key role in enhancing the model's efficiency and performance. The manuscript is well-written and provides clear explanations. The adoption of a fully convolutional architecture is well-motivated, as it significantly improves training efficiency compared to approaches using triplet loss or standard contrastive loss. Additionally, the hard negative mining technique is a critical component in deep metric learning. There are, however, minor typographical errors on line 49 and in the caption for Figure 2. The authors frequently use the term "semantic correspondence," but the evidence does not support the claim that the model learns semantic meanings of object parts. Instead, it appears to extract abstract visual features shared across instances of the same object part. The term "geometric correspondence" would be more appropriate. Furthermore, the experimental results are qualitatively underwhelming. In Figure 7, the HN-ST model fails to correctly match the dog, boat, and bus examples. This further supports the observation that the model does not learn semantic correspondences, as these example pairs exhibit significant differences in visual perspectives between the query and test samples.