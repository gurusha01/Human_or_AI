The paper introduces a multimodal learning framework inspired by residual learning to tackle the visual question answering (VQA) problem. It achieves state-of-the-art performance on VQA benchmarks. The most compelling aspect is the attention representation derived through backpropagation without requiring explicit attention parameters. If my understanding is correct, the standout contribution lies in visualizing VQA attention without relying on attention parameters. Adding some foundational references on VQA would enhance the manuscript's accessibility for readers unfamiliar with the domain. Given the growing popularity of VQA, the content in section 4.1 feels redundant and occupies considerable space. Clarifying the TrimZero explanation in lines 136-137 would be beneficial. Additionally, in section 4.2 on postprocessing, the update step v = v + 1 (line 154) is unclear. Overall, the results are impressive.