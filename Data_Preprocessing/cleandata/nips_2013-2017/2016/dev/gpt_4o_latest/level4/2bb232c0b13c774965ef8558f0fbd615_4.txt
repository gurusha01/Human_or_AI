This paper proposes a modification to the collapse layer of the standard MD-LSTMs employed for handwriting recognition, initially introduced in [18]. The study is also closely aligned with [6], which addresses the same taskâ€”decoding at the character level for each time step. The proposed modification involves replacing global average pooling with weighted average pooling (Eq. 2). These weights are learned through a recurrent attention layer (MD-LSTM) that utilizes both the previous weights and the encoded feature vector. A CTC loss is employed to compute transcription loss directly, eliminating the need for character- or line-level alignment. Additionally, the attention mechanism can be interpreted as a form of "soft" line segmentation. The paper compares its approach with other methods, including [6], and demonstrates better or comparable performance. From my perspective, the primary difference between this work and [6] is that, in this method, an entire line is decoded at each time step instead of a single character. While this leads to improved speed and performance, I am not entirely convinced that the technical novelty is sufficient. (Please clarify this in the rebuttal if I have overlooked something, and I will gladly reconsider my rating.) Furthermore, it would have been valuable to visualize the attention mechanism in cases where the number of lines in the input image exceeds the number of time steps the algorithm is configured to run, as the paper notes that the method still performs well in such scenarios.