The paper presents a specific implementation of a deep memory network incorporating an attention mechanism, designed to address hierarchical, long-term action planning in complex problems. The network is trained using behavioral cloning on expert trajectories and tested on a novel task: predicting basketball player behavior from real-world play data. The paper thoroughly examines and evaluates various architectural choices of the model. It is generally well-written (aside from minor stylistic issues noted below) and tackles an important problem: learning hierarchical behavioral policies from data. The chosen benchmark is interesting and represents a refreshing departure from the standard benchmarks commonly used in control tasks and imitation learning. The experiments are comprehensive, and the authors analyze different model configurations in isolation, enabling them to attribute performance improvements to specific modeling decisions. This is a significant strength. The model design appears reasonable, and the connection between hierarchical policies and attention mechanisms from language modeling is a logical and promising idea.
Despite these positive aspects, the paper has some notable shortcomings, which I will outline and elaborate on below. Overall, I believe the paper could be accepted if the issues related to the somewhat confusing notation, the formulation of the loss function, and the training details are addressed (and the low-quality figures are replaced).
Related Work and Citations:
- The paper omits citations to relevant work in imitation learning, inverse reinforcement learning (IRL), and apprenticeship learning, which could also be applied to the presented problem. The proposed algorithm can, in fact, be interpreted as a specific instance of imitation learning, commonly referred to as "behavioral cloning" in the community.
- The terminology in the related work section is inconsistent, as the terms "policy" and "planner" are used interchangeably. This may seem unusual to readers from the reinforcement learning (RL) community. Additionally, referring to DQN as a "shallow planner" is somewhat misleading terminology.
Notation Issues and Loss Functions:
- In Equations (2) and (3), the auxiliary transfer function is not conditioned on \( st \), but based on Figure 3 and the network definition later in the paper, it seems clear that \( g \) is predicted from \( st \), and thus \( m_g \) should be conditional on it. Is this omission due to an assumption of independence?
- Am I correct in understanding that the posterior derived in Equation (3) (which involves discretizing the state and action space) is not actually used during training? It seems that the model components are trained in a supervised manner, and fine-tuning occurs only at the micro-action level without computing the integral. Alternatively, does the probability density in Equation (7) correspond to Equation (3)? This is unclear from the context.
- Between Equations (3) and (7), the dependency on the history seems to disappear. I assume this is an oversight, but it reflects sloppy notation.
- My primary concern with the loss function is related to the footnote on page 4. While it is not entirely clear what the loss in Equation (7) represents (see earlier comments), it appears to be symmetrical with respect to the outputs of the micro and macro networks. Since these networks operate on the same input and there is no supervision on macro-goals during fine-tuning, the loss function seems to have a trivial local minimum where one network outputs a uniform distribution. If this is the case, the only factors preventing convergence to this solution are the initialization and the use of small learning rates. Relying on the optimizer to avoid such flaws in the objective function is not an ideal approach.
Experimental Details:
- During evaluation, I assume the correct approach would be to sample from the posterior defined in Equation (3), but this is not explicitly mentioned. If this is not the case, what evaluation method is used? Do you simply select the maximum action from the deterministic network output?
- While the chosen task is interesting and aligns well with the proposed model, it would have been valuable to include an additional standard task where hierarchical behavior is expected to provide an advantage. This would facilitate comparison with existing work. This concern is less critical if the dataset used in the paper will be made publicly available.
Visualizations:
- Although the model does not reproduce exact test trajectories, it would have been helpful to include the closest trajectory from the training data (e.g., aligned using time-warping) for comparison with the test cases. This would demonstrate that the model is not simply memorizing the training data.
- Additional visualizations for baseline methods would have been useful, at least in the supplementary material.
Human Preference Study:
- The statement "During extrapolation, the other nine players do not animate" is unclear. I assume this means the players are not animated in the visualization but are still simulated for the network input. If so, this should be clarified. Additionally, this approach seems questionable, as it may limit the experts' ability to distinguish between real and generated strategies. It would have been more intuitive to highlight the target player instead.
Evaluation of the Macro-Planner:
- Figure 6b suggests that the agent rarely visits the actual macro-goals. This is surprising and could be a result of the fine-tuning procedure, which lacks macro-level supervision. This observation supports the concern that the model may not effectively separate macro and micro actions, as discussed earlier.
Benchmark Analysis:
- The variety of model combinations studied is commendable, as it enables clear attribution of performance gains to specific design choices.
- The evaluation in Table 2 has a significant limitation: it reports classification accuracy for a discretized 17x17 action space. Given the granularity of this discretization, low accuracies are unsurprising. While this metric aligns with the training objective, it does not account for cases where predicted actions are close to the ground truth but differ slightly, resulting in 0% accuracy. A distance-based metric would have been more appropriate.
- It would also have been helpful to evaluate stochastic action selection, which does not appear to be included in the table (though this is not explicitly stated).
- Reporting mean and variance (or medians with quantiles) across multiple test folds would have allowed for better assessment of statistical significance.
Minor Issues:
- The figures in the paper are of poor quality, making the model diagrams difficult to interpret. These should be replaced with vector graphics or high-resolution versions. It is surprising this was not identified prior to submission—was there an issue during the submission process?
- In-text citations do not follow the correct style. Please refer to the author guidelines and use the appropriate citation format (e.g., \citet/\citep with natbib).
- Several minor grammatical errors and missing articles were noted throughout the text. Specific examples include:
  - Line 22: "timescale low-level actions" → "timescale of low-level actions"
  - Line 33: The green "player" in Figure 1 is not mentioned in the text.
  - Line 68: "previous work which focus on" → "previous work which focuses on"
  - Line 96: "policy should simultaneously optimizes behavior" → "policy should simultaneously optimize behavior"
  - Line 176: "we used an simultaneously predicted the next 4" → "we used and simultaneously predicted the next 4"
  - Line 184: "but instead learns the final output from a feature" → "which instead learns the final output from a feature"
  - Line 255: "may applicable" → "may be applicable"