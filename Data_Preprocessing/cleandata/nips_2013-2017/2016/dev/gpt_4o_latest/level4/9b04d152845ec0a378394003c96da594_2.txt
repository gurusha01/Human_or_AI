The authors introduced a novel network architecture to extend deep residual learning for handling multi-modal inputs, specifically applied to the visual question-answering (VQA) task. Unlike prior approaches that employed explicit attention mechanisms, the proposed method builds on earlier work by utilizing element-wise multiplication between question and visual feature vectors to represent the joint residual function, which can be interpreted as an implicit attention mechanism. The authors explored several architectural variants and ultimately adopted a relatively simple learning block within a 3-block layered network. Their combination of deep residual learning and implicit attention proved effective on the Visual QA dataset, surpassing other state-of-the-art methods. 
To illustrate the attention effect of multi-modal residual learning, the authors proposed a visualization technique that highlights the difference between the visual input and the joint residual mapping through back-propagation across each learning block. Examples from the VQA task effectively demonstrated the implicit attention mechanism in action. By integrating two powerful concepts—deep residual learning and element-wise multiplication for implicit attention—the authors devised a solution applicable to general multi-modal tasks. Their experiments were meticulously conducted to identify an optimal architecture and hyper-parameters for the VQA task, yielding results that significantly outperformed prior studies employing various deep learning techniques.
However, it would be beneficial for the authors to provide additional comparisons with existing methods, particularly in terms of model parameter size and the amount of data required for training. Furthermore, it would be insightful to separately evaluate the contributions of residual learning and implicit attention to the VQA task, which could help clarify which component is most critical. The proposed visualization approach was compelling, and the examples convincingly demonstrated its utility in explaining the implicit attention mechanism. That said, further clarification on the differences between the three images at each block layer would enhance understanding, and an intuitive explanation of why a three-layer architecture appears optimal for this task would be valuable.