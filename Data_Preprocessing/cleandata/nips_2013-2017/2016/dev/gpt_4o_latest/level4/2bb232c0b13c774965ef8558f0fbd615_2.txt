The paper presents a valuable method for jointly segmenting and transcribing handwritten paragraphs into text. The authors employ MDLSTMs to achieve two key objectives: 1) encoding the input and 2) implementing an attention mechanism that effectively segments individual lines of text. A Bidirectional-LSTM decoder is then trained to generate text corresponding to the concatenated encoded sequences. The entire paragraph is optimized using a CTC loss function. This work represents a meaningful advancement over prior approaches, particularly those requiring explicit line segmentation, and demonstrates practical applicability. 
However, there are areas where the paper could be strengthened. For instance, incorporating a baseline that uses only Convolutional or Affine [5] layers would have been insightful, as these architectures are simpler to train compared to MDLSTMs. Additionally, the model's complexity could escalate if another layer of MDLSTMs with CTCs were introduced to identify paragraph boundaries. While the authors show improvements over their own baseline and achieve better results on the Rimes benchmark, the performance on the IAM benchmark does not surpass previous state-of-the-art results.