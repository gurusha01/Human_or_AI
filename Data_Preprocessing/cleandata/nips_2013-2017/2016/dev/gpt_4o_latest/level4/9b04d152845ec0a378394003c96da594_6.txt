This paper introduces Multimodal Residual Networks (MRN) for multimodal residual learning in visual question-answering tasks. The proposed framework employs CNN models for visual feature extraction and RNN models for processing language information. The MRN model is designed to effectively learn joint representations by integrating visual and linguistic information. The authors report state-of-the-art performance on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Additionally, they propose a novel method to visualize the attention mechanism of the joint representations. While the MRN demonstrates effectiveness for the Visual QA dataset and related tasks, the contributions of the paper are relatively straightforward. To the best of my knowledge, employing deep neural networks (DNNs) for joint representation learning is a common approach. The paper does not provide an analysis of which component in the framework—CNN, RNN, or MRN—contributes most significantly to the results. Including such a discussion or evidence would enhance the impact and depth of the work.