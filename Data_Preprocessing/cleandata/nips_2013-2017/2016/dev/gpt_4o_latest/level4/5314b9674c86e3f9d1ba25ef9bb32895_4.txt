The authors tackle the problem of image classification using tensor networks and optimization algorithms tailored for these structures. They provide a minimal theoretical evaluation of their approach and present results on the MNIST dataset. However, I have several concerns regarding the paper. First, the paper could be better situated within the existing body of literature. For example, classification is a primary application of generalized linear models (GLMs), such as logistic regression, and tensor methods have previously been employed to optimize mixtures of GLMs [1]. The paper lacks depth in its presentation. The authors essentially introduce a new algorithm but fail to provide compelling reasons for the reader to take an interest in it. Classification and regression algorithms are typically evaluated based on their expressive power (e.g., being dense in L2 space, for which the authors provide only an informal argument here) and their rates of convergence. For most regression and classification methods, minimax optimal rates are well-established, and I would expect some discussion of the statistical properties of the proposed regressor. Alternatively, the algorithm could be justified through computational advantages, such as improved runtimes or complexity compared to existing methods, but this aspect is also not addressed. Lastly, strong empirical performance could serve as motivation, but the authors provide only limited results on the MNIST dataset, with the empirical evaluation confined to a brief section in the middle of page 6. As a result, the paper comes across as yet another heuristic approach to a well-studied machine learning problem, leaving the reader questioning its significance. 
Some specific comments:  
- Line 19: What is meant by "exponentially large"? Exponential in what parameter?  
- Line 95: "The cost function for which we found our best test results..."â€”this phrasing is unclear. Since performance is defined in terms of a cost function, what exactly is meant by "best test results"?  
- Equation 6: How is \( \delta^\ell{Ln} \) defined? Does it equal 1 if \( L_n = \ell \)?  
- Line 168: What is meant by "the possible set of functions is quite general"? Please clarify.  
- Line 176: A basis is, by definition, complete. What you seem to mean is an orthonormal system.  
- The references should be listed in alphabetical order.  
[1] Provable Tensor Methods for Learning Mixtures of Generalized Linear Models; Sedghi et al.; AISTATS 2016.