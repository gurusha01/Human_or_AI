This paper introduces the use of the Barzilai-Borwein (BB) method to determine the learning rate for the stochastic gradient method and its variants. While the authors provide a linear convergence proof for the SVRG-BB method, the paper lacks a convergence analysis for the SGD-BB method. The proposed step size learning approach is novel for SVRG, but similar approaches for SGD have been extensively studied. Below are my main concerns:
1. In Fig. 1 (d, e, f), it is intriguing to observe that the BB step size converges to a constant value. However, in the paper by Allen-Zhu and Hazan, "Variance reduction for faster non-convex optimization," Fig. 5 demonstrates that a well-tuned, decreasing learning rate achieves a faster convergence rate for SVRG. Could you clarify why the BB step size converges to a constant in your method?  
2. There is a significant body of work on step size learning for SGD. In the experiments presented in the SGD-BB section, I believe additional comparisons with other methods, such as Adam and Adagrad, would strengthen the evaluation.