This paper presents a neural model adapted from [21] for visual question answering (VQA). The core concept involves iteratively refining the question representation by selecting relevant image regions. Each input image is processed using a VGG model, and features are extracted from 20 candidate regions per image. The image features are projected into a latent space shared with the question features. For the question, each word is embedded into a word vector using an embedding matrix, and the sequence of embedded vectors is fed into a GRU at each time step. The final hidden state of the GRU serves as the question representation, which is also mapped into the shared latent space. Through each reasoning layer, the question representation is updated using a multilayer perceptron (MLP) and weighted pooling, where an attention mechanism is applied. The final answer is produced via a softmax layer. Experimental results on the COCO-QA dataset demonstrate that the proposed model outperforms existing state-of-the-art approaches, while results on the VQA dataset are competitive with the best-performing models. The paper is generally well-written and clear. However, it could benefit from including basic descriptions of the baseline models and other state-of-the-art methods. The use of an attention mechanism in the weighted pooling step likely contributes to the performance gains, which seems reasonable. A comparison of different question representation pooling mechanisms would be a valuable addition, as the original paper [21] neither reported results for this aspect nor specified the exact mechanism used. Further clarification on the use of a single layer in the MLP would also be helpful. Additionally, the third example question in Figure 3 could be explained more clearly, as it appears that the question representation remains unchanged after being updated with one reasoning layer. A detailed analysis of the results would strengthen the paper. While the model builds on [21], this work seems to be the first to adapt that model for the VQA task. The authors have successfully applied an effective text-based question answering model to the VQA domain. Both this paper and [21] represent interesting contributions that open up new research directions in the QA/VQA field.