This paper introduces a model termed "communication-based machine translation" (CMT), which jointly enhances translation models for both translation directions by leveraging monolingual data through iterative forth- and back-translation, coupled with feedback collection after each sampled translation. The primary objective is to effectively utilize monolingual data to improve machine translation systems trained on bilingual corpora. The model is evaluated on two language pairs and compared against a standard neural machine translation (NMT) system and another system trained on pseudo-bilingual data. Similar objectives have been pursued in prior work, such as incorporating a target-side language model for re-ranking translations [1] or integrating it into the NMT system via deep or shallow fusion [2]. However, the paper does not adequately review the breadth of related work in this area, focusing narrowly on recent contributions by Sennrich et al. Given the extensive prior research on exploiting monolingual data for MT, more empirical comparisons are necessary to substantiate the claimed advantages of the proposed system. For instance, using the same dataset as [2] would have enabled a direct comparison. Additionally, prior work on unsupervised training of noisy-channel models [3] is overlooked and should be acknowledged. 
The paper also lacks comparisons to other communication-based learning frameworks in reinforcement learning. Instead of repeatedly describing the concept of two players communicating via translation models, the paper would benefit from a more comprehensive review and comparison with prior work. While the authors assert that their model opens new avenues for learning translation models from scratch using monolingual data, this claim is questionable. Both Ranzato et al. and Shen et al. [4], who explored reinforcement learning for translation models, observed that a warm-start model is essential due to the weakness of the feedback signal. Although the authors assume in line 116 that translation begins with "weak" models, they ultimately rely on "well-trained" models (line 152). The only experiment supporting significant improvements with monolingual data and a "weak" model is for Fr→En. Furthermore, the proposed extension to multiple languages in a translation chain is not as straightforward as suggested, as identifying the source of translation errors becomes increasingly challenging, potentially leading to noisy updates.
While the metaphor of a two-player game is appealing, it does not align well with the actual algorithm. The assumption that each player understands only one language allows for scoring via a language model, but the communication reward depends on knowledge of the translation model, which is unavailable to the players in the described setup. To validate the experiments as evidence supporting the proposed model, more details about the training setup (e.g., the number of monolingual and bilingual training instances) are required, along with stronger comparisons to prior work (e.g., using the original datasets from referenced studies and state-of-the-art systems as baselines). In summary, while the paper presents an interesting idea, it lacks sufficient experimental comparisons to closely related work, making it difficult to accept in its current form. 
[1] Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Martin Kay and Christian Boitet, editors, Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 1071–1080. Indian Institute of Technology Bombay.  
[2] Gulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H. C., ... & Bengio, Y. (2015). On using monolingual corpora in neural machine translation. arXiv preprint arXiv:1503.03535.  
[3] Mylonakis, Markos, Khalil Sima'an, and Rebecca Hwa. "Unsupervised estimation for noisy-channel models." Proceedings of the 24th international conference on Machine learning. ACM, 2007.  
[4] Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., & Liu, Y. (2015). Minimum risk training for neural machine translation. arXiv preprint arXiv:1512.02433.