The paper introduces two algorithms tailored for a relatively novel framework known as combinatorial partial monitoring (CPM). This framework merges the concepts of finite partial monitoring and combinatorial multi-armed bandits. The authors propose two algorithms designed to handle a stochastic adversary and derive theoretical regret bounds for both algorithms, encompassing both distribution-dependent and distribution-independent cases. The first algorithm, termed PEGE, integrates the classical forced-exploration technique from multi-armed bandits with the globally observable set to address CPM. The second algorithm, PEGE2, begins by estimating the gap \(\Delta\) between the expected rewards of the best action (assumed to be unique) and the second-best action. This estimate is subsequently utilized as input for PEGE. Additionally, the paper explores how this framework can be applied to online ranking with feedback. The primary contribution of the paper lies in presenting an algorithm that eliminates the assumption of a unique optimal action while still achieving state-of-the-art regret bounds, up to logarithmic factors. The framework considered in the paper is compelling, and the proposed algorithms are both straightforward and well-crafted. I particularly appreciate the PEGE algorithm for addressing the unrealistic assumption of a unique optimal action. 
1. My only question is: Intuitively, why does PEGE2 achieve a problem-dependent regret bound of \(O(\log T)\) when \(T_0\) is set to \(O(T^{2/3})\)?  
2. It would also be beneficial to include a discussion of this intuition in the paper.