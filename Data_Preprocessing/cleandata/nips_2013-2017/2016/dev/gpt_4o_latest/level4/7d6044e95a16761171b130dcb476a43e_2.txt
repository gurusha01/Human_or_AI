The proposed method essentially presents a finite mixture model, where each component distribution is a Gaussian latent variable model, with the likelihood means and covariances parameterized through neural networks. The core idea of the paper aligns with existing deep generative model (DGM) frameworks, in which the assumed likelihood is a finite mixture of Gaussians. What sets this work apart from existing DGMs lies in the formulation of the latent variable posteriors: rather than employing a Gaussian posterior (conditioned on the mixture component) parameterized via deep neural networks, the authors adopt a conventional approach that introduces a set of Normal-Wishart hyper-priors over the Gaussian posterior mean and precision matrix. I must admit that I am unable to grasp the rationale behind the authors' decision to forgo variational posterior amortization in favor of this more straightforward approach. Amortized variational posteriors represent the state-of-the-art methodology in the field and are widely recognized for their ability to enhance both modeling and inferential performance. The authors should have provided a much more thorough justification for this choice. Furthermore, the experiments presented in the paper fail to deliver compelling empirical evidence to demonstrate the effectiveness and practical value of the proposed approach. The evaluations are confined to a few simulated datasets and lack comprehensive comparisons with state-of-the-art methods.