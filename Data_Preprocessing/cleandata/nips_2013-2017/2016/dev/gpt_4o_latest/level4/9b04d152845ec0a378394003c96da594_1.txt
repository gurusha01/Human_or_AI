This paper integrates a residual-like network with multimodal fusion of language and visual features for the Visual Question Answering (VQA) task. The results on standard benchmarks are impressive, demonstrating notable progress. Although the contribution in terms of novelty is somewhat limited, accepting this paper could facilitate further advancements by enabling others to build upon the state-of-the-art results. Specifically, the paper combines a residual-like architecture with multimodal fusion for VQA, with a key innovation being the use of multiplicative interactions to merge visual features and word embeddings. The reported results surpass the state-of-the-art by a significant margin. However, the reliance on multiple pretrained models and embeddings warrants a more thorough analysis of their individual contributions. Additionally, referencing equations from other papers should be avoided; for instance, in Section 3.1, it would be preferable to reproduce the equations directly within this paper. Question: In Section 5.2, what would happen if sigmoid(W_q*q) were used as the attentional mask for visualization?