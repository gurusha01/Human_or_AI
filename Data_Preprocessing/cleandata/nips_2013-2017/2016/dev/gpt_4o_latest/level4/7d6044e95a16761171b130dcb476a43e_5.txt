The authors present a method for learning models that integrate nonlinear likelihoods derived from neural networks with structured latent variables. Their approach synthesizes several established techniques, including stochastic variational inference, message passing, and backpropagation via the reparameterization trick. Optimization efficiency is enhanced where applicable through the use of conjugate exponential families and natural gradients. The paper is well-written, featuring illustrative examples and thoughtful commentary on related work. It effectively combines multiple pre-existing methods and acknowledges recent studies that adopt a similar approach [7,19,20,21,22], which are cited and briefly discussed in Section 5. However, it would be beneficial to include a short discussion on Belanger and McCallum's ICML 2016 work, Structured Prediction Energy Networks. The provided examples effectively demonstrate scalability, and the accompanying videos are a valuable addition. Could the authors further clarify the unique contributions of their work? My primary concern is whether the integration of these existing tools constitutes a sufficiently novel contribution for this conference. That said, I do not hold a strong opinion on this matter and am not a specialist in this specific domain.