The paper introduces a method for constructing confidence intervals and p-values for the selection of groups of covariates. This method is applicable to a wide range of techniques, including forward selection, iterative hard-thresholding, and group lasso, which are used as illustrative examples in the study. The primary challenge addressed by the paper lies in accounting for the fact that the same dataset is used both to select the groups and to statistically assess their importance. The novelty of the work is its ability to construct confidence intervals, as prior methods for groups of covariates were limited to p-values. The contribution is built upon a key lemma that characterizes the distribution of truncated projections of Gaussian variables. The paper includes two small-scale experiments, one on synthetic data and another on real-world data.
Clarity/Presentation/Related Work/Presentation of Contributions: The paper is generally well-written and clearly presented. It is well-structured, providing a strong introduction to the problem and outlining the challenges and contributions (e.g., paragraph lines 103-115). However, there are a few areas where additional details would enhance the clarity (see specific comments below).
Technical Level: The paper appears technically sound, with Lemma 1 and Theorem 1 offering significant and non-trivial contributions. Nevertheless, some aspects of the proofs require further clarification (see detailed comments below).
Experiments: The experimental section could be improved, as it currently feels somewhat underwhelming. For instance, the purpose and conclusions of the second experiment (Section 4.2) are unclear. Additionally, further comparisons and discussions with alternative approaches would strengthen the experimental evaluation. For example, since the core challenge of this work is to handle the same dataset for both group selection and statistical inference, it would be valuable to compare the proposed method against a simpler strategy that splits the data, even if such a strategy is expected to reduce model selection accuracy and inference power.
Detailed Comments:
- It would be helpful to clarify earlier in the paper whether the groups are assumed to form a partition (i.e., whether overlaps are allowed).
- A discussion on extending the methodology beyond quadratic loss (or equivalently, the Gaussian model) would be insightful.
- Condition (1) should be elaborated upon: Why is this the appropriate quantity to consider?
- In Theorem 1, more details should be provided about the quantities being conditioned on and the random variables over which probabilities are computed.
- How is the equation \(\hat{fY}(L\alpha) = \alpha\) solved numerically (line 252)?
- A discussion on the computational complexity of the proposed approach across different scenarios (e.g., IHT, group lasso) would be beneficial.
- The purpose and takeaway of the second experiment (Section 4.2) remain unclear and should be better articulated.
- Could the proposed methodology be applied to techniques such as those described in [a] and references therein? Additionally, how would the proposed approach compare to a straightforward bootstrap method (e.g., along the lines of [b] but adapted for group-sparse estimators)?
Supplementary Material:
- At the beginning of the proof of Theorem 1, doesn't the continuity of \(t \mapsto fy(t)\) also need to be established to ensure the existence of \(L\alpha\)?
- Further details should be provided regarding the proofs of the properties of \(f_y\), such as its limiting behaviors and monotonicity.
References:
[a] Ndiaye, E.; Fercoq, O.; Gramfort, A. & Salmon, J. GAP Safe screening rules for sparse multi-task and multi-class models. Advances in Neural Information Processing Systems, 2015, 811-819  
[b] Bach, F. Bolasso: model consistent Lasso estimation through the bootstrap. Proceedings of the International Conference on Machine Learning (ICML), 2008