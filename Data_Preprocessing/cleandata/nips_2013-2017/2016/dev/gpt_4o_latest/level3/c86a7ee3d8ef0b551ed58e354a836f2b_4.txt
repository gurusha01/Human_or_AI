This paper addresses the critical problem of automatically determining step sizes for stochastic optimization methods, specifically SGD and SVRG, by leveraging the Barzilai-Borwein (BB) method. The authors propose two algorithms, SGD-BB and SVRG-BB, and provide theoretical and experimental insights into their performance. While the paper makes a meaningful contribution to the field, there are notable strengths and weaknesses that merit discussion.
Strengths:
1. Relevance and Importance: The paper tackles a significant challenge in stochastic optimization—automatically determining step sizes—offering a practical alternative to manual tuning or diminishing step sizes, which are often inefficient.
2. Theoretical Contributions: The authors provide a linear convergence analysis for SVRG-BB and SVRG-I under strongly convex objectives, filling a gap in the literature by proving the convergence of SVRG-I for the first time.
3. Experimental Validation: Numerical experiments demonstrate that BB step sizes achieve convergence rates comparable to or better than those obtained with best-tuned step sizes for both SGD and SVRG. This highlights the practical utility of the proposed methods.
4. Computational Efficiency: The BB step size computation incurs minimal additional cost, making the proposed methods computationally attractive.
5. Potential for Generalization: The authors discuss the applicability of BB step sizes to other SGD variants, broadening the scope of their approach.
Weaknesses:
1. Incomplete Theoretical Analysis: While SVRG-BB is rigorously analyzed, no convergence rate analysis is provided for SGD-BB. The smoothing technique introduced for SGD-BB appears heuristic and lacks theoretical justification, undermining the rigor of the method.
2. Limited Experimental Scope: Experiments are restricted to binary classification tasks (logistic regression and SVM). The absence of tests on regression problems or non-convex objectives, such as neural networks, limits the generalizability of the results.
3. Comparison with Related Work: The paper does not compare SGD-BB to other automatic step size methods, such as AdaGrad or Adam, nor does it discuss memory efficiency. This omission weakens the positioning of the proposed methods relative to existing techniques.
4. Initial Convergence Issues: The authors note slow convergence in SVRG-BB during initial epochs due to low step sizes but fail to provide sufficient intuition or analysis to address this issue.
5. Minor Issues: A typo in the supplementary material and the lack of sensitivity analysis for certain parameters detract from the overall clarity and polish of the paper.
Recommendation:
While the paper makes a valuable contribution by proposing BB-based step size schemes and analyzing their performance, the lack of theoretical guarantees for SGD-BB and the limited experimental scope are significant shortcomings. To strengthen the paper, the authors should:
1. Provide a convergence rate analysis for SGD-BB and justify the smoothing technique theoretically.
2. Expand experiments to include regression and non-convex problems, as well as comparisons with other adaptive step size methods.
3. Address the initial slow convergence in SVRG-BB with additional analysis or insights.
Arguments for Acceptance:
- Addresses an important and practical problem in stochastic optimization.
- Provides a novel application of the BB method to stochastic settings.
- Demonstrates strong empirical performance for SVRG-BB and SGD-BB.
Arguments Against Acceptance:
- Lacks theoretical guarantees for SGD-BB.
- Experiments are narrowly focused and do not explore broader applications or comparisons.
- Misses a discussion on related work and memory efficiency.
In conclusion, the paper has potential but requires additional theoretical and experimental work to fully establish its contributions. I recommend acceptance only if the authors address the major weaknesses in a revised version.