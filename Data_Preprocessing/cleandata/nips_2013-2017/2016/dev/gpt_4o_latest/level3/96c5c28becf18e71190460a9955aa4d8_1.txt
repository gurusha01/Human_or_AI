This paper explores the optimization of decision thresholds in drift-diffusion models (DDMs), a well-established framework for modeling decision-making in two-alternative forced choice (2AFC) tasks. The authors frame decision-making under uncertainty as a competitive evidence accumulation process, where decisions are triggered upon reaching a threshold. By incorporating Wald's cost function, they derive an optimal decision threshold that minimizes both stopping time and decision error. To address the challenge of optimizing highly stochastic single-trial rewards, the authors propose two methods: a reinforcement learning-based REINFORCE algorithm and a Bayesian optimization approach. Both methods are validated against exhaustive optimization and applied to experimental data, with REINFORCE emerging as a more biologically plausible model of animal learning.
Strengths:  
The paper makes a significant contribution to understanding decision threshold learning, a key aspect of perceptual decision-making. The use of reinforcement learning and Bayesian optimization to address the stochasticity of single-trial rewards is novel and well-motivated. The authors provide a thorough theoretical foundation, linking their work to the sequential probability ratio test (SPRT) and existing DDM literature. The comparison of the two methods is comprehensive, highlighting trade-offs between computational efficiency, convergence speed, and variance. Additionally, the application of the REINFORCE method to animal learning data is a compelling demonstration of its relevance to neuroscience, particularly in modeling basal ganglia function.
Weaknesses:  
While the theoretical contributions are strong, the paper would benefit from further validation using real human or animal data. Although the authors compare their methods to experimental acquisition data, the analysis remains limited and indirect. A direct application of their framework to behavioral datasets would significantly enhance the impact and practical relevance of the work. Additionally, the paper does not sufficiently address the question of how "optimal" human or animal decision-making truly is. Insights into deviations from optimality, or how the proposed methods account for such deviations, would provide a more nuanced understanding of decision-making processes.
Pro-Con Arguments for Acceptance:  
- Pro: The paper addresses a challenging and important problem in decision-making research, providing novel and theoretically sound solutions.  
- Pro: The REINFORCE method offers a biologically plausible model that aligns with experimental data, making it relevant to both machine learning and neuroscience communities.  
- Con: The lack of direct application to real-world data limits the paper's immediate impact.  
- Con: The discussion on the optimality of human decision-making is underdeveloped, leaving a gap in the broader implications of the work.
Conclusion:  
This paper is a strong theoretical contribution to the field of decision-making under uncertainty. However, its impact would be significantly enhanced by applying the proposed methods to real behavioral data and addressing the nuances of human and animal decision-making optimality. I recommend acceptance with minor revisions to address these points.