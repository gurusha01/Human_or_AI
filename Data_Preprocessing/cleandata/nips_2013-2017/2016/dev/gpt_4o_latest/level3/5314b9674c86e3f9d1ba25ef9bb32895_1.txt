This paper explores the application of Matrix Product States (MPS), a type of tensor network, to multiclass supervised classification tasks, with a focus on leveraging their efficiency in representing high-dimensional tensors. The authors adapt MPS to parameterize non-linear kernel learning models and demonstrate the approach on the MNIST dataset, achieving a test error rate of less than 1%. The paper builds on prior work ([3], [4]) but introduces a novel gradient descent-based optimization for MPS, contrasting with the unsupervised approach in [4]. Additionally, it proposes a tensor-inspired encoding for fixed-length vector data and discusses the interpretability of the learned model through the structure of the tensor network.
Strengths
1. Technical Soundness: The paper is technically rigorous, with a clear presentation of the MPS optimization algorithm and its application to kernel learning. The use of gradient descent for optimizing MPS is a meaningful contribution that distinguishes this work from prior approaches.
2. Performance: Achieving a sub-1% test error on MNIST with relatively low bond dimensions (e.g., \(m=120\)) is impressive and demonstrates the practical potential of the method.
3. Interpretability: The discussion on how the MPS structure enables feature selection and model interpretability is a valuable addition, offering insights into the inner workings of the proposed approach.
4. Efficiency: The linear scaling of the training cost with the dataset size is a significant advantage over traditional kernel methods, which often scale quadratically or worse.
Weaknesses
1. Accessibility: The paper assumes prior familiarity with MPS and tensor networks, which could hinder accessibility for readers outside the tensor network or quantum physics communities. A more comprehensive introduction to these concepts is recommended.
2. Incremental Contribution: While the gradient descent optimization is novel, the overall contribution feels incremental, as the work heavily builds on prior studies ([3], [4]). The differences with these references should be elaborated earlier in the paper for clarity.
3. Clarity Issues: The paper could benefit from clearer explanations of certain technical details, such as the "zig-zag ordering" (line 153), which appears inconsistent with Figure 8. Additionally, dimensions of vectors and matrices should be explicitly stated to improve readability.
4. Limited Discussion of Neural Networks: The connection to neural networks, briefly mentioned at the end, deserves more attention. Expanding on this could help position the work within the broader machine learning landscape.
5. Initialization and Local Minima: The paper does not adequately address the sensitivity of the MPS optimization to initialization or strategies for avoiding local minima, which are critical concerns in gradient-based methods.
6. Transparency: The tools used for Singular Value Decomposition (SVD) should be explicitly mentioned to enhance reproducibility.
Arguments for Acceptance
- The paper demonstrates strong performance on a benchmark dataset and introduces a novel optimization approach for MPS.
- The interpretability and efficiency of the method make it a promising direction for further research.
- The work aligns with the conference's focus on advancing machine learning methodologies.
Arguments Against Acceptance
- The contribution is incremental, with significant overlap with prior work ([3], [4]).
- Accessibility and clarity issues may limit the paper's impact on a broader audience.
- Key aspects, such as initialization and local minima sensitivity, are not sufficiently addressed.
Recommendation
Overall, the paper is a solid contribution to the application of tensor networks in machine learning. However, to maximize its impact, the authors should address the clarity and accessibility concerns, elaborate on differences with prior work, and provide a more in-depth discussion of neural network connections and optimization challenges. I recommend acceptance with minor revisions.