The paper introduces Multimodal Residual Networks (MRN), a novel approach for visual question answering (VQA) inspired by deep residual learning. The authors extend residual learning to multimodal tasks by leveraging element-wise multiplication for joint residual mappings, enabling efficient integration of visual and linguistic information. The proposed method achieves state-of-the-art results on the VQA dataset for both Open-Ended and Multiple-Choice tasks. Additionally, the paper presents a novel visualization technique using backpropagation to highlight attention effects, despite the absence of explicit attention parameters.
Strengths:
1. Technical Contribution: The adaptation of residual learning to multimodal tasks is innovative. The use of element-wise multiplication for joint residual mappings addresses bottlenecks in previous attention-based models like SAN, offering a simpler yet effective alternative.
2. State-of-the-Art Performance: The results on VQA benchmarks are impressive, with significant improvements over prior methods. The authors provide thorough quantitative evaluations, including comparisons with alternative models and ablation studies.
3. Novel Visualization Method: The backpropagation-based visualization of attention effects is a valuable contribution, offering interpretability without relying on explicit attention parameters. This could inspire further research in implicit attention mechanisms.
4. Clarity of Results: The experimental results are well-documented, with detailed analyses of alternative models, hyperparameters, and visual feature choices.
Weaknesses:
1. Background and Accessibility: While the paper is technically sound, it assumes familiarity with VQA and residual learning. Adding more background references on VQA would make the paper more accessible to non-experts.
2. Redundancy in Section 4.1: The description of the VQA dataset in Section 4.1 is overly detailed and occupies unnecessary space. This section could be condensed to focus on the key aspects relevant to the experiments.
3. Clarity Issues: 
   - The explanation of TrimZero (lines 136-137) is brief and unclear. A more detailed description of its mechanism and impact on training efficiency would enhance understanding.
   - The update rule `v = v + 1` in line 154 of Section 4.2 is mentioned without sufficient context. Elaborating on its purpose and implications would clarify its role in postprocessing.
Arguments for Acceptance:
- The paper addresses a significant problem in multimodal learning and advances the state of the art in VQA.
- The proposed MRN framework is both innovative and practical, with potential applications beyond VQA.
- The visualization method is a noteworthy contribution to the interpretability of deep learning models.
Arguments Against Acceptance:
- The paper could improve accessibility by providing more background and clarifying certain technical details.
- Minor redundancies and unclear explanations detract from the overall clarity.
Recommendation:
I recommend acceptance of this paper, contingent on addressing the clarity issues and condensing redundant sections. The contributions are substantial, and the results demonstrate the potential impact of MRN on multimodal learning tasks.