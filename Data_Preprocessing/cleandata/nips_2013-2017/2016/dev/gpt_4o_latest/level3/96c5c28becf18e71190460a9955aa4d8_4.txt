The paper investigates the learning of decision thresholds in the drift-diffusion model (DDM) through two distinct methods: the REINFORCE algorithm and Bayesian optimization. By framing threshold learning as a reinforcement learning problem, the authors aim to optimize single-trial rewards derived from Wald's cost function, which is challenging due to the stochastic nature of the rewards. The study is noteworthy for its dual approach: REINFORCE, a policy gradient method inspired by neural network training, and Bayesian optimization, which uses Gaussian processes to model the reward function. Both methods successfully approximate optimal thresholds, though they differ in convergence speed, computational cost, and variance. The authors also compare their findings to animal learning behaviors, suggesting that REINFORCE may better model biological learning processes.
Strengths:
1. Novelty and Scope: The study addresses an important open question in decision neuroscience—how animals learn decision thresholds—by proposing two distinct computational methods. The use of REINFORCE as a biologically plausible model and Bayesian optimization as a machine learning approach is a compelling combination.
2. Technical Soundness: The methods are rigorously implemented and validated against exhaustive optimization. The authors provide detailed theoretical and experimental analyses, including comparisons of convergence speed, variance, and computational cost.
3. Biological Relevance: The comparison of REINFORCE results to animal learning data (e.g., in rodents and bats) is a valuable contribution, bridging computational modeling and experimental neuroscience.
4. Clarity: The paper is well-written and organized, with clear explanations of the methods and results. Figures effectively illustrate key findings.
Weaknesses:
1. Asymptotic Behavior: A critical issue is the differing asymptotic values in Figures 2D and 3D, despite the existence of a unique Bayes risk minimum. The paper does not adequately address this discrepancy, raising questions about the robustness of the methods.
2. Lack of Empirical Data Integration: While the study references neural and behavioral data, it does not directly incorporate such data into the modeling or validation process. This limits the paper's relevance to experimental neuroscience.
3. Baseline in REINFORCE: The REINFORCE method could benefit from reinforcement comparison to reduce gradient variance, as the authors themselves acknowledge. This omission weakens the method's practical utility.
4. Time-Dependent Thresholds: Prior work (e.g., Drugowitsch et al.) has shown that optimal thresholds are often time-dependent. The paper does not address this limitation, which restricts the generalizability of the proposed methods.
Pro and Con Arguments for Acceptance:
- Pro: The paper is a significant contribution to understanding threshold learning, combining computational rigor with biological plausibility. It introduces novel methods and provides valuable insights into decision-making processes.
- Con: The lack of integration with empirical data and the unresolved asymptotic discrepancies reduce its impact. Additionally, the omission of time-dependent thresholds limits its applicability to real-world scenarios.
Recommendation: While the paper has notable strengths, addressing the raised concerns would significantly enhance its contribution. I recommend acceptance with major revisions, particularly focusing on resolving the asymptotic discrepancies, integrating empirical data, and exploring time-dependent thresholds.