This paper proposes using the Barzilai-Borwein (BB) method to automatically compute step sizes for Stochastic Gradient Descent (SGD) and its variant, Stochastic Variance Reduced Gradient (SVRG), leading to two new algorithms: SGD-BB and SVRG-BB. The authors address a critical challenge in stochastic optimization—choosing an appropriate step size—by leveraging the BB method, which eliminates the need for manual tuning or diminishing step sizes. The paper also provides a linear convergence proof for SVRG-BB and, as a by-product, fills a gap in the literature by proving the linear convergence of SVRG with Option I (SVRG-I). Experimental results demonstrate that SVRG-BB performs competitively with, and sometimes better than, SVRG with best-tuned step sizes, even when starting from arbitrary initial step sizes.
Strengths:
1. Technical Soundness: The paper is technically solid, with rigorous theoretical analysis supporting the proposed methods. The linear convergence proof for SVRG-BB and SVRG-I is a valuable contribution to the literature.
2. Practical Relevance: The BB method's ability to automatically tune step sizes makes the proposed algorithms highly practical, especially in scenarios where manual tuning is infeasible or time-consuming.
3. Experimental Validation: The experiments are thorough and demonstrate the competitiveness of SVRG-BB and SGD-BB across various datasets and optimization problems. The results show that the BB-based methods can adaptively find optimal step sizes, often outperforming existing approaches.
4. Clarity and Organization: The paper is well-written and logically structured. The motivation for using the BB method is clearly articulated, and the analyses are easy to follow for readers familiar with stochastic optimization.
5. Incremental but Valuable Contribution: While the contribution is incremental, it complements existing work on SVRG and provides a practical solution to a long-standing problem in stochastic optimization.
Weaknesses:
1. Limited Scope of Theoretical Guarantees: The theoretical analysis focuses primarily on SVRG-BB. While SGD-BB is introduced and tested experimentally, the lack of theoretical guarantees for its convergence is a notable limitation.
2. Incremental Nature: The use of the BB method in optimization is not entirely novel, and its application to stochastic methods, while useful, may not represent a groundbreaking advancement.
3. Broader Applicability: The paper does not explore the potential of the BB method for other stochastic optimization variants beyond SGD and SVRG. This limits the generality of the contribution.
Suggestions for Improvement:
1. Extend the theoretical analysis to include convergence guarantees for SGD-BB, as this would strengthen the paper's theoretical contributions.
2. Explore the applicability of the BB method to other stochastic optimization methods, such as SAGA or Adam, to demonstrate its broader utility.
3. Provide more detailed comparisons with state-of-the-art adaptive step size methods like AdaGrad or Adam to contextualize the practical impact of the proposed algorithms.
Recommendation:
Accept with Minor Revisions. The paper makes a solid and practical contribution to stochastic optimization by addressing a critical issue of step size selection. While the contribution is incremental, the theoretical and experimental results are compelling, and the proposed methods have significant practical relevance. Addressing the theoretical guarantees for SGD-BB and extending the scope of applicability would further enhance the paper's impact.