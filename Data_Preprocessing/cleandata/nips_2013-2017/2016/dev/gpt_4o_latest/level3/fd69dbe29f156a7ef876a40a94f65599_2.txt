Review of the Paper
This paper presents a novel adaptation of the neural reasoning framework for the visual question answering (VQA) task by treating image regions as a sequence of "facts" and iteratively updating question representations based on interactions with these regions. The authors propose a reasoning network that integrates object proposals and attention mechanisms to focus on relevant image regions, achieving state-of-the-art performance on the COCO-QA dataset and competitive results on the VQA dataset.
Strengths:
1. Adaptation of Neural Reasoning Framework: The paper effectively adapts the neural reasoning framework, originally designed for text-based question answering, to the VQA domain. This is a non-trivial extension, as it involves handling image regions as facts and integrating visual and textual modalities.
2. Attention Mechanism: The use of an attention mechanism to focus on relevant image regions is well-motivated and demonstrates the model's ability to handle complex visual relations.
3. Performance on COCO-QA: The model achieves slightly better performance than state-of-the-art methods on the COCO-QA dataset, showcasing its potential in specific VQA tasks.
4. Qualitative Analysis: The visualization of attention masks and updated question representations provides valuable insights into the model's reasoning process and its ability to focus on relevant regions.
Weaknesses:
1. Limited Generalization to VQA Dataset: While the model performs well on COCO-QA, its results on the VQA dataset are only comparable or slightly worse than existing methods. This raises questions about its generalizability and robustness across datasets.
2. Ambiguous Questions: The paper claims to address ambiguous questions requiring image context for disambiguation, but the experimental results and qualitative examples do not convincingly demonstrate that the proposed method handles such questions better than other models.
3. Lack of Comparative Discussion: The paper does not sufficiently discuss the pros and cons of the proposed model compared to state-of-the-art methods, particularly on the VQA dataset. For instance, the limitations in handling counting tasks and ambiguous questions are not adequately explored.
4. Unintuitive Figures: Figure 3 and its associated questions are noted as unintuitive and ambiguous, which detracts from the clarity of the qualitative analysis.
5. Overemphasis on COCO-QA: The paper focuses heavily on COCO-QA results, which may not fully represent the challenges of the broader VQA task.
Suggestions for Improvement:
1. Provide a more detailed analysis of why the model underperforms on the VQA dataset compared to COCO-QA and discuss potential avenues for improvement.
2. Include additional experiments or ablation studies to demonstrate the model's ability to handle ambiguous questions more effectively than existing methods.
3. Improve the clarity and relevance of qualitative examples, especially in Figure 3, to better illustrate the model's strengths.
4. Expand the discussion on the limitations of the model, such as its counting ability and reliance on object proposals, to provide a more balanced evaluation.
Recommendation:
While the paper introduces a novel adaptation of the neural reasoning framework and achieves promising results on COCO-QA, its limited generalization to the VQA dataset and insufficient discussion of its comparative advantages and limitations weaken its overall contribution. I recommend acceptance with minor revisions, provided the authors address the concerns regarding ambiguous questions, comparative analysis, and clarity of qualitative examples. This work has potential but requires further refinement to make a stronger impact on the VQA community.