The paper proposes a novel approach, Multimodal Residual Networks (MRN), for visual question-answering (VQA), leveraging deep residual learning principles to effectively integrate visual and language modalities. By utilizing element-wise multiplication for joint residual mappings, MRN addresses the bottleneck issues observed in prior attention-based models like Stacked Attention Networks (SAN). The authors achieve state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks, demonstrating the method's effectiveness. Additionally, the paper introduces an innovative visualization technique to interpret the attention effects of joint representations, providing valuable insights into the model's decision-making process. 
Strengths:  
The paper is technically sound, with well-supported claims validated through extensive experiments. The proposed MRN method is a significant contribution to the field, as it extends deep residual learning to multimodal tasks, enabling deeper and more efficient networks. The results on the Visual QA dataset are compelling, outperforming existing state-of-the-art methods. The novel visualization technique is particularly noteworthy, as it provides a high-resolution interpretation of implicit attention effects, setting it apart from explicit attention mechanisms in prior works. The paper is well-written, clear, and organized, making it accessible to readers. The experiments are thorough, exploring alternative models, hyperparameter choices, and the impact of various design decisions, which strengthens the validity of the conclusions.
Weaknesses:  
While the paper references related works, it could benefit from a more detailed comparison with recent advancements, such as the "Dynamic Memory Networks for Visual and Textual Question Answering" (ICML 2016). This would help situate MRN more clearly within the broader landscape of multimodal learning. Additionally, while the visualization method is innovative, its practical utility in improving model performance or guiding future designs is not fully explored. The paper also acknowledges that performance on certain answer types, such as "Number," remains suboptimal compared to human benchmarks, leaving room for further improvement.
Pro and Con Arguments for Acceptance:  
Pro:  
1. The method is novel and achieves state-of-the-art results, advancing the field of VQA.  
2. The visualization technique offers a unique perspective on implicit attention mechanisms.  
3. The paper is well-executed, with clear writing and robust experimental validation.  
Con:  
1. Limited discussion of comparisons with recent related works.  
2. The practical implications of the visualization method are not fully explored.  
3. Performance on certain answer types remains below human-level accuracy.  
Conclusion:  
This paper represents a strong scientific contribution to the field of multimodal learning and visual question-answering. Despite minor shortcomings, its novelty, clarity, and experimental rigor make it a valuable addition to the conference. I recommend acceptance, with a suggestion to include a more detailed discussion of related works and practical implications of the visualization method.