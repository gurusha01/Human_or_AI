The paper proposes using the Barzilai-Borwein (BB) method to compute step sizes for stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) methods, introducing two algorithms: SGD-BB and SVRG-BB. The authors provide a linear convergence proof for SVRG-BB on strongly convex functions and demonstrate its efficacy in numerical experiments. However, the paper lacks a convergence analysis for SGD-BB, which is a notable gap given the extensive research on SGD variants. The work addresses step size learning for SVRG, an underexplored area compared to the well-studied step size adaptation for SGD, positioning the paper as a valuable contribution to the field.
Strengths:
1. Novelty and Originality: The application of the BB method to stochastic optimization is innovative, particularly for SVRG, where step size learning is less explored. The linear convergence proof for SVRG-BB is a significant theoretical contribution, and the by-product proof for SVRG-I fills a gap in the literature.
2. Practical Relevance: The proposed methods eliminate the need for manual step size tuning, which is time-consuming in practice. The numerical results show that both SGD-BB and SVRG-BB perform comparably to or better than their counterparts with best-tuned step sizes.
3. Clarity: The paper is well-organized, with detailed explanations of the BB method, its adaptation to SVRG and SGD, and the smoothing technique for SGD-BB. The numerical experiments are thorough and demonstrate the practical utility of the proposed methods.
Weaknesses:
1. Convergence Analysis for SGD-BB: While the authors provide a convergence proof for SVRG-BB, the lack of theoretical guarantees for SGD-BB weakens the paper's overall rigor. This omission is particularly concerning given the extensive research on SGD variants.
2. Experimental Comparisons: The experiments lack comparisons with widely-used adaptive methods such as Adam and Adagrad, especially in the SGD-BB section. Including these baselines would strengthen the empirical evaluation and contextualize the performance of SGD-BB.
3. Step Size Behavior: The reviewer finds it intriguing that the BB step size converges to a constant in Figures 1(d-f). However, the paper does not explain why this occurs, especially since prior work suggests that a diminishing learning rate often leads to faster convergence for SVRG.
Arguments for Acceptance:
- The paper introduces a novel and practical approach to step size learning for SVRG, supported by strong theoretical and experimental results.
- The linear convergence proof for SVRG-BB and the supplementary proof for SVRG-I are valuable contributions to the literature.
Arguments Against Acceptance:
- The lack of convergence analysis for SGD-BB is a significant theoretical gap.
- The experimental section could be improved by including comparisons with other adaptive methods and addressing the observed behavior of the BB step size.
Recommendation:
The paper makes a meaningful contribution to step size learning in stochastic optimization, particularly for SVRG. However, addressing the theoretical gap for SGD-BB and expanding the experimental comparisons would significantly enhance its impact. I recommend acceptance with minor revisions to address these concerns.