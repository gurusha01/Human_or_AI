This paper addresses the critical problem of determining optimal action timing in decision-making under uncertainty, a topic of significant interest in neuroscience and machine learning. By framing the problem as finding the optimal decision threshold in the drift-diffusion model (DDM) for two-alternative forced choice tasks, the authors connect their work to foundational theories like Wald's sequential probability ratio test (SPRT) and Bayes risk minimization. The proposed methods, REINFORCE and Bayesian optimization, offer distinct approaches to learning these thresholds, with REINFORCE grounded in reinforcement learning and Bayesian optimization leveraging probabilistic modeling. Both methods are validated against exhaustive optimization, demonstrating their efficacy in learning near-optimal thresholds.
Strengths:  
The paper is well-motivated and addresses a significant problem in decision-making under uncertainty. The connection between DDM and SPRT is well-established, and the authors effectively extend this framework by introducing learning mechanisms for decision thresholds. The comparison of REINFORCE and Bayesian optimization is thorough, highlighting trade-offs in convergence speed, computational cost, and variance. The authors also provide a compelling argument for the biological plausibility of REINFORCE, supported by comparisons to animal learning data and its alignment with basal ganglia function. The results are robust, and the paper offers valuable insights into the trade-offs between computational efficiency and biological realism.
Weaknesses:  
While the paper is technically sound, it has several limitations. First, the models focus exclusively on Bayes risk and do not incorporate the biological cost of observing and accumulating evidence, which is a critical factor in real-world decision-making. This omission raises questions about the ecological validity of the proposed methods. Second, while REINFORCE is argued to be biologically plausible, the paper does not provide a concrete implementation that could be directly mapped onto neural circuits. Third, the Bayesian optimization method, while computationally expensive, is not explored in terms of potential improvements, such as incremental updates to the Gaussian process. Finally, the discussion on extending the methods to multi-alternative tasks or time-varying thresholds is speculative and lacks experimental validation.
Pro and Con Arguments for Acceptance:  
Pro:  
- The paper addresses a significant and well-defined problem.  
- The methods are novel and rigorously evaluated.  
- The REINFORCE method aligns well with biological learning mechanisms.  
- The results advance understanding of decision threshold optimization.  
Con:  
- Lack of consideration for the biological cost of evidence accumulation.  
- No biologically plausible implementation of the proposed methods.  
- Limited exploration of potential improvements to Bayesian optimization.  
Conclusion:  
This paper makes a valuable contribution to the field of decision-making under uncertainty by proposing and evaluating two methods for learning decision thresholds. However, the lack of a biologically plausible implementation and the omission of evidence accumulation costs limit its impact. I recommend acceptance with minor revisions to address these concerns.