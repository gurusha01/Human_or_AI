The paper introduces a novel method for selective inference in group sparsity, enabling the construction of confidence intervals and p-values for selected variable groups. This work builds on prior selective inference methods, such as those by Lee et al. (2016) and Loftus and Taylor (2015), and extends them to the group-sparse setting. The authors' key contribution is a technical result that precisely characterizes the distribution of the magnitude of the projection of data onto a given subspace, conditioned on a selection event. This result, termed the "truncated projection lemma," generalizes the "polyhedral lemma" of Lee et al. to group sparsity, allowing for inference on group-sparse selection methods like group lasso, iterative hard thresholding (IHT), and forward stepwise regression. The paper also provides numerical experiments on simulated and real-world datasets to validate the utility of the proposed method.
Strengths:
1. Technical Novelty: The paper makes a significant technical contribution by deriving the truncated projection lemma, which extends selective inference to group-sparse settings. This is a meaningful advancement over prior work that was limited to non-grouped sparse regression.
2. Broad Applicability: The method is applicable to a range of group-sparse selection algorithms, including group lasso, IHT, and forward stepwise regression, making it versatile and relevant to practitioners.
3. Practical Utility: The authors demonstrate the method's practical utility through experiments on simulated data and a real-world dataset (California county health data). The results show that the method provides valid p-values and confidence intervals, even in finite-sample settings.
4. Theoretical Rigor: The paper is well-grounded in theory, with clear derivations and proofs provided for the main results. The authors also discuss the limitations of existing methods and how their approach overcomes these challenges.
5. Clarity: The paper is well-organized and clearly written, with detailed explanations of the methodology, theoretical results, and experimental setup.
Weaknesses:
1. Computational Complexity: While the method is theoretically sound, the computational burden of evaluating the region \( R_Y \) for some algorithms (e.g., group lasso) may limit its scalability to very high-dimensional datasets. The reliance on numerical approximations for certain cases, such as group lasso, could also introduce variability in results.
2. Limited Real-World Validation: Although the method is tested on a real-world dataset, the scope of applications is relatively narrow. Additional experiments on diverse datasets and domains would strengthen the paper's claims of practical utility.
3. Comparison to Alternatives: While the paper positions itself as an extension of prior work, it does not provide a direct empirical comparison to alternative selective inference methods in terms of computational efficiency or statistical power.
Arguments for Acceptance:
- The paper makes a novel and significant contribution to the field of selective inference, addressing a gap in the literature for group-sparse settings.
- The method is theoretically rigorous and broadly applicable, with potential to impact both research and practice.
- The experimental results support the validity and utility of the proposed approach.
Arguments Against Acceptance:
- The computational complexity of the method, particularly for group lasso, may limit its practical applicability in high-dimensional settings.
- The paper could benefit from additional real-world applications and comparisons to alternative methods.
Recommendation:
I recommend acceptance of this paper. Its contributions to selective inference in group sparsity are significant, and the method has the potential to advance the state of the art in this area. However, the authors should consider addressing the computational challenges and expanding the scope of real-world applications in future work.