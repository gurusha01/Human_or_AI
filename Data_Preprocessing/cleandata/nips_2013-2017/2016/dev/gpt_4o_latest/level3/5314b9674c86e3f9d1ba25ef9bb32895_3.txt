The paper presents an innovative approach that bridges the fields of tensor networks and machine learning by leveraging matrix product states (MPS) for image classification tasks. The authors demonstrate the efficacy of their method on the MNIST dataset, achieving less than 1% test error with relatively small MPS bond dimensions. This work is significant as it introduces a novel application of tensor networks, traditionally used in physics, to supervised learning tasks, offering computational efficiency and interpretability. The adaptive nature of the MPS structure and its ability to regularize the model by reducing parameters are particularly noteworthy contributions.
Strengths:
1. Interdisciplinary Contribution: The paper effectively combines tensor network methods with machine learning, opening new avenues for research in both fields.
2. Performance: Achieving sub-1% test error on MNIST with small bond dimensions is impressive and demonstrates the potential of the proposed method.
3. Efficiency: The approach scales linearly with the training set size, a significant improvement over traditional kernel methods.
4. Interpretability: The use of MPS provides insights into feature selection and model structure, which is a valuable advantage over many black-box machine learning models.
5. Reproducibility: The authors provide publicly available code, which enhances the paper's accessibility and impact.
Weaknesses:
1. Clarity of Presentation: The explanation of approximating the weight tensor as an MPS in Section 5 is unclear and could benefit from additional detail and examples. Similarly, the process of obtaining \(\tilde{\Phi}_n\) from the data and the meaning of "leading-order update" in equations (7)-(8) require further elaboration.
2. Connection to Prior Work: The relationship between the proposed method and the approach in [15] (Cichocki) is insufficiently discussed. Clarifying the differences and any added complexities would strengthen the paper.
3. Discussion on Regularization: While the authors mention the regularization benefits of the MPS structure, the discussion lacks depth and fails to connect to existing work on spectral regularization and SVD truncation.
4. Algorithmic Usability: Adding a step-by-step algorithm would significantly improve the paper's clarity and usability for practitioners.
5. Fourier Features: Equation (3) appears to involve Fourier features, but the connection to random Fourier features in large-scale problems is not explored, missing an opportunity to link the work to broader machine learning literature.
Suggestions for Improvement:
1. Enhance the clarity of technical sections, particularly those involving MPS approximation and gradient updates, with diagrams or pseudocode.
2. Provide a detailed comparison with [15] and other related works to better situate the contribution within the existing literature.
3. Expand the discussion on regularization, explicitly linking it to prior research on spectral methods.
4. Include a step-by-step algorithm for the proposed method to aid reproducibility and understanding.
5. Discuss the potential connections between the proposed feature map and random Fourier features to position the work within the broader context of kernel methods.
Recommendation:
While the paper has some clarity and contextualization issues, its contributions are significant and original. The proposed method is technically sound, demonstrates strong empirical performance, and has the potential to inspire further research. I recommend acceptance with minor revisions, focusing on improving the clarity of presentation and strengthening the connections to prior work.