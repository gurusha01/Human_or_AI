The paper introduces a novel algorithm, Truncated Generalized Gradient Flow (TGGF), for solving systems of quadratic equations, a problem that is NP-hard in general. The authors propose a two-stage approach: an orthogonality-promoting initialization and a refinement stage using truncated generalized gradient updates. Unlike prior work, such as Chen and Candes (2015), the truncation method in TGGF retains large-sized gradients, which the authors argue improves performance. Theoretical guarantees are provided for the noiseless case, and numerical simulations demonstrate the algorithm's superiority over state-of-the-art methods like Truncated Wirtinger Flow (TWF) and Wirtinger Flow (WF). However, the paper has several areas for improvement.
Strengths:
1. Novelty and Contributions: The paper introduces a new truncation rule and an orthogonality-promoting initialization, which are significant departures from existing methods. The theoretical guarantees, including exponential convergence and near-optimal sample complexity, are compelling contributions.
2. Performance: Numerical experiments show that TGGF achieves lower sample complexity and faster convergence compared to TWF and WF. For example, TGGF achieves perfect recovery with a measurement-to-unknown ratio of 3 for real Gaussian models, compared to 5 for TWF.
3. Scalability: The algorithm operates in linear time, O(mn log(1/Îµ)), which is proportional to the time required to read the data. This makes it computationally efficient for large-scale problems.
4. Clarity in Numerical Comparisons: The paper provides detailed numerical results, including success rates and relative errors, which convincingly demonstrate the advantages of TGGF.
Weaknesses:
1. Definition 1 (Generalized Gradient): The introduction of the generalized gradient lacks sufficient justification or reference in the main text. This leaves a gap in understanding its role in the algorithm.
2. Clarity Issues: Equation 9 is unclear due to the undefined variable \( h \). Additionally, the statement on Line 154 invoking the strong law of large numbers is questionable, as it does not account for the dependence among summands caused by data-dependent fixed points.
3. Typographical Errors: Line 145 likely contains a typo where "necessary" should be replaced with "sufficient."
4. Limited Scope: The paper focuses exclusively on the noiseless case in its theoretical analysis. Addressing the noisy case or discussing the challenges of generalizing the main theorem would enhance the paper's completeness.
5. Assumptions: The algorithm heavily relies on Gaussian data assumptions, as noted in Line 164. Its applicability to non-Gaussian distributions is not discussed, limiting its generalizability.
6. Practical Validation: While the algorithm is tested on synthetic data, real-world applications, such as image recovery, are not explored. Including such experiments would strengthen the paper's practical relevance.
Recommendation:
The paper makes a significant contribution to the field of nonconvex optimization and quadratic equation solving, with strong theoretical and empirical results. However, the lack of clarity in some definitions, limited discussion of noisy cases, and absence of real-world validations are notable weaknesses. I recommend acceptance with minor revisions, provided the authors address the clarity issues, justify the use of the generalized gradient, and discuss the algorithm's applicability to non-Gaussian data and noisy settings.
Pro/Con Summary:
Pros:
- Novel algorithm with strong theoretical guarantees.
- Superior empirical performance compared to state-of-the-art methods.
- Linear-time complexity suitable for large-scale problems.
Cons:
- Clarity issues in definitions and equations.
- Limited discussion of noisy cases and non-Gaussian data.
- Lack of real-world experimental validation.