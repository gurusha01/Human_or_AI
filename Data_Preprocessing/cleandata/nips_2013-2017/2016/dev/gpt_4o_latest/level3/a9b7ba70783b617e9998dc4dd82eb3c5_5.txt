This paper introduces methods for constructing and summarizing confidence sets for the unknown true cluster tree, addressing a critical gap in statistical inference for clustering. The authors propose a framework that combines metrics on trees, bootstrap-based confidence set construction, and pruning techniques to identify statistically significant features of an empirical cluster tree. The methods are demonstrated on synthetic datasets and a real-world Graft-versus-Host Disease (GvHD) dataset, showcasing their utility in both simulated and practical scenarios.
The work builds on prior research in density clustering and hierarchical clustering, particularly the foundational contributions of Hartigan (1975) and Chaudhuri and Dasgupta (2010). While previous studies have focused on estimation and consistency of cluster trees, this paper is among the first to address statistical inference for these structures. The authors also leverage recent advances in computational topology to explore metrics on trees, such as the `∞ metric and merge distortion metrics, which are critical for constructing confidence sets.
Strengths:
1. Novelty: The paper addresses an important and underexplored problem in clustering—quantifying uncertainty in cluster tree estimation. The introduction of confidence sets and pruning techniques is a significant contribution to the field.
2. Technical Rigor: The theoretical framework is well-developed, with clear definitions, proofs, and justifications for the choice of metrics and methods. The use of the bootstrap for confidence set construction is particularly well-motivated.
3. Practical Utility: The application to the GvHD dataset demonstrates the real-world relevance of the proposed methods, highlighting their ability to discern meaningful differences between distributions.
4. Clarity of Results: The synthetic examples effectively illustrate the pruning techniques and their ability to remove noise-induced features, enhancing interpretability.
Weaknesses:
1. Lack of Comparative Evaluation: The paper does not compare its methods against existing approaches for clustering or tree-based inference. While the authors argue that their work is novel, a comparison with related methods (e.g., single-linkage clustering or other hierarchical clustering techniques) would strengthen the experimental validation.
2. Bandwidth Selection: The reliance on the Silverman reference rule for bandwidth selection is a limitation, as this method may not be optimal for tree inference. A more targeted bandwidth selection strategy would improve the robustness of the results.
3. Limited Exploration of Metrics: While the paper justifies the use of the `∞ metric, it dismisses the modified merge distortion metric (dMM) without fully exploring alternative approaches to address its statistical inference challenges. This limits the scope of the proposed framework.
4. Scalability: The computational feasibility of the methods, particularly for high-dimensional or large-scale datasets, is not thoroughly discussed.
Recommendation:
I recommend acceptance with minor revisions. The paper presents a novel and technically sound contribution to statistical inference for cluster trees, with clear potential for impact in both theoretical and applied domains. However, the authors should address the lack of comparative evaluation and provide more discussion on bandwidth selection and scalability. Including a comparison with existing methods would significantly strengthen the paper's experimental section.
Arguments for Acceptance:
- Novel and impactful contribution to statistical inference for clustering.
- Rigorous theoretical framework and practical utility demonstrated on real data.
- Clear and well-organized presentation of methods and results.
Arguments Against Acceptance:
- Lack of comparison with other methods weakens the experimental validation.
- Limited discussion of computational scalability and alternative metrics.
Overall, this paper is a valuable addition to the field and aligns well with the scope of the conference.