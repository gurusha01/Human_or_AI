This paper addresses the critical problem of statistical inference for cluster trees, which represent hierarchical structures of high-density clusters in a dataset. The authors make several notable contributions. First, they investigate metrics for comparing cluster trees, demonstrating the equivalence of the \( l\infty \) and merge distortion norms, and ultimately opting for \( l\infty \) due to its suitability for statistical inference. Second, they propose a method to construct asymptotic confidence sets for cluster trees using kernel density estimation (KDE) and bootstrapping, providing a rigorous framework for quantifying uncertainty in cluster tree estimation. Third, they introduce a pruning procedure to simplify cluster trees by removing statistically insignificant features, along with a partial ordering framework to compare trees. The paper also highlights the importance of constructing confidence intervals for cluster trees and provides significant theoretical advancements in this area.
Strengths:
1. Novelty and Significance: The paper addresses a critical gap in clustering literature by focusing on statistical inference for cluster trees, a topic that has received limited attention. The introduction of confidence sets and pruning procedures is a significant theoretical contribution that could have broad applicability in clustering and density estimation.
2. Technical Rigor: The theoretical analysis is thorough, with clear definitions, proofs, and justifications for the chosen metrics. The equivalence of \( l_\infty \) and merge distortion norms is particularly well-argued and relevant for the proposed inference framework.
3. Practical Utility: The pruning procedures and confidence set construction are practical tools that enhance the interpretability and robustness of cluster tree estimates. The application to both synthetic and real-world datasets (e.g., GvHD) demonstrates the utility of the proposed methods.
4. Clarity of Results: The experiments are well-designed and effectively illustrate the benefits of pruning and confidence set construction. The pruning procedure successfully identifies meaningful features while eliminating noise, as shown in both synthetic and real datasets.
Weaknesses:
1. Metric Limitations: While the paper justifies the use of \( l\infty \), the inability to construct confidence sets using the modified merge distortion metric (\( d{MM} \)) is a limitation. \( d_{MM} \) aligns more closely with the cluster tree structure, and its exclusion leaves open questions about whether better metrics could be developed for this task.
2. Bandwidth Selection: The reliance on the Silverman reference rule for bandwidth selection is suboptimal for tree inference. A more targeted bandwidth selection method could improve the robustness and accuracy of the proposed methods.
3. Complexity of Confidence Sets: The confidence sets are described as containing infinitely complex trees, which may hinder interpretability. While the pruning procedure addresses this partially, a more comprehensive approach to identifying minimal trees in the confidence set would be valuable.
4. Limited Related Work Discussion: While the paper references foundational works, it could benefit from a broader discussion of related clustering and statistical inference methods, particularly recent advances in hierarchical clustering and computational topology.
Recommendation:
This paper is a strong contribution to the field of clustering and statistical inference. Its theoretical advancements and practical tools are likely to be of significant interest to the NeurIPS community. However, addressing the limitations of \( d_{MM} \) and bandwidth selection, as well as providing a more comprehensive discussion of related work, would strengthen the paper further. I recommend acceptance, with minor revisions to address these points.
Arguments for Acceptance:
- Novel and significant contributions to statistical inference for cluster trees.
- Thorough theoretical analysis and practical utility demonstrated through experiments.
- High relevance to the NeurIPS audience, given the focus on clustering and density estimation.
Arguments Against Acceptance:
- Limitations in metric selection and bandwidth optimization.
- Complexity of confidence sets may hinder interpretability.
- Related work discussion could be expanded.