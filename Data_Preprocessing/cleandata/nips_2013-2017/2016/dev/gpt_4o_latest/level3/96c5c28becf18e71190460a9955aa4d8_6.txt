This paper explores the challenging problem of learning decision thresholds in two-alternative forced-choice (2-AFC) tasks, applying Williams' REINFORCE algorithm and Bayesian optimization to address the stochastic nature of rewards derived from Wald's cost function. The authors frame the problem as reinforcement learning, where the goal is to minimize the difference between the optimal decision policy's reward and the collected reward. While the algorithms themselves are not novel, the focus on threshold estimation in this context is a fresh and underexplored contribution to decision science.
The paper is technically sound, with both methods successfully converging to near-optimal thresholds. Bayesian optimization demonstrates faster convergence (∼500 trials) compared to REINFORCE (∼5000 trials), but at the expense of higher computational cost and greater variance in threshold estimates. The REINFORCE method, on the other hand, aligns better with animal learning behavior, as evidenced by its resemblance to acquisition curves observed in rodent and bat experiments. This makes REINFORCE a plausible model for biological learning mechanisms, particularly in the basal ganglia.
However, the paper has several weaknesses. The methods section lacks clarity, with insufficient detail about the algorithms and their implementation. For instance, the explanation of the REINFORCE learning rule and its connection to the neural network policy could be more explicit. Similarly, the Bayesian optimization approach would benefit from a clearer description of the acquisition function and Gaussian process modeling. The absence of illustrative figures to visually compare the two methods further hampers understanding. Additionally, the discussion section is overly verbose and could be condensed to focus on key insights.
Another limitation is the fixation of other parameters, which reduces the problem to a naive parameter search. While this simplification aids tractability, it limits the broader applicability of the methods, particularly for time-dependent thresholds in finite-horizon tasks. The paper also does not sufficiently address related work in decision science, where similar problems have been studied using alternative approaches.
In summary, the paper addresses an important and underexplored problem with technically sound methods, but its impact is somewhat diminished by clarity issues, limited novelty in the algorithms, and a lack of broader applicability. To improve, the authors should enhance the clarity of the methods, include illustrative figures, condense the discussion, and address related work more comprehensively. While the work is relevant to psychology and cognitive science, its contributions are incremental rather than groundbreaking. 
Arguments for acceptance:
- Novel focus on decision threshold estimation in 2-AFC tasks.
- Technically sound methods with validation against exhaustive optimization.
- REINFORCE method aligns well with biological learning behavior.
Arguments against acceptance:
- Lack of clarity and insufficient methodological detail.
- Limited novelty in the algorithms and parameter fixation.
- Incremental contribution with limited applicability to broader scenarios.
Recommendation: Weak accept.