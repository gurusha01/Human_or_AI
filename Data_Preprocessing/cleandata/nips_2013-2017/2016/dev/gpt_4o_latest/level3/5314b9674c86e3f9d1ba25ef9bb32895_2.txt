This paper explores the application of tensor networks, specifically matrix-product state (MPS) decomposition, to supervised learning tasks, with a focus on data representation and classification. The authors adapt concepts from physics to machine learning, proposing an MPS-based framework for parameterizing non-linear kernel learning models. They demonstrate their approach on the MNIST dataset, achieving a test error rate below 1% for a bond dimension of 120. The paper also discusses the representational power of MPS and its implications for feature selection and model interpretability.
Strengths:
1. Novelty in Cross-Disciplinary Approach: The paper introduces tensor network techniques from physics into machine learning, which is a promising direction for leveraging physics-inspired methods in data-driven tasks.
2. Efficient Optimization: The proposed MPS-based model offers computational advantages, with training scaling linearly with the dataset size, a significant improvement over traditional kernel methods.
3. Interpretability: The discussion on the implicit feature selection and the structure of the learned model adds value, providing insights into the inner workings of the MPS framework.
4. Reproducibility: The authors provide publicly available code, which is commendable and facilitates further exploration by the community.
Weaknesses:
1. Lack of Background for ML Audience: The paper assumes familiarity with tensor networks and MPS decomposition, which may alienate readers from the machine learning community. A more accessible introduction to these concepts is needed.
2. Dense and Ambiguous Notation: The notation is overly dense, with ambiguities in equations and definitions that hinder comprehension. For example, the explanation of the sweeping optimization algorithm could benefit from clearer diagrams and step-by-step elaboration.
3. Limited Experimental Validation: The experimental results are restricted to the MNIST dataset, which is a relatively simple benchmark. The lack of details on data splitting and the absence of experiments on more diverse datasets limit the generalizability of the findings.
4. Insufficient Explanation of Hyperparameters: Critical parameters, such as the choice of local feature maps and bond dimensions, are inadequately justified, leaving questions about their applicability to other data modalities.
5. Unclear Novelty: The paper does not sufficiently differentiate its contributions from prior work, such as [4], and misses an opportunity to explore the connection between tensor networks and ANOVA kernels in depth.
6. No Discussion on Uniqueness of MPS Decomposition: The impact of the non-uniqueness of MPS representations on learning and generalization is not addressed, which is a significant gap.
Recommendation:
While the paper presents an interesting and potentially impactful approach, its weaknesses in clarity, experimental rigor, and novelty articulation prevent it from being a strong candidate for acceptance in its current form. To improve, the authors should:
1. Provide a more accessible introduction to tensor networks for machine learning readers.
2. Clarify notation and equations to enhance readability.
3. Expand experimental validation to include diverse datasets and provide details on data splitting.
4. Justify hyperparameter choices and discuss the implications of MPS decomposition's non-uniqueness.
5. Clearly articulate the novelty of their contributions relative to prior work.
Pro/Con Summary:
- Pros: Novel cross-disciplinary approach, computational efficiency, interpretability, reproducibility.
- Cons: Limited accessibility, weak experimental validation, unclear novelty, insufficient hyperparameter explanation.
Final Decision: Weak Reject.