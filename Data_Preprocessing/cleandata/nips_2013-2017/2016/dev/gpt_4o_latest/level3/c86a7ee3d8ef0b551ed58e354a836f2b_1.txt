The paper proposes a novel approach to adapt step-sizes in stochastic gradient descent (SGD) and its variant, stochastic variance reduced gradient (SVRG), using the Barzilai-Borwein (BB) method. This eliminates the need for predefined step-size schemes, which often require tedious manual tuning. The authors introduce a smoothing technique to stabilize the BB step-size in SGD, and they demonstrate the efficacy of their methods—SGD-BB and SVRG-BB—through theoretical analysis and numerical experiments.
Strengths:
1. Novelty and Practical Relevance: The use of the BB method to compute adaptive step-sizes in stochastic optimization is innovative and addresses a critical challenge in SGD: step-size selection. This work could significantly reduce the manual effort required in hyperparameter tuning, making it highly relevant for practitioners.
2. Theoretical Contributions: The authors provide a rigorous analysis, proving the linear convergence of SVRG-BB for strongly convex objectives. As a by-product, they also establish the linear convergence of SVRG-I, filling a gap in the literature.
3. Empirical Validation: Numerical experiments on standard datasets for logistic regression and SVM demonstrate that SGD-BB and SVRG-BB achieve performance comparable to or better than SGD and SVRG with best-tuned step-sizes. The methods also outperform some advanced SGD variants, showcasing their practical utility.
4. Generality: The BB step-size and smoothing technique are adaptable to other SGD variants, broadening the applicability of the proposed methods.
Weaknesses:
1. Overshooting in Initial Iterations: As highlighted in Figure 1, the BB method exhibits strong overshooting toward very small step-sizes in the initial iterations. This behavior appears suboptimal and may delay convergence in early stages.
2. Smoothing Technique Drawback: The smoothing formula in SGD-BB reintroduces a deterministic decrease (1/k+1), which could replicate some limitations of predefined schemes. This partially undermines the paper's goal of avoiding predefined step-size schedules.
3. Theoretical Gap in SGD-BB: While SVRG-BB is supported by strong theoretical guarantees, the convergence analysis for SGD-BB is less comprehensive, relying on heuristic arguments for the smoothing technique.
4. Lemma 1 Correction: The expectation in Lemma 1 should be a conditional expectation, as noted. This oversight, though minor, raises concerns about the rigor of the theoretical analysis.
Pro and Con Arguments for Acceptance:
Pros:
- The paper addresses a significant and practical problem in stochastic optimization.
- It introduces a novel and theoretically sound method (SVRG-BB) with strong empirical results.
- The work is well-positioned to impact both research and practice in machine learning.
Cons:
- The overshooting issue and the deterministic nature of the smoothing technique warrant further investigation.
- The theoretical analysis for SGD-BB is less robust compared to SVRG-BB.
- Minor errors, such as the correction needed in Lemma 1, detract from the overall rigor.
Recommendation:
Overall, this paper makes a meaningful contribution to adaptive step-size methods in stochastic optimization. While some concerns remain, particularly regarding SGD-BB and the smoothing technique, the strengths outweigh the weaknesses. I recommend acceptance, provided the authors address the overshooting issue and clarify the theoretical underpinnings of SGD-BB in a revised version.