This paper introduces a novel modification to the collapse layer of standard Multi-Dimensional Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) for offline handwriting recognition. By replacing the traditional global average pooling with a weighted average pooling mechanism, the authors aim to enable end-to-end recognition of handwritten paragraphs without requiring explicit line segmentation. The weights for pooling are learned using a recurrent attention layer, implemented as an MDLSTM, which dynamically incorporates previous weights and encoded feature vectors. The model employs Connectionist Temporal Classification (CTC) loss, allowing it to bypass the need for character or line-level alignment during training. The attention mechanism is interpreted as a "soft" line segmentation, iteratively focusing on one text line at a time.
The paper demonstrates competitive or superior performance compared to prior methods, including [6], on the Rimes and IAM datasets. Notably, the proposed system decodes an entire line at each time step, offering significant speed and performance improvements over character-level decoding. However, the reviewer questions the level of technical novelty compared to [6], as the proposed method appears to be an evolution rather than a fundamentally new approach. Clarification on this point is requested in the rebuttal. Additionally, the reviewer suggests visualizing the attention maps when input image lines exceed the algorithm's time steps to better understand the model's behavior in such cases.
Strengths:
1. End-to-End Processing: The proposed model eliminates the need for explicit line segmentation, addressing a critical bottleneck in handwriting recognition pipelines.
2. Performance: The system achieves competitive or better results compared to state-of-the-art methods, even without ground-truth line segmentation.
3. Efficiency: Decoding full lines at each time step significantly improves computational efficiency.
4. Interpretability: The attention mechanism provides interpretable insights into the model's implicit line segmentation.
Weaknesses:
1. Technical Novelty: While the modification to the collapse layer is effective, its novelty relative to [6] is unclear and warrants further explanation.
2. Scalability: The model is limited to paragraph-level transcription and cannot handle arbitrary reading orders or complex document layouts without additional preprocessing.
3. Attention Visualization: The paper would benefit from visualizations of attention behavior in edge cases, such as when the number of lines exceeds the model's time steps.
Recommendation:
The paper is a strong contribution to the field of handwriting recognition, advancing the state of the art in end-to-end paragraph transcription. However, the authors should clarify the novelty of their approach compared to [6] and address the scalability limitations in future work. Visualizing attention maps in challenging scenarios would also strengthen the paper. Overall, this work is well-suited for acceptance, provided the authors address these concerns in the rebuttal.