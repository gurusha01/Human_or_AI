The paper explores the application of tensor networks, specifically matrix product states (MPS), to supervised learning tasks, with a focus on image classification using the MNIST dataset. The authors propose a novel optimization algorithm inspired by the density matrix renormalization group (DMRG) method from physics and demonstrate its ability to parameterize non-linear kernel learning models. The paper highlights the computational advantages of tensor networks, such as linear scaling with training set size and adaptive bond dimensions, and reports a test error of less than 1% on MNIST for a bond dimension of 120. The authors also provide an interpretation of the learned model's structure and discuss the representational power and implicit feature selection of tensor networks.
Strengths:
1. Novelty: The paper introduces an interesting application of tensor networks, a concept from quantum physics, to machine learning. This interdisciplinary approach is innovative and could inspire further exploration of tensor methods in AI.
2. Computational Efficiency: The proposed algorithm demonstrates linear scaling with the training set size, which is a significant improvement over traditional kernel methods.
3. Empirical Results: Achieving less than 1% test error on MNIST is a strong result, especially given the relatively small bond dimensions used.
4. Interpretability: The paper provides insights into the structure of the learned model, highlighting the feature selection capabilities of tensor networks, which is a valuable contribution.
Weaknesses:
1. Theoretical Underpinning: The paper lacks a rigorous theoretical evaluation of the proposed method. There is no formal analysis of its expressive power, statistical properties, or generalization guarantees, leaving the approach largely heuristic.
2. Limited Empirical Evaluation: The results are restricted to the MNIST dataset, which is relatively simple and well-studied. This limits the generalizability of the findings to more complex datasets or tasks.
3. Insufficient Literature Embedding: The paper does not adequately connect its work to existing literature on generalized linear models (GLMs) or tensor methods for optimizing mixtures of GLMs. This oversight weakens the contextualization of the contribution.
4. Terminology and Clarity: The paper contains unclear terminology (e.g., "exponentially large") and ambiguous phrasing (e.g., "best test results"). Some definitions, such as delta^ellLn, are incomplete, and there are inaccuracies, such as confusing "basis" with "orthonormal system."
5. References and Organization: The references are not alphabetically ordered, and the paper could benefit from better organization and clarity in certain sections.
Recommendation:
While the paper introduces an intriguing approach and achieves promising results, its weaknesses in theoretical grounding, empirical evaluation, and literature embedding are significant. To strengthen the contribution, the authors should:
1. Provide a more thorough theoretical analysis of the proposed method.
2. Extend empirical evaluations to more challenging datasets and tasks.
3. Improve the clarity and precision of the writing and terminology.
4. Situate the work more comprehensively within the existing literature.
Arguments for Acceptance:
- Novel and interdisciplinary approach.
- Promising empirical results on MNIST.
- Potential for broader applications and interpretability.
Arguments Against Acceptance:
- Limited theoretical and empirical evaluation.
- Weak connections to existing literature.
- Issues with clarity and organization.
Overall, the paper has potential but requires substantial revisions to meet the standards of a high-impact conference like NeurIPS.