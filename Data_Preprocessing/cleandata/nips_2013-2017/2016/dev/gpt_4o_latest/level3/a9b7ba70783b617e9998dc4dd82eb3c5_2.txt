This paper presents a novel approach to statistical inference for cluster trees of probability density functions, focusing on hierarchical high-density regions. The authors introduce a bootstrap-based confidence set for cluster trees and propose methods to extract statistically significant features from these sets. They also explore metrics for comparing cluster trees, ultimately advocating for the use of the \(d_\infty\) metric due to its computational tractability and well-understood statistical properties. The paper is applied to both synthetic and real-world datasets, including a Graft-versus-Host Disease (GvHD) dataset, demonstrating the utility of the proposed methods.
Strengths:
1. Originality and Significance: The paper addresses a critical gap in clustering methodologies by introducing statistical inference techniques for cluster trees. This is an important contribution, as it allows researchers to distinguish genuine topological features from noise, advancing the interpretability of density clustering.
2. Technical Soundness: The use of bootstrap methods for constructing confidence sets is well-justified, and the pruning methods proposed for simplifying cluster trees are both intuitive and statistically grounded. The authors also provide theoretical guarantees for their confidence sets, which are dimension-independentâ€”a significant advantage in high-dimensional settings.
3. Clarity: The paper is well-organized and clearly written, with detailed explanations of the proposed methods and their theoretical underpinnings. The inclusion of synthetic and real-world examples enhances the accessibility and practical relevance of the work.
4. Broader Impact: The methods have potential applications in various domains, such as bioinformatics (as demonstrated with the GvHD dataset), where understanding hierarchical structures in data is crucial.
Weaknesses:
1. Relation to Nonparametric Inference: While the focus on \(ph\) (the biased density) instead of \(p0\) (the true density) is justified for cluster tree comparison, the implications for density comparison are less clear. A more thorough discussion of this trade-off would strengthen the paper.
2. Metric Selection: The choice of the \(d\infty\) metric, while practical, could benefit from further justification. The paper dismisses the \(d{MM}\) metric due to its lack of Hadamard differentiability but does not explore alternative metrics in sufficient depth. This limits the scope of the analysis.
3. Pruning Methodology: The pruning methods, while effective, are somewhat heuristic. The authors acknowledge that there may be other trees in the confidence set that are simpler but do not provide a comprehensive algorithm to identify them. This is an open question that warrants further exploration.
4. Bandwidth Selection: The reliance on the Silverman reference rule for bandwidth selection is a limitation. A more targeted bandwidth selection method tailored to tree inference would enhance the robustness of the approach.
Arguments for Acceptance:
- The paper introduces a novel and impactful method for statistical inference in clustering, addressing a significant gap in the literature.
- The theoretical contributions, particularly the dimension-independent convergence rates, are robust and well-supported.
- The practical applications demonstrated in the experiments highlight the utility of the proposed methods.
Arguments Against Acceptance:
- The paper could benefit from a deeper exploration of alternative metrics and a more thorough discussion of its relationship to nonparametric density inference.
- The pruning methods and bandwidth selection strategy, while functional, leave room for improvement and refinement.
Recommendation:
Overall, this paper makes a strong contribution to the field of statistical clustering and density estimation. While there are areas for improvement, the originality, technical rigor, and practical relevance of the work justify its acceptance. I recommend acceptance with minor revisions to address the noted weaknesses.