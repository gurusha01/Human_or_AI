The paper introduces a novel approach to adaptive step-size selection for stochastic gradient descent (SGD) and its variant, stochastic variance reduced gradient (SVRG), using the Barzilai-Borwein (BB) method. This work addresses a critical challenge in stochastic optimization: the difficulty of selecting appropriate step sizes, which traditionally requires manual tuning or diminishing schedules. The authors propose two algorithms, SGD-BB and SVRG-BB, and provide theoretical guarantees, including the first proof of linear convergence for SVRG with "Option I" (final iterate of the inner iteration), a practical choice often used but previously lacking theoretical justification. Numerical experiments demonstrate that the proposed methods are competitive with or superior to SGD and SVRG with best-tuned step sizes, making this a promising contribution to the field.
Strengths:
1. Novelty and Practical Relevance: The use of the BB method for adaptive step-size selection in stochastic optimization is innovative. The automatic computation of step sizes aligns with practical needs, reducing the reliance on manual tuning.
2. Theoretical Contributions: The paper provides rigorous proofs of linear convergence for SVRG-BB and SVRG with Option I, filling a gap in the literature.
3. Experimental Validation: The numerical results are compelling, showing that SGD-BB and SVRG-BB perform comparably to or better than their fixed step-size counterparts. The authors also demonstrate that the methods are robust to the choice of initial step sizes.
4. Clarity and Organization: The paper is well-structured, with clear explanations of the algorithms and theoretical results. The inclusion of a smoothing technique for SGD-BB to stabilize step sizes is a thoughtful addition.
5. Addressing Limitations: The authors acknowledge the quadratic dependence on problem constants as a limitation but emphasize the practical advantage of automatic step-size selection.
Weaknesses:
1. Lack of Empirical Comparisons: The paper does not include comparisons with existing step-size tuning heuristics, such as SAG or MISO line-search methods. This omission weakens the empirical completeness of the work.
2. Dependence on Unknown Constants: While the BB method is theoretically justified, its dependence on unknown constants limits its practical applicability in some scenarios.
3. Performance Ceiling: The BB step-size bounding approach does not outperform the best fixed step-size in the SVRG setting, suggesting that there is room for improvement.
4. Condition Number Restriction: Although the revised version addresses a severe restriction on the condition number, the method still requires careful parameter choices for convergence guarantees.
Suggestions for Improvement:
1. Include empirical comparisons with existing line-search strategies (e.g., Le Roux et al., Mairal) to strengthen the experimental section.
2. Discuss potential strategies to mitigate the dependence on unknown constants in the BB method.
3. Explore hybrid approaches that combine the BB method with other adaptive techniques to address its performance ceiling in certain settings.
Recommendation:
This paper makes a significant contribution to the field of stochastic optimization by introducing a theoretically sound and practically relevant adaptive step-size strategy. While the lack of comparisons with existing methods and dependence on unknown constants are notable limitations, the strengths of the work outweigh these weaknesses. I recommend acceptance, provided the authors address the empirical comparison gap in a future revision.