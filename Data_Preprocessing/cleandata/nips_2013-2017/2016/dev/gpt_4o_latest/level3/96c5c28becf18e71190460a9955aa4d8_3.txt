This paper investigates decision threshold optimization in the drift-diffusion model (DDM) using two distinct methods: a reward-modulated learning rule based on Williams' REINFORCE algorithm and Bayesian optimization with a Gaussian process. The authors demonstrate that both methods can converge to near-optimal thresholds, validated against exhaustive optimization. While Bayesian optimization converges faster, REINFORCE is computationally less expensive and exhibits lower variance in threshold estimates. The paper also highlights the biological plausibility of REINFORCE as a model for animal learning, supported by comparisons with behavioral data from rodents and bats.
Strengths:  
The paper is well-written and provides a thorough evaluation of practical algorithms for decision threshold optimization. The use of simulations to validate the methods is robust, and the comparison between REINFORCE and Bayesian optimization is insightful. The authors contribute to the theoretical literature on DDMs by addressing a challenging optimization problem involving stochastic rewards. Additionally, the discussion on the biological plausibility of REINFORCE and its alignment with animal learning data is a valuable contribution to the field.
Weaknesses:  
Despite its strengths, the paper has notable limitations. First, while the theoretical contributions are significant, the lack of direct applications to neuroscience or psychology limits its interdisciplinary impact. The cognitive or biological plausibility of the algorithms is not sufficiently explored, and supporting empirical evidence is minimal. For instance, while REINFORCE is suggested as a plausible model for basal ganglia function, this claim could benefit from more direct empirical validation or simulation of neural mechanisms. Second, the paper does not explain why Bayesian optimization exhibits larger variance in threshold estimates, which is a critical observation. Third, the absence of learning curves as a function of computation time makes it difficult to assess algorithm performance under computational constraints. Finally, the paper does not discuss or compare its methods with prior neural network models for threshold optimization, such as those by Simen et al. (2006) and Frank (2006), which would provide a more comprehensive context for the proposed approaches.
Minor Issues:  
The paper contains unclear definitions (e.g., W0/W1 as error costs), grammatical errors, and incorrect terminology in some sections. These should be addressed for clarity and precision.
Arguments for Acceptance:  
- The paper provides a novel and rigorous evaluation of two optimization methods for decision thresholds in DDMs.  
- The REINFORCE method is a promising candidate for modeling reward-driven learning in animals.  
- The theoretical contributions are significant and could inspire future work in both machine learning and neuroscience.
Arguments Against Acceptance:  
- The lack of applications to neuroscience or psychology limits the paper's broader relevance.  
- Key issues, such as the variance in Bayesian optimization and comparisons with prior neural network models, are not addressed.  
- The absence of learning curves and insufficient empirical evidence weaken the practical evaluation of the methods.
Recommendation:  
I recommend acceptance with major revisions. The paper's theoretical contributions are strong, but addressing the weaknesses and incorporating comparisons with prior work would significantly enhance its impact.