This paper addresses the inference problem for cluster trees in density estimation, proposing a bootstrap-based method for constructing confidence sets and a pruning mechanism for visualization. The authors explore various metrics for comparing trees and focus on the $\ell_\infty$ metric, supported by theoretical analysis and numerical examples. The work is positioned as a novel contribution to statistical inference for cluster trees, building on prior work in density clustering and tree metrics.
Strengths
The paper makes a significant theoretical contribution by introducing a bootstrap-based confidence set for cluster trees and proposing a partial ordering for pruning statistically insignificant features. The use of the $\ell_\infty$ metric is well-motivated, given its computational tractability and statistical properties. The authors provide a rigorous theoretical framework, including asymptotic guarantees for the proposed confidence sets, and demonstrate the utility of their approach on synthetic and real-world datasets. The pruning mechanism for simplifying trees is particularly valuable for visualization and interpretability, addressing a practical challenge in hierarchical clustering.
The discussion of related work is thorough, situating the paper within the broader literature on density clustering and tree metrics. The authors also highlight the limitations of alternative metrics, such as the merge distortion metric, and justify their focus on the $\ell_\infty$ metric. This clarity in motivation strengthens the paper's originality and coherence.
Weaknesses
While the theoretical development is robust, the numerical examples fall short of effectively showcasing the richness of the proposed confidence sets. For instance, the synthetic examples primarily demonstrate the pruning mechanism but do not explore the variability or interpretability of the confidence sets in depth. The "Mickey Mouse" example raises concerns about the consistency of cluster heights, which could undermine the method's reliability in certain cases.
The partial ordering for comparing trees is limited, as it assumes that the main split occurs at the same location across different estimates. This assumption is restrictive and may reduce the applicability of the method within confidence sets. Additionally, the metrics in Section 3 are defined for general functions, not necessarily density functions representing the same population, which warrants clarification.
The definition of $p{inf}$ and $a$ in Lemma 1 is problematic for densities without compact support, and the equivalence of $d\infty$ and $d_M$ for different functions is not well-justified, particularly given the known bias of kernel density estimators at peaks and valleys. These issues could impact the robustness of the proposed method.
Recommendation
Pros for acceptance:
1. Theoretical rigor and novelty in addressing statistical inference for cluster trees.
2. Practical pruning mechanism for simplifying and visualizing cluster trees.
3. Clear motivation and positioning within the literature.
Cons for acceptance:
1. Limited empirical validation of the proposed confidence sets.
2. Restrictive assumptions in the partial ordering and metrics.
3. Unresolved questions about metric equivalence and applicability to non-compact densities.
Overall, the paper makes a valuable theoretical contribution but requires stronger empirical validation and clarification of key assumptions. I recommend acceptance with minor revisions to address these concerns.