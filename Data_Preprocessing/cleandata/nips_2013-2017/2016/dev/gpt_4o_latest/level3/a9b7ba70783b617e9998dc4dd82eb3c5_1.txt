This paper addresses the problem of statistical inference for cluster trees derived from density functions, proposing a novel method to construct confidence sets for such trees. The authors utilize kernel density estimators (KDE) to estimate cluster trees and measure accuracy using the sup-norm of the underlying density. They propose a bootstrap-based approach to construct confidence sets and introduce a pruning framework to simplify cluster trees while preserving key topological features. The paper is validated through experiments on both simulated datasets and a real-world Graft-versus-Host Disease (GvHD) dataset.
Strengths:
1. Novelty and Significance: The focus on statistical inference for cluster trees is a significant contribution, as this area has seen limited work compared to density estimation. The proposed methods address the critical issue of distinguishing genuine topological features from noise, which is essential for robust clustering.
2. Methodological Contributions: The introduction of a partial ordering to prune statistically insignificant features is a practical and interpretable approach. The pruning algorithms are well-motivated and demonstrate utility in both synthetic and real-world scenarios.
3. Validation: The experiments, particularly on the GvHD dataset, effectively showcase the practical applications of the proposed methods. The pruning results align well with the expected structure of the data.
4. Clarity: The paper is well-written and organized, making the technical content accessible. The authors provide detailed explanations of their methods and include visualizations that enhance understanding.
Weaknesses:
1. Curse of Dimensionality: While the authors claim that a fixed bandwidth enhances convergence rates in high-dimensional settings, the paper lacks guarantees or a principled method for bandwidth selection. The arbitrary choice of bandwidth (e.g., Silverman's rule) undermines the robustness of the approach in high dimensions.
2. Theorem 3: The lack of a proof for Theorem 3 is a significant omission, especially since it does not explicitly show dependency on parameter \( B \). This raises concerns about the rigor of the theoretical guarantees.
3. Metric \( d{MM} \): The paper discusses the modified merge distortion metric (\( d{MM} \)) but ultimately dismisses it due to its unsuitability for bootstrap-based inference. The authors should consider omitting mentions of \( d_{MM} \) altogether, as it distracts from the core contributions.
4. Computational Complexity: While the proposed methods are computationally feasible, the paper does not discuss the scalability of the bootstrap procedure or pruning algorithms for very large datasets.
Arguments for Acceptance:
- The paper introduces a novel and impactful approach to statistical inference for cluster trees, addressing a gap in the literature.
- The pruning framework is both interpretable and effective, with demonstrated utility in real-world applications.
- The work is well-written and accessible, making it a valuable contribution to the NeurIPS community.
Arguments Against Acceptance:
- The lack of a principled bandwidth selection method and guarantees for high-dimensional settings limits the generalizability of the approach.
- The omission of the proof for Theorem 3 and its unclear dependency on \( B \) weakens the theoretical rigor.
- The inclusion of \( d_{MM} \), despite its dismissal, adds unnecessary complexity to the paper.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a strong contribution to the field, but the authors should address the concerns regarding bandwidth selection, provide the missing proof for Theorem 3, and remove mentions of \( d_{MM} \). These changes will strengthen the paper's impact and clarity.