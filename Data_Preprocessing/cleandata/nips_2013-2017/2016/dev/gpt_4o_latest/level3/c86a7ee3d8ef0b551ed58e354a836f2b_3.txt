This paper addresses a critical challenge in stochastic gradient descent (SGD) methods: selecting an appropriate step size without manual tuning or diminishing schedules. By incorporating the Barzilai-Borwein (BB) step size into SGD and its variants, particularly stochastic variance reduced gradient (SVRG), the authors propose two novel algorithms, SGD-BB and SVRG-BB. The paper's main contribution is the theoretical and empirical demonstration that SVRG-BB achieves linear convergence for strongly convex functions with Lipschitz continuous gradients, eliminating the need for step size tuning when the parameter \(m\) is sufficiently large. Additionally, the authors provide numerical experiments showing that the BB step size adapts to optimal values, achieving convergence rates comparable to or better than manually tuned step sizes.
Strengths:
1. Theoretical Contribution: The paper rigorously proves the linear convergence of SVRG-BB and, as a by-product, establishes the previously missing convergence proof for SVRG-I. This is a valuable theoretical advancement for the field.
2. Practical Relevance: By automating step size selection, the proposed methods address a significant practical bottleneck in SGD-based optimization, making them highly relevant for large-scale machine learning problems.
3. Empirical Validation: Numerical experiments on standard datasets demonstrate that SVRG-BB and SGD-BB achieve performance comparable to or better than SGD and SVRG with optimally tuned step sizes. The results also show that the BB step size converges to optimal values within a few epochs, underscoring its robustness.
4. Generality: The BB step size is shown to be adaptable to other SGD variants, such as SAG, broadening the applicability of the proposed approach.
Weaknesses:
1. Sensitivity to \(m\): While the paper simplifies step size tuning, it shifts the burden to tuning \(m\), which the authors acknowledge as a limitation. The empirical sensitivity of \(m\) is not extensively studied, leaving questions about its practical impact.
2. Time Complexity: Theorem 1 indicates that the time complexity increases linearly with \(m\), and doubling \(m\) proportionally slows convergence. This trade-off between computational cost and convergence speed could limit the method's scalability.
3. Empirical Scope: While the experiments are promising, they are limited to three datasets and two problem types (logistic regression and SVM). Broader evaluations on diverse tasks and datasets would strengthen the empirical claims.
4. Clarity of Presentation: While the paper is generally well-organized, some sections, such as the smoothing technique for SGD-BB, could benefit from additional explanation and simplification to improve accessibility for non-expert readers.
Arguments for Acceptance:
- The paper provides a novel and practical solution to a longstanding problem in stochastic optimization.
- Theoretical contributions, particularly the convergence proof for SVRG-I, are significant and fill a gap in the literature.
- Empirical results demonstrate the potential of the proposed methods to outperform existing approaches.
Arguments Against Acceptance:
- The reliance on tuning \(m\) and its sensitivity are not adequately addressed, which could undermine the practical utility of the method.
- The experimental evaluation, while promising, is somewhat narrow in scope and lacks diversity in datasets and problem types.
Recommendation:
This paper makes a meaningful contribution to the field of stochastic optimization by proposing a theoretically sound and practically relevant method for automatic step size selection. However, the sensitivity to \(m\) and the limited empirical scope are notable weaknesses. I recommend acceptance, provided the authors address the sensitivity of \(m\) in more detail and expand the empirical evaluation in a future revision.