The paper introduces Multimodal Residual Networks (MRN), a novel architecture extending deep residual learning to multimodal inputs for the Visual Question Answering (Visual QA) task. The authors propose a method that combines deep residual learning with implicit attention, achieved through element-wise multiplication, rather than relying on explicit attention mechanisms. This approach is validated through extensive experiments, demonstrating state-of-the-art performance on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Additionally, the authors present a novel visualization technique using back-propagation to illustrate the attention effects of their model, even in the absence of explicit spatial attention.
Strengths:  
The paper makes a significant contribution by extending residual learning to multimodal tasks, addressing the bottleneck issues in existing attention-based models like Stacked Attention Networks (SAN). The use of implicit attention through element-wise multiplication is both innovative and computationally efficient, avoiding the need for explicit attention parameters. The authors provide thorough experimental validation, including ablation studies and comparisons with alternative architectures, to justify their design choices. The proposed visualization technique is another strong point, offering interpretability for a model that lacks explicit attention mechanisms. The results, which outperform existing state-of-the-art methods, underscore the significance of the approach. Furthermore, the paper is well-organized, with clear explanations of the methodology, experiments, and results.
Weaknesses:  
While the paper is technically sound, certain aspects could benefit from further elaboration. For instance, a deeper analysis of the individual contributions of residual learning and implicit attention to the model's performance would strengthen the claims. Additionally, the rationale behind the choice of a three-block layered architecture, while empirically validated, could be better explained. The visualization technique, though novel, could be more rigorously compared to existing attention visualization methods to highlight its unique advantages. Finally, the paper does not provide a detailed comparison of model parameter sizes and data requirements with competing methods, which would be valuable for assessing the model's scalability and efficiency.
Pro and Con Arguments for Acceptance:  
Pro:  
1. Novel and effective combination of residual learning and implicit attention for multimodal tasks.  
2. State-of-the-art performance on a challenging dataset.  
3. Innovative visualization technique enhancing model interpretability.  
4. Thorough experimental design and validation.  
Con:  
1. Limited analysis of individual contributions of key components (residual learning vs. implicit attention).  
2. Insufficient explanation of architectural choices, such as the three-block design.  
3. Lack of comparison in terms of parameter efficiency and data requirements.  
Conclusion:  
Overall, this paper represents a significant advancement in multimodal learning and Visual QA. Despite minor areas for improvement, the strengths far outweigh the weaknesses. The novel methodology, strong empirical results, and potential for broader applicability make this paper a valuable contribution to the field. I recommend acceptance, with minor revisions to address the noted weaknesses.