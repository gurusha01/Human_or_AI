The paper proposes Multimodal Residual Networks (MRN) for visual question answering (VQA), leveraging element-wise multiplication for joint residual mapping. The authors extend deep residual learning to multimodal tasks, aiming to integrate vision and language representations effectively. The paper also introduces a novel visualization method to interpret the attention effects of joint residual mappings, arguing that MRN operates as an implicit attention model. The authors claim state-of-the-art results on the VQA dataset for both Open-Ended and Multiple-Choice tasks, while exploring alternative architectures and hyperparameter configurations.
While the paper is technically sound and provides a thorough experimental evaluation, it falls short in several critical areas. First, the novelty of the proposed approach is limited. The use of element-wise multiplication for joint residual mapping has been explored in prior works, such as [1], and the paper does not sufficiently differentiate itself from these existing frameworks. The authors acknowledge the influence of Stacked Attention Networks (SAN) [29] but fail to provide a compelling argument for why MRN represents a significant advancement beyond these methods. The visualization method, while interesting, is not sufficiently impactful to offset the lack of originality in the core architecture.
In terms of quality, the experiments are well-documented, and the authors explore a variety of configurations to validate their approach. However, the claim of achieving state-of-the-art performance is misleading. While the results are competitive, they do not surpass the latest benchmarks in the field, particularly for challenging answer types like "Number" and "Other." This undermines the significance of the contribution, as the method does not demonstrably advance the state of the art.
The paper is clearly written and well-organized, with detailed explanations of the methodology and experimental setup. However, the related work section could be expanded to better contextualize the contributions within the broader literature. For instance, the paper does not adequately address how it compares to other recent multimodal learning approaches beyond SAN.
In summary, the strengths of the paper lie in its clear presentation and thorough experimentation. However, the lack of novelty and underwhelming performance relative to state-of-the-art methods limit its impact. Arguments in favor of acceptance include the detailed exploration of alternative models and the introduction of a novel visualization technique. Arguments against acceptance include the lack of originality, limited significance of the results, and misleading claims about state-of-the-art performance. While the paper is a solid piece of work, it does not meet the bar for acceptance at a top-tier conference like NeurIPS.