The paper addresses the problem of learning optimal decision thresholds in the drift-diffusion model (DDM) for two-alternative forced choice tasks, proposing a meta-optimality framework to minimize Bayes risk by combining decision time and type I/II errors. The authors frame the optimization as a continuous-armed bandit problem and compare two approaches: a reinforcement learning method based on the REINFORCE algorithm and Bayesian optimization using Gaussian processes. While the paper tackles an interesting and relevant question with potential implications for psychophysics, it has notable strengths and weaknesses.
Strengths:
1. Relevance and Scope: The paper addresses a fundamental problem in decision-making models, with implications for psychophysics, neuroscience, and machine learning. The exploration of learning mechanisms for decision thresholds is novel and bridges reinforcement learning and psychophysics.
2. Methodological Rigor: The mathematical formulation of the problem, including the derivation of single-trial rewards from Wald's cost function, is sound. The comparison of REINFORCE and Bayesian optimization is well-motivated and systematically evaluated.
3. Empirical Results: The paper provides empirical evidence for the effectiveness of both methods in learning near-optimal thresholds. The REINFORCE method, in particular, is shown to align with observed animal learning behavior, adding biological plausibility.
4. Potential Impact: The work contributes to understanding how decision thresholds might be learned in biological systems and offers insights into reinforcement learning and Bayesian optimization in stochastic environments.
Weaknesses:
1. Assumptions and Justifications: The paper does not adequately justify key assumptions, such as the use of meta-reasoning and the specific design choices for the loss function and bandit strategy. These omissions weaken the theoretical foundation of the work.
2. Clarity and Presentation: The manuscript contains mathematical typos and conceptual ambiguities that hinder understanding. For instance, the derivation of the reward function and the rationale for specific parameter choices could be more clearly explained.
3. Novelty and Real-World Relevance: While the methods are effective, the findings are not sufficiently novel. The REINFORCE algorithm and Bayesian optimization are well-established techniques, and their application here does not significantly advance the state of the art. Moreover, the lack of evaluation against real-world psychophysics data limits the practical impact of the results.
4. Premature Conclusions: The paper's claim that REINFORCE is a plausible model of animal learning is intriguing but feels premature. The comparison with animal data is limited and lacks rigorous statistical validation.
Recommendation:
While the paper has potential, it feels underdeveloped in its current form. The authors should address the unclear assumptions, improve the clarity of the presentation, and provide stronger empirical validation, particularly with real-world data. The work is a promising step toward understanding threshold learning in DDMs, but it requires further refinement to make a significant contribution to the field. 
Pro: Interesting problem, sound methodology, and potential biological relevance.  
Con: Insufficient justification of assumptions, unclear presentation, limited novelty, and weak real-world validation.  
Decision: Borderline reject.