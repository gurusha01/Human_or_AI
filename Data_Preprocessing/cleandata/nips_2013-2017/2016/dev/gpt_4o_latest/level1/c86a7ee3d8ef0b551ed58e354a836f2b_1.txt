This paper introduces two novel algorithms, SGD-BB and SVRG-BB, which leverage the Barzilai-Borwein (BB) method to adaptively compute step sizes for stochastic gradient descent (SGD) and stochastic variance reduced gradient (SVRG) methods. The authors address a critical challenge in stochastic optimization: selecting appropriate step sizes without manual tuning or reliance on diminishing schedules. By incorporating the BB method, the proposed algorithms dynamically adjust step sizes during runtime, achieving competitive or superior performance compared to manually tuned alternatives. The authors provide theoretical guarantees, including linear convergence for SVRG-BB on strongly convex objectives, and offer numerical experiments demonstrating the efficacy of their methods on logistic regression and SVM tasks.
Strengths:
1. Novelty and Originality: The paper proposes a novel application of the BB method to stochastic optimization, which is a significant departure from traditional step size strategies. The automatic computation of step sizes is a practical and impactful contribution.
2. Theoretical Contributions: The authors rigorously prove the linear convergence of SVRG-BB and, as a by-product, establish the previously missing convergence proof for SVRG-I. These results advance the theoretical understanding of variance-reduced methods.
3. Practical Relevance: The proposed methods eliminate the need for manual step size tuning, a time-consuming and error-prone process in practice. The experiments demonstrate that SGD-BB and SVRG-BB achieve comparable or better performance than state-of-the-art methods with best-tuned step sizes.
4. Comprehensive Evaluation: The authors test their methods on standard datasets and compare them against well-established baselines. The results are robust, showing that the BB-based methods adaptively converge to optimal step sizes.
5. Clarity: The paper is well-organized, with detailed explanations of the algorithms, theoretical analysis, and experimental results. The inclusion of smoothing techniques for SGD-BB is a thoughtful addition to enhance stability.
Weaknesses:
1. Limited Scope of Experiments: While the experiments are convincing, they are restricted to a few datasets and tasks (logistic regression and SVM). Testing on a broader range of machine learning problems, such as deep learning or non-convex optimization, would strengthen the paper's claims.
2. Sensitivity Analysis: Although the authors claim that the methods are not sensitive to initial step sizes, a more detailed sensitivity analysis across diverse settings would provide additional confidence.
3. Computational Overhead: The BB step size computation introduces additional overhead compared to fixed step size methods. While the authors claim the cost is negligible, a quantitative analysis of runtime overhead would be beneficial.
4. Limited Discussion of Related Work: The paper could better contextualize its contributions by discussing recent advancements in adaptive step size methods, such as Adam or AdaGrad, and contrasting them with the BB approach.
Recommendation:
Pros for Acceptance:
- Novel and practical contribution to step size selection in stochastic optimization.
- Strong theoretical guarantees and rigorous analysis.
- Promising empirical results that demonstrate the effectiveness of the proposed methods.
Cons for Acceptance:
- Limited experimental scope and lack of runtime analysis.
- Insufficient exploration of connections to other adaptive methods.
Overall, this paper makes a meaningful contribution to the field of stochastic optimization and is well-suited for presentation at NIPS. I recommend acceptance, with the suggestion to expand the experimental scope and provide runtime comparisons in future work.