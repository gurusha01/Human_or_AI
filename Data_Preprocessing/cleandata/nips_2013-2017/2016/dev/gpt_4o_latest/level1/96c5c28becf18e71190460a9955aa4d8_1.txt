This paper investigates how animals learn decision thresholds in perceptual decision-making tasks modeled by the drift-diffusion model (DDM). The authors propose two methods for optimizing decision thresholds: a reinforcement learning approach based on Williams' REINFORCE algorithm and a Bayesian optimization method using Gaussian processes. Both methods aim to maximize single-trial rewards derived from Wald's cost function, a challenging optimization problem due to the stochastic nature of rewards. The authors validate their methods against exhaustive optimization and compare their performance in terms of convergence speed, computational cost, and variance. They also explore the biological plausibility of REINFORCE as a model for animal learning, supported by comparisons with experimental data from rodents and bats.
The paper builds on foundational work in decision-making (e.g., Ratcliff's DDM and Wald's SPRT) and connects reinforcement learning with optimal decision-making frameworks. The REINFORCE method is grounded in policy gradient techniques, while Bayesian optimization leverages probabilistic modeling to guide threshold selection. The authors' contribution is novel in applying these methods to the specific problem of threshold learning in DDMs and in demonstrating their efficacy through extensive simulations.
Strengths:
1. Novelty and Relevance: The paper addresses an open question in neuroscience—how decision thresholds are learned—by combining reinforcement learning and Bayesian optimization, bridging machine learning and biological decision-making.
2. Thorough Evaluation: The authors rigorously compare the two methods across multiple metrics (e.g., convergence speed, computational cost, variance) and validate their results against exhaustive optimization.
3. Biological Plausibility: The REINFORCE method aligns well with experimental data on animal learning and resembles cortico-striatal plasticity models, enhancing its relevance to neuroscience.
4. Clarity of Presentation: The paper is well-organized, with clear explanations of the methods, results, and their implications. Figures effectively illustrate key findings.
Weaknesses:
1. Computational Cost of Bayesian Optimization: While Bayesian optimization converges faster, its high computational cost and variance limit its practical applicability, especially for large-scale problems.
2. Limited Biological Validation: Although the REINFORCE method aligns with animal learning data, the comparison is qualitative. Quantitative validation with experimental datasets would strengthen the claims.
3. Simplified Assumptions: The study assumes static decision thresholds and equal priors, which may not generalize to more complex or dynamic decision-making scenarios.
4. Scalability: The paper briefly mentions extending the methods to multi-alternative decisions but does not provide concrete results or analysis for such scenarios.
Arguments for Acceptance:
- The paper addresses a significant and underexplored problem with innovative methods.
- It provides a solid theoretical foundation and extensive empirical validation.
- The REINFORCE method has strong implications for understanding biological learning mechanisms.
Arguments Against Acceptance:
- The computational inefficiency of Bayesian optimization limits its practical utility.
- The biological relevance of the REINFORCE method requires further quantitative validation.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a meaningful contribution to both machine learning and neuroscience, but addressing the computational limitations of Bayesian optimization and providing more robust biological validation would enhance its impact.