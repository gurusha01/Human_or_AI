Review
This paper addresses the problem of selective inference in the context of group-sparse linear regression, providing tools to construct confidence intervals and p-values for selected groups of variables. The authors' main technical contribution is a novel result, termed the "truncated projection lemma," which characterizes the conditional distribution of the magnitude of a projection onto a subspace, enabling inference for a broad class of group-sparse selection methods. These include the group lasso, iterative hard thresholding (IHT), and forward stepwise regression. The paper demonstrates the utility of these tools with experiments on simulated data and real-world health data from California counties.
The work builds on prior results in selective inference, particularly the polyhedral lemma of Lee et al. (2016) and the group-sparse inference methods of Loftus and Taylor (2015). While previous work focused on hypothesis testing for selected groups, this paper extends these results by providing confidence intervals for effect sizes, which is a significant advancement. The authors also generalize the framework to handle a broader set of group-sparse selection methods, which is a notable contribution to the field.
Strengths
1. Technical Novelty: The truncated projection lemma is a significant theoretical contribution, extending the selective inference literature to group-sparse settings in a rigorous and generalizable way.
2. Broad Applicability: The proposed methods are applicable to multiple group-sparse selection algorithms, including forward stepwise regression, IHT, and the group lasso. This versatility enhances the paper's impact.
3. Experimental Validation: The paper provides thorough experimental results on both simulated and real data, demonstrating the practical utility of the proposed methods. The empirical coverage of confidence intervals is well-analyzed.
4. Clarity of Presentation: The paper is well-organized, with clear explanations of the technical results and their implications. The use of examples and pseudo-code for algorithms aids in understanding.
Weaknesses
1. Computational Complexity: The group lasso inference relies on numerical approximations, which may be computationally expensive, especially for large datasets. The authors could have provided more discussion on the scalability of their methods.
2. Limited Real-World Applications: While the California health data example is interesting, additional real-world applications would strengthen the paper's significance and demonstrate its broader impact.
3. Assumptions on Known Variance: The methods assume that the noise variance σ² is known, which may not hold in many practical scenarios. Addressing this limitation or discussing extensions to unknown variance settings would improve the paper.
4. Comparison with Alternative Methods: The paper does not provide a direct comparison of its confidence intervals and p-values with alternative approaches (e.g., bootstrap or Bayesian methods). Such comparisons would help contextualize the advantages of the proposed methods.
Arguments for Acceptance
- The paper makes a clear and significant theoretical contribution to selective inference in group-sparse regression.
- The methods are broadly applicable and extend existing work in meaningful ways.
- The experimental results are thorough and validate the theoretical claims.
Arguments Against Acceptance
- The computational feasibility of the methods, particularly for the group lasso, is not fully addressed.
- The paper could benefit from additional real-world applications and comparisons with alternative inference methods.
Recommendation
Overall, this paper represents a strong contribution to the field of selective inference and is well-suited for presentation at NeurIPS. While there are some limitations, particularly regarding computational scalability and real-world applications, these do not detract significantly from the paper's quality and impact. I recommend acceptance, with minor revisions to address the noted weaknesses.