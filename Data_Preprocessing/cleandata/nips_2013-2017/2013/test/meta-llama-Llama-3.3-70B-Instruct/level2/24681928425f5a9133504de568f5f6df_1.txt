This paper presents a unified formalization of stochastic And-Or grammars and an unsupervised approach to learning their structures and parameters. The authors propose a novel algorithm that iteratively induces compositions and reconfigurations in a unified manner, optimizing the posterior probability of the grammar. The approach is evaluated on event grammars and image grammars, achieving comparable or better performance than previous approaches.
The paper's main claims are: (1) a unified formalization of stochastic And-Or grammars that is agnostic to the type of data being modeled, (2) an unsupervised approach to learning the structures and parameters of such grammars, and (3) the approach's effectiveness in learning event grammars and image grammars.
The support for these claims is provided through a combination of theoretical analysis and empirical evaluation. The authors derive a set of sufficient statistics from the training data, which enables efficient computation of the posterior gain of adding an And-Or fragment to the grammar. The empirical evaluation demonstrates the approach's performance on two datasets, showing competitive or superior results compared to previous approaches.
The paper's strengths include its novelty, technical soundness, and clarity of presentation. The authors provide a clear and concise introduction to stochastic And-Or grammars, and their approach is well-motivated and theoretically grounded. The empirical evaluation is thorough and well-designed, providing convincing evidence for the approach's effectiveness.
However, there are some limitations and potential areas for improvement. The paper assumes a context-free subclass of stochastic And-Or grammars, which may not be sufficient for modeling more complex data. Additionally, the approach relies on a sparsity prior to penalize large grammars, which may not always be effective in practice. The authors could also provide more discussion on the computational complexity of their approach and its scalability to larger datasets.
Overall, the paper presents a significant contribution to the field of stochastic grammars and unsupervised learning. The approach has the potential to be applied to a wide range of data types, and the authors' evaluation demonstrates its effectiveness in learning event grammars and image grammars.
Arguments pro acceptance:
* The paper presents a novel and well-motivated approach to learning stochastic And-Or grammars.
* The approach is theoretically grounded and provides a clear and concise introduction to the topic.
* The empirical evaluation is thorough and well-designed, demonstrating the approach's effectiveness.
Arguments con acceptance:
* The paper assumes a context-free subclass of stochastic And-Or grammars, which may not be sufficient for modeling more complex data.
* The approach relies on a sparsity prior, which may not always be effective in practice.
* The authors could provide more discussion on the computational complexity of their approach and its scalability to larger datasets.