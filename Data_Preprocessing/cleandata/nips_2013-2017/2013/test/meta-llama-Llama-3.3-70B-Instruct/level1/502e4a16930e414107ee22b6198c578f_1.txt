This paper presents a comprehensive analysis of the number of iterations required by Policy Iteration (PI) algorithms to converge to the optimal policy in a Markov Decision Process (MDP). The authors consider two variations of PI: Howard's PI and Simplex-PI, and provide new upper bounds on the number of iterations required by these algorithms.
The paper starts by introducing the basic concepts of MDPs and PI algorithms, and then presents a review of existing results on the number of iterations required by Howard's PI and Simplex-PI. The authors then present their own results, which improve upon existing bounds in several cases.
One of the main contributions of the paper is the presentation of a contraction property for both Howard's PI and Simplex-PI, which allows the authors to derive new bounds on the number of iterations required by these algorithms. The authors also present a detailed analysis of the case where the discount factor is less than 1, and show that both algorithms are weakly polynomial in this case.
The paper also presents a new result for Simplex-PI, which shows that this algorithm is strongly polynomial for a certain class of MDPs. This result is based on a structural assumption on the MDP, which is similar to an assumption made in a previous paper.
The authors also discuss the possibility of extending their results to Howard's PI, but note that this is a more challenging problem. They present some partial results in this direction, but leave the general problem open.
Overall, the paper presents a significant contribution to the understanding of PI algorithms and their convergence properties. The results presented in the paper are likely to be of interest to researchers in the field of reinforcement learning and MDPs.
Strengths:
* The paper presents a comprehensive analysis of the number of iterations required by PI algorithms, and improves upon existing bounds in several cases.
* The authors present a clear and detailed exposition of their results, and provide a thorough review of existing work in the area.
* The paper presents a new result for Simplex-PI, which shows that this algorithm is strongly polynomial for a certain class of MDPs.
Weaknesses:
* The paper assumes a significant amount of background knowledge in MDPs and PI algorithms, which may make it difficult for non-experts to follow.
* Some of the proofs presented in the paper are quite technical, and may require careful reading to understand.
* The paper leaves open the general problem of extending the results to Howard's PI, which may be seen as a limitation.
Arguments for acceptance:
* The paper presents a significant contribution to the understanding of PI algorithms and their convergence properties.
* The results presented in the paper are likely to be of interest to researchers in the field of reinforcement learning and MDPs.
* The paper is well-written and provides a clear and detailed exposition of the results.
Arguments against acceptance:
* The paper assumes a significant amount of background knowledge in MDPs and PI algorithms, which may limit its appeal to a broader audience.
* Some of the proofs presented in the paper are quite technical, and may require careful reading to understand.
* The paper leaves open the general problem of extending the results to Howard's PI, which may be seen as a limitation.