This paper presents a novel approach to efficient exploration in reinforcement learning, called posterior sampling for reinforcement learning (PSRL). The algorithm is conceptually simple, computationally efficient, and allows an agent to encode prior knowledge in a natural way. The authors establish an Õ(τS √ AT ) bound on expected regret, which is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm.
The paper relates to previous work on reinforcement learning, particularly on optimistic algorithms, which introduce optimism about poorly-understood states and actions to encourage exploration. The authors argue that PSRL offers several advantages over optimistic algorithms, including computational efficiency, the ability to incorporate prior knowledge, and a more straightforward analysis.
The paper is well-written, and the authors provide a clear and concise explanation of the algorithm and its analysis. The theoretical results are supported by simulation experiments, which demonstrate that PSRL outperforms existing algorithms with similar regret bounds.
Here are the arguments pro and con acceptance:
Pro:
1. Novel approach: PSRL offers a new perspective on efficient exploration in reinforcement learning, which is different from the traditional optimistic approach.
2. Theoretical guarantees: The authors provide strong theoretical guarantees for PSRL, including an Õ(τS √ AT ) bound on expected regret.
3. Computational efficiency: PSRL is computationally efficient, which makes it suitable for large-scale reinforcement learning problems.
4. Prior knowledge: PSRL allows an agent to encode prior knowledge in a natural way, which can be beneficial in many practical applications.
Con:
1. Limited analysis: The analysis of PSRL is limited to the Bayesian setting, and it is not clear how the algorithm performs in the frequentist setting.
2. Comparison to existing algorithms: While the authors compare PSRL to UCRL2, it is not clear how PSRL performs compared to other state-of-the-art algorithms.
3. Simulation experiments: The simulation experiments are limited to a few environments, and it is not clear how PSRL performs in more complex environments.
Overall, I believe that this paper makes a significant contribution to the field of reinforcement learning, and I recommend acceptance. The paper presents a novel approach to efficient exploration, provides strong theoretical guarantees, and demonstrates the effectiveness of the algorithm in simulation experiments. While there are some limitations to the analysis and simulation experiments, I believe that these can be addressed in future work. 
Quality: 9/10
The paper is well-written, and the authors provide a clear and concise explanation of the algorithm and its analysis. The theoretical results are strong, and the simulation experiments demonstrate the effectiveness of the algorithm.
Clarity: 9/10
The paper is well-organized, and the authors provide a clear and concise explanation of the algorithm and its analysis.
Originality: 8/10
While PSRL is a novel approach to efficient exploration, the idea of posterior sampling is not new, and the authors build on existing work in this area.
Significance: 9/10
The paper makes a significant contribution to the field of reinforcement learning, and the results have the potential to impact the development of more efficient reinforcement learning algorithms.