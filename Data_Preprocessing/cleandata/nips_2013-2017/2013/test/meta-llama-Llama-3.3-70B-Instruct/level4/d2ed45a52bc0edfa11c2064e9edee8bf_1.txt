This paper presents a novel algorithm for visual concept learning, which enables the recognition of images belonging to a specific concept based on a limited number of example images. The algorithm integrates object classification techniques from machine vision with Bayesian generalization methods from cognitive science, demonstrating improved generalization performance compared to baseline methods. The authors also introduce a large-scale dataset for visual concept learning, leveraging ImageNet images and human annotations.
The problem of visual concept learning is intriguing and highly relevant to the NIPS community. The paper is of high quality, well-written, and engaging to read.
However, there are several points that require the authors' attention:
- In Section 3.2, the authors assert that their criteria (eq. 1) for selecting levels of nested concepts result in sub-category, basic, super-basic, and super-category levels in the taxonomy. I would like to see empirical evidence to support this claim.
Additionally, I suggest citing Rosch et al. (1976) when discussing basic-level categories, as connecting the ideas from Rosch's work to this research would be fascinating.
- There appears to be a potential error in the equations in Section 4.1. Should equation 3 be P(h|X) instead of P(X|h)? I am unclear about the reasoning behind using |h|^-N instead of |h|^-1 in equation 4 (perhaps it should be P(X|h) rather than P(xi|h)). Furthermore, the expression p(xnew|h) on line 226 seems inconsistent with equation 4, which is perplexing (it might be more accurate to use p(x_newâˆˆC|h)).
- The description of the extension of [5] (HB) is unclear. Shouldn't the subtree maintain accuracy over the 5 example images rather than the query images (line 345)?
Overall, this is a well-written paper on an interesting topic, addressing concept learning using tools from both machine vision and cognitive science. I believe it deserves acceptance.