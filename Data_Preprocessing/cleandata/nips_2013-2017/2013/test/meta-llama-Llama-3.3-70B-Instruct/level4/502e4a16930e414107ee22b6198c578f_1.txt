Summary: 
This paper examines the complexity of policy iteration (PI) in finite Markov Decision Processes (MDPs) with n states and m actions per state, presenting several key findings. 
Notably, the paper improves upon existing bounds for Howard's PI by removing a log(n) factor and for Simplex PI by reducing its multiplicative constant by half. 
Furthermore, for deterministic MDPs, it enhances a previous bound for Simplex PI by decreasing the power of dependence on n by one, with this result being generalized to stochastic MDPs under specific constraints.
Originality and significance: 
The study of PI complexity is a crucial area of interest within the reinforcement learning (RL) community. Although the finite setting may not be directly applicable in practice, understanding this fundamental case is essential before exploring more complex scenarios. This research builds upon earlier work initiated by Yinyu Ye in 2011 and subsequently developed by Post and Ye, as well as Hansen, Miltersen, and Zwick. While some improvements are incremental, and the core proof ideas originate from Ye's 2011 work, the paper's contributions lie in its streamlined proofs, which are more accessible to RL researchers than Ye's linear programming-based approach. The extension of strong polynomial results to stochastic MDPs under structural assumptions is a significant novel aspect of the paper.
Clarity: 
The paper is generally well-written, although it requires careful review to address minor errors, such as typos (noted on lines 285 and 292), and to improve fluency in certain sections (e.g., the phrase "However, on the other hand" on line 303 could be rephrased for better flow).
Soundness: 
Based on the review, the results appear to be sound, although a detailed verification of all proof elements was not conducted due to time constraints.
Minor issues: 
- On line 367, it would be beneficial to specify the lemma used. 
The paper mildly improves previous results on the time complexity of PI while making the proofs more accessible to RL researchers. More importantly, it demonstrates that the class of MDPs for which Simplex PI is strongly polynomial is significantly larger than previously known.