The paper presents a novel approach to learning discriminative image patches that are predictive of the image class. The method begins by considering all possible patches in an image collection and identifies the discriminative patches as the centers of patch clusters, obtained through a discriminative version of the mean-shift algorithm. This algorithm incorporates discriminativeness by dividing the kernel density estimate of positive patches by that of negative patches.
The strengths of the paper include:
- The importance of the problem of learning discriminative patches or parts.
- The impressive classification performance of the proposed discriminative patches, particularly on the MIT Scene 67 dataset.
- The presence of several interesting insights in the formulation.
However, there are also several weaknesses:
- The authors start with the mean-shift algorithm and modify it extensively, to the point where the mean-shift interpretation may no longer be useful.
- Various aspects of the formulation and derivation are based on heuristics, some of which are changed on a dataset-by-dataset basis.
- The learning algorithm lacks formal guarantees of correctness or convergence.
- There are minor errors in the formal derivation.
Detailed comments:
The empirical results are strong enough to warrant consideration for publication, but the formulation and technical derivation require significant improvement. Specifically:
l.125: 
The notation should be clarified, as the expression max(d(xi,w) - b, 0) assumes a triangular kernel with d() being the negative of the Euclidean distance and a negative bandwidth b, which is counter-intuitive. A more natural expression would be max(b - d(xi,w), 0).
Eq. (1): 
The term arglocalmax is never defined.
Eq. (2): 
This equation indicates that the algorithm is not maximizing a ratio of density values, but rather an energy function with an adaptive bandwidth. The density is given by E(w) = sumi max(d(xi^+) - b(w), 0), where the bandwidth b(w) is selected as a function of the current point w. Several aspects of this formulation require clarification, including:
(a) Normalization of the triangular kernel to obtain a proper density estimator.
(b) Comparison to standard adaptive mean shift, where the bandwidth is chosen as a function of x_i rather than w.
(c) Handling cases where multiple values of b(w) satisfy the constraint in (2).
Eq. (3): 
The squared Euclidean distance d^2() reduces to the inner product, not the Euclidean distance d(). Restricting the domain to the unit sphere may require modifying the densities to have this manifold as the domain.
Eq. (5): 
This equation marks a substantial departure from the mean-shift algorithm. The proposed method applies hill climbing to determine a clustering of the data, whereas mean shift applies hill climbing to each w independently. Practical differences include:
- Mean shift is non-parametric, whereas the proposed method starts with a fixed number of cluster centers.
- The introduction of soft associations between data points and clusters (alpha_ij) is more similar to K-means than mean shift.
Optimization method: 
The method is heuristic and lacks formal guarantees of correctness or convergence.
Experiments: 
The baselines [5,8] have mechanisms to avoid or remove redundant patches, which are not incorporated in the baseline. The experiments on Scene-67 may be influenced by tuning of the representation, and re-running the experiment with the baseline discriminative patches may be necessary.
Other minor comments: 
Several minor technical problems in the derivation of the algorithm were noted, including unclear leaps and minor formal errors. The fact that several aspects of the method are heuristics, and such heuristics are tuned on a dataset basis, was not addressed. Despite these issues, the solid experimental results support accepting the paper, but the existence of these formal problems should be taken into account when making a decision.