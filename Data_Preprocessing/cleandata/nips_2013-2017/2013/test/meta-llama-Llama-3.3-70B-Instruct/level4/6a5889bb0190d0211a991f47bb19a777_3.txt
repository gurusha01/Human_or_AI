This paper presents the initial regret bounds for a reinforcement learning algorithm characterized by three key components: maintaining a posterior over the MDP, sampling one MDP from the prior, and following the optimal policy for the sampled MDP. Referred to as "Posterior-Sampling Reinforcement Learning" (PSRL), this algorithm has been previously proposed but lacked regret bounds. Notably, all existing algorithms with established regret bounds operate on the principle of optimism in the face of uncertainty, making this the first regret bound analysis for an algorithm that deviates from this principle. Although the regret bound for this algorithm is not as tight as those based on optimism, it exhibits substantial exploitability in typical cases and appears to offer greater overall efficiency in practice.
If this paper indeed presents the first regret result for an algorithm not grounded in optimism and the result is accurate, it warrants acceptance. The paper is commendable for its clarity, although a detailed understanding of the proof eluded me, potentially overlooking errors.
The work adopts an episodic formulation with fixed-length episodes for simplicity, an assumption that, while unrealistic, is acceptable for a pioneering result. Undesirable practical aspects of the algorithm, including this assumption, could potentially be addressed in future work if the primary result holds. The paper touches on a related concept in a later section, which, although valuable, could be omitted to make room for discussions on comparing the state of the art.
The claim that the algorithm's regret bound is near the state of the art should be substantiated by explicitly stating what constitutes the current state of the art. This clarification is essential for assessing the proximity of the new algorithm's bound to the existing benchmarks and for evaluating the significance of the new bound. The paper should elaborate on the criteria used to determine the closeness of the new algorithm's bound to the state of the art, providing a clearer context for its contribution.
Overall, the paper offers the first regret bound and promising empirical results for a reinforcement learning algorithm based on posterior sampling, distinguishing it from the traditional approach of optimism in the face of uncertainty.