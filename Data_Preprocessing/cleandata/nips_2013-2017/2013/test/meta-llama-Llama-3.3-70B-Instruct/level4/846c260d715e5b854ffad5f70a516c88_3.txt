The authors propose a Monte-Carlo tree search algorithm for online planning in Markov decision processes (MDPs) with unknown transition probabilities, utilizing Thompson sampling for action selection to expand the search tree. Thompson sampling, a randomized Bayesian algorithm, is employed to minimize regret in multi-armed stochastic bandit problems, with actions chosen according to the posterior probability of being the best action. The probability of returns is modeled as a normal distribution for X{s,\pi(s)} and a mixture of normal distributions for X{s,a}, facilitating the application of Thompson sampling.
The idea of leveraging Thompson sampling in Monte-Carlo tree search is commendable, given recent studies demonstrating its empirical superiority over alternative methods like UCB. However, a key challenge in using Thompson sampling in MDPs lies in modeling the probability of returns in a Bayesian setting, which the authors address. Several questions and concerns arise:
(1) While presenting the distribution of X_{s,\pi(s)} as a normal distribution may be justified for leaf nodes in the search tree, where \pi denotes the roll-out policy, this assumption may not hold for upper-level nodes due to policy changes.
(2) The claim that Thompson sampling converges to the optimal action with probability 1 by treating each decision node as a multi-armed bandit (MAB) problem is questionable. The MAB problems in non-leaf nodes appear to be non-stationary due to policy changes as returns X_{s,\pi(s)} change, and there is no known convergence guarantee for Thompson sampling in such scenarios. This raises doubts about the convergence of the policy found by DNG-MCTS to the optimal one.
(3) Clarification is needed regarding the term "iteration" in the experiments section, specifically whether it refers to the iteration of DNG-MCTS in the "OnlinePlanning" function.
(4) Reporting the CPU running time would provide valuable insight into the computational efficiency of DNG-MCTS, particularly in comparison to UCT.
Minor comments include errors in references, with some papers listed as arXiv publications when they have actually been published in conferences.
In terms of quality, the paper has several flaws. While clarity is maintained through well-structured writing, originality is present, and significance is notable due to the interest of the RL community, the authors must address the mentioned flaws. 
Additionally, the application of the central limit theorem (CLT) for Markov chains in Section 3.1 is problematic. The original CLT states that the empirical mean of f(st) converges to the true expectation, but the modified version presented by the authors seems to imply that the sum of f(st) follows a normal distribution, which is questionable as the sum diverges as n approaches infinity. This casts doubt on the justification of the normal assumption for the sum of rewards, despite its practical reasonableness.
Overall, the authors provide a novel MCTS algorithm using Thompson sampling, which is of interest to the RL community, but the soundness of the approach requires further clarification and justification.