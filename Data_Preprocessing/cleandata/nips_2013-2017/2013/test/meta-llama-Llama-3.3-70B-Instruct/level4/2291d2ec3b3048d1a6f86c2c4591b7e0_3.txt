This paper presents a convex framework for partitioning dataset(s) into K subsets, referred to as latent domains, such that each subset exhibits similar semantic class distributions and is as distinct from others as possible. The distinctiveness, termed maximum distinctiveness, is achieved by maximizing the pairwise mean differences of subsets in the Reproducing Kernel Hilbert Space (RKHS) induced by a chosen kernel. The identification of K, the number of latent domains, is determined by maximizing the average learnability within each latent domain, which measures how well a classifier can be learned.
However, the assumption that the class distributions within each domain should be as similar as possible may be limiting for certain tasks, as it may not always hold true. For instance, in an animal classification task, the latent domains (e.g., poses) may not have similar distributions within each class. Specifically, while both horses and cows may be depicted in left-right standing poses, horses are rarely shown in sitting poses, whereas cows are.
Although the concept of latent domain discovery is not novel, the idea of controlling class label distributions to improve latent domain identification within a relaxed convex framework is a new contribution. The approach of determining the number of latent domains by evaluating the quality of classification within each domain is also noteworthy, as it ensures that each latent domain has a sufficient number of samples for each class, enabling better generalization and learning of class discrimination.
The paper could benefit from clearer definitions and distinctions between the concepts of dataset, domain, and latent domain, accompanied by supporting examples. Specifically, section 4.2 requires clarification, as the terms "dataset" and "domain" are used interchangeably, referring to Si as both datasets and source domains. Furthermore, the notation maxk r(U_k,B) is used in equation 7 without being defined.
The experimental validation appears to be sufficient, with reasonable improvements over the baselines. However, the selection of source and target datasets for reporting results lacks clear justification, and alternative evaluation methods, such as leave-one-dataset-out adaptation, may be more suitable. The qualitative results provide a helpful visual representation of the algorithm's achievements.
On line 074, the problem of learning latent domains is stated as an unsupervised learning problem, which may be misleading since the methods employ semantic class labels. The discovery of latent domains via encouraging similar class distributions in each latent domain, formulated in a relaxed convex framework, is a notable technical contribution. Nevertheless, some concepts in the paper and the experimental validation require clarification to enhance the overall quality of the work.