This paper presents a regret analysis for posterior sampling in episodic reinforcement learning, specifically for the algorithm known as Thompson sampling or PSRL. The approach involves sampling an MDP model from the posterior at the start of each episode, following the optimal policy for that episode, updating the model posterior, and repeating the process. Despite its empirical success, PSRL has lacked strong theoretical guarantees until now. The authors provide the first regret analysis, yielding a bound of O(τ  S  √(A * T)), where H is the episode length and T is the total number of steps.
The paper makes a significant theoretical contribution to reinforcement learning in finite-state, finite-action, episodic MDPs, building on recent advances in multi-armed bandit literature to understand the finite-sample performance of Bayesian-style RL algorithms. The writing is clear, explaining complex ideas intuitively despite the heavy technical details.
While the PSRL algorithm itself is not new, the novelty of the paper lies in its regret analysis, particularly Lemma 2, which equates expected regret with the regret in the sampled MDP. This observation, similar to one used in multi-armed bandits, appears sufficiently novel in the context of reinforcement learning. The significance of this work stems from extending analytic ideas from multi-armed bandits to MDPs, showing strong regret bounds for posterior sampling, which may generate further interest in the RL community.
However, it is crucial to note that the nature of the bounds presented is fundamentally different from existing literature, being average-case rather than worst-case bounds. This distinction, enabled by averaging over the prior/posterior, makes Lemma 2 possible but may result in weaker expected bounds compared to worst-case bounds. Clarification on this point would strengthen the paper.
The comparison of PSRL with UCRL2 raises questions, as UCRL2 is designed for continuing tasks aiming to optimize average reward, whereas PSRL targets episodic tasks with undiscounted, finite-horizon total reward. Their regret bounds, therefore, are not directly comparable. A more relevant comparison might be with work like Claude-Nicolas Fiechter's "Efficient Reinforcement Learning," which considers finite-horizon problems within the PAC framework.
The empirical superiority of PSRL over UCRL2 in simple experiments is not surprising, given UCRL2's conservative nature and the difference in problem types they address. Minor corrections, such as specifying "high probability" in Line 278 and addressing the assumption in Line 411 about the optimal average reward's correlation with episodic length, would improve the paper.
Overall, the paper offers an interesting analysis of an important algorithm, albeit with regret bounds not directly comparable to previous results. Its theoretical contribution, clarity, and potential to inspire further research in posterior sampling-based algorithms for reinforcement learning are noteworthy, despite the limited and simple nature of the experiments presented.