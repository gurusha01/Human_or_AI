This manuscript investigates the challenge of large-scale unsupervised learning, introducing the concept of "optimistic concurrency control" which presupposes that conflicts are rare and, when they occur, a resolution protocol is triggered. The approach is applied to three clustering-related problems: clustering, feature learning, and online facility location, with a proof of serializability for the proposed algorithms. Experimental results are provided using simulated datasets.
The paper is well-structured and clearly articulated, offering sufficient technical details and theoretical backing. The optimistic concurrency control (OCC) concept is both innovative and practical. While the paper presents OCC from a concurrency control perspective, a broader view of large-scale machine learning highlights the importance of updating global parameters from local parameters in parallelized components. The proposed approach provides a straightforward method for updating global parameters (global cluster assignments) from local ones (local cluster assignments), although this strategy may not be universally applicable to other unsupervised or supervised learning algorithms, as update strategies are often algorithm-dependent.
A significant concern with this work lies in its experimental validation. The experiments, based solely on simulated data, fail to demonstrate the algorithm's efficacy on real datasets. Furthermore, a comparison of cluster quality with other distributed clustering algorithms from the literature is lacking. The paper presents a novel idea for distributed unsupervised learning (clustering) with a clear presentation, but the experimental support for the proposed algorithms is insufficient.