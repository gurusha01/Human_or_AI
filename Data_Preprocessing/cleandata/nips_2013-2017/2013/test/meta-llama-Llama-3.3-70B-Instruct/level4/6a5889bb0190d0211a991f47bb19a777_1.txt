Title: (More) Efficient Reinforcement Learning via Posterior Sampling 
Summary: This paper presents a Thompson sampling-inspired approach, termed posterior sampling for reinforcement learning, designed for episodic finite-horizon reinforcement learning. It provides a regret analysis demonstrating state-of-the-art performance, alongside simulations that exhibit superior empirical performance on certain toy problems compared to other algorithms with similar regret bounds.
Comments: 
The paper is exceptionally clear and well-written, with a contribution that is succinctly identified: it demonstrates the feasibility and efficiency of applying a Thompson sampling-type approach in a reinforcement learning context, backed by theoretical analysis. 
Regarding the regret bound, the authors mention that the regret bound of PSRL is close to the current state of the art. For clarity, it would be beneficial to include the specific state-of-the-art values in this paper for direct comparison. 
A common inquiry in the realm of multi-armed bandit algorithms pertains to the practical magnitude of the constant in the big O notation; elucidating this aspect would provide further insight. 
Comparative analyses with the KL-UCRL algorithm (as outlined in "Optimism in reinforcement learning and Kullback-Leibler divergence" by S. Filippi, O. Capp√©, A. Garivier) could offer additional context. KL-UCRL, which operates on the optimism principle, has been shown to outperform UCRL2 in the riverSwim environment, with a regret bound of $\tilde O(|S|\sqrt{|A&T})$, similar to the bounds achieved in this work.
Minor comments include: 
- On line 99, replacing $s1$ with $si$ appears necessary for consistency;
- Line 192 contains a minor typographical error, where "we how we can" should be corrected to "how we can";
- The term "markov" on line 434 should be capitalized as "Markov" for correctness;
- Uniformity in reference formatting (e.g., Firstname Name, F. Name) would enhance readability. 
Overall, this paper is technically robust, well-written, and engaging, presenting a significant contribution to the field.