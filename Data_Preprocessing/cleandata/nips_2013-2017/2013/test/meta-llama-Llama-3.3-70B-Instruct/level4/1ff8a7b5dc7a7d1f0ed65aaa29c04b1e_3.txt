I appreciate the authors' observation regarding the potential advantage of older data having undergone more layers of processing compared to newer data. The proposed approach of utilizing a deep RNN to improve performance on shorter-term tasks, in addition to other potential benefits, seems reasonable. Another potential strategy could be to incorporate additional layers of nonlinearities between each time step; has this been explored?
The importance of gradient normalization and its role in avoiding the "problem of bifurcations" is not entirely clear to me. A relevant ICML 2013 paper by researchers from the University of Montreal investigated gradient truncation and optimization in RNNs, which may be pertinent to this discussion. Furthermore, I wonder whether normalizing the gradient in this manner could potentially disrupt the "averaging" behavior of SGD.
In the context of Wikipedia compression tasks, there is also relevant work by Mikolov et al. that may be worth comparing to the present study.
Among the experiments designed to examine the roles of each layer in terms of time-scale and abstraction, I find the text generation experiment (Table 2) to be the most convincing. However, as the authors themselves note, interpreting the effects of "brain surgery" on RNNs can be problematic due to the complex interdependencies between layer outputs.
A significant empirical question remains unanswered in this paper: do the higher layers truly excel at processing more abstract and long-term properties of the text, or would relocating these units to the first layer yield similar results? The fact that higher layers assume these roles after training is encouraging but incomplete evidence.
I notice that the regular RNN has fewer parameters in these experiments, which may render the comparison unfair. A more compelling demonstration would be to show that deeper RNNs outperform standard RNNs with an equivalent number of parameters or units. For instance, comparing a 2-layer DRNN to a standard RNN with the same number of units would strengthen the paper's claims.
Instead of Figure 2, it would be more informative to see how DRNNs of varying depths perform on benchmarks when trained from scratch, potentially with wider layers to compensate for differences in parameter and unit counts.
Minor suggestions:
- The terms DRNN-AO and IO should be defined in the text, not just in Figure 1.
This paper explores a hybrid architecture combining deep and recurrent neural networks, called DRNNs, which integrate recurrent connections at each layer operating through time. The authors demonstrate the effectiveness of this architecture for text prediction and compression compared to existing approaches. A significant portion of the paper is devoted to experiments arguing that higher-level layers process more abstract and long-term structures in the data. While these experiments are convincing, I have some reservations and would like to see additional experiments.
I believe it is worthwhile to investigate these deep temporal networks and gain insight into their functioning after training. The paper is well-written, easy to read, and commendably honest about its potential limitations, which I appreciate.