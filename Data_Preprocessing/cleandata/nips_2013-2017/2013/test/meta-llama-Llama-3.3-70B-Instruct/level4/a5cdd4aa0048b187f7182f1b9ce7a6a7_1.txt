The paper presents a significant development in multiclass classification by introducing calibrated convex surrogate losses. Notably, when the n by k loss matrix, where n represents the size of the label set and k represents the size of the prediction set, has a rank of d, the minimization of this surrogate loss is equivalent to solving a d-dimensional least squares problem. Furthermore, converting the solution to a prediction involves a d-dimensional linear maximization over a set of size k. The authors provide two illustrative examples, specifically precision at q and expected rank utility, where n and k are exponentially large, yet predictions can be efficiently calculated. Additionally, they discuss two more examples, mean average precision and pairwise disagreement, characterized by exponential n and k, where calibrated predictions appear to be computationally infeasible, but efficiently computable predictions are calibrated for restricted probability distribution sets.
This contribution is exciting and seemingly crucial for a broad range of multiclass losses with significant practical implications. Technically, it builds upon two key results from reference [16], with the important low-rank observation having been previously noted in [16]. The paper is well-structured and clearly written, facilitating an easy understanding of the complex concepts presented.
However, the primary critique lies in the results concerning calibration with respect to restricted sets of probability distributions. For instance, Theorem 4 states that the efficiently computable prediction rule is calibrated for a specific family P_reinforce, but it lacks clarity on why this family is of interest, the underlying intuition, and whether there are notable examples of distributions that satisfy these conditions. Similar inquiries apply to the distribution set considered in Theorem 7, highlighting a need for further elucidation or justification of these restricted sets.
Minor observations include a correction needed at line 227, where the expression should be 1(\sigma(i)\le q) instead of 1(\sigma^{-1}(i)\leq q), with a similar adjustment required at line 236. At line 242, the values 1 and 0 appear to be interchanged. The use of max(yi-v,0) in place of yi at line 254 seems unnecessary and could be simplified by redefining the label set. Lastly, the definition of u^p_{ij} is only provided for i\geq j, with the symmetry extension mentioned in the appendix but not in the main paper, potentially causing confusion. Overall, the paper demonstrates the significance of calibrated convex surrogate losses for multiclass classification, particularly when dealing with low-rank loss matrices, making it an important contribution to the field with applications to several significant losses.