The paper introduces a novel Bayesian inference approach in Monte Carlo Tree Search (MCTS) utilizing Thompson sampling for action selection, referred to as the Dirichlet-NormalGamma MCTS (DNG-MCTS) algorithm. This method approximates the cumulative reward of following the current policy from a given state, denoted as X_{s,\pi(s)}, using a normal distribution with a NormalGamma prior. The state transition probabilities are modeled using Dirichlet distributions. The action selection strategy employs Thompson sampling, where the expected cumulative reward for each action is calculated using a parametric distribution with parameters drawn from the posterior distributions, and the action with the highest expected value is chosen. The authors demonstrate the effectiveness of the proposed method through several benchmark tasks, showing that it can achieve slightly faster convergence compared to the UCT algorithm. Additionally, theoretical properties related to convergence are provided.
While there is extensive research on MCTS, the specific approach presented in this paper appears to be novel. However, the use of a NormalGamma prior for density approximation of cumulative rewards is not new, having been previously proposed by Dearden et al. in 1998. Similarly, modeling state transitions with a Dirichlet prior is a well-established approach. Therefore, the originality of the proposed modeling and inference methods in Section 3.2 may be limited. Nonetheless, the combination of this modeling with an action selection strategy based on Thompson sampling is innovative and may be of interest to related research areas.
The experimental results presented are somewhat weak. Given the vast amount of work on MCTS, the baseline methods used in the comparison are limited, primarily focusing on the classic UCT algorithm, with the exception of the CTP problem. More comprehensive experiments are necessary to assess computational costs and sample complexity, particularly in comparison to advanced methods discussed in references [b, d, e].
The majority of the paper is well-written, but the assumptions outlined in Section 3.1 require clarification. There appears to be a contradiction in the treatment of the cumulative reward X{s,\pi(s)}, which is argued to realistically follow a normal distribution. However, given that X{s,a} is modeled as a mixture of normal distributions, X{s,\pi(s)} should inherently be a mixture of normal distributions as well, with the number of mixtures increasing exponentially from leaf states to the root. This suggests that the assumption of X{s,\pi(s)} following a normal distribution may not be realistic. Instead, it could be viewed as a practical approximation. The relationship between cumulative rewards in successive time steps may also be related to the distributional Bellman equation discussed in reference [c]. It is recommended that this section be revised carefully in the final version to address these concerns.
Furthermore, there are specific technical details that require attention. For instance, in lines 137-139, the notation N(n\mu, \delta^2/n) seems to be incorrect and should possibly be N(n\mu, n\delta^2). Additionally, the summation \sum_n f appears to diverge as n approaches infinity, which could lead to inconsistencies in the mathematical derivations.
References [a] through [e] provide a backdrop of existing work in Bayesian Q-learning, Bayesian inference in MCTS, parametric return density estimation for reinforcement learning, and efficient Bayes-adaptive reinforcement learning, highlighting the potential for the proposed DNG-MCTS algorithm to contribute to the advancement of MCTS methodologies. Overall, the paper presents an interesting approach to Bayesian MCTS with a Thompson-sampling-based action selection strategy, demonstrating its potential to converge faster than the UCT algorithm in several benchmark tasks.