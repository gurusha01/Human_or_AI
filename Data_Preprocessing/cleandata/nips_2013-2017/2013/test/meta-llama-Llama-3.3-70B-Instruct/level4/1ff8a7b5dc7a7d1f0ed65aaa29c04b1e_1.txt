The authors introduce a novel deep architecture that integrates the hierarchical structure of deep learning with time-series modeling, reminiscent of Hidden Markov Models (HMMs) or recurrent neural networks. The proposed training algorithm constructs the network layer-by-layer, utilizing supervised pre-training with a next-letter prediction objective. Experimental results demonstrate that, after training large networks for approximately 10 days, the network's performance on a Wikipedia dataset, initially published by Hinton et al., surpasses that of previous work. The authors then delve into a detailed analysis and discussion of the network's task approach, highlighting how higher layers model long-term dependencies and how correspondence between opening and closing parentheses is represented as a "pseudo-stable attractor-like state".
This paper exhibits good quality, with clear writing, well-designed experiments, and supportive figures. However, my primary concern lies in the experimental design. The comparison is limited to a single dataset, which appears to have limited existing research, and direct comparisons are only made with other RNN-related models, omitting potentially more common hierarchical HMMs that can solve similar tasks. Although Table 1 lists other approaches, they are applied to a different corpus, rendering them incomparable.
The paper's contribution lies more in its analysis of the network's operational mechanisms, which is thoroughly examined, rather than its achievements. While the paper proposes a new deep architecture combining hierarchical deep learning features with time-series modeling, its well-written analysis and modeling are overshadowed by somewhat inconclusive results, as the paper's primary objective is to introduce a new architecture.