This paper presents a comprehensive comparison of human active learning, optimization, and interpolation/extrapolation of 1D functions, yielding two primary conclusions: (a) humans outperform modern optimization algorithms, and (b) human behavior aligns largely with Bayesian inference, search, and prediction based on Gaussian process assumptions regarding function smoothness.
However, I have reservations regarding conclusion (a), which I consider less crucial than conclusion (b). I am not convinced that the authors have adequately accounted for the additional information available to humans. A critical concern is the range of the y-axis, which remains an issue despite the authors' attempt to mitigate it by jittering the y-offset. Specifically, I would like to know the range of the function relative to the displayed range, as well as whether these relative ranges remain constant across trials. This information is essential because humans can infer the relative position of a point on the y-axis, whereas optimization algorithms lack this knowledge.
The paper employs a multitude of measures to evaluate the mismatch between algorithmic and human behavior, which I find confusing and largely unnecessary. A more straightforward approach might involve providing the algorithm with the first K points observed by a subject and then comparing the K+1th point selected by the algorithm with the one chosen by the human. This could offer a more intuitive and diagnostic measure of comparison.
I am also overwhelmed by the noisiness and apparent lack of diagnosticity of the various measures presented in Figure 4. Furthermore, I struggle to understand the significance of agreement on the distribution of search step sizes. Instead of this convoluted array of measures, I would like to see a more in-depth analysis of human consistency across subjects. While the low standard deviations of crude performance across individuals suggest some level of consistency, it is insufficient. I propose calculating the split-half correlations of performance across different trials and assessing how well the distribution of clicks from one half of the subjects predicts the distribution of clicks from the other half. This would enable a more meaningful comparison of across-subject agreement with the agreement between humans and various algorithms.
In Experiment 6, I question the authors' assumption that the first click should be to the maximum variance point. A more comprehensive decision-theoretic analysis of the problem is necessary. Given that both clicks are equally important, it is unclear whether the optimal strategy is to maximize information gain on the first click. If the maximum is unambiguous, wouldn't it be more appropriate to click on that point?
On a minor note, I was confused by the claim that subjects use the 'gradient,' as they do not have direct access to this information. Instead, they likely rely on an estimated smoothness, which further supports the use of Gaussian process-like algorithms that can estimate smoothness.
In conclusion, this paper presents a rich set of experiments on human optimization, but it would benefit from a more thoughtful and streamlined approach to evaluating model fit and estimating across-subject consistency. By using fewer, more carefully selected measures and providing a more detailed analysis of human consistency, the authors could strengthen their conclusions and improve the overall impact of the paper.