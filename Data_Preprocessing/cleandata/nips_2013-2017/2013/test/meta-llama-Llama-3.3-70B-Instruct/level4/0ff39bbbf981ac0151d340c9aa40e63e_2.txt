This paper proposes a novel two-step approach for learning cross-lingual topic representations by leveraging matrix completion and latent semantic analysis techniques. The method utilizes a projected gradient descent algorithm to solve the matrix completion problem. The experimental results demonstrate the superiority of the proposed method over both mono-lingual and cross-lingual baselines in sentiment analysis tasks using parallel Amazon review data. Additionally, learning curves generated with varying sizes of unlabeled data highlight the algorithm's ability to learn accurate cross-lingual topic representations with a relatively small amount of parallel data.
The quality of the experiments is high, with thorough comparisons to two baselines, including plain bag-of-words and cross-lingual LSA, as well as two state-of-the-art cross-lingual dimensionality reduction algorithms, providing strong support for the claims made. 
The paper is well-organized, making it easy to follow and reproduce the results, as all parameters and their selection processes are clearly described. A minor suggestion is to provide a more detailed explanation of the motivation behind using matrix completion in the introduction, although it is adequately described in Section 3.
While cross-lingual representation learning is not a new task, the application of matrix completion to this area is innovative, and the proposed algorithm achieves superior performance compared to previous methods. 
The significance of this work lies in its consistently higher results compared to other baselines, indicating the potential of matrix completion as a preprocessing step for cross-lingual sentiment classification tasks. This formulation may also be effective in other tasks, suggesting a promising direction for future research. The proposed two-step method, which combines matrix completion with latent semantic analysis for cross-lingual sentiment classification, outperforms mono-lingual baselines and other cross-lingual baselines, including a state-of-the-art method, and remains stable across different sizes of unlabeled parallel training data.