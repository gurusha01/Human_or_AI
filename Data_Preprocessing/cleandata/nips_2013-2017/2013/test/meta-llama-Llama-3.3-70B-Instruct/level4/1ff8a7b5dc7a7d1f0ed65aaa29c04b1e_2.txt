REPLY TO REBUTTAL: 
- I largely concur with the suggestion to focus on a single task. 
- Experimenting with truncated backpropagation, such as using 30 steps, could still be beneficial. Although it may not learn to balance parentheses as effectively, it could potentially achieve comparable entropies at a significantly reduced cost, which would be worth verifying.
This study examines the effectiveness of deep recurrent neural networks. 
The paper presents intriguing findings, including the discovery that deeper recurrent neural networks outperform traditional RNNs when the number of parameters is held constant. Other notable results include an analysis demonstrating that deeper networks retain more "long-term information" and the finding that standard backpropagation through time (BPTT) can train character-level RNNs to balance parentheses, a task previously thought to be achievable only with hierarchical frameworks.
However, the paper claims to introduce deep RNNs, which is not accurate, as they have been used in prior research, such as in http://www.cs.toronto.edu/~graves/icassp_2013.pdf, and likely in earlier studies. The paper should not assert that it has introduced this architecture, as it is acceptable to focus on analysis without presenting new architectures.
The analysis, although meaningful, would be more compelling if it were applied to multiple problems, such as speech recognition tasks. Without this, the findings may be specific to the problem at hand. 
Lastly, the hierarchical framework experiments may have been unnecessarily costly, given that truncated backpropagation, introduced in the 1990s, was successfully used by Mikolov (as seen in his PhD thesis) to train RNNs as excellent language models. It is likely that truncated BPTT would also perform well in this context, and confirming this would be a valuable addition. 
This work provides an interesting examination of deep RNNs, yielding two primary results: 1) deep RNNs surpass standard RNNs in character-level language modeling when parameters are fixed; 2) deeper layers exhibit more long-range structure; and 3) BPTT can train character-level RNNs to balance parentheses. Although the results are intriguing, the paper's focus on analysis would be strengthened by applying a similar examination to a speech task, such as demonstrating the superiority of deeper RNNs and the presence of long-range information in their deeper layers.