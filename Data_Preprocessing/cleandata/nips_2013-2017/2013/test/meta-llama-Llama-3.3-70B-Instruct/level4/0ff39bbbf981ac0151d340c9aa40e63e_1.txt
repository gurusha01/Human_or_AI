The paper presents a matrix completion approach for cross-domain classification, leveraging labeled data from an auxiliary domain and unlabeled parallel data. This approach involves a two-step process: first, constructing an incomplete matrix encompassing all documents and domains, which is then completed under low-rank and sparsity constraints using the projected gradient descent algorithm. Second, the feature dimension of the completed matrix is reduced via Latent Semantic Indexing (LSI), followed by training a standard classifier on the resulting representation. The paper also provides a convergence guarantee for the projected gradient descent algorithm and reports favorable experimental results.
Several aspects of the paper warrant further clarification and discussion:
- The decision to reduce the feature dimension of the completed matrix M from d to k using LSI, attributed to M's alleged inability to handle feature sparseness, requires elaboration. Is this step primarily motivated by computational convenience, given that M* is expected to be relatively dense despite sparsity enforcement? Furthermore, the choice of LSI over other dimensionality reduction techniques, such as Principal Component Analysis (PCA), needs justification.
- The paper appears to exploit external data in two distinct ways: utilizing labeled source language data to enhance the learning of labeled target language data, and leveraging unlabeled parallel data. Clarifying and separating these aspects, with separate results, would be beneficial for understanding the contributions of each.
- The experimental settings, particularly in section 5.2, are somewhat unclear. The training data consists of 4,000 labeled source language documents, 100 labeled target language documents, and 2,000 unlabeled parallel documents, with classification results reported on the remaining 1,900 labeled target language documents. The variation in the number of unlabeled parallel documents from 200 to 2,000 in section 5.3 and the comparative performance of CL-KCCA and TSL in Figure 1 versus Table 1 also require explanation.
- The assumption that a fully observed document-term matrix would be low-rank, despite being sparse, needs clarification. Typically, sparsity does not guarantee low-rankness, as documents are rarely linear combinations of other documents, suggesting a rank equal to the minimum of the number of documents and the vocabulary size.
- Providing intuition on why TSL outperforms other methods, including CCA, which seems like a more natural fit for cross-language representation learning, would be valuable. The matrix completion approach, particularly the enforcement of the low-rank condition on M*, appears to be a key factor, but how it compares to other techniques is not clear.
- Information on the algorithm's running time, specifically how the projected gradient descent algorithm compares in terms of computational time to methods like CL-LSI, CL-KCCA, and CL-OPCA, would be useful.
Minor corrections and suggestions include fixing the notation from (ij) to (i,j) two sentences before equation (2) and removing "such as" from the sentences preceding equations (4) and (5). Overall, while the paper proposes a simple, effective, and generally well-presented method, some technical points require clarification to enhance the manuscript's clarity and impact.