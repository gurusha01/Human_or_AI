This paper addresses the growing need to run machine learning algorithms on larger datasets by exploring the application of optimistic concurrency control (OCC) to distributed machine learning. OCC is a programming paradigm that enables distributed algorithms to run on infrastructure with no shared memory and slow communication between processing units, assuming individual processing units can freely access shared state and verifying this assumption at certain computation checkpoints.
The authors provide a compelling argument for investigating OCC in the machine learning context, building on existing work on embarrassingly parallel algorithms, distributed algorithms using locking paradigms, and coordination-free approaches. They apply OCC to three algorithms: DP-means, the Facility Location problem, and BP-means, with clear and understandable explanations. One minor point worthy of mention in the appendix is the initial cluster assignment process, specifically how to argmin_{\mu \in C} when C is empty.
Upon examining Algorithm 3, a concern arises regarding the expense of sending x_i between processing units in a distributed setting. Given the high cost of moving datasets around a cluster compared to moving parameters, a discussion on this point would be valuable. The authors provide correctness proofs for the algorithms, which appear to be correct based on a cursory review of the appendix.
The evaluation consists of two experiments: one on a small synthetic dataset demonstrating modest correction needs and another on Spark using Amazon AWS, showcasing convincing results on large datasets. However, the nature of the data used in the larger experiment (synthetic or real) is unclear. A minor notation suggestion is to use "i.i.d." instead of "iid" for consistency. Overall, the paper is well-written and effectively explores the application of OCC to distributed machine learning algorithms.