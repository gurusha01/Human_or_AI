The manuscript provides complexity bounds for policy iteration in two algorithmic variants: Howard PI, which updates the policy at all states with a superior action, and a version analogous to Simplex in the linear programming formulation, where policy changes occur only at states with maximum advantage. The paper offers a simplified re-derivation of existing bounds and achieves improvements on some of these bounds.
The contribution of the paper is noteworthy, with proofs that appear to be correct. Although the novelty is somewhat limited due to its foundation on existing research, the manuscript still manages to enhance these preceding results. The writing is clear and understandable.
Minor suggestions for improvement include:
- Line 43: Replacing "When at state" with "At state" for better clarity.
- The definition of advantage on line 085 is missing a prime symbol, which, if not corrected, renders the statement trivial. Additionally, line 097 should read "one state" instead of "on state".
- Line 112 could be improved by changing "and that" to simply "that".
- A minor typo is present on line 195, where "the the" should be corrected to a single "the".
- Line 217 starts with "Material and", which seems out of place and could be revised for better flow.
- On line 367, the reference to "this lemma" is ambiguous and should specify which lemma is being referred to.
- In the proof of Lemma 3 (line 72), the expression \max{s,\bar{\pi}} should be clarified as \maxs \max_{\bar{\pi}} to indicate the order of operations, assuming that is the intended meaning.
- Including examples of MDP types where Corollary 2 applies would greatly enhance the manuscript's utility. Overall, the paper presents a valuable advancement in the bounds for policy iteration, building upon and improving existing limits. While it does not radically depart from established proof techniques, it constitutes a welcome contribution to the field.