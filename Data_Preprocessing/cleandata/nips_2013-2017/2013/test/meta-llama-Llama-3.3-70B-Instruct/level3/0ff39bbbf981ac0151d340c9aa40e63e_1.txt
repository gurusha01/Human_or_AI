This paper proposes two approaches to address the problem of risk minimization in the presence of random classification noise. The first approach uses unbiased estimators of the loss function, while the second approach leverages a reduction of risk minimization under noisy labels to classification with weighted 0-1 loss. The paper provides theoretical guarantees for both approaches and demonstrates their effectiveness through experiments on synthetic and benchmark datasets.
The strengths of the paper include its ability to provide general results in the setting of convex surrogates without any assumptions on the true distribution. The authors also provide two different approaches to suitably modifying any given surrogate loss function, which lead to similar risk bounds. The paper resolves an elusive theoretical gap in the understanding of practical methods like biased SVM and weighted logistic regression, showing that they are provably noise-tolerant.
However, there are some weaknesses and areas for improvement. The paper could benefit from clearer notation and organization, as some sections and equations are difficult to follow. Additionally, the authors could provide more intuition on why the proposed methods perform better than other methods like CCA. The running time of the algorithm and its comparison to other methods like CL-LSI, CL-KCCA, and CL-OPCA could also be discussed.
Some minor changes to the paper could include fixing notation inconsistencies and removing unnecessary phrases. The authors could also consider providing more experimental results to demonstrate the robustness of the proposed methods to different types of noise and datasets.
Overall, the paper makes a significant contribution to the field of machine learning and provides a valuable insight into the problem of risk minimization under random classification noise. The proposed methods are easy to implement and have impressive performance even at high noise rates, making them a useful tool for practitioners.
Arguments for acceptance:
* The paper provides general results in the setting of convex surrogates without any assumptions on the true distribution.
* The authors provide two different approaches to suitably modifying any given surrogate loss function, which lead to similar risk bounds.
* The paper resolves an elusive theoretical gap in the understanding of practical methods like biased SVM and weighted logistic regression.
* The proposed methods are easy to implement and have impressive performance even at high noise rates.
Arguments against acceptance:
* The paper could benefit from clearer notation and organization.
* The authors could provide more intuition on why the proposed methods perform better than other methods like CCA.
* The running time of the algorithm and its comparison to other methods like CL-LSI, CL-KCCA, and CL-OPCA could be discussed.
* The paper could benefit from more experimental results to demonstrate the robustness of the proposed methods to different types of noise and datasets.