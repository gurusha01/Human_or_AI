This paper proposes a novel approach to distributed clustering for k-means and k-median objectives, providing a distributed coreset construction algorithm that improves communication complexity over existing methods. The authors extend the applicability of random Fourier features to general Euclidean spaces and propose a Nystrom method for general kernels, relying on good landmark selection. 
The paper's key contribution is the development of a distributed algorithm for constructing a global coreset, which is the same size as the centralized construction and only requires a single value to be communicated to each node. This approach excels in sparse networks with large diameters, where previous methods require coresets that are quadratic in the size of the diameter for k-median and quartic for k-means.
The paper is well-motivated and accessible, making a nice contribution to the DPP thread in machine learning. The experimental results on large-scale data sets demonstrate that the proposed algorithm outperforms other coreset-based distributed clustering algorithms. The authors provide a detailed comparison with other coreset algorithms, highlighting the advantages of their approach.
The strengths of the paper include its technical soundness, clarity, and originality. The authors provide a thorough analysis of the algorithm's performance, including a proof sketch of the main theorem. The paper is well-organized, and the writing is clear and concise.
However, there are some minor issues with the paper. The authors could provide more detailed derivations and clarifications in some sections, particularly in the proof sketch of Theorem 1. Additionally, the paper violates the NIPS margins in some places.
Overall, I recommend acceptance of the paper, despite some minor issues. The paper's contributions to the field of distributed clustering and its potential impact on the machine learning community make it a valuable addition to the conference.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to distributed clustering.
* The algorithm improves communication complexity over existing methods.
* The paper is well-motivated and accessible, making a nice contribution to the DPP thread in machine learning.
* The experimental results demonstrate the algorithm's effectiveness.
Arguments con acceptance:
* The paper violates the NIPS margins in some places.
* Some sections could benefit from more detailed derivations and clarifications.
* The paper could be improved with additional experiments and comparisons with other algorithms.