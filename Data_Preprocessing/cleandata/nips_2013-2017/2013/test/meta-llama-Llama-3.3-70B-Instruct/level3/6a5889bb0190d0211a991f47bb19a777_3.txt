This paper presents a novel approach to distributed machine learning using optimistic concurrency control (OCC), which offers a balance between the mutual exclusion and coordination-free approaches. The authors demonstrate the effectiveness of OCC in distributing unsupervised learning algorithms, including DP-Means, Online Facility Location (OFL), and BP-Means. The paper provides a clear and well-organized presentation of the OCC pattern, its application to various algorithms, and a thorough analysis of its correctness and scalability.
The strengths of the paper include its originality, technical soundness, and significance. The authors introduce a new paradigm for distributed machine learning, which has the potential to improve the scalability and efficiency of various algorithms. The paper provides a rigorous analysis of the OCC approach, including proofs of serializability and scalability, which demonstrates the correctness and potential of the method. The experimental results, both in simulated and distributed settings, demonstrate the effectiveness of the OCC algorithms in achieving high parallelism and scalability.
However, there are some weaknesses and areas for improvement. The paper assumes an episodic formulation with fixed-length episodes, which may not be realistic in all scenarios. Additionally, the comparison to the state of the art is not clearly stated, making it difficult to assess the significance of the results. The paper could benefit from a more detailed discussion of the limitations and potential extensions of the OCC approach.
Arguments pro acceptance:
* The paper introduces a novel and original approach to distributed machine learning, which has the potential to improve the scalability and efficiency of various algorithms.
* The authors provide a rigorous analysis of the OCC approach, including proofs of serializability and scalability, which demonstrates the correctness and potential of the method.
* The experimental results demonstrate the effectiveness of the OCC algorithms in achieving high parallelism and scalability.
Arguments con acceptance:
* The paper assumes an episodic formulation with fixed-length episodes, which may not be realistic in all scenarios.
* The comparison to the state of the art is not clearly stated, making it difficult to assess the significance of the results.
* The paper could benefit from a more detailed discussion of the limitations and potential extensions of the OCC approach.
Overall, the paper presents a significant contribution to the field of distributed machine learning, and its strengths outweigh its weaknesses. With some revisions to address the limitations and comparisons to the state of the art, the paper has the potential to be a high-quality contribution to the conference.