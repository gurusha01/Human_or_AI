In this well-crafted and comprehensive paper, the authors examine human active learning, optimization, and inter/extrapolation of 1D functions. They arrive at two primary conclusions: (a) humans outperform modern optimization algorithms, and (b) human behavior aligns closely with Bayesian inference/search/prediction, assuming Gaussian process priors about function smoothness.
I have some reservations about conclusion (a), which I consider less critical than (b). Specifically, I am not entirely convinced that the authors have accounted for all the additional information available to human participants. A key issue lies in the range of the y-axis. While the authors mention jittering the y-offset to mitigate this, the numerical details are crucial: what is the range of the function relative to the displayed range, and how does this compare to the jitter range? Are these relative ranges consistent across trials? For instance, if a point appears near the bottom of the screen, participants can infer that it is far from the maximum, whereas points near the top suggest proximity to the maximum y-value. Optimization algorithms lack access to this contextual information and thus cannot leverage it during the search process.
Although the paper presents a variety of interesting measures to capture different aspects of the (mis)alignment between human and algorithmic behavior, I find this array of metrics somewhat overwhelming and, in many cases, unnecessary. A more streamlined approach could involve a single, focused measure, such as the following: provide the algorithm with the first K points observed by a participant, have the algorithm select point K+1, and then compare the K+1th selections made by the algorithm and the participant. Alternatively, another approach might work better, but as it stands, Figure 4 feels overly noisy and lacks diagnostic clarity. Additionally, I struggle to understand the significance of agreement on the distribution of search step sizes and why this measure is critical.
Regarding human consistency, I would prefer to see a more focused analysis of across-subject agreement rather than the current complex array of measures. While the low standard deviations in participants' overall performance hint at consistency, this evidence is insufficient. For example, what are the split-half correlations of performance across trials? How well does the distribution of clicks from one half of the participants predict the distribution of clicks from the other half? This latter measure is particularly important for comparing across-subject agreement with the agreement between human behavior and various algorithms. Such comparisons could also be extended to many of the marginal measures currently used in the paper.
In Experiment 6, I am unclear why the authors assume that the first click should target the point of maximum variance. A complete decision-theoretic analysis of this problem would be valuable. If both clicks are "equally important" (though the authors' intent here is somewhat ambiguous), it is not obvious that maximizing information on the first click is the optimal strategy. For example, if the maximum is already unambiguous, wouldn't clicking directly on it be the more rational choice?
Minor points: I found the claim that participants use the "gradient" somewhat misleading. Since participants do not have direct access to the gradient, it seems more accurate to say they rely on an "estimated smoothness." This observation further supports the idea that human behavior might be approximated by a Gaussian process-like algorithm that performs such estimations. 
Overall, the paper offers a rich set of (occasionally redundant) experiments on human optimization, evaluated through an extensive model comparison. It would benefit from fewer but more carefully chosen measures for assessing model fit, as well as more rigorous analyses of across-subject consistency.