This paper addresses the compelling problem of visual concept learning, with a particular focus on determining the appropriate degree of generalization for a concept. To this end, the authors first concentrate on generating a large-scale dataset of concepts at varying levels of abstraction from ILSVRC, supplemented by human judgments collected via AMT. They then propose a Bayesian concept learning algorithm to address perceptual uncertainty. While the problem of learning concepts at different levels of abstraction is indeed intriguing, this reviewer finds the algorithm's presentation to be unclear and lacking in intuitive explanations. Furthermore, the experimental evaluation is relatively weak due to the absence of comparisons with strong baselines.
Detailed comments are as follows:
1. In Equations (4) and (5), is I(.) intended to represent an indicator function? Please clarify.
2. In Equation (4), the notation I(xi \subseteq h) suggests that $h$ is a set and $xi$ is also a set. This is confusing. Why is $xi$ treated as a set? In contrast, Equation (5) uses I(\hat{y}i \in h), which implies that $\hat{y}_i$ is an individual item. Please provide clarification.
3. In Equation (5), it appears that the summation applies only to the first term, A{j\hat{y}i}, while the remaining terms are independent of the index $j$. As a result, the summation over A{j\hat{y}i} reduces to a single column vector. Is this the intended outcome? The motivation behind this equation remains unclear.
4. There seems to be a typographical error in the last line of page 5: "… the true leaf node is $j$ given the classifier output being $j$." Please verify whether the repeated use of $j$ is intentional.
5. On line 243 of page 5, the statement "for example, as the number of examples that are all Dalmatians increases, it becomes increasingly likely that the concept is just Dalmatians and not dogs in general even though both are logically possible, …" is valid. However, what does this imply for the hypothesis size? The definition of the hypothesis is unclear in the paper. The authors seem to equate hypothesis size with the number of samples belonging to a hypothesis. How is this definition applicable in the real world, given that the number of potential images for any concept can be infinitely large? Is it reasonable to define hypothesis size based on the number of samples in ImageNet?
6. Due to the numerous ambiguities in the algorithm's presentation, it is challenging to fully understand the proposed method and assess why it is effective, especially given that the algorithm is described only at a high level.
7. The baseline methods are insufficiently robust and fail to provide convincing comparisons. For instance:
   a) In the naïve vision approach, only GIST features are utilized, whereas the proposed algorithm employs an overcomplete feature set comprising 160K dimensions. This creates an unfair comparison between the two methods.
   b) The paper references recent work on deep neural networks [10, 9] in Section 4.3. Why are DNNs or their variants not included as baseline methods? Is the exclusion solely because DNNs require more training data? If DNNs could outperform the proposed algorithm using training data readily available from ImageNet, it raises questions about the value of this work.
In summary, while the paper tackles an interesting problem of learning concepts at varying levels of abstraction and demonstrates commendable effort in constructing a large-scale concept learning dataset with human judgments from AMT, the algorithm's presentation lacks clarity and intuitive explanations. Additionally, the experimental evaluation is unconvincing due to the overly simplistic implementation of some baseline methods and the omission of stronger baselines.