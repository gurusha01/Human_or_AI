The paper tackles the challenge of binary classification in scenarios where training labels are corrupted by class-conditional random noise. To address this, the authors introduce two surrogate-loss-based learning approaches: the first leverages a straightforward symmetry property of the loss function to construct an unbiased estimator of the non-noisy risk, while the second employs a weighted 0-1 loss derived from a suitable reduction of the noisy label learning problem.
This work represents a clear and substantial contribution. It offers novel theoretical insights (e.g., learnability with convex surrogates under noise, noise-tolerance properties of SVMs, etc.) alongside robust empirical findings. The paper is well-structured and well-written, making the results accessible and compelling.
I primarily have questions regarding potential future directions:  
- The authors briefly mention adversarial noise. Before exploring this, could something be said about learnability under monotonic noise, as defined by Bylander?  
- Additionally, how might the proposed methods extend to more complex noise models, such as Constant Partition Classification Noise (CPCN)?  
Overall, this is an excellent paper that delivers significant results for learning binary classifiers from noisy labels using convex surrogates. The technical contributions are impactful, the writing is clear, and the experimental results are strong.