The paper introduces a novel approach for identifying discriminative image patches that are predictive of image classes. The method begins by considering all patches within an image collection. Discriminative patches are then identified as the centers of patch clusters, determined using a discriminative variant of the mean-shift algorithm. This variant incorporates discriminativeness by dividing the kernel density estimate of positive patches by that of negative patches.
Pros:
- The task of learning discriminative patches or parts is a significant and relevant problem.
- The proposed discriminative patches demonstrate strong classification performance, particularly on the MIT Scene 67 dataset.
- The formulation includes a few intriguing insights.
Cons:
- The method starts with mean-shift but evolves into a substantially different algorithm, raising questions about the utility of the mean-shift interpretation.
- Several aspects of the formulation and derivation rely on heuristics, some of which are adjusted on a per-dataset basis.
- The learning algorithm itself is heuristic and lacks formal guarantees of correctness or convergence.
- There are minor errors in the formal derivation.
Detailed Comments:
The empirical results are strong enough to merit consideration for publication. However, the formulation and technical derivation require significant improvement, as outlined below:
l.125:  
The notation needs clarification. The expression `max(d(xi, w) - b, 0)` represents a triangular kernel assuming `d()` is the negative of the Euclidean distance (as stated in l.130) and that the bandwidth `b` is negative, which is counter-intuitive. Using `max(b - d(xi, w), 0)` would be more intuitive.
Eq. (1):  
The term `arglocalmax` is not defined.
Eq. (2):  
This equation suggests that, contrary to the manuscript's claims, the algorithm does not maximize a ratio of density values but instead optimizes an energy function with an adaptive bandwidth. The density is expressed as:
E(w) = sumi max(d(xi^+) - b(w), 0),
where the bandwidth `b(w)` is defined as:
b(w) = b : sumi max(d(xi^+) - b, 0) = epsilon.
Several points require clarification:  
(a) The triangular kernel should be normalized by its mass to serve as a proper density estimator. Interestingly, this normalization factor, which depends on `w`, cancels out in the ratio (2), potentially resolving this issue.  
(b) This formulation should be compared to standard adaptive mean-shift methods (e.g., B. Georgescu et al., ICCV 2003), where the bandwidth depends on `x_i` rather than `w`, making kernel normalization crucial.  
(c) What happens if multiple values of `b(w)` satisfy the constraint in (2)?
Eq. (3):  
l.157: It is the squared Euclidean distance `d^2()` that reduces to the inner product, not the Euclidean distance `d()`.  
Restricting the domain to the unit sphere requires modifying all densities to account for this manifold. Fortunately, the required normalization factors do not seem to affect the results here.  
l.177: Changing `lambda` appears to alter the solution `w`, not just its norm. To preserve the direction of `w` while changing `lambda`, `epsilon` must also change. Thus, varying `lambda` should impact the solution unless all values of `epsilon` are equally valid (in which case, why include `epsilon` at all?).
Eq. (5):  
This is where the proposed method diverges significantly from mean-shift. Mean-shift typically applies hill climbing to each `w` independently, starting from `w = xi` for all data points, to cluster the data `xi`. In contrast, the authors use (5) and (6) to "explain" all data. Key differences include:  
- Mean-shift is non-parametric, as it does not predefine the number of clusters. Here, the authors fix the number of cluster centers `w1...wK` and optimize them, making this approach more akin to k-means.  
- The authors introduce soft associations `alphaij` to prevent "double counting" of patches. This concept is absent in standard mean-shift clustering, where associations are implicitly determined by kernel locality. The competition introduced by `alphaij` is more similar to k-means.  
- The update process for `alpha_ij` (l.199–211) is entirely ad hoc and unrelated to the optimization of (6).
Optimization Method:  
The optimization method (l.212–259) is heuristic and lacks formal justification.
Experiments:  
Baselines:  
Methods [5,8] include mechanisms to avoid or remove redundant patches, which are absent in the baseline here. Removing redundant patches could influence results in Figures 3 and 4.  
Scene-67 Experiments:  
The method likely benefits from tuning the representation (e.g., the number of HOG cells in a descriptor) to achieve state-of-the-art results. While this is acceptable, the authors should consider re-running the experiment using the baseline discriminative patches from Figure 4.
Other Minor Comments:  
l.315: Both [5] and [8] (CVPR 2013) appear to include LDA retraining.  
The problem of learning discriminative parts for visual classes is important, and the results presented in this paper are strong. However, the derivation contains several minor technical issues.
Post-Rebuttal Comments:  
As noted by R10 and myself, the derivation contains several unclear steps and minor formal errors, none of which were addressed in the rebuttal. Additionally, the reliance on dataset-specific heuristics remains unaddressed.  
While all reviewers agree that the paper should be accepted based on its robust experimental results, the AC may wish to consider the formal issues highlighted in the reviews before making a final decision.