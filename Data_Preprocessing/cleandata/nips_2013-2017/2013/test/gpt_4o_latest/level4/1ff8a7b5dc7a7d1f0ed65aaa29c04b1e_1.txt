The authors introduce a novel deep architecture that integrates the hierarchical nature of deep learning with time-series modeling techniques commonly associated with HMMs or recurrent neural networks. The proposed training methodology incrementally constructs the network layer-by-layer through supervised (pre-)training using a next-letter prediction objective. Experimental results reveal that, after training extremely large networks for approximately 10 days, the model outperforms prior approaches on a Wikipedia dataset originally published by Hinton et al. The authors further delve into an in-depth analysis of the network's behavior, illustrating, for instance, how higher layers capture long-term dependencies and how the correspondence between opening and closing parentheses is represented as a "pseudo-stable attractor-like state."
The paper is of high quality, well-written, and supported by well-designed experiments and informative figures. However, my primary concern lies with the experimental evaluation. The comparisons are limited to a single dataset, which appears to have limited prior work. Furthermore, the direct comparisons are restricted to RNN-related models, while alternative approaches, such as hierarchical HMMs, which are capable of addressing similar tasks, are not included. Although Table 1 lists other methods, they are evaluated on a different corpus, rendering direct comparisons infeasible.
The paper's main contribution lies in its detailed analysis of how the proposed network functions (which is thoroughly examined) rather than in its empirical performance. While this aligns with the paper's focus, the results remain somewhat inconclusive. Overall, the paper introduces a new deep architecture that combines the hierarchical properties of deep learning with time-series modeling. It is well-written and provides valuable insights into the model's behavior, but the experimental evaluation could be more comprehensive.