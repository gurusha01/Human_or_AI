The paper introduces calibrated convex surrogate losses tailored for multiclass classification. When the n by k loss matrix (where n represents the size of the label set and k represents the size of the prediction set) has rank d, minimizing the surrogate loss reduces to solving a d-dimensional least squares problem. Subsequently, converting the solution into a prediction involves performing a d-dimensional linear maximization over a set of size k. The paper provides two illustrative examples (precision at q and expected rank utility) where n and k are exponentially large, yet predictions can be computed efficiently. Additionally, it discusses two other examples (mean average precision and pairwise disagreement) where n and k are also exponentially large, but for these cases, calibrated predictions cannot be computed efficiently. However, the authors demonstrate that efficiently computable predictions remain calibrated for certain restricted sets of probability distributions.
This work represents a significant and exciting contribution, addressing a broad range of multiclass losses with substantial practical relevance. From a technical perspective, the results build on two key findings from reference [16], with the critical low-rank observation already established in that prior work. The paper is well-written and easy to follow.
The primary critique concerns the results on calibration with respect to restricted sets of probability distributions. For example, Theorem 4 establishes that the efficiently computable prediction rule is calibrated for a specific family, P_reinforce. However, the paper does not adequately explain why this family is of interest. What is the underlying intuition for considering this family? Are there compelling examples of distributions that satisfy these conditions? Similar concerns arise for the set of distributions discussed in Theorem 7.
Minor comments:  
- Line 227: The expression 1(σ⁻¹(i) ≤ q) should be corrected to 1(σ(i) ≤ q). (Similarly, see line 236.)  
- Line 242: The values 1 and 0 appear to be interchanged.  
- Line 254: Including max(yᵢ - v, 0) instead of simply yᵢ seems unnecessary. Why not redefine the label set as {0, 1, ..., s - v}ʳ?  
- Line 320: The term uᵖᵢⱼ is only defined for i ≥ j. While the appendix mentions that it is symmetrized, this detail is not clarified in the main paper.  
Overall, the paper provides a valuable contribution by addressing calibrated convex surrogate losses for multiclass classification, particularly in the context of low-rank loss matrices. This work has significant implications for several practically important loss functions.