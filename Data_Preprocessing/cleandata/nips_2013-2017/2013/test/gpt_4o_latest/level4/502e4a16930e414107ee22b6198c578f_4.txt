The paper establishes bounds on the complexity of policy iteration for two variants of the algorithm: one that updates the policy at all states where there exists an action superior to the current policy (referred to by the authors as Howard PI), and another that updates the policy only at states with maximal advantage (corresponding to executing the Simplex method in the linear programming formulation of the algorithm). The authors re-derive some known bounds in a more streamlined manner and also achieve improvements on certain existing bounds.
The paper offers an engaging contribution, and the proofs appear to be correct. While the level of novelty is moderate, as the work builds on prior results, it does provide meaningful enhancements to these results. The paper is written in a clear and comprehensible manner.
Minor comments:  
- Line 043: "When at state" -> "At state"  
- Line 085: The definition of advantage is missing a \prime, as otherwise it becomes trivially incorrect.  
- Line 097: "on state" -> "one state"  
- Line 112: "and that" -> "that"  
- Line 195: "the the"  
- Line 217: "Material and"  
- Line 367: The phrase "this lemma" is ambiguousâ€”please clarify which lemma is being referenced.  
- In the proof of Lemma 3 (line 72), you write \max{s,\bar{\pi}}. The order of maximization is significant here, so it would be clearer to write \maxs \max_{\bar{\pi}} (assuming this is the intended interpretation).  
- Including examples of specific types of MDPs where Corollary 2 applies would be highly beneficial.  
Overall, the paper advances the current bounds on policy iteration, contributing incremental yet valuable progress to the field. While it does not introduce fundamentally new proof techniques, it represents a solid and worthwhile addition to the existing body of work.