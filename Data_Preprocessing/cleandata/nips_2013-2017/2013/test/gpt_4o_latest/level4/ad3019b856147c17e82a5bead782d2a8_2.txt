The authors demonstrate empirically that an existing method for explaining an invariance model in visual recognition systems can be generalized to handle transformations beyond affine ones, including out-of-plane rotation, changes in illumination, and, most notably, variations in background clutter. Since these transformations pose significant challenges in developing invariant descriptors for objects in images, this work is compelling as it systematically addresses each of these transformations within the framework proposed in [1]. 
The paper relies heavily on [1], which demands substantial elaboration to ensure clarity and accessibility. Nonetheless, the manuscript is exceptionally well-written, highly original, and provides a solid theoretical foundation for various aspects of feature design, such as pooling strategies. Beyond its primary contribution of empirically extending the approach in [1] to a broader range of transformations, I believe this work offers computer vision researchers—particularly those focused on designing or learning representations—a principled framework for understanding the necessary (or sufficient) properties required for invariant features.