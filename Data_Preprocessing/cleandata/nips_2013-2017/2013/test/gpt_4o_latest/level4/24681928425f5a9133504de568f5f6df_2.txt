This paper introduces a method for simultaneously learning the structure and parameters of stochastic AND-OR grammars. These grammars, which incorporate both AND rules and OR rules, are capable of representing various recursive phenomena, such as natural language and other types of grammars. The authors propose an elegant approach for unsupervised structure learning by iteratively introducing new AND-OR fragments and evaluating their model based on likelihood and prior gains.
The paper includes experiments on two tasks: learning event grammars and learning image grammars. In both cases, the proposed method achieves results that are competitive with existing approaches.
Overall, I found the paper to be a promising contribution, as it provides a feasible approach for learning stochastic grammars that utilize AND and OR rules. However, I have the following concerns:
1) The authors do not address the scalability of their learning algorithm. How well does it perform when applied to datasets with thousands of samples?
2) While the experiments are interesting, I would have liked to see an application to natural language sentences, as this is arguably the most intuitive use case for such grammars. For instance, would the proposed method be applicable to a dataset like the Penn Treebank, which has been a focus of prior work (e.g., Klein and Manning)? Although the paper demonstrates the effectiveness of the method on two tasks, additional experiments on natural language data would have strengthened the contribution.