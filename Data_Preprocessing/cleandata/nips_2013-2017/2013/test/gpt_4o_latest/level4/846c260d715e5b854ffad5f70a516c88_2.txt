This paper introduces a novel contribution to the extensive and continually expanding field of MCTS research. The proposed approach models the distribution of accumulated rewards in the search tree as a mixture of Gaussians, employing a Dirichlet prior for the mixture weights and a NormalGamma prior for the mixtures themselves. The authors justify their assumption of normality using the central limit theorem and provide a concise explanation of how the hyperparameters of the priors are set. By integrating this Bayesian framework with Thompson sampling, the paper presents an elegant variation of MCTS that appears to mitigate the heuristic-driven nature of UCB while achieving a more robust balance between exploration and exploitation.
The paper is well-structured, and the ideas and mathematical formulations are clearly articulated. While there are language issues scattered throughout the text, they do not render the paper incomprehensible (though these should be addressed, particularly in the introduction). The authors effectively introduce the necessary background concepts, justify their modeling assumptions, present the proposed model, and integrate the components cohesively. However, a more detailed comparison between Thompson sampling and UCB would strengthen the paper, particularly in terms of their respective assumptions, advantages, and limitations.
The empirical evaluation is thorough, but the paper would benefit from a more explicit comparison of computational complexity with UCT, including quantitative metrics or visualizations. It is evident that the proposed algorithm is computationally more demanding than UCT, but the extent of this additional cost remains unclear. Is the increased runtime on the order of seconds, hours, or days? UCT's success is largely attributed to its speed, which allows for significantly more sampling within the same timeframe compared to more complex methods, thereby yielding more accurate empirical estimates despite its simplicity. Additionally, it would be valuable to disentangle the contributions of Thompson sampling and the Dirichlet-NormalGamma distribution from the overall performance improvement over vanilla UCT. Can these gains be quantified independently?
One concern pertains to the assumptions underlying the claims of convergence to a Normal distribution. These claims rely on the premise that the future policy is fixed, making the rollout a Markov chain. However, as I understand it, the policy is not static and evolves over time due to learning or exploratory randomness. Wouldn't this dynamic nature of the policy undermine the validity of these claims? That said, the empirical results suggest that these assumptions do not significantly hinder the method's performance.
In summary, this is a strong paper that proposes a novel enhancement to Monte Carlo tree search by employing Bayesian mixture modeling to represent the accumulated rewards of actions in an MDP. The idea is conceptually sound, well-explained, and supported by empirical evidence. However, minor issues remain regarding language clarity, computational complexity analysis, and the validity of certain assumptions, which should be addressed to further strengthen the work.