Review - Paper 68 – Bayesian Optimization Explains Human Active Search  
The authors investigate various optimization strategies for one-dimensional continuous functions and their connection to human optimization behavior. They employ a diverse set of continuous functions (with one exception), including polynomial, exponential, trigonometric, and the Dirac function. Additionally, they examine how individuals interpolate and extrapolate noisy samples from a latent function—a topic with a long-standing tradition in psychology under the umbrella of function learning—and how participants choose additional samples to observe in the context of interpolation or extrapolation tasks. Overall, their findings suggest that Gaussian processes outperform approximately 20 other tested optimization methods in describing human performance.  
Overall, I found this paper highly engaging and believe it is a strong submission to NIPS. The authors address compelling problems relevant to machine learning-oriented cognitive scientists, such as function optimization and active interpolation/extrapolation. While the stimuli may be somewhat abstract for many cognitive psychologists, the paper is well-written, and aside from a few minor issues, the presentation is strong.  
However, I have two primary criticisms of the paper, in addition to several minor points outlined below.  
1. Unlike most function learning experiments, the experimental stimuli and procedure in this study are highly abstract (e.g., dots from a function and the task of "finding the maximum value of a function"). This lack of ecological validity makes it challenging to interpret the implications for real-world human function learning. The functions used in the experiments are unlikely to resemble those encountered in everyday life, and the strong influence of context on human learning is not adequately addressed. It remains unclear how the results might generalize to more contextually rich scenarios, which could be an intriguing avenue for future research. Simply stating that "many real-world problems can be translated into our synthetic tasks" is insufficient; concrete examples and a justification for this claim are necessary. That said, the results are compelling, as participants appear to engage effectively with the task.  
2. The paper would benefit from stronger integration with prior research on human function learning. For instance, it is unclear why the authors did not adopt the standard cover story and stimuli commonly used in function learning experiments. While I understand their decision to move beyond simple positive or negative linear functions (though, given that f20 in Experiment 1 is unbounded, including these functions would seem reasonable), the inclusion of a piecewise linear function would have been a prudent choice. Since f16 is nearly a negative linear function and has been shown to be easy to learn in previous studies, it follows that it would also be one of the easiest functions to maximize. Incorporating such connections to prior work would have strengthened the paper and made the results more interpretable.  
Minor concerns:  
- I would have preferred to see the discussion of previous psychological work earlier in the paper (e.g., in the introduction) rather than relegated to the end.  
- Figure 1 is not legible when printed in black and white.  
- The stimuli generation process in Experiment 1 seems unusual. Why not generate the functions directly by sampling random coefficients rather than generating random points and fitting a polynomial? If there is a rationale for this approach, it should be clarified in the paper.  
- Replace "Model-free results" with "Human results."  
- At times, the framing of the paper ventures into problematic theoretical territory regarding the level of analysis. For example, phrases like "humans might be following a GP" or "humans may use GP" suggest a process-level interpretation, for which the authors provide no evidence. A more appropriate phrasing would be, "Results of optimization tasks suggest that human clicks during search for a maximum of a 1D function can be predicted by a Gaussian process model." (Note: "a Gaussian processes model" should be corrected to "a Gaussian process model.")  
- The authors should report the rank-order correlation of the dependent variables for human participants and each optimization method (correlation over different function types).  
- It is unclear why different tolerance levels were used for different optimization algorithms. While this does not appear to bias the results in favor of the authors' conclusions, an explanation would be helpful.  
- Figures 3 and 5 are difficult to read when printed in black and white.  
- The text in Figure 4 is too small.  
- Results of Experiment 2 and 3: The statement, "Interestingly, on Deg5 functions, GP is closer to human clicks than the actual function (signed-rank test, p = 0.053), implying that GP captures clicks well in this case," raises questions about whether this result would hold after correcting for multiple comparisons (as Deg2 and Deg3 were also tested).  
- Experiments 2 and 4 are essentially function learning experiments.  
- In Experiment 5, why were participants always asked to guess the value of f(200) rather than a range of values? For the active extrapolation task, why not ask participants to predict the value of a point adjacent to f(200), which would essentially involve guessing that value?  
- It would have been interesting to include a control method for interpolation and extrapolation tasks, such as estimating the parameters of a polynomial of degree up to m (with m varied).  
- Some recent work on optimal foraging presented at last year's NIPS (e.g., Abbott, Austerweil, & Griffiths, 2011) and the GP account proposed by Hills et al. (2011) might provide relevant connections to this study.  
In summary, this is a strong paper, though it would benefit from greater integration with prior research and the use of more ecologically valid stimuli or cover stories.