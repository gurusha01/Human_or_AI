REPLY TO REBUTTAL:  
- I largely agree with the comment regarding the single task focus.  
- It would still be valuable to experiment with truncated backpropagation using, for instance, 30 steps. While it may be less likely to learn to balance parentheses (though this should be verified), it could still achieve competitive entropy levels at a significantly reduced computational cost.  
This paper explores the utility of deep recurrent neural networks (RNNs).  
It arrives at several intriguing conclusions: when the number of parameters is held constant, deeper RNNs outperform standard RNNs. Additional notable findings include an analysis demonstrating that deeper networks retain more "long-term information" and the observation that standard backpropagation through time (BPTT) can train character-level RNNs to balance parentheses, contrary to prior beliefs that this was only feasible with Hessian-free (HF) optimization.  
However, the paper claims to introduce the deep RNN architecture, which has been previously used—for example, in http://www.cs.toronto.edu/~graves/icassp_2013.pdf, and likely even earlier. As such, the paper should avoid claiming to have introduced this architecture. It is perfectly acceptable to focus on analysis without proposing new architectures.  
The analysis itself is meaningful but would be more compelling if it were extended to multiple problems, such as a speech-related task. Otherwise, the findings risk being specific to the chosen problem and may not generalize.  
Lastly, the HF experiments were likely computationally expensive. Truncated backpropagation through time (introduced in the 1990s) has been shown to be effective, as demonstrated in Mikolov's PhD thesis, where it was used to train RNNs to perform well as language models. It is therefore plausible that truncated BPTT would also perform well in this context, and it would be beneficial to confirm this experimentally.  
This paper provides an interesting analysis of deep RNNs. Its primary contributions are: 1) demonstrating that deep RNNs outperform standard RNNs with a fixed number of parameters on character-level language modeling; 2) showing that deeper layers capture more long-range dependencies; and 3) suggesting that truncated BPTT can train character-level RNNs to balance parentheses. While the results are promising, the paper would be significantly strengthened by extending the analysis to a speech task—for instance, to confirm that deeper RNNs outperform standard RNNs and that their deeper layers capture more long-range information in such a domain.