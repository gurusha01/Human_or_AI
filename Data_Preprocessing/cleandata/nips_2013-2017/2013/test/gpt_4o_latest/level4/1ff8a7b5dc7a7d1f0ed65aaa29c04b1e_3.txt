I appreciate the discussion on how older data might benefit from additional layers of processing compared to newer data. The suggestion to use a deep RNN for improved performance on shorter-term tasks seems reasonable and could offer other advantages as well. Another natural extension would be to include additional layers of nonlinearities between each timestep. Have you explored this possibility?
How critical were the gradient normalization techniques, and how exactly do they "avoid the problem of bifurcations"? There is a relevant paper from ICML 2013 by researchers at the University of Montreal that examines gradient truncation and optimization in RNNs, which might provide useful insights here. Additionally, could normalizing the gradient in this manner interfere with the "averaging" behavior of SGD?
Regarding prior work on Wikipedia compression tasks, you might want to compare your results to those of Mikolov et al., who have conducted relevant studies in this area.
Among the experiments designed to investigate the distinct roles of each layer in terms of time-scale and abstraction, I found the text generation experiment (as presented in Table 2) to be the most compelling. However, as the authors themselves note in the paragraph on line 241, interpreting the effects of such "brain surgery" on RNNs is challenging due to the complex interdependencies that likely exist between the outputs of different layers.
In my view, the most significant gap in the empirical analysis is the question of whether the higher layers are genuinely better at handling abstract and long-term properties of the text compared to if these units were placed in the first layer. In other words, are the higher layers leveraging the additional levels of processing in a meaningful way? While the observation that these layers take on seemingly more abstract and long-term roles after training is promising, it does not provide conclusive evidence.
I also noticed that the standard RNN in these experiments appears to have fewer parameters, as 2767^2  5 > 2119^2 (accounting for both recurrent and inter-layer weight matrices). This discrepancy could make the comparison somewhat unfair. A stronger case would be made if the deeper RNNs outperformed a standard RNN with an equivalent number of parameters or, ideally, the same number of units. For example, comparing a 2-layer DRNN to an RNN with the same number of units would eliminate the parameter advantage and make the comparison more robust. This would significantly bolster the paper's claims.
Additionally, instead of Figure 2, it would have been more informative to see how DRNNs of varying depths performed on the benchmarks when trained from scratch. Including wider layers in shallower models to account for differences in parameter or unit counts would further strengthen the analysis.
---
Minor Comments:
- DRNN-AO and IO should be defined in the main text, not just in Figure 1.  
- This paper introduces a hybrid architecture called DRNNs, which combine deep networks with recurrent connections at each layer operating through time. The authors demonstrate that this architecture performs well for text prediction and compression tasks compared to existing approaches.  
- A significant portion of the paper is dedicated to experiments arguing that higher-level layers capture more abstract and long-term structures in the data. While these experiments are generally convincing, I have some reservations (as detailed above) and would like to see a few additional experiments to address these concerns.  
- I believe this work contributes valuable insights into deep temporal networks and their post-training behavior. The paper is well-written, intellectually honest, and transparent about its potential limitations, which I greatly appreciate.