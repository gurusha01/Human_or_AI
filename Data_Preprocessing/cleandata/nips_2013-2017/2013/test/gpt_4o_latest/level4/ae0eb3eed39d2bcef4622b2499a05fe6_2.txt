Running machine learning algorithms on increasingly larger datasets is becoming essential. A particularly relevant and practical research direction has recently emerged, focusing on exploring different programming paradigms to transform well-known machine learning algorithms into distributed algorithms—allowing them to operate on infrastructures without shared memory and with slow communication between processing units.
This paper introduces a widely recognized pattern, "optimistic concurrency control," into the machine learning domain. As the authors highlight, prior work has explored embarrassingly parallel algorithms, distributed algorithms based on locking paradigms, and coordination-free approaches. Optimistic concurrency control (OCC) is a technique that assumes each processing unit can initially access shared state freely. At specific checkpoints during computation, the algorithm verifies whether this assumption has compromised correctness; if it has, parts of the computation are rolled back or corrected.
The authors present a compelling argument for investigating OCC in the context of machine learning. They apply OCC to three algorithms: DP-means, the Facility Location problem, and BP-means. I found the algorithm's explanation to be clear and easy to follow. A minor point that could be addressed in the appendix is the initialization of the first clusters—specifically, how the algorithm handles the argmin_{\mu \in C} operation when the set C is initially empty.
While reviewing Algorithm 3, I wondered about the cost of transferring x_i between processing units in a distributed setting. In my experience, moving datasets across a cluster tends to be more expensive than transferring parameters. Although I don't have a specific solution, it would be valuable to include a discussion on this potential bottleneck.
The authors also provide correctness proofs for the algorithms. While I only skimmed the proofs in the appendix, they appear accurate. In the evaluation, the authors first demonstrate their approach on a small synthetic dataset, effectively illustrating that the number of corrections required is modest. They then conduct a larger experiment using Spark on Amazon AWS, showcasing strong performance on large datasets. However, it was unclear to me whether the data used in this larger experiment was synthetic or real-world.
line 33: iid => i.i.d. Overall, this is a well-written paper that effectively demonstrates how optimistic concurrency control can be leveraged to implement distributed machine learning algorithms.