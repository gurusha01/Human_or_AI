Review - Paraphrased Version
Summary:  
The paper investigates the computational complexity of policy iteration (PI) for finite Markov Decision Processes (MDPs) with \(n\) states and \(m\) actions per state across various scenarios. The following contributions are presented:  
- For Howard's PI, the authors improve an existing bound by eliminating a \(\log(n)\) factor.  
- For Simplex PI, they enhance a prior bound by reducing its multiplicative constant by half.  
- In the deterministic MDP setting, they refine a previous bound for Simplex PI by reducing the exponent of \(n\)-dependence by one.  
- This result is further extended to stochastic MDPs under additional structural constraints.  
Originality and Significance:  
The complexity of PI is a topic of considerable interest within the reinforcement learning (RL) community. While the finite MDP setting may not have direct practical applications, it is crucial to thoroughly understand this simpler case before tackling more complex scenarios. This paper builds upon a line of research initiated by Yinyu Ye in 2011 and subsequently advanced by Post and Ye, as well as Hansen, Miltersen, and Zwick. Although some of the improvements presented are incremental, the proofs are significantly more streamlined and accessible to RL researchers compared to Ye's original work, which heavily relied on the terminology of linear programming. The most notable contribution is the extension of strong polynomial results to stochastic MDPs under specific structural assumptions, which constitutes the paper's most innovative aspect.  
Clarity:  
The paper is generally well-written, though the authors should carefully proofread it to address a few typographical errors (e.g., lines 285, 292). Additionally, certain phrases could be rephrased for improved fluency (e.g., "However, on the other hand" on line 303 is awkward).  
Soundness:  
The results appear to be sound, although I did not have the opportunity to verify all the details thoroughly.  
Minor Issues:  
- Line 367: Please specify the lemma being referenced.  
This paper provides modest improvements to prior results on the time complexity of PI while presenting proofs that are more accessible to RL researchers. More importantly, it demonstrates that the class of MDPs for which Simplex PI is strongly polynomial is significantly broader than previously known.