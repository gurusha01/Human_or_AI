This paper addresses the challenge of large-scale unsupervised learning and introduces the paradigm of "optimistic concurrency control" (OOC). This paradigm operates under the assumption that conflicts are rare, and when they do occur, a conflict-resolution protocol is triggered. The proposed approach is applied to three clustering-related problems: clustering, feature learning, and online facility location. The authors also establish the serializability of the proposed algorithms and present experimental results based on simulated datasets.
The paper is well-structured and clearly articulated, offering sufficient technical details and theoretical justification. The concept of OOC is both innovative and practical. While the paper frames the idea within the context of concurrency control, the broader issue in large-scale machine learning is how to effectively update global parameters using local parameters from parallelized components. The proposed method provides a straightforward and practical mechanism for updating global parameters (global cluster assignments) based on local ones (local cluster assignments). However, this update strategy may not generalize to other unsupervised or supervised learning algorithms, as it is inherently algorithm-dependent. 
The primary concern with this work lies in its experimental evaluation. The experiments fail to convincingly demonstrate the practical utility of the proposed algorithms. Firstly, the evaluation is limited to simulated datasets, which do not offer insights into the algorithm's applicability to real-world data. Secondly, the paper lacks a comparison of clustering quality with other distributed clustering algorithms from the existing literature. 
Overall, the paper presents an elegant approach to distributed unsupervised learning (clustering) with clear exposition. However, the experimental results are insufficient to substantiate the practical effectiveness of the proposed algorithms.