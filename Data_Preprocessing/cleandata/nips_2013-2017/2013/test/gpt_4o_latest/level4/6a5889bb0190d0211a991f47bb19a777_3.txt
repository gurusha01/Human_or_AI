This paper establishes the first regret bounds for a reinforcement learning algorithm that operates by 1) maintaining a posterior distribution over the MDP, 2) sampling a single MDP from this posterior, and 3) executing the policy optimal for the sampled MDP. While this type of "Posterior-Sampling Reinforcement Learning" (PSRL) algorithm has been previously proposed, no regret bounds had been derived for it until now. In contrast, all existing algorithms with regret bounds rely on the principle of optimism in the face of uncertainty. This work represents the first regret analysis of an algorithm that does not adhere to this principle. Although the regret bound derived here is not as tight as those for optimism-based algorithms, this approach demonstrates significantly greater exploitative behavior in typical practical scenarios and appears to be much more computationally efficient overall.
If it is indeed correct that this is the first regret bound for an algorithm not based on optimism, and if the result itself is valid, then the paper merits acceptance.
The clarity of the writing is commendable. However, I did not fully follow the proof in detail, so there is a possibility that I may have overlooked errors.
For simplicity, the paper assumes an episodic reinforcement learning setting with fixed-length episodes. While this assumption is somewhat unrealistic, it is acceptable for a foundational result that introduces a novel direction. Furthermore, the limitations and other practical shortcomings of the algorithm could likely be addressed in future work, assuming the main result holds. The paper briefly touches on one related extension in a later section, which is a useful inclusion but could be omitted if space constraints necessitate addressing the issue raised in the following paragraph.
The paper asserts that the regret bound for its algorithm is close to the state of the art but does not explicitly define what the state of the art is. This omission should be addressed to clarify the claim and enable readers to assess whether the new algorithm's bound is indeed comparable. Additionally, the paper should elaborate on the specific sense in which the new bound is considered "close."
This work provides the first regret bound and promising empirical results for a reinforcement learning algorithm grounded in posterior sampling rather than optimism in the face of uncertainty.