Review - Title: (More) Efficient Reinforcement Learning via Posterior Sampling  
Summary: The paper introduces a Thompson sampling-inspired method for episodic finite-horizon reinforcement learning, termed posterior sampling for reinforcement learning (PSRL). The authors provide a regret analysis demonstrating state-of-the-art performance, along with simulations that highlight superior empirical performance on certain toy problems compared to other algorithms with comparable regret bounds.  
Comments:  
The paper is well-structured, clearly written, and easy to follow. The contribution is explicitly outlined: demonstrating the feasibility of employing a Thompson sampling-based approach in reinforcement learning and providing a theoretical analysis to show its efficiency.  
Regarding the regret bound, the authors claim that the regret bound of PSRL is close to state-of-the-art results. It would be helpful to explicitly mention these state-of-the-art values within the paper for better context and comparison.  
A common question in the domain of multi-armed bandit algorithms is the practical magnitude of the constant hidden in the O() notation. It would be valuable to discuss this aspect in the paper.  
Additionally, comparisons could be drawn with the KL-UCRL algorithm (Optimism in reinforcement learning and Kullback-Leibler divergence - S. Filippi, O. Capp√©, A. Garivier). KL-UCRL, which is based on the optimism principle, has been shown to outperform UCRL2 empirically on the riverSwim environment and achieves a regret of $\tilde{O}(|S|\sqrt{|A|T})$. Including such comparisons would strengthen the paper.  
Minor comments:  
- Line 99: It appears that $s1$ should be replaced with $si$.  
- Line 192: "we how we can" should be corrected to "how we can".  
- Line 434: "markov" should be capitalized to "Markov".  
- The references should be formatted consistently (e.g., Firstname Lastname vs. F. Lastname).  
Overall, this is a technically sound, well-written, and engaging paper.