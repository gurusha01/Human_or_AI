The paper introduces a Bayesian inference framework within Monte Carlo Tree Search (MCTS) that employs a Thompson sampling-based action-selection strategy, referred to as the Dirichlet-NormalGamma MCTS (DNG-MCTS) algorithm. The proposed approach models the accumulated reward of following the current policy from a given state, \( X_{s,\pi(s)} \), using a normal distribution with a NormalGamma prior. State transition probabilities are estimated via Dirichlet distributions. The action-selection mechanism leverages Thompson sampling, where the expected cumulative reward for each action is computed using a parametric distribution with parameters sampled from the posterior distributions, and the action with the highest expected reward is chosen. The authors validate their method on several benchmark tasks, demonstrating that it converges (slightly) faster than the UCT algorithm. Theoretical convergence properties are also discussed.
While there is extensive research on MCTS, I am not aware of prior work that adopts the specific approach proposed in this paper. However, approximating the cumulative reward with a NormalGamma prior is not novel and was previously introduced by Dearden et al. (1998) [a]. Similarly, modeling state transitions with a Dirichlet prior is well-established. Therefore, the originality of the modeling and inference methods described in Section 3.2 is limited. Nevertheless, the integration of this modeling framework with a Thompson sampling-based action-selection strategy appears to be novel and could be of interest to researchers in related fields.
The experimental results presented in the paper are somewhat weak. Despite the abundance of prior work on MCTS, the baseline comparisons are restricted to the classic UCT algorithm (with the exception of the CTP problem). More comprehensive experiments, including evaluations of computational cost and sample complexity, as well as comparisons with advanced methods such as those in [b, d, e], would strengthen the paper.
The majority of the paper is clearly written, but the assumptions in Section 3.1 are not entirely clear and may contain inconsistencies. Specifically, while the cumulative reward \( X{s,a} \) is modeled as a mixture of normal distributions, the authors argue that assuming \( X{s,\pi(s)} \) follows a normal distribution is realistic. However, \( X{s,\pi(s)} \) should theoretically be a mixture of normal distributions, with the number of components growing exponentially from leaf states to the root. As such, the assumption of \( X{s,\pi(s)} \) being normally distributed may not be realistic. That said, this could be considered a practical approximation for modeling the distribution of \( X_{s,\pi(s)} \). Additionally, the relationship between cumulative rewards at successive time steps may be related to the distributional Bellman equation discussed in [c]. I recommend revising this section carefully in the final version.
Regarding lines 137â€“139, the notation \( N(n\mu, \delta^2/n) \) appears to be incorrect. Should it instead be \( N(n\mu, n\delta^2) \)? Additionally, does \( \sum_n f \) diverge as \( n \to \infty \)?
[a] Richard Dearden, Nir Friedman, Stuart J. Russell: Bayesian Q-Learning. AAAI/IAAI 1998.  
[b] Gerald Tesauro, V. T. Rajan, Richard Segal: Bayesian Inference in Monte-Carlo Tree Search. UAI 2010.  
[c] Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, Toshiyuki Tanaka: Parametric Return Density Estimation for Reinforcement Learning. UAI 2010.  
[d] John Asmuth and Michael L. Littman: Approaching Bayes-optimality using Monte-Carlo Tree Search. In Proc. 21st Int. Conf. Automat. Plan. Sched., 2011.  
[e] Arthur Guez, David Silver, Peter Dayan: Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. NIPS 2012.  
In summary, the paper presents a Bayesian MCTS framework with a Thompson sampling-based action-selection strategy and demonstrates its ability to converge faster than the UCT algorithm on benchmark tasks.