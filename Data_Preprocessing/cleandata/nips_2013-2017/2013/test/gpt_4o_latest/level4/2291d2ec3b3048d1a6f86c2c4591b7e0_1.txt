The objective of this study is to automatically identify latent domains within a training dataset, which are subsequently utilized within a domain adaptation framework to enhance classification performance on a test dataset. The authors propose a function that quantifies the difference between two feature vectors using a specified kernel. The aim is to partition the data points into domains such that the function is maximized across all pairs of domains. The problem is formulated as an integer programming task with two constraints: each data point is assigned to exactly one domain, and the class label distribution within each domain must align with the overall label distribution of the entire dataset. To make the problem tractable, it is relaxed into a continuous optimization problem involving a quadratic cost function with linear constraints. The number of domains is determined through cross-validation.
The proposed method is evaluated on two datasets: static images from [2] and the IXMAS multi-view action dataset from [15]. A key highlight is that the approach demonstrates improved performance compared to the domain adaptation method of [19].
Strengths: The paper is well-written, and to the best of my knowledge, the approach is novel (though I do not consider myself an expert in domain adaptation). The observed performance improvements over [19] are noteworthy.
Weaknesses: At this stage, I am slightly inclined to recommend rejection. However, I have two primary concerns that I would like to see addressed in the rebuttal, which could potentially change my decision:
(i) The motivation for this work is unclear. On line 77, the paper claims that "simply clustering images by their appearance is prone to reshaping datasets into per-category domains." First, what evidence supports this claim? Second, how does the model formulation in Section 2 address this issue? Specifically, how does the approach ensure that the data is not reshaped into per-category domains?
(ii) Relatedly, on line 154, what is the rationale for the second constraint ("label prior constraint")? I am curious about the performance of the model without this constraint. Additionally, the paper should include a baseline where the data is partitioned using k-means clustering or unsupervised object discovery methods (e.g., Sivic et al., ICCV '05) based on appearance vectors. Furthermore, what is the performance when the dataset is randomly partitioned into equal subsets?
Additional comments:
+ Line 82: The phrase "maximally different in distribution from each other" is used throughout the paper. It would be helpful to clarify what this refers to. Distribution over what?
+ Line 166: Does the formulation of \(\beta_{mk}\) not already imply that it resides on a simplex?
+ Line 182: The constraints mentioned here appear to differ from those introduced earlier (starting on line 153). Could this be clarified?
+ Line 215: What is the justification or proof for the bound presented here?
+ Line 289: Please provide more details regarding the use of the geodesic flow kernel [4]. Was this reimplemented, or was publicly available source code utilized?
+ Lines 313/340: It would be valuable to include insights into the observed performance differences. Understanding why the proposed approach outperforms the baselines is important. Additionally, showcasing systematic failures of the baselines that the proposed method successfully addresses would strengthen the paper.
+ Eq (1): The notation \(M'k\) should be corrected to \(Mk'\).
+ Consider including the following citation:  
  "Unbiased Look at Dataset Bias," A. Torralba and A. Efros, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.
The rebuttal satisfactorily addressed my concerns regarding the paper's motivation and the necessity of the label prior constraint. I am now slightly inclined to recommend acceptance.