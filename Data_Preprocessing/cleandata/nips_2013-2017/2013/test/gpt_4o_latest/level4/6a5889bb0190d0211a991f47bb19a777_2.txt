Paraphrased Review:
Summary: This paper conducts a regret analysis for episodic reinforcement learning inspired by posterior sampling (commonly referred to as Thompson sampling). The proposed approach involves sampling an MDP model from the posterior at the start of each episode, executing the optimal policy for this sampled MDP during the episode, updating the posterior model, and repeating the process. While this algorithm, known as PSRL, has demonstrated strong empirical performance in the past, it has largely been treated as a heuristic without rigorous theoretical guarantees. This work provides the first regret analysis for PSRL, establishing a bound of \(O(\tau \cdot S \cdot \sqrt{A \cdot T})\), where \(H\) is the episode length and \(T\) is the total number of steps. Numerical experiments on simple MDPs reveal that PSRL significantly outperforms UCRL2, an alternative algorithm with a comparable regret bound.
Quality: The paper makes a valuable theoretical contribution to reinforcement learning in finite-state, finite-action, episodic MDPs. By leveraging recent advancements in the multi-armed bandit literature, it represents a meaningful step toward understanding the finite-sample performance of Bayesian-style reinforcement learning algorithms.
Clarity: The paper is very well-written. Despite the technical complexity, the authors effectively explain the core ideas in a clear and intuitive manner.
Originality: While the PSRL algorithm itself is not novel, the paper's contribution lies in its first regret analysis for the algorithm. The cornerstone of the analysis is Lemma 2, which equates the expected regret to the regret in the sampled MDP (under the assumption that the MDP is drawn from the posterior). A similar observation has been employed in prior work to establish expected regret bounds for posterior sampling in multi-armed bandits [15]. Although much of the remaining analysis follows standard reinforcement learning techniques, the application of Lemma 2 in this context, particularly for expected regret, is sufficiently novel.
Significance: Posterior sampling has been highly successful in the multi-armed bandit setting, sparking significant interest in its theoretical properties. This paper is the first to extend these analytical ideas to MDPs, demonstrating strong regret bounds for posterior sampling. Beyond its immediate results, the paper has the potential to inspire further exploration in the reinforcement learning community into a new class of algorithms based on posterior sampling.
Detailed Comments:
- While the analysis and regret bounds are compelling, the paper should explicitly clarify that the nature of these bounds differs fundamentally from those in the existing literature. Specifically, prior bounds are worst-case guarantees, whereas the bounds in this paper are average-case (averaged over the prior/posterior). This distinction is critical, as the averaging is what enables Lemma 2. Consequently, the paper's expected bounds may be weaker than worst-case bounds. The authors are encouraged to address these points in the paper.
- The paper compares PSRL solely against UCRL2. The choice of UCRL2 as the baseline requires justification. As described, PSRL is designed for episodic tasks and aims to optimize undiscounted, finite-horizon total rewards, whereas UCRL2 is tailored for continuing tasks and focuses on optimizing average rewards. This fundamental difference means that their regret bounds are not directly comparable: UCRL2's bound depends on a diameter parameter, while PSRL's bound involves the horizon length. While there may be a way to relate these bounds, it is not immediately clear how. 
- Instead of UCRL2, another relevant work to consider is:
  Claude-Nicolas Fiechter: Efficient Reinforcement Learning. COLT 1994: 88-97.  
  Although Fiechter's work is in the PAC framework, it addresses finite-horizon problems and thus seems more closely aligned with the present study.
- The simple experiments demonstrate that PSRL significantly outperforms UCRL2 empirically. However, this result is unsurprising given UCRL2's inherently conservative nature. Furthermore, due to the differences in the types of problems the two algorithms are designed to address (as noted above), the empirical comparison provides limited insight.
- Line 255: "depends only the" â†’ "depends only on the"
- Line 278: Please specify the exact meaning of "high probability."
- Line 411: Is it overly restrictive to assume that the optimal average reward is uncorrelated with the episodic length?
Conclusion: This paper offers an insightful analysis of an important algorithm, although the regret bounds are not directly comparable to prior results. The work is primarily theoretical, with limited experimental validation on simple MDPs.