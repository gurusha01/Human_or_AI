The authors propose a Monte-Carlo tree search (MCTS) algorithm tailored for online planning in Markov Decision Processes (MDPs) with unknown transition probabilities. They incorporate Thompson sampling—a well-established randomized Bayesian algorithm for minimizing regret in multi-armed stochastic bandit problems—for action selection during tree expansion. To enable the use of Thompson sampling, the authors model the return \( X{s,\pi(s)} \) as a normal distribution and \( X{s,a} \) as a mixture of normal distributions, allowing actions to be chosen based on the posterior probability of being optimal.
I find the idea of leveraging Thompson sampling in MCTS compelling, as recent empirical studies suggest that Thompson sampling outperforms the widely used UCB algorithm in certain contexts. The primary challenge in applying Thompson sampling to MDPs lies in modeling the probability of returns within a Bayesian framework, which this paper addresses. However, I have several concerns and questions:
1. While modeling \( X_{s,\pi(s)} \) as a normal distribution seems reasonable for leaf nodes of the search tree where \( \pi \) is the roll-out policy, this assumption appears problematic for nodes higher up in the tree. The policy changes at these levels, which may invalidate the authors' claim.  
2. The authors state in lines 263-264 that "Thompson sampling converges to find the optimal action with probability 1 by treating each decision node in the tree as a multi-armed bandit (MAB)." I find this claim unconvincing. For decision nodes other than leaf nodes, the MAB problems appear non-stationary due to policy changes as \( X_{s,\pi(s)} \) evolves. To my knowledge, there is no theoretical guarantee of convergence for Thompson sampling in non-stationary MAB problems. Consequently, I am uncertain whether the policy derived by DNG-MCTS converges to the optimal one.  
3. In the experiments section (lines 315-316, 354-355, 368), does the term "iteration" refer to the iteration of DNG-MCTS within the "OnlinePlanning" function in Figure 1? Clarification would be helpful.  
4. In the final paragraph of the experiments section, the authors note that DNG-MCTS requires more computation time than UCT. It would be beneficial to include the CPU running time for a more comprehensive comparison.
Minor comments: The references contain some inaccuracies. For example, papers [7, 8, 15, 18] have been published in conferences rather than remaining on arXiv.
Quality: While the paper introduces an interesting idea, it has some notable flaws.  
Clarity: The paper is well-written and easy to follow.  
Originality: To the best of my knowledge, the paper presents original work.  
Significance: The main idea is of interest to the reinforcement learning community, but the identified flaws should be addressed for stronger impact.
---
The original central limit theorem (CLT) for Markov chains is expressed as \( \sqrt{n}(n^{-1} \sum{t=0}^n f(st) - E[f(st)]) \rightarrow N(0, \sigma^2) \) as \( n \rightarrow \infty \), indicating that the empirical mean of \( f(st) \) converges to its true expectation. In Section 3.1, the authors modify the CLT to show that \( \frac{1}{\sqrt{n}}(\sum{t=0}^n f(st) - n\mu) \rightarrow N(0, \sigma^2) \). However, as Reviewer 5 has already pointed out, the sum of \( f(s_t) \) follows \( N(n\mu, n\sigma^2) \), and this sum diverges as \( n \) approaches infinity. Thus, while the normality assumption on the sum of rewards may be practically reasonable, I find it difficult to accept that the CLT justifies this assumption.
In summary, the authors present a novel MCTS algorithm leveraging Thompson sampling, which is of interest to the RL community. However, the theoretical soundness of the approach requires further clarification.