Overall, I found the paper to be well-written.
The claim in the abstract (and later reiterated in the Experiments section) that the model achieves 88% accuracy when 80% of the labels are flipped appears to be incorrect. In reality, only 40% of the labels are being flipped, as the probability of a label flip is determined by a convex combination of \(\rho+\) and \(\rho-\), rather than their sum. Regardless, the primary objective should be to achieve better accuracy as the noise rate approaches 0.5, not 1.0 (where the problem becomes trivial, as all labels are simply flipped).
It might be worth considering a comparison of your methods with Kearns' Statistical Query (SQ) framework. Since minimizing a convex loss can be achieved using gradient-based methods (which have statistical analogues), this would inherently provide tolerance to random classification noise. While Kearns' framework does not directly account for class-conditional noise, I believe this aspect could be addressed relatively easily. However, I am uncertain about the specific excess risk bounds that could be derived through such SQ-based simulations.
Minor comments:  
--------------  
1. The term "PU learning" is used without ever being explicitly defined.  
2. The phrase "so-called" is used frequently, particularly in reference to zero-one loss. Why is it "so-called"? If you take issue with the term, it would be helpful to explain why.  
---  
Update: Regarding the use of the SQ model, even when employing a surrogate loss, your optimization problem can be solved using an algorithm that relies solely on statistical queries rather than direct access to data points.  
This paper addresses the problem of learning in the presence of random classification noise. Unlike PAC-like models, the primary focus here is on minimizing a convex loss function (with respect to the true model), which can often be achieved through suitable modifications even in the presence of noisy labels. The paper also includes experimental evaluations of the proposed methods alongside comparisons with related techniques.