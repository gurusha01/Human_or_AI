This paper introduces a convex framework for partitioning datasets into K subsets, ensuring that each subset exhibits similar semantic class distributions while being as distinct as possible from the others. The distinction, referred to as maximum distinctiveness, is achieved by maximizing the pairwise mean differences of subsets in the RKHS induced by a chosen kernel. These subsets are termed latent domains. The determination of K (the number of latent domains) is accomplished by maximizing the average learnability, which reflects how effectively a classifier can be trained within each latent domain.
While enforcing similar class distributions within each domain prevents clusters from being dominated by a single class, this assumption could be restrictive for latent domain discovery in certain scenarios. For example, if latent domains correspond to poses in an animal classification task (e.g., horses and cows), the class distributions within each latent domain may not necessarily align. Specifically, both horses and cows may appear in a left-right standing pose, but horses are rarely depicted in a sitting pose, unlike cows.
Although latent domain discovery is not a novel concept, the proposed approach of regulating class label distributions to enhance latent domain identification within a (relaxed) convex framework is innovative. Additionally, determining the number of latent domains by evaluating classification quality within each domain is a noteworthy practice. This ensures that each latent domain contains a sufficient number of samples per class to facilitate better generalization and discrimination.
However, the paper suffers from ambiguity in its use of key concepts such as dataset, domain, and latent domain, which can lead to confusion. These terms should be explicitly defined, ideally with illustrative examples. This issue is particularly evident in Section 4.2, where the experimental setup requires clarification. It appears that the terms dataset and domain are used interchangeably, as Si is referred to both as datasets and source domains. This inconsistency, along with the lack of definition for maxk r(U_k, B) in Equation 7, should be addressed to improve clarity.
The experimental validation is generally adequate, with results showing reasonable improvements over the baselines. However, the rationale behind the selection of specific source and target datasets for evaluation is not well explained. For instance, a leave-one-dataset-out adaptation strategy might provide a more comprehensive assessment. The qualitative results are valuable for visualizing the algorithm's outputs.
Finally, the statement on line 074, which frames the problem of learning latent domains as an unsupervised learning task, could be misleading since the method relies on semantic class labels. Despite these issues, the formulation of latent domain discovery by encouraging similar class distributions within a (relaxed) convex framework represents a significant technical contribution. Nevertheless, the paper would benefit from clearer concept definitions and improved experimental validation.