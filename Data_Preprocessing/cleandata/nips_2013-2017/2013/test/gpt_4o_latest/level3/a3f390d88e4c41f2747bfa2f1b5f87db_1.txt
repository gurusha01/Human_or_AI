Review of the Paper
This paper investigates human performance in 1D function optimization tasks and compares it to 24 well-known optimization algorithms. The authors conclude that (a) humans outperform these algorithms in terms of efficiency, and (b) human behavior aligns with Bayesian inference principles, specifically Gaussian processes (GP), which account for function smoothness. The study further explores human function learning and active search strategies through a series of controlled experiments. The results suggest that Gaussian processes provide a unifying theoretical framework for explaining human behavior in optimization, interpolation, and extrapolation tasks.
Strengths:
1. Novelty and Scope: The paper addresses an underexplored area by systematically comparing human optimization strategies with machine algorithms. The use of Gaussian processes as a theoretical framework to model human behavior is both novel and significant.
2. Comprehensive Experimental Design: The authors conduct seven experiments with diverse tasks (optimization, interpolation, extrapolation) and a large number of participants, providing robust empirical evidence for their claims.
3. Theoretical Contribution: The paper extends previous work on Gaussian processes by demonstrating their applicability to active learning and search, not just passive function learning.
4. Potential Impact: The findings could inspire the development of more efficient optimization algorithms by incorporating human-like strategies, such as leveraging smoothness assumptions and balancing exploration and exploitation.
Weaknesses:
1. Concern with Point (a): The claim that humans outperform optimization algorithms may not fully account for implicit cues humans might use, such as the y-axis range or visual patterns. While the authors attempt to control for this, the possibility of humans leveraging additional information remains a concern.
2. Comparison Measures: The array of measures used to compare humans and algorithms is overly complex and lacks clarity. A simpler diagnostic approach, such as comparing the K+1th point chosen by humans and algorithms after observing the first K points, would make the results more interpretable.
3. Figure 4 Critique: Figure 4 is noisy and fails to provide clear diagnostic insights. The importance of certain measures, such as agreement on search step size distributions, is questionable.
4. Human Consistency: The paper does not rigorously assess across-subject agreement. Methods like split-half correlations or comparing distributions of clicks between subject groups could strengthen the analysis.
5. Experiment 6 Assumption: The assumption that the first click should target the maximum variance point is debatable. A decision-theoretic analysis is needed to justify this strategy.
6. Experiment Redundancy: The paper includes a large number of experiments, some of which feel redundant. Focusing on fewer experiments with more thoughtful measures of model fit and human consistency would improve the clarity and impact of the work.
Suggestions for Improvement:
1. Simplify the comparison metrics to make the results more interpretable and diagnostic.
2. Provide a more rigorous analysis of across-subject consistency.
3. Reduce the number of experiments and focus on key insights, particularly those that directly support the Gaussian process hypothesis.
4. Justify assumptions in Experiment 6 with a decision-theoretic framework.
5. Improve the clarity of visualizations, especially Figure 4, to make the results more accessible.
Arguments for Acceptance:
- The paper addresses a novel and significant problem, providing insights into human optimization strategies and their alignment with Gaussian processes.
- The experimental design is comprehensive, and the findings have the potential to inspire new algorithmic developments.
Arguments Against Acceptance:
- The claim that humans outperform algorithms may not fully account for implicit cues.
- The complexity of comparison measures and redundancy in experiments detracts from the paper's clarity and focus.
Recommendation:
This paper is a valuable contribution to the field, but it requires revisions to address the concerns outlined above. I recommend acceptance with major revisions, focusing on simplifying the analysis, improving clarity, and addressing the identified methodological issues.