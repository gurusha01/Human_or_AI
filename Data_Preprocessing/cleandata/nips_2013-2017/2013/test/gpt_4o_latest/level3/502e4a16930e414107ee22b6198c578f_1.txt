Review of the Paper
This paper presents significant advancements in the theoretical understanding of Policy Iteration (PI) algorithms for solving Markov Decision Processes (MDPs). The authors focus on two variations of PI: Howard's PI and Simplex-PI, and provide improved complexity bounds for both algorithms. The key contributions include removing a log(n) factor from the complexity bound of Howard's PI, halving the multiplicative constant for Simplex-PI, and reducing the power of dependence on n for deterministic MDPs. Additionally, the results are extended to stochastic MDPs under structural assumptions, which is a novel and impactful contribution.
Strengths:
1. Howard's PI Improvement: The removal of a log(n) factor from the complexity bound of Howard's PI is a notable theoretical improvement, streamlining prior results and advancing the state of the art.
2. Simplex-PI Improvement: The halving of the multiplicative constant for Simplex-PI and the reduction in dependence on n for deterministic MDPs are meaningful contributions that enhance the understanding of the algorithm's efficiency.
3. Extension to Stochastic MDPs: The extension of strong polynomial results to stochastic MDPs under structural assumptions is the most novel aspect of the paper. This broadens the applicability of the results and addresses a challenging open problem in the field.
4. Clarity of Proofs: The authors provide more accessible proofs compared to prior work (e.g., Ye 2011), making the results more approachable for reinforcement learning researchers.
5. Technical Soundness: The results appear sound, and the contraction properties and structural assumptions are well-motivated. The authors also provide detailed proofs in the appendix and supplementary material.
Weaknesses:
1. Clarity Issues: While the paper is generally well-written, it contains minor typos and occasional awkward phrasing. Specific lemma references could be clarified for better readability.
2. Howard's PI for Stochastic MDPs: The analysis of Howard's PI for stochastic MDPs remains incomplete. The authors acknowledge this limitation and suggest it as future work, but it leaves a gap in the paper's scope.
3. Assumptions for Stochastic MDPs: The structural assumptions required for the stochastic MDP results may limit their generality. Further discussion on the practical implications of these assumptions would strengthen the paper.
4. Experimental Validation: The paper is purely theoretical, and while the results are compelling, experimental validation on real-world MDPs or benchmarks would enhance its impact.
Arguments for Acceptance:
- The paper makes substantial theoretical contributions, particularly the extension to stochastic MDPs.
- The improvements to complexity bounds for both Howard's PI and Simplex-PI are significant and advance the state of the art.
- The streamlined proofs make the results more accessible to a broader audience.
Arguments Against Acceptance:
- The incomplete analysis of Howard's PI for stochastic MDPs leaves an important question unanswered.
- The reliance on structural assumptions for stochastic MDPs may limit the general applicability of the results.
- The lack of experimental validation reduces the practical impact of the work.
Conclusion:
This paper provides valuable theoretical insights into the complexity of PI algorithms and extends the analysis to stochastic MDPs, which is a noteworthy contribution. While there are some limitations, particularly in the analysis of Howard's PI for stochastic MDPs, the strengths of the paper outweigh its weaknesses. I recommend acceptance, with minor revisions to address clarity issues and improve the discussion of structural assumptions.