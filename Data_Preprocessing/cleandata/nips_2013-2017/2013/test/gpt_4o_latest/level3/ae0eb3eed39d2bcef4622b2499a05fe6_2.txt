The paper presents a novel approach to distributed machine learning by introducing Optimistic Concurrency Control (OCC), a paradigm that balances correctness and parallelism. Unlike traditional mutual exclusion or coordination-free methods, OCC assumes free access to shared states and resolves conflicts through validation and correction. This approach is applied to three algorithms: DP-means, Online Facility Location (OFL), and BP-means, with theoretical correctness proofs and experimental validation.
Strengths:
1. Novelty and Relevance: The application of OCC to distributed machine learning is innovative, bridging the gap between strict concurrency and coordination-free paradigms. The paper addresses a critical challenge in distributed ML—balancing scalability with correctness—making it a significant contribution to the field.
2. Clarity and Accessibility: The paper is well-written and provides clear explanations of the algorithms and their OCC adaptations. The pseudocode for DP-means and OFL is particularly helpful for reproducibility.
3. Theoretical Rigor: The correctness proofs for serializability and approximation guarantees (e.g., for OFL) are sound and extend the theoretical properties of serial algorithms to their distributed counterparts.
4. Experimental Validation: The experiments on both synthetic and large-scale datasets using Spark on AWS demonstrate the practical scalability of the proposed methods. The results are convincing, particularly the near-perfect scaling for DP-means and BP-means.
5. Broader Impact: The OCC framework has potential applications beyond the algorithms discussed, offering a generalizable approach to distributed ML.
Weaknesses:
1. Data Movement Costs: While the paper acknowledges that moving data (e.g., \(x_i\)) between processing units can be costly, it does not provide a detailed analysis of how this overhead compares to parameter movement. This could be a significant limitation in real-world distributed systems.
2. Initial Cluster Assignment: A minor clarification on how initial cluster assignments are handled would improve the paper's completeness. Adding this to the appendix would be sufficient.
3. Data Source Ambiguity: The source of the large datasets used in the AWS experiments is unclear. Providing this information would enhance the reproducibility and transparency of the results.
4. Typographical Error: The term "iid" should be corrected to "i.i.d." for consistency and accuracy.
Pro and Con Arguments:
Pro: The paper introduces a novel and practical framework for distributed ML, with strong theoretical guarantees and demonstrated scalability. It is well-written and addresses a critical problem in the field.  
Con: The analysis of data movement costs and the lack of clarity on dataset sources slightly detract from the paper's completeness.
Recommendation:
This paper is a strong candidate for acceptance. It makes a meaningful contribution to distributed machine learning by introducing OCC, a novel paradigm that balances correctness and scalability. While minor clarifications and additional analysis would enhance the work, the overall quality and significance of the contribution justify its inclusion in the conference.