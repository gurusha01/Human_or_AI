The paper presents a significant theoretical contribution to reinforcement learning (RL) by providing the first regret analysis for Posterior Sampling for Reinforcement Learning (PSRL) in episodic Markov Decision Processes (MDPs). The authors establish a regret bound of \(O(\tau \cdot S \cdot \sqrt{A \cdot T})\), which is close to the state-of-the-art for RL algorithms, and demonstrate PSRL's empirical superiority over UCRL2 in simple MDPs. This work extends the theoretical insights of posterior sampling from multi-armed bandits to MDPs, offering a compelling alternative to optimism-based exploration strategies.
Strengths:
1. Theoretical Contribution: The regret analysis of PSRL is novel and bridges a critical gap in the literature. The use of Lemma 2 to connect expected regret with sampled MDP regret is particularly elegant and insightful.
2. Clarity: Despite the technical complexity, the paper is well-written and provides intuitive explanations of key ideas. The theoretical results are clearly stated and rigorously derived.
3. Significance: By extending posterior sampling to episodic MDPs, the paper opens new avenues for Bayesian RL research. The simplicity and computational efficiency of PSRL make it a promising alternative to optimism-based algorithms.
4. Empirical Validation: The simulations demonstrate that PSRL outperforms UCRL2 in specific MDPs, highlighting its practical utility.
Weaknesses:
1. Average-Case Bounds: The regret bounds are posterior-averaged (Bayesian) rather than worst-case, which is less common in RL literature. This distinction should be emphasized more clearly in the text.
2. Comparison with UCRL2: While the empirical comparison with UCRL2 is interesting, the two algorithms have fundamentally different objectives and regret guarantees. This limits the informativeness of the comparison, and additional justification for this choice is needed.
3. Limited Empirical Scope: The empirical evaluation is restricted to simple MDPs like RiverSwim and randomly generated environments. Broader benchmarks would strengthen the practical claims.
4. Related Work: The paper could benefit from referencing Fiechter's work on finite-horizon problems to provide additional context. This omission weakens the positioning of the contribution within the broader RL literature.
Minor Issues:
- Line 255: The phrase "depends only on the" is incomplete.
- Line 278: The term "high probability" should be clarified.
- Line 411: The assumption regarding the correlation between optimal average reward and episodic length needs to be addressed explicitly.
Recommendation:
This paper makes a strong theoretical contribution to the field of reinforcement learning by providing regret guarantees for PSRL and demonstrating its empirical advantages. However, its empirical validation is somewhat limited, and the comparison with UCRL2 could be better justified. Despite these limitations, the novelty and significance of the theoretical results make this paper a valuable addition to the conference. I recommend acceptance, provided the authors address the noted weaknesses, particularly clarifying the scope of the regret bounds and expanding the empirical evaluation.
Arguments for Acceptance:
- Novel and rigorous regret analysis for PSRL.
- Clear and well-organized presentation of complex ideas.
- Potential to inspire further research in Bayesian RL.
Arguments Against Acceptance:
- Limited empirical evaluation and scope.
- Comparison with UCRL2 lacks sufficient justification.
- Average-case bounds may not align with standard RL benchmarks.