The paper introduces a novel approach to episodic finite-horizon reinforcement learning called Posterior Sampling for Reinforcement Learning (PSRL), which leverages Thompson sampling for efficient exploration. Unlike traditional methods that rely on optimism in the face of uncertainty, PSRL samples a Markov Decision Process (MDP) from a posterior distribution at the start of each episode and follows the optimal policy for the sampled MDP. The paper provides a regret analysis, demonstrating an \( \tilde{O}(\tau S \sqrt{AT}) \) bound, which is close to state-of-the-art, and presents empirical results showing superior performance over existing algorithms like UCRL2 on toy problems.
Strengths:
1. Technical Contribution: The paper makes a significant contribution by extending the use of posterior sampling to reinforcement learning, providing one of the first regret bounds for an algorithm not based on optimism. The regret bound is theoretically sound and competitive with state-of-the-art methods.
2. Clarity and Writing: The paper is well-written, clearly presenting the problem, methodology, and theoretical results. The inclusion of simulation results further strengthens the paper by demonstrating PSRL's practical advantages.
3. Novelty: The use of posterior sampling in reinforcement learning is a fresh perspective, and the paper effectively highlights its computational simplicity and statistical efficiency compared to optimistic algorithms.
4. Empirical Validation: The empirical results convincingly show that PSRL outperforms UCRL2 in both episodic and non-episodic settings, particularly in challenging environments like RiverSwim.
Weaknesses:
1. Regret Bound Comparison: While the paper claims near state-of-the-art regret bounds, it does not explicitly compare these bounds to the exact values of state-of-the-art algorithms like UCRL2 or REGAL. Including such comparisons would provide better context for the theoretical contribution.
2. Constant in Regret Bound: The paper does not address practical concerns regarding the constant hidden in the \( O(\cdot) \) notation of the regret bound. This could impact the real-world applicability of the algorithm.
3. Empirical Comparisons: The paper lacks a direct comparison with the KL-UCRL algorithm, which has shown better empirical performance in some environments. Including this comparison would strengthen the empirical claims.
4. Minor Issues: There are minor typos, formatting inconsistencies, and a lack of uniformity in the references, which could be improved for better presentation.
Arguments for Acceptance:
- The paper provides a novel and theoretically sound approach to reinforcement learning.
- It demonstrates strong empirical performance and computational efficiency.
- The work is well-executed and addresses a significant problem in the field.
Arguments Against Acceptance:
- The omission of explicit comparisons to state-of-the-art regret bounds and the KL-UCRL algorithm weakens the paper's positioning.
- Practical concerns about the regret bound constant remain unaddressed.
Recommendation:
I recommend acceptance of this paper, as its contributions to reinforcement learning are significant, and it opens up new avenues for research on posterior sampling. However, the authors should address the highlighted weaknesses in the final version to strengthen the paper further.