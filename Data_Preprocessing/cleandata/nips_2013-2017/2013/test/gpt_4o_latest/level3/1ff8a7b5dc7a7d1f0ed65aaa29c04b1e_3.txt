This paper explores the architecture and performance of Deep Recurrent Neural Networks (DRNNs) for character-level language modeling, demonstrating their ability to model temporal hierarchies and long-term dependencies. The authors propose DRNNs as an extension of traditional RNNs, where each layer is a recurrent network that processes the hidden states of the previous layer, enabling hierarchical temporal processing. They achieve state-of-the-art performance on a Wikipedia text corpus using stochastic gradient descent (SGD), and provide evidence of emergent time scales in DRNN layers. The paper also analyzes the role of individual layers in text prediction and their contributions to long-term dependencies, such as closing parentheses.
Strengths:
1. Novelty and Significance: The paper introduces a compelling hierarchical extension to RNNs, addressing the limitations of traditional RNNs in modeling multi-scale temporal dependencies. This is a significant contribution to the field of temporal modeling and sequence learning.
2. Experimental Rigor: The authors provide extensive experimental results, including comparisons with baseline RNNs and state-of-the-art methods. The analysis of layer-wise contributions and emergent time scales is particularly insightful.
3. Clarity and Accessibility: The paper is well-written and easy to follow, making complex concepts accessible to readers. The inclusion of detailed experimental setups and results enhances reproducibility.
4. State-of-the-Art Performance: The DRNN achieves competitive results on a challenging text prediction task, demonstrating its practical utility.
5. Intellectual Honesty: The authors acknowledge limitations, such as the unfair comparison due to parameter differences and the lack of evidence for higher-layer abstraction capabilities.
Weaknesses:
1. Gradient Normalization Concerns: The use of gradient normalization to prevent bifurcations raises questions about its impact on SGD's averaging behavior. A deeper exploration of this issue, possibly referencing the ICML 2013 work on gradient truncation, would strengthen the paper.
2. Fairness in Comparisons: The comparison between DRNNs and regular RNNs may be biased due to differences in parameter counts. Benchmarking DRNNs with equivalent parameter budgets would provide a fairer evaluation.
3. Layer Abstraction Evidence: While the paper claims that higher layers process abstract and long-term properties, it lacks direct evidence to substantiate this claim. Additional experiments or analyses are needed to validate this hypothesis.
4. Interpretability of Layer Interdependencies: The text generation experiment (Table 2) is persuasive but raises concerns about interpreting the interdependencies between layers. A more detailed discussion of these dynamics would be beneficial.
5. Terminology Definition: Key terms like DRNN-AO and DRNN-1O are only defined in Figure 1 and not in the main text, which could confuse readers.
Recommendation:
The paper is a valuable contribution to the field of deep temporal networks and sequence modeling. Its strengths in novelty, experimental rigor, and clarity outweigh its weaknesses. However, addressing the concerns about gradient normalization, fair comparisons, and layer abstraction evidence would significantly enhance its impact. I recommend acceptance, contingent on revisions to address these issues. 
Arguments for Acceptance:
- Novel and significant contribution to hierarchical temporal modeling.
- Rigorous experiments demonstrating state-of-the-art performance.
- Clear and accessible presentation.
Arguments Against Acceptance:
- Concerns about fairness in comparisons and gradient normalization.
- Lack of direct evidence for higher-layer abstraction capabilities.
- Limited interpretability of layer interdependencies.
Overall, this paper advances our understanding of deep recurrent architectures and their potential for modeling complex temporal hierarchies.