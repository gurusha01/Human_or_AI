The paper investigates the complexity of Policy Iteration (PI) algorithms for Markov Decision Processes (MDPs) with a focus on two variations: Howard's PI and Simplex-based PI. It provides a detailed analysis of the number of iterations required for convergence under different conditions, improving upon some existing bounds and simplifying the derivation of others. The authors also extend the analysis to stochastic MDPs and deterministic cases, offering new insights into the structural properties of MDPs that influence convergence rates.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with correct proofs and well-supported claims. The authors leverage contraction properties and structural assumptions to derive new bounds, demonstrating a deep understanding of the problem.
2. Incremental Contribution: While the novelty is limited, the paper makes incremental progress by refining existing bounds (e.g., improving Howard's PI by a factor of \(O(\log n)\)) and providing a more direct proof for Simplex-PI. These results are valuable for researchers working on MDPs and optimization algorithms.
3. Clarity: The writing is generally clear, and the logical flow of the paper is easy to follow. The authors provide sufficient background on MDPs and PI algorithms, making the work accessible to readers familiar with the field.
Weaknesses:
1. Limited Novelty: The work builds heavily on existing results, and the improvements, while interesting, are incremental rather than groundbreaking. For example, the improvement for Simplex-PI is only by a factor of 2 in some cases.
2. Typos and Notation: There are minor issues with typos and inconsistent notation, which could confuse readers. For example, some symbols are not clearly defined or are used ambiguously in certain sections.
3. Utility of Results: The paper would benefit from including concrete examples of MDPs to illustrate the implications of Corollary 2. This would make the results more practical and relatable for practitioners.
4. Howard's PI Analysis: The analysis for Howard's PI under structural assumptions (e.g., Assumption 2) is restrictive and does not fully address the general case, leaving open questions about its broader applicability.
Arguments for Acceptance:
- The paper provides a solid theoretical contribution by refining bounds for PI algorithms and extending the analysis to stochastic MDPs.
- The proofs are correct, and the results are relevant to the optimization and reinforcement learning communities.
- The work is well-written and accessible, with clear explanations of the key ideas.
Arguments Against Acceptance:
- The novelty is limited, as the paper primarily builds on existing results rather than introducing fundamentally new techniques.
- The practical utility of the results is somewhat limited without illustrative examples or empirical validation.
- The analysis for Howard's PI is incomplete, leaving significant gaps in understanding its complexity in general settings.
Recommendation:
I recommend acceptance with minor revisions. While the paper's novelty is limited, its contributions are solid and relevant. Addressing the issues with typos, notation, and the inclusion of examples for Corollary 2 would significantly enhance the paper's clarity and utility.