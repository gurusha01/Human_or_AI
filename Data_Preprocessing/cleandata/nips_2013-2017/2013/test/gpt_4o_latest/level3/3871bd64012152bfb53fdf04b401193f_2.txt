The paper addresses the challenging problem of binary classification in the presence of class-conditional noisy labels (CCN) and proposes two innovative surrogate-loss-based methods. The first method constructs an unbiased estimator of the non-noisy risk using a symmetry condition, while the second method employs a weighted 0-1 loss to adjust for the noise. Both approaches are supported by rigorous theoretical analysis and demonstrate strong empirical performance.
The paper makes significant contributions to the field. First, it provides theoretical guarantees for risk minimization under CCN without restrictive assumptions on the true distribution, filling a notable gap in the literature. The authors prove learnability with convex surrogates and establish the noise-tolerance of widely-used methods like biased SVMs and weighted logistic regression. The results are compelling, particularly the demonstration that these practical methods are provably robust to label noise. Second, the proposed algorithms are computationally efficient and achieve impressive accuracy even under high noise rates, outperforming or matching state-of-the-art methods on benchmark datasets.
The paper is well-written, with a clear structure and detailed exposition of the problem, methods, and results. The theoretical analysis is thorough, and the empirical evaluation is robust, covering both synthetic and real-world datasets. The experiments convincingly demonstrate the efficacy of the proposed methods, particularly their resilience to noise and competitiveness with existing approaches.
However, there are some limitations and areas for improvement. While the paper addresses CCN noise effectively, it does not explore learnability under richer noise models, such as the CPCN noise or monotonic noise, which could broaden its applicability. Additionally, the reliance on known noise rates in some experiments may limit the practical utility of the methods in real-world scenarios where noise rates are often unknown. Although the authors suggest cross-validation as a workaround, further discussion on the robustness of the methods to noise rate misspecification would strengthen the paper.
Strengths:
- Original theoretical contributions, including noise-tolerance guarantees for practical methods.
- Robust empirical results demonstrating high accuracy under significant noise.
- Clear and well-organized presentation of methods and results.
- Efficient algorithms with provable guarantees.
Weaknesses:
- Limited exploration of more complex noise models.
- Dependence on known noise rates in some cases.
- Lack of detailed discussion on robustness to noise rate misspecification.
Recommendation:
This paper is a strong contribution to the field of machine learning under noisy labels. Its theoretical insights and practical implications make it a valuable addition to the conference. I recommend acceptance, with minor revisions to address the limitations noted above.