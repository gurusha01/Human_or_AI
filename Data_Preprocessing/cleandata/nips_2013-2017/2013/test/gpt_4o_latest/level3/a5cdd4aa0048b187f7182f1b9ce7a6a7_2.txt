The paper presents a novel method for designing convex least-squares-style surrogate losses for high-arity multiclass learning problems characterized by low-rank loss structures. This work builds on and extends prior research, including the foundational work of Ramaswamy and Agarwal (2016), by providing explicit constructions of calibrated surrogate losses that operate on low-dimensional spaces. The authors demonstrate the applicability of their framework to a variety of subset ranking problems, including Precision@q, Expected Rank Utility (ERU), Mean Average Precision (MAP), and Pairwise Disagreement (PD), which are widely used in information retrieval and ranking tasks.
Strengths:  
The paper is technically sound and provides rigorous theoretical analysis to support its claims. The explicit construction of surrogate losses and their calibration proofs are clearly presented and build on established results in the literature. The authors address a significant limitation in prior work by reducing the dimensionality of surrogate spaces, making the proposed approach more practical for high-dimensional problems. Furthermore, the paper introduces efficient mappings for certain losses, such as Precision@q and ERU, which can be implemented using standard algorithms like sorting. The work also extends the state of the art by identifying conditions under which calibrated surrogates can be designed for challenging losses like MAP and PD, even when computational hardness is a concern.
The paper is well-written, with a logical structure that guides the reader through the theoretical contributions and their applications. The inclusion of examples, such as the Precision@q and ERU losses, helps illustrate the practical implications of the proposed method. The authors also provide a thoughtful discussion of computational challenges, such as NP-hard mappings, and propose alternative solutions under specific conditions.
Weaknesses:  
While the paper is comprehensive, the presentation of Theorem 3 could be improved by stating the condition in matrix form for clarity, as this would make the result more accessible to readers less familiar with the notation. Additionally, while the authors address computational challenges for MAP and PD losses, the practical feasibility of implementing the proposed surrogates for very large-scale problems remains somewhat unclear. Experimental validation of the theoretical results would strengthen the paper's impact, as it would demonstrate the practical utility of the proposed surrogates in real-world scenarios.
Arguments for Acceptance:  
1. The paper addresses a significant problem in machine learning by providing a general framework for designing calibrated surrogates for low-rank losses.  
2. The theoretical contributions are novel, well-supported, and extend the state of the art.  
3. The work is relevant to a broad audience, particularly those working on ranking and information retrieval tasks.  
4. The paper is clearly written and provides actionable insights for practitioners.
Arguments Against Acceptance:  
1. The lack of experimental validation limits the immediate practical applicability of the results.  
2. Computational challenges for certain mappings (e.g., MAP and PD) are acknowledged but not fully resolved.  
3. Some aspects of the presentation, such as Theorem 3, could be made more accessible.
Recommendation:  
Overall, the paper makes a strong theoretical contribution and advances the understanding of surrogate loss design for multiclass learning problems. I recommend acceptance, with a suggestion to improve the clarity of Theorem 3 and encourage the authors to include experimental results in future work.