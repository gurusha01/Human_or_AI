This paper investigates the utility of Deep Recurrent Neural Networks (DRNNs) for character-level language modeling, demonstrating their ability to model temporal hierarchies and long-term dependencies. The authors argue that DRNNs, which stack recurrent layers hierarchically, naturally capture multi-scale temporal structures, outperforming standard RNNs in tasks requiring long-range memory. They achieve state-of-the-art performance on a Wikipedia text corpus using simple stochastic gradient descent (SGD). The paper also challenges prior assumptions by showing that plain backpropagation through time (BPTT) is sufficient for training DRNNs on tasks like balancing parentheses. However, the paper focuses more on analysis rather than introducing novel architectures, as DRNNs have been explored in earlier works.
Strengths:
1. Comprehensive Analysis: The paper provides a detailed exploration of the temporal hierarchies and time scales that emerge in DRNNs, offering valuable insights into their behavior.
2. Empirical Results: The authors achieve competitive performance on a challenging character-level language modeling task, demonstrating the practical utility of DRNNs.
3. Challenging Assumptions: The finding that plain BPTT suffices for training DRNNs on complex tasks like balancing parentheses is noteworthy and challenges prior assumptions in the field.
4. Reproducibility: The training setup and experimental details are clearly described, enabling reproducibility.
Weaknesses:
1. Lack of Novelty: The claim of introducing DRNNs is inaccurate, as similar architectures have been explored in prior work (e.g., stacked LSTMs). The paper does not adequately differentiate its contributions from these earlier studies.
2. Limited Scope: The analysis is confined to character-level language modeling. Extending the evaluation to other domains, such as speech processing or time-series prediction, would strengthen the generalizability of the findings and rule out problem-specific artifacts.
3. Truncated BPTT: While the authors emphasize the effectiveness of plain BPTT, they do not test truncated BPTT, a cost-effective alternative that could offer practical advantages.
4. Clarity on Contributions: The paper focuses on analysis rather than proposing new architectures, which is acceptable but should be explicitly clarified to avoid confusion.
Arguments for Acceptance:
- The paper provides a rigorous analysis of DRNNs, contributing to the understanding of hierarchical temporal processing in recurrent networks.
- The empirical results are competitive, and the insights into time scales and long-term dependencies are valuable for future research.
Arguments Against Acceptance:
- The lack of novelty in the architecture limits the paper's originality.
- The narrow focus on a single task reduces the broader applicability of the findings.
- The omission of truncated BPTT experiments leaves a gap in the evaluation of training methods.
Recommendation:
While the paper offers meaningful insights into DRNNs and their temporal hierarchies, the lack of novelty and limited scope are significant drawbacks. If the authors clarify their contributions, extend the analysis to other tasks, and explore truncated BPTT, the paper would be much stronger. For now, I recommend weak acceptance, as the analysis and results are valuable, but the paper falls short of advancing the state of the art in a demonstrable way.