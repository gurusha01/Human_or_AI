This paper investigates human optimization behavior in 1D continuous function tasks and compares it to a range of optimization algorithms, finding that Gaussian Processes (GPs) provide the best alignment with human performance. The authors conduct seven experiments with non-math undergraduates, exploring interpolation, extrapolation, and optimization tasks. They conclude that humans outperform state-of-the-art optimization algorithms in efficiency and that GPs offer a promising theoretical framework to model human search strategies.
Strengths:
The paper addresses an intriguing and underexplored intersection of machine learning and cognitive science, making it relevant to both communities. The experimental design is thorough, with multiple controlled experiments that systematically test hypotheses about human behavior. The results are compelling, showing that humans outperform well-established optimization algorithms and that GPs can model human behavior effectively. The paper is well-written, with clear explanations of experimental procedures and results. The findings have theoretical significance, suggesting that GPs could serve as a unifying framework for understanding human function learning and search behavior, and practical implications for improving optimization algorithms.
Weaknesses:
1. Ecological Validity: The experimental stimuli and tasks are highly abstract, limiting the generalizability of the findings to real-world scenarios. While the authors justify their choice to focus on 1D functions to eliminate confounds, this abstraction may oversimplify the complexities of real-world optimization problems.
2. Integration with Prior Work: The paper misses opportunities to connect its findings with previous research on human function learning and related fields like optimal foraging and active vision. Standard stimuli or benchmarks from prior studies could have been used to better contextualize the results.
3. Theoretical Overreach: Some claims about humans following a GP are speculative and lack direct evidence. For instance, the assertion that humans balance exploration and exploitation using GP-based strategies could be better substantiated.
4. Experiment 5 Design: Always querying \( f(200) \) in the extrapolation task restricts the exploration of alternative strategies and may bias the results.
Minor Concerns:
- Prior psychological work should be introduced earlier for better context.
- Figures (e.g., 1, 3, 4, 5) are difficult to interpret in black-and-white print, and text sizes are too small.
- Stimuli generation in Experiment 1 appears unnecessarily complex; simpler methods could achieve similar objectives.
- Rank-order correlations of dependent variables across function types should be reported for better cross-condition comparisons.
- The tolerance levels for optimization algorithms are not well-explained, though they likely do not bias the results.
Arguments for Acceptance:
- The paper tackles a novel and important problem, providing strong experimental evidence and a well-articulated theoretical framework.
- The findings are significant for both cognitive science and machine learning, with potential applications in algorithm design and human-machine interaction.
- The experiments are well-designed, and the results are robust.
Arguments Against Acceptance:
- The lack of ecological validity and limited integration with prior work reduce the broader impact of the findings.
- Some theoretical claims are speculative and would benefit from additional evidence or modeling.
Recommendation:
This is a strong paper with compelling results, but it would benefit from addressing the noted weaknesses. I recommend acceptance with minor revisions, particularly to improve the integration with prior research, clarify theoretical claims, and enhance the presentation of figures.