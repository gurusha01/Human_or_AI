This paper introduces novel distributed clustering algorithms for k-median and k-means objectives, focusing on reducing communication overhead while maintaining theoretical guarantees on clustering quality. The key innovation lies in the construction of a global coreset using a distributed approach, which improves communication complexity over prior methods and adapts to general communication topologies. Experimental results on synthetic and UCI datasets demonstrate that the proposed algorithms outperform existing coreset-based distributed clustering methods under the same communication cost.
Strengths:
1. Technical Contribution: The paper provides a significant advancement in distributed clustering by presenting a coreset construction algorithm that reduces communication costs while maintaining provable guarantees. The improvement over prior work, particularly in sparse networks or graphs with large diameters, is well-motivated and demonstrated.
2. Experimental Validation: The experiments are comprehensive, covering multiple datasets and network topologies. The results convincingly show the superiority of the proposed method in terms of clustering quality and communication efficiency.
3. Theoretical Rigor: The paper includes detailed proofs and theoretical guarantees for the proposed algorithms, ensuring their soundness and applicability.
4. Practical Relevance: The work addresses an important problem in distributed machine learning, making it relevant for real-world applications where communication costs are a bottleneck.
Weaknesses:
1. Clarity of Presentation: While the paper is technically sound, it is highly technical and dense. Key derivations and theorem values are not explained in sufficient detail, making it challenging for readers to follow the proofs and intuitions without significant effort. A more accessible explanation of the main ideas would improve clarity.
2. Scope of Experiments: The experiments are limited to low-dimensional datasets, raising concerns about the approximation quality of the coresets in high-dimensional settings. Since many real-world datasets are high-dimensional, this limitation reduces the generalizability of the results.
3. Title and Focus: The title is overly broad and does not reflect the specific focus on center-based clustering and coresets. A more precise title would better align with the paper's contributions and scope.
4. Comparison to Related Work: While the paper discusses related work, the comparison could be more thorough, especially with recent advances in distributed clustering. Additionally, the paper could better contextualize its contributions within the broader landscape of distributed machine learning.
Arguments for Acceptance:
- The paper makes a clear and significant contribution to distributed clustering, addressing both theoretical and practical challenges.
- The proposed algorithms are novel, technically sound, and demonstrate superior performance in experiments.
- The work has strong potential for impact in distributed machine learning and related fields.
Arguments Against Acceptance:
- The lack of detailed explanations for key derivations and proofs limits accessibility and understanding for a broader audience.
- The experiments do not address high-dimensional datasets, which are critical for many real-world applications.
- The title and presentation could be improved to better reflect and communicate the paper's contributions.
Recommendation:
Overall, the paper is a valuable contribution to the field of distributed clustering and is well-suited for the conference. However, the authors should address the clarity issues, expand experiments to high-dimensional datasets, and refine the title for better alignment with the paper's focus. I recommend acceptance with minor revisions.