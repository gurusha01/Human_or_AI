This paper introduces a novel approach to reinforcement learning (RL) by providing the first regret bounds for an algorithm based on posterior sampling rather than the widely-used optimism in the face of uncertainty (OFU). The proposed Posterior Sampling for Reinforcement Learning (PSRL) algorithm is conceptually simple, computationally efficient, and capable of incorporating prior knowledge. The authors establish an expected regret bound of \( \tilde{O}(\tau S \sqrt{AT}) \), which is close to state-of-the-art bounds for RL algorithms. They also demonstrate through simulations that PSRL outperforms optimistic algorithms like UCRL2 in practical scenarios.
Strengths:
1. Novelty and Contribution: The paper addresses a significant gap in RL literature by providing theoretical guarantees for posterior sampling, a method historically viewed as heuristic. This is a meaningful contribution to the field.
2. Practical Efficiency: While the regret bound is not as tight as those for OFU-based algorithms, the simulations convincingly show that PSRL performs better in practice, especially in challenging environments like RiverSwim.
3. Clarity: The paper is well-written and organized, making the algorithm and its theoretical underpinnings accessible to readers. The authors effectively highlight the advantages of PSRL, such as its computational simplicity and ability to encode prior knowledge.
4. Potential for Future Work: The episodic formulation, while limiting, is a reasonable assumption for this foundational work. The authors acknowledge this limitation and suggest that practical extensions are possible.
Weaknesses:
1. Proof Details: While the theoretical analysis is rigorous, some proof details were challenging to follow, leaving room for potential errors. A clearer exposition or additional intuition behind key steps would strengthen the paper.
2. Comparison with State-of-the-Art: The paper does not explicitly compare its regret bounds to those of state-of-the-art algorithms like UCRL2 or REGAL. Including such comparisons would substantiate the claim that PSRL is close to the theoretical lower bounds.
3. Space Allocation: The discussion of a related idea could be omitted to make space for a more thorough comparison with existing work or additional experimental results.
4. Assumptions: The assumption of fixed-length episodes is unrealistic for many real-world applications. While acceptable for theoretical work, this limits the immediate applicability of the results.
Recommendation:
The paper makes a strong theoretical and practical contribution to RL by establishing regret bounds for posterior sampling and demonstrating its empirical advantages. However, the lack of explicit comparisons with state-of-the-art regret bounds and the incomplete clarity of proofs are notable weaknesses. If these issues can be addressed in a revision, the paper would be a compelling addition to the conference. 
Arguments for Acceptance:
- Novel and significant contribution to RL theory.
- Demonstrates practical advantages of PSRL over optimistic algorithms.
- Well-written and organized, with clear motivation and results.
Arguments Against Acceptance:
- Lack of explicit comparison with state-of-the-art regret bounds.
- Proof details are not fully transparent, leaving room for doubt.
- Unrealistic episodic assumption limits immediate applicability.
Overall, I recommend acceptance, contingent on addressing the issues of comparison and proof clarity. This work has the potential to inspire further research and practical adoption of posterior sampling in RL.