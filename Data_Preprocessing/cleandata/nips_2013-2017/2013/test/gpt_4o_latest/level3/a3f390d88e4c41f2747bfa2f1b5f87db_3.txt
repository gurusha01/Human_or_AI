This paper investigates the relationship between human behavior and optimization algorithms in 1D function optimization tasks, with a focus on Bayesian Optimization (BO) using Gaussian Processes (GP). The authors demonstrate that BO algorithms with GP priors better capture human search behavior than other optimization methods. Through a series of experiments, they provide evidence that GPs offer a unifying theoretical framework for explaining human function learning and search strategies. The study is novel and contributes unique experimental data, comparing human performance against 24 optimization algorithms, and exploring the interplay between exploration and exploitation in human decision-making.
Strengths:
1. Originality and Significance: The paper addresses an important and underexplored question—how human optimization strategies compare to algorithmic approaches—and provides a novel perspective by leveraging GPs as a unifying framework. The results are significant, as they suggest that human-like strategies could inspire more efficient optimization algorithms.
2. Comprehensive Experimental Design: The authors conducted seven experiments with 99 participants, systematically exploring human behavior in optimization, interpolation, and extrapolation tasks. The inclusion of both passive and active learning tasks strengthens the generalizability of their findings.
3. Insightful Contributions: The paper highlights the potential for BO algorithms to model human behavior and suggests future directions, such as integrating composite sampling and stopping strategies to better match human performance.
4. Clarity of Writing: The text is well-written and effectively communicates the key findings and implications, making it accessible to both cognitive scientists and machine learning researchers.
Weaknesses:
1. Technical Simplifications: The algorithms used for comparison are overly simplistic relative to the complexity of human decision-making. The separation of sampling and stopping with arbitrary rules undermines the realism of the comparison.
2. Ambiguity in Algorithm Contributions: The relative contributions of the GP model versus the sampling policy (e.g., UCB) to the performance of BO algorithms remain unclear. This distinction is critical for understanding the mechanisms underlying the observed results.
3. Clarity of Figures: The figures lack clear descriptions, making it difficult to interpret some results. For instance, Figure 3's claims are unclear, and Figure 6's histograms are not adequately explained.
4. Minor Errors and Ambiguities: Potential errors in reported numbers and ambiguities in second-order measures detract from the paper's technical rigor.
Suggestions for Improvement:
1. Incorporate a joint sampling and stopping policy to better align algorithmic behavior with human strategies, as suggested by Figure 4e.
2. Clarify the individual contributions of GP learning and sampling policies to BO performance.
3. Improve figure descriptions and ensure all visualizations are self-explanatory.
4. Address minor errors and ambiguities in the reported results.
Overall Assessment:
This is a strong paper that makes a valuable contribution to understanding human optimization behavior and its implications for algorithm design. While methodological and clarity issues need to be addressed, the paper's originality and significance outweigh its weaknesses. I recommend acceptance, provided the authors address the identified concerns.