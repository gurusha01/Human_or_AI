This paper presents a unified formalization of stochastic And-Or grammars (AOGs) and proposes an unsupervised learning approach for both their structure and parameters. The authors address the challenge of compactly representing compositionality and reconfigurability in data such as images and events. By iteratively optimizing the posterior probability of the grammar, their method learns compositions and reconfigurations in a unified manner, starting from a trivial initial grammar. The approach is evaluated on event and image grammar learning tasks, demonstrating comparable or superior performance to prior methods. This work builds on previous research in stochastic grammars for natural language processing, image modeling, and event modeling, while extending structure learning techniques from works such as [14â€“18].
Strengths
1. Unified Framework: The formalization of stochastic AOGs as agnostic to data types is a significant contribution, enabling the method to generalize across diverse domains like text, images, and events.
2. Unified Learning of Compositions and Reconfigurations: The proposed And-Or fragment approach addresses limitations in prior methods that separately learn these components, leading to more compact and robust grammars.
3. Posterior Optimization: Unlike heuristic-based methods, the explicit optimization of posterior probability ensures a principled approach to grammar learning.
4. Empirical Results: The method demonstrates strong performance in both event and image grammar tasks, outperforming prior approaches in terms of F-measure and perplexity. The experiments are well-designed, with evaluations on both synthetic and real datasets.
5. Scalability: The algorithm is computationally efficient, as evidenced by its ability to handle datasets with thousands of samples within reasonable runtimes.
Weaknesses
1. Clarity: While the paper is technically sound, certain sections, such as the derivation of likelihood and prior gains, are dense and could benefit from additional explanation or illustrative examples to improve accessibility for a broader audience.
2. Limited Evaluation Scope: The evaluation focuses on specific domains (event and image grammars). While the framework is claimed to be general, additional experiments on other data types (e.g., text or multimodal data) would strengthen the claim of generality.
3. Assumptions on Context-Freeness: The method assumes context-freeness in the grammar, which may limit its applicability to more complex, context-sensitive data.
4. Comparison with State-of-the-Art: While the method is compared against prior approaches [15, 17, 18], it would be useful to benchmark against more recent advances in grammar learning or related fields like neural-symbolic methods.
Arguments for Acceptance
- The paper makes a significant contribution to unsupervised grammar learning by unifying structure and parameter learning in stochastic AOGs.
- The method is novel, technically sound, and demonstrates strong empirical performance.
- The work is relevant to the NeurIPS community, addressing topics like probabilistic modeling, unsupervised learning, and structured data representation.
Arguments Against Acceptance
- The clarity of the presentation could be improved, particularly in technical derivations.
- The evaluation lacks diversity in data types, which limits the demonstration of the framework's generality.
- The assumptions of context-freeness and reliance on Viterbi likelihood may restrict the applicability of the method to certain domains.
Recommendation
Overall, this paper provides a valuable contribution to the field of probabilistic modeling and grammar learning. While there are areas for improvement, particularly in clarity and evaluation scope, the strengths of the proposed approach outweigh its weaknesses. I recommend acceptance with minor revisions to improve clarity and broaden the evaluation.