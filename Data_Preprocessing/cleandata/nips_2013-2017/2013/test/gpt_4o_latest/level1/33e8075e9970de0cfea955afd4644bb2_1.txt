This paper presents a novel machine learning framework for constructing graph wavelets that adapt to a given class of signals, addressing limitations in existing graph wavelet constructions that rely solely on graph structure. The authors leverage the lifting scheme, drawing an analogy between its iterative nature and deep auto-encoder networks, to design wavelets that achieve sparse signal representation. The training process is unsupervised, employing a greedy pre-training approach akin to auto-encoders. The resulting wavelet transform is linear, efficient in time and memory, and demonstrates improved sparsity for test signals on both synthetic and real-world datasets. The paper also explores applications in semi-supervised learning and image processing, showcasing the versatility of the proposed method.
Strengths
1. Technical Novelty: The paper introduces a unique combination of wavelet theory and machine learning, particularly the use of auto-encoders for wavelet construction. This is a significant departure from traditional graph wavelet methods, which do not directly incorporate signal-specific properties.
2. Theoretical Rigor: The authors provide a detailed mathematical formulation of their approach, ensuring that key wavelet properties such as vanishing moments and locality are preserved. The tying of update and predict operators is well-motivated and effectively prevents trivial solutions.
3. Experimental Validation: The experiments are thorough, covering synthetic signals, real-world graph signals (e.g., temperature data), and image processing tasks. The results consistently demonstrate the advantages of the proposed method in terms of sparsity and generalization.
4. Practical Relevance: The framework is computationally efficient, with linear time and memory complexity, making it suitable for large-scale applications. The ability to handle both training and unseen signals is a notable strength.
Weaknesses
1. Clarity: While the technical content is rich, the paper is dense and may be challenging for readers unfamiliar with wavelet theory or auto-encoders. The exposition could benefit from additional diagrams and simplified explanations of key concepts.
2. Scope of Evaluation: Although the experiments are diverse, the paper does not compare its method against state-of-the-art adaptive graph wavelet techniques (e.g., spectral wavelets or diffusion wavelets) in terms of sparsity or computational efficiency.
3. Generality: The reliance on hierarchical graph partitioning and the use of Haar wavelets as a baseline may limit the applicability of the method to graphs where such partitions are not easily defined or meaningful.
4. Training Dependency: The performance of the wavelets is heavily dependent on the quality and representativeness of the training functions. While the authors propose a smoothness prior for cases without training data, this may not generalize well to all applications.
Arguments for Acceptance
- The paper addresses a significant gap in the field by introducing signal-adaptive graph wavelets, a novel contribution with potential impact across various domains.
- The proposed method is theoretically sound, computationally efficient, and experimentally validated on diverse datasets.
- The connection between wavelets and deep learning is timely and opens up new avenues for interdisciplinary research.
Arguments Against Acceptance
- The paper's clarity and accessibility could be improved, particularly for a broader audience at NeurIPS.
- The lack of direct comparisons with existing adaptive graph wavelet methods leaves some questions about the relative performance of the proposed approach.
Recommendation
I recommend acceptance of this paper, as it makes a significant and original contribution to the intersection of graph signal processing and machine learning. However, the authors should consider improving the clarity of their presentation and including comparisons with alternative methods to strengthen the paper further.