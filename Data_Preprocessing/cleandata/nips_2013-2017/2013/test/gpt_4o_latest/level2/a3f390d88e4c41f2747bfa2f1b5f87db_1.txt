This paper investigates human strategies for 1D function optimization and compares them to 24 established optimization algorithms, proposing Gaussian Processes (GP) as a unifying theoretical framework for modeling human behavior in function learning and active search. The authors conducted seven experiments involving 99 undergraduate students from non-mathematics majors, systematically examining human performance in optimization, interpolation, and extrapolation tasks. The results demonstrate that humans outperform state-of-the-art optimization algorithms in terms of efficiency (fewer function calls) and accuracy, with Bayesian Optimization (BO) methods based on GP being the closest to human behavior. The authors further argue that GP provides a principled explanation for human search strategies, balancing exploration and exploitation.
Strengths:
1. Novelty and Scope: The paper addresses a unique and underexplored question—how human optimization strategies compare to algorithms—and provides a comprehensive experimental framework to investigate this. The focus on 1D optimization eliminates confounding factors, allowing for a clear analysis of basic search mechanisms.
2. Empirical Rigor: The study is methodologically robust, involving a large number of participants and diverse function families. The authors carefully control experimental conditions to ensure fair comparisons between humans and algorithms.
3. Theoretical Contribution: The proposal of GP as a unifying framework for human function learning and search is compelling and well-supported by the data. The findings extend prior work on GP in human cognition, offering new insights into active learning and optimization.
4. Practical Implications: The results suggest avenues for improving optimization algorithms by incorporating human-inspired strategies, such as more efficient exploration-exploitation trade-offs and stopping criteria.
Weaknesses:
1. Limited Generalizability: The study focuses exclusively on 1D functions, which, while simplifying the analysis, limits the applicability of the findings to higher-dimensional or real-world optimization problems. The authors acknowledge this limitation but do not provide concrete plans for extending the work.
2. Algorithm Comparisons: While the authors compare human performance to a wide range of algorithms, certain details about parameter tuning and fairness of comparison (e.g., computational resources) could be elaborated further.
3. Behavioral Modeling: Although GP is shown to approximate human behavior well, the paper does not fully explore alternative models or hybrid approaches that might better capture human decision-making nuances.
4. Clarity: The paper is dense and could benefit from improved organization, particularly in the results section. For instance, the discussion of experiments 2–5 could be streamlined to focus on key insights rather than exhaustive details.
Arguments for Acceptance:
- The paper makes a significant contribution to understanding human optimization strategies and their relationship to machine learning algorithms, which is highly relevant to the NIPS audience.
- The experimental design is rigorous, and the findings are well-supported by data, with clear implications for both cognitive science and algorithm development.
- The exploration of GP as a unifying framework is innovative and opens new directions for research in active learning and optimization.
Arguments Against Acceptance:
- The limited generalizability to higher-dimensional problems and real-world tasks may reduce the broader impact of the findings.
- The paper's dense presentation and occasional lack of clarity may hinder accessibility for readers unfamiliar with the experimental setup or GP-based methods.
Recommendation:
I recommend acceptance with minor revisions. The paper is a strong scientific contribution with novel insights, but it would benefit from improved clarity and a more explicit discussion of its limitations and future directions.