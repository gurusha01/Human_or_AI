The paper presents a novel application of Optimistic Concurrency Control (OCC) to distributed machine learning algorithms, specifically targeting unsupervised learning tasks such as clustering, feature modeling, and online facility location. The authors propose a middle-ground approach between strict concurrency constraints and coordination-free paradigms, leveraging OCC to ensure correctness while maintaining high parallelism. The paper introduces distributed versions of DP-Means, BP-Means, and Online Facility Location (OFL) algorithms, demonstrating their correctness and scalability through theoretical analysis and empirical evaluation.
Strengths:
1. Novelty and Contribution: The paper makes a significant contribution by adapting OCC, a concept traditionally used in database systems, to distributed machine learning. This approach bridges the gap between correctness and scalability, which is a critical challenge in distributed learning.
2. Theoretical Rigor: The authors provide strong theoretical guarantees, including proofs of serializability and approximation bounds for the distributed algorithms. The equivalence of the distributed algorithms to their serial counterparts is a notable achievement.
3. Practical Relevance: The proposed algorithms are implemented on Spark and evaluated on large-scale datasets, demonstrating their practical applicability and scalability. The experiments show near-perfect scaling for DP-Means and BP-Means, which is impressive.
4. Clarity of Algorithms: The pseudocode for the OCC-based algorithms is well-documented, making it easier for readers to understand and potentially reproduce the work.
5. Comprehensive Evaluation: The paper includes both simulated and distributed experiments, providing a thorough evaluation of the proposed methods.
Weaknesses:
1. Limited Scope of Applications: While the paper focuses on unsupervised learning, it would benefit from a discussion of how OCC could be extended to other machine learning paradigms, such as supervised learning or reinforcement learning.
2. Scalability Analysis Assumptions: The scalability analysis relies on assumptions about data distribution (e.g., well-spaced clusters), which may not hold in real-world scenarios. A more robust analysis or experiments on diverse datasets would strengthen the claims.
3. First Epoch Bottleneck: The experiments reveal that the first epoch often incurs significant overhead due to synchronization. While subsequent epochs scale well, addressing this bottleneck explicitly would improve the practical utility of the algorithms.
4. Comparison with Alternatives: The paper briefly mentions alternative approaches, such as divide-and-conquer schemes, but does not provide a direct experimental comparison. Including such comparisons would highlight the advantages of OCC more clearly.
5. Conflict Detection as a Control Knob: The discussion on probabilistically accepting non-serializable operations is intriguing but remains speculative. A preliminary exploration of this idea would enhance the paper's impact.
Recommendation:
The paper is a strong candidate for acceptance. It introduces a novel and well-supported approach to distributed machine learning, with theoretical and practical contributions that advance the state of the art. However, addressing the scalability assumptions, first-epoch bottleneck, and providing more experimental comparisons would further strengthen the work. I recommend acceptance with minor revisions to address these points. 
Pro Arguments:
- Novel application of OCC to machine learning.
- Strong theoretical guarantees and practical implementation.
- Comprehensive evaluation demonstrating scalability.
Con Arguments:
- Limited discussion of broader applicability.
- Assumptions in scalability analysis may not generalize.
- Lack of direct experimental comparison with alternative methods.
Overall, the paper is a valuable contribution to the field and aligns well with the goals of the conference.