This paper proposes novel distributed clustering algorithms for k-median and k-means objectives, emphasizing reduced communication complexity and provable guarantees. The central contribution is a distributed coreset construction method that operates efficiently over general communication topologies, outperforming prior approaches in both theoretical bounds and experimental evaluations. By leveraging local approximations and message-passing techniques, the proposed algorithms achieve significant improvements in communication cost while maintaining clustering quality. Experimental results on large-scale datasets validate the practical utility of the approach, demonstrating superior performance compared to existing coreset-based distributed clustering methods.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with well-defined theoretical guarantees for the proposed algorithms. The proofs are detailed and grounded in established coreset theory, ensuring the validity of the claims.
2. Novelty: The work introduces a distributed coreset construction algorithm that avoids the inefficiencies of combining local coresets, a significant improvement over prior methods like [23]. The reduction in communication complexity, especially for sparse networks with large diameters, is a notable advancement.
3. Practical Relevance: The experimental results demonstrate clear advantages in real-world scenarios, with the proposed method achieving better clustering quality for the same communication cost compared to baselines.
4. Clarity: The paper is well-organized, with a logical flow from problem definition to methodology, theoretical analysis, and experimental validation. The inclusion of algorithms and proof sketches aids comprehension.
5. Comprehensive Evaluation: The experiments cover various network topologies and data partitioning methods, providing a robust evaluation of the algorithm's performance.
Weaknesses:
1. Limited Discussion of Limitations: While the paper is strong in its contributions, it lacks a thorough discussion of potential limitations, such as scalability to extremely high-dimensional data or the impact of network latency on performance.
2. Comparison Scope: The experimental comparison is primarily against [23] and a basic coreset combination approach. Including additional baselines or more recent distributed clustering methods would strengthen the evaluation.
3. Reproducibility: Although the theoretical framework is detailed, the implementation details of the experiments (e.g., parameter tuning, hardware specifications) are sparse, which may hinder reproducibility.
Arguments for Acceptance:
- The paper addresses a critical problem in distributed clustering with a novel and technically sound solution.
- It demonstrates significant improvements over existing methods in both theory and practice.
- The work is relevant to the NeurIPS audience, given its focus on scalable machine learning algorithms for distributed systems.
Arguments Against Acceptance:
- The lack of a detailed discussion on limitations and broader comparisons slightly weakens the paper's impact.
- Reproducibility could be improved with more experimental details.
Suggestions for Improvement:
1. Include a dedicated section discussing the limitations of the proposed approach and potential avenues for future work.
2. Expand the experimental comparison to include more recent distributed clustering algorithms.
3. Provide additional implementation details to enhance reproducibility.
Recommendation:
Overall, this paper makes a strong contribution to the field of distributed clustering and aligns well with the NeurIPS focus on scalable and theoretically grounded machine learning methods. While there are minor areas for improvement, the strengths far outweigh the weaknesses. I recommend acceptance.