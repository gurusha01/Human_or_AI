The paper presents a novel two-step representation learning method for cross-language text classification, addressing the challenge of disjoint feature spaces between languages. The authors propose a matrix completion approach to recover missing entries in a bilingual document-term matrix, followed by latent semantic indexing (LSI) to induce low-dimensional cross-lingual representations. The method is evaluated on cross-language sentiment classification tasks using Amazon product reviews, demonstrating superior performance over competing methods, particularly when the number of parallel bilingual documents is small.
Strengths:
1. Novelty: The proposed combination of matrix completion and LSI is innovative and addresses a significant limitation in prior cross-lingual representation learning methods. Unlike approaches like CL-LSI or CL-KCCA, this method explicitly fills missing entries in the document-term matrix, leveraging co-occurrence patterns across languages.
2. Technical Soundness: The paper provides a rigorous formulation of the matrix completion problem, including a convex optimization framework with convergence guarantees. The use of a scalable projected gradient descent algorithm is well-justified and appropriate for large-scale datasets.
3. Empirical Validation: Extensive experiments on 18 cross-language sentiment classification tasks convincingly demonstrate the method's effectiveness. The proposed method consistently outperforms baselines and other state-of-the-art methods, particularly in low-resource settings with limited parallel data.
4. Practical Relevance: The method's ability to perform well with a small number of parallel bilingual documents makes it highly practical for real-world applications where labeled or parallel data is scarce.
5. Clarity: The paper is well-organized, with detailed explanations of the methodology, optimization algorithm, and experimental setup. The inclusion of parameter sensitivity analysis and performance trends with varying parallel data sizes adds depth to the evaluation.
Weaknesses:
1. Limited Discussion of Limitations: While the method performs well with small parallel datasets, the paper does not explore potential limitations, such as scalability to extremely large vocabularies or computational overhead compared to simpler methods like CL-LSI.
2. Comparison with Neural Approaches: The paper does not compare its method with recent neural representation learning techniques, such as multilingual embeddings (e.g., mBERT), which are increasingly popular in cross-lingual NLP tasks.
3. Reproducibility: While the methodology is described in detail, the paper does not provide access to code or implementation details, which could hinder reproducibility.
Suggestions for Improvement:
1. Include a discussion of how the method scales with larger datasets and vocabularies, as well as its computational efficiency compared to simpler baselines.
2. Compare the proposed method with neural approaches, such as multilingual transformers, to provide a more comprehensive evaluation.
3. Release the implementation or pseudocode for the matrix completion and LSI steps to facilitate reproducibility.
Recommendation:
Overall, this paper makes a significant contribution to cross-language text classification by proposing a novel and effective representation learning method. The strong empirical results and practical applicability make it a valuable addition to the field. I recommend acceptance, with minor revisions to address the weaknesses and suggestions outlined above.