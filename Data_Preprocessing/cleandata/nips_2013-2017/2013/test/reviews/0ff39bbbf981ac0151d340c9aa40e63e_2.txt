Summary: This paper presents a two-step method to learn a cross-lingual topic representation using matrix completion and latent semantic analysis techniques. It uses a projected gradient descent algorithm to optimize the matrix completion problem. Experimental results show that the proposed method outperforms mono-lingual baseline and cross-lingual baselines on sentiment analysis task on parallel Amazon review dataset. Learning curves with different sizes of unlabeled data illustrate that the proposed algorithm learns highly accurate cross-lingual topic representation with relatively small number of parallel data. 
Quality: Experiments were thoroughly carried out to support the claim. The propose two-step learning algorithm is carefully compared with two baselines (plain bag-of-words, cross lingual LSA) and two state-of-the-art cross-lingual dimensionality reduction algorithms. 
Clarity: This paper is well-organized. All the parameters (including how to select them) were written in the paper; it is easy to reproduce the result. Just a minor comment: I would suggest explaining the motivation of using matrix completion more in Introduction (though it is described in Section 3). 
Originality: Although the task of cross-lingual representation learning is not new, the application of matrix completion to this task is novel and it is clear that the proposed algorithm achieves better performance than previous methods in the literature. 
Significance: The result of the two-step approach is consistently higher than other baselines. We can learn form this work that matrix completion can be used as a preprocessing for cross-lingual sentiment classification task, and this kind of formulation may be effective in other tasks as well. This paper proposes a two-step method to learn a cross-lingual topic representation which perform matrix completion before latent semantic analysis on cross-lingual sentiment classification task. Experimental results show that the proposed method outperforms mono-lingual baseline and other cross-lingual baselines including one of the state-of-the-art methods, and is stable even when trained on different sizes of unlabeled parallel data.