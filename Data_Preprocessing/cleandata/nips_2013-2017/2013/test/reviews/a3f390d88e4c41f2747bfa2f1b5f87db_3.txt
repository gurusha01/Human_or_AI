This paper compared human behavior on several active search tasks to a set of established active search algorithms, and found that the Bayesian optimization algorithms combined with GP prior are better in capturing human data. 
Quality: 
In general, the paper is technically sound. Nevertheless, one major problem I have regarding their method is that the forms of the algorithms are not very proper (they are too simple) to solve the more complicated tasks as given to the humans. For example, the algorithms do not incorporate the step cost of querying a location whereas the humans do. The humans are essentially solving a composite sampling/stopping problem, whereas the algorithms separate sampling and stopping, and use seemingly arbitrary stopping rules that are not sensitive to the objective (i.e. higher hit with smaller function calls). This might not affect the general conclusion of this paper (that BO algorithms with GP can capture human search well), but it needs to be addressed to make the comparison between human and algorithms really fair. As I see it, Figure 4e actually implies that if the algorithms use a more sensible, joint policy for search and stopping, they could have achieved the same performance as people. I see that the authors partially addressed this issue in their discussion, but I think this issue needs more elaboration. 
Another problem I have is that most of the BO algorithms use different methods than the non-Bayesian algorithms to decide where to query the next location. How do the authors separate the contribution of GP and the sampling policy? For example, GP-UCB looks pretty good. Is it because of GP learning or the UCB sampling policy? 
Clarity: 
Overall, the text is pretty well-written, but the figures lack clear descriptions (see list of specific questions). 
Originality: 
This paper is original. 
Significance: 
I have problems with the method as stated above, but I think the paper poses an important and interesting question that opens many future directions, along with unique, interesting experimental data. It also considered a pretty complete set of the most well-known search algorithms. 
Minor comments: 
- Page 3 1st paragraph: shouldn't it be 25 functions and 23 subjects? 
- Figure 3: I don't see MPI intersect the box (as claimed in the text).. 
- Page 4: Which second-order measure captures the order of the query sequence? I guess I'm not clear how "mean shortest distance" is calculated. 
- What are the histograms in several plots in Figure 6? 
 A good paper. It is original and addresses important, interesting question. Some problems with the current form of their approach/method.