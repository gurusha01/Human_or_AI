The authors propose a new deep architecture, which combines the hierarchy of deep learning with time-series modelling known from HMMs or recurrent neural networks. The proposed training algorithm builds the network layer-by-layer using supervised (pre-)training a next-letter prediction objective. The experiments demonstrate that after training very large networks for about 10 days, the network performance on a Wikipedia dataset published by Hinton et al. improves over previous work. The authors then proceed to analyze and discuss details of how the network approaches its task. For example, long-term dependencies are modelled in higher layers, correspondence between opening and closing parenthesis are modelled as a "pseudo-stable attractor-like state". 
The paper is of good quality. It is well-written, experiments are set up well and the figures support the findings well. My main issue with the paper is the experiments. The comparison is done on one dataset only, which does not seem to have much work on it. Direct comparison is only with other RNN-related models, not with maybe more common hierarchical HMMs etc., which are able to solve similar tasks (Tab. 1 lists other approaches, but on a different corpus, which makes them incomparable). 
The contribution of the paper is more in the analysis of /how/ the network operates (which is analyzed quite well) than in what it achieves, but this is not what the paper sets out for.  A new deep architecture is proposed, combining hierarchical features of deep learning with time-series modelling. The paper is well-written, the model is analyzed well, but results are somewhat inconclusive.