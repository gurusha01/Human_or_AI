The paper presents bounds on the complexity of policy iteration for two versions of the algorithm: one which changes the policy at all states where there is an action better than the current policy (which the author call Howard PI), and one which only changes policy at states with a maximal advantage (which corresponds to running Simplex in the linear programming version of the algorithm). The paper re-derives some existing bounds in a simpler way and also improves some existing bounds. 
The paper presents an interesting contribution and the proofs seem correct. The novelty is not very high, as the paper builds on existing results, but nonetheless, there are improvements on these results. The writing in the paper is clear. 
Small comments: 
- line o43: When at state -> At state 
- The definition of advantage (line 085) is missing a \prime, otherwise it's trivially - line 097: on state -> one state 
- line 112: and that -> that 
- line 195: the the 
- line 217: Material and 
- line 367: "this lemma" - which lemma do you mean? 
- In the proof of Lemma 3, line 72, you have a \max{s,\bar{\pi}}. Here, the order in which you take the max matters, so you should write \maxs \max_{\bar{\pi}} to clarify (I am assuming this is what is done). 
- It would be very useful to have some examples of types of MDPs in which Corollary 2 holds. The paper presents improvements on the current bounds of policy iteration, making progress on existing bounds. It is a nice contribution, though it does not depart in a major way from existing proof techniques.