In this well-written and dense paper the authors compare human active learning, optimization, and inter/extrapolation of 1D functions. Their two conclusions are that (a) people outperform modern optimization algorithms, and (b) their behavior is largely consistent with bayesian inference/search/prediction based on gaussian process assumptions about function smoothness. 
I have some concerns about point (a) (arguably less important than point b). I'm not convinced that the authors eliminated all the additional information people have. Critically, the range of the y axis remains an issue. The authors say that they jittered the y offset to alleviate the problem, but I think numbers here are important: what is the range of the function, relative to the displayed range (the remainder being the range of the jitter). Are these relative ranges constant across trials? In general, if a point appears at the bottom of the screen, subjects know that it must be far from the maximum, if it appears close to the top, it must be close to the maximum y value. None of the optimization algorithms know this, and thus can't use it to help search. 
While this paper has a lot of interesting measures, which each speak to different aspects of the (mis)match between algorithm and human behavior, I find this array of measures confusing, and largely unnecessary. How about a single measure as follows: 
give the algorithm the first K points a subject observed. Have the algorithm pick point K+1. Compare the K+1th point for the algorithm and for the person. 
Maybe something else would work better. However, looking at figure 4, I am overwhelmed by the noisiness and apparent lack of diagnosticity of the assorted measures. I also find it hard to assess why agreement on the distribution of search step sizes is important. 
Human consistency. Instead of the convoluted array of measures, I'd like to see some space dedicated to assessing across-subject agreement. The low standard deviations of crude performance across people are suggestive of consistency, but not enough. How high are the split-half correlations of performance over the different trials? How well does the distribution of clicks from one half of the subjects predict the distribution of clicks from the other half? This later measure I find particularly important to be able to compare the across-subject agreement to the agreement with various algorithms (this could be done for most of the marginal measures the authors use). 
I'm not sure why in Expt 6 the authors thought the first click should be to the maximum variance point. I think a complete decision theoretic analysis of the problem is important. Given that both clicks are "equally important" (ok, admittedly I don't know what the authors meant here), it's not so clear that the correct strategy is to gain maximal information on the first click. if the maximum is unambiguous, wouldn't that be the correct place to click? 
Minor points: I was a bit confused about the claim that subjects use the 'gradient': of course, they don't have access to the gradient, so they use something like the 'estimated smoothness' -- which is another further point to argue for some gaussian process like algorithm that does this sort of estimation. 
 The paper has a rich set of (somewhat redundant) experiments on human optimization assessed via a large model bake-off. It would be improved by using fewer, more thoughtful measures by which to evaluate model fit and by more careful estimates of across-subject consistency.