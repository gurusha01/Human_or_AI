Title: (More) Efficient Reinforcement Learning via Posterior Sampling 
Summary: The paper proposes a Thompson sampling-type approach for episodic finite-horizon reinforcement learning, called posterior sampling for reinforcement learning. A regret analysis showing state of the art performance is provided, as well as simulations showing better empirical performance on some toy problems than other algorithms with similar regret bounds. 
Comments: 
The paper is clear and very well written. The contribution is also clearly identified: showing that is possible to use a Thompson sampling-type approach in a RL setting, and also theoretically analyse that is can be efficient. 
Regarding the regret bound, the authors say that the regret bound of PSRL is close to state of the art. It would be nice to recall in this paper what are such state-of-the art values. 
This is a rather frequent question in the field of multi-armed bandit algorithms, but how large is the constant in practice (in the O())?; 
Comparisons could be made with the algorithm KL-UCRL (Optimism in reinforcement learning and Kullback-Leibler divergence - S. Filippi, O. CappÃ©, A. Garivier), an algorithm based on the optimism principle, which was proven to have better empirical performance than UCRL2 on the riverSwim environment, with a regret in $\tilde O(|S|\sqrt{|A&T})$ as well; 
Minor comments: 
- l.99: it seems that the $s1$ should be replaced by a $si$; 
- l.192: "we how we can" -> how we can; 
- l.434: "markov" -> Markov; 
- references should be uniformed (Firstname Name, F. Name, etc) This is a technically strong, well-written and interesting paper.