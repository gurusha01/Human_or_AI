The authors study the problem of binary classification in the presence of random, class-conditional noise in the training data. They propose two approaches based on a suitable modification of a given surrogate loss function and derive performance bounds. More specifically, they provide guarantees for risk minimization of convex surrogates under random label noise in the general setting, and without any assumptions on the true distribution. Moreover, they provide two alternative approaches for modifying a given surrogate loss function. Experiments are presented on synthetic data and some benchmark datasets. 
The paper is well-written and appears to be technically sound. The theoretical results are non-trivial but not extremely profound either, especially in light of existing work on related setting. 
What could be called into question is the relevance of the setting. Is the assumption of having constant class noise in the training data indeed a realistic one? This question appears to be legitimate despite the theoretical nature of the paper.  The paper is well-written and appears to be technically sound. The theoretical results are non-trivial but not very profound either, especially in light of existing work on related setting. What could be called into question is the relevance of the setting.