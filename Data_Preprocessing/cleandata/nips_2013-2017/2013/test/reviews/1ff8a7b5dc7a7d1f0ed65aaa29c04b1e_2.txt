REPLY TO REBUTTAL: 
- I mostly agree with the one task comment. 
- It would still be worth trying truncated backprop with e.g., 30 steps. It is less likely to learn to balance parentheses (but it would be good to verify that), but it may still achieve competitive entropies at a fraction of the cost. 
The usefulness of deep recurrent neural networks is investigated. 
The paper reaches interesting conclusions: when holding the number 
of parameter constant, deeper recurrent neural networks outperform 
standard RNNs. Other interesting findings include an analysis 
that shows the deeper networks to contain more "long term information", 
and the result that plain BPTT can train character-level RNNs 
to balance parentheses, where previously it was thought to be possible 
only with HF. 
But it claims to introduce the deep RNN, but those been used before; 
for example, in http://www.cs.toronto.edu/~graves/icassp_2013.pdf, and likely 
much earlier than that. So the paper should not claim to have introduced this 
architecture. It is OK to not introduce new architectures and to focus 
on an analysis. 
The analysis, while meaningful, would be even more interesting if it 
were shown on several problems, perhaps something on speech. Otherwise 
the findings may be an artifact of the problem. 
Finally, the HF experiments were likely too expensive, since the 
truncated backprop approach (that was introduced in the 90) was 
successfully used by Mikolov (see his PhD thesis) to train RNNs 
to be excellent language models. Thus it is likely that truncated 
BPTT would do good here too, and it would be nice to obtain a 
confirmation of this fact.  This work presents an interesting analysis of deep RNNs. The two main results are: 1) deep RNNs outperform standard RNNs when the number of parameters is fixed on character-level language modelling; 2) the deeper layers exhibit more long-range structure; and 3) truncated BPTT can train character-level RNNs to balance parentheses.The results are interesting, but given that it is only an analysis, the paper would be a lot stronger if it had a similar analysis on some speech task (for example, to show that deeper RNNs do better, and to show that their deeper layers have more long range information)>