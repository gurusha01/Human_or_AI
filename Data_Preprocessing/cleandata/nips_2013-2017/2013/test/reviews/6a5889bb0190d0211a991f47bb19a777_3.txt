This paper provides the first regret bounds for a reinforcement learning algorithm that 1) maintains a posterior over the MDP, 2) samples one MDP from the prior, and 3) follows the policy that is optimal for that one MDP. This kind of "Posterior-Sampling Reinforcement Learning" (PSRL) algorithm has been proposed before, but without regret bounds. All algorithms with regret bounds are based on the principle of optimism in the face of uncertainty. This is the first regret bound analysis of an algorithm not based on this principle. This algorithm's bound is not quite as tight as those based on this principle, but in practice in typical cases it is substantially more exploitive, and appears to be much more efficient overall. 
If it is correct that this is the first regret result for such an algorithm (not based on optimism), and if this result is correct, then the paper should be accepted. 
The paper is written with admirable clarity. Nevertheless, I did not understand the proof in detail, so I may have missed errors. 
For simplicity, the work assumes an episodic formulation with fixed-length episodes. This is unrealistic, but is ok for a result that breaks new ground such as this one. This and other aspects of the algorithm that are undesirable in practice can (probably) be removed in future work if the main result shown here holds up. One related idea is discussed in a later section of the paper. This is good, but it could be omitted if it made room to include what is asked for in the paragraph below. 
The paper claims that the regret bound for its algorithm is close to the state of the art, but does not state what the state of the art is. It should do this, both as part of making its claim clear and allowing the reader to judge that the new algorithm's bound is in fact close. The paper should discuss the sense in which the new algorithm's bound is close. 
Please don't include citations as parts of sentences. 
 The first regret bound, and promising empirical results, for a reinforcement learning algorithm based on posterior sampling rather than optimism in the face of uncertainty.