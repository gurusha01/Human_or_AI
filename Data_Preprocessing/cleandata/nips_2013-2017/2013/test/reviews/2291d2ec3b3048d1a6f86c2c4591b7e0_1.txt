The goal of this work is to automatically discover latent domains in a training set, which is subsequently used in a domain adaptation framework to yield improved classification performance on a test set. The paper defines a function that measures the difference between two feature vectors over a specified kernel. The goal is to partition the data points into domains such that the function is maximized over the set of points across each pair of domains. The problem is formulated as an integer programming problem with two constraints: each point is assigned to exactly one domain and the distribution over class labels in each domain must match the input distribution over the entire point set. The problem is relaxed to a continuous optimization over a quadratic cost with linear constraints. Finally, the number of domains is found via cross validation. 
The approach is evaluated over two datasets: static images of [2] and the IXMAS multi-view action dataset of [15]. A highlight is that improved performance is shown over the domain adaptation approach of [19]. 
Positives: The paper is well-written and as far as I'm aware the approach is novel (although I'm not an expert on domain adaptation). The performance gains over [19] is also appreciated. 
Negatives: At this point I slightly lean towards reject. I have two main concerns that I would like to see addressed in the rebuttal that may convince me to change my score: 
(i) The motivation for this paper is not clear to me. On line 77 the paper argues that "simply clustering images by their appearance is prone to reshaping datasets into per-category domains". First, what is the evidence for this claim? Second, how does the model formulation in Section 2 overcome this issue, i.e. how is not reshaping into per-category domains enforced? 
(ii) Somewhat related, on line 154, why is the second constraint ("label prior constraint") needed? I'm curious what would the performance be without this constraint. In fact, a baseline where the data is partitioned using k-means clustering or unsupervised object discovery (e.g. Sivic et al ICCV '05) over the appearance vectors should be shown. Also, what is the performance when the dataset is randomly partitioned into equal sets? 
Some additional comments: 
+ Line 82, "maximally different in distribution from each other": This is mentioned throughout the paper. It would be good to clarify what this means. Distribution over what? 
+ Line 166: Doesn't \beta_{mk} as formulated already live on a simplex? 
+ Line 182: These seem to be different constraints than formulated before (starting on line 153), no? 
+ Line 215: What is the justification/proof for this bound? 
+ Line 289: Please provide more details on the use of the geodesic flow kernel [4]. Is this a reimplementation or was publicly available source code used? 
+ Lines 313/340: Please provide some insights into the differences in performance. I want to better understand why the proposed approach is performing better. It would be good to show systematic failures of the baselines that the proposed approach overcomes. 
+ Eq (1): M'k => Mk' 
+ This citation may be good to include as well: 
Unbiased Look at Dataset Bias A. Torralba, A. Efros. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011. The rebuttal addressed my concerns regarding the paper motivation and the label prior constraint. I lean slightly towards accept.