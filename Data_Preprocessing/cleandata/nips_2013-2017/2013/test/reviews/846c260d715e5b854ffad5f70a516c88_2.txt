This paper provides a novel addition to the large and still-growing body of MCTS research. The idea is to represent the distribution over accumulated rewards in the search tree as a mixture of Gaussians by using a Dirichlet as a prior on mixture weights, and a NormalGamma as a prior for the mixtures themselves. The authors justify (using the central limit theorem) their assumptions of normality and give a brief overview of setting the hyperparameters of the priors. This Bayesian formulation is combined with Thompson sampling to provide a nice version of MCTS that seems to avoid the more heuristic nature of UCB and be more robust in its explore/exploit tradeoff. 
The structure and presentation of the paper are good, and the ideas and math are presented clearly. There are language issues throughout, but not enough to make it unintelligible (although these should be cleaned up -- especially in the introduction). The paper does a good job of introducing the requisite concepts, stating and justifying the underlying modeling assumptions, presenting the model, and then putting it all together -- though a more descriptive comparison between Thompson sampling and UCB would be good, especially their specific assumptions and strengths/weaknesses. 
The empirical evaluation is solid; however, the authors should include a more thorough comparison of computational complexity with UCT (with numbers / figures). Clearly, this algorithm is more complex than UCT and so will run slower, but how much slower? Seconds? Hours? Days? One of the reasons UCT is so effective is that it's extremely fast and enables much more sampling in the same amount of time as more complicated methods, thus providing more accurate empirical estimates, even if they're not computed as cleverly. Also, I'm curious how much benefit came from just Thompson sampling and how much from using the Dirichlet-NormalGamma distribution versus vanilla UCT. Is there some way to quantify these gains separately? 
A question I have, regarding the assumptions made, is that these claims of convergence to a Normal rely on the fact that the future policy is fixed and thus the rollout is a Markov chain. However, as I understand it, this policy is not fixed and changes over time (either through learning or exploratory randomness). Doesn't this invalidate the claims made? (though, empirically, it seems like the assumptions aren't that harmful) This is a solid paper that presents a small novel improvement to Monte carlo tree search in the form of using Bayesian mixture modeling to represent the accumulated reward of actions in an MDP. The idea is solid, explained well, and empirically validated but the paper has minor issues in terms of language, computational complexity, and, possibly, the validity of certain assumptions.