The authors empirically show that an existing approach that explains an invariance model for visual recognition system can be extended beyond affine transformation, such as out-of-plane rotation, illumination, and most importantly, altered background clutter. Given that these transformations are major hurdles for building invariant descriptors for objects in imagery, this work is quite interesting since the authors explicitly handled them, one by one, in the framework introduced in [1]. 
The paper heavily rely on [1], which requires significant elaboration to clearly understand. The paper is very clearly written and quite original and provide good theoretical backgrounds for various aspect for feature design, such as pooling methods. In addition to its original contribution of empirically showing the approach in [1] can be extended to various transformation, I think this work will give computer vision researchers, especially who focus on designing/learning representation, a principled guideline regarding to necessary (or sufficient) properties for invariant features.