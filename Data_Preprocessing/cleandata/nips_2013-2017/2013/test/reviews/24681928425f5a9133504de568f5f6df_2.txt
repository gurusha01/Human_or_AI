This paper presents a method for learning the structure as well as the parameters of stochastic AND-OR grammars. Such grammars contain AND rules and OR rules and can be used to represent several recursive phenomena, including natural language and even grammars. The authors present a nice method for unsupervised structure learning of these grammars by introducing new AND-OR fragments at consecutive steps, and measuring the likelihood and prior gains of their model. 
The authors present experiments on two tasks: learning event grammars and learning image grammars. In both they achieve results that are competitive with prior art. 
I liked the overall paper as it seems to be a tractable way of learning stochastic grammars that can be modeled using AND and OR rules. My criticism of the paper stems from the following observations: 
1) The authors do not mention how tractable the learning algorithm is. Will it scale to thousands of datapoints? 
2) I would have liked to seen experiments on natural language sentences as natural language is the most obvious application of such grammars. Will it be even possible to learn using the presented methods on the Penn Treebank dataset for example, on which previous work has focused on (say, Klein and Manning)? This paper presents a way of estimating the structure and parameters of stochastic AND-OR grammars and presents nice results on two tasks; I would have liked to see more experiments, especially on natural language data.