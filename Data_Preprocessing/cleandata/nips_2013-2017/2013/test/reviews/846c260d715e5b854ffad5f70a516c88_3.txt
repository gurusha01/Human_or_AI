The authors present a Monte-Carlo tree search algorithm for online planning in MDPs with unknown transition probabilities. They use Thompson sampling, which is known to be a randomized Bayesian algorithm to minimize regret in the multi-armed stochastic bandit problems, for the action selection to expand the search tree. They also present the probability of the return X{s,\pi(s)} and X{s,a} as a normal distribution and a mixture of normal distributions respectively in order to use Thompson sampling, which chooses an action according to the posterior probability of being the best action. 
I think it is a good idea to exploit Thompson sampling in Monte-Carlo tree search since some recent studies have empirically showed that Thompson sampling outperforms a popular alternative, UCB. The problem of using Thompson sampling in MDPs is modeling the probability of the returns in the Bayesian setting, which is addressed in this paper. However I have a few questions. 
(1) I think it is ok to present the distribution of the return X{s,\pi(s)} as a normal distribution when X{s,\pi(s)} denotes the leaf node in the search tree and \pi is the roll-out policy. However, the other nodes in the upper levels do not satisfy the authors' claim since the policy changes. 
(2) The authors say that "Thompson sampling converges to find the optimal action with probability 1 by treating each decision node in the tree as MAB" in lines 263-264, but I cannot believe this statement. The MAB problems in the decision nodes except for the leaf nodes seem to be non-stationary because the policies change as the returns X_{s,\pi(s)} change. As far as I know, there is no convergence guarantee of Thompson sampling in non-stationary MAB problems. Therefore, I am not sure that the policy found by DNG-MCTS converges to the optimal one. 
(3) Does the word "iteration" in the experiments section (lines 315-316, 354-355, 368) denote the iteration of DNG-MCTS in the function "OnlinePlanning" in figure 1? 
(4) In the last paragraph of the experiments section, the authors say that DNG-MCTS requires more computation time than UCT. It would be good to report the CPU running time. 
Minor comments: There are some errors in the references. Some papers such as [7, 8, 15, 18] have been already published in some conferences rather than arXiv. 
Quality: I think this paper has a few flaws. 
Clarity: The paper is well written as easy to follow. 
Originality: The paper is original to my knowledge. 
Significance: The main idea of the paper is quite interest to the RL community but I think the authors should clarify a few flaws. 
----- 
The original central limit theorem (CLT) for Markov chains is \sqrt{n}(n^(-1) \sum{t=0}^n f(st) - E[f(st)]) \rightarrow N(0,\sigma^2) as n \rightarrow \infty. It says that the empirical mean of f(st) converges to the true expectation of f(st). In Section 3.1, the authors slightly modified the CLT and showed that \frac{1}{\sqrt{n}}(\sum{t=0}^n f(st) - n\mu) \rightarrow N(0,\sigma^2). As the reviewer5 already pointed out, I think that the sum of f(st) follows N(n\mu, n\sigma^2) and the sum of f(s_t) diverges as n goes to infinity. Thus it is difficult for me to accept that the CLT justifies the Normal assumption on the sum of rewards although the assumption seems to be practically reasonable. 
 The authors provide a novel MCTS algorithm by using Thompson sampling, which is quite interest to the RL community, but the soundness is somewhat lacking.