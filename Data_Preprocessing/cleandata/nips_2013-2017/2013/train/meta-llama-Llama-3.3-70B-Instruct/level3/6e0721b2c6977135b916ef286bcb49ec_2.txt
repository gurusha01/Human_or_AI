This paper introduces a novel approach to reducing the number of parameters in deep neural networks by exploiting the structure in the weights. The authors propose a technique for predicting the weights of a network using a low-rank approximation of the weight matrix, which is achieved by representing the weight matrix as the product of two smaller matrices. The key idea is to carefully construct one of the factors, while learning only the other factor, to achieve a substantial reduction in the number of parameters without compromising the performance of the network.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the proposed technique. The introduction provides a clear motivation for the work, highlighting the limitations of current approaches to distributed learning and the potential benefits of reducing the number of parameters in deep networks. The authors also provide a thorough review of related work, highlighting the differences between their approach and existing methods for reducing the number of parameters.
The technical contributions of the paper are significant, and the authors provide a detailed analysis of the proposed technique, including a discussion of the choice of dictionary and the interpretation of the predicted weights. The experimental results are impressive, demonstrating that the proposed technique can achieve significant reductions in the number of parameters without compromising the performance of the network.
However, there are some areas where the paper could be improved. The authors could provide more insight into the choice of dictionary and the selection of the indices α, which are crucial to the success of the proposed technique. Additionally, the authors could provide more discussion on the potential applications of the proposed technique, beyond the specific examples presented in the paper.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, and the authors provide a clear and well-organized presentation of their ideas. The proposed technique is novel and significant, and the authors provide a thorough analysis of the related work. The paper is well-written, and the authors provide enough information for the expert reader to reproduce the results.
Arguments pro acceptance:
* The paper introduces a novel and significant technique for reducing the number of parameters in deep neural networks.
* The proposed technique is technically sound, and the authors provide a clear and well-organized presentation of their ideas.
* The experimental results are impressive, demonstrating that the proposed technique can achieve significant reductions in the number of parameters without compromising the performance of the network.
Arguments con acceptance:
* The paper could benefit from more insight into the choice of dictionary and the selection of the indices α.
* The authors could provide more discussion on the potential applications of the proposed technique, beyond the specific examples presented in the paper.
* Some of the related work could be discussed in more detail, to provide a clearer understanding of the differences between the proposed technique and existing methods.