This paper presents a significant contribution to the reinforcement learning (RL) community by applying a previously published algorithm, classification-based modified policy iteration (CBMPI), to solve the game of Tetris with comparable performance to black-box optimization techniques. The empirical results are convincing, demonstrating that CBMPI outperforms other approximate dynamic programming (ADP) algorithms and achieves similar results to the cross-entropy (CE) method, but with considerably fewer samples.
The paper is well-written and easy to follow, providing a clear overview of the problem, the proposed approach, and the experimental results. However, there are some minor clarity issues, such as confusing definitions and inconsistent terminology, which could be improved.
One of the strengths of the paper is its ability to provide insights into applying RL algorithms to non-toy domains, such as Tetris. The authors' conjecture that Tetris is a game whose policy space is easier to represent and search than its value function space is interesting and well-supported by the experimental results.
The choice of optimization algorithm, CMA-ES, is also noteworthy, and it would be interesting to explore the use of other types of optimizers, such as gradient-based optimizers, to see how they compare.
Some potential areas for improvement include providing more detailed discussions of the results, including the influence of the state distribution used for sampling rollout states, and sharing the code used in the experiments to facilitate reproducibility. Additionally, adding confidence intervals to the graphs would help to provide a clearer understanding of the results.
Overall, the paper presents a significant contribution to the RL community, and with some minor improvements, it has the potential to be a strong paper. The arguments for acceptance include:
* The paper presents a novel application of CBMPI to Tetris, achieving comparable performance to state-of-the-art methods.
* The empirical results are convincing, demonstrating the effectiveness of CBMPI in both small and large boards.
* The paper provides insights into applying RL algorithms to non-toy domains, such as Tetris.
The arguments against acceptance include:
* The paper lacks a detailed discussion on the reasons for the improvement in performance, including the influence of the state distribution used for sampling rollout states.
* The paper could benefit from more detailed discussions of the results and additional experiments to explore the use of other types of optimizers.
* The code used in the experiments is not shared, which could limit reproducibility.