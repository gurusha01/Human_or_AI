This paper proposes a novel approach to reducing the number of parameters in deep neural networks by exploiting the structure of the weights. The authors extend the work of Schwing et al. to include active learning protocols using entropy of local variable marginals to estimate uncertainty. Two active learning variants are proposed: separate active and joint active, with joint active performing the best and achieving 90% annotation savings.
The empirical evaluation demonstrates the effectiveness of joint active learning, batch mode, and querying partial labels, with sensitivity to epsilon and computational reuse saving time. The paper is of high quality, with a state-of-the-art model for structured learning with latent variables and active learning method achieving dramatic savings over random sampling.
However, I raise concerns about the realism of the simulations and the lack of discussion on the costs of active learning, as well as some semantic issues with partial labels. The paper is generally clear, but could benefit from more explanation of the task-loss function, notation, and graphical models expertise.
The work is considered original, but not groundbreaking, and is likely to be useful but not have a significant impact. The paper provides a thorough analysis of the proposed approach and its applications, but it would be beneficial to see more comparisons with existing methods and a more detailed discussion of the limitations and potential extensions of the approach.
Some minor comments on the abstract, notation, and wording throughout the paper are also provided. Overall, the paper is well-written and provides a valuable contribution to the field of deep learning, but could benefit from some revisions to address the concerns mentioned above.
Arguments pro acceptance:
- The paper proposes a novel approach to reducing the number of parameters in deep neural networks.
- The empirical evaluation demonstrates the effectiveness of the proposed approach.
- The paper is well-written and provides a thorough analysis of the proposed approach and its applications.
Arguments con acceptance:
- The paper lacks a detailed discussion of the costs of active learning.
- The simulations may not be realistic.
- The paper could benefit from more comparisons with existing methods and a more detailed discussion of the limitations and potential extensions of the approach.