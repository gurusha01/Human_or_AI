This paper explores the idea of reducing the number of parameters in deep neural networks by exploiting the structure in the weights. The authors propose a technique for predicting the weights of a network using a low-rank approximation of the weight matrix, and demonstrate its effectiveness on several benchmark tasks. The paper is well-motivated, and the idea of reducing the number of parameters in neural networks is an important one, particularly in the context of distributed training.
The paper's strengths include its clear explanation of the motivation behind the technique, and its thorough experimental evaluation on several benchmark tasks. The authors also provide a detailed analysis of the trade-offs between different approaches to constructing the dictionary, and demonstrate the effectiveness of their technique in reducing the number of dynamic parameters.
However, the paper also has some weaknesses. The empirical results do not always support the central claims of the paper, and the authors could benefit from more comparisons with common techniques such as PCA preprocessing. Additionally, the paper's quality is impacted by unclear explanations, missing intuition, and a lack of extensive empirical follow-through, particularly in the discussion of columnar architectures and convolutional nets.
The technique of maintaining a PCA of intermediate layers is a conceptual starting point, but requires more work and empirical justification. The claim that parallel training can be accelerated by the proposed techniques requires more justification and empirical evidence, including the implementation and testing of encoder/decoder algorithms.
The paper lacks clarity in certain sections, including the misuse of terms, poor equation numbering, and unclear figure placement. However, the idea of looking at weight matrices as a continuous function has precedent, but the idea of accelerating training by communicating a small fraction of matrix values is new and original.
Overall, the paper has significant potential for researchers trying to parallelize neural network training across low-bandwidth channels, but the empirical results do not support the most important claims. With further refinement and experimentation, this technique could be a valuable contribution to the field of deep learning.
Arguments pro acceptance:
* The paper explores a novel and important idea in deep learning
* The technique has the potential to significantly reduce the number of parameters in neural networks
* The paper provides a thorough experimental evaluation on several benchmark tasks
Arguments con acceptance:
* The empirical results do not always support the central claims of the paper
* The paper lacks clarity in certain sections
* The technique requires more work and empirical justification to be fully convincing.