This paper introduces the GreeDi algorithm, a distributed protocol for maximizing monotone submodular functions in a MapReduce environment. The algorithm is designed to work in a distributed system, where the data is partitioned across multiple machines, and each machine processes its local data in parallel. The authors provide theoretical guarantees for the performance of GreeDi, showing that it can achieve a solution close to the centralized greedy algorithm, which is known to be optimal for submodular maximization.
The paper is well-written, and the authors provide a clear explanation of the algorithm and its theoretical guarantees. The experimental results demonstrate the effectiveness of GreeDi on several applications, including exemplar-based clustering and active set selection, and show that it can scale to large datasets.
One of the strengths of the paper is its ability to handle decomposable submodular functions, which are common in many machine learning applications. The authors provide a theoretical analysis of the performance of GreeDi on such functions, and demonstrate its effectiveness in practice.
However, there are some questions and concerns that need to be addressed. One of the main concerns is the communication cost between mappers and reducers, including the amount of data that needs to be shuffled. The authors do not provide a clear analysis of this cost, which could be a significant bottleneck in practice. Additionally, the maximum number of items that can be reduced to a single key is not clear, which could affect the performance of the algorithm.
Another question is the number of iterations needed in the worst case for the GreeDi algorithm to converge. The authors provide some theoretical guarantees, but it is not clear how these guarantees translate to practice.
Overall, the paper provides a significant contribution to the field of distributed optimization, and the GreeDi algorithm has the potential to be widely used in practice. However, further analysis and experimentation are needed to fully understand its performance and limitations.
Arguments pro acceptance:
* The paper introduces a novel distributed protocol for submodular maximization, which is an important problem in machine learning.
* The authors provide theoretical guarantees for the performance of the algorithm, which is a significant contribution to the field.
* The experimental results demonstrate the effectiveness of the algorithm on several applications, and show that it can scale to large datasets.
Arguments con acceptance:
* The paper lacks a clear analysis of the communication cost between mappers and reducers, which could be a significant bottleneck in practice.
* The maximum number of items that can be reduced to a single key is not clear, which could affect the performance of the algorithm.
* The number of iterations needed in the worst case for the GreeDi algorithm to converge is not clear, which could affect its performance in practice.