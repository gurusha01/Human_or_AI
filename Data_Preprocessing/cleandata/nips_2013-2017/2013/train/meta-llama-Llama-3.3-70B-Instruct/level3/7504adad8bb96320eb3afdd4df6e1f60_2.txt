This paper presents an empirical analysis of the application of a classifier-based approximate policy iteration algorithm, called CBMPI, to the game of Tetris. The authors compare the performance of CBMPI with other methods, including DPI, lambda-PI, and CE, and demonstrate that CBMPI achieves state-of-the-art results in both small and large Tetris boards.
The paper's main strength lies in its empirical analysis, which provides valuable insights into the performance of CBMPI in Tetris. The authors' efforts to compare CBMPI with other methods and to analyze the effect of different parameters and features on its performance are commendable. The results show that CBMPI outperforms other ADP algorithms, including lambda-PI, and achieves comparable performance to CE, a state-of-the-art method in Tetris, while using significantly fewer samples.
However, the paper has several weaknesses. Firstly, the authors make no theoretical or algorithmic contributions, and the paper's significance relies solely on its empirical analysis. Secondly, the authors' hypothesis that policies may be easier to represent than value functions in Tetris is not adequately supported, and the paper's discussion of this topic is limited. Furthermore, the experimental comparisons are plagued by confounding factors, such as the use of different state features and prior knowledge, which undermine the validity of the conclusions drawn from them.
Additionally, the paper raises concerns about the fairness of the comparisons between CBMPI and other methods. For example, CBMPI uses a generative model, while lambda-PI and CE do not. Moreover, CBMPI uses additional features that are not used by other methods, which may give it an unfair advantage. The paper's results are also affected by the choice of features, and the authors do not provide a thorough analysis of the impact of feature selection on the performance of CBMPI.
Another concern is the lack of variance analysis in the paper's results, which makes it difficult to assess the statistical significance of the performance differences observed. Finally, the paper's conclusion that CBMPI achieves equal performance with fewer samples than CE is misleading, as CBMPI does not equal CE's final performance and is only faster in the short term.
In terms of quality, the paper is well-written and easy to follow, but it lacks clarity in some sections, particularly in the description of the algorithms and the experimental setup. The paper's originality is limited, as it applies an existing algorithm to a well-studied benchmark task. However, the paper's significance lies in its empirical analysis, which provides new insights into the performance of CBMPI in Tetris.
Overall, I would rate this paper as follows:
* Quality: 7/10
* Clarity: 8/10
* Originality: 6/10
* Significance: 8/10
I would recommend accepting this paper, but with revisions to address the concerns mentioned above, particularly with regards to the fairness of the comparisons and the lack of variance analysis. Additionally, the authors should provide a more thorough analysis of the impact of feature selection on the performance of CBMPI and clarify the description of the algorithms and the experimental setup. 
Arguments for acceptance:
- The paper provides a thorough empirical analysis of CBMPI in Tetris.
- The results show that CBMPI achieves state-of-the-art performance in both small and large Tetris boards.
- The paper provides new insights into the performance of CBMPI in Tetris.
Arguments against acceptance:
- The paper makes no theoretical or algorithmic contributions.
- The experimental comparisons are plagued by confounding factors.
- The paper lacks variance analysis and a thorough analysis of the impact of feature selection on the performance of CBMPI.