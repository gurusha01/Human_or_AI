This paper proposes a novel approach to approximate inference in high-treewidth graphical models by projecting the intractable distribution onto a "fast-mixing" set, where Gibbs sampling is guaranteed to converge rapidly to the stationary distribution. The authors derive a dual algorithm to solve the projection under several divergences, including Euclidean distance, piecewise KL-divergence, and reversed KL-divergence. The experimental results show that the proposed methods are more accurate than variational approximations and Gibbs sampling on the original parameters with finite time.
The paper is well-written and easy to follow, with a clear explanation of the background and related work. The authors provide a thorough discussion of the limitations and potential extensions of their approach. The use of a simple condition to guarantee rapid mixing, namely that the spectral norm of the matrix of interaction strengths is less than one, is a key contribution of the paper.
However, there are some limitations and potential areas for improvement. The spectral norm bound is somewhat conservative, and a tighter bound on when rapid mixing will occur would be more informative. Additionally, the authors assume that the graph structure is known, which may not always be the case in practice. The use of a stochastic gradient descent algorithm to minimize the zero-forcing KL-divergence may also require careful tuning of parameters to achieve good performance.
The paper raises several interesting questions and potential directions for future work. For example, can the approach be extended to general Markov random fields, and how can the parameters be learned using fast-mixing distributions? The authors also mention the potential use of fast-mixing distributions for learning, which could be an interesting area of exploration.
Overall, the paper presents a novel and promising approach to approximate inference in high-treewidth graphical models, with potential applications in a variety of fields. While there are some limitations and areas for improvement, the paper provides a solid foundation for future research and development.
Arguments pro acceptance:
* The paper proposes a novel approach to approximate inference in high-treewidth graphical models.
* The authors derive a dual algorithm to solve the projection under several divergences.
* The experimental results show that the proposed methods are more accurate than variational approximations and Gibbs sampling on the original parameters with finite time.
* The paper is well-written and easy to follow, with a clear explanation of the background and related work.
Arguments con acceptance:
* The spectral norm bound is somewhat conservative, and a tighter bound on when rapid mixing will occur would be more informative.
* The authors assume that the graph structure is known, which may not always be the case in practice.
* The use of a stochastic gradient descent algorithm to minimize the zero-forcing KL-divergence may require careful tuning of parameters to achieve good performance.