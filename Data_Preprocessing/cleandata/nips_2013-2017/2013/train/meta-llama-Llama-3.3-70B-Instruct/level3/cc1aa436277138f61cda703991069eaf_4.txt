This paper explores the problem of aggregating low-quality answers from crowds to obtain accurate results, focusing on the number of control examples required for optimal results. The authors evaluate two estimation strategies, two-stage estimation and joint estimation, using a Gaussian model with worker ability parameters to derive optimal numbers of control items. 
The paper's main strength lies in its thorough analysis of the trade-off between the empirical error and the Rademacher complexity of the class of classifiers. The authors provide a data-dependent bound on the generalization error of a top-down multiclass hierarchical classifier, which sheds light on the trade-off between flat versus hierarchical classification. The bound suggests that hierarchical classifiers are well-suited for unbalanced, large-scale taxonomies, while flat classifiers are preferred for well-balanced taxonomies.
However, the analysis assumes uniform task assignments to workers, which may not be realistic in most crowdsourcing situations. This limitation potentially restricts the method's applicability. Furthermore, the paper lacks an evaluation of the method's robustness to non-uniform task assignments and does not discuss extensions to discrete values.
The authors also fail to mention existing work on incorporating control items into statistical quality control, such as Tang & Lease and Kajino & Kashima. This omission is notable, as these studies may have informed the development of the authors' approach.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, and the organization is logical. The authors provide sufficient information for an expert reader to reproduce the results.
Regarding originality, the paper presents a novel combination of familiar techniques, and the application of the Rademacher complexity to hierarchical classification is new. The authors adequately reference related work, and the paper's contributions are clearly distinguished from existing research.
The significance of the paper lies in its potential to advance the state of the art in large-scale taxonomy classification. The authors' findings have implications for the design of crowdsourcing systems and the development of more accurate classification methods. The paper provides a unique perspective on the trade-off between flat and hierarchical classification, and the results are likely to be of interest to researchers and practitioners in the field.
Arguments pro acceptance:
* The paper presents a thorough analysis of the trade-off between empirical error and Rademacher complexity.
* The authors provide a data-dependent bound on the generalization error of a top-down multiclass hierarchical classifier.
* The paper has the potential to advance the state of the art in large-scale taxonomy classification.
Arguments con acceptance:
* The analysis assumes uniform task assignments to workers, which may not be realistic.
* The paper lacks an evaluation of the method's robustness to non-uniform task assignments.
* The authors fail to mention existing work on incorporating control items into statistical quality control.
Overall, the paper is well-written, and the authors' contributions are significant. While there are some limitations and omissions, the paper's strengths outweigh its weaknesses, and it is a valuable addition to the field.