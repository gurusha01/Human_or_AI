This paper introduces a novel approach to sparse precision matrix estimation in high dimensions using the CLIME estimator. The authors propose an inexact alternating direction method of multipliers (ADMM) algorithm for CLIME, which can scale up to millions of dimensions and use hundreds of cores in a shared-memory or distributed-memory architecture. The algorithm is shown to have a convergence rate of O(1/T) for both the objective function and the residuals of optimality conditions.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed algorithm and its theoretical guarantees. The experimental results demonstrate the scalability and efficiency of the proposed algorithm, outperforming state-of-the-art methods on both synthetic and real datasets.
One of the strengths of the paper is its ability to leverage the sparse and low-rank structure of the problem to reduce computational complexity. The authors propose a block cyclic data distribution scheme, which can achieve load balance and scalability in parallel computation. The use of low-rank multiplications and sparse matrix operations can significantly reduce the computational cost of matrix multiplication.
However, there are some minor issues with the paper. The authors claim that their method is the first major advance in 7 years, but there is a lack of more recent references to support this claim. Additionally, the paper could benefit from a comparison with an "outside" approach, such as k-means for clustering, to demonstrate the strengths and weaknesses of the proposed algorithm.
The paper also has some minor grammar issues, such as incorrect use of "proximum" and "allow to" constructions. Furthermore, the optimality of the clustering approach is not thoroughly discussed, and alternative methods could be considered.
In terms of originality, the paper presents a novel combination of existing techniques, including ADMM and block cyclic data distribution. The authors demonstrate the effectiveness of their approach in solving large-scale sparse precision matrix estimation problems, which is a significant contribution to the field.
The significance of the paper lies in its ability to scale up machine learning algorithms to "Big Data" settings. The proposed framework can be applied to a variety of large-scale constrained optimization problems, making it a valuable contribution to the field.
Overall, the paper is well-written, and the authors provide a clear and concise explanation of the proposed algorithm and its theoretical guarantees. The experimental results demonstrate the scalability and efficiency of the proposed algorithm, making it a strong contribution to the field.
Arguments for acceptance:
* The paper presents a novel approach to sparse precision matrix estimation in high dimensions using the CLIME estimator.
* The proposed algorithm is shown to have a convergence rate of O(1/T) for both the objective function and the residuals of optimality conditions.
* The experimental results demonstrate the scalability and efficiency of the proposed algorithm, outperforming state-of-the-art methods on both synthetic and real datasets.
* The paper leverages the sparse and low-rank structure of the problem to reduce computational complexity.
Arguments against acceptance:
* The paper lacks more recent references to support the claim that their method is the first major advance in 7 years.
* The paper could benefit from a comparison with an "outside" approach, such as k-means for clustering, to demonstrate the strengths and weaknesses of the proposed algorithm.
* The optimality of the clustering approach is not thoroughly discussed, and alternative methods could be considered.
* The paper has some minor grammar issues, such as incorrect use of "proximum" and "allow to" constructions.