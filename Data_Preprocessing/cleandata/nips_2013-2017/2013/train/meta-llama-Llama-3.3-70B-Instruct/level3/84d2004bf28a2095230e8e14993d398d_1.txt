This paper presents a novel distributed protocol, GREEDI, for maximizing a submodular function subject to cardinality constraints. The authors provide a thorough theoretical analysis of the protocol's performance and demonstrate its effectiveness through extensive large-scale experiments using Hadoop.
The paper's main strengths lie in its ability to tackle large-scale submodular optimization problems, which are common in machine learning applications. The GREEDI protocol is shown to perform well compared to naive distributed approaches, and its scalability is demonstrated on massive datasets. The authors also provide a detailed analysis of the protocol's performance under various conditions, including the presence of geometric structure in the data.
However, there are some areas that require improvement. The notation in Theorem 4.2 is confusing, with kappa and k being hard to distinguish, making it difficult to read and understand. Additionally, the bound in Theorem 4.2 lacks commentary on its tightness, particularly with regards to the factor min(m,k) inside the bound.
The experiments should report generalized performance metrics, such as negative log predictive probability, to provide insight into the algorithm's effect on generalization. It is also unclear which experiments in Section 5 handle decomposable functions, requiring further clarification.
Furthermore, there are some errors in the figures. Figure 1(e) has a labeling error on the x-axis, which should be labeled as k instead. Figure 1(f) shows an inconsistent ratio at the smallest m, starting below 1, unlike Figures 1(a)-1(d) which start at 1. The dip in Figure 1(c) when k=10 requires explanation to understand the underlying cause.
To improve the paper, the authors could add references to related research, such as the Today Module of Yahoo!, to provide readers with further context and opportunities for research. Overall, the paper presents a significant contribution to the field of distributed submodular optimization, and with some revisions, it has the potential to be a strong publication.
Arguments pro acceptance:
- The paper presents a novel and efficient distributed protocol for submodular optimization.
- The protocol is shown to perform well compared to naive distributed approaches.
- The paper provides a thorough theoretical analysis of the protocol's performance.
- The experiments demonstrate the scalability of the protocol on massive datasets.
Arguments con acceptance:
- The notation in Theorem 4.2 is confusing and requires clarification.
- The bound in Theorem 4.2 lacks commentary on its tightness.
- The experiments should report generalized performance metrics.
- There are errors in the figures that require correction.
- The paper could benefit from additional references to related research.