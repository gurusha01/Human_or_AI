This paper presents a significant contribution to the field of machine learning by establishing the linear convergence of the proximal gradient method (PGM) for solving trace norm-regularized problems. The authors prove that the PGM achieves a linear convergence rate without requiring strong convexity of the loss function, which is a common assumption in many existing convergence results.
The paper is well-written and follows a clear structure, starting with an introduction to the problem and its applications, followed by a detailed presentation of the main results and their proofs. The authors use a Lipschitzian error bound technique, which is a powerful tool for analyzing the convergence of first-order methods.
One of the strengths of the paper is its ability to provide a linear convergence result for a large class of loss functions, including those that are not strongly convex. This makes the result more applicable to real-world problems, where the loss function may not have a strong convexity property.
However, the paper could benefit from a clearer comparison to the vector case of l1-norm regularization. The authors mention that their result is the first to establish linear convergence for trace norm-regularized problems without strong convexity, but it would be helpful to discuss how their result relates to existing results for l1-norm regularization.
Additionally, the paper has some minor issues, such as notation inconsistencies and broken references, which need to be addressed for clarity. The authors could also provide more discussion and conclusions to balance the technical proof of Lemma 3.2 in Section 3.
In terms of originality, the paper presents a new result that advances the state of the art in the field. The authors' use of a Lipschitzian error bound technique is novel and provides a new perspective on the convergence of first-order methods.
The significance of the paper lies in its ability to provide a linear convergence result for a large class of loss functions, which makes it more applicable to real-world problems. The result has the potential to impact the field of machine learning, particularly in areas such as matrix completion and matrix classification.
Overall, I would recommend accepting this paper, but with some revisions to address the minor issues and provide more discussion and conclusions.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of machine learning by establishing the linear convergence of the PGM for solving trace norm-regularized problems.
* The result is novel and advances the state of the art in the field.
* The paper has the potential to impact the field of machine learning, particularly in areas such as matrix completion and matrix classification.
Arguments con acceptance:
* The paper could benefit from a clearer comparison to the vector case of l1-norm regularization.
* The paper has some minor issues, such as notation inconsistencies and broken references, which need to be addressed for clarity.
* The authors could provide more discussion and conclusions to balance the technical proof of Lemma 3.2 in Section 3.