This paper proposes a novel approach to decision tree learning, called decision jungles, which are ensembles of rooted decision directed acyclic graphs (DAGs). The authors argue that traditional decision trees face a fundamental limitation, namely exponential growth with depth, which can be problematic for memory-constrained hardware. Decision jungles address this issue by allowing multiple paths from the root to each leaf, resulting in a more compact and powerful discriminative model.
The paper presents a thorough analysis and evaluation of the method, comparing it to competing methods on multiple datasets. The authors propose two local search-based algorithms for learning the DAGs, LSearch and ClusterSearch, and demonstrate that the simpler LSearch algorithm is more effective. The experiments show that decision jungles can reduce memory consumption while improving generalization, and that the merging of nodes has a regularizing effect on the model.
The paper is well-written, and the authors provide sufficient references to related work. The approach is a logical extension of previous work on decision trees and DAGs, and the authors clearly explain the advantages of their method. The use of multiple approximations, including LSearch and ClusterSearch, contributes to the method's efficiency, but also raises questions about potential limitations and failure cases.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem of decision tree learning and the limitations of traditional approaches
* A thorough evaluation of the method on multiple datasets, including comparisons to competing methods
* A novel and efficient approach to learning DAGs, with a clear explanation of the algorithms and their advantages
The weaknesses of the paper include:
* The potential limitations and failure cases of the method are not fully explored, and the authors could provide more discussion on the potential risks and challenges of using decision jungles
* The relationship between the merging of nodes and the regularizing effect on the model could be further explored, and the authors could provide more insight into the underlying mechanisms
Overall, the paper presents a significant contribution to the field of decision tree learning, and the authors demonstrate the effectiveness of their approach on a range of datasets. The paper is well-written, and the authors provide a clear and thorough evaluation of their method.
Arguments pro acceptance:
* The paper presents a novel and efficient approach to decision tree learning, with a clear explanation of the algorithms and their advantages
* The authors demonstrate the effectiveness of their approach on a range of datasets, including comparisons to competing methods
* The paper is well-written, and the authors provide sufficient references to related work
Arguments con acceptance:
* The potential limitations and failure cases of the method are not fully explored, and the authors could provide more discussion on the potential risks and challenges of using decision jungles
* The relationship between the merging of nodes and the regularizing effect on the model could be further explored, and the authors could provide more insight into the underlying mechanisms.