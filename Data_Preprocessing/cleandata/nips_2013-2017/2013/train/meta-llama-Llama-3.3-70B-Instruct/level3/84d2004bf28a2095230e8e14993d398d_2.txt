This paper addresses the important problem of submodular function maximization in a distributed framework, providing a simple and practical algorithm called GREEDI. The algorithm is designed to work in a MapReduce style parallel computation model, making it suitable for large-scale machine learning applications. The paper presents a thorough theoretical analysis of the algorithm's performance, including bounds on its approximation ratio and running time. The experimental results demonstrate the effectiveness of GREEDI on several applications, including sparse Gaussian process inference and exemplar-based clustering, on tens of millions of data points using Hadoop.
The strengths of the paper include its clear presentation, well-organized structure, and thorough analysis of the algorithm's performance. The authors provide a detailed discussion of the problem, related work, and the intuition behind the GREEDI algorithm. The theoretical analysis is rigorous and provides valuable insights into the algorithm's behavior. The experimental results are impressive, demonstrating the scalability and effectiveness of GREEDI on large-scale datasets.
However, there are some weaknesses in the paper. The experimental section is too concise, lacking explanations of baseline comparison methods and results, and omitting discussion of run-time or CPU cost. The idea of lazy evaluation is not adequately discussed, despite its potential to significantly improve efficiency and scalability. Additionally, the paper lacks clarity on what is meant by "fit on one machine", specifically regarding RAM, disk, or CPU processing limitations.
To improve the paper, I suggest cutting certain sections, such as section 6, pseudo-code for algorithm 1, and paragraphs 2 and 3 of section 3.2, to improve the paper's conciseness. The authors should also provide more detailed explanations of the experimental setup, including the baseline methods and results, and discuss the run-time and CPU cost of the algorithm.
Overall, I believe that this paper makes a significant contribution to the field of submodular optimization and distributed machine learning. The GREEDI algorithm has the potential to be widely adopted in large-scale machine learning applications, and the paper's theoretical analysis and experimental results provide a solid foundation for its use.
Arguments pro acceptance:
* The paper addresses an important problem in machine learning and provides a novel solution.
* The algorithm is simple, practical, and scalable, making it suitable for large-scale applications.
* The theoretical analysis is rigorous and provides valuable insights into the algorithm's behavior.
* The experimental results are impressive, demonstrating the effectiveness of GREEDI on large-scale datasets.
Arguments con acceptance:
* The experimental section is too concise and lacks detailed explanations of the baseline methods and results.
* The idea of lazy evaluation is not adequately discussed, despite its potential to improve efficiency and scalability.
* The paper lacks clarity on what is meant by "fit on one machine", specifically regarding RAM, disk, or CPU processing limitations.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, provide more detailed explanations of the experimental setup, and discuss the run-time and CPU cost of the algorithm.