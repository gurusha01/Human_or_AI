This paper presents a significant contribution to the field of symbolic dynamic programming for Markov Decision Processes (MDPs) with factored state and action spaces. The authors introduce two novel algorithms, Factored Action Modified Policy Iteration (FA-MPI) and Opportunistic Policy Iteration (OPI), which improve upon existing state-of-the-art algorithms in terms of scalability.
The paper is well-structured and clearly written, making it easy to follow the authors' line of reasoning. The introduction provides a thorough background on the problem of symbolic dynamic programming for MDPs with factored state and action spaces, and the authors' motivation for developing new algorithms.
The technical contributions of the paper are substantial. FA-MPI is a novel algorithm that conducts exact policy evaluation steps by treating the policy as a constraint on normal Bellman backups. OPI, on the other hand, is a more conservative policy backup that enforces the policy constraint opportunistically, only when it does not increase the size of the value function representation. The authors provide a thorough analysis of the properties of OPI, including its convergence guarantees and its relationship to asynchronous policy iteration.
The experimental results presented in the paper demonstrate the effectiveness of the proposed algorithms. The authors evaluate the performance of FA-MPI and OPI on several benchmark domains, including Inventory Control, SysAdmin, and Elevator Control. The results show that OPI significantly outperforms FA-MPI and other state-of-the-art algorithms in terms of scalability, and that the use of memory-bounding can further improve the performance of OPI.
The paper has some minor weaknesses. The notation and terminology used in the paper may be unfamiliar to some readers, and the authors could have provided more explanations and examples to help clarify the concepts. Additionally, some of the sections, such as the discussion of the related work, feel a bit rushed and could have been expanded upon.
Overall, however, this is a strong paper that makes significant contributions to the field of symbolic dynamic programming. The authors' ideas are novel, well-motivated, and thoroughly evaluated, and the paper is well-written and easy to follow. I recommend acceptance of this paper, and I believe that it will be of interest to researchers and practitioners in the field of artificial intelligence and decision-making under uncertainty.
Arguments pro acceptance:
* The paper presents novel and significant contributions to the field of symbolic dynamic programming.
* The authors provide a thorough analysis of the properties of their algorithms, including convergence guarantees and relationships to other algorithms.
* The experimental results demonstrate the effectiveness of the proposed algorithms and provide a thorough evaluation of their performance.
* The paper is well-structured and clearly written, making it easy to follow the authors' line of reasoning.
Arguments con acceptance:
* The notation and terminology used in the paper may be unfamiliar to some readers, and the authors could have provided more explanations and examples to help clarify the concepts.
* Some of the sections, such as the discussion of the related work, feel a bit rushed and could have been expanded upon.
* The paper may benefit from additional examples and illustrations to help clarify the concepts and algorithms presented.