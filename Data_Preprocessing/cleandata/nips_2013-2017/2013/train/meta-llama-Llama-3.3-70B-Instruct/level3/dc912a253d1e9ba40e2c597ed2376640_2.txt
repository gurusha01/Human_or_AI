This paper proposes a distributed stochastic dual coordinate ascent (DisDCA) algorithm for solving regularized loss minimization problems in a distributed framework. The algorithm combines stochastic dual coordinate ascent with parallelized training, introducing a novel trade-off between communication and computation. However, the significance of this contribution is limited by poorly designed experiments.
The authors' trade-off bounds do not account for the impact of input dimension, particularly in cases of sparse data, which is common in big data. The experiments only measure the number of iterations, not total running time, and omit crucial details such as communication latency and architecture. The paper's results are misleading, as increasing the number of examples per process reduces the number of iterations but increases computation cost per iteration.
The experiments lack significance, as they only show a trivial reduction in the number of iterations as a function of parameters, and do not demonstrate the actual training time or optimal parameters. The authors' claim of a "parameter-free" algorithm is incorrect, as the choice of lambda is critical, and the experiments do not properly demonstrate the communication/computation trade-off.
The paper has several technical issues, including confusing proof statements, unclear terminology, and poorly formatted plots. The writing is also unclear, making it difficult to understand the authors' contributions and the significance of their results.
In terms of quality, the paper is technically sound, but the claims are not well-supported by theoretical analysis or experimental results. The paper is not well-organized, and the writing is not clear. The authors do not adequately inform the reader about the limitations of their approach and the potential applications of their algorithm.
In terms of originality, the paper proposes a novel algorithm, but the idea of combining stochastic dual coordinate ascent with parallelized training is not new. The authors do not provide a clear comparison with existing work, and the experiments do not demonstrate the superiority of their approach.
In terms of significance, the paper does not address a difficult problem in a better way than previous research. The results are not important, and other people are unlikely to use these ideas or build on them. The paper does not advance the state of the art in a demonstrable way, and the authors do not provide unique data, conclusions, or approaches.
Overall, I would not recommend accepting this paper in its current form. The authors need to address the technical issues, improve the writing, and provide more significant and well-designed experiments to demonstrate the effectiveness of their algorithm.
Arguments for acceptance:
* The paper proposes a novel algorithm that combines stochastic dual coordinate ascent with parallelized training.
* The authors provide a theoretical analysis of the trade-off between communication and computation.
Arguments against acceptance:
* The experiments are poorly designed and do not demonstrate the significance of the authors' contributions.
* The paper has several technical issues, including confusing proof statements and unclear terminology.
* The authors' claims are not well-supported by theoretical analysis or experimental results.
* The paper does not address a difficult problem in a better way than previous research.
* The results are not important, and other people are unlikely to use these ideas or build on them.