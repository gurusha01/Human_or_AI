This paper presents a novel approach to reducing the number of parameters in deep neural networks by exploiting the structure in the weight space. The authors propose a technique for predicting the weights of a network using a low-rank approximation of the weight matrix, which is achieved by representing the weight matrix as a product of two smaller matrices. The approach is extremely general and can be applied to a broad range of models, including multilayer perceptrons, convolutional networks, and reconstruction ICA.
The paper is well-written and clearly explains the intuition behind the technique, as well as the mathematical formulation. The experiments demonstrate the effectiveness of the approach, showing that it is possible to predict more than 95% of the weights of a network without any drop in accuracy. The authors also provide a thorough discussion of related work and highlight the differences between their approach and other methods for reducing the number of parameters in neural networks.
One of the strengths of the paper is its ability to provide a significant reduction in the number of dynamic parameters, which can lead to improvements in computational efficiency and scalability. The approach is also orthogonal to other techniques for reducing the number of parameters, such as tying weights in a tiled or convolutional pattern, and can be easily incorporated into existing architectures.
However, there are some limitations to the paper. The experiments are limited to a few datasets and models, and it would be beneficial to see more extensive evaluations on a wider range of tasks and architectures. Additionally, the paper could benefit from more qualitative results and intuitive discussions to explain the active learning process and why the approach works. The authors also mention that the selection of the set of indices α is done uniformly at random, but it would be interesting to explore other options for selecting α and to investigate the impact of this choice on the performance of the approach.
In terms of novelty, the paper presents a unique approach to reducing the number of parameters in deep neural networks, and the experiments demonstrate its effectiveness. However, the paper could benefit from a more thorough discussion of related work and a clearer explanation of how the approach differs from other methods. The authors should also consider citing relevant work, such as Siddiquie et al, to acknowledge existing research on active learning for structured output spaces.
Overall, the paper presents a significant contribution to the field of deep learning, and the approach has the potential to improve the efficiency and scalability of neural networks. With some additional experiments and discussions, the paper could be even stronger and more convincing.
Arguments pro acceptance:
* The paper presents a novel and effective approach to reducing the number of parameters in deep neural networks.
* The approach is extremely general and can be applied to a broad range of models.
* The experiments demonstrate the effectiveness of the approach, showing that it is possible to predict more than 95% of the weights of a network without any drop in accuracy.
Arguments con acceptance:
* The experiments are limited to a few datasets and models, and more extensive evaluations are needed.
* The paper could benefit from more qualitative results and intuitive discussions to explain the active learning process and why the approach works.
* The selection of the set of indices α is done uniformly at random, and it would be interesting to explore other options for selecting α and to investigate the impact of this choice on the performance of the approach.