This paper proposes a novel approach to evaluating the performance of workers in crowdsourcing tasks by leveraging control questions. The authors introduce two methods: a two-stage estimator and a joint-estimator of biases and true labels. The two-stage estimator requires O(sqrt(L)) control questions to minimize the MSE, while the joint-estimator method requires O(L/sqrt(n)) control questions, which is a better bound, especially when the total number of items labeled (n) is large.
The joint-estimator method relies on the structure of the assignment graph of items to workers and performs optimally when the graph is an expander, such as a random graph. The paper includes experiments that validate the theoretical results and investigate the robustness of the methods when assumptions are violated. The results show that the joint-estimator is less robust than the two-stage method.
The paper is well-written, and the authors provide a clear explanation of the proposed methods and their theoretical analysis. The experiments are well-designed and provide valuable insights into the performance of the proposed methods.
The strengths of the paper include:
* The proposal of two novel methods for evaluating worker performance in crowdsourcing tasks
* A thorough theoretical analysis of the proposed methods, including bounds on the number of control questions required
* Well-designed experiments that validate the theoretical results and investigate the robustness of the methods
The weaknesses of the paper include:
* The joint-estimator method may not perform well when the assignment graph is not an expander
* The paper could benefit from a more detailed discussion of the implications of the results and their potential applications in real-world crowdsourcing tasks
Overall, the paper is well-written, and the proposed methods have the potential to improve the evaluation of worker performance in crowdsourcing tasks. The results are significant, and the paper makes a valuable contribution to the field.
Arguments for acceptance:
* The paper proposes novel methods for evaluating worker performance in crowdsourcing tasks
* The theoretical analysis is thorough and provides valuable insights into the performance of the proposed methods
* The experiments are well-designed and provide valuable results
Arguments against acceptance:
* The joint-estimator method may not perform well when the assignment graph is not an expander
* The paper could benefit from a more detailed discussion of the implications of the results and their potential applications in real-world crowdsourcing tasks
Recommendation: Accept. The paper makes a valuable contribution to the field, and the proposed methods have the potential to improve the evaluation of worker performance in crowdsourcing tasks. The results are significant, and the paper is well-written.