This paper explores the application of approximate dynamic programming (ADP) algorithms to the game of Tetris, with a focus on classification-based modified policy iteration (CBMPI). The authors propose that Tetris is a game where good policies are easier to represent and learn than their corresponding value functions, and therefore, ADP algorithms that search in a policy space should perform better than those that search in a value function space.
The paper presents an extensive experimental evaluation of CBMPI, comparing its performance to the cross-entropy (CE) method and the λ-policy iteration (λ-PI) algorithm. The results show that CBMPI achieves the best reported results in the literature for Tetris in both small 10×10 and large 10×20 boards, with a significant improvement over previous ADP algorithms. Notably, CBMPI uses considerably fewer samples than CE to achieve similar performance in the large board.
However, the paper has some weaknesses. The writing is poor, with a too-long abstract and unclear terminology. The experiment design is hard to understand, and key terms are introduced without definition, making it difficult to follow the paper's arguments. The model results are not compelling, with both the full and myopic models hitting the same qualitative patterns, and the authors fail to discuss the implications of this finding. The analysis presented in the paper is unclear, with opaque policy-space graphs and a lack of mention of human performance, making it difficult to evaluate the model's effectiveness.
Despite these weaknesses, the paper has some strengths. The emphasis on multiple model comparison, empirical data, and novelty of deriving theorems about the optimal stopping rule are notable contributions. The paper's focus on policy-space search is an interesting direction, and the results demonstrate the potential of CBMPI in achieving good performance in Tetris.
In terms of quality, the paper is technically sound, but the claims are not well-supported by theoretical analysis or experimental results. The paper is not well-organized, and the writing is unclear, making it difficult to understand the authors' arguments. The originality of the paper is somewhat reduced due to its similarity to existing work, but the application of CBMPI to Tetris is a novel contribution. The significance of the paper is moderate, as it addresses a difficult problem in a better way than previous research, but the results are not groundbreaking.
Overall, I would recommend that the authors revise the paper to address the weaknesses mentioned above, including improving the writing, clarifying the experiment design, and providing more compelling model results and analysis. With these revisions, the paper has the potential to make a significant contribution to the field of ADP and reinforcement learning.
Arguments pro acceptance:
* The paper presents a novel application of CBMPI to Tetris, achieving the best reported results in the literature.
* The emphasis on policy-space search is an interesting direction, and the results demonstrate the potential of CBMPI in achieving good performance.
* The paper provides an extensive experimental evaluation, comparing CBMPI to other ADP algorithms and demonstrating its effectiveness.
Arguments con acceptance:
* The writing is poor, with a too-long abstract and unclear terminology.
* The experiment design is hard to understand, and key terms are introduced without definition.
* The model results are not compelling, with both the full and myopic models hitting the same qualitative patterns.
* The analysis presented in the paper is unclear, with opaque policy-space graphs and a lack of mention of human performance.