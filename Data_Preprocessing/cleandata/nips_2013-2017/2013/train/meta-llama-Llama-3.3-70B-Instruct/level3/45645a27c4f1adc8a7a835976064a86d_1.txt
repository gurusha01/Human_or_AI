This paper proposes a novel approach to decision tree learning, introducing the concept of decision jungles, which are ensembles of rooted decision directed acyclic graphs (DAGs). The authors motivate this approach by highlighting the limitations of traditional decision trees, which grow exponentially with depth, making them impractical for memory-constrained hardware. The proposed method jointly optimizes the features and structure of the DAGs, using a local search-based algorithm to minimize an objective function.
The paper is well-written, and the authors provide a clear explanation of the proposed approach, including the optimization algorithms and the experimental setup. The results show that decision jungles can achieve higher predictive accuracy and reduce memory consumption compared to traditional decision forests and their variants.
The technical contributions of the paper seem novel and significant, as they address a fundamental limitation of decision trees and provide a new approach to learning compact and accurate classifiers. The use of DAGs instead of trees allows for more flexible and efficient modeling, and the joint optimization of features and structure is a key innovation.
However, there are some areas that could be improved. The paper could benefit from a more detailed analysis of the experimental results, including a discussion of the statistical significance of the improvements and a comparison with other state-of-the-art methods. Additionally, the authors could provide more insight into the trade-offs between the different optimization algorithms and the choice of hyperparameters.
Some minor points that could be improved include the use of more precise language in the mathematical formulations, such as adding expectations in equation (4), and clarifying the stopping criterion for the Gibbs sampler. Furthermore, the authors could provide more details on the computational resources required for training and testing the decision jungles, as well as the potential applications of this approach in real-world scenarios.
Overall, the paper presents a significant contribution to the field of machine learning, and the proposed approach has the potential to improve the accuracy and efficiency of decision tree-based classifiers. The authors have demonstrated the effectiveness of decision jungles on several challenging classification tasks, and the results are promising.
Arguments pro acceptance:
* The paper proposes a novel and significant approach to decision tree learning, addressing a fundamental limitation of traditional decision trees.
* The results show that decision jungles can achieve higher predictive accuracy and reduce memory consumption compared to traditional decision forests and their variants.
* The paper is well-written, and the authors provide a clear explanation of the proposed approach and the experimental setup.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the experimental results, including a discussion of the statistical significance of the improvements.
* The authors could provide more insight into the trade-offs between the different optimization algorithms and the choice of hyperparameters.
* Some minor points, such as the use of more precise language in the mathematical formulations and clarifying the stopping criterion for the Gibbs sampler, could be improved.