This paper introduces a novel policy iteration algorithm called MB-OPI for symbolic MDPs with factored-action dynamics, which generalizes existing algorithms and provides a way to trade representational complexity. The algorithm addresses the technical challenge of multiplying explicit policy representation into the current value function, which can increase its size, by defining a pruning procedure to conservatively combine policy and value diagrams. The results show a 2-6x improvement over existing approaches in terms of solution time, demonstrating the effectiveness of the MB-OPI algorithm.
The paper makes a theoretical contribution to the literature on symbolic MDP planning by introducing the concept of pruning as an alternative to ADD products and proving its guarantees. The empirical results support the idea that pruning offers an MPI approach to SDP planning that avoids representational bloat and provides a significant speedup.
The paper is well-written and easy to follow, but could benefit from additional background information and clarification on certain technical points. For example, the authors could provide more context on the Relational Dynamic Influence Diagram Language (RDDL) and its application to the experimental domains. Additionally, the authors could include more discussion on practical problems with many parallel actions and provide a toy test case to highlight the improvement over existing approaches.
The paper has some minor errors and suggestions for improvement, including clarifying certain definitions, rewording sentences for correctness, and adding explanations for technical concepts. For instance, the authors could provide a clearer explanation of the pruning procedure and its relationship to the policy constraint.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is original, and the approach is novel, building on existing work in symbolic MDP planning.
The significance of the paper lies in its ability to address the scalability of symbolic planning under uncertainty with factored states and actions. The results demonstrate a significant improvement over existing approaches, and the algorithm has the potential to be applied to a wide range of domains.
Arguments for acceptance:
* The paper introduces a novel algorithm that generalizes existing approaches and provides a significant improvement in terms of solution time.
* The paper makes a theoretical contribution to the literature on symbolic MDP planning.
* The empirical results demonstrate the effectiveness of the algorithm and its ability to avoid representational bloat.
Arguments against acceptance:
* The paper could benefit from additional background information and clarification on certain technical points.
* The paper has some minor errors and suggestions for improvement.
* The authors could provide more discussion on practical problems with many parallel actions and include a toy test case to highlight the improvement over existing approaches.
Overall, the paper is well-written, and the contributions are significant. With some minor revisions to address the suggestions for improvement, the paper is ready for publication.