This paper presents a novel application of the classification-based modified policy iteration (CBMPI) algorithm to the game of Tetris, achieving state-of-the-art results in both small and large boards. The authors' conjecture that Tetris is a game where good policies are easier to represent and learn than their corresponding value functions is well-supported by the experimental results. The paper is well-organized, and the writing is clear, making it easy to follow the authors' arguments and methodology.
The strengths of the paper include tackling an under-explored problem in the context of approximate dynamic programming (ADP) and reinforcement learning (RL), proposing a novel and practical algorithm, and providing adequate experiments and a good review of previous work. The authors' use of CBMPI, which searches in a policy space instead of a value function space, is a key contribution, and the results demonstrate its effectiveness in Tetris.
However, there are some weaknesses to the paper. The lack of theoretical guarantees and reliance on heuristics are notable limitations. Some methods are used outside their original context and guarantees, which may raise concerns about the robustness of the results. Additionally, the presentation of figures and experiments could be improved for better clarity and readability.
To improve the paper, the authors could provide more clarity on the decisions involved in the algorithm, such as the choice of features and hyperparameters. Using supplementary materials for proofs and details could also enhance the paper's readability. Furthermore, the authors could consider providing more insights into the trade-offs between different components of the algorithm, such as the value function approximation and policy search.
In terms of future research, it would be interesting to see the authors provide theoretical guarantees for the CBMPI algorithm and explore its application to other domains. Decoupling the sparking params update and structure updates in the algorithm could also be a fruitful direction for future research.
Overall, the paper presents a significant contribution to the field of ADP and RL, and with some revisions to address the weaknesses, it has the potential to be a strong publication. The pros and cons of the paper are:
Pros:
* Tackles an under-explored problem in ADP and RL
* Proposes a novel and practical algorithm (CBMPI)
* Provides adequate experiments and a good review of previous work
* Achieves state-of-the-art results in Tetris
Cons:
* Lack of theoretical guarantees
* Reliance on heuristics
* Some methods used outside their original context and guarantees
* Presentation of figures and experiments could be improved
Recommendations for future research:
* Provide theoretical guarantees for the CBMPI algorithm
* Explore the application of CBMPI to other domains
* Decouple the sparking params update and structure updates in the algorithm
* Investigate the trade-offs between different components of the algorithm.