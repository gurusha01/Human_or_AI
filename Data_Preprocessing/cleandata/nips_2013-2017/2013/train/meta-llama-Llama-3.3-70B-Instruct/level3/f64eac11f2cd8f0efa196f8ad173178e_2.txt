This paper presents a novel approach to improving the scalability of symbolic planning under uncertainty with factored states and actions. The authors introduce two main contributions: Factored Action Modified Policy Iteration (FA-MPI) and Opportunistic Policy Iteration (OPI). FA-MPI is a symbolic implementation of Modified Policy Iteration (MPI) for factored actions, which views policy evaluation as policy-constrained value iteration. However, the authors found that the strict enforcement of policy constraints in FA-MPI can lead to large memory requirements, making it sometimes worse than traditional value iteration.
To address this issue, the authors propose OPI, which opportunistically enforces policy constraints only when they do not increase the size of the value function representation. OPI is shown to be a convergent algorithm that lies between policy evaluation and Bellman backup, and its policy backup is bounded between the full Bellman backup and the true policy backup. The authors also provide a memory-bounded version of OPI, which allows for a space-time tradeoff.
The experimental results demonstrate that OPI scales significantly better than state-of-the-art symbolic planners, including SPUDD and FA-MPI, on several domains with large action spaces. The results also show that OPI converges faster and uses less memory than FA-MPI, especially when the number of policy evaluation steps is increased. Additionally, the authors demonstrate that combining OPI with memory bounding leads to even better scalability.
The paper is well-organized, and the authors provide a clear and detailed explanation of their approach. The theoretical analysis is sound, and the experimental results are convincing. However, the paper could benefit from more discussion on the limitations of the approach and potential future directions. For example, the authors mention that the growth of the value and policy diagrams with problem complexity is a bottleneck, but they do not provide a clear solution to this issue.
Overall, the paper presents a significant contribution to the field of symbolic planning under uncertainty, and the results demonstrate the potential of OPI to improve the scalability of symbolic planners. The paper is well-written, and the authors provide a clear and detailed explanation of their approach, making it easy to follow and understand.
Strengths:
* The paper presents a novel approach to improving the scalability of symbolic planning under uncertainty with factored states and actions.
* The authors provide a clear and detailed explanation of their approach, including theoretical analysis and experimental results.
* The results demonstrate that OPI scales significantly better than state-of-the-art symbolic planners on several domains with large action spaces.
Weaknesses:
* The paper could benefit from more discussion on the limitations of the approach and potential future directions.
* The growth of the value and policy diagrams with problem complexity is a bottleneck, but the authors do not provide a clear solution to this issue.
* The paper assumes that the reader is familiar with the background material, which may make it difficult for non-experts to follow.
Arguments for acceptance:
* The paper presents a significant contribution to the field of symbolic planning under uncertainty.
* The results demonstrate the potential of OPI to improve the scalability of symbolic planners.
* The paper is well-written, and the authors provide a clear and detailed explanation of their approach.
Arguments against acceptance:
* The paper could benefit from more discussion on the limitations of the approach and potential future directions.
* The growth of the value and policy diagrams with problem complexity is a bottleneck, but the authors do not provide a clear solution to this issue.
* The paper assumes that the reader is familiar with the background material, which may make it difficult for non-experts to follow.