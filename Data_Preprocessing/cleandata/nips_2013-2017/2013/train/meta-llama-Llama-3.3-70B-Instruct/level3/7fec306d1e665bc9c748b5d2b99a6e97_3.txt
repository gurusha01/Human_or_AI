This paper proposes a novel approach to reducing the number of free parameters in neural networks by learning a subset of parameters and predicting the remaining ones through interpolation. The method is based on the intuition that there is strong redundancy in learned parameters. The authors evaluate their approach on several architectures and datasets, demonstrating that learning fewer than 50% of the parameters is sufficient without losing performance.
The proposed method is relatively simple and assumes a low-rank decomposition of the weight matrix, with one of the matrices fixed using prior knowledge about the data. While the idea of reducing parameters in neural network-like architectures is not novel, the authors' approach differs in that it exploits prior knowledge to constrain one of the matrices.
The experimental section demonstrates the practical usefulness and wide applicability of the proposed method. However, the evaluation of the data-dependent kernel is limited, and it would be beneficial to see more emphasis on scenarios where there is no obvious topology in the data that can be exploited. Additionally, the paper could benefit from a more compelling experimental use case demonstrating the impact of the approach in practice, including a detailed discussion of how the reduction in parameters translates into computational and memory savings.
The authors' decision to limit themselves to a particular set of basis functions derived from kernel regression is also questioned, and exploring other linear basis representations of filters could be more efficient. Furthermore, the paper lacks a comparison with state-of-the-art results from the literature, which would put the reported performance into perspective and make it easier to judge the effectiveness of the approach.
The evaluation of the data-dependent kernel is somewhat limited, and it would be helpful to include more experiments with different datasets and architectures. The authors are also asked to provide more insights into why the naive low-rank scheme is working poorly and whether removing redundancy in the parameterization could be beneficial.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, and the organization is logical. However, some sections could be improved for better clarity and readability.
The originality of the paper lies in its novel approach to reducing parameters in neural networks, which exploits prior knowledge to constrain one of the matrices in the low-rank decomposition. The significance of the paper is that it provides a new perspective on parameter reduction in neural networks and demonstrates the potential of the proposed approach in practice.
Overall, the paper is well-written, and the proposed approach is novel and interesting. However, there are some limitations and areas for improvement, particularly in terms of evaluating the data-dependent kernel and comparing with state-of-the-art results. With some revisions to address these concerns, the paper has the potential to make a significant contribution to the field.
Arguments for acceptance:
* The paper proposes a novel approach to reducing parameters in neural networks, which is technically sound and well-supported by theoretical analysis and experimental results.
* The approach is relatively simple and assumes a low-rank decomposition of the weight matrix, with one of the matrices fixed using prior knowledge about the data.
* The experimental section demonstrates the practical usefulness and wide applicability of the proposed method.
Arguments against acceptance:
* The evaluation of the data-dependent kernel is limited, and it would be beneficial to see more emphasis on scenarios where there is no obvious topology in the data that can be exploited.
* The paper lacks a comparison with state-of-the-art results from the literature, which would put the reported performance into perspective and make it easier to judge the effectiveness of the approach.
* The authors' decision to limit themselves to a particular set of basis functions derived from kernel regression is questioned, and exploring other linear basis representations of filters could be more efficient.