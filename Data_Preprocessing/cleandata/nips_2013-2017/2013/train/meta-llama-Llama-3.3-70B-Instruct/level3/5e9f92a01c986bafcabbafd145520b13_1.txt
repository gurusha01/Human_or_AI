This paper proposes a novel approach to decision tree learning, introducing the concept of decision jungles, which are ensembles of rooted decision directed acyclic graphs (DAGs). The authors argue that traditional decision trees face a fundamental limitation, as their size grows exponentially with depth, making them impractical for memory-constrained hardware. Decision jungles aim to address this issue by allowing multiple paths from the root to each leaf, reducing the number of nodes required.
The paper's strength lies in its application of modern mathematical techniques to address an important problem in machine learning. The authors propose two local search-based algorithms, LSearch and ClusterSearch, to optimize the objective function over both the structure of the graph and the features. The experiments demonstrate that decision jungles can reduce memory consumption while improving generalization, outperforming standard decision forests and their variants on several classification tasks.
However, the paper's presentation is overly complex, making it challenging for non-experts to follow. The notation and terminology used are not always clearly defined, and the text could benefit from additional explanations and examples to facilitate understanding. Furthermore, the lack of analysis of actual data and the limited evaluation on a few datasets may raise concerns about the generalizability of the results.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful in evaluating both the strengths and weaknesses of their work. The clarity of the paper could be improved, but the organization is generally good. The originality of the approach is a significant strength, as it offers a novel solution to a well-known problem in machine learning. The significance of the results is also notable, as decision jungles have the potential to improve the efficiency and accuracy of decision tree learning in various applications.
Arguments pro acceptance:
* The paper proposes a novel and original approach to decision tree learning.
* The experiments demonstrate significant improvements in memory efficiency and generalization.
* The authors provide a thorough analysis of the results and discuss the limitations of their approach.
Arguments con acceptance:
* The paper's presentation is overly complex and may be challenging to follow.
* The lack of analysis of actual data and limited evaluation on a few datasets may raise concerns about generalizability.
* The paper could benefit from additional explanations, examples, and clarifications to facilitate understanding.
Overall, the paper makes a valuable contribution to the field of machine learning, and with some revisions to improve clarity and presentation, it has the potential to be a strong addition to the conference program.