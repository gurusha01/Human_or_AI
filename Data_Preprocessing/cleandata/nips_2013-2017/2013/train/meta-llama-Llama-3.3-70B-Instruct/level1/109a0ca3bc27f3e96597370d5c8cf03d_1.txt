This paper presents a comprehensive study of structured Schatten norms for tensors, with a focus on their application to convex-optimization-based tensor decomposition. The authors introduce a new class of norms, including the "overlapped" and "latent" approaches, and provide a detailed analysis of their properties and performance.
The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work. The technical contributions of the paper are significant, including the establishment of a duality result between the overlapped and latent Schatten norms, and the derivation of consistency and deterministic bounds for the latent approach.
The authors also provide a thorough comparison of the overlapped and latent approaches, both theoretically and empirically. The numerical experiments demonstrate that the latent approach can outperform the overlapped approach in many cases, particularly when the unknown tensor is only low-rank in one mode.
The paper is well-organized, and the authors provide a clear and concise presentation of the main results. The proofs of the technical results are relegated to the supplementary material, which makes the paper easier to follow.
The strengths of the paper include:
* A clear and concise introduction to the background and motivation of the work
* Significant technical contributions, including the establishment of a duality result and the derivation of consistency and deterministic bounds
* A thorough comparison of the overlapped and latent approaches, both theoretically and empirically
* Well-organized and easy to follow presentation
The weaknesses of the paper include:
* The paper assumes a significant amount of background knowledge in tensor decomposition and convex optimization, which may make it difficult for non-experts to follow
* Some of the technical results, such as the duality theorem, may be difficult to understand without a strong background in functional analysis
* The numerical experiments are limited to a specific setting, and it would be interesting to see more extensive experiments to confirm the results
Overall, I would recommend accepting this paper for publication. The technical contributions are significant, and the paper provides a thorough and well-organized presentation of the results. The authors have also provided a clear and concise introduction to the background and motivation of the work, which makes the paper accessible to a wide range of readers.
Arguments pro acceptance:
* Significant technical contributions
* Well-organized and easy to follow presentation
* Thorough comparison of the overlapped and latent approaches
* Clear and concise introduction to the background and motivation of the work
Arguments con acceptance:
* Assumes significant background knowledge in tensor decomposition and convex optimization
* Some technical results may be difficult to understand without a strong background in functional analysis
* Limited numerical experiments
Rating: 9/10
Confidence: 8/10
Recommendation: Accept with minor revisions. The authors should consider adding more background information and explanations to make the paper more accessible to non-experts. Additionally, more extensive numerical experiments would be useful to confirm the results.