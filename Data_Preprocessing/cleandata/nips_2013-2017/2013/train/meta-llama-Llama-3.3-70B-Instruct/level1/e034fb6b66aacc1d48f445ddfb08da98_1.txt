This paper introduces a novel approach to Interactive Reinforcement Learning (IRL) called Policy Shaping, which interprets human feedback as direct policy advice rather than converting it into a reward or value. The authors propose a Bayesian algorithm, Advise, that maximizes the information gained from human feedback by utilizing it as direct policy labels. The paper provides a thorough comparison of Advise with state-of-the-art approaches, including Action Biasing, Control Sharing, and Reward Shaping, and demonstrates its robustness to infrequent and inconsistent human feedback.
The paper is well-organized, and the writing is clear and concise. The authors provide a comprehensive review of related work, highlighting the limitations of existing approaches and motivating the need for a new paradigm. The technical contributions of the paper are sound, and the experimental evaluation is thorough and well-designed. The results show that Advise outperforms the baseline methods in several cases, particularly in scenarios with reduced feedback consistency and frequency.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem of IRL and the limitations of existing approaches
* A thorough review of related work, highlighting the contributions of the paper
* A sound technical contribution, with a well-designed algorithm and experimental evaluation
* Robustness to infrequent and inconsistent human feedback, which is a common challenge in IRL
The weaknesses of the paper include:
* The assumption that the human feedback is consistent with the optimal policy, which may not always be the case in practice
* The need for a manual estimate of the feedback consistency, which may be challenging to obtain in real-world scenarios
* The limited evaluation of the algorithm in larger domains, which may be necessary to demonstrate its scalability
Arguments pro acceptance:
* The paper introduces a novel and well-motivated approach to IRL, which addresses the limitations of existing methods
* The technical contributions are sound, and the experimental evaluation is thorough and well-designed
* The results demonstrate the robustness of Advise to infrequent and inconsistent human feedback, which is a common challenge in IRL
Arguments con acceptance:
* The assumption of consistent human feedback may not always hold in practice, which may limit the applicability of the algorithm
* The need for a manual estimate of the feedback consistency may be challenging to obtain in real-world scenarios
* The limited evaluation of the algorithm in larger domains may raise concerns about its scalability.
Overall, the paper is well-written, and the technical contributions are sound. The results demonstrate the potential of Advise to improve the robustness of IRL to infrequent and inconsistent human feedback. With some additional evaluation and refinement, the algorithm has the potential to make a significant impact in the field of IRL.