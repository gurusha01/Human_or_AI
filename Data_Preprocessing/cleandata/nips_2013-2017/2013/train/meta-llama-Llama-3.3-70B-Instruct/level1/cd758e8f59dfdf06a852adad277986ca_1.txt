This paper presents a significant contribution to the field of density-based clustering, particularly in the context of high-dimensional data. The authors investigate the problem of estimating the cluster tree for a density supported on or near a smooth d-dimensional manifold M isometrically embedded in R^D. They analyze a modified version of the k-nearest neighbor-based algorithm proposed by Chaudhuri and Dasgupta (2010) and demonstrate that it is consistent and has fast rates of convergence for data on or near a low-dimensional manifold.
The paper's main strengths include:
1. Theoretical guarantees: The authors provide rigorous theoretical guarantees for the algorithm's performance, including finite sample bounds and consistency results.
2. Manifold adaptivity: The algorithm adapts to the unknown manifold, eliminating the need for explicit manifold estimation.
3. Improved rates: The authors show that the sample complexity for identifying salient clusters is independent of the ambient dimension D, which is a significant improvement over previous results.
4. Robustness to noise: The paper also explores the robustness of the algorithm to noise, considering both clutter noise and additive noise models.
However, there are some potential weaknesses and areas for improvement:
1. Assumptions: The paper relies on several assumptions, such as the manifold being smooth and having a bounded condition number. While these assumptions are reasonable, they may not always hold in practice.
2. Computational complexity: The algorithm's computational complexity is not explicitly discussed, which could be a concern for large-scale datasets.
3. Parameter tuning: The algorithm has several parameters that need to be tuned, which can be challenging in practice.
To improve the paper, the authors could consider:
1. Providing more numerical experiments: Additional numerical experiments would help to demonstrate the algorithm's performance in practice and provide a more comprehensive understanding of its strengths and weaknesses.
2. Exploring more realistic noise models: While the paper considers two noise models, more realistic noise models, such as Gaussian noise or other types of corruption, could be explored to further demonstrate the algorithm's robustness.
3. Developing a more efficient algorithm: Improving the algorithm's computational complexity could make it more practical for large-scale datasets.
Overall, the paper presents a significant contribution to the field of density-based clustering and provides a robust and efficient algorithm for estimating the cluster tree in high-dimensional data. With some additional work to address the potential weaknesses and areas for improvement, the paper has the potential to be even more impactful.
Arguments pro acceptance:
1. The paper presents a significant contribution to the field of density-based clustering.
2. The algorithm is robust and efficient, with theoretical guarantees and improved rates.
3. The paper explores the robustness of the algorithm to noise, which is an important consideration in practice.
Arguments con acceptance:
1. The paper relies on several assumptions that may not always hold in practice.
2. The algorithm's computational complexity is not explicitly discussed, which could be a concern for large-scale datasets.
3. The paper could benefit from additional numerical experiments and exploration of more realistic noise models.