This paper proposes a fast algorithm for ridge regression, called Subsampled Randomized Hadamard Transform-Dual Ridge Regression (SRHT-DRR), which is designed to handle the case where the number of features is much larger than the number of observations (p >> n). The authors provide a detailed analysis of the algorithm's performance, including risk bounds and experimental results on synthetic and real-world data.
The main idea of the paper is to precondition the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features, which reduces the computational cost of the dual ridge regression solution from O(n^2p) to O(np log(n)). The authors also provide a risk inflation bound, which shows that the risk of SRHT-DRR is at most (1 + ∆)^2 times the risk of the true dual ridge regression solution, where ∆ is a small constant that depends on the rank of the design matrix and the subsampling size.
The experimental results show that SRHT-DRR achieves significant speedups with only small loss of accuracy compared to the true dual ridge regression solution. The authors also compare SRHT-DRR with PCA and randomized PCA, and show that SRHT-DRR outperforms these methods in terms of both accuracy and computational cost.
The strengths of the paper include:
* The proposed algorithm is fast and efficient, making it suitable for large-scale datasets.
* The authors provide a detailed analysis of the algorithm's performance, including risk bounds and experimental results.
* The experimental results show that SRHT-DRR outperforms other methods, including PCA and randomized PCA.
The weaknesses of the paper include:
* The algorithm is limited to the case where p >> n, and it is not clear how well it will perform in other scenarios.
* The authors assume that the design matrix has a low rank, which may not always be the case in practice.
* The paper could benefit from more extensive experimental results, including comparisons with other methods and evaluations on a wider range of datasets.
Overall, the paper is well-written and provides a clear and detailed presentation of the proposed algorithm and its analysis. The experimental results are convincing, and the paper makes a significant contribution to the field of machine learning.
Arguments pro acceptance:
* The paper proposes a fast and efficient algorithm for ridge regression, which is an important problem in machine learning.
* The authors provide a detailed analysis of the algorithm's performance, including risk bounds and experimental results.
* The experimental results show that SRHT-DRR outperforms other methods, including PCA and randomized PCA.
Arguments con acceptance:
* The algorithm is limited to the case where p >> n, and it is not clear how well it will perform in other scenarios.
* The authors assume that the design matrix has a low rank, which may not always be the case in practice.
* The paper could benefit from more extensive experimental results, including comparisons with other methods and evaluations on a wider range of datasets.
Rating: 8/10
Recommendation: Accept, with minor revisions to address the weaknesses mentioned above.