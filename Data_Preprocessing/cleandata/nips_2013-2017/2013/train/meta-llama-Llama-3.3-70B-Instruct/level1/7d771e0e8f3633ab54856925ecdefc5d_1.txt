This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions, a crucial problem in the field of artificial intelligence. The authors introduce two novel algorithms, Factored Action Modified Policy Iteration (FA-MPI) and Opportunistic Policy Iteration (OPI), which improve upon existing state-of-the-art algorithms. FA-MPI conducts exact policy evaluation steps by treating the policy as a constraint on normal Bellman backups, while OPI applies policy constraints opportunistically, only when they do not increase the size of the value function representation.
The paper relates to previous work at NIPS and elsewhere, building upon existing research in symbolic dynamic programming (SDP) and Markov Decision Processes (MDPs). The authors provide a thorough review of related work, including Structured Policy Iteration (SPI) and Factored Action Regression (FAR). The paper's contributions are significant, as they provide a new interpretation of Modified Policy Iteration (MPI) and introduce a novel convergent algorithm that lies between value iteration and policy iteration.
The strengths of the paper include its technical soundness, clarity, and originality. The authors provide a detailed and well-organized presentation of their algorithms, including pseudocode and examples. The empirical results demonstrate significant improvements in scalability over state-of-the-art algorithms. The paper also provides a thorough analysis of the trade-offs between policy evaluation and policy improvement, as well as the impact of memory bounding on the algorithms.
The weaknesses of the paper are relatively minor. One potential concern is the complexity of the algorithms, which may make them difficult to implement and understand for non-experts. Additionally, the paper could benefit from more discussion on the potential applications of the algorithms and their limitations.
Arguments pro acceptance:
* The paper presents novel and significant contributions to the field of symbolic planning under uncertainty.
* The algorithms are technically sound and well-motivated, with a clear and thorough presentation.
* The empirical results demonstrate significant improvements in scalability over state-of-the-art algorithms.
* The paper provides a thorough analysis of the trade-offs between policy evaluation and policy improvement.
Arguments con acceptance:
* The complexity of the algorithms may make them difficult to implement and understand for non-experts.
* The paper could benefit from more discussion on the potential applications of the algorithms and their limitations.
* The paper assumes a certain level of background knowledge in SDP and MDPs, which may make it inaccessible to some readers.
Overall, the paper is well-written, technically sound, and presents significant contributions to the field. I recommend acceptance, with minor revisions to address the potential weaknesses mentioned above.