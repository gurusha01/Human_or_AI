This paper proposes decision jungles, a novel ensemble learning method that uses rooted decision directed acyclic graphs (DAGs) instead of traditional decision trees. The authors argue that decision trees face a fundamental limitation, namely exponential growth with depth, which can be a problem for memory-constrained hardware. Decision jungles aim to address this issue by allowing multiple paths from the root to each leaf, resulting in a more compact and powerful discriminative model.
The paper relates to previous work on decision trees and forests, including the use of Bayesian sampling-based approaches and large-margin tree classifiers. However, the authors highlight that their approach is distinct in that it jointly optimizes the features and structure of the DAGs during training. The paper also discusses the relationship between decision trees and DAGs, and how the latter can capture more complex classification boundaries.
The authors propose two node merging algorithms, LSearch and ClusterSearch, which are used to optimize the objective function over the split parameters and child assignments. The experiments demonstrate that decision jungles can achieve significantly better generalization and memory efficiency compared to standard decision forests and their variants. The results show that jungles can reduce the number of nodes required to achieve a certain accuracy, resulting in a lower memory footprint.
The paper is well-written, and the authors provide a clear explanation of the proposed method and its relationship to previous work. The experiments are thorough and demonstrate the effectiveness of decision jungles on a range of classification tasks. The use of local search-based algorithms for optimization is a good choice, and the authors provide a detailed analysis of the results.
Strengths:
* The paper proposes a novel and interesting approach to ensemble learning that addresses a fundamental limitation of decision trees.
* The authors provide a clear and detailed explanation of the proposed method and its relationship to previous work.
* The experiments are thorough and demonstrate the effectiveness of decision jungles on a range of classification tasks.
Weaknesses:
* The paper could benefit from a more detailed analysis of the time complexity of the proposed algorithms.
* The authors could provide more insight into the choice of hyperparameters, such as the maximum tree width and the number of iterations for the LSearch algorithm.
* The paper could benefit from a more detailed comparison with other state-of-the-art ensemble learning methods.
Arguments pro acceptance:
* The paper proposes a novel and interesting approach to ensemble learning that addresses a fundamental limitation of decision trees.
* The experiments demonstrate the effectiveness of decision jungles on a range of classification tasks.
* The paper is well-written, and the authors provide a clear explanation of the proposed method and its relationship to previous work.
Arguments con acceptance:
* The paper could benefit from a more detailed analysis of the time complexity of the proposed algorithms.
* The authors could provide more insight into the choice of hyperparameters.
* The paper could benefit from a more detailed comparison with other state-of-the-art ensemble learning methods.
Overall, I believe that the paper is a good contribution to the field of machine learning and ensemble learning. The proposed method is novel and interesting, and the experiments demonstrate its effectiveness on a range of classification tasks. With some minor revisions to address the weaknesses mentioned above, I believe that the paper would be a strong candidate for acceptance.