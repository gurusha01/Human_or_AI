This paper presents a novel application of approximate dynamic programming (ADP) to the game of Tetris, a popular benchmark for optimization techniques. The authors conjecture that Tetris is a game where good policies are easier to represent and learn than their corresponding value functions, and therefore, ADP algorithms that search in a policy space should perform better than traditional value function-based ADP methods.
The paper provides a thorough review of the literature on Tetris and ADP, highlighting the limitations of previous approaches and the success of black box optimization methods such as cross entropy (CE). The authors then introduce the classification-based modified policy iteration (CBMPI) algorithm, which searches in a policy space using a classifier to approximate the greedy policy.
The experimental results demonstrate the effectiveness of CBMPI in both small and large Tetris boards, outperforming previous ADP methods and achieving performance comparable to or better than CE with significantly fewer samples. The authors also provide a detailed analysis of the algorithm's parameters and features, highlighting the importance of careful selection and tuning.
The paper's strengths include:
* A clear and well-motivated conjecture about the nature of Tetris and the potential benefits of policy-based ADP methods
* A thorough review of the literature and a clear explanation of the CBMPI algorithm
* Extensive experimental results that demonstrate the effectiveness of CBMPI in both small and large Tetris boards
* A detailed analysis of the algorithm's parameters and features
The paper's weaknesses include:
* The lack of a clear theoretical analysis of the CBMPI algorithm and its convergence properties
* The reliance on a specific set of features and parameters, which may not be optimal or generalizable to other domains
* The limited comparison to other ADP methods and the lack of a clear explanation for why CBMPI outperforms them
Overall, the paper presents a significant contribution to the field of ADP and Tetris, demonstrating the potential of policy-based methods and providing a new state-of-the-art result in the large Tetris board. However, further research is needed to fully understand the theoretical properties of CBMPI and to explore its applicability to other domains.
Arguments pro acceptance:
* The paper presents a novel and well-motivated approach to ADP in Tetris
* The experimental results demonstrate significant improvements over previous ADP methods
* The paper provides a clear and detailed explanation of the CBMPI algorithm and its parameters
Arguments con acceptance:
* The lack of a clear theoretical analysis of the CBMPI algorithm and its convergence properties
* The reliance on a specific set of features and parameters, which may not be optimal or generalizable to other domains
* The limited comparison to other ADP methods and the lack of a clear explanation for why CBMPI outperforms them
Recommendation: Accept with minor revisions to address the weaknesses mentioned above, including providing a clearer theoretical analysis and exploring the generalizability of the approach to other domains.