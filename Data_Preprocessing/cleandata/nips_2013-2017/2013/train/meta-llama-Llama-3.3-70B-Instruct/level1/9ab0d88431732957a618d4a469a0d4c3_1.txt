This paper proposes a new formulation for learning multiple models from data, called Regularized Weighting (RW), which seeks to assign a distribution of weights over data points to each model. The approach is designed to be robust to outliers and fat-tailed noise, and is shown to have a non-trivial breakdown point for the case of clustering. The authors provide a generalization bound for the RW formulation and propose an efficient optimization procedure to solve it.
The paper is well-written and clearly motivated, with a thorough discussion of the limitations of existing approaches to multiple model learning. The proposed RW formulation is novel and interesting, and the authors provide a detailed analysis of its properties and behavior. The experimental results demonstrate the robustness of the RW approach to outliers and fat-tailed noise, and the theoretical analysis provides a solid foundation for understanding the method's behavior.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem of multiple model learning and the limitations of existing approaches
* A novel and interesting formulation for learning multiple models, which is shown to be robust to outliers and fat-tailed noise
* A thorough analysis of the properties and behavior of the proposed RW formulation, including a generalization bound and an efficient optimization procedure
* Experimental results that demonstrate the effectiveness of the RW approach in practice
The weaknesses of the paper include:
* The paper assumes a certain level of familiarity with the existing literature on multiple model learning, which may make it difficult for non-experts to follow
* The optimization procedure proposed in the paper may not be efficient for very large datasets, and may require further optimization or approximation techniques to scale to larger problems
* The paper could benefit from a more detailed comparison to existing methods for multiple model learning, including a discussion of the advantages and disadvantages of the RW approach relative to other methods
Overall, I believe that this paper makes a significant contribution to the field of machine learning, and provides a novel and interesting approach to learning multiple models from data. The paper is well-written and clearly motivated, and the authors provide a thorough analysis of the properties and behavior of the proposed RW formulation.
Arguments pro acceptance:
* The paper proposes a novel and interesting formulation for learning multiple models, which is shown to be robust to outliers and fat-tailed noise
* The authors provide a thorough analysis of the properties and behavior of the proposed RW formulation, including a generalization bound and an efficient optimization procedure
* The experimental results demonstrate the effectiveness of the RW approach in practice
Arguments con acceptance:
* The paper assumes a certain level of familiarity with the existing literature on multiple model learning, which may make it difficult for non-experts to follow
* The optimization procedure proposed in the paper may not be efficient for very large datasets, and may require further optimization or approximation techniques to scale to larger problems
* The paper could benefit from a more detailed comparison to existing methods for multiple model learning, including a discussion of the advantages and disadvantages of the RW approach relative to other methods.