This paper presents a distributed optimization algorithm, Distributed Stochastic Dual Coordinate Ascent (DisDCA), for solving regularized loss minimization problems in a distributed framework. The algorithm is designed to work in a star network with multiple machines, where each machine accesses its own set of training examples. The authors provide a convergence analysis of the algorithm, including a tradeoff between computation and communication, and demonstrate its effectiveness through experiments on real datasets.
The paper is well-written and clearly explains the algorithm, its convergence analysis, and the experimental results. The authors provide a thorough review of related work, including distributed stochastic gradient descent methods and alternating direction methods of multipliers. The paper also includes a detailed comparison with other distributed algorithms, such as stochastic average gradient and stochastic ADMM.
The strengths of the paper include:
* The algorithm is well-motivated and has a strong theoretical foundation, with a clear analysis of the tradeoff between computation and communication.
* The experimental results demonstrate the effectiveness of the algorithm in practice, with competitive performance compared to other distributed algorithms.
* The paper provides a thorough review of related work and a detailed comparison with other distributed algorithms.
The weaknesses of the paper include:
* The algorithm is limited to a specific type of problem, regularized loss minimization, and may not be applicable to other types of optimization problems.
* The paper assumes a star network topology, which may not be representative of all distributed computing environments.
* The experimental results are limited to two datasets, and it would be beneficial to see results on a wider range of datasets and problem sizes.
Overall, the paper is well-written and presents a significant contribution to the field of distributed optimization. The algorithm is well-motivated and has a strong theoretical foundation, and the experimental results demonstrate its effectiveness in practice.
Arguments for acceptance:
* The paper presents a novel and well-motivated algorithm for distributed optimization.
* The algorithm has a strong theoretical foundation, with a clear analysis of the tradeoff between computation and communication.
* The experimental results demonstrate the effectiveness of the algorithm in practice.
Arguments against acceptance:
* The algorithm is limited to a specific type of problem and may not be applicable to other types of optimization problems.
* The paper assumes a star network topology, which may not be representative of all distributed computing environments.
* The experimental results are limited to two datasets, and it would be beneficial to see results on a wider range of datasets and problem sizes.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions. The paper is well-written and presents a significant contribution to the field of distributed optimization. However, the authors should consider addressing the limitations of the algorithm and providing additional experimental results to demonstrate its effectiveness on a wider range of datasets and problem sizes.