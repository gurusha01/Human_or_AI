This paper presents a novel approach to distributed submodular maximization, a fundamental problem in machine learning and data mining. The authors propose a simple, two-stage protocol called GREEDI, which can be easily implemented using MapReduce-style computations. The paper provides a thorough theoretical analysis of the approach, including bounds on its performance compared to the centralized greedy solution.
The strengths of the paper include:
* The proposed approach is highly scalable and can handle massive datasets, making it suitable for real-world applications.
* The theoretical analysis provides strong guarantees on the performance of the approach, including bounds on the approximation ratio and the running time.
* The experimental evaluation demonstrates the effectiveness of the approach on several applications, including exemplar-based clustering and active set selection.
The weaknesses of the paper include:
* The approach assumes that the submodular function can be evaluated independently on each machine, which may not be the case in all applications.
* The theoretical analysis relies on certain natural conditions, such as the existence of a metric on the data elements, which may not always hold in practice.
* The experimental evaluation could be more comprehensive, including more applications and datasets.
Arguments pro acceptance:
* The paper presents a novel and scalable approach to distributed submodular maximization, which is a fundamental problem in machine learning and data mining.
* The theoretical analysis provides strong guarantees on the performance of the approach, making it a reliable choice for practitioners.
* The experimental evaluation demonstrates the effectiveness of the approach on several applications, including exemplar-based clustering and active set selection.
Arguments con acceptance:
* The approach assumes that the submodular function can be evaluated independently on each machine, which may not be the case in all applications.
* The theoretical analysis relies on certain natural conditions, which may not always hold in practice.
* The experimental evaluation could be more comprehensive, including more applications and datasets.
Overall, I believe that the paper presents a significant contribution to the field of machine learning and data mining, and its strengths outweigh its weaknesses. I recommend acceptance. 
Quality: 8/10
The paper is technically sound, and the theoretical analysis is thorough and well-presented. However, the approach assumes that the submodular function can be evaluated independently on each machine, which may not be the case in all applications.
Clarity: 9/10
The paper is well-written, and the presentation is clear and concise. The authors provide a thorough introduction to the problem and the proposed approach, making it easy to follow.
Originality: 8/10
The paper presents a novel approach to distributed submodular maximization, which is a fundamental problem in machine learning and data mining. However, the approach builds on existing work on submodular maximization and distributed optimization.
Significance: 9/10
The paper presents a significant contribution to the field of machine learning and data mining, with potential applications in several areas, including exemplar-based clustering, active set selection, and sparse Gaussian process inference.