This paper proposes a parameter server system for distributed machine learning (ML) that follows a Stale Synchronous Parallel (SSP) model of computation. The SSP model allows workers to read older, stale versions of model parameters from a local cache, rather than waiting for the latest updates from a central storage. This approach maximizes the time workers spend on useful computation, while still providing correctness guarantees. The paper provides a proof of correctness under SSP and demonstrates its effectiveness through empirical results on several ML problems, showing faster convergence compared to fully-synchronous and asynchronous schemes.
The paper is well-written, and the authors provide a clear explanation of the SSP model and its benefits. The theoretical analysis is thorough, and the experimental results are convincing. The paper also provides a good overview of related work in the field.
The strengths of the paper include:
* The proposal of a novel SSP model that balances computation and communication in distributed ML
* A thorough theoretical analysis of the SSP model, including a proof of correctness and convergence bounds
* Empirical results demonstrating the effectiveness of SSP on several ML problems
* A good overview of related work in the field
The weaknesses of the paper include:
* The paper assumes a specific distributed architecture, which may not be applicable to all scenarios
* The experimental results are limited to a few ML problems and may not generalize to other problems
* The paper could benefit from a more detailed discussion of the trade-offs between computation and communication in distributed ML
Arguments for acceptance:
* The paper proposes a novel and effective approach to distributed ML
* The theoretical analysis is thorough and well-supported by empirical results
* The paper provides a good overview of related work in the field
Arguments against acceptance:
* The paper assumes a specific distributed architecture, which may limit its applicability
* The experimental results are limited and may not generalize to other problems
* The paper could benefit from a more detailed discussion of the trade-offs between computation and communication in distributed ML
Overall, I believe that the paper is well-written and provides a significant contribution to the field of distributed ML. While there are some limitations to the paper, the strengths outweigh the weaknesses, and I recommend acceptance. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and empirical results.
Clarity: 9/10
The paper is well-written, and the authors provide a clear explanation of the SSP model and its benefits.
Originality: 8/10
The paper proposes a novel approach to distributed ML, but it builds on existing work in the field.
Significance: 9/10
The paper provides a significant contribution to the field of distributed ML, and the results have the potential to impact the development of large-scale ML systems.