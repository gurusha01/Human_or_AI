This paper presents a novel technique for reducing the number of parameters in deep neural networks by exploiting the structure in the weights of the network. The authors demonstrate that by representing the weight matrix as a low-rank product of two smaller matrices, they can predict a significant portion of the weights without a substantial drop in accuracy. The technique is shown to be effective in various architectures, including multilayer perceptrons, convolutional networks, and reconstruction ICA.
The paper is well-written, and the authors provide a clear motivation for their work, as well as a thorough explanation of their technique. The experiments are well-designed, and the results are impressive, with the authors able to predict more than 95% of the weights in some cases without a drop in accuracy.
The strengths of the paper include:
* The technique is novel and has the potential to significantly reduce the number of parameters in deep neural networks, which could lead to improvements in computational efficiency and scalability.
* The authors provide a thorough analysis of their technique, including a discussion of the potential benefits and limitations.
* The experiments are well-designed, and the results are impressive, demonstrating the effectiveness of the technique in various architectures.
The weaknesses of the paper include:
* The technique may not be applicable to all types of neural networks, and the authors do not provide a clear understanding of when the technique is likely to be effective.
* The authors do not provide a detailed analysis of the computational complexity of their technique, which could be an important consideration in practice.
* Some of the experiments could be more thorough, with more variations in the architecture and hyperparameters explored.
Arguments for acceptance:
* The paper presents a novel and potentially significant technique for reducing the number of parameters in deep neural networks.
* The experiments are well-designed, and the results are impressive, demonstrating the effectiveness of the technique.
* The paper is well-written, and the authors provide a clear motivation for their work, as well as a thorough explanation of their technique.
Arguments against acceptance:
* The technique may not be applicable to all types of neural networks, and the authors do not provide a clear understanding of when the technique is likely to be effective.
* The authors do not provide a detailed analysis of the computational complexity of their technique, which could be an important consideration in practice.
* Some of the experiments could be more thorough, with more variations in the architecture and hyperparameters explored.
Overall, I believe that the paper is a strong contribution to the field of deep learning, and I recommend acceptance. However, I also believe that the authors could benefit from addressing some of the weaknesses mentioned above, such as providing a more detailed analysis of the computational complexity of their technique and exploring more variations in the architecture and hyperparameters. 
Quality: 8/10
The paper is technically sound, and the authors provide a clear explanation of their technique. However, some of the experiments could be more thorough, and the authors do not provide a detailed analysis of the computational complexity of their technique.
Clarity: 9/10
The paper is well-written, and the authors provide a clear motivation for their work, as well as a thorough explanation of their technique.
Originality: 9/10
The technique presented in the paper is novel and has the potential to significantly reduce the number of parameters in deep neural networks.
Significance: 9/10
The paper has the potential to make a significant contribution to the field of deep learning, and the results are impressive, demonstrating the effectiveness of the technique in various architectures.