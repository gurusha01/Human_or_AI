Summary: 
This paper proposes an inference algorithm for binary latent feature models (LFMs), which represent each observation as a binary vector indicating the presence or absence of latent features. The algorithm, based on Expectation-Maximization (EM) and utilizing a shrinkage mechanism, learns the binary vectors and infers the number of latent features (K). This approach is reminiscent of nonparametric Bayesian latent feature models like the Indian Buffet Process (IBP), but instead of using prior distributions for model parsimony, it employs Factorized Asymptotic Bayesian Inference (FAB), a model selection criterion recently introduced for mixture models and Hidden Markov Models (HMMs). FAB expresses the log marginal likelihood in terms of the expected complete log-likelihood plus a model selection term that encourages the shrinkage of latent features by specifying an upper bound on K and learning the correct K during inference. The algorithm outperforms other methods, including Gibbs sampling, variational inference, and MAP estimation for the IBP, in terms of running time and inference quality on large datasets.
Quality: The technical quality of the paper appears sound, with correct algorithmic details. The ability to handle large datasets and learn the number of latent features is particularly appealing.
Clarity: The methodology section is reasonably clear, although some experimental details lack clarity, as discussed below.
Originality: While the proposed method builds upon the recently introduced FAB framework for mixture models and HMMs, its application to latent feature models is novel.
Significance: The paper addresses the important problem of efficient inference in latent feature models while also inferring the number of latent features.
Weak Points: 
- The paper lacks a discussion on the limitations of the proposed algorithm, including scenarios where it might perform worse than standard Gibbs sampling.
- There is no consideration of potential convergence difficulties, given the EM-like procedure.
- The algorithm is restricted to binary latent feature models and cannot be applied to factor analysis or probabilistic PCA.
- The experimental evaluation is insufficient and flawed in some aspects, as detailed below.
- The behavior of the algorithm in the small-data regime, where the asymptotic argument may not hold, is unclear and warrants discussion or experimental investigation.
- The paper overlooks recent works on efficient inference in LFMs, which should be discussed.
- The comparison with the MEIBP algorithm is invalid due to the artificial simulation and block data experiments not meeting the requirements for MEIBP (non-negative linear Gaussian models).
Other Comments: 
- For fairness, the IBP Gibbs sampler should be given the known noise value in the block images data experiment, rather than using a heuristic.
- The experimental settings for VB-IBP are not detailed enough, and the infinite variational version could be used for comparison.
- The discovery of about 70 latent features by accelerated Gibbs sampling on the 49-dimensional Sonar data seems suspect, possibly due to bad initialization or a poorly specified noise variance value.
- The reference to [21] is incorrect, as it pertains to MAP estimation for the IBP, not Gibbs sampling.
- Recent work on efficient inference using small-variance asymptotics for nonparametric LFMs, such as "MAD-Bayes," should be discussed.
Minor Comments: 
- The suggestion in [1,22] for real-data experiments was to set the standard deviation to 0.25 times the standard deviation of examples across all dimensions, not 0.75.
Comments after Author Feedback: 
- The experimental methodology needs better explanation, including justifications for the behavior of certain algorithms.
- Reference [21] should be corrected to reflect that it performs MAP inference, not sampling.
- The reference to MAD-Bayes inference for the IBP should be included. The proposed inference method for LFMs is interesting, especially its scalability to large datasets and ability to infer K. However, the experimental evaluation requires more careful execution.