The authors have presented a feature compression method that involves optimizing a formulation, which accounts for both pointer and dictionary costs. To address the non-convex nature of this formulation, due to its binary constraints, the constraints are relaxed to box constraints within the range of 0 to 1, and the Alternating Direction Method of Multipliers (ADMM) is employed to solve the resulting relaxed problem. The experimental results are compared to several other approaches.
The proposed method for compressive feature learning is based on a loss compression formulation that considers both pointer and dictionary costs, leading to a non-convex problem due to the binary constraints. This formulation appears to be novel.
In tackling the relaxed problem, the authors utilize ADMM, which involves solving a series of relaxed problems dependent on the parameter d, incorporating a reweighting scheme. A question arises regarding the efficiency of the proposed compression approach. It is noted that for subsequent classification tasks, the proposed method can lead to improved classification speed, which is a logical outcome.
Following feature extraction, the elastic net method is applied for classification purposes. This method involves two tuning parameters; however, the process for tuning these parameters via cross-validation, such as using a two-dimensional grid search, is not clearly outlined.
After considering the comments from other reviewers and the authors' response, the original recommendation is maintained. The method proposed by the authors aims to compress features by optimizing a formulation that considers both pointer and dictionary costs. The non-convex formulation, resulting from binary constraints, is addressed by relaxing these constraints to box constraints between 0 and 1, with ADMM used to solve the relaxed problem.