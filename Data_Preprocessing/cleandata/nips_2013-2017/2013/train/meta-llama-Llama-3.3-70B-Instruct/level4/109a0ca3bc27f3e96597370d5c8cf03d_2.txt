Additional comments after the rebuttal: 
The authors must provide a more compelling motivation for the problem's relevance to machine learning, making the paper more accessible to a broader audience. To achieve this, they should introduce a latent variable model, accompanied by a graphical representation, where the tensor decomposition learns the parameters. For instance, consider a model where latent variable $h1$ is connected to observed variable $x1$, $h2$ to $x2$, and so on, with a linear relationship between hidden and observed variables and arbitrary dependencies among the $hi$'s. In this scenario, the observed moment tensor would have a Tucker decomposition, and its multilinear rank would be determined by the dimensions of $h1$, $h_2$, etc. The authors should add this discussion and ideally include an illustrative figure. Furthermore, they should note that it is possible for only some hidden variables to have small dimensions, not all, which is a key advantage of the new result presented in the paper.
The paper explores convex-optimization-based tensor decomposition using structured Schatten norms for tensors. The authors utilize the overlapped Schatten norm and demonstrate its ability to yield better reconstruction guarantees. The paper is well-written and makes significant technical contributions. Addressing a few technical and notational issues will enhance the paper's readability.
*Clarification about Theorem 2 and its proof: 
The statement of Theorem 2 is technically inaccurate, as the components are recovered up to a permutation. Although a minor issue, the authors should acknowledge this.
The transition from equation (12) to equation (13) in Theorem 2 requires justification.
The proof of Lemma 5 is missing, which is a crucial component of Theorem 2.
It would be beneficial to explicitly clarify the proof of Lemma 4 instead of referring to another paper, despite its straightforwardness.
*Minor issues: 
The authors should include more related work on Higher-Order Singular Value Decomposition (HOSVD) and other tensor decomposition methods. They can discuss how higher-order SVD and its variants provide approximation bounds for tensor decomposition, as outlined in Kolda's review on tensor decompositions (section 4).
The authors refer to the notion of rank as mode-rank, which is also known as multilinear rank. They should mention this and note that there are different notions of tensor rank in the introduction.
A brief discussion on the computational complexity of the employed methods would be beneficial.
The authors' discussion about incoherence after equation (10) requires clarification. They use the term to imply that the "energy" should be distributed across different mixture components and modes, meaning no component or mode can have too high a singular value. However, this is not equivalent to the incoherence conditions in [1,3,10], which are used for sparse and low-rank recovery. The condition in [1,3,10] involves the singular vectors being incoherent with the basis vectors, differing from the condition presented here. It is suggested that this comparison be removed.
*Typos/notational issues: 
The superscript notation for components $\hat{\cal{W}}^{(k)}$ is confusing, and an alternative symbol for components is recommended.
The notation $\bar{r}k$ and $\underline{r}k$ for k-mode rank is inconsistent and should be changed. Introducing a notation like $mult\_rank(tensor)$ to denote the multilinear rank could be beneficial.
There is a missing parenthesis in the last line of page 12 in the supplementary material.
In the abstract, "result for structures Schatten" should be corrected to "structured". The paper is well-written and makes a novel contribution to finding a good Tucker tensor decomposition using an overlapped Schatten norm, providing guarantees for recovery. However, some technical clarifications are necessary, which the authors should address.