This paper proposes a reduction approach aimed at downsizing deep models to enhance training efficiency in deep learning. The primary contributions include: (1) investigating prior knowledge to identify redundancy in deep models, such as spatial parameter smoothness in image data; and (2) utilizing kernel ridge regression for interpolating parameters from a subset.
The paper is well-structured and clearly articulated, with the work appearing to be original. However, the authors' claim that their approach is highly generalizable, including applicability to non-image tasks, is not adequately substantiated. Although they outline potential extensions for handling non-image data, such as autoencoder fine-tuning, the experimental section lacks validation on non-image datasets, rendering this aspect of their claim weak.
Furthermore, the experimental evaluations on image datasets are limited to small-scale datasets with relatively simple patterns. It is challenging to ascertain whether methods that perform well on datasets like CIFAR would yield comparable results on more complex and realistic datasets such as ImageNet. Consequently, the overall value and impact of this work are not entirely persuasive.
Upon reviewing the authors' rebuttal, my assessment remains unchanged. While the work introduces novel and intriguing concepts for reducing deep network sizes, the experimental demonstrations fall short of convincingly establishing the value of this research.