This manuscript explores the tradeoff inherent in hierarchical classification, where deeper hierarchies require more decisions, potentially propagating errors, whereas flatter hierarchies involve fewer decisions but with increased complexity due to higher cardinality. 
The authors propose a data-dependent generalization error bound for kernel-based hypotheses, with their main theorem establishing an upper bound on the generalization error of a hierarchical classifier. This bound is expressed in terms of empirical error and Rademacher complexity, which respectively favor flat and deep classifiers.
The paper leverages insights from this generalization bound to develop a strategy for pruning hierarchical classifiers. However, the methodology involving a metaclassifier, as described starting on line 307, lacked clarity, particularly in terms of its motivation and relation to improvements in the generalization bound.
The manuscript is well-written and addresses a significant problem, contributing theoretically to the field of hierarchical classification. A major concern is the lack of clear guidance on applying the authors' insights to practical hierarchical classification tasks. The authors could enhance the paper by providing a more detailed explanation of the practical implications for pruning hierarchies to improve classification performance. Overall, while the paper excels as a theoretical contribution to hierarchical classification, it falls short in translating these theoretical insights into practical procedures for enhancing hierarchical classification outcomes.