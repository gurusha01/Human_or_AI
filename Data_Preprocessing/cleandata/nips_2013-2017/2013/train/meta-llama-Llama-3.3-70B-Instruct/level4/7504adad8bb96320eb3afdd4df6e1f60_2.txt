This manuscript examines the application of a classifier-based approximate policy iteration algorithm, referred to as CBMPI, to the game of Tetris, a widely studied benchmark task in reinforcement learning. The performance of CBMPI is compared to that of DPI, a variant that omits the regression step used for value function estimation; lambda-PI, a related policy iteration method that utilizes eligibility-trace-like backups and omits the classification step of CBMPI; and CE, a policy-search method currently considered state-of-the-art in Tetris. The experiments are conducted on both small and large boards, employing different state features, and the best policies obtained across runs for each method are further evaluated over 10,000 games.
Although this paper does not offer any theoretical or algorithmic advancements, its empirical analysis has the potential to be highly significant. Given that Tetris is a well-studied benchmark task, achieving a substantial improvement over the current state-of-the-art is a notable accomplishment. Furthermore, since the state-of-the-art CE approach is characterized by high sample complexity, making progress in obtaining strong performance from more sample-efficient value-function methods is of considerable interest, particularly if the results provide insight into why previous efforts with value-function methods were less successful.
However, two primary concerns are raised. Firstly, the paper's attempt to address the question of why CBMPI outperforms other methods is found to be unsatisfactory. Secondly, the experimental comparisons are compromised by confounding factors that substantially undermine the validity of the conclusions drawn from them.
Regarding the explanation for CBMPI's success, the authors propose that the effectiveness of CE suggests that, in Tetris, policies may be easier to represent than value functions. This hypothesis is reasonable and consistent with the notion that good policies can sometimes be simpler than accurate value functions, which may give policy search an advantage. Nevertheless, the authors' subsequent suggestion that this hypothesis implies the use of ADP methods that search in policy space rather than value function space is not convincing. They propose CBMPI as a candidate, but it is essentially an actor-critic method that explicitly represents both the value function and the policy, thereby not circumventing the difficulties of representing a value function. Although CBMPI conducts a search in policy space for a classifier-based policy consistent with estimated Q-values, which differs from many ADP methods, no argument is provided as to why this should make Tetris easier to solve. The superior performance of CBMPI compared to lambda-PI, which does not explicitly search in policy space, may lend credence to the authors' claim, but confounding factors in these comparisons render the conclusion unconvincing.
The experimental comparisons are plagued by several concerns. Firstly, CBMPI relies on rollouts starting from a set of states sampled from a distribution mu, which already raises questions about comparing it to CE, as CBMPI requires a generative model whereas CE only requires a trajectory model. Furthermore, the set of states is not sampled from a stationary mu but from trajectories generated by a strong Tetris policy, DU, assumed to be available as prior knowledge. However, lambda-PI and CE were deprived of this prior knowledge, which raises serious questions about the fairness of the comparisons. 
Additionally, in the small-board experiments using the D-T features, CBMPI was optimized for the feature settings, and the best performance was obtained using D-T plus the 5 RBF height features. However, it appears that the other methods used only the D-T features, making the comparisons in Figure 4 potentially unfair. In Figure 5a-c, where all methods use the same Bertsekas features, no performance improvement is observed for CBMPI. In Figure 5d, CBMPI again shows some performance improvement, but it is unclear whether the other methods are using the same features.
Moreover, while CBMPI outperforms DPI after 4 iterations, DPI performs better from 2-4 iterations. It is misleading to conclude that CBMPI achieved equal performance with fewer samples, as this is only true at specific points on the learning curve. CBMPI learns faster but does not equal CE's final performance.
Another concern is the lack of variance analysis in the experimental results, which is particularly problematic given the high variance in scores for a given policy in Tetris. The differences presented in Table 1 are likely to be statistically insignificant due to the high cross-game variance.
A minor comment is that Figures 4 and 5 would be more easily interpretable if the x-axis represented samples rather than iterations, allowing for a direct and fair comparison of the methods.
In conclusion, this paper presents strong results for value-function methods on Tetris but fails to provide a convincing explanation for why this is the case. The empirical comparisons are compromised by confounding factors, rendering the conclusions unconvincing.