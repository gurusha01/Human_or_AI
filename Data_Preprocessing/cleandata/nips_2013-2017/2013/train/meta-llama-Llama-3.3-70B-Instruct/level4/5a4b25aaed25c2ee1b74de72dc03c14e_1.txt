This paper proposes an innovative approach to enhance the mixing rate of Gibbs sampling in pairwise Ising models with strong interactions, which are notorious for being "slow-mixing". The authors introduce several projections to "fast-mixing" models, essentially identifying parameters that closely approximate the original model while being sufficiently weak to satisfy a spectral bound, thereby ensuring rapid mixing. Experimental results demonstrate improved marginal estimates for certain regimes, outperforming variational methods, such as mean field and belief propagation variants, and Gibbs sampling in the original model, given a fixed computation time.
The technical framework relies heavily on prior research establishing conditions for rapid mixing, as seen in references [4,8]. However, the concept of projecting onto the nearest rapidly mixing model as an approximate inference method appears to be a novel contribution. 
In the context of "toy tiny Ising models", the results show promising improvements over a range of baselines, including standard sampling and contemporary variational methods. Nevertheless, it is crucial to approach these findings with a heightened sense of skepticism regarding their scalability to larger models. The experiments involve comparisons of mixing times of samplers across different models, making it challenging to assess how these will scale with problem size. Furthermore, the projection step required to construct the fast-mixing model seems computationally expensive, and its cost does not appear to be accounted for. Additionally, it is unclear whether generalization beyond binary states will be feasible, as establishing convergence bounds in more general cases is significantly more daunting.
CLARITY:
The presentation is generally clear and accessible, considering the technical content. However, some sections require reorganization and clarification:
* The paper frequently references "standard results" without providing citations or proof sketches, such as Theorem 6. Lemma 5 is also left unproven, with the reference only proving the special case of zero-field.
* The properties of the dependency matrix R should be discussed, including its tractability. The maximization involved appears to make it intractable, which may explain the invocation of Lemma 5.
* The latter half of Section 4 is difficult to follow, with references to g, M, and Lambda before their introduction. The statement of Theorem 4 includes unexplained notation. If this optimization is to be discussed, more explanation is necessary.
* In Section 1, the notation for KL divergence is used inconsistently, with KL(q||p) representing both directions of the divergence. The notation should be adjusted to reflect the conventional usage in related literature.
EXPERIMENTS:
The time comparison in Figure 2 seems to exclude the computation required for projection, as hinted at in Section 6.1, although the statement is ambiguous. Clarification is needed, and if the projection time is indeed excluded, results including this time should be provided. As it stands, the proposed methods appear to benefit from the output of a sophisticated variational optimization without incurring a penalty, which diminishes the convincingness of the improvement over standard Gibbs.
It is disappointing that experiments on larger models are not included, given the common dependence of Gibbs mixing times on problem size. Alternative experimental regimes could be explored, such as using models with symmetries that allow for the computation of true marginals or taking the output of a long sampling run as the truth.
Although KL(\theta||\psi) is generally intractable, exploring its potential as a "best case" scenario for sampling in an approximate model could be interesting. Junction tree methods could be employed for the toy models used in the submission.
The significant difference between mean field error and the error from reverse KL projection deserves discussion, especially since mean field is a degenerate case of the reverse KL projection. 
The concept of approximating slow-mixing models by projecting to the closest fast-mixing model is intriguing, and the paper leverages recent work on mixing bounds elegantly. However, concerns regarding experimental comparisons and the limited applicability of this approach to larger models warrant further attention.