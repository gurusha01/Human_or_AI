This paper introduces the parsimonious triangular model (PTM), a constrained version of the mixed-membership triangular model (MMTM), which reduces the parameter space from O(K^3) to O(K) for faster inference. The authors develop a stochastic variational inference algorithm for PTM and propose additional approximation techniques to enhance scalability. Experimental results on synthetic and real-world datasets demonstrate that PTM can achieve stronger statistical power and competitive accuracy with existing methods.
Quality: 
While PTM appears to be an intriguing specialization of MMTM, its practical advantages in terms of scalability with respect to K (the number of possible roles) are unclear. To empirically evaluate the method's value, it is essential to investigate how learning MMTM with large K can be beneficial. Given that MMSB and MMTM are mixed-membership models, using small K may not be as problematic as in single-membership models. A more comprehensive analysis would involve comparing the performance of MMTM and PTM as a function of K, which could demonstrate PTM's superiority with large K. Unfortunately, such experiments are lacking. Furthermore, the selection of K values in real-data experiments for both PTM and MMSB is not clearly described, which could lead to biased results.
The authors are somewhat reserved regarding the implications of their simplifying assumptions. Although constraining the parameter space can improve scalability, it is crucial to discuss the potential limitations of the model. If the parameter matrix B in MMSB and MMTM indeed contains O(K^3) distinct values, PTM may perform poorly. The choice of true B in synthetic data experiments is not explicitly described, and the authors should have explored various B values to demonstrate both favorable and unfavorable scenarios for PTM. Understanding when PTM succeeds and fails is more valuable than solely presenting its successes.
On the other hand, the authors introduce several approximation techniques to improve PTM's scalability, which are interesting and effective. However, the paper fails to adequately describe and analyze the impact of these approximations. Key questions remain unanswered: (1) What is the effect of choosing the threshold Î´, and how does it influence the results on real-world and synthetic data? (2) Why is PTM immune to the issues that affect a-MMSB, such as downsampling compromising accuracy? (3) What is the impact of the O(K) approximation on local updates, and how does the quality of the approximation change when sampling more triples?
In summary, the experimental setup may not be fair to competing methods, and the discussion of the algorithm's/model's potential weaknesses is inadequate.
Clarity: The paper is well-written and organized.
Originality: Although the modeling approach is somewhat incremental, achieving scalability through parameter constraints is an interesting and original direction, particularly in the context of recent work focused on generalizing existing models.
Significance: If the simplifying assumptions in the model and inference algorithm hold in practice, PTM could have a significant impact on the application of latent-space models to networks, as many current models struggle to scale to large datasets.
Additional Comments: The title may be too broad, as there are other scalable approaches to latent space models, and the authors propose a scalable algorithm for a specific model, PTM. Furthermore, it is unclear whether PTM is truly a probabilistic method, as the authors' argument that it is similar in spirit to the bag-of-words assumption is unconvincing. The generative model for networks is not properly defined, and the majority of the posterior probability mass is placed on non-networks, which may lead to issues with the basic principles of likelihood in "probabilistic" methods. The authors' proposed modeling assumptions and inference techniques may have unintended consequences that require further investigation.