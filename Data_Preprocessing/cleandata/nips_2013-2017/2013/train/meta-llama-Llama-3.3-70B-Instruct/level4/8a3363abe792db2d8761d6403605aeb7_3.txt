This paper presents a comprehensive treatment of total variation on hypergraphs, including its convexity, relationship to cuts, and applications to learning and normalized cuts for clustering. The authors also derive proximity operators as part of optimization algorithms and provide numerical experiments comparing their approach to a popular method by Zhou from 2006. The paper effectively navigates multiple sub-disciplines, offering a wealth of ideas and theorems, although the complexity of the normalized cut problem limits the scope of the results.
The paper remains focused despite covering a broad range of topics and is well-written, with only occasional grammatical issues. Section 5, which falls within my area of expertise, is satisfactory. However, I am less familiar with existing work on spectral clustering and hypergraph approaches, and the authors' claim of being the first major advance in 7 years since Zhou's work warrants some skepticism. If true, this paper deserves acceptance and is likely to become well-known.
Several comments are worth noting: 
- The comparisons with Zhou's method, which uses the same hypergraph framework, could be strengthened by including an "outside" approach, such as k-means for clustering, to provide a more comprehensive evaluation.
- The optimal cut should be referred to as "an optimal cut" due to symmetry.
- The section surrounding Theorem 4.1 requires more explanation, particularly regarding the application of the second equation.
- The concept of "tight relaxation" needs clarification, as it is unclear whether it refers to the tightest relaxation or a tighter one compared to the linear eigenproblem relaxation.
- In Section 5, the method from [24] has been extended, and it can exploit smooth functions, which could be beneficial for convergence rates.
- The definition of total variation on hypergraphs appears to generalize anisotropic TV, and exploring the possibility of defining an isotropic TV on hypergraphs could be interesting.
- Proposition 5.1 can be improved by avoiding the sort and directly finding the largest and smallest entries, reducing the complexity to O(n).
- The experiments are well-done, but considering edges between data points with similar feature values could be beneficial.
- The table on page 8 requires explanation, and the claim that the method minimizes the normalized cut should be revised to "attempts to minimize" due to the non-convex nature of the problem.
Minor grammatical corrections include using "proximity operator" instead of "proximum" and "proxima," rephrasing sentences with "allow to" constructions, and correcting the usage of "however" and "thus." Overall, this paper has the potential to be significant, introducing a new topic and exploring it thoroughly with good results, but the authors should ensure they have not overlooked recent relevant work.