This manuscript presents a pruning method that facilitates symbolic modified policy iteration in large factored action Markov Decision Processes (MDPs). The proposed technique builds upon regular MPI, extending policy iteration and valuation, and incorporates prior research on partially bound actions, with experimental validation.
I mildly endorse this paper for acceptance, as the primary contribution seems to be an important implementation detail. The experiments demonstrate that pruning is beneficial for value iteration, necessary for modified policy iteration due to memory constraints, and that modified policy iteration enhances convergence in factored action MDPs.
The paper is well-motivated, but the notation is occasionally inconsistent and difficult to follow. For instance, Algorithm 3.2 is referred to as "Prune" but denoted as \cal P elsewhere, and it is unclear from the text that T^Q(V) is a function of states and actions, or even that the variables are binary.
My primary concern is that I struggled to comprehend the details of Theorem 1. Specifically, it is unclear why \hat T^Q\pi may differ from T^Q\pi. Is pruning required at every step, or is a single pruning sufficient? Does repeated pruning cause overestimation, and is the convergence theorem identical for FA-MPI and OPI?
Proposition 2 appears straightforward. Are there any guarantees on the extent of reduction in the pruned tree's size?
Despite these concerns, I suggest accepting this paper. At a high level, it is well-motivated and clearly written, and the experiments showcase its capability to address previously intractable problems.