On page 2, the claim that standard reductions are inapplicable for proving NP-hardness warrants further nuance, as certain reduction classes may not facilitate basing hardness of learning on RP â‰  NP, a notion supported by the work of Applebaum, Barak, and Xiao.
The distinction made between not relying on cryptographic assumptions and the complexity-theoretic assumption used is noteworthy. However, the relative strengths of these assumptions, particularly in comparison to the existence of one-way functions, require clarification. Research by Noam Livne suggests that average-case hard functions in NP might be a weaker assumption than the existence of one-way functions. A discussion on the comparative hardness of the assumptions employed in this work versus those in other studies, such as whether the results hold under strong cryptographic assumptions like the hardness of factoring, would be beneficial.
Regarding the upper bounds, it seems plausible that using the presented techniques, one could demonstrate that H_{n, k+1} can be learned with approximately \tilde{O}(n^k/\epsilon^2) samples. Furthermore, exploring whether the lower bound could be extended, potentially under different assumptions, to show an increasing gap between information-theoretic sample complexity and computational complexity as k grows, presents an interesting avenue for further investigation.
Minor corrections include:
1. Line 130 should refer to the proof "of" item 1.
2. On line 343, the intention seems to be y_k = b.
Incorporating a discussion about the complexity assumption in relation to other assumptions would significantly enhance the paper's value, particularly for a theoretical audience. The result highlights that "hardness conjectures" in learning may be weaker than those commonly accepted in other areas.
The presentation of a simple concept class where a separation exists is indeed intriguing. Clarifying that the assumption is not necessarily weaker than the existence of one-way functions would add precision to the narrative.
The paper contributes to the line of research initiated by Decatur, Goldreich, and Ron, demonstrating a separation between information-theoretic sample complexity and computational complexity under a specific complexity-theoretic assumption for learning halfspaces with low Hamming weight Boolean vectors. While polynomial-time learning is infeasible with sufficient sample complexity for information-theoretic learning, it becomes possible with a significantly larger sample size.