This manuscript proposes a novel convex formulation for training a two-layer model in a supervised learning setting. The approach involves integrating large margin losses into the training objectives, utilizing indmax transfer for the second layer and a multi-label perception model with step transfer for the first layer, ultimately applying convex relaxations that facilitate global optimization of the two-layer model.
The manuscript is well-structured and clearly explains the process of combining these elements to derive a convex objective, providing an interesting and informative insight into the methodology. The authors effectively outline the technical steps taken to achieve the convex formulation, presenting notable intermediate results along the way. From a deep learning perspective, the proposed global training methodology is substantial, and the experimental results convincingly demonstrate the importance of global training.
Overall, the manuscript presents a robust convex approach to training a conditional two-layer model, with sound technical contributions and a significant, insightful conclusion.