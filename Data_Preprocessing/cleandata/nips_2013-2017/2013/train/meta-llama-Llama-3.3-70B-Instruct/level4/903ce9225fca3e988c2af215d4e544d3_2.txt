This manuscript presents a compelling example of a learning problem that exhibits a striking dichotomy: it becomes computationally feasible with a sufficient amount of data, but remains computationally intractable (albeit information-theoretically tractable) with limited data. This intractability is grounded in a complexity-theoretic assumption regarding the hardness of differentiating satisfiable 3SAT formulas from random ones at a specified clause density, namely the 3MAJ variant of the conjecture.
The authors focus on the problem of learning halfspaces over 3-sparse vectors, complementing their negative results with nearly matching positive results, contingent upon a stronger complexity-theoretic conjecture - that hardness persists even for random formulas with a density of n^mu above the satisfiability threshold. However, the algorithmic results are relegated to the Appendix and lack discussion. It appears that these results build upon modifications of Hazan et al.'s 2012 work, and a high-level overview of the algorithm would be beneficial, even if formal proofs of correctness cannot be included in the main body.
Overall, the paper is well-liked, but its presentation could be significantly improved. The proof of 3.1 is more transparent than the high-level intuition provided on page 6, which could be condensed without sacrificing clarity, thereby allowing for a discussion of the algorithmic results in section 4.
Some minor suggestions include rephrasing 'refuting random 3' as 'distinguishing high-value formulas from random' and correcting typos such as 'it would have necessarily return' and 'with proper efficient algorithm'. Additionally, 'proof item 1' on page 3 and 'B(n)-decomposable' in Appendix A could be revised for clarity. This paper has the potential to become a classic example of the distinction between information-theoretic and computational tractability thresholds, and with revisions, its presentation can be enhanced to better showcase its significant results.