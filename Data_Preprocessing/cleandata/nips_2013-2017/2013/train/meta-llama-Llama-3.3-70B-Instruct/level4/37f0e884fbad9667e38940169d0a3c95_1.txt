I assessed this manuscript as having a quality of 7 out of 10, with clarity and originality scores of 8 and significance rated at 7.
Summary:
The authors address the optimization of a smooth, strongly convex function over a convex constraint set, where the gradient mapping update can be computed efficiently. They propose a hybrid algorithm, Epoch Mixed Gradient Descent (EMGD), which leverages both the expensive full gradient and a cheaper stochastic gradient oracle. EMGD achieves a linear convergence rate, requiring O(log 1/eps) calls to the full gradient oracle and O(k^2 log(1/eps)) calls to the stochastic gradient oracle, provided the condition number is not excessively large. This approach could be faster in theory than existing methods.
The core idea behind EMGD involves replacing a full gradient step with a fixed number of mixed gradient steps, combining the full gradient (computed once per epoch) with stochastic gradients (which vary within an epoch). By averaging the iterates within an epoch, the authors demonstrate a constant decrease in suboptimality, independent of the condition number. A self-contained proof of the convergence rate is provided, although no experiments are included.
Evaluation Summary:
The strengths of this paper include its novel and interesting algorithmic idea, a clean and simple theory, and the ability to achieve a linear convergence rate for regularized empirical loss minimization with a condition number that does not multiply the number of training examples. The algorithm is more general, applicable to constrained optimization, and has a simpler proof compared to existing methods like SAG. The paper is clearly written, with a self-contained proof.
However, the paper lacks experimental validation to demonstrate the practical relevance of the algorithm. Additionally, there is no discussion of the algorithm's limitations or drawbacks, particularly in comparison to existing algorithms. The proof could benefit from high-level comments to justify the essential insights used in its construction.
Quality:
The paper is technically sound, but experiments would have enhanced its value. The authors should extend the discussion on limitations and drawbacks, particularly in section 3.4, and provide a more concrete analysis of the sum of n functions example to highlight the differences with SAG and Nesterov. A significant drawback of EMGD is the need to fix the number of steps within an epoch in advance, unlike SCDA and SAG, which can adapt and potentially benefit from faster practical convergence.
Clarity:
The paper is fairly clear, with a useful summary in Table 1. However, the proof would be more accessible with additional high-level comments explaining the motivation behind certain defined quantities.
Originality:
The paper presents a novel combination of known techniques.
Significance:
While the practical relevance of the algorithm is not yet demonstrated, the theoretical contribution could have an impact, particularly given the simplicity of the proof compared to other methods like SAG.
Detailed Suggestions:
Several suggestions are provided for improving the clarity and completeness of the paper, including specifying citations for rates in Table 1, clarifying the conditions under which SCDA and SAG operate, and correcting minor typos and notational inconsistencies. The authors should also consider discussing the regime in which the regularization parameter is C/n, leading to a condition number of ~= n/C', and cite relevant work on hybrid deterministic-stochastic methods.