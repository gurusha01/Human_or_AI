SUMMARY 
This article explores a model of associative memory where patterns are embedded in a low-dimensional manifold, with the structure of these patterns captured by "constraint-neurons" that, alongside pattern neurons, form a memory network. The authors propose a recall algorithm for this network, demonstrating that intrinsic noise enhances recall performance. 
The paper is well-written and appears to be technically sound from my perspective.
MAJOR COMMENTS 
1. Biological Plausibility: While the paper begins by discussing associative memory and the brain, the proposed model seems somewhat detached from biological reality. 
- For instance, the implementation of step 4 in Algorithm 2 in a biologically realistic network is unclear, as it presupposes the retention of memory over the tmax iterations of Algorithm 1 (cf. Step 3 in Algorithm 3).
- The interpretation of the integer-valued state of the neuron is also ambiguous; are these states indicative of spike versus non-spike activity, and if so, how does this binary code (S = 2) differ from the coding used in the constraint neurons?
2. The architecture bears resemblance to Restricted Boltzmann Machines (RBMs), with pattern neurons analogous to visible neurons and constraint neurons akin to hidden neurons. Given that RBMs also incorporate intrinsic noise, which is adapted through synaptic strength to ensure the marginal distribution over visible units aligns with the data, it is anticipated that deterministic neurons would yield poorer performance. Thus, the novelty of intrinsic noise being beneficial is somewhat diminished. Furthermore, the authors' assertion that Deep Belief Nets (and by extension, RBMs) are exclusively used for classification and not pattern completion is inaccurate, as RBMs can indeed be utilized for pattern completion.
3. Alternative approaches to associative memories, such as the one proposed by Lengyel et al. in [R1], conceptualize recall as a Bayesian inference process. This process leverages knowledge of the stored patterns' structure (prior), the learning rule, and the input corruption process (likelihood), yielding a posterior distribution that is a deterministic function of the prior and likelihood, thereby not necessitating additional noise unless sampling from the posterior distribution is intended. This raises the question of how the "optimal" recall rule in a Bayesian context does not require noise, whereas the presented recall rule does.
[R1] Lengyel, M., Kwag, J., Paulsen, O., & Dayan, P. (2005). Matching storage and recall: hippocampal spike timing–dependent plasticity and phase response curves. Nature Neuroscience, 8(12), 1677–1683.
The paper is well-written, and to the best of my judgment, it is technically sound.