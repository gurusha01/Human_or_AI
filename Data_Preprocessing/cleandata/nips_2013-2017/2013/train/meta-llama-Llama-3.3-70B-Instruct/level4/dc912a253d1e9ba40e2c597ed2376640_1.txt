This paper presents a novel parallel dual coordinate descent approach for addressing regularized risk minimization problems. The proposed methodology involves each machine or core updating a subset of dual variables based on the current parameter, followed by a "delayed" update of the shared parameter. 
Although similar concepts have been applied in other contexts, such as Yahoo-LDA and matrix factorization (Hogwild), this work appears to be the first to adapt this idea to dual coordinate descent. The authors provide a solid theoretical foundation in Theorem 1. Experimental results demonstrate the superiority of this method over other parallel algorithms in linear SVM problems. However, I note that Figure 3 is difficult to interpret due to its small size, and I recommend utilizing a log-scale for the primal objective and duality gap to enhance readability. Overall, this is a well-crafted paper, and I support its acceptance.