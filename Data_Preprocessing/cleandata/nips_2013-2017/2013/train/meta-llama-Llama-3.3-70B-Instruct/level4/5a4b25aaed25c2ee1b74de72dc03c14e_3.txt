The effectiveness of Gibbs sampling in Markov Random Fields (MRFs), specifically in the context of the Ising model with two labels, is heavily dependent on the mixing rate of the underlying Monte Carlo Markov chain. The proposed approach to inference involves two key steps:
1. Projecting the input model, which may not exhibit fast mixing, onto the set of models that are known to mix rapidly.
2. Performing inference on the resulting fast-mixing model.
The domain of fast-mixing models is characterized by a bound on the spectral norm of the matrix comprising the absolute values of Ising edge strengths. "Projection" is formally defined in terms of divergences of the Gibbs distribution, with the constraint of preserving the graph structure. For projection in Euclidean distance, this is achieved by dualizing the initial task and employing the LBFGS-B method. In contrast, for other divergences, including KL, piecewise KL, and reversed KL divergences, a projected gradient algorithm is utilized. Notably, the computation of the projection under reversed KL divergence necessitates Gibbs sampling, albeit on a fast-mixing model.
The paper presents an extensive experimental evaluation on small random models, comparing the approximated marginals with the true marginals across several methods: the proposed approach (with various divergences), loopy BP, TRW, MF, and Gibbs sampling on the original model. The assessment not only considers accuracy but also conducts a runtime-vs-accuracy evaluation. The results demonstrate that the proposed methods consistently outperform TRW, MF, and LBP in terms of accuracy. Furthermore, for a reasonable range of runtimes, they also surpass Gibbs sampling on the original model. Among the proposed methods, the one utilizing reversed KL divergence exhibits the most consistent superior performance.
Several aspects of the paper warrant further clarification or improvement:
- The projected gradient algorithm outlined in section 5.1 involves two nested loops, with the inner loop being LBFGS-B. It would be beneficial to provide details on the termination criteria for the inner iterations.
- The plots in Figure 2 (and the supplement) have a horizontal axis labeled as "number of samples," which seems misleading since sampling is exclusively used for reversed KL divergence. A more appropriate labeling could be "runtime of the algorithm." Additionally, reporting the runtime of LBP, TRW, and MF would facilitate a fair accuracy-runtime comparison among all tested algorithms. Providing absolute running times in seconds would enhance the interpretability of the experimental results.
- Consideration should be given to experimental comparisons involving larger models. An intriguing option could be utilizing models from the paper by Boris Flach, which allow for polynomial inference.
- Minor corrections include replacing "onto the tree" with "onto graph T" on page 222, and potentially using "subgradient" instead of "derivative" on page 226.
Overall, the paper presents interesting and convincing empirical results. However, its practical utility might be limited due to high runtime, an aspect that requires further clarification in the rebuttal.