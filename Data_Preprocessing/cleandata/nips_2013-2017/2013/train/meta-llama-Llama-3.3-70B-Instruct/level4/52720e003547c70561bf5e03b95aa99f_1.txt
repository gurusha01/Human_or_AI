The authors address the issue of sparse precision matrix estimation using CLIME and introduce a scalable variant, CLIME-ADMM, accompanied by a distributed computational framework. The empirical evaluation compares CLIME-ADMM to state-of-the-art methods, including DC-QUIC, Tiger, and Flare.
The notation is dispersed throughout Sections 1, 2, and 3, which may be improved by consolidating it into a single table for enhanced readability. For instance, the definitions of \rho and \eta are not provided in the text.
The proposed block cyclic data distribution is intriguing, but it is unclear why this approach would yield load balance and scalability. Furthermore, there is a lack of empirical evidence to support this claim.
The algorithm's current setting of the column block size appears to be ad hoc, and it would be beneficial to have a theoretical foundation or intuitive guidance for selecting k.
A broader concern is that the algorithm may not be truly distributed, as it does not explicitly discuss mechanisms for message passing and communication. It might be more accurate to consider this a parallel implementation.
Minor suggestions include:
Section 1: Correcting "tunning" to "tuning" in the phrase "where \lambda is a tunning parameter".
Section 5.1: Replacing "parameter tunning free" with "tuning" in the description of Tiger.
Section 2: Changing "CLIME is summerized in" to "CLIME is summarized in".
The paper presents a large-scale parallel algorithm for estimating sparse precision matrices using CLIME, with the novelty of estimating the precision matrix by column blocks rather than column-by-column. An empirical analysis using OpenMPI and Scalapak is also provided.