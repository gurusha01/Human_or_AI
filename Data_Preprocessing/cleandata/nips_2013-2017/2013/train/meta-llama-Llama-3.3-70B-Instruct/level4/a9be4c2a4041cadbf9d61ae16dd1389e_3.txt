This paper presents a convex relaxation of a two-layer neural network, which is clearly and effectively communicated. The experimental results demonstrate promising performance on real-world datasets. However, a major concern is the approach's scalability. 
The utilization of Semidefinite Programming (SDP) for convex relaxation is not a new concept, having been extensively employed over a decade ago. Although the formulation is elegant, the scalability issue inherent in this type of relaxation is a significant drawback. Specifically, the optimization problem scales with the square of the instance size (t^2), rendering it feasible only for small-scale datasets. To better assess scalability, a comparison of training times among different algorithms would be beneficial.
Algorithm 2 leverages the low rank of matrix N, but it does not guarantee a small rank, which may limit its applicability. In the synthetic experiments, a more comprehensive comparison could be made by using the Nystrom approximation to RBF SVM with randomly selected bases, rather than a one-layer linear SVM, to provide a more accurate assessment. Overall, while the paper is well-written, the SDP convex relaxation approach lacks novelty, and its scalability and comparability to other methods require further investigation.