This manuscript provides a rigorous mathematical examination of the dropout procedure in deep neural networks, marking a pioneering effort to theoretically validate this hitherto heuristic method. Previous studies, albeit limited to shallow networks, have hinted at dropout's role in facilitating a form of averaging, specifically geometric. The present work distinguishes itself by not only proving this property for deep neural networks but also demonstrating that a normalized, weighted geometric version yields superior approximations compared to the conventional geometric average. Notably, equations 11, 12, and 13 constitute significant contributions, poised to substantially influence future research on deep neural networks, particularly in theoretical analyses.
Minor comments 
----------------------- 
In Equation 7, the factor c should precede the exponential term in the penultimate term, and a closing bracket is missing at the expression's end.
Consider extracting the parenthesis in Equation 25 following the differential.
It is possible that lambdai in Equation 26 should actually be deltai.
The assertion that p_i = 0.5 yields the highest level of regularization was not entirely clear. The authors could provide further clarification on this point, as it underpins the current heuristic justification for dropout and is a crucial observation.
Figure 3 on page 8 appears too small; rearranging the figures on this page and eliminating the space between them could improve presentation. 
The original introduction is reiterated for emphasis: This paper presents a mathematical analysis of the dropout procedure in deep neural networks. As far as I know, this is the first attempt to prove the somewhat heuristically used dropout procedure. There have been some suggestions in prior work (at least for the shallow case) that dropout performs some form of an averaging (geometric to be precise). But this is the first attempt to prove this property for deep neural networks and show that the normalized version of the weighted geometric version provides better approximations than the traditional geometric average. In particular, the three equations 11, 12, 13 are important contributions of this paper and will have a greater impact on future work on deep neural networks, especially in their theoretical analysis.