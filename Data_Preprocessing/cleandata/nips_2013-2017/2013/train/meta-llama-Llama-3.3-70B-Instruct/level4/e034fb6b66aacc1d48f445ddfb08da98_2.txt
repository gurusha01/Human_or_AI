This manuscript introduces a novel approach to leveraging human feedback for enhancing the performance of a reinforcement-learning agent. The key innovation lies in translating human feedback into an estimate of action optimality, which may be inconsistent, rather than relying on traditional reward signals. The proposed algorithm demonstrates superior performance compared to existing state-of-the-art methods in two toy domains. Overall, I found this to be an excellent paper that effectively motivates the problem, clearly presents a new concept, and thoroughly compares its performance to other cutting-edge algorithms. My feedback primarily consists of suggestions for further improvement.
One aspect that I particularly appreciated was the utilization of a simulated human teacher, which allows for systematic manipulation of feedback likelihood and consistency. However, I would have liked to see an exploration of even lower feedback likelihoods, below 1%, to further assess the algorithm's robustness.
A concern that arose is the potential for systematic inconsistency in human feedback, which is a common phenomenon in psychology, particularly in reward shaping for training new behaviors. For instance, when teaching a rat to pull a chain, the reward is initially given for approaching the chain, then for touching it, and finally for pulling it, with each previous behavior no longer being rewarded. It would be beneficial to investigate how the proposed algorithm, Advise, handles such systematic inconsistencies in human feedback, which may be encountered when working with expert human trainers.
In Section 4.2, the assumption that humans are aware of only one optimal action seems somewhat restrictive. It would be interesting to examine the algorithm's performance when this assumption is relaxed, allowing the human teacher to vacillate between shaping different optimal actions. A brief discussion on this topic would be valuable.
Another issue that arises when working with human feedback is delay, which can lead to inconsistency due to varying response rates. For example, positive feedback might only be provided after an intervening action. I believe that the Advise approach, which accommodates inconsistency, may be more robust to such delays, and this aspect could be further explored.
Some minor suggestions for improvement include:
* Line 053: The phrase "from MDP reward" could be rephrased for better clarity.
* Section 5.1: The winning condition in Pac-Man is not explicitly stated; it would be helpful to specify whether it involves eating both dots.
* Table 1 and figures: The second column could be more clearly labeled as "Reduced Frequency" instead of "Reduced Feedback." Additionally, the ordering of conditions differs between Table 1 and the subsequent figures.
* Lines 234-247: The relationship between control sharing and action biasing could be elaborated upon for better understanding.
* Lines 294: The prior estimate of the missing value could be clarified.
* Figure 2: It is unclear why only the cases where Advise does not provide improvement are plotted, aside from the ideal case.
* Lines 369-370: It would be more accurate to take the closest overestimate of C, erring on the side of caution.
* Figures 4 and 5: The text and axis labels in the figures are too small and could be enlarged for better readability.
Overall, this paper presents a novel approach to using human feedback for improving reinforcement-learning agents, with promising results and a clear comparison to state-of-the-art methods.