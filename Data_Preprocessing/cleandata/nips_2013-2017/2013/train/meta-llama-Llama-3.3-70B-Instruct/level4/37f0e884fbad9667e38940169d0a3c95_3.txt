This paper introduces a novel hybrid approach to stochastic gradient descent, combining both stochastic and batch gradients. The proposed algorithm guarantees convergence to an epsilon-accurate solution, utilizing O(log(1/eps)) full gradients and O(k^2 log(1/eps)) stochastic gradients.
The convergence proof appears to be correct and innovative, aside from a few minor errors that are detailed below.
However, my primary concern lies in the algorithm's relevance to the machine learning context, which is the conference's primary focus. Typically, in machine learning algorithms, strong convexity is ensured by the regularizer, resulting in the value of mu being of the order of the number of samples N, i.e., mu = C * N, where C is independent of N. Under this assumption, the proposed method outperforms batch gradient descent only when the number of samples is bounded by O(C^3/L^3), which does not seem to be a practically interesting regime. Furthermore, the convergence rate of the proposed algorithm holds only in high probability, whereas the convergence rate of batch gradient descent is deterministic.
This point is crucial and requires careful discussion to demonstrate the algorithm's genuine advantage over batch gradient descent and to establish the paper's relevance to the machine learning community.
Minor comments include:
- Equation (6) should be corrected to ||w^* - \hat{w}||^2.
- The function used in equation (7) should be specified on line 288.
- The equality in equation (286) should be removed, as it does not contribute to understanding and instead decreases clarity.
- In equation (13), absolute values should be replaced with norms.
- The fact that L â‰¥ lambda should be explicitly stated, even if it is obvious, for the sake of clarity.
- In equation (4), x^ should be replaced with w^, and f should be replaced with F. Additionally, the first term in the max is always greater than the second term, as per Lemma 1.
- The condition number should be precisely defined as a function of lambda and L.
Overall, the novelty of the hybrid stochastic/batch gradient descent approach is not clearly advantageous over standard batch gradient descent in practical machine learning optimization problems.