This manuscript introduces a novel policy iteration algorithm, termed MB-OPI, designed for symbolic Markov Decision Processes (MDPs) that incorporate factored-action and factored-state dynamics. The algorithm provides a mechanism to balance representational complexity between value and policy iteration for MDPs defined over algebraic decision diagrams (ADDs), akin to how MPI facilitates a trade-off in computational complexity. By doing so, the authors extend existing algorithms that previously considered factored actions and memory constraints in isolation.
A primary technical hurdle addressed in the paper is the significant increase in size of the value function that occurs when an explicit policy representation is multiplied into it, a necessary step in ADD policy iteration. The authors overcome this challenge by introducing a pruning procedure that conservatively combines policy and value diagrams, thereby controlling the increase in size without resorting to naive multiplication.
The results, presented in terms of solution time, demonstrate a notable improvement of approximately 2-6 times over existing approaches. The paper is technically sound and well-written, contributing theoretically to the literature on symbolic MDP planning by introducing the concept of pruning as an alternative to ADD products. It also generalizes existing work in symbolic dynamic programming and appears to represent the state of the art for planning with factored actions.
Empirical results support the notion that pruning offers an MPI approach to symbolic dynamic programming (SDP) planning that mitigates representational bloat and yields a significant speedup. The manuscript is generally well-structured and easy to follow.
To further enhance the paper, it would be beneficial to include additional background information on SDP solving using ADDs for representing value and Dynamic Bayesian Networks (DBNs), the basic policy iteration approach utilizing ADD product, and the distinction between multiplying a policy into a value function versus pruning a value function with a policy.
Moreover, discussions on practical problems involving numerous parallel actions, for which factoring actions is crucial, and the inclusion of a toy test case highlighting the best-case improvement over SPUDD and FAR would be valuable additions.
Several minor suggestions for clarity and correctness are also noted, including definitions, figure readability, and equation explanations, to improve the overall quality and accessibility of the manuscript.
In conclusion, the paper presents a well-defined advancement in decision-diagram-based planning for symbolic MDPs, with empirical and theoretical results indicating that the proposed algorithm, MB-OPI, is currently the state of the art for planning with factored actions.