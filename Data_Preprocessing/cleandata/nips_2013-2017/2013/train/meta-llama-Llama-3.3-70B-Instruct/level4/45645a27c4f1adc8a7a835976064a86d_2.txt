Summary: 
The authors introduce a novel algorithmic approach for efficient model selection in latent feature models, leveraging factorized asymptotic Bayesian (FAB) inference and the factorized information criterion (FIC). Notably, FAB and FIC have been previously established for mixture models, where they facilitate tractable model selection through factorization and Laplace approximation of a model's log likelihood. The key contribution of this work lies in extending FAB to latent feature models, incorporating a mean field approximation for latent inference and an accelerated shrinkage mechanism to reduce the global number of selected features.
A distinctive aspect of this method is its enhanced automaticity in model selection, minimizing the need for manual tweaking. The authors demonstrate the efficacy of their approach through experiments on both synthetic and real datasets, highlighting a substantial improvement in computational efficiency.
Quality: 
The presented results are comprehensive, involving a thorough evaluation of the method against a broad spectrum of competing approaches across multiple datasets. The references provided are adequate and supportive of the research.
Clarity: 
The paper is well-organized and clearly written, facilitating easy comprehension of the complex concepts and methodologies presented.
Originality/Significance: 
This work represents a logical and significant extension of prior research in model selection for latent feature models (LFMs) using FAB inference and FIC. The innovation stems from combining multiple approximations into a single, effective method. However, it would be beneficial to explore the limitations of these approximations, specifically under what conditions they might lead to suboptimal performance. Given the interplay of several approximations (including FIC, mean field, and shrinkage), understanding the potential breakdown points of the method would provide valuable insights into its robustness and applicability.