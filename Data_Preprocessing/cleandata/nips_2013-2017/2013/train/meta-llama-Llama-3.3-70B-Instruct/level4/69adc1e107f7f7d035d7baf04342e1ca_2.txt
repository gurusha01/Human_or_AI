Summary of the paper: 
This manuscript re-examines the concept of decision DAGs for classification purposes, offering an alternative to traditional decision trees by allowing node merging at each layer, thus mitigating exponential growth with depth. This approach contrasts with decision trees that rely on pruning to control size and prevent overfitting. The authors frame the learning process as an empirical risk minimization problem, aiming to concurrently learn the DAG structure and node split parameters. Two algorithms are proposed for greedy, layer-wise learning of structure and parameters, utilizing an information-gain objective. Experimental comparisons with baseline methods, including ensembles of fixed-size decision trees, indicate that ensembles of decision DAGs achieve enhanced generalization performance for a given model size, measured by total node count in the ensemble.
Quality: 
The overall quality of this manuscript is commendable. The literature review is comprehensive, the model and algorithms are intuitively sound, and the experimental design is compelling, effectively exploring the impact of varying design parameters. However, a notable omission is the comparison of training and evaluation times across different approaches, a critical consideration that warrants attention. 
Clarity: 
The manuscript is generally well-written and clear. The model presentation is lucid, and the experiments are easily understandable. A minor point of confusion arises regarding whether the ensemble training incorporates bagging alongside other randomized learning elements. Additionally, the introduction of "energy" in section 3.1 seems abrupt; clarification on its relation to empirical risk would be beneficial. A minor grammatical error is also present at the conclusion of the LSearch description.
Originality: 
While the underlying concepts (decision DAGs, ensembles, empirical risk minimization, and information gain) are established, the novelty of this work lies in their cohesive integration and the development of intuitive learning algorithms. 
Significance: 
Given the widespread adoption of random forests in machine learning practice, any method offering improvements without substantial computational or implementation overhead could have significant implications. The authors' proposal of ensembles of randomized decision DAGs for improved classification under memory constraints, framed within empirical risk minimization, demonstrates potential through thorough experiments across several datasets.