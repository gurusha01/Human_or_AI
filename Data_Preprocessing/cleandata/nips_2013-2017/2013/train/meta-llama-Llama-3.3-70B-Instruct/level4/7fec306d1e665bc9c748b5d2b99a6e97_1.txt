Review - Summary:
The paper "Predicting Parameters in Deep Learning" investigates the notion that the conventional representation of neural network parameters contains significant redundancy. 
Pros:
* The observation and interpretation presented are intriguing
* The experiments demonstrate promising evidence
Cons:
* The empirical results fail to substantiate the central claims
* The paper lacks comparisons with common techniques such as PCA preprocessing for weight matrix factoring
Quality:
The authors note the difficulty of training a parameter matrix W factored as a product UV at several points in the paper. It would be beneficial if they could provide insight into why this is the case. One possible reason is that if U and V are initialized to 0, naive backpropagation will not update them. On the other hand, if W is trained normally but constrained to be a product of U and V, some computational overhead can be traded for reduced communication costs during updates.
A more fundamental concern is that the paper's central observation of weight redundancy, combined with the use of a spatial exponential kernel, suggests that similar benefits could be achieved through PCA of the input or downsampling images and kernels for convolution, which are common practices. PCA preprocessing also represents a low-rank factorization of the first layer's weights.
Maintaining a PCA of intermediate layers is not a common practice but could be useful and serves as a conceptual starting point for the authors' work on "data-driven expander matrices."
The discussion of columnar architectures and convolutional nets seems reasonable but largely hypothetical. Refactoring matrices in various architectures without extensive empirical support appears to deviate from the paper's core thesis.
The empirical aspect of this paper requires more work. The authors' claim that "there exist models whose parameters can be predicted" is weak. The intended claim, as implied by the introduction, should be that "there exist large models that work better than naively downsized versions, and using our techniques, we do less damage to the large models than simply downsizing them." However, this central claim is not supported by empirical results.
The claim that parallel training can be accelerated using these techniques also needs more justification. The proposed method introduces new computational overhead, and specific encoder/decoder algorithms for transmitting and rebuilding weight matrices need to be implemented and tested to complete this argument.
Clarity:
The term "collimation" is misused; although it is a word, it does not relate to making columns. 
The phrase "the the" is an error.
The term "redundancy" might not accurately describe the symmetry/equivalence set around line 115.
Section 2, "Low-rank weight matrices," is a brief background section and could be merged into the introduction.
Figure 1 is only referenced in the introduction but actually illustrates the method in Section 3.2. It would be clearer to move Figure 1 to Section 3.2.
Equations should be numbered for the sake of reviewers.
Originality:
The concept of treating weight matrices in neural networks as continuous functions has precedent, as does the observation of weight redundancy. However, the idea that training can be accelerated by communicating a small fraction of randomly chosen matrix values is novel.
Significance:
As the authors note in their introduction, this work could be highly interesting to researchers attempting to parallelize neural network training across very low-bandwidth channels.
Edit: After reviewing the authors' rebuttal, I have increased my quality score. I apparently misunderstood the discussion of columns and its relation to the experimental work. I hope the authors' defense of their experimental work is included in future revisions. The paper presents some good ideas and stimulates thought, but the empirical results do not support the most important and interesting claims. The algorithm for accelerating parallel training is only outlined.