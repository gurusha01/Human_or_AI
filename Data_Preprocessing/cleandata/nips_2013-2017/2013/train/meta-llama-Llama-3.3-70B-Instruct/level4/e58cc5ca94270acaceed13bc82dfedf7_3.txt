The paper presents novel theories leveraging side information to decrease sample complexity and demonstrates their efficacy in transductive multi-label learning. 
In the proof, the assumption that matrices A and B are orthonormal (as stated in the top paragraph of page 4) could be clarified further, particularly in regards to how the results would be affected if A and B were not orthogonal. Additionally, simplification of the proof may be achievable given that U and V are linear combinations of column vectors in A and B.
The unconstrained optimization problem (3) bears a strong resemblance to the problem addressed in [1]. By indexing the set of observations Ω from 1 to k, such that Ωk = (ik, jk), and defining Ck = A{ik, .} B{jk, .}^T, problem (3) can be re-expressed as minZ ∑k \|Mk - tr(Z Ck)\|^2 + λ |Z|_{tr}, which is identical to the problem solved in [1]. Notably, [1] also provides consistency results and an efficient algorithm, making a comparison to the algorithm for multi-label learning experiments worthwhile.
The statement at the end of the second paragraph suggests that recent efforts to address the issue come at the cost of losing performance guarantees. However, this is not entirely accurate, as paper [5] cited in the original paper does offer convergence guarantees. Furthermore, several papers, including [2], provide efficient algorithms that do not require updating the full matrix and also have convergence guarantees.
A typo is present in the proof of Lemma 1 in the supplementary material, where line 063 should read (1 - ||P_{T^⊥}||), with the ⊥ symbol missing.
References [1] Francis R. Bach, Consistency of Trace Norm Minimization, JMLR 2008, and [2] M. Jaggi, M. Sulovsky, A Simple Algorithm for Nuclear Norm Regularized Problems, ICML 2010, are relevant to the discussion. The authors provide new theoretical and empirical results on sample complexity for matrix completion, which should be compared to the analysis by Bach (2008) that addressed a similar problem.