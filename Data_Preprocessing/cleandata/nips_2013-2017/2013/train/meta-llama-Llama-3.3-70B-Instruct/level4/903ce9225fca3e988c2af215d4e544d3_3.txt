Summary of paper:
This paper explores the problem of learning half-spaces over 3-sparse input in {+1,-1,0}^n, demonstrating a significant gap between statistical and computational complexity in learning. Although the information-theoretic sample complexity for this problem is of order n/eps^2, it is shown that efficient learning using only order n/eps^2 samples is impossible, assuming the hardness of refuting random 3CNF formulas. In fact, under a stronger version of this hardness assumption, it is proven that efficient learning using order n^{1+mu}/eps^2 samples is not possible for appropriate mu in [0,0.5), even for improper learning.
In contrast, the paper presents an efficient learning algorithm for these half-spaces using samples of size n^2/eps^2, highlighting a substantial disparity between the sample complexity of learning from an information-theoretic perspective and that of efficient learning. Notably, this gap disappears when learning 2-sparse vectors.
Comments:
A potential direction for future research is to investigate whether the positive result of efficiently learning H{n,3} in n^2/eps^2 can be extended to learning H{n,k}. The current result relies heavily on the use of Hazan et al.'s algorithm for learning H{n,2} and lemma A.4. It would be interesting to explore the possibility of extending lemma A.4 to accommodate H{n,k} and to determine the dependence on k in such a scenario.
Overall, the paper is well-structured and engaging, with a relatively straightforward yet insightful reduction and proof. The provision of hardness results for improper learning is particularly noteworthy, as this has traditionally been a challenging task that cannot be accomplished using standard NP hardness assumptions. The simplicity of the hardness assumption and reduction in this paper makes it likely that the result could be applied to other learning scenarios. Therefore, I firmly believe that this work merits publication.