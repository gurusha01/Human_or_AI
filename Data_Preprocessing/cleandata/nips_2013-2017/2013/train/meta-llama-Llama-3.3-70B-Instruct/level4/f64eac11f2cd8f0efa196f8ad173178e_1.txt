This manuscript presents a theoretically sound approach to adaptive step size selection for policy gradient methods, wherein the lower bound of expected policy improvement is maximized. Notably, an approximated version of the step size can be computed using only the estimated gradient, resulting in computational complexity comparable to that of standard policy gradient methods. The derivation of the lower bound relies on a mild assumption of a fixed standard deviation for the Gaussian policy. Experimental results demonstrate the efficacy of the proposed method, particularly when the fixed standard deviation is sufficiently large.
In terms of quality, clarity, and originality, the paper's motivation is well-articulated, and its organization is logical. The manuscript offers valuable theoretical insights, including Theorem 4.3 and Corollary 4.4, although a detailed examination of the lemma and theorem derivations is not feasible. The originality of the work is unquestionably high.
Regarding significance, while the Newton method is a conventional approach to adaptive step-size selection, the semi-positive definiteness of the second-order derivative of the expected return with a Gaussian policy is not guaranteed. Therefore, an adaptive step-size method based on maximizing the lower bound presents a viable alternative to traditional policy gradient methods. However, as the authors acknowledge in the introduction, several policy gradient extensions, such as EM policy search, have been proposed for stable policy updates. Consequently, more extensive experimental comparisons with these methods would substantially enhance the convincingness of the results.
Minor comments and inquiries include:
* The addition of a conclusion section would be beneficial.
* It appears unlikely that convergence to the optimal policy could be achieved with only one update for Ïƒ=5 (as reported in Table 1). The manuscript provides valuable theoretical analyses and a promising alternative to policy gradient methods. Nevertheless, more comprehensive experimental comparisons with state-of-the-art methods, such as EM policy search, would be necessary to further substantiate the findings.