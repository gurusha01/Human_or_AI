The authors build upon the foundation established by [Schwing et al., Efficient Structured Prediction with Latent Variables for General Graphical Models; ICML 2012] by integrating active learning protocols, which utilize the entropy of local variable marginals to quantify uncertainty. This approach can be viewed as a form of local variable uncertainty sampling for structured predictions involving partially observed variables. To achieve this, the authors propose two active learning variants: (1) separate active learning, where each round entails a separate inference step over unlabeled and partially labeled variables following learning on observed variables, and (2) joint active learning, which involves a joint learning procedure over labeled, partially labeled, and unlabeled instances. Additionally, they explore the effects of batch sizes and the warm-starting of the learning procedure between rounds. The empirical evaluation, conducted on the 3D layout of rooms (a computer vision problem previously explored in [Schwing et al., ICML 2012]), yields several key findings: (1) joint active learning outperforms other methods, achieving an annotation savings of approximately 90%, (2) batch mode is effective for reasonably small batches, (3) querying partial labels is more efficient than querying full labels, (4) the method exhibits sensitivity to epsilon, and (5) computational reuse results in significant time savings.
Overall, the paper presents a compelling demonstration of active learning in a challenging setting with a specific formulation. The quality, clarity, originality, and significance of the paper will be addressed separately.
Quality: The paper is of high quality, leveraging a state-of-the-art model for structured learning with latent variables and achieving substantial savings through active learning compared to random sampling. However, from an active learning perspective, the reported savings might be somewhat misleading, as they are based on simulations that assume a direct one-to-one correspondence between labels and costs, an assumption that has been criticized in the literature (e.g., [Kapoor et al., Selective supervision: Guiding supervised learning with decision-theoretic active learning; IJCAI 2007], [Settles et al., Active learning with real annotation costs; NIPS WS 2008], among others). These studies suggest that active learning often selects the most difficult examples for labeling, which can lead to higher annotation costs. The paper could benefit from a discussion on this aspect, although the experiments comparing full vs. partial labels provide some insight. Another concern relates to the semantics of partial labels during querying time, as discussed in [Culotta and McCallum, AAAI 2005] and [Roth and Small, ECML 2006], which seems to contrast with the statement in the last paragraph of page 1. Clarification that the output space is fully observable would resolve this discrepancy.
Clarity: The paper is generally clear, although some details required reference to [Schwing et al., ICML 2012] for full understanding. Areas for improvement include: (1) providing examples or discussion on the task-loss function, as its explanation relies heavily on the previous work, (2) addressing potential notation confusion between s and y, which appears to stem from cutting and pasting from the ICML paper, and (3) explaining the notation $\ell^c$, which might be challenging for readers without explicit graphical models expertise.
Originality: While the extension of active learning to this setting is straightforward and based on uncertainty sampling, it represents a novel application that has not been previously explored. However, the combination of these elements could have been conceived by anyone familiar with the areas involved.
Significance: This paper is likely to have a moderate impact, serving as a companion piece to [Schwing et al., ICML 2012]. The ideas, although relatively straightforward, are well-executed. The absence of theoretical results and the non-trivial setting of epsilon in deployed settings suggest that the code might be more useful than the paper itself for practical applications.
Several minor comments are also noteworthy: 
- The abstract should be revised to remove unusual spacing, changing "$\sim 10\%$" to "$\sim10\%$".
- Page 1 could be clarified by changing "have to deal with exponentially sized output variables" to "have to deal with output spaces which are exponentially sized in the number of local variables".
- Page 2's mention of "fully labeled training sets" might benefit from distinguishing between "labeled" and "observed", although this is at the authors' discretion.
- Page 5 contains a minor typo, where "as well as the messages $\lambda$ and believes" should be corrected to "...beliefs".
The authors successfully extend the work of [Schwing et al., ICML 2012] to include active learning protocols via entropy of the local variable marginals, demonstrating empirical success on a challenging task and potential for generalization to similar problems. While there are minor issues to address, the paper presents a useful, albeit straightforward, extension of previous work.