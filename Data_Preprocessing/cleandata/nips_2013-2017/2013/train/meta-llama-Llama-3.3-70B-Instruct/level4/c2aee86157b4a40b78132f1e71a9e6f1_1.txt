This paper introduces a novel online approach to solving Partially Observable Markov Decision Processes (POMDPs), building upon existing state-of-the-art methods such as POMCP and AEMS2. The key innovation lies in pruning the forward search tree using regularization of the policy size, accompanied by a performance bound on the value estimate that guides the search. Empirical results are presented, comparing the new method to other recent online and offline POMDP approaches, including the introduction of the new LaserTag domain, which is a valuable contribution likely to be reused by other researchers.
The central idea of controlling online search complexity through policy size constraints is intriguing and has not been previously explored in online POMDP methods. It offers valuable insights into balancing computational effort and solution quality in POMDPs, potentially having a significant impact on solving large-scale POMDPs. The authors' intention to share the code is a definite plus.
However, predicting the impact of this work is challenging. The empirical results do not clearly delineate which aspects of the proposed R-DESPOT framework contribute the most to its performance in practice. Specifically, it is unclear whether the particle filtering, DESPOT structure, or regularization itself is the primary factor. A more detailed analysis is necessary to understand the contributions of each component.
Several concerns regarding the experiments are raised. Re-computing AEMS2 results using a contemporary DESPOT implementation could provide more accurate comparisons, especially since the current AEMS2 results are five years old and appear competitive with DESPOT/R-DESPOT on smaller domains. For larger domains, experimenting with AEMS2 using particle filtering belief representation could help determine whether belief filtering or value approximation is more critical. Additionally, speculating on why the POMCP results are poorer than those in [3] would be beneficial. A more in-depth discussion in Section 6 would significantly enhance the paper by clarifying the impact of the work.
The paper is well-written, particularly in the introduction, related work, and the basic DESPOT framework sections. However, the latter parts of the paper contain technical details that require clarification. Specifically, it would be beneficial to:
- Clearly outline the differences between DESPOT and R-DESPOT, including whether the distinction lies solely in the PRUNE function (line 301), and provide pseudo-code for this function.
- Summarize the key differences between POMCP and DESPOT, potentially highlighting the use of lower/upper bounds in DESPOT.
- Discuss the looseness of the lower bound and whether the actual bound in expression (1) or a hand-tuned parameter L is used.
- Provide more details on the experimental selection of key parameters alpha and K (or L), including whether the same parameters were used across all experiments and the amount of offline time dedicated to their selection.
- Explain how the time constraint (1 sec / action) is imposed in practice and whether parameters exceeding the time limit are ignored.
This paper has the potential to be very strong, offering a potentially significant contribution to POMDP solving. However, it lacks technical detail, which may hinder the reproducibility of the results, and there are notable limitations in the discussion and results that make assessing the work's potential impact challenging. A more detailed response from the authors addressing these concerns would be beneficial.