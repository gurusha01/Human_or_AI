This manuscript presents a solution to large-scale submodular maximization problems, focusing on modifications to the traditional greedy algorithm under cardinality constraints to accommodate massive data sets.
The problem statement is well-motivated, given the existing body of work aimed at accelerating or parallelizing the inherently sequential greedy algorithm for submodular maximization. The use of the MapReduce programming paradigm to express the algorithm is also justified.
The primary technical contribution lies in the analysis of a two-round algorithm, where the input is distributed across machines, each approximating a solution to its portion of the input using a sequential greedy algorithm, and then combining these solutions to obtain the final result. A key aspect is that each machine outputs a solution larger than k/m, where k is the desired solution size and m is the number of machines. The analysis, while straightforward, reveals an inherent dependency on both k and m. Additionally, the paper provides supplementary results for special cases, including smooth spaces and decomposable functions.
The experimental results demonstrate the efficacy of the two-round algorithm compared to the standard greedy approach.
On the positive side, the paper tackles a significant problem and proposes a practical modification to the standard algorithm.
However, several negatives are noted:
1. The theoretical aspects of the paper are somewhat trivial, with obvious analysis and repetitive proofs that rely on strong assumptions. For instance, Theorem 4.2 does not leverage randomized input partitioning or provide a lower bound.
2. The paper lacks a round-memory-approximation tradeoff, presenting restrictive results that are unclear in their potential for generalization to multiple rounds. This limitation makes it significantly weaker than preceding work, such as the WWW paper by Chierichetti et al. or the SPAA paper by Lattanzi et al.
3. The experimental results include unnecessary baselines and fail to highlight the importance of oversampling by modifying the greedy/merge approach to make greedy a function of local solution sizes.
Additional suggestions include:
1. Investigating whether stronger bounds can be proven for Theorem 4.2 under randomized input partitioning.
2. Examining the SPAA 2013 paper by Kumar, Moseley, Vassilvitskii, and Vattani, which addresses a similar problem in a more general context and provides a multi-round algorithm with better approximation.
3. Potentially merging the results of Theorems 4.3 and 4.4, as they appear to rely on related assumptions.
4. Comparing the proposed algorithm with those presented in the Chierichetti et al. paper and the SPAA 2013 paper.
5. Clarifying the meaning of "suitable choice" on page 4, line 167.
6. Elucidating the comment regarding "... unless P = NP" on page 5, line 227, as the reasoning behind this statement is unclear.
Overall, while the paper addresses an important problem, its theoretical contributions are limited, making it a weak paper from a theoretical standpoint.