The paper proposes a novel "hybrid" approach that combines deterministic and stochastic methods for first-order optimization, allowing the algorithm to query both full gradients and stochastic gradients. This methodology enables the authors to demonstrate that the condition number dependency can be eliminated from the rate of convergence.
The authors present a theoretically interesting and significant result, which states that, assuming knowledge of the condition number (i.e., both strong convexity and strong smoothness constants of the objective), and given access to a first-order hybrid oracle, a suitably chosen mix of deterministic and stochastic gradient steps can achieve a high-probability convergence rate of O(log(1/epsilon)) for batch optimization.
However, several concerns arise with the current state of the paper. Firstly, the theoretical analysis is limited to the scenario where the condition number is perfectly known beforehand, without exploring the algorithm's behavior when this hyper-parameter is misspecified. The current result implies that the dependence on the condition number can be removed from the convergence rate when the condition number is known in advance. If analyzing the cases where the condition number is unknown is challenging, experimental evaluations could provide valuable insights. Unfortunately, the paper lacks an experiments section, which is a significant concern given that the main contribution is a new algorithm.
Detailed examination of the proposed algorithm, EGD, reveals that it relies on an update rule defined by Eq. 8, utilizing the "mixed gradient" from Eq. 7, which requires setting the hyper-parameter η. This, in turn, necessitates knowledge of the condition number κ, comprising both the strong convexity modulus λ and the strong smoothness modulus L. The authors do not provide guidelines or theoretical arguments for setting κ beforehand. Therefore, either this parameter needs to be estimated, which would require a corresponding estimation procedure supported by numerical experiments, or it is assumed known, in which case the theoretical analysis or experimental section should address the cases where this hyper-parameter is misspecified.
Furthermore, the smooth and strongly convex case, although popular in theoretical analysis and realistic in many situations, can be too restrictive. Other settings, such as non-strongly convex cases, are also noteworthy, as they arise in various situations. The algorithm's presentation as a "mixed" update per iteration, rather than an alternation of deterministic and stochastic gradient steps, raises questions about its relation to similar algorithms that interleave these steps. A discussion on how the proposed algorithm compares to such methods would be beneficial.
Additionally, the authors do not review related work on hybrid deterministic-stochastic optimization algorithms, such as the work by Friedlander and Schmidt (2012). A comparison of convergence rates would be valuable in this context. A thorough experimental study, including a detailed comparison with regular SGD, averaged SGD, and recent proposals for stochastic first-order optimization, would significantly enhance the paper.
A minor concern is that the optimization setting considered in the paper is not clearly stated. The purpose of the paper is to leverage the benefits of both deterministic and stochastic optimization, but it does not specify whether it aims to solve the deterministic optimization problem with a stochastic algorithm or the stochastic approximation problem. Clarifying this would help situate the theoretical setup and claims within the appropriate context. 
Overall, while the paper presents an interesting theoretical result, addressing the aforementioned concerns, particularly through experimental evaluations and a clearer exposition of the optimization setting and relation to existing work, would strengthen the manuscript.