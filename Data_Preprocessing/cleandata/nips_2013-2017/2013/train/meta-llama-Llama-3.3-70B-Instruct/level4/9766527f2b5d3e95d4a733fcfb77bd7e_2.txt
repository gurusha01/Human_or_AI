Review- quality: 5 (out of 10) 
clarity: 6 
originality: 6 
significance: 9 
SUMMARY: This paper proposes an acceleration of the stochastic gradient optimization algorithm by utilizing the 'control variate' technique to reduce the variance of noisy gradient estimates. The control variate, a vector with high correlation to the noisy gradient but easier to compute, is used to achieve this reduction. The authors demonstrate the effectiveness of this approach through examples in regularized logistic regression and latent Dirichlet allocation (LDA), showcasing faster empirical convergence. 
EVALUATION: 
Pros: 
- The concept of variance reduction for stochastic gradient descent (SGD) using control variates is innovative and has the potential for significant impact.
- The motivation behind the approach is convincing, with concrete examples and a generalizable idea based on Taylor approximations.
- The paper is relatively easy to follow.
Cons: 
- The experimental section is somewhat lacking, with only one dataset used for logistic regression and an inconsistent setup for LDA experiments.
- The related work section is incomplete, omitting key references such as "Variational Bayesian Inference with Stochastic Search" by John Paisley et al.
QUALITY: While the theoretical foundation for the approach is sound, the empirical execution falls short. Notably, the logistic regression experiments are limited to a single dataset, and the LDA experiments lack a standardized setup, making comparisons challenging. The choice of different mini-batch sizes and held-out test sets in the LDA experiments raises questions about the baseline results. A more comprehensive comparison, including the use of different mini-batch sizes as a variance reduction technique, would strengthen the paper.
CLARITY: The paper is generally clear, with figure 1 being particularly effective. However, clarification is needed on how covariance quantities are estimated in the experiments, specifically whether empirical covariances from the mini-batch are used. Figure 2 is difficult to read and would benefit from markers to distinguish between lines.
ORIGINALITY: Although the use of variance reduction techniques for SGD is novel in this context, the paper lacks a thorough discussion of related work. The omission of relevant references, such as the work by John Paisley et al. on using control variates for variational Bayesian inference, is notable. The authors should address how their contribution differs from and builds upon existing research.
SIGNIFICANCE: The potential impact of this work is considerable, given the widespread use of SGD in large-scale optimization. The approach could be generalized to various settings beyond the applications presented. While the empirical evidence is not robust, the theoretical intuition is compelling, suggesting that further investigation could yield significant improvements.
== Other detailed comments == 
- Line 058: "discuss" should be changed to "discussion".
- Line 089: It should be clarified that matrix A depends on w.
- Equation (5): The symmetry of Cov(g, h) should be addressed, potentially by using -(cov(g,h) + cov(g,h)^T).
- Equation (6): The correct term should be cov(h,g) or cov(g,h)^T, not cov(g,h).
- Paragraph 150-154: The maximal correlation case could be highlighted, where setting hd = gd results in zero variance, albeit with the caveat of inefficient computation of E[h_d].
- Lines 295-296: The ambiguity in the sentence regarding what is "explicit" should be resolved.
- Figure 3: The specifics of how Pearson's coefficient was computed, including the choice of w and whether true or estimated covariances were used, should be provided.
=== Update after rebuttal == 
The authors should ensure a comprehensive coverage of related work and implement the suggested corrections in the final version. The contribution over existing research, such as Paisley et al.'s work, should be clearly articulated. While the experiments remain a weakness, the idea of variance reduction for SGD and its potential impact are noteworthy.