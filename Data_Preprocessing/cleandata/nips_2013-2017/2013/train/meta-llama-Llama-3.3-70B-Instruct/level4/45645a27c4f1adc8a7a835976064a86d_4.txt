This paper presents an extension of the factorized asymptotic Bayesian (FAB) inference approach to latent feature models, building upon its successful application to mixture models and hidden Markov models. The FAB method involves maximizing a lower bound of the factorized information criterion (FIC), which converges to the marginal log-likelihood. Previously, FAB was limited to models where the Hessian matrix of the complete likelihood is block diagonal. The authors overcome this limitation by deriving a lower bound of the FIC for latent feature models, demonstrating that it shares the same representation as the FIC for mixture models. The authors evaluate their approach using both synthetic and real-world datasets, comparing it to alternative methods such as fast Gibbs sampling with Indian Buffet processes, variational Bayes, and maximum-expectation IBP. The results demonstrate the superiority of the proposed FAB/LFM method, outperforming others in both predictive performance and computational efficiency.
The paper is well-structured and clearly explains the derivation of the proposed method. The extension of FAB from mixture models to latent feature models can be considered incremental, yet it presents an interesting and efficient approach. Although I am not an expert in this area, I believe the paper would be of significant interest to the NIPS community. Overall, the paper presents a compelling idea, and the results are convincing, leading me to support its acceptance.