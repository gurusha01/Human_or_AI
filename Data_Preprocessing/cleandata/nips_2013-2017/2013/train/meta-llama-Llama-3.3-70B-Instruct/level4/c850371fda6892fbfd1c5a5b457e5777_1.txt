This manuscript presents a novel approach to parameter tuning for a given training algorithm, ensuring differential privacy. The proposed method takes a set of training and validation examples, model parameters, a training algorithm, and a performance measure, and outputs a differentially private hypothesis based on predefined privacy parameters. The core concept underlying this procedure is the introduction of (\beta1, \beta2, \delta)-stability, which quantifies the stability of performance with respect to changes in the training and validation sets. This stability notion guides the application of the exponential mechanism, for which a utility bound is also derived.
The procedure involves constructing a tree where each node represents the differentially private partial sum of its descendant nodes' values. During the execution of the differentially private FTAL, the learner queries this tree to obtain a differentially private partial sum of previous gradients. Notably, only O(log T) accesses are required to learn this partial sum, resulting in a variance of O(log T) for the noise added to each value.
The authors propose two algorithms: one for the full information model and another for the bandit setting. In the full information setting, the algorithm achieves improved regret bounds, particularly for strongly convex and L-Lipschitz cost functions, where a regret bound of O(poly log T) is attained, comparable to FTAL. In the bandit setting, the authors introduce the "one-shot gradient" technique to evaluate gradients and demonstrate optimal regret bounds with respect to T for strongly convex and L-Lipschitz cost functions.
However, the manuscript could benefit from improved clarity, as the problem settings are challenging to follow. Specifically, it is unclear what private instances are being protected by differential privacy. The definitions of oblivious and adaptive adversaries in the bandit setting are also unclear. Additionally, several lines require revision for better understanding: line 107's reference to a "stronger setting" is ambiguous, the sentence in lines 229-232 is hard to follow, and line 305 incorrectly refers to the "one-shot gradient" technique as "one-point gradient."
Despite these clarifications, the manuscript demonstrates significant improvement in the regret bounds of differentially private online learning in the full information model.