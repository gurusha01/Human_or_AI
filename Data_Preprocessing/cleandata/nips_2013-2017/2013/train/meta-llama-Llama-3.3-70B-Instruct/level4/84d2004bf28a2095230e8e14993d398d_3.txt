This manuscript presents the GreeDi algorithm, designed to optimize monotone submodular functions under a cardinality constraint within a distributed system framework, specifically leveraging the mapreduce paradigm. The authors conduct an extensive experimental evaluation across a diverse range of datasets and theoretically demonstrate that the distributed approach achieves an objective value that remains within a reasonable bound of its centralized counterpart.
A key requirement for the optimized function is that it must be "decomposable," meaning it can be expressed as the sum of multiple submodular functions. This property is crucial as it allows the function to be evaluated without dependence on the entire dataset.
To further enhance the clarity and completeness of the manuscript, I would appreciate explicit answers to the following questions:
1) What is the precise amount of communication required between the mappers and reducers in the mapreduce implementation? Specifically, how much data must be shuffled, as this directly impacts the communication cost in this setup?
2) What is the maximum number of items that can be reduced to a single key? This metric is essential for assessing the potential overload of a single machine.
3) What is the maximum number of iterations required in the worst-case scenario?