This paper presents a new model selection criterion for binary latent feature models, which is similar to variational Bayes but instead uses the Bayesian Information Criterion (BIC) to approximately integrate out parameters. The authors demonstrate improved performance in terms of held-out likelihood scores compared to existing implementations of the Indian Buffet Process (IBP). 
The proposed method appears to be a reasonable approach, motivated by a plausible asymptotic argument, and offers a computational advantage over other IBP inference methods by reducing the complexity to an EM algorithm with additional penalties. 
However, the technical contributions, while novel, seem to be incremental, building upon the work of [3] and [4]. Some aspects of the mathematical exposition are unclear, such as in equation (3), where the log sum of z values could be infinite if all values are zero, which has a non-zero probability under any non-degenerate variational distribution. This raises questions about the Finite Information Criterion (FIC) score. 
Furthermore, Theorem 2 lacks clarity on the asymptotic regime and assumptions, particularly regarding the behavior of K as N increases. The application of the BIC to each component individually seems to require a fixed finite K, contradicting the infinite number of components in the IBP model. 
Additionally, there are minor issues, such as the definition of pk(X | z{\cdot, k}) in section 2, which should only include data points assigned to mixture component k, and equation (4), which is missing an expectation on the left-hand side. 
The quantitative results in Table 1 show that the proposed method achieves higher predictive likelihood scores in less time compared to alternative methods. However, the stopping criterion for the Gibbs sampler is not specified, making it difficult to interpret the results. The improvements in predictive likelihood are significant, but it is unclear whether they are due to the model selection criterion or the algorithms getting stuck in local optima. 
The results would be more convincing with evidence that the difference is due to the model selection criterion rather than implementation details. The fact that the Gibbs sampler learns an excessively large number of components in some datasets suggests that it may be getting stuck in local optima. 
Overall, the proposed approach seems reasonable but incremental, with some imprecise mathematical formulations. While the experimental results show improvements in running time and predictive likelihood, further analysis is needed to interpret these results and understand the source of the improvements.