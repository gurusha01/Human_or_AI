This manuscript proposes a novel approach to parameter estimation in hidden Markov models by leveraging tensor factorization, where only a limited fraction of observations are available. The authors' contribution lies in recognizing the tensor factorization structure inherent in the problem, building upon the work of Anandkumar et al. Although identifying this structure is relatively straightforward, the paper takes an interesting turn by demonstrating the recovery of transition distributions from expectations over sums. The inclusion of sample complexity results and toy simulations adds depth to the discussion. Overall, the paper presents a reasonable contribution with some innovative ideas.
To enhance the presentation, it would be beneficial to introduce the actual model and problem statement earlier, ideally before Theorem 1, to provide readers with a clear understanding of the paper's objectives. The current exposition dedicates excessive space to reviewing the tensor decomposition framework; a concise description of key concepts, supplemented by a citation to the original work or an appendix, would suffice.
In Section 3.1, it is crucial to explicitly state that the model generates $N$ independent and identically distributed replicates. Initially, the text's ambiguity regarding the feasibility of obtaining multiple replicates from a single random draw from $\pi_0$ caused confusion.
The theorems, while accurately stating true facts, lack contextual guidance on the important properties to focus on. For instance, the factorization structure outlined in line 062 of Appendix A.1 could be integrated into the main text to provide clarity. Additionally, more intuitive explanations, particularly for Theorem 3, would be appreciated.
The experimental section is notable for establishing a qualitative relationship between empirical findings and theoretical bounds. The relative ease of learning $U$ is unsurprising, but a comparison with the Expectation-Maximization (EM) algorithm would be valuable, given the motivation that EM may become stuck in local optima, whereas this method does not. Furthermore, experiments using real data, especially from sources cited in the context of learning from non-sequence data, would strengthen the paper.
On a technical note, primes should be positioned above the subscripts in $M2$ and $M3$ (lines 318 and 320). This paper applies tensor factorization techniques to the novel setting of learning a Hidden Markov Model (HMM) from non-sequence data. While the paper could be improved to convey more insight, it remains a reasonable contribution to the field.