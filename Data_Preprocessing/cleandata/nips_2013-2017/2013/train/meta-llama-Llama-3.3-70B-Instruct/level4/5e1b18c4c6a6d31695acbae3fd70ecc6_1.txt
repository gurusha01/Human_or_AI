The proposed method involves document compression to expedite processing in tasks such as classification, utilizing a coding scheme akin to the Lempel-Ziv algorithm, which stores pointers to recurring substrings within the document. This approach is formulated as a combinatorial optimization problem, with its solution approximated through a series of convex problems solved using the Alternating Direction Method of Multipliers (ADMM). 
Experiments conducted on text classification problems demonstrate that document compression yields improvements in memory and computational efficiency, albeit with a slight decrease in precision. 
I find the approach intriguing, although my familiarity with the Natural Language Processing (NLP) literature is limited, making it challenging for me to fully assess the novelty of the method. I have a few minor observations: 
- The statement "an optimal lossless compression of D..." requires clarification, specifically regarding whether the coding scheme achieves optimality in terms of minimum entropy.
- It is unclear whether the reweighting scheme can be accurately interpreted as a majorization-minimization procedure in this context.
- Minimizing equation (4) with respect to w involves computing the Fenchel conjugate of the weighted linfty-norm, which requires a projection onto the weighted l1-norm, the dual norm of the linfty-norm. Upon adding a non-negativity constraint, this entails a projection onto the simplex. Notably, algorithms for projecting onto the simplex have been established, as seen in works such as Brucker's "An O(n) algorithm for quadratic knapsack problems" (1984), and Bach et al.'s "Optimization with Sparsity-Inducing Penalties" (2012), which discusses the computation of Fenchel conjugates for norms. 
The paper presents a document compression technique aimed at accelerating document processing, which appears to be effective.