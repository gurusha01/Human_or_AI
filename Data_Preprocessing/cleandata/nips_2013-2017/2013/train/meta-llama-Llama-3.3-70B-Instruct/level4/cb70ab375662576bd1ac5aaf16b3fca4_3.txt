This paper presents a spectral learning-based approach for learning Markov models and hidden Markov models in a non-sequential setting, where the goal is to learn from sparse realizations obtained at random times. The authors provide rigorous proofs for empirical moments and establish a sample complexity bound. The paper is well-structured and easy to follow.
However, it would be beneficial to include a brief review of existing methods, such as maximum likelihood-based approaches, to motivate the non-sequential sequential model learning. Instead of reproducing the tensor decomposition algorithm of Anandkumar et al., the authors could focus on introducing the general idea of expressing parameters as symmetric observable tensors, as shown in equation 2. This could involve omitting Algorithm 2 and Theorem 1.
The moment equations for learning a Markov model in a non-sequential setting are defined on page 5, allowing for the recovery of the expected transition probability matrix T and parameters π of the Dirichlet prior using the symmetric orthogonal tensor decomposition algorithm. However, the proposed search heuristic for recovering the transition matrix P could be clarified by presenting it as pseudo-code. The assumption of a zero entry in P is reasonable for large Markov chains with sparse transitions. Additionally, it is noted that π could be expressed exactly as α / α0, rather than proportionally, to facilitate understanding of the moment equation proofs.
The experimental results are limited to synthetic data, and it is essential to validate the algorithm on real-life data to demonstrate its effectiveness. A comparison with conventional learning algorithms would also provide justification for the proposed approach.
In Section 4, the authors mention that as the number of data items N increases, the take-off point in Figure 1(a) approaches the true value r = 0.3. However, it is unclear why the projection error is non-zero when r is less than 0.3 and zero when r is larger. A comment on this issue would be helpful. Furthermore, the legend of Figure 1 should specify that the logarithms are base 10, rather than base 2.
The motivation behind choosing spectral learning algorithms over maximum likelihood-based approaches is partly due to their speed. The authors should consider including a comparison of the proposed algorithm's speed with a maximum likelihood-based approach, such as EM. Alternatively, they could explore using a matrix eigendecomposition-based algorithm, which would be computationally cheaper than the tensor decomposition approach.
Overall, this is a well-written and executed paper that addresses an interesting problem. With some revisions to address the mentioned concerns, it has the potential to make a significant contribution to the field.