This paper introduces the concept of \Sigma-optimality as a criterion for active learning in Gaussian Random Fields (GRFs), building upon the previously proposed V-optimality, which focuses on minimizing L2 loss by querying nodes. However, the authors argue that V-optimality may not be suitable for classification problems due to its surrogate nature for the more relevant 0/1 loss.
Although \Sigma-optimality has been explored in other contexts, its application to active learning is novel. The paper demonstrates that \Sigma-optimality yields a submodular function, providing a guaranteed (1 âˆ’ 1/e) approximation to the global optimum. Experimental results show that \Sigma-optimality outperforms V-optimality and other active learning criteria when using a greedy algorithm.
Similar to V-optimality, \Sigma-optimality tends to select nodes with high variance or those correlated with high-variance nodes. Nonetheless, the authors observe that \Sigma-optimality also favors nodes with greater influence, typically located at cluster centers, as indicated by Equation 3.7.
The experimental evaluation on synthetic datasets, generated according to the assumed model, reveals that \Sigma-optimality surpasses V-optimality and random selection. The results are presented for specific model parameters \beta and \delta. It would be intriguing to investigate the scenarios where \Sigma-optimality fails to outperform other methods, such as when random or V-optimality are comparable or superior. Additionally, exploring the performance on sparse or highly connected graphs could provide valuable insights.
The poor performance of MIG and, to a lesser extent, Uncertainty-based active learning is surprising. An explanation for this phenomenon would be beneficial, and it would be helpful to include details on how MIG was implemented and to discuss the differences between these methods and the proposed approach.
The paper is well-written, and its significance lies primarily in the demonstrated improved performance through experimental evaluation. However, the reasons behind its superior performance over other methods are not entirely clear. The work represents a modest extension of previous ideas, particularly the application of V-optimality to the same problem and the adaptation of \Sigma-optimality from recent research. The proven submodularity guarantee constitutes an incremental contribution, and the novelty of the paper stems from the use of \Sigma-optimality as an active learning criterion in GRFs for classification, a well-established problem.