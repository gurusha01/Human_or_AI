The paper presents a Bayesian approach to incorporating human feedback into reinforcement learning, building upon and comparing to the baseline of Bayesian Q-learning. This method is experimentally validated on two domains, Pac-Man and Frogger.
Quality
-------
The paper is generally well-structured and logical, but several issues were identified:
1) Although the authors assert that their method does not convert feedback into rewards or values, the calculation of delta_{s,a} from the count of right/wrong labels essentially achieves this conversion.
2) The proposed approach is referred to as "policy shaping," which may be misleading since feedback is provided per action rather than per entire policy or episode; a more fitting name could be "action shaping."
Clarity
-------
The paper is well-written and coherent, but some sections require clarification:
1) In Section 4.2, the authors initially assume that an action's optimality in a given state is independent of labels provided for other actions in the same state, leading to formula (1). However, formula (2) contradicts this assumption by relying on delta_{s,j} values from other actions in the same state.
2) The description of how human feedback is provided (as binary right/wrong labels immediately following an action) is introduced too late in the text (around line 112, page 3) and should be presented earlier.
3) The use of pre-recorded human demonstrations to produce a simulated oracle in Section 5.3 is not entirely clear.
Originality
-----------
Some of the most intriguing problems, such as the credit assignment problem (mentioned on line 125) and the case of multiple optimal actions per state, are left for future work.
The proposed method for resolving multiple sources appears simplistic, as it assigns equal weight to both probabilities regardless of their reliability. A more sophisticated approach would be to utilize the value of C to assess the reliability of human feedback and incorporate this into the decision-making process.
Significance
------------
In this reviewer's opinion, the demonstrated improvement from using additional human feedback is not substantial enough to justify the significant amount of extra information required by the algorithm. If the "oracle" information were directly provided to the agent as demonstrations, it could initiate learning from a high-return initial policy and further improve it during episodes.
The paper proposes a Bayes-optimal method for integrating binary right/wrong action labels from humans into reinforcement learning. While the paper is well-written, further improvements are needed in terms of clarity, originality, and significance to enhance its overall impact.