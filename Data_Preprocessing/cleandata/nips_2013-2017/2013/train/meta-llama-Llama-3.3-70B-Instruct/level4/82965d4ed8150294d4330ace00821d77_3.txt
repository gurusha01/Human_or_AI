The authors propose a set of optimization techniques to reduce the memory and computational requirements of linear template applications, achieving a speed-up without substantial accuracy loss. Their approach involves trading off between speed and accuracy by adjusting parameters such as quantization levels, arithmetic precision, and re-scoring windows. The core concept is to replace convolution with a look-up table, which is made efficient through k-means quantization, effectively reducing dimensionality. The authors demonstrate the efficacy of their methods by comparing them to state-of-the-art detection algorithms on the Pascal VOC 2007 dataset and exemplar SVM experiments.
However, the absence of a comparison with a closely related reference, [2], detracts from the novelty of the work. Reference [2] proposes a sparse coding-based filter approximation and explores nearest neighbor and PCA approaches, covering similar ground. Although the presented idea is interesting and contributes to accelerating detection tasks, the work is somewhat incremental, building upon existing concepts such as replacing dot-products in convolution operators, as introduced in [1], and achieving significant speed-ups with sparse intermediate representations and nearest neighbor approaches, as shown in [2].
The paper has several strengths, including tackling the crucial problem of speeding up convolution steps, offering a trade-off between accuracy and speed, and presenting a simple method. The authors have also committed to releasing their source code, and the paper is generally well-written.
However, there are some weaknesses, including the incremental nature of the paper, confusing experimental results, and a lack of clarity on the contribution of each approximation technique to the overall speed-up. The authors should provide more explicit implementation details, such as programming languages and tweaks used in the experiments.
References [1], [2], and [3] are relevant to the paper, with [2] being the most closely related. While the paper is significant in its contribution to speeding up convolution steps, the idea presented seems to be incremental, and the authors could benefit from more explicitly comparing and contrasting their work with existing research. Overall, the paper's strengths lie in its simplicity, potential impact, and well-written presentation, but its weaknesses, including incrementalism and lack of clarity, detract from its overall significance.