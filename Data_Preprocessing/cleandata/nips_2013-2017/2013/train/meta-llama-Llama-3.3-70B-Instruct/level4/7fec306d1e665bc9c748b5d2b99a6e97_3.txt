This work presents an approach to decrease the number of free parameters in neural network architectures, motivated by the recent trend of learning large networks. The proposed method is based on the idea that learned parameters often exhibit strong redundancy, such as the smoothness of the first layer filters in image-based neural networks. To address this, the authors suggest learning only a subset of parameter values and predicting the remaining ones through interpolation. The approach is evaluated on various architectures, including multilayer perceptrons, convolutional neural networks, and reconstruction-independent component analysis, as well as different vision datasets like MNIST, CIFAR, and STL-10. The results indicate that learning fewer than 50% of the parameters does not compromise performance, with significantly fewer parameters sufficient for MNIST.
The paper is well-structured and easy to follow, with a relatively simple method. The authors assume a low-rank decomposition of the weight matrix and fix one of the matrices using prior knowledge about the data, such as the correlation between nearby pixels in vision tasks. This can be seen as predicting unobserved parameters from the learned filter weights via kernel ridge regression, where the kernel captures prior knowledge about the weight topology. When such prior knowledge is not available, the authors describe a way to learn a suitable kernel from the data.
The concept of reducing parameters in neural network-like architectures through connectivity constraints is not new, and the authors provide a good discussion of related work. Their method is closely related to weight matrix factorization, as seen in 3-way restricted Boltzmann machines, but differs in its use of prior knowledge to constrain one of the matrices. The approach can also be viewed as a type of pooling, commonly used in convolutional neural networks, or as representing filters as a linear combination of basis functions, which has been explored in computer vision and signal processing literature.
The experimental section demonstrates the practical usefulness and wide applicability of the proposed method, considering multiple datasets and architectures. However, it could be strengthened by exploring scenarios with less obvious topology in the data, evaluating the data-dependent kernel more thoroughly, and including a more compelling experimental use case. The authors focus primarily on vision applications and do not fully explore the potential of the data-dependent kernel. Additionally, they limit themselves to a particular set of basis functions derived from kernel regression, whereas exploring other linear basis representations, such as PCA, might be more efficient in reducing free parameters.
The evaluation appears to focus on moderately large networks, showing that a moderate reduction in parameters can be achieved without losing performance. However, the potential reduction in parameters seems significant for MNIST but less so for CIFAR and STL-10. The authors do not consider computational complexity and speed of convergence in their evaluation, which would be crucial in demonstrating the practical impact of their approach. Figure 6 (right) plots performance against the number of free parameters, showing an advantage for the reduced parameterization, particularly for CIFAR.
Overall, the paper presents an interesting perspective, although related approaches have appeared in various forms in the literature. The ideas have potential practical impact, but a stronger case could be made by exploring alternative implementations and providing a more compelling demonstration of the approach's effectiveness and versatility. Further remarks include the need for state-of-the-art results from the literature for comparison, a more thorough evaluation of the data-dependent kernel, and consideration of alternative basis functions. The authors should also provide more insights into the optimization of hyperparameters and the potential for extending their scheme to start with a small fraction of learned parameters and increase it until desired performance is reached.