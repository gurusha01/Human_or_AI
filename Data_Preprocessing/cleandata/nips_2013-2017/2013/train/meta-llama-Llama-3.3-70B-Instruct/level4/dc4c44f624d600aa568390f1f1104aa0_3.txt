The authors present a novel generalized method of moments (GMM) framework for estimating parameters in the Plackett-Luce model, which involves decomposing complete rankings into pairwise comparisons to iteratively estimate the required quantities. 
I find this approach intriguing, particularly the examination of how the method of decomposition affects the consistency and quality of the solutions obtained, leading to a trade-off between computational time and efficiency. 
However, a significant limitation of the GMM approach lies in its applicability, as the results are restricted to full rankings, whereas real-world preference aggregation problems often involve partial rankings or preferences. Although the authors mention plans to extend their work to the partial case, including some preliminary results on this aspect would substantially enhance the paper's impact. 
Regarding the experimental evaluation, several points warrant discussion. 
- The experiment using real data is not entirely persuasive, as it compares models on a dataset where neither achieves a satisfactory fit, with one model's solution being used as the ground truth. The rationale behind choosing this particular dataset is unclear. 
- A comparison with a gradient descent optimization of the Plackett-Luce log likelihood would be valuable. In my experience, gradient descent is effective for PL models, especially when the relative order of parameters is more critical than their absolute values, such as in Kendall correlation criteria. A carefully implemented gradient descent should also offer better efficiency per iteration and lower storage requirements than GMM. Are there any available results on this comparison? 
In its current form, while the proposed approach shows promise, its impact on the relevant research field is likely to be limited, and addressing these concerns could significantly improve its contribution.