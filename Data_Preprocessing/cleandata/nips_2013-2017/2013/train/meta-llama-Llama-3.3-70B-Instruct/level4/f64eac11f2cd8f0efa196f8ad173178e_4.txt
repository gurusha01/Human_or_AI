This manuscript presents a novel policy-gradient reinforcement learning approach, wherein the step size is adaptively determined by maximizing a lower bound on the expected performance gain. Initially, the authors consider a scenario where the policy gradient is known, establishing a lower bound on the performance difference between the original and updated policies following a gradient-based update. Given that this bound is a fourth-order polynomial, they then examine a special case where the stochastic policy is represented as a Gaussian distribution with fixed standard deviation and a mean that is a linear combination of state features, yielding a quadratic lower bound with a single maximum for positive step sizes. The authors subsequently relax the assumption of a known policy gradient, deriving a high-probability lower bound based on an epsilon-optimal estimate of the policy gradient. They demonstrate how existing techniques, such as REINFORCE and G(PO)MDP/PGT, can provide these estimates, resulting in an adaptive step size policy gradient method that learns from sample trajectories without requiring an environmental model. The proposed method is evaluated through a simple simulation experiment using a 1-dimensional toy problem.
Overall, the paper is of high quality and well-written, making a compelling argument that existing research on policy-gradient methods has primarily focused on improving gradient estimators, leaving room for advancement in automating step sizes. The theoretical contributions are substantial, providing a principled framework for automatic step size selection by maximizing lower bounds on expected performance gain. The paper's impact extends beyond the specific algorithm, offering general results that can be applied to derive algorithms with different policy representations and gradient estimators.
However, the empirical evaluation is a notable weakness, as it only considers a toy problem with a single policy parameter. The results show that manual step size selection can lead to slow learning or divergence, whereas the adaptive method consistently converges within the threshold. For large standard deviations of the Gaussian policy, the adaptive method outperforms fixed step sizes, but for small standard deviations, it performs worse, suggesting a trade-off between policy determinism and the tightness of the lower bound. The authors also present results using REINFORCE and PGT estimators, but these results primarily demonstrate that increasing the number of trajectories improves gradient estimates and learning performance. Unfortunately, even with many trajectories, the errors remain significant, leading to loose bounds and raising concerns about the practical implications of the results.
Moreover, the empirical results only compare the proposed method to fixed-step size variants, omitting comparisons to other baseline methods, such as EM-based policy-gradient-like methods, which do not require tuning a free parameter. 
In conclusion, the paper is well-written and makes a significant theoretical contribution, but the empirical evaluation is preliminary and yields mixed results, highlighting the need for further experimentation to fully demonstrate the practical effectiveness of the proposed adaptive step size policy gradient method.