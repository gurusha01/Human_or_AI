This manuscript presents a novel formulation of the n-gram selection problem for document classification, framing it as a lossless compression task that is solved through iterative relaxation of the initial combinatorial problem. Although the concept of unsupervised feature learning via compression is not entirely new, the specific approach outlined in this paper is intriguing and appears to be original, yielding reasonable performance in small-scale experiments. The paper's clarity and motivation are notable strengths, with the algorithm's presentation being generally comprehensible, albeit relying on a substantial supplementary section for full details. However, a more in-depth analysis of the algorithm's computational characteristics, including experiments that investigate the trade-offs between computational cost and accuracy, would be beneficial in assessing the method's scalability, particularly in light of its claimed parallelizability. Furthermore, on the experimental front, comparisons with widely used lossy representation techniques, such as embeddings (as discussed in the works of Bengio et al., 2006; Collobert and Weston, 2008; and Mikolov et al., 2010, among others), would provide valuable context. Additionally, an examination of the trade-offs between model size and accuracy achieved by this method in comparison to sparsifying regularization applied to uncompressed n-gram features would be insightful. Overall, the paper offers an elegant and well-motivated approach to formalizing n-gram feature selection for document classification as a lossless compression problem, complete with an efficient relaxation algorithm for the hard combinatorial problem at its core, though it would benefit from further analysis and experimentation to fully realize its potential.