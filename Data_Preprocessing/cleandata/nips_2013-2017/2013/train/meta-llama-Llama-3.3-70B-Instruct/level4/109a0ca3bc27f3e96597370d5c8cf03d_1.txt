This manuscript explores the application of convex norm-regularized optimization techniques to the factorization of approximately low-rank tensors, with a particular emphasis on the "latent" norm approach, as introduced in [25], and its comparison to the previously utilized "overlap" norm approach. The authors derive theoretical bounds for the mean-squared-error behavior of the "latent" norm under various noise assumptions, which are shown to be favorable compared to the existing analysis of the overlap norm. This supports the empirically observed superior performance of the "latent" norm approach in previous studies. The theoretical findings are further validated through numerical simulations. Additionally, the paper presents a generalized form of both norms and establishes a duality theorem for this generalization.
The manuscript is well-structured and appears to be technically sound, although a detailed examination of the proofs in the supplementary material was not conducted. The results may be of interest to researchers focused on optimization methods for tensor decompositions. However, several concerns were noted: 
(i) The scope of the results is narrow and primarily technical, with only peripheral connections to machine learning, which may limit their potential impact at NIPS. While the authors justify the relevance of tensor factorizations in machine learning, they do not explore the proposed approaches within the context of machine learning problems, either theoretically or experimentally. 
(ii) The findings seem somewhat incremental, and the techniques employed do not appear to be particularly novel. The results provide additional theoretical support for a recently proposed approach [25], which was already demonstrated to be effective empirically. 
A minor but notable issue is the absence of lower bounds to complement the upper bounds provided. This omission is potentially problematic, as the upper bounds are used for comparative purposes between different algorithms, arguing for the superiority of one over another.
Additional Comments:
- The abstract suggests the proposal of a new class of structured Schatten norms for tensors, which might be misleading. The primary focus of the paper is on analyzing existing approaches (overlap and latent), with the generalization being briefly introduced in section 2.1 and used to prove a dual norm (Lemma 1). No algorithms are proposed based on these generalized Schatten norms, nor is their behavior analyzed or tested experimentally.
- Figure 1 is presented early in the introduction but lacks context without the background information provided in section 2 onwards.
- Minor corrections were noted: "Tensor is" should be "Tensor modelling is" (Line 38), a comma is missing after "Conventionally" (Line 40), and "also in such situations" could be rephrased as "in such situations as well" (Line 395). The paper's primary contribution lies in its theoretical error bounds for a recently proposed low-rank tensor decomposition approach. While technically sound, the results may be considered somewhat incremental and potentially limited in their impact at NIPS.