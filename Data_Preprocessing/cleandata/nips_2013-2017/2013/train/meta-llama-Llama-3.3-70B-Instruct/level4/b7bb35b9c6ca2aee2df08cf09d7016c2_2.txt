The paper proposes a novel approach to constructing a parameter server for distributed machine learning systems, providing each client with a view of parameters that have a bounded degree of staleness. By leveraging a combination of caches, the client interface ensures that all updates to the parameter array that occur after a fixed deadline are visible, along with more recent updates whenever possible. This interface presents a unified view of parameters that incorporates most updates, along with a best-effort service for more recent updates. The authors demonstrate that this simple semantic preserves the theoretical guarantees of cyclic delay methods while achieving significant practical speedups. Empirical analysis across multiple problems and cluster configurations reveals that the advantage stems from a combination of increased efficiency over Bulk Synchronous Parallel (BSP) and optimization progress per update over asynchronous methods.
This paper introduces a straightforward set of semantics that improves upon existing "parameter server" schemes in large-scale machine learning applications. The method not only retains simple theoretical guarantees but also exhibits substantial speedups over obvious competitors in reasonably sized tests, making it a positive contribution. 
The approach is expected to handle well-known problems in loaded clusters, such as stragglers, and the caching mechanism may reduce overhead dramatically for the slowest nodes, providing a degree of load balancing that is challenging to achieve through other means. However, it would be beneficial to demonstrate whether this effect is sufficient to enable the "catch-up" phenomenon claimed in the paper.
The discussion of read semantics is provided, along with details on the cache policy, which falls through when the local cache is stale due to the read-my-writes policy. To clarify, do all local writes get written through to all caches or servers? A sentence of clarification on the writing semantics would facilitate reproduction of the work.
The authors make a compelling case for their approach, as is typical of well-structured systems papers. 
The advantages of this paper include:
- Fairly simple semantics
- Preservation of basic theoretical guarantees for typical methods, such as Stochastic Gradient Descent (SGD)
- Experimental analysis demonstrating actual speedups on multiple distributed problems, with the speedup originating from expected sources
The disadvantages are:
- A comparison to cyclic delay methods would be a welcome addition
- The approach may be more complex to re-implement compared to asynchronous or BSP methods; it is unclear whether the moderate speedup justifies the additional complexity
Update
The authors' clarifications and additions have addressed my concerns, presenting a reasonable argument that resolves my limited qualms. The paper proposes a framework for parameter servers in machine learning applications, offering moderate speedups and appealing theoretical and practical properties.