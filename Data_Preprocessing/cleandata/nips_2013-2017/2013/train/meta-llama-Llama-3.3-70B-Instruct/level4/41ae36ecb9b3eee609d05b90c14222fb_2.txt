This paper demonstrates the linear convergence of the standard proximal gradient method for problems regularized by the trace norm, relaxing the requirement for strong convexity of the loss function, a crucial consideration in numerous machine learning and signal processing applications.
The manuscript is well-organized and clearly written, closely adhering to the Lipschitzian error bound technique outlined by Tseng [20], which had previously left open the application to the nuclear norm case. Upon review, the core of the proof appears to be correct, aligning with my understanding.
However, a notable omission is a direct comparison or analogy to the vector case involving l1-norm regularization, as discussed in [20,22] and related studies. It would be beneficial to establish a relationship between the results and techniques employed in the l1-case and those for the trace norm, including an explanation for why techniques from the l1-case do not directly apply to the trace norm scenario. Given the current presentation, the l1-norm case should logically follow as a corollary from the trace norm results.
When evaluating the paper's contribution, this aspect should be considered, as some might view the extension of l1-results to the trace norm as somewhat expected, although I personally find it to be a valuable addition.
Independently, I suggest a minor adjustment to the paper's title, as the primary novel contribution pertains more to the loss function aspect than to imposing a structured form on the trace norm regularization. The increased flexibility introduced by the linear operator A is actually on the loss side, not within the regularizer itself.
Regarding minor issues:
- The notation could be clarified, particularly the term "P" which is referred to as a sampling or mask operator. Since "P" is fixed, "sampling" might imply randomness unnecessarily.
- Reference l337 appears to be broken.
- The definitions of "Q-" and "R-" linear convergence might not be widely recognized and could be reiterated for clarity.
- The residual R(X) defined in equation (6) could be further elucidated in relation to its original definition in [20] to enhance interpretability.
Generally, I recommend balancing the extensive technical proof of Lemma 3.2 in Section 3 with more discussion and possibly additional conclusions.
Following the author's feedback:
I appreciate the responses to my queries. Concerning the title, my comment was aimed at the term "structured," which I found unnecessary in this context, though I leave this decision to the authors. The paper's core contribution, proving linear convergence under weaker assumptions than strong convexity for trace norm regularized problems, remains significant for many applications.