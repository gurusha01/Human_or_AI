Here is a paraphrased version of the review:
-- Update 
A few additional considerations are worth noting. The recent work by Simo Saarka on continuous-discrete time systems, which was referenced, may provide an interesting contrast, particularly in its use of Gaussian cubature as a deterministic approximation method. This could be a valuable addition to the discussion and future research directions. Furthermore, I wonder if it would be possible to develop an algorithm directly from the variational Gaussian approach, which would offer a well-defined objective function for optimization, potentially reducing numerical issues and allowing for interpretation in terms of marginal likelihood. Subsequently, low-order marginal corrections could be incorporated using cumulant perturbations, as seen in the work of Opper for EP, and also discussed by Barber and van de Laar (http://arxiv.org/pdf/1105.5455.pdf). I look forward to reading the final version of the paper.
-- Original 
The paper proposes an algorithm for approximate Bayesian inference in models with both continuous and discrete time observations, which can be formulated within the framework of latent Gaussian models. The authors employ a parallel expectation propagation algorithm to derive a principled approach for inference and learning in both continuous and discrete time settings. This EP inference algorithm is embedded within an EM algorithm to learn model parameters and marginal distributions. The algorithm's effectiveness is demonstrated in several experimental settings.
Overall, I found the paper to be enjoyable and appreciated how it extends the applicability of approximate message passing to a broader class of models.
One interesting aspect was the observation that EP updates for continuous time limits converge to variational Gaussian updates, which is related to the latent Gaussian structure. However, I wondered if there is a more fundamental reason underlying this connection.
The algorithm appears to be robust due to the implied fractional updating, but I would appreciate comments on any implementation challenges encountered, such as slow convergence of parameter learning or numerical stability issues.
The algorithm's cubic complexity, resulting from the inverses in the inference and M-step updating, is a limitation. I would like to see a discussion on approaches to scaling up such algorithms.
In the experimental section, it would be beneficial to include plots that provide insight into the algorithm's convergence. Additionally, it would be interesting to demonstrate the advantages of having an estimate of the marginal likelihood. For example, in figure 3C, each individual point could be plotted with a size proportional to the marginal likelihood value.
Overall, the paper is well-written and extends the applicability of approximate Bayesian inference methods to continuous and discrete time settings, which will be of interest to many readers.