This study seeks to provide insight into the novel neural network algorithm of dropout by establishing a theoretical foundation, thereby offering guidance on the selection of relevant parameters to achieve optimal performance.
Quality: The research itself is of high caliber, but the manuscript's quality can be enhanced. Several typos and undefined symbols are present, and the inclusion of a summary of the key findings at the conclusion would significantly improve the manuscript's readability.
Clarity: The paper is, for the most part, well-written, albeit with notable exceptions due to typos, undefined terms, and the absence of a summary. A list of suggested corrections includes:
- Introduction, line 7, potentially contains a typo or ambiguity.
- Equation (1) lacks a definition for 'l'.
- Section 3.1 requires a definition for 'n'. Clarification is needed on whether stochasticity 'S' is a result of dropout, and the basis for 'm=2^n', which appears to represent the number of different inputs in the network. There is also a typo in equation 7.
- Section 5.1 contains a typo in equation 25 and an extra full stop immediately after equation 28.
- The first line of section 5.2 is missing a '+' in the definition of the sigmoidal function.
- The layout of figure captions is inconsistent.
- References in the text begin with number [6].
- A summary or conclusions section is notably absent.
Originality: To my knowledge, this paper presents original work, addressing crucial questions about setting up a network with dropout.
Significance: I consider the article significant. It prompts the intriguing question of whether parallels can be drawn between the dropout algorithm, when applied to connecting weights instead of nodes, and the release probability of biological synapses, which can be unreliable and thus implement a form of "dropout" mechanism. This work is interesting as it grounds the dropout algorithm in solid mathematical foundations. However, the manuscript would benefit from correction of the existing typos and improvement in readability.