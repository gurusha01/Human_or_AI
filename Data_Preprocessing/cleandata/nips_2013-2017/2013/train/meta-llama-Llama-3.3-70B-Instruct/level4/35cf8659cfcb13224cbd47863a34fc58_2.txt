The authors present a generalized Bayesian nonparametric stochastic block model, extending it to a hierarchical framework to infer a hierarchical structure of entities based on observed links, rather than a flat clustering. They introduce a learning algorithm utilizing agglomerative clustering to select a tree representing valid partitions of entities. Empirical results from two social connectivity datasets demonstrate that their approach outperforms the Infinite Relational Model (IRM) in terms of both speed and accuracy.
This paper addresses an intriguing issue that is likely to resonate with a significant portion of the NIPS audience. From a technical standpoint, while Bayesian nonparametric methods are conceptually appealing, they often suffer from slow performance in practice. Meanwhile, social scientists are increasingly employing sophisticated Bayesian models for data analysis, making a hierarchical extension to a popular network-discovery model a valuable contribution. This paper effectively bridges these two aspects.
The key innovation lies in restricting allowed partitions to conform to a tree structure, enabling efficient computation of the marginal likelihood of the data. In contrast, the IRM must consider an exponential number of possible partitions. However, the relationship between the learning procedure for the trees and the generative story behind the Bayesian model is not entirely clear, particularly given the greedy nature of the algorithm. It is unclear whether this procedure can be viewed as performing true posterior inference or obtaining a point estimate. Clarification of Section 4 would be beneficial to address this concern.
The results are impressive, with the proposed method performing comparably to or slightly better than the IRM in terms of various performance measures, including AUC, predictive log-likelihood, and accuracy. Although the asymptotic improvements in these scores may not be substantial in absolute terms, the claimed runtime is significantly faster than IRM inference.
To enhance the overall quality of the paper, improvements in clarity and writing are necessary. Several distracting spelling and grammar errors are present, such as on line 126. Additionally, the prose is sometimes difficult to follow, with vague phrasing in the introduction (e.g., lines 47-50) and unclear descriptions of the datasets used for training (Monastery networks and NIPS authorship). The mathematical notation, such as Equation 5, is occasionally obtuse and unnecessary. Figure 3 is also confusing, particularly for readers unfamiliar with the monastery dataset. Consider using a more familiar dataset or providing additional context to facilitate understanding. Section 4, which presents the core novel contribution, requires multiple readings to comprehend.
If these issues can be addressed, the proposed approach has sufficient novelty and strong results to warrant publication. The authors propose a novel Bayesian model of graph edge structure, inspired by the IRM and akin to agglomerative clustering. While the core learning algorithm is insightful and the experimental results are strong, the writing quality requires improvement. With revisions to enhance clarity and address the mentioned concerns, this paper has the potential to make a valuable contribution to the field.