This paper integrates a recent advancement in stochastic dual coordinate ascent with the conventional method of parallelizing training by assigning a mini-batch of examples to each process and averaging the resulting gradients. Although this integration is novel, its most significant contribution lies in the trade-off bounds between communication and computation, which has the potential to be a highly influential work by introducing a new methodology. However, the paper's significance is limited by its poorly designed experiments. Notably, the authors fail to discuss the impact of input dimension on the bounds, particularly in the case of sparse data, which is commonly encountered in big data scenarios.
Two major limitations are evident in the significance of this trade-off:
- The discussion on page 5 omits the number of dimensions, d. For dense data, this could be considered a constant factor since both communication and computation costs are linear in d (assuming the main computational loop is W.x, which is O(d)). However, for sparse data, the computation cost for a single dot product scales with the number of non-zero features, denoted as d'. Thus, the total computation cost per iteration becomes O(md'), and in the worst-case scenario where no features from the m examples overlap, the communication cost also becomes O(md'), losing the critical m ratio between them as analyzed on page 5. This is particularly relevant given that in the KDD cup data, only one feature out of 1 million is non-zero.
- The experiments only consider the number of iterations and not the total running time, which includes communication latency. Furthermore, the time per iteration depends on the parameters. Therefore, the experiments reported in figure 3, which show that increasing m always reduces the number of iterations, are misleading because each iteration scales as O(m*d) in computation cost and O(d) in communication cost. A more convincing approach would be to present a curve showing the total training time, with optimal values for m and K that minimize this time.
In summary, the current experiments have limited significance as they merely demonstrate a reduction in the number of iterations as a function of m and K, a trivial observation that does not require theorem 1. Additionally, the authors' mention of using openMPI lacks detail about the architecture, such as the number of nodes, cores per node, or whether it's a single multicore setup with shared memory, which would imply no communication costs.
The most intriguing observation from theorem 1 is the existence of an "effective region" for K and m. However, the experiments only show that decreasing lambda provides more flexibility in choosing m and K. Establishing an effective upper threshold for the mK product, supported by actual training times, would be a significant result.
Detailed comments include:
- The mention of "tens of hundreds of CPU cores" is ambiguous and should be clarified as either thousands of cores or tens of clusters with hundreds of cores. If communication costs are a factor, the focus should be on clusters rather than multicores with shared memory.
- The proof of theorem 1, while correct, includes unnecessary parts. Specifically, the last sentence referencing "T0" is confusing since there is no T0 in the context.
- The repetition that DisDCA is parameter-free is misleading because the choice of lambda is critical.
- On page 3, line 128, the terminology regarding alpha being the associated dual of w, not x, may need clarification.
- Figure 3 contains a typo ("varing" instead of "varying") and the plots are difficult to read.
Overall, the algorithm presented is novel and interesting, with a theoretical derivation of the communication/computation trade-off, but the experiments fail to adequately demonstrate this trade-off.