This manuscript presents a distributed variant of stochastic dual coordinate ascent, namely DisDCA, which retains the convergence guarantees of its centralized counterpart, SDCA. Notably, DisDCA does not require parameter tuning, unlike ADMM, and its empirical performance is comparable to that of a finely tuned ADMM implementation.
The paper is technically rigorous and well-organized, with a clear and concise presentation. The proposed algorithm, although novel, is straightforward to follow, and the discussion on the trade-offs between computational and communication costs is particularly insightful. It is commendable that the authors have addressed the competitive regime of distributed learning algorithms, a topic of significant interest in the current research landscape, yet often overlooked in terms of in-depth analysis. Overall, the paper is well-written, and my criticisms are minor, primarily concerning the readability of the figures in both the main manuscript and supplementary materials, which could be improved by relegating some of the results to multiple pages in the supplementary section, potentially including a representative sample in Figure 3. This paper makes a solid contribution and is of relevance to the NIPS community, offering distributed DCA as a viable alternative to ADMM.