The paper presents a groundbreaking RL controller that achieves performance comparable to black-box optimization techniques in the challenging domain of Tetris, a benchmark that has historically posed difficulties for RL algorithms, making it a relevant and timely contribution to the RL community.
Quality
The paper's primary contribution lies in the application of the previously published CBMPI algorithm to the game of Tetris, yielding competitive results with the state-of-the-art Cross-Entropy method (CE) optimization algorithm. The empirical results are compelling, and the significant improvement in performance compared to prior attempts to apply RL controllers to this domain is noteworthy. However, the paper could benefit from a more in-depth discussion on the potential reasons behind this improvement. For instance, the introduction mentions the key conjecture that techniques searching in the policy space rather than the value function space are more effective, but further elaboration would enhance the paper. Specifically, two areas warrant more detailed exploration: 
1. The state distribution used for sampling rollout states, which is derived from a highly proficient Tetris controller, raises questions about the extent to which this choice influences CBMPI's performance. It would be informative to report on any alternative solutions that were tried but did not yield satisfactory results, such as whether the states visited by CBMPI during learning constitute a reasonable rollout distribution.
2. The use of CMA-ES as the optimization algorithm in the "Classifier" stage of CBMPI is intriguing, given its relation to CE, as discussed in "Path Integral Policy Improvement with Covariance Matrix Adaptation" (ICML 2012). A natural inquiry is how replacing CMA-ES with other types of optimizers, such as gradient-based methods, would impact the results.
Clarity
The paper is well-structured, easy to follow, and clearly states its contributions. Several minor suggestions for improvement include: 
- Clarifying the definition of the "Classifier" in line 222, as the output seems to be more aligned with "labels" than differences of Q values.
- Correcting the terminology in lines 266/267, where CMA-ES is referred to as a "classifier" instead of an optimization algorithm.
- Providing a rationale for the disjoint presentation of graphs 4a and 4d, while graph 5d includes a comprehensive performance comparison for all algorithms.
- Rectifying the spelling error in Section 3.1, "Experimental".
Originality and Significance
The paper offers a novel solution to a challenging problem, providing valuable insights into the application of RL algorithms to complex domains. Although it does not introduce new algorithmic or theoretical tools, the results are significant for the RL community. 
Several questions arise: 
1. Would the authors be willing to share their code to facilitate the reproduction of their experiments if the paper is accepted?
2. Given the high variance of scores in Tetris, it would be beneficial to include confidence intervals for the mean values in the graphs to assess the statistical relevance of the observed differences, such as those in Figure 4D.
3. Regarding the claim in lines 393-395, it is unclear how CBMPI would perform if it received the same number of samples as CE, whether it would improve or if the learning curve has already plateaued.
4. Concerning the comparison of the best policies, it would be helpful to know how the scores for policies DU and BDU were computed, whether they were taken from the referenced paper [20] or obtained by executing those policies using the authors' code base, and to understand the performance of the best policy discovered by CE in the authors' experiments. A more detailed discussion of the results would enhance the paper's impact and usefulness to the community, as it presents a valuable contribution to the literature, shedding new light on the previously underexplored phenomenon of RL algorithms' poor performance in Tetris.