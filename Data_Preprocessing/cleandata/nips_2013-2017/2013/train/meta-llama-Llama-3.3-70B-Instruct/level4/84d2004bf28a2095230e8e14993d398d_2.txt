This paper presents a compelling approach to addressing a significant problem domain, offering a straightforward and practical algorithm that is easily implementable, alongside robust empirical results. The theoretical analysis is thorough and transparent, acknowledging the limitations of the method while making reasonable assumptions for learning tasks.
However, a major critique of the paper is its concise nature, particularly in the experimental section, which lacks detailed explanations of baseline comparison methods and result interpretations. For instance, Figure 1 is dense with information. The notable performance of the greedy/max approach warrants further discussion. Moreover, the paper fails to provide a discussion on runtime or CPU costs for the methods, which is surprising given the emphasis on scalability.
The concept of lazy evaluation is surprisingly underdiscussed in the paper, despite its potential to significantly enhance efficiency. This is a crucial aspect that should be explored, especially in a scalability-focused paper, as it may be unfamiliar to some engineering-oriented readers. Combining lazy evaluation with mapreduce could potentially circumvent the need for the approximation method presented, by allowing for efficient parallel processing of subsequent rounds after the initial selection.
The introduction to section 3 could benefit from mentioning the concept of "diminishing returns" to provide an intuitive understanding of submodularity for unfamiliar readers. Furthermore, the paper lacks clarity on what is meant by "fit on one machine," specifically whether it refers to RAM, disk space, or CPU processing capacity. The first paragraph of section 2 suggests CPU cost as the primary bottleneck, but disk I/O is often a significant concern as well. The benefits of utilizing a large-RAM machine with multiple cores are not explored.
To accommodate the requested additional details, some sections could be condensed or removed. Section 6 does not contribute substantially to the paper and could be cut. The pseudo-code for algorithm 1 is also unnecessary. The first two sentences and the last sentence of paragraph 3 in section 1 could be removed, and the remaining sentence merged with the preceding one. The first paragraph of section 2 could be significantly shortened, and paragraphs 2 and 3 of section 3.2 could be omitted, with the potential addition of discussing a naive approach involving sampling the dataset.
The paper proposes a novel method for submodular maximization in a distributed framework, demonstrating its soundness through theoretical analysis under reasonable assumptions and reporting favorable results across various applications.