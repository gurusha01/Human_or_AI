This paper presents an analysis of two optimization objectives for active learning on Gaussian Random Fields (GRFs), specifically Sigma-Optimality and the V-Objective, with the latter having been previously proposed but not thoroughly examined. The authors demonstrate that the maximization versions of these objectives are submodular, thereby providing approximation guarantees for the performance of greedy algorithms. Additionally, they establish that the covariance matrix of GRFs satisfies the Suppressor-free condition, a property that enables good performance guarantees for greedy algorithms by mitigating the adverse effects of suppressors in subset selection.
Theoretical insights offered by the paper are noteworthy, particularly the finding that GRFs are suppressor-free, a characteristic that was previously unknown to be satisfied by a general class of covariance matrices. This result has significant implications both theoretically and practically, as it underscores the importance of suppressor-free conditions in ensuring the efficacy of greedy algorithms for subset selection problems.
Although the novelty of the Sigma-Optimality criteria is somewhat diminished by its prior proposal in an Active Surveying context, the authors' rigorous analysis and derivation of approximation guarantees via submodularity are commendable. The analysis of the V-Objective, on the other hand, offers new insights into its submodular nature and the performance guarantees of greedy algorithms for its maximization.
However, the paper could benefit from improvements in its presentation. Certain sections lack technical precision, which, while not affecting the correctness of the analysis, may cause confusion. For instance, the distinction between minimization and maximization versions of the problem is not consistently maintained, with the paper sometimes incorrectly suggesting that results applicable to the maximization objective also hold for the minimization objective. Specifically, the greedy multiplicative performance guarantees and submodularity are properties of the maximization objective, not the minimization objective, as implied in several places, including the beginning of Section 2.3.
Furthermore, there are inaccuracies in the discussion of computational complexity and the role of the suppressor-free property. The calculation of the global optimum is not rendered intractable by submodularity, and a greedy solution is not necessarily required, as stated in the first paragraph of page 4. Additionally, the suppressor-free property does not prove Equation 2.3, as Equation 2.3 is merely a definition, not a theorem or lemma that requires proof. It is also worth noting that the proper citation for the greedy bound for submodular maximization is [Nemhauser et al., 1978], rather than Streeter and Golovin.
The experiments section yields a surprising result, with the Mutual-Information criteria performing worse than random selection. This outcome warrants further investigation to understand the underlying reasons and potential implications for the application of active learning criteria in practice.
In conclusion, the authors' analysis of active learning criteria for GRFs and their demonstration of the submodularity of the objectives, as well as the suppressor-free condition of GRFs, constitute valuable theoretical contributions. However, to enhance the paper's clarity and accuracy, it is essential to address the noted imprecisions in notation and sentences, ensuring that the technical content is presented in a manner that is both precise and accessible to readers.