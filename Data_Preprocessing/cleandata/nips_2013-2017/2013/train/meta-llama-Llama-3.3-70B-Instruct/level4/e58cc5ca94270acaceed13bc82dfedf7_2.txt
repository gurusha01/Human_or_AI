This paper explores the matrix completion problem within the context of collaborative filtering and multi-label learning, with a key contribution being the incorporation of side information on the rows and columns of the matrix into the model. By doing so, the authors demonstrate how leveraging this side information can lead to improvements in sample complexity compared to the standard matrix completion formulation. Specifically, they propose a model where the matrix of interest, M, can be decomposed as M = A Z0 B^T, with A and B representing the known side information. This formulation reduces the degrees of freedom when A and B are low-dimensional, as the unknown matrix Z0 is significantly smaller than M, intuitively suggesting better sample complexity guarantees.
The authors extend the usual nuclear norm minimization program and show that the sample complexity required to guarantee a unique minimizer is O(μ^2 r (ra + rb) log n) for an n × n matrix of rank r, with Z0 of dimension ra × rb, and μ related to the incoherence parameter. This result implies that when the side information is low-dimensional (ra, r_b small), the sample complexity is sublinear in the matrix dimensions, an improvement over standard results.
To complement their theoretical findings, the authors conduct simulations and experiments in the context of multi-label learning. These simulations demonstrate that their algorithm is both computationally and statistically more efficient than singular value thresholding, a standard matrix completion algorithm. Furthermore, their algorithm outperforms several baselines on multiple datasets in the multi-label learning application.
The paper addresses an interesting problem and contributes to the matrix completion literature by incorporating side information. The results are noteworthy, and the paper is well-written. However, the proof techniques, while novel, do not offer significant technical innovation given the existing work on matrix completion. Consequently, the contribution seems somewhat modest. It would be beneficial to explore necessary conditions in the noiseless case and guarantees for the noisy version of the matrix completion problem. The experiments are convincing but could be presented more effectively, potentially by using graphs instead of tables to improve readability and save space.
Several questions arise from the paper:
1. The main paper lacks discussion on the role of Ω0 and Ω1 and the condition Ω1 ≥ q Ω0. Providing intuition on why this condition is necessary and under which regimes it is satisfied would be valuable.
2. Lemma 4 in the appendix requires Ω0 ≤ |Ω| ≤ Ω1, but the main theorem does not include an upper bound on the sample complexity. Clarification on the necessity of the upper bound on |Ω| and its intuition is needed.
3. The decision to hold out test instances from the experiments, rather than randomly sampling the label matrix and testing on unobserved entries, warrants explanation.
4. The paper's title is misleading, as "Speedup" implies a computational focus, which is not the primary contribution. The paper makes an interesting, albeit small, contribution to the matrix completion literature by studying how incorporating side information can lead to improved sample complexity guarantees. The analysis, while using standard convex analysis tools, does not offer substantial technical innovation.