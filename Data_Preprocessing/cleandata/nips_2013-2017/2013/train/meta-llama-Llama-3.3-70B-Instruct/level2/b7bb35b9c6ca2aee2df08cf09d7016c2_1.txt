This paper proposes a novel distributed machine learning (ML) system, called Stale Synchronous Parallel (SSP), which aims to maximize the time computational workers spend doing useful work on ML algorithms while providing correctness guarantees. The SSP model allows workers to read older, stale versions of shared parameters from a local cache, reducing the time spent waiting for communication with a central server. The authors provide a proof of correctness under SSP and demonstrate its effectiveness through empirical results on several ML problems, showing faster convergence compared to fully-synchronous and asynchronous schemes.
The paper is well-structured, and the authors provide a clear explanation of the SSP model, its theoretical analysis, and experimental results. The writing is clear, and the use of figures and tables helps to illustrate the concepts and results. The authors also provide a comprehensive review of related work, highlighting the differences between their approach and existing distributed ML systems.
The strengths of the paper include:
* The proposal of a novel distributed ML system that addresses the trade-off between computation and communication
* A thorough theoretical analysis of the SSP model, including a proof of correctness and convergence bounds
* Empirical results demonstrating the effectiveness of SSP on several ML problems
* A comprehensive review of related work, highlighting the differences between SSP and existing approaches
The weaknesses of the paper include:
* The evaluation is limited to a few ML problems, and it would be beneficial to see results on a broader range of tasks
* The implementation of SSPtable, the parameter server, is not fully described, and some details are omitted for brevity
* The paper could benefit from a more detailed discussion of the potential limitations and challenges of the SSP approach
Overall, the paper presents a significant contribution to the field of distributed ML, and the proposed SSP model has the potential to improve the efficiency and scalability of ML algorithms. The authors demonstrate a good understanding of the field, and the paper is well-written and easy to follow.
Arguments for acceptance:
* The paper proposes a novel and interesting approach to distributed ML
* The theoretical analysis is thorough and well-presented
* The empirical results demonstrate the effectiveness of the proposed approach
* The paper is well-written and easy to follow
Arguments for rejection:
* The evaluation is limited to a few ML problems
* The implementation details of SSPtable are not fully described
* The paper could benefit from a more detailed discussion of the potential limitations and challenges of the SSP approach
Recommendation: Accept, with minor revisions to address the weaknesses mentioned above.