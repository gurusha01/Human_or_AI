This paper proposes a novel algorithm, Epoch Mixed Gradient Descent (EMGD), for smooth and strongly convex optimization problems. The main claim of the paper is that EMGD can reduce the number of full gradients needed from O(√κ log 1/ǫ) to O(log 1/ǫ) by utilizing both full and stochastic gradients. The paper provides a thorough theoretical analysis of the algorithm, including a proof of its convergence rate and a comparison with other optimization algorithms.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a good background on convex optimization and motivates the need for reducing the number of full gradients. The related work section provides a comprehensive overview of existing optimization algorithms and their limitations.
The algorithm itself is well-described, and the theoretical analysis is rigorous and detailed. The authors provide a clear explanation of the key components of the algorithm, including the mixed gradient descent step and the use of stochastic gradients. The proof of the convergence rate is based on a martingale difference sequence and the Hoeffding-Azuma inequality, which is a common technique in stochastic optimization.
The paper also provides a comparison with other optimization algorithms, including Nesterov's algorithm, SAG, and SDCA. The authors show that EMGD has a lower computational cost than the full gradient method when the condition number κ is less than or equal to n^2/3, where n is the number of functions in the objective function.
The strengths of the paper include:
* A novel algorithm that reduces the number of full gradients needed for smooth and strongly convex optimization problems
* A thorough theoretical analysis of the algorithm, including a proof of its convergence rate
* A comparison with other optimization algorithms, including Nesterov's algorithm, SAG, and SDCA
The weaknesses of the paper include:
* The algorithm requires knowledge of the condition number κ, which may not be available in practice
* The paper does not provide experimental results to demonstrate the effectiveness of the algorithm in practice
Overall, the paper is well-written and provides a significant contribution to the field of optimization. The algorithm proposed in the paper has the potential to improve the efficiency of optimization algorithms in practice, especially when the condition number κ is large.
Arguments for acceptance:
* The paper proposes a novel algorithm that reduces the number of full gradients needed for smooth and strongly convex optimization problems
* The paper provides a thorough theoretical analysis of the algorithm, including a proof of its convergence rate
* The paper compares the algorithm with other optimization algorithms and shows its advantages
Arguments against acceptance:
* The algorithm requires knowledge of the condition number κ, which may not be available in practice
* The paper does not provide experimental results to demonstrate the effectiveness of the algorithm in practice
Recommendation: Accept with minor revisions. The authors should consider adding experimental results to demonstrate the effectiveness of the algorithm in practice and discussing the limitations of the algorithm, including the requirement of knowledge of the condition number κ.