This paper studies the problem of estimating continuous quantities using a crowdsourcing approach, where workers' reliabilities and biases are unknown and diverse. The authors investigate the value of control items, which are items with known answers used to evaluate workers' performance, and provide a simple rule of thumb for crowdsourcing practitioners. The paper presents theoretical results for the optimal number of control items under different scenarios and provides empirical results on simulated and real datasets.
The main claims of the paper are: (1) the optimal number of control items for the two-stage estimator is O(√`), where ` is the budget, and (2) the optimal number of control items for the joint estimator is O(`/√nt), where nt is the number of target items. The authors also show that the joint estimator is more sensitive to model misspecification than the two-stage estimator.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and the related work. The theoretical analysis is rigorous, and the empirical results are convincing. The paper also highlights important practical issues such as model misspecification and heteroscedasticity.
The strengths of the paper are:
* The paper provides a thorough theoretical analysis of the optimal number of control items for both the two-stage and joint estimators.
* The authors provide empirical results on simulated and real datasets, which demonstrate the effectiveness of their approach.
* The paper highlights important practical issues such as model misspecification and heteroscedasticity.
The weaknesses of the paper are:
* The paper assumes a Gaussian model for the crowdsourced labels, which may not always be realistic.
* The authors do not provide a clear comparison between the two-stage and joint estimators in terms of their performance.
* The paper could benefit from more discussion on the implications of the results for crowdsourcing practitioners.
Overall, the paper is well-written, and the authors provide a significant contribution to the field of crowdsourcing. The results are interesting and have important implications for practitioners. However, the paper could benefit from more discussion on the limitations of the approach and the implications of the results for real-world applications.
Arguments for acceptance:
* The paper provides a thorough theoretical analysis of the optimal number of control items for both the two-stage and joint estimators.
* The authors provide empirical results on simulated and real datasets, which demonstrate the effectiveness of their approach.
* The paper highlights important practical issues such as model misspecification and heteroscedasticity.
Arguments against acceptance:
* The paper assumes a Gaussian model for the crowdsourced labels, which may not always be realistic.
* The authors do not provide a clear comparison between the two-stage and joint estimators in terms of their performance.
* The paper could benefit from more discussion on the implications of the results for crowdsourcing practitioners.
Recommendation: Accept with minor revisions. The authors should address the weaknesses of the paper by providing more discussion on the limitations of the approach and the implications of the results for real-world applications. Additionally, the authors should consider providing more comparison between the two-stage and joint estimators in terms of their performance.