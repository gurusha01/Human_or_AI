This paper introduces Policy Shaping, a novel approach to incorporating human feedback into Reinforcement Learning (RL), and presents Advise, a Bayesian algorithm that maximizes the utility of feedback by using it as direct policy labels. The authors argue that traditional methods, which convert feedback into rewards or values, are limited and propose a more effective characterization of human feedback.
The paper is well-structured, and the authors provide a clear overview of the related work, the Policy Shaping paradigm, and the Advise algorithm. The experimental setup is thorough, and the results demonstrate the effectiveness of Advise in comparison to state-of-the-art methods, such as Action Biasing, Control Sharing, and Reward Shaping.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem of incorporating human feedback into RL
* A thorough review of related work and a clear explanation of the limitations of traditional methods
* A well-designed experimental setup that evaluates the performance of Advise in various scenarios
* A detailed analysis of the results, which highlights the advantages of Advise over other methods
The weaknesses of the paper include:
* The assumption that the human feedback is consistent and accurate, which may not always be the case in real-world scenarios
* The lack of a clear method for estimating the feedback consistency, C, which is a critical parameter in the Advise algorithm
* The limited evaluation of Advise in larger domains, which may affect its scalability and robustness
Arguments for acceptance:
* The paper presents a novel and well-motivated approach to incorporating human feedback into RL
* The experimental results demonstrate the effectiveness of Advise in comparison to state-of-the-art methods
* The paper provides a thorough analysis of the results and highlights the advantages of Advise
Arguments against acceptance:
* The assumption of consistent and accurate human feedback may not be realistic in real-world scenarios
* The lack of a clear method for estimating the feedback consistency, C, may limit the applicability of Advise
* The limited evaluation of Advise in larger domains may raise concerns about its scalability and robustness
Overall, the paper presents a well-motivated and novel approach to incorporating human feedback into RL, and the experimental results demonstrate the effectiveness of Advise. However, the limitations of the paper, such as the assumption of consistent and accurate human feedback and the lack of a clear method for estimating the feedback consistency, should be addressed in future work. With some revisions to address these limitations, the paper has the potential to make a significant contribution to the field of RL.