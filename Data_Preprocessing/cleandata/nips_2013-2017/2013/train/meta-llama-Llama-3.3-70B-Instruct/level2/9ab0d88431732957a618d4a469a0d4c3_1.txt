This paper proposes a new formulation for learning multiple models from data, called Regularized Weighting (RW), which seeks to assign a distribution of weights over data points to each model. The approach is designed to be robust to outliers and fat-tailed noise, and is shown to have a non-trivial breakdown point for the case of clustering. The authors provide a generalization bound for the RW formulation and propose an efficient optimization procedure to solve it.
The main claims of the paper are: (1) the RW formulation is robust to outliers and fat-tailed noise, (2) it has a non-trivial breakdown point for clustering, and (3) it provides a generalization bound. The support for these claims comes from theoretical analysis, including the derivation of the breakdown point and generalization bound, as well as empirical evaluations on synthetic and real-world datasets.
The paper is well-written and clearly organized, making it easy to follow. The authors provide a thorough introduction to the problem of multiple model learning and motivate the need for a robust approach. The RW formulation is well-explained, and the theoretical analysis is rigorous and convincing. The empirical evaluations provide additional evidence for the effectiveness of the approach.
The usefulness of the paper lies in its potential to provide a general and robust framework for learning multiple models from data. The approach can be applied to a variety of problems, including clustering, subspace segmentation, and Gaussian mixture models. The generalization bound and efficient optimization procedure make it a practical tool for real-world applications.
The paper reflects common knowledge in the field, and the authors demonstrate a good understanding of relevant literature. The references are comprehensive and relevant, and the citations are proper.
The novelty of the paper lies in the proposed RW formulation, which is a significant improvement over existing approaches. The authors provide a clear explanation of how their approach differs from previous contributions and demonstrate its advantages.
The completeness of the paper is good, with sufficient details provided for reproducibility. The authors provide a clear description of the optimization procedure and the theoretical analysis.
The limitations of the paper are acknowledged, including the potential challenge of highly unbalanced classes in the data. The authors provide suggestions for future work, including the development of methods to estimate the relative frequencies of the classes.
Overall, I would recommend accepting this paper. The strengths of the paper include its clear and well-organized presentation, rigorous theoretical analysis, and empirical evaluations. The weaknesses are minor and do not detract from the overall quality of the paper.
Arguments pro acceptance:
* The paper proposes a novel and robust formulation for learning multiple models from data.
* The theoretical analysis is rigorous and convincing, providing a generalization bound and breakdown point for clustering.
* The empirical evaluations provide additional evidence for the effectiveness of the approach.
* The paper is well-written and clearly organized, making it easy to follow.
Arguments con acceptance:
* The paper may benefit from additional empirical evaluations on more diverse datasets.
* The challenge of highly unbalanced classes in the data is acknowledged, but not fully addressed.
* The optimization procedure may require further refinement for very large datasets.