This paper proposes a general approach to variance reduction in stochastic gradient optimization using control variates. The authors demonstrate the effectiveness of their approach on two practical problems: MAP estimation for logistic regression (a convex problem) and stochastic variational inference for latent Dirichlet allocation (a non-convex problem). The main claim of the paper is that their approach can reduce the variance of the noisy gradient, leading to faster convergence and better performance.
The support for this claim comes from theoretical analysis and empirical experiments. The authors show that their approach can reduce the variance of the noisy gradient by constructing a control variate that is highly correlated with the noisy gradient. They also provide empirical results on several datasets, including covtype, Nature, New York Times, and Wikipedia, which demonstrate the effectiveness of their approach in terms of convergence rate and predictive performance.
The paper is well-written, and the authors provide a clear and concise explanation of their approach and its theoretical foundations. The experiments are well-designed, and the results are convincing. The paper also provides a good review of related work and discusses the limitations of their approach.
The strengths of the paper include:
* The authors propose a novel approach to variance reduction in stochastic gradient optimization, which is a significant contribution to the field.
* The paper provides a clear and concise explanation of the approach and its theoretical foundations.
* The experiments are well-designed, and the results are convincing.
The weaknesses of the paper include:
* The paper assumes that the control variate is highly correlated with the noisy gradient, which may not always be the case.
* The authors do not provide a thorough analysis of the computational cost of their approach, which could be a limitation in practice.
* The paper could benefit from a more detailed discussion of the limitations of their approach and potential avenues for future work.
Overall, I would recommend accepting this paper for publication. The paper makes a significant contribution to the field of stochastic gradient optimization, and the results are convincing. However, the authors should address the weaknesses mentioned above in the final version of the paper.
Arguments for acceptance:
* The paper proposes a novel approach to variance reduction in stochastic gradient optimization.
* The paper provides a clear and concise explanation of the approach and its theoretical foundations.
* The experiments are well-designed, and the results are convincing.
Arguments against acceptance:
* The paper assumes that the control variate is highly correlated with the noisy gradient, which may not always be the case.
* The authors do not provide a thorough analysis of the computational cost of their approach.
* The paper could benefit from a more detailed discussion of the limitations of their approach and potential avenues for future work.
Rating: 8/10
Confidence: 0.8
Recommendation: Accept with minor revisions.