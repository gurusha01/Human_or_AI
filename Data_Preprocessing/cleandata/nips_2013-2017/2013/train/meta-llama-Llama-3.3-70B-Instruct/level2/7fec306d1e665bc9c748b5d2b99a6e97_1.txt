This paper presents a novel approach to reducing the number of parameters in deep neural networks by exploiting the structure in the weights. The authors demonstrate that by representing the weight matrix as a low-rank product of two smaller matrices, they can predict a significant portion of the weights without a substantial drop in accuracy. The technique is shown to be effective in various architectures, including multilayer perceptrons, convolutional networks, and reconstruction ICA.
The paper is well-written, and the ideas are clearly presented. The authors provide a thorough analysis of the technique, including its strengths and weaknesses. The experimental results are impressive, with the authors demonstrating that they can predict more than 95% of the weights in some cases without a drop in accuracy.
The technique presented in this paper has the potential to be highly useful in practice, as it could significantly reduce the computational resources required to train large neural networks. The authors also highlight the potential for this technique to be used in conjunction with other methods for reducing the number of parameters, such as tying weights in a convolutional pattern.
One potential limitation of the technique is that it requires a good understanding of the structure in the weights, which may not always be available. The authors address this limitation by proposing several methods for constructing dictionaries, including data-driven approaches. However, further research may be needed to fully explore the potential of this technique.
In terms of novelty, the paper presents a significant improvement over existing approaches to reducing the number of parameters in deep neural networks. The technique is orthogonal to other methods, such as dropout and rectified units, and could potentially be used in conjunction with these methods to further improve performance.
The paper is well-organized, and the authors provide a clear and concise summary of the main ideas. The related work section is thorough, and the authors provide a clear discussion of the strengths and weaknesses of their approach compared to other methods.
Overall, I would recommend accepting this paper for publication. The technique presented is novel, effective, and has the potential to be highly useful in practice. The paper is well-written, and the authors provide a thorough analysis of the technique, including its strengths and weaknesses.
Arguments for acceptance:
* The paper presents a novel and effective technique for reducing the number of parameters in deep neural networks.
* The technique has the potential to be highly useful in practice, as it could significantly reduce the computational resources required to train large neural networks.
* The paper is well-written, and the authors provide a thorough analysis of the technique, including its strengths and weaknesses.
* The experimental results are impressive, with the authors demonstrating that they can predict more than 95% of the weights in some cases without a drop in accuracy.
Arguments against acceptance:
* The technique may require a good understanding of the structure in the weights, which may not always be available.
* Further research may be needed to fully explore the potential of this technique.