This paper presents a significant contribution to the field of online learning, providing differentially private algorithms for a large class of online learning problems in both full information and bandit settings. The authors modify the popular mirror descent approach, specifically the follow-the-approximate-leader (FTAL) algorithm, to design private online learning algorithms that minimize a convex loss function.
The main claims of the paper are: (1) the authors provide a general technique for making a large class of online learning algorithms differentially private, (2) their algorithms achieve regret bounds that match the dependence on the input length T of the optimal non-private regret bounds up to logarithmic factors in T, and (3) their algorithms require logarithmic space and update time.
The support for these claims is strong, with a thorough analysis of the algorithms' privacy and regret guarantees. The authors provide a detailed proof of the privacy guarantee, showing that their algorithm is ε-differentially private. They also provide a regret analysis, showing that their algorithm achieves a regret bound of O(p log^2.5 T) for strongly convex functions and O(√pT log^2.5 T) for general convex functions.
The usefulness of the ideas presented in the paper is high, as they provide a significant improvement over previous work on private online learning. The authors' algorithms are practically useful, as they can be used in a variety of applications where privacy is a concern, such as online advertising and recommendation systems.
The paper demonstrates a good understanding of the field, with a thorough review of previous work on private online learning and a clear explanation of the challenges and limitations of existing approaches. The authors also provide a clear and concise explanation of their algorithms and analysis, making it easy to follow and understand.
The novelty of the paper is high, as it presents a new approach to private online learning that achieves significant improvements over previous work. The authors' use of the FTAL algorithm and the tree-based sum protocol is innovative and effective, and their analysis provides new insights into the trade-offs between privacy and regret in online learning.
The completeness of the paper is good, with a clear and concise presentation of the algorithms and analysis. The authors provide all the necessary details, including pseudocode and proofs, to allow for reproducibility and verification of their results.
The limitations of the paper are acknowledged by the authors, who note that their regret bounds for general convex functions have the form O(√T/ε) and that they would like to have a regret bound where the parameter 1/ε is factored out with lower order terms in the regret.
Overall, this paper is well-written, well-organized, and provides a significant contribution to the field of online learning. The authors' ideas are novel, useful, and well-supported, and their analysis is thorough and insightful.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of online learning, providing a new approach to private online learning that achieves significant improvements over previous work.
* The authors' algorithms are practically useful, as they can be used in a variety of applications where privacy is a concern.
* The paper demonstrates a good understanding of the field, with a thorough review of previous work on private online learning and a clear explanation of the challenges and limitations of existing approaches.
* The authors' analysis is thorough and insightful, providing new insights into the trade-offs between privacy and regret in online learning.
Arguments con acceptance:
* The paper's regret bounds for general convex functions have the form O(√T/ε), which may not be optimal.
* The authors' analysis ignores the additional source of randomness in non-private bandit algorithms, which may affect the privacy guarantees of their algorithm.
* The paper raises several open questions, including the possibility of improving the regret bounds for general convex functions and the design of private bandit algorithms that harness the randomness of non-private bandit algorithms.