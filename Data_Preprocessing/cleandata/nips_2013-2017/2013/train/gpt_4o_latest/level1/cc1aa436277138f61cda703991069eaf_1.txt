This paper addresses the problem of estimating continuous quantities using crowdsourcing, focusing on the trade-off between allocating control items (with known answers) and target items (with unknown answers). The authors provide theoretical and empirical analyses to determine the optimal number of control items under different scenarios. They propose two consensus methods: a two-stage estimator, which scores workers based on control items and then predicts target items, and a joint estimator, which incorporates both control and target items into a unified model. The paper derives rules of thumb for the optimal allocation of control items, showing that the optimal number scales as \(O(\sqrt{\ell})\) for the two-stage estimator and \(O(\ell / \sqrt{n_t})\) for the joint estimator. The authors validate their findings on simulated data and real-world datasets, such as price estimation and NFL game predictions.
Strengths
1. Theoretical Contributions: The paper provides rigorous theoretical analysis of the trade-offs in allocating control items, deriving closed-form solutions and asymptotic results for different models (e.g., bias-only, bias-variance).
2. Practical Relevance: The rules of thumb for control item allocation are highly practical and directly applicable to crowdsourcing practitioners.
3. Empirical Validation: The authors validate their theoretical predictions on both simulated and real-world datasets, demonstrating the robustness of their findings.
4. Novelty: While prior work has explored consensus methods and worker reliability estimation (e.g., Dawid and Skene, 1979; Karger et al., 2011), this paper uniquely focuses on the optimal allocation of control items, filling an important gap in the literature.
5. Clarity of Results: The paper clearly distinguishes between the two consensus methods and highlights their respective strengths and weaknesses, particularly under model misspecification.
Weaknesses
1. Model Assumptions: The theoretical results rely heavily on Gaussian models, which may not generalize well to real-world scenarios with more complex noise distributions or dependencies.
2. Sensitivity to Misspecification: The joint estimator, while theoretically efficient, is shown to perform poorly under model misspecification. This limitation is acknowledged but not fully addressed.
3. Limited Discussion of Practical Implementation: While the rules of thumb are useful, the paper could provide more guidance on how practitioners might identify when model misspecification is likely or how to select between the two estimators in practice.
4. Scalability: The computational complexity of the joint estimator, particularly for large-scale datasets, is not discussed in detail.
Arguments for Acceptance
- The paper makes a significant theoretical contribution to the field of crowdsourcing by addressing a practical and underexplored problem.
- The empirical results are robust and align well with the theoretical predictions.
- The work is novel and builds meaningfully on prior research in consensus methods and worker reliability estimation.
Arguments Against Acceptance
- The reliance on Gaussian assumptions may limit the generalizability of the findings.
- The joint estimator's sensitivity to model misspecification raises concerns about its practical applicability.
Recommendation
I recommend acceptance of this paper, as it provides a valuable contribution to the field of crowdsourcing and offers practical insights for practitioners. However, the authors should consider expanding their discussion of model misspecification and providing more practical guidance for real-world applications.