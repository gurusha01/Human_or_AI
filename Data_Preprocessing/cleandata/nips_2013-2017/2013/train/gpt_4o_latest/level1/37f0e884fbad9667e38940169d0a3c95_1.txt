The paper introduces a novel optimization algorithm, Epoch Mixed Gradient Descent (EMGD), designed to address the computational inefficiency of gradient-based methods for smooth and strongly convex optimization problems with high condition numbers. By leveraging both full and stochastic gradients, EMGD achieves a significant reduction in the number of full gradient evaluations from \(O(\sqrt{\kappa} \log \frac{1}{\epsilon})\) to \(O(\log \frac{1}{\epsilon})\), where \(\kappa\) is the condition number. The algorithm divides the optimization process into epochs, combining full gradients with stochastic gradients in a mixed gradient descent step. Theoretical analysis demonstrates that EMGD achieves linear convergence with high probability, requiring \(O(\kappa^2 \log \frac{1}{\epsilon})\) stochastic gradients. The paper also compares EMGD to existing methods, highlighting its advantages in constrained optimization, lower memory requirements, and stronger convergence guarantees.
Strengths:
1. Novelty and Originality: The proposed EMGD algorithm introduces a unique approach by combining full and stochastic gradients, which is distinct from prior methods like SAG and SDCA. This hybrid approach is well-motivated and addresses a practical challenge in optimization.
2. Theoretical Contributions: The paper provides rigorous theoretical analysis, including convergence guarantees and complexity bounds. The reduction in full gradient evaluations is a notable improvement over existing methods.
3. Broader Applicability: Unlike SAG and SDCA, which are limited to unconstrained problems, EMGD can handle both constrained and unconstrained optimization tasks, making it more versatile.
4. Practical Considerations: The algorithm requires less memory (\(O(d)\)) compared to SAG and SDCA (\(O(n)\)), which is advantageous for high-dimensional problems. Additionally, the linear convergence guarantee with high probability is stronger than the expectation-based guarantees of competing methods.
Weaknesses:
1. Dependence on Condition Number: The algorithm requires prior knowledge of the condition number \(\kappa\), which may not always be available or easy to estimate in practice. While the authors suggest future work on estimating \(\kappa\), this limitation affects the immediate usability of the method.
2. Computational Trade-offs: While EMGD reduces the number of full gradients, it requires \(O(\kappa^2 \log \frac{1}{\epsilon})\) stochastic gradients, which may be computationally expensive for very high \(\kappa\). The paper could benefit from empirical validation to assess the practical impact of this trade-off.
3. Comparative Analysis: Although the paper compares EMGD to SAG and SDCA, the discussion is largely theoretical. Experimental results would strengthen the claims and provide insights into the algorithm's real-world performance.
4. Clarity: While the theoretical analysis is thorough, some sections, particularly the proofs, are dense and may be challenging for readers unfamiliar with the mathematical background. Simplifying or summarizing key steps would enhance readability.
Recommendation:
I recommend acceptance of this paper, contingent on addressing the clarity and empirical validation concerns. The proposed EMGD algorithm is a significant contribution to the field of convex optimization, offering a novel approach with strong theoretical guarantees and practical advantages. However, experimental results and a discussion on estimating \(\kappa\) would further strengthen the paper. 
Arguments for Acceptance:
- Novel and theoretically sound algorithm with clear improvements over existing methods.
- Broader applicability to constrained optimization problems.
- Strong theoretical guarantees, including linear convergence with high probability.
Arguments Against Acceptance:
- Lack of empirical experiments to validate theoretical claims.
- Dependence on prior knowledge of the condition number.
- Dense theoretical exposition that could benefit from improved clarity.
Final Rating: 7/10 (Good paper with minor revisions needed).