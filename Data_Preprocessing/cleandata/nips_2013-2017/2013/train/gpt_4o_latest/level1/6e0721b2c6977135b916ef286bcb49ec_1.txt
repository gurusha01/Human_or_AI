This paper addresses the problem of matrix sparsification in the streaming model, proposing a novel sampling distribution to construct a sparse sketch \( B \) of a matrix \( A \) while minimizing the spectral norm error \( \|A - B\|2 \). The authors introduce a distribution that adapts to the sampling budget \( s \), combining \( L1 \)- and \( L2 \)-based sampling techniques. The proposed method achieves four key properties: (1) closed-form sampling probabilities computable with minimal information about \( A \), (2) compatibility with streaming data, (3) highly compressible non-zero entries in \( B \), and (4) near-optimality compared to the offline optimal distribution. Theoretical guarantees are provided, supported by the Matrix Bernstein inequality, and the method is shown to outperform existing \( L1 \)- and \( L_2 \)-based approaches in experiments on real-world and synthetic datasets.
Strengths
1. Technical Soundness: The paper is technically rigorous, leveraging advanced random matrix theory to derive sampling probabilities. The use of the Matrix Bernstein inequality is well-motivated and provides strong theoretical guarantees.
2. Practical Relevance: The method is designed for the streaming model, making it applicable to large-scale data scenarios such as recommendation systems and text analysis.
3. Originality: The adaptive sampling distribution, which balances \( L1 \)- and \( L2 \)-based sampling depending on the budget \( s \), is novel and addresses limitations of prior work.
4. Experimental Validation: The experiments demonstrate the efficacy of the proposed method across diverse datasets, showing significant improvements over existing techniques in terms of spectral norm error and space efficiency.
5. Clarity of Results: The paper provides clear comparisons with prior methods, highlighting the advantages of the proposed approach in both theoretical and empirical settings.
Weaknesses
1. Complexity of Presentation: While the theoretical contributions are significant, the paper is dense and may be challenging for readers unfamiliar with random matrix theory. Simplifying explanations or providing more intuition could improve accessibility.
2. Limited Discussion of Practical Constraints: The paper assumes that row \( L_1 \)-norm ratios are either known or easily estimated. While this is reasonable in some cases, a more detailed discussion of how to handle scenarios where such information is unavailable would strengthen the work.
3. Experimental Scope: Although the experiments are thorough, they focus on a limited number of datasets. Additional experiments on other types of matrices (e.g., highly structured or noisy matrices) could provide further insights into the method's robustness.
Arguments for Acceptance
- The paper presents a significant theoretical advancement in matrix sparsification, with strong guarantees and practical applicability.
- The experimental results convincingly demonstrate the superiority of the proposed method over existing approaches.
- The work addresses a critical problem in large-scale data analysis, making it relevant to the NeurIPS community.
Arguments Against Acceptance
- The dense theoretical presentation may limit accessibility for a broader audience.
- The reliance on prior knowledge of row \( L_1 \)-norm ratios could be a limitation in some practical scenarios.
Recommendation
I recommend acceptance of this paper. It makes a substantial contribution to the field of matrix sparsification, providing both theoretical insights and practical benefits. While the presentation could be improved for clarity, the strengths of the work outweigh its weaknesses.