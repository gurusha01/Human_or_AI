The paper presents novel differentially private algorithms for online learning in both full-information and bandit settings, addressing a critical challenge of ensuring privacy while maintaining competitive regret bounds. The authors extend the popular "Follow the Approximate Leader" (FTAL) framework to incorporate differential privacy by leveraging the tree-based aggregation protocol for maintaining noisy gradient sums. This approach achieves regret bounds that closely match the optimal non-private results in terms of dependence on the input length \( T \), up to logarithmic factors. The paper also introduces the first private algorithms for bandit online learning, a previously unexplored area, and demonstrates their effectiveness under both oblivious and adaptive adversaries.
Strengths:
1. Technical Contributions: The paper makes significant advancements in private online learning, particularly in the bandit setting, where no prior work existed. The adaptation of FTAL to the private setting is both elegant and technically sound.
2. Regret Bounds: The regret bounds achieved in the full-information setting (e.g., \( O(\sqrt{T} \log^{2.5} T) \) for general convex functions) are competitive with non-private counterparts, which is a strong result given the added complexity of ensuring privacy.
3. Space and Time Efficiency: The algorithms are efficient, requiring only \( O(\log T) \) space and update time, making them practical for large-scale applications.
4. Clarity of Privacy Guarantees: The paper provides rigorous proofs of differential privacy guarantees, ensuring that the algorithms meet the stringent requirements of this privacy model.
5. Novelty: The work is highly original, particularly in its application of private algorithms to the bandit setting and its use of tree-based aggregation for gradient sums.
Weaknesses:
1. Dependence on Dimension \( p \): The regret bounds for the bandit setting include a dependence on the problem dimension \( p \), which can be significant in high-dimensional settings. This limitation is acknowledged but not fully addressed.
2. Practical Evaluation: The paper lacks empirical validation of the proposed algorithms. While the theoretical results are strong, experimental results would provide additional confidence in their practical applicability.
3. Complexity of Analysis: The proofs and algorithms, while rigorous, are dense and may be challenging for readers unfamiliar with the technical details of differential privacy and online learning.
4. Suboptimal Bandit Regret: The regret bounds for private bandit algorithms (\( O(T^{2/3}) \) for oblivious adversaries and \( O(T^{3/4}) \) for adaptive adversaries) are worse than the non-private bounds (\( O(\sqrt{T}) \)) for simpler settings like multi-arm or linear bandits. This gap highlights room for improvement.
Arguments for Acceptance:
- The paper addresses a significant and timely problem at the intersection of online learning and differential privacy.
- It introduces novel algorithms and techniques that are likely to inspire future research in private machine learning.
- The theoretical results are rigorous and advance the state of the art in both full-information and bandit settings.
Arguments Against Acceptance:
- The lack of experimental results limits the paper's ability to demonstrate the practical impact of the proposed methods.
- The regret bounds for the bandit setting, while novel, are suboptimal compared to non-private algorithms, leaving open questions about their competitiveness.
Recommendation:
Overall, this paper makes a strong theoretical contribution to the field of private online learning and is well-suited for NIPS. While the lack of empirical validation and suboptimal bandit regret bounds are limitations, the novelty and rigor of the work outweigh these concerns. I recommend acceptance, with the suggestion that the authors include experimental results in a future version to strengthen the practical relevance of their work.