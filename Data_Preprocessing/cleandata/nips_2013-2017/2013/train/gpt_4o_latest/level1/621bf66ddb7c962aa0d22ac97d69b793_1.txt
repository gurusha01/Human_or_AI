The paper proposes a novel algorithm, Subsampled Randomized Hadamard Transform-Dual Ridge Regression (SRHT-DRR), for efficiently solving ridge regression in the high-dimensional setting where the number of features (p) far exceeds the number of observations (n). The authors leverage a Randomized Walsh-Hadamard Transform (SRHT) combined with feature subsampling to precondition the design matrix, reducing the computational complexity of ridge regression from \(O(n^2p)\) to \(O(np \log(n))\). The paper provides theoretical risk bounds for the proposed method and demonstrates its effectiveness through experiments on synthetic and real-world datasets.
The work builds on prior research in randomized algorithms for large-scale regression, such as the use of random projections (e.g., SRHT and Fourier transforms) to uniformize data and reduce dimensionality. The authors position their work as an improvement over existing dual ridge regression methods and randomized PCA approaches, citing relevant literature (e.g., [1], [4], [6], [16]) to contextualize their contributions. They also highlight the limitations of alternative methods like PCA in the \(p \gg n\) regime, particularly in terms of computational cost and statistical consistency.
Strengths:
1. Technical Soundness: The paper provides a rigorous theoretical analysis, including risk inflation bounds and computational cost comparisons, which are well-supported by mathematical derivations and prior work.
2. Computational Efficiency: The proposed algorithm achieves significant speedups (up to 70% reduction in computational cost) compared to traditional ridge regression, with minimal loss in accuracy.
3. Experimental Validation: Extensive experiments on both synthetic and real-world datasets (e.g., UCI ARCENE) demonstrate the practical utility of SRHT-DRR. The results convincingly show that SRHT-DRR outperforms PCA and randomized PCA in terms of both speed and accuracy.
4. Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The inclusion of detailed comparisons with alternative methods enhances its accessibility.
Weaknesses:
1. Limited Scope of Applications: While the paper focuses on ridge regression, it does not explore extensions to other regression methods (e.g., logistic regression) or broader machine learning tasks, which could enhance its impact.
2. Assumptions on Data: The theoretical analysis assumes a fixed design setting and specific properties of the data matrix (e.g., rank and orthonormality), which may not always hold in real-world scenarios.
3. Practical Considerations: The paper briefly mentions implementation details (e.g., handling non-power-of-2 dimensions) but could provide more guidance for practitioners, such as parameter tuning for \(p_{subs}\) or handling non-Gaussian noise.
Arguments for Acceptance:
- The paper addresses a significant computational bottleneck in high-dimensional ridge regression, a problem of increasing relevance in the era of big data.
- The combination of theoretical rigor and empirical validation makes the contribution both robust and practical.
- The proposed method is novel, well-motivated, and demonstrates clear advantages over existing techniques.
Arguments Against Acceptance:
- The scope is somewhat narrow, focusing solely on ridge regression without exploring broader implications or extensions.
- The reliance on specific assumptions (e.g., fixed design, Gaussian noise) may limit the generalizability of the results.
Recommendation:
Overall, the paper makes a strong contribution to the field of randomized algorithms for high-dimensional regression. Its theoretical and practical advancements are well-aligned with the goals of the conference. I recommend acceptance, with minor revisions to address the practical considerations and broader applicability of the method.