The paper presents a novel approach to learning the structure of Markov networks by formulating the problem as a constraint satisfaction task and leveraging Boolean constraint-solving technologies such as SAT, MAXSAT, SMT, and ASP. The authors introduce a new characterization of separators in Markov networks using a "balancing condition," which simplifies the encoding of the problem and enables the use of existing solver technologies to compute globally optimal network structures. The paper demonstrates the efficacy of this approach through experiments on two datasets, showing that the method can find optimal solutions where previous stochastic methods could not guarantee optimality.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with detailed theoretical formulations and proofs supporting the proposed balancing condition and its equivalence to maximum weight spanning trees. The authors also provide clear encodings of the problem into constraint languages, ensuring reproducibility.
2. Novelty: The use of Boolean constraint-solving methods for learning Markov network structures is innovative and represents a significant departure from traditional stochastic search approaches. The balancing condition is a particularly novel contribution that simplifies the problem formulation.
3. Significance: The ability to guarantee optimality in Markov network structure learning is a meaningful advancement over stochastic methods, which often struggle with convergence and optimality guarantees. This work could inspire further research into the application of constraint-solving technologies in probabilistic graphical models.
4. Clarity: The paper is well-written and organized, with clear explanations of the theoretical foundations, problem formulation, and experimental results. The inclusion of detailed proofs and constraints enhances the paper's clarity and rigor.
5. Empirical Validation: The experimental results on the "heart" and "econ" datasets are compelling, particularly the demonstration of optimality for the "econ" dataset, which had previously been unsolved.
Weaknesses:
1. Scalability: The proposed method's scalability is a concern. The exponential growth of the search space with the number of variables limits its applicability to small datasets, as evidenced by the 14-hour runtime for the "econ" dataset. While this limitation is acknowledged, it restricts the method's practical utility for larger real-world problems.
2. Comparison to Prior Work: Although the paper references stochastic methods and their limitations, a more detailed empirical comparison with these methods would strengthen the evaluation. For example, comparing runtime and solution quality on larger datasets would provide a clearer picture of the trade-offs.
3. Solver Dependency: The approach relies heavily on existing solver technologies, and the performance varies significantly across solvers. For instance, only one solver (HCLASP) was able to solve the "econ" dataset optimally. This suggests that the method's success depends on solver-specific optimizations, which may limit its generalizability.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound approach to Markov network learning, advancing the state of the art.
- The balancing condition and its efficient encoding are significant contributions with potential applications beyond the immediate problem.
- The experimental results demonstrate the method's ability to solve previously intractable problems.
Arguments Against Acceptance:
- The method's scalability is limited, making it less applicable to large datasets.
- The reliance on specific solvers and lack of broader empirical comparisons reduce the generalizability and practical impact of the work.
Recommendation:
I recommend acceptance of this paper, as its contributions to the theoretical and methodological aspects of Markov network learning are significant. However, the authors should address scalability concerns and provide a more detailed comparison with existing methods in the final version.