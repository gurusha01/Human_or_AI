This paper introduces Advise, a novel Bayesian approach to integrating non-expert human feedback into reinforcement learning (RL) through a paradigm called Policy Shaping. Unlike traditional methods that convert human feedback into rewards or values, Advise treats feedback as direct policy labels, enabling more efficient learning. The authors argue that this approach avoids the pitfalls of reward shaping, such as ad hoc parameter tuning and sensitivity to infrequent or inconsistent feedback. Experimental results in two domains (Pac-Man and Frogger) demonstrate that Advise outperforms state-of-the-art methods like Action Biasing, Control Sharing, and Reward Shaping, particularly in scenarios with noisy or sparse feedback. The paper also highlights Advise's robustness to domain size and feedback quality, showcasing its potential for real-world applications.
Strengths
1. Novelty: The paper introduces a fresh perspective on human feedback by formalizing it as policy advice rather than converting it into rewards. This approach is both innovative and well-motivated, addressing limitations of existing methods.
2. Technical Soundness: The Bayesian formulation of Advise is rigorous and well-supported by theoretical derivations. The use of feedback consistency (C) as a parameter is particularly compelling, as it directly models the reliability of human input.
3. Empirical Validation: The experiments are comprehensive, covering various feedback conditions (e.g., infrequent, inconsistent) and domain complexities. The results convincingly demonstrate Advise's advantages over competing methods.
4. Clarity: The paper is well-written and logically organized. Key concepts, such as the derivation of the feedback policy and the combination of multiple policy sources, are clearly explained.
5. Significance: By addressing the challenges of noisy and sparse human feedback, Advise advances the state of the art in Interactive Reinforcement Learning (IRL). Its robustness and simplicity make it a promising candidate for real-world deployment.
Weaknesses
1. Assumption of Known Feedback Consistency (C): The algorithm relies on an accurate estimate of C, which may not be readily available in real-world scenarios. While the authors acknowledge this limitation and suggest future work on online estimation, the current approach lacks a mechanism for dynamically adapting to varying feedback quality.
2. Limited Real-World Validation: The experiments use simulated human feedback, which, while systematic, may not fully capture the complexities of real human interactions (e.g., credit assignment errors, cognitive biases).
3. Scalability: Although the paper demonstrates robustness to larger domains, the computational cost of Bayesian updates for large-scale RL problems is not discussed in detail.
4. Comparative Baselines: While the paper compares Advise to several state-of-the-art methods, it does not include recent advances in imitation learning or other human-in-the-loop RL techniques, which could provide additional context for its contributions.
Arguments for Acceptance
- The paper addresses a significant and challenging problem in RL: integrating human feedback effectively.
- The proposed method is novel, technically sound, and empirically validated.
- Advise demonstrates clear advantages over existing methods, particularly in robustness to noisy and sparse feedback.
Arguments Against Acceptance
- The reliance on a known or static feedback consistency parameter (C) limits the practicality of the approach in dynamic, real-world settings.
- The lack of real-world human experiments weakens the claim of applicability to non-expert users.
Recommendation
Overall, this paper makes a strong contribution to the field of Interactive Reinforcement Learning by introducing a novel and robust approach to leveraging human feedback. While there are some limitations regarding real-world applicability and scalability, these do not detract significantly from the paper's core contributions. I recommend acceptance, with a suggestion to include a discussion of potential methods for online estimation of feedback consistency in future work.