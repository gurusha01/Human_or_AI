This paper introduces a novel framework for learning on hypergraphs by leveraging the total variation (TV) as a regularization functional, overcoming limitations of existing tensor-based and graph-approximation methods. Unlike prior approaches, which either restrict hypergraphs to k-uniform structures or approximate them via clique or star expansions, the proposed method directly incorporates the hypergraph structure. The authors define the total variation on hypergraphs using the Lov√°sz extension of the hypergraph cut, and extend this to a family of regularization functionals that interpolate between TV and Laplacian-type smoothness. They apply these regularizers to semi-supervised learning (SSL) and clustering tasks, deriving scalable optimization algorithms based on primal-dual methods. Experimental results demonstrate that the proposed method outperforms existing approaches, particularly in cases where hypergraph structure is critical.
Strengths:
1. Technical Novelty: The paper introduces a principled way to define and utilize total variation on hypergraphs, which is a significant departure from prior work relying on graph approximations. This is a novel contribution to the field of hypergraph learning.
2. Scalability: The authors develop efficient optimization algorithms, including proximal mappings and primal-dual methods, to handle the computational challenges posed by hypergraphs. This is particularly important for large datasets.
3. Empirical Validation: The proposed method is rigorously evaluated on multiple datasets, showing superior performance in SSL and clustering tasks compared to the clique expansion method of Zhou et al. [11]. The experiments convincingly demonstrate the benefits of directly modeling hypergraph structure.
4. Theoretical Contributions: The paper extends the concept of normalized cuts to hypergraphs and provides a tight relaxation via a nonlinear eigenproblem, which is theoretically sound and practically useful.
Weaknesses:
1. Clarity: While the paper is technically dense and well-referenced, some sections (e.g., the derivation of proximal mappings and the RatioDCA algorithm) are difficult to follow without additional background knowledge. A more detailed explanation or illustrative examples would improve accessibility.
2. Limited Baselines: The experimental comparisons focus primarily on the clique expansion method. While this is a standard baseline, additional comparisons with other state-of-the-art hypergraph learning methods would strengthen the empirical claims.
3. Hypergraph Construction: The paper acknowledges that the hypergraph construction process (e.g., using categorical features) is somewhat ad hoc. This is a critical step in hypergraph-based learning, and further exploration or justification of the construction process would be valuable.
4. Performance on Noisy Data: The method struggles on datasets like 20-Newsgroups, where label noise accumulates in large hyperedges. A discussion on how to mitigate this limitation would be helpful.
Arguments for Acceptance:
- The paper addresses a significant gap in hypergraph learning by proposing a novel, theoretically grounded, and empirically validated framework.
- The scalability of the proposed algorithms makes the method practical for real-world applications.
- The theoretical contributions, particularly the extension of normalized cuts to hypergraphs, are valuable to the broader machine learning community.
Arguments Against Acceptance:
- The clarity of some sections could be improved, which may hinder reproducibility.
- The experimental evaluation, while strong, could be more comprehensive with additional baselines and datasets.
Recommendation:
I recommend acceptance of this paper. Its contributions to hypergraph learning are both novel and impactful, and the proposed methods have the potential to advance the state of the art in this area. However, the authors should address the clarity issues and provide more details on hypergraph construction in the final version.