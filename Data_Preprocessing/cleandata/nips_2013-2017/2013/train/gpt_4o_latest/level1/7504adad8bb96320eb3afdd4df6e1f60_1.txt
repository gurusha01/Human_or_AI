This paper investigates the application of classification-based modified policy iteration (CBMPI), a relatively novel approximate dynamic programming (ADP) algorithm, to the game of Tetris. The authors aim to address the long-standing challenge of poor performance by traditional value function-based ADP methods in Tetris, compared to policy search methods like the cross-entropy (CE) algorithm. They hypothesize that Tetris is a domain where good policies are easier to represent and learn than their corresponding value functions. By leveraging CBMPI, which searches directly in policy space rather than value function space, the authors achieve state-of-the-art results in both small (10×10) and large (10×20) Tetris boards, surpassing or matching CE while using significantly fewer samples.
The paper makes a strong contribution to the field of reinforcement learning and ADP by demonstrating that CBMPI can outperform traditional ADP methods and even rival CE in Tetris, a notoriously difficult benchmark. The experimental results are thorough, with extensive comparisons across algorithms (CBMPI, CE, λ-PI, and DPI) and feature sets (Dellacherie-Thiery and Bertsekas). Notably, CBMPI achieves a new record of 51,000,000 lines on the large Tetris board, setting a benchmark for future research. The authors also provide detailed insights into the trade-offs between rollout length, sample efficiency, and feature representation, which are valuable for practitioners.
Strengths:
1. Novelty: The paper introduces CBMPI to Tetris, a domain where ADP methods have historically struggled, and demonstrates its effectiveness.
2. Experimental Rigor: The authors conduct comprehensive experiments, varying parameters, feature sets, and board sizes, and compare against state-of-the-art methods.
3. Significance: The results advance the state of the art in Tetris, providing a new benchmark and demonstrating the potential of policy-space search in ADP.
4. Clarity: The paper is well-organized, with clear explanations of algorithms, experimental setups, and results.
Weaknesses:
1. Generality: While the results in Tetris are impressive, the paper does not explore whether the findings generalize to other domains where policy space might also be easier to represent than value space.
2. Feature Dependence: The performance of CBMPI heavily relies on carefully selected features (e.g., Dellacherie-Thiery), raising questions about its applicability in domains without such well-engineered features.
3. Limited Discussion of Limitations: The paper could benefit from a more explicit discussion of the limitations of CBMPI, such as its reliance on rollout-based training and potential computational overhead in high-dimensional policy spaces.
Arguments for Acceptance:
- The paper addresses a challenging problem in reinforcement learning and ADP, achieving significant advancements in Tetris performance.
- It introduces a novel perspective on policy-space search in ADP, which could inspire further research in other domains.
- The experimental results are robust, well-documented, and reproducible.
Arguments Against Acceptance:
- The findings are domain-specific, with limited exploration of generalizability to other benchmarks.
- The reliance on pre-defined features may limit the applicability of the approach in less structured environments.
Recommendation: Accept. The paper makes a substantial contribution to the field by demonstrating the efficacy of CBMPI in Tetris and providing a new benchmark for future research. While its generalizability remains an open question, the results are significant and merit publication.