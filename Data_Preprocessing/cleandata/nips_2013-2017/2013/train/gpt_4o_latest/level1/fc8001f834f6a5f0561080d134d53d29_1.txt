This paper addresses the problem of approximate nearest neighbor (ANN) retrieval in both metric and non-metric spaces by enhancing the VP-tree with two novel learning-to-prune methods: density estimation through sampling and "stretching" of the triangle inequality. The authors evaluate their approach on datasets with Euclidean, KL-divergence, and Itakura-Saito distance functions, and compare it against state-of-the-art methods such as the bbtree, multi-probe LSH, and permutation-based techniques. The results indicate that the proposed VP-tree with a learned pruner is competitive and, in many cases, significantly faster for datasets with low or moderate intrinsic dimensionality. The paper also provides theoretical insights into the applicability of VP-trees to non-metric spaces, supported by a proof for KL-divergence.
Strengths:
1. Technical Contribution: The paper introduces two simple yet effective learning-to-prune methods, which are novel in the context of VP-trees. The use of piecewise linear approximations for pruning is particularly interesting and demonstrates a practical trade-off between efficiency and accuracy.
2. Thorough Evaluation: The experimental setup is robust, with comparisons across multiple datasets and distance functions. The authors provide detailed results on rank approximation quality, retrieval speed, and intrinsic dimensionality, offering a comprehensive performance analysis.
3. Theoretical Insights: The proof of applicability for VP-trees in non-metric spaces, particularly for KL-divergence, is a valuable theoretical contribution that extends the utility of VP-trees beyond traditional metric spaces.
4. Practical Improvements: The optimization of logarithmic computations for KL-divergence demonstrates attention to implementation details, yielding significant performance gains.
Weaknesses:
1. Limited Novelty in Learning-to-Prune: While the pruning methods are effective, they rely on relatively straightforward techniques (e.g., grid search for parameter tuning). More sophisticated learning approaches could have been explored.
2. Scalability Concerns: The paper does not explicitly address how the proposed methods scale with very large datasets or extremely high-dimensional spaces, which are common in modern ANN applications.
3. Clarity of Presentation: The paper is dense and could benefit from clearer organization, particularly in the experimental section. For example, the connection between theoretical results and empirical findings could be more explicitly stated.
4. Comparison Scope: While the paper compares against several state-of-the-art methods, it does not include recent advances in deep learning-based ANN methods, which could provide additional context for the significance of the results.
Arguments for Acceptance:
- The paper makes a meaningful contribution to ANN retrieval, particularly for non-metric spaces, an area with fewer established methods.
- The experimental results are compelling, showing clear advantages of the proposed approach for datasets with low or moderate intrinsic dimensionality.
- The theoretical proof of applicability to non-metric spaces strengthens the scientific rigor of the work.
Arguments Against Acceptance:
- The novelty of the pruning methods is somewhat incremental, and the paper does not explore more advanced learning techniques.
- The lack of scalability analysis and omission of comparisons to deep learning-based methods limit the broader applicability of the results.
Recommendation:
Overall, this paper provides a solid contribution to the field of ANN retrieval, particularly in non-metric spaces, and is well-suited for NIPS. While there are areas for improvement, the strengths outweigh the weaknesses. I recommend acceptance with minor revisions, particularly to improve clarity and expand the discussion on scalability and comparisons to newer methods.