This paper addresses the problem of estimating the cluster tree for a density \( f \) supported on or near a smooth \( d \)-dimensional manifold \( M \) embedded in a high-dimensional space \( \mathbb{R}^D \). The authors extend the work of Chaudhuri and Dasgupta (2010) by analyzing a modified k-nearest neighbor (k-NN) algorithm, demonstrating that the algorithm achieves convergence rates dependent only on the intrinsic dimension \( d \), not the ambient dimension \( D \). The paper also introduces a framework for clustering when data is concentrated near a manifold (rather than strictly on it) and provides a sample complexity lower bound for a class of manifold-oblivious clustering algorithms. These contributions are significant for high-dimensional clustering tasks, where the curse of dimensionality often poses challenges.
Strengths
1. Theoretical Contributions: The paper provides rigorous theoretical guarantees for the modified k-NN algorithm, showing consistency and convergence rates that depend only on the intrinsic dimension \( d \). This is a notable improvement over prior work, as it addresses the manifold hypothesis in clustering.
2. Novelty: The extension of Chaudhuri and Dasgupta's algorithm to manifold-supported densities is a meaningful advancement. The introduction of the framework for clustering with noise near manifolds is particularly innovative and broadens the applicability of the method.
3. Clarity of Results: The authors clearly state their main contributions, including the dependence of sample complexity on \( d \) and the construction of a lower bound instance. These results are well-motivated and supported by detailed proofs.
4. Relevance: The work is highly relevant to the NeurIPS community, as it addresses clustering in high-dimensional spacesâ€”a core challenge in machine learning and data analysis.
Weaknesses
1. Clarity and Accessibility: While the theoretical results are rigorous, the paper is dense and may be difficult for non-experts to follow. The authors could improve accessibility by providing more intuition behind key results and algorithms.
2. Empirical Validation: The paper lacks experimental results to validate the theoretical findings. Demonstrating the algorithm's performance on synthetic or real-world datasets would strengthen the paper and provide practical insights.
3. Assumptions: The manifold assumptions (e.g., bounded condition number, smoothness) may limit the applicability of the results in real-world scenarios where data often deviates from idealized conditions. The authors could discuss the robustness of their method under less restrictive assumptions.
4. Comparison to Related Work: While the paper references prior work extensively, it could benefit from a more explicit comparison of the proposed algorithm's performance (theoretical or empirical) against other state-of-the-art methods for manifold-based clustering.
Arguments for Acceptance
- The paper makes significant theoretical contributions, advancing the state of the art in manifold-based clustering.
- The focus on intrinsic dimensionality rather than ambient dimensionality is a crucial step forward for high-dimensional data analysis.
- The results are rigorous and well-supported by proofs.
Arguments Against Acceptance
- The lack of empirical validation limits the practical impact of the work.
- The paper's technical density may hinder its accessibility to a broader audience.
- The assumptions on the manifold may restrict the generalizability of the results.
Recommendation
I recommend acceptance of this paper, as its theoretical contributions are substantial and relevant to the NeurIPS community. However, the authors should consider adding empirical validation and improving the clarity of their exposition in the final version.