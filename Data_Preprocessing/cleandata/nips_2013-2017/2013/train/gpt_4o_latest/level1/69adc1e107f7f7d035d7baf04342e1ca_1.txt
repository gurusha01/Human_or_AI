Review of "Decision Jungles: Compact and Accurate Classifiers"
This paper introduces "decision jungles," a novel approach to classification that extends decision forests by replacing decision trees with rooted decision directed acyclic graphs (DAGs). The authors propose two new node-merging algorithms, LSearch and ClusterSearch, which jointly optimize the features and structure of the DAGs during training. The primary motivation is to address the exponential memory growth of decision trees with depth, which limits their applicability on memory-constrained devices like mobile or embedded processors. The authors demonstrate that decision jungles achieve significantly reduced memory usage while improving generalization compared to decision forests and other baselines. The paper evaluates the method on several datasets, including semantic image segmentation tasks and UCI classification datasets, showing that decision jungles achieve comparable or superior accuracy with a smaller memory footprint.
Strengths:
1. Quality: The paper is technically sound, with a clear formulation of the problem and well-motivated solutions. The proposed algorithms are rigorously described, and the experiments are comprehensive, covering a variety of datasets and baseline comparisons. The use of information gain as the objective function aligns with established practices in decision tree training, and the results are well-supported by quantitative and qualitative evidence.
2. Clarity: The paper is well-organized and clearly written. The authors provide detailed explanations of decision DAGs, their relationship to decision trees, and the optimization algorithms. The inclusion of visual examples (e.g., segmentation results) enhances understanding.
3. Originality: The concept of decision jungles is novel, particularly the integration of node merging during training to optimize both structure and features. While rooted DAGs have been explored in prior work, this paper uniquely focuses on their use for improving generalization and memory efficiency in ensemble methods.
4. Significance: The work addresses a practical and important problem in machine learningâ€”scaling decision tree-based models to memory-constrained environments. The demonstrated improvements in memory efficiency and generalization make this a valuable contribution to the field, particularly for applications in computer vision and embedded systems.
Weaknesses:
1. Limited Scope of Evaluation: While the paper evaluates decision jungles on several datasets, the focus is heavily skewed toward image segmentation tasks. Broader evaluation on diverse domains (e.g., text or time-series data) would strengthen the generalizability of the claims.
2. Optimization Complexity: The proposed optimization algorithms, particularly LSearch, appear computationally expensive, as acknowledged by the authors. While the results justify the additional cost, further discussion on scalability to very large datasets or ensembles would be helpful.
3. Comparison with Non-Tree-Based Methods: The paper primarily compares decision jungles to decision forests and their variants. Including comparisons with other compact classifiers (e.g., neural networks with pruning or quantization) would provide a more comprehensive evaluation of the method's advantages.
Arguments for Acceptance:
- The paper presents a novel and well-executed idea with clear practical relevance.
- The experimental results are compelling, showing significant improvements in memory efficiency and generalization.
- The writing is clear, and the contributions are well-situated within the context of prior work.
Arguments Against Acceptance:
- The evaluation could be broadened to include more diverse datasets and comparisons with non-tree-based methods.
- The computational cost of the proposed algorithms may limit their applicability in some scenarios.
Recommendation: Accept with minor revisions. The paper makes a strong contribution to the field, but addressing the above weaknesses would further enhance its impact.