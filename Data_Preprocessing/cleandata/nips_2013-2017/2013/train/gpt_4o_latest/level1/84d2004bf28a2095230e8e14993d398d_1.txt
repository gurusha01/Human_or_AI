Review of the Paper
Summary:  
The paper addresses the problem of submodular function maximization under cardinality constraints in a distributed setting, a critical challenge for large-scale machine learning tasks such as clustering and sparse Gaussian process inference. The authors propose GREEDI, a two-stage distributed algorithm implemented in a MapReduce framework. GREEDI approximates the centralized greedy solution by first running local greedy algorithms on data partitions and then merging the results for a second round of greedy selection. The paper provides theoretical guarantees on the performance of GREEDI, demonstrating that under certain conditions, its solution is competitive with the centralized approach. Extensive experiments on exemplar-based clustering and active set selection, including large-scale implementations on Hadoop, validate the method's scalability and effectiveness.
Strengths:  
1. Relevance and Impact: The paper tackles a significant problem in large-scale machine learning, where centralized algorithms are impractical due to data size. The proposed method is directly applicable to real-world scenarios, as evidenced by experiments on datasets with tens of millions of points.
2. Theoretical Analysis: The authors provide rigorous theoretical guarantees, including worst-case bounds and conditions under which GREEDI approaches the centralized solution. The analysis is well-grounded and extends to datasets with geometric structure and decomposable functions.
3. Scalability: The implementation of GREEDI in Hadoop and its application to massive datasets (e.g., 80 million Tiny Images) demonstrate the method's scalability and practical utility.
4. Experimental Validation: The experiments are comprehensive, comparing GREEDI against several baselines and showing consistent superiority in performance. The inclusion of both synthetic and real-world datasets strengthens the results.
5. Clarity of Contributions: The paper clearly delineates its contributions, including the algorithm, theoretical results, and experimental findings.
Weaknesses:  
1. Assumptions and Limitations: While the theoretical guarantees are robust, they rely on assumptions such as uniform random partitioning and Lipschitz continuity, which may not hold in all practical scenarios. The paper could benefit from a discussion of how sensitive GREEDI is to violations of these assumptions.
2. Comparison with Other Distributed Methods: Although the paper compares GREEDI to naive baselines, it does not benchmark against more sophisticated distributed submodular optimization methods (if any exist). This omission limits the scope of the evaluation.
3. Clarity of Presentation: While the paper is generally well-written, some sections, particularly the theoretical analysis, are dense and may be challenging for readers unfamiliar with submodular optimization. Additional intuition or visual aids could improve accessibility.
4. Communication Costs: The paper emphasizes GREEDI's low communication requirements but does not provide a detailed analysis of communication overhead compared to centralized or other distributed methods.
Arguments for Acceptance:  
- The paper addresses a critical and timely problem in large-scale machine learning.  
- It provides a novel, theoretically sound, and practically validated solution.  
- The experiments convincingly demonstrate the algorithm's effectiveness and scalability.  
Arguments Against Acceptance:  
- The paper lacks comparisons with more advanced distributed algorithms, if available.  
- Some theoretical assumptions may limit the generalizability of the results.  
Recommendation:  
I recommend acceptance of this paper, as it makes a significant contribution to distributed submodular optimization and demonstrates strong theoretical and empirical results. However, the authors should consider addressing the aforementioned weaknesses, particularly by discussing the sensitivity of GREEDI to its assumptions and providing a more detailed analysis of communication costs.