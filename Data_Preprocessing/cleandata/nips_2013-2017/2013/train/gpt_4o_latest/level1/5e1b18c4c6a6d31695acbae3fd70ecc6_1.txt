This paper presents a novel unsupervised feature selection method for text data based on the principle of Minimum Description Length (MDL) and a dictionary-based compression scheme. The authors formulate the problem as a binary optimization task, which they solve approximately using reweighted linear programs and the Alternating Directions Method of Multipliers (ADMM). The method identifies word k-grams that minimize the cost of lossless text reconstruction, resulting in a significantly reduced feature space (by two orders of magnitude) without sacrificing performance on supervised tasks like text categorization. The paper demonstrates the utility of the compressed features in both supervised and unsupervised tasks, achieving state-of-the-art classification accuracy on benchmark datasets (e.g., 20 Newsgroups and IMDb) while reducing training time and data requirements.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with a well-defined optimization framework and a clear connection to MDL principles. The iterative reweighting and ADMM-based solution is efficient and scalable, with linear complexity in document length for fixed k.
2. Significance: The method addresses a critical challenge in text processing—feature selection—by providing a compact, interpretable feature set that retains performance. The reduction in feature space and training time is particularly impactful for large-scale datasets.
3. Originality: The approach is novel in its explicit use of compression for feature selection, overcoming limitations of prior compression-based methods (e.g., instability of LZ77 due to document order). The integration of MDL principles into feature selection is a unique contribution.
4. Clarity: The paper is well-written and organized, with detailed explanations of the methodology, optimization algorithm, and experimental setup. The inclusion of comparisons with existing methods and ablation studies strengthens the paper's claims.
Weaknesses:
1. Generality: While the method is applicable to sequential data beyond text, the paper primarily focuses on text datasets. Additional experiments on other types of sequential data (e.g., clickstreams or biological sequences) would enhance the generality of the approach.
2. Evaluation Scope: The experiments focus on standard datasets and tasks (e.g., text categorization). Exploring more diverse applications, such as clustering or ranking, could better illustrate the versatility of the method.
3. Parameter Sensitivity: The paper briefly discusses the impact of the pointer cost parameter (λ), but a more thorough analysis of its sensitivity and guidelines for tuning it would benefit practitioners.
4. Comparative Baselines: While the method is compared to full k-gram models and LZ77, additional comparisons with other state-of-the-art unsupervised feature selection methods would provide a more comprehensive evaluation.
Recommendation:
I recommend acceptance of this paper. Its contributions are significant, particularly in the context of unsupervised feature selection for text data. The method is innovative, technically sound, and demonstrates strong empirical performance. While there are areas for improvement, such as broader evaluation and parameter analysis, these do not detract from the paper's overall quality and impact.
Arguments for Acceptance:
- Novel and principled approach to unsupervised feature selection.
- Strong empirical results with significant reductions in feature space and training time.
- Clear and reproducible methodology with detailed algorithmic insights.
Arguments Against Acceptance:
- Limited exploration of non-text datasets and tasks.
- Lack of comparisons with other unsupervised feature selection methods.
Overall, the paper makes a valuable contribution to the field and aligns well with the scope and standards of NIPS.