This paper addresses the challenging problem of learning dynamic models, specifically first-order Markov models and hidden Markov models (HMMs), from non-sequential data. Unlike traditional approaches that rely on Expectation-Maximization (EM) and non-convex optimization, the authors propose a novel method based on moment matching and tensor decomposition. They provide formal guarantees for parameter recovery under reasonable assumptions about the generative process, making this the first work to offer such theoretical assurances for learning from non-sequence data. Simulation results further validate the proposed methods, demonstrating their effectiveness in recovering model parameters.
Strengths:
1. Novelty and Originality: The paper tackles a relatively unexplored problem—learning dynamic models from non-sequential data—and introduces a fresh perspective using spectral learning and tensor decomposition. This approach is a significant departure from traditional EM-based methods, which are harder to analyze and lack formal guarantees.
2. Theoretical Contributions: The authors rigorously derive conditions under which their methods can recover model parameters and provide formal guarantees for both first-order Markov models and HMMs. This is a notable advancement in the field.
3. Clarity and Organization: The paper is well-structured, with clear explanations of the problem, assumptions, and methodology. The connection to related work, such as Latent Dirichlet Allocation (LDA) and spectral learning for latent variable models, is well-articulated.
4. Simulation Results: The simulation results are compelling, showing convergence of the proposed methods and validating the theoretical findings. The heuristic for estimating the success probability \( r \) is particularly interesting and practical.
Weaknesses:
1. Assumptions: The proposed method relies on several assumptions, such as the Dirichlet distribution for initial state distributions and the geometric distribution for time steps. While these are reasonable in some contexts, their applicability to real-world data might be limited.
2. Sample Complexity: The sample complexity analysis reveals a high dependency on parameters such as the number of states \( m \), hidden states \( k \), and precision \( \epsilon \). This could limit the scalability of the method to large-scale problems.
3. Practicality: While the theoretical guarantees are strong, the paper lacks a demonstration of the method on real-world datasets. This would have strengthened the case for its practical utility.
4. Estimation of \( r \): The heuristic for estimating \( r \) is not formally analyzed, and its robustness in different settings remains unclear.
Arguments for Acceptance:
- The paper addresses a novel and important problem with significant theoretical contributions.
- The proposed method is innovative and well-supported by rigorous analysis.
- The simulation results are promising and align with the theoretical guarantees.
Arguments Against Acceptance:
- The reliance on specific assumptions may limit the generalizability of the method.
- The lack of real-world experiments makes it difficult to assess the practical impact.
- The high sample complexity could be a bottleneck for large-scale applications.
Recommendation:
Overall, this paper makes a strong theoretical contribution to the field of machine learning and dynamic model learning. While there are some limitations in terms of practicality and assumptions, the novelty and rigor of the work make it a valuable addition to the conference. I recommend acceptance, with a suggestion to include real-world experiments and further discussion on relaxing the assumptions in future work.