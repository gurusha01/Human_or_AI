Paraphrased Review:
Additional Comments Post-Rebuttal:
The authors need to significantly improve the motivation for the problem in the context of machine learning to make the paper more accessible to a broader audience. They should explicitly present the latent variable model (along with a graphical model representation) where this tensor decomposition approach can be used to learn the parameters. For instance, consider a model where latent variable \( h1 \) connects to observed variable \( x1 \), \( h2 \) to \( x2 \), and so forth. Assume a linear relationship between the hidden and observed variables, with arbitrary dependencies among the \( hi \)'s. In such a scenario, the observed moment tensor has a Tucker decomposition, and its multilinear rank corresponds to \( \text{dim}(h1), \text{dim}(h_2), \ldots \). The authors should incorporate this discussion and ideally include a diagram illustrating this model. Additionally, they should highlight the possibility that only some hidden variables may have small dimensions while others do not, which is a key advantage of the new results presented in the paper.
The paper focuses on convex-optimization-based tensor decomposition using structured Schatten norms for tensors. The authors utilize the overlapped Schatten norm and demonstrate how it improves reconstruction guarantees. The paper is well-written and offers technically significant contributions. Addressing a few technical and notational issues would enhance its clarity and readability.
Clarifications Regarding Theorem 2 and Its Proof:
- The statement of Theorem 2 is not entirely accurate: the components are only recovered up to a permutation. While this is a minor issue, it should be explicitly mentioned.
- In Theorem 2, the claim that (12) directly leads to (13) requires further justification.
- The proof of Lemma 5 is missing, even though it plays a critical role in Theorem 2. This proof should be included.
- While the proof of Lemma 4 is straightforward, it would be better to provide an explicit explanation rather than referring to another paper.
Minor Issues:
- The authors should include additional related work on HOSVD and other tensor decomposition methods. For example, they could discuss how higher-order SVD and its variants provide approximation bounds for tensor decomposition. Refer to Kolda's review on tensor decompositions (Section 4).
- The term "mode-rank" used by the authors is also known as "multilinear rank." This should be clarified in the introduction, along with a note about the existence of different notions of tensor rank.
- A brief discussion of the computational complexity of the proposed methods would be beneficial.
- The discussion on incoherence after (10) needs clarification. The authors describe incoherence as requiring the "energy" to be distributed across different mixture components and modes, such that no component or mode has excessively high singular values. However, they compare this to the incoherence conditions in [1,3,10], which is misleading. In [1,3,10], incoherence pertains to sparse+low-rank recovery, where the singular vectors are incoherent with the basis vectors. This is distinct from the condition in this paper. It is recommended to remove this comparison.
Typos/Notational Issues:
- The superscript notation for components \( \hat{\mathcal{W}}^{(k)} \) is unclear. Consider using a different symbol for these components.
- The notations \( \bar{r}k \) and \( \underline{r}k \) are both used to represent the k-mode rank, which is confusing. It would be better to introduce a unified notation, such as \( \text{mult\_rank}(\text{tensor}) \), to denote the multilinear rank.
- On the last line of page 12 in the supplementary material, a parenthesis is missing.
- In the abstract, "result for structures Schatten" should be corrected to "result for structured Schatten."
Overall, the paper is well-written and makes a novel contribution by proposing a Tucker tensor decomposition approach using an overlapped Schatten norm, along with guarantees for recovery. However, several technical clarifications are needed, which I hope the authors will address.