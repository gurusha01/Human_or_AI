The paper conducts an empirical evaluation of classification-based policy iteration (CBPI) on the Tetris benchmark problem. While it does not present any novel technical contributions, the authors focus on analyzing how various algorithmic parameters influence performance.
The findings indicate that direct policy iteration (DPI), a variant of CBPI that bypasses value functions, performs comparably to both the cross-entropy method and classification-based policy iteration. This result is not particularly surprising, as DPI shares significant similarities with the cross-entropy method in terms of policy representation and optimization.
The analysis and comparisons are convincing in demonstrating that the standard Tetris features are inadequate for representing value functions optimized through approximate policy iteration. However, the paper falls short of explaining why this is the case or offering guidance on designing better features. This limitation reduces the broader significance of the results and their applicability to other domains. Furthermore, the paper would benefit from including comparisons with other algorithms that compute approximate value functions using methods distinct from policy iteration. As presented, the results suggest that policy iteration struggles to compute effective value functions, but this does not preclude other algorithms from succeeding. For instance, Smoothed Approximate Linear Programming [Desai, Farias, et al., 2012] has demonstrated moderately promising results on this domain and should be considered in the evaluation.
Overall, the paper provides a thorough evaluation of several prominent algorithms on Tetris, a key benchmark problem in reinforcement learning. While the results highlight that classification-based policy iteration outperforms DPI, the lack of deeper insights into the underlying reasons for the differences in solution quality limits the generalizability and practical relevance of the findings to other domains.