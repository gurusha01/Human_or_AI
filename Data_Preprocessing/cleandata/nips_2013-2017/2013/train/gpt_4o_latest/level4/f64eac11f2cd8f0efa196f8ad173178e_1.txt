This paper introduces a theoretically grounded adaptive step size for policy gradient methods, aiming to maximize the lower bound of the expected policy improvement. The approximated step size can be computed using only the estimated gradient, ensuring that the computational complexity remains comparable to standard policy gradient methods. A mild assumption is made that the standard deviation of the Gaussian policy is fixed to derive the lower bound. Experimental results demonstrate that the proposed method performs effectively when the fixed standard deviation is sufficiently large.
Quality, clarity, and originality:  
The paper clearly articulates its motivation and is well-organized. It offers potentially valuable theoretical contributions, such as Theorem 4.3 and Corollary 4.4, though I was unable to verify the detailed derivations of the lemmas and theorems. The originality of the work is unquestionably high.
Significance:  
While the Newton method is a standard approach for adaptive step size, the second-order derivative of the expected return under a Gaussian policy is not guaranteed to be semi-positive definite. Therefore, the proposed adaptive step size, based on maximizing the lower bound, serves as a promising alternative to conventional policy gradient methods. However, as noted in the introduction, there are several existing extensions of policy gradients aimed at achieving stable policy updates, such as EM policy search. Including more experimental comparisons with these methods would strengthen the paper's claims and enhance its impact.
Minor comments and questions:  
- A conclusion section should be added.  
- Is it truly feasible to converge to the optimal policy with just one update for \(\sigma = 5\) (as shown in Table 1)?  
The paper provides valuable theoretical insights and a promising alternative to standard policy gradient methods. However, its contributions would be more compelling if additional experimental comparisons with state-of-the-art methods, such as EM policy search, were included.