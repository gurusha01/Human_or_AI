The paper introduces a "hybrid" deterministic-stochastic approach for first-order optimization, combining queries to a deterministic oracle (full gradients) and a stochastic oracle (stochastic gradients). The authors demonstrate that this method eliminates the dependency on the condition number in the convergence rate.
The authors present a compelling and noteworthy theoretical result. Specifically, under the assumption that the condition number is known (i.e., both the strong convexity and strong smoothness constants of the objective are provided) and that a first-order hybrid oracle (capable of querying both stochastic and deterministic gradients) is available, it is shown that a carefully chosen combination of deterministic and stochastic gradient steps achieves, with high probability, an \(O(\log(1/\epsilon))\) convergence rate for batch optimization.
However, there are several issues with the current state of the paper. First, the theoretical analysis is limited to the scenario where the condition number is perfectly known in advance, and it does not address the algorithm's behavior when this hyper-parameter is misspecified. Essentially, the result states that the dependence on the condition number (\(\kappa\)) can be removed from the convergence rate of a first-order optimization algorithm, provided that \(\kappa\) is known beforehand. If extending the theoretical analysis to cases where \(\kappa\) is unknown is challenging, this gap could have been addressed through experiments. Unfortunately, the absence of an experimental section is the second major concern. Given that the paper's primary contribution is a new algorithm, it would be reasonable to include experiments to validate the theoretical results and evaluate the algorithm's practical performance on empirical data.
Detailed Comments
The proposed algorithm (EGD) is based on the update rule specified in Eq. 8 (page 4), which uses the "mixed gradient" defined in Eq. 7. Consequently, EGD requires \(\eta\) as a hyper-parameter, which depends on the condition number \(\kappa\) (referred to as the "conditional number" in the paper), encompassing both the "strong convexity modulus" \(\lambda\) and the "strong smoothness modulus" \(L\). As far as I understand, the authors do not provide any guidance or theoretical justification for setting \(\kappa\) a priori.
This leads to two possibilities: either this parameter must be estimated, in which case the paper lacks an estimation procedure (even a heuristic approach supported by numerical experiments would suffice), or the parameter is assumed to be known, as the paper is primarily theoretical. In the latter case, the theoretical analysis or experimental section should address the algorithm's performance when this hyper-parameter is misspecified. This would involve studying the convergence rate when the hyper-parameter is set too high or too low.
While the smooth and strongly convex case is a common assumption in theoretical analysis and is realistic in many scenarios, it can also be overly restrictive. Other settings, such as the non-strongly convex case, are equally important and arise in various contexts. For example, Bach & Moulines (2011) provide a theoretical analysis of different behaviors in convex versus strongly convex settings. Extending the analysis to non-strongly convex cases would enhance the paper's scope.
There are additional concerns. The idea of blending deterministic and stochastic gradient steps in a first-order optimization algorithm is not novel. However, the algorithm in the paper is not explicitly formulated as an alternation between deterministic and stochastic gradient steps with varying frequencies. Instead, it is presented as a single "mixed" update per iteration (within an epoch), followed by a gradient-like update step. It would be valuable to discuss how the proposed algorithm compares to a similar approach that alternates between deterministic and stochastic gradient steps, particularly in the unconstrained case.
The paper also overlooks a related body of work on hybrid deterministic-stochastic optimization algorithms. For instance, the authors could discuss and compare their method's convergence rates with those in [Hybrid Deterministic-Stochastic Methods for Data Fitting, M. Friedlander, M. Schmidt, SISC, 2012]. This reference also provides a review of earlier works on this topic, which would enrich the discussion.
Finally, a comprehensive experimental evaluation would significantly strengthen the paper. Such a study could include comparisons with standard methods like SGD, averaged SGD, and recent advancements in stochastic first-order optimization (e.g., SAG). This would provide empirical evidence for the theoretical claims and demonstrate the algorithm's practical utility.
A minor issue is that the optimization setting considered in the paper is not clearly defined. The goal appears to be achieving the best of both worlds: exponential convergence rates from deterministic optimization and reduced dependence on the condition number from stochastic optimization. However, the authors do not explicitly state whether they aim to solve the deterministic optimization problem \(\minw F(w) = \frac{1}{n} \sum{i=1}^n Fi(w)\) using a "stochastic" (or more accurately, "randomized") algorithm, or the stochastic approximation problem \(\minw \mathbb{E}[F(w)]\). Notably, algorithms like SAG and SDCA are randomized methods designed for the deterministic optimization problem \(\minw F(w) = \frac{1}{n} \sum{i=1}^n F_i(w)\). The theoretical setup in Section 3.1 is ambiguous in this regard, and none of the subsequent claims clarify the intended setting. This ambiguity could be easily resolved with a clearer statement of the problem.