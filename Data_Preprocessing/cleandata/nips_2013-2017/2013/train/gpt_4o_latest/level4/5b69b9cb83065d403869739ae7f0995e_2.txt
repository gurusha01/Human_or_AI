Review of "Low-rank matrix reconstruction and clustering"
This paper introduces a novel algorithm for low-rank matrix reconstruction, leveraging Belief Propagation (BP) message-passing within a Bayesian framework for the reconstruction problem. The algorithm, detailed in the "Supplementary Material," incorporates two simplifying approximations, each assuming a large number of rows or columns in the input matrix. The evaluation is conducted in an innovative way by reframing clustering as a matrix reconstruction problem, comparing the proposed method against Lloyd's K-means algorithm and Variational Bayes Matrix Factorization (VBMF), which appears to be the only prior message-passing-based reconstruction algorithm.
Cons
There are several reasons to question the acceptance of this paper. Since the algorithm is evaluated on a non-standard problem (clustering reformulated as matrix factorization), it is challenging to extrapolate the results to conventional matrix reconstruction tasks. For example, the two cited references for VBMF—Lin and Teh (2007) and Raiko, Ilin, and Karhunen (2007)—evaluate their algorithms on the Netflix dataset. It would have been preferable if the authors had used the same dataset for comparability. While BP is generally more accurate than Variational methods, the use of a novel evaluation criterion raises concerns about the algorithm's true competitiveness. If datasets like Netflix are unavailable, the authors should explicitly justify this in the paper.
The algorithm itself represents a relatively straightforward application of BP to a problem previously tackled using Variational Message Passing. While novel, it is not particularly groundbreaking. The most intriguing aspect is the approximations introduced in the limits \(N \to \infty\) and \(m \to \infty\), where \(m \times N\) represents the dimensions of the input matrix. However, the validity of these approximations, described only in the supplementary material, is not directly tested, and their explanation could be clearer.
The paper also has significant issues with citation of prior work. Initially, the abstract and Section 2.2 give the impression that the application of matrix factorization to clustering is an original contribution. However, other reviewers pointed out that this connection is well-established. The paper should make this clear to readers. If the connection is too well-known to warrant specific citations, the authors should explicitly state this. Otherwise, prior work should be cited.
Additionally, Expectation Propagation (EP) should be cited, as it is commonly associated with applying BP to continuous distributions. The relationship between EP and the proposed algorithm should also be clarified. Furthermore, the algorithm is most naturally understood as an application of BP, but this is not mentioned until Section 4.1. This key fact should be highlighted in the abstract.
Pros
The paper is engaging and presents a new algorithm with potential utility. The mathematics is generally comprehensible, and while I did not verify the derivations in detail, they appear replicable. Although the use of a novel evaluation criterion invites skepticism, the K-means problem may be sufficiently general to provide a fair comparison of algorithms. The experiments convincingly demonstrate the benefits of the proposed method.
Clarity
The paper's introduction and summary of prior work are clear and accessible. However, relegating the algorithm's derivation to the supplementary material is a drawback. A brief outline of the derivation should be included in the main paper. The experiments section is well-written, though some plots are difficult to interpret in black and white.
The algorithm's description is challenging to follow, and the derivation's correctness is difficult to verify. The mathematical presentation is the weakest aspect of the paper. For example, the factors of \(1 / m \tau_w\) in equations 9a, 9b, 9d, and 9e could be factored out to improve readability. Similarly, the last two terms in 9b and 9e could be combined.
The notation is cumbersome, with an overabundance of hats, tildes, subscripts, and superscripts. For instance, \(\tau_w\) could simply be \(\tau\), and the hats in Section 4 could be dropped with a brief explanation. The 't' superscripts in the messages seem unnecessary; replacing "=" with "\(\gets\)" would eliminate the need for these indices. Simplifying the notation would make the algorithm more accessible.
The exposition could also be improved. For example, it should be noted that equation 5 is the negative logarithm of equation 2, and that equation 8 is equation 2 raised to the power of \(\beta\). Algorithm 2 is nearly row-column symmetrical, and this could be explicitly stated to reduce redundancy. The functions in equation 10 should be explained near their definition. For instance, instead of providing an equation for \(f\beta\), it would be clearer to state that it represents the mean of \(q\beta\). Similarly, the scaled covariance matrix \(G_\beta(b, \Lambda; p)\) should be explained earlier.
The parameter \(\beta\) is inconsistently propagated through the notation. While \(f\) and \(G\) include \(\beta\) subscripts, the messages that depend on them do not. Since \(\beta\) is a global variable, its explicit inclusion in subscripts could be eliminated for clarity. Before equations (15) and (16), the authors could simply state, "In the limit \(\beta \to \infty\), \(f\) and \(G\) take the following form."
On a deeper level, certain aspects of the algorithm remain unclear. For example, what is the significance of the \(m\) factor in the additive noise variance? Does it influence applications of matrix factorization or the approximations used in the derivation? It also appears to be the only factor disrupting row-column symmetry in the algorithm. These points should be addressed in the paper.
Other Questions
- Is it standard to use a tilde to denote column vectors? This was initially unclear.
- Why is \(N\) capitalized while \(m\) is lowercase?
- In Section 5, a citation should be provided for VBMF-MA to clarify the primary reference for its implementation. The two earlier citations for variational matrix factorization make it unclear which one is relevant (or if both are).
Supplementary Material
In equation 5, it would be clearer to write the two terms involving \(z\) as a product of exponentials to emphasize that one is the probability density function. In Section 2, the step from equation 14 to 15 regarding big-\(O\) terms of \(m\) is unclear and could use further explanation. Additionally, the novelty of the algorithm's derivation should be clarified in both the supplementary material and the main paper.
The English is generally strong, but there are occasional issues with article usage. For example, the title could be revised to "Low-rank matrix reconstruction and clustering using an approximate message passing algorithm." Below are some additional suggestions for improving the language:
Page 1  
- "Since properties" → "Since the properties"  
- "according to problems" → "according to the problem"  
- "has flexibility" → "has enough flexibility"  
- "motivate use of" → "motivate the use of"  
Page 2  
- "We present results" → "We present the results"  
Page 3  
- "We regard that" → "We see that"  
- "maximum accuracy clustering" could be italicized  
- "Previous works" → "Previous work"  
Page 4  
- "particularized" → "specialized"  
Page 5  
- "plausible properties" → "discernible properties"?  
Page 6  
- "stuck to bad local minima" → "stuck in bad local minima"?  
Page 7  
- "This is contrastive to that" → "This is in contrast to the fact that"  
In the supplementary material:  
- "of these message" → "of these messages"  
- "a few number" → "a few"  
The diagram in Figure 1 of the supplementary material is helpful. Including additional diagrams in the main paper would enhance clarity.
Conclusion
The mathematics and algorithm are interesting, and the paper represents the first application of belief propagation to matrix factorization, filling an important gap in the literature. However, the paper requires significant improvements in clarity, notation, and scholarship before it can be accepted. In its current form, it places an undue burden on readers, which would reflect poorly on the conference if accepted.