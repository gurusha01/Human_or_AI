This paper introduces a novel policy-gradient reinforcement learning approach where the step size is adaptively determined by maximizing a lower bound on the expected performance gain. Initially, the authors examine the scenario where the policy gradient is known and derive a lower bound on the performance difference between the original policy and the updated policy obtained by following the policy gradient. Given that this bound is a fourth-order polynomial, the authors then focus on a specific case where the stochastic policy is modeled as a Gaussian distribution with a fixed standard deviation and a mean that is a linear combination of state features. In this scenario, they derive a quadratic lower bound on the step size, which exhibits a single maximum for positive step sizes. Subsequently, the authors relax the assumption of a known policy gradient and establish a high-probability lower bound based on an epsilon-optimal estimate of the policy gradient. They further demonstrate how existing techniques, such as REINFORCE and G(POMDP)/PGT, can be employed to estimate the policy gradient, resulting in an adaptive step size policy-gradient method that operates without requiring a model of the environment and learns solely from sample trajectories. The proposed approach is evaluated through a simple simulation experiment on a 1-dimensional toy problem.
Overall, this is a well-crafted and high-quality paper. The authors convincingly argue that much of the existing research on policy-gradient methods has concentrated on improving gradient estimators, leaving significant potential for advancements in automating step size selection. The theoretical contributions of the paper are notable, as they provide a rigorous framework for automatically determining step sizes by maximizing lower bounds on expected performance gain. Beyond presenting a specific algorithm, the paper offers general theoretical insights that could inform the development of algorithms with alternative policy representations and gradient estimators.
The primary limitation of the paper lies in its empirical evaluation, which is restricted to a toy problem involving the optimization of a single policy parameter. The results using the true gradient illustrate that manual step size choices can lead to slow learning (if too small) or divergence (if too large), whereas the adaptive method consistently converges within the specified threshold. For Gaussian policies with large standard deviations, the adaptive method outperforms all tested fixed step sizes. However, for small standard deviations, its performance is inferior, which the authors attribute to a trade-off between policy determinism and the tightness of the lower bound.
The authors also present results for scenarios where the true gradient is unknown, using the REINFORCE and PGT estimators. However, it is unclear what these results substantively demonstrate beyond the expected observation that increasing the number of trajectories improves gradient estimation and learning performance. More concerningly, the results reveal that even with many trajectories, the gradient estimation errors remain substantial, resulting in loose bounds that hinder strong performance. This raises questions about the practical applicability of the proposed method and whether its contributions are primarily theoretical.
Additionally, the empirical evaluation lacks comparisons with other baseline methods. Notably, the EM-based policy-gradient-like methods by Kobers & Peters and Vlassis et al., mentioned in the introduction, appear to be particularly relevant baselines, as they also avoid the need for tuning free parameters.
In summary, this is a well-written paper with a significant theoretical contribution. However, the empirical evaluation is limited and produces mixed results for the proposed method.