DETAILED COMMENTS:
This paper investigates the use of control variates to mitigate the variance in stochastic gradient descent algorithms, addressing a critical issue for all stochastic approximation methods. While the concept of control variates is not novel in the machine learning community (see, for instance, "John Paisley, David M. Blei, and Michael I. Jordan, Variational Bayesian Inference with Stochastic Search, in: Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012"), the authors extend its application to widely-used models such as LDA and NMF. Additionally, the paper includes experimental results in the context of logistic regression and LDA.
The use of control variates in logistic regression and variational inference was previously explored in [Paisley'2012], albeit with differences in implementation compared to this work. The authors should cite this reference, clearly articulate the contributions and novelties of their approach, and provide a comparison with the control variates employed in that study.
In the logistic regression experiments, the comparison between variance reduction and standard SGD is conducted using the SAME fixed step size for both methods. This approach appears questionable, as a fair comparison should involve selecting the optimal step sizes independently for variance reduction and standard SGD.
What is the computational overhead introduced by employing control variates? Since the paper does not include a computational complexity analysis, it would be beneficial to provide comparisons in terms of CPU time or wall-clock time.
In summary, while this paper explores the use of control variates to reduce variance in stochastic gradient descent, the novelty of the proposed method needs to be clarified. Furthermore, the justification for using fixed step sizes in the logistic regression experiments requires further explanation.