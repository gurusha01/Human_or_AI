This paper investigates minimax bounds for probability estimation under the constraint that individual data privacy must be preserved. The authors adopt a privacy definition known as local privacy. They analyze two probability estimation problems: multinomial estimation and density estimation. For both problems, the authors establish sharp minimax rates of convergence. Specifically, for the discrete multinomial estimation problem, they show that local privacy reduces the effective sample size quadratically in the privacy parameter α. Since α is often treated as a small constant, the effective sample size remains comparable to the non-private case. For the density estimation problem, the authors demonstrate that the optimal rate achievable in the non-private setting is no longer attainable when local privacy constraints are imposed.
Overall, the results are compelling, and the proofs appear to be correct based on my review. To the best of my knowledge, the minimax bound for density estimation under privacy constraints has not been explored in prior literature.
My primary concern is that the minimax bound for multinomial estimation is closely related to prior work on noise complexity in differential privacy, yet these connections are not adequately discussed. Specifically, two highly relevant papers are Hardt & Talwar, On the Geometry of Differential Privacy (STOC, 2010), and De, Lower Bounds in Differential Privacy (TCC, 2012). These works address worst-case lower bounds for the error of linear queries under differential privacy constraints. The probability estimation problem in this paper can be viewed as a special case of the linear counting query studied in those papers, with both using the same L₂ error metric. The key distinction is that this paper focuses on local differential privacy, which imposes stricter constraints than differential privacy. Consequently, lower bounds for differential privacy also apply to local privacy. I am curious whether the bounds for local privacy derived in this paper improve upon the existing bounds for differential privacy.
For the density estimation problem, the presentation of the results is somewhat unclear. The authors claim that the lower bound in the local privacy setting is higher than in the non-private setting. However, the minimax bound in this paper pertains to the specific case where the density can be expanded using a trigonometric basis. I am uncertain whether the lower bound for the non-private setting (Equation 13) applies to the general Sobolev space defined in Definition 1 or is restricted to the trigonometric basis case. A fair comparison requires that the non-private lower bound also holds for the trigonometric basis.
Additional comments to the rebuttal:
The authors' response partially clarifies the connection to prior worst-case lower bounds. However, I would like to point out that the work by Nikolov, Talwar, and Zhang, The Geometry of Differential Privacy: The Sparse and Approximate Cases (STOC, 2013), also considers Mean Square Error and is relevant to this discussion. I recommend that the authors incorporate explanations and references to these missing works in the paper.
In summary, this paper establishes sharp minimax bounds for probability mass function (pmf) and probability density function (pdf) estimation under privacy guarantees. The results are interesting, and the paper is well-written. However, the lack of discussion regarding known results on noise complexity lower bounds for differential privacy, which are highly relevant to this work, is a notable omission.