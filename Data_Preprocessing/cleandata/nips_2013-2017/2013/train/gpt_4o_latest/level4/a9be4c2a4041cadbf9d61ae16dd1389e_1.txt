The paper tackles the problem of training a two-layer network for classification with latent variables. The authors introduce a convex SDP relaxation for the originally non-convex training problem and propose an approximate optimization algorithm to solve the resulting SDP formulation. Proof-of-concept experiments demonstrate encouraging results, showing that the proposed algorithm outperforms both globally optimized single-layer models and the same two-layer model trained using local alternating minimization.
The proposed reformulation enabling the SDP relaxation is both novel and interesting. Overall, the paper is reasonably clear, although some sections of the text are dense. The work appears to be technically sound.
The primary weakness lies in the complexity of the resulting SDP problem (21). The authors should discuss the basic properties of their proposed optimization algorithm for solving (21), such as the computational time required for the benchmark problems and whether the algorithm yields an exact solution (e.g., the stopping condition used). This information is crucial because a convex problem is not necessarily easy to solve in practice; a convex relaxation can still be computationally intractable, and it is important to clarify whether this is the case here. Nevertheless, even if the relaxation is computationally challenging, it remains a valuable contribution.
Minor comments:  
- Equation (17): A variable is missing below the first minimum.  
- Equation (18): It seems that the condition \( N = \Phi' \Phi \) should be included in the set definition.  
- Line 331: Typo in "(d) Synhetic results."  
- Line 356: It is unclear why the transductive evaluation used does not require computing the network responses, and thus knowledge of \( W \) and \( V \).  
- The paper does not specify the actual size of the problem instances for (21) solved in the experiments.  
The proposed SDP relaxation represents an intriguing approach to better approximate an important class of non-convex training problems. While the current algorithm may not yet be practical, it has the potential to inspire the development of more efficient methods in the future.