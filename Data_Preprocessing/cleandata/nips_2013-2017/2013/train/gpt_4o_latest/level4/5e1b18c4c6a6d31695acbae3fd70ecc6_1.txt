The method involves compressing documents to expedite their processing in tasks such as classification. The coding scheme is inspired by the Lempel-Ziv algorithm, utilizing pointers to substrings that appear multiple times within the document. The proposed technique is framed as a combinatorial optimization problem, which is approximately solved through a sequence of convex problems tackled using the ADMM framework. 
Experiments are conducted on text classification tasks, demonstrating that document compression yields improvements in memory usage and computational efficiency, albeit with a slight trade-off in precision.
I found the approach to be intriguing, though I lack sufficient familiarity with the NLP literature to fully assess the novelty of the contribution. I have only a few minor comments:  
- The phrase "an optimal lossless compression of D..." requires clarification. Is the coding scheme optimal in the sense of achieving minimum entropy?  
- It is unclear whether the reweighting scheme can genuinely be interpreted as a majorization-minimization procedure. Could the authors confirm if this interpretation holds in this context?  
- Minimizing equation (4) with respect to w corresponds to computing the Fenchel conjugate of the (weighted) linfty-norm, which involves projecting onto a weighted l1-norm (the dual norm of the linfty-norm). When incorporating the non-negativity constraint, this translates to projecting onto the simplex. Efficient algorithms for simplex projection have been well-established and bear resemblance to the method described in the paper. For reference, see:  
  - Brucker, "An O(n) Algorithm for Quadratic Knapsack Problems," 1984.  
  - Bach et al., "Optimization with Sparsity-Inducing Penalties," 2012, which discusses the computation of Fenchel conjugates for norms.  
Overall, the paper introduces a method for document compression aimed at accelerating document processing tasks. The approach appears to be novel and demonstrates promising performance.