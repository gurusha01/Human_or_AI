The paper proposes a convex relaxation for a two-layer neural network. It is well-written, and the experiments demonstrate good performance on "real" datasets. However, my primary concern lies in the scalability of the proposed approach.
The use of SDP for convex relaxation has been extensively studied and was widely applied over a decade ago, so there is little novelty in this aspect. While the formulation is elegant, scalability remains a significant limitation for this type of relaxation. Specifically, the optimization problem involves t^2 variables, where t is the size of the instances, making it likely only practical for toy datasets. To address the scalability concern, it would be beneficial to include a comparison of training times across different algorithms.
Algorithm 2 leverages the low rank of N, but there is no guarantee that N will consistently have a small rank.
For the synthetic experiments, an RBF SVM could likely achieve competitive performance. A more appropriate comparison would involve using the Nystrom approximation to RBF SVM with randomly selected bases, rather than a one-layer linear SVM.
Overall, the paper is well-written, but the use of SDP convex relaxation lacks novelty.