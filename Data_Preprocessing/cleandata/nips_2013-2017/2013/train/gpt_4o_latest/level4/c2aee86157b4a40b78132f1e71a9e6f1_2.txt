This paper introduces a determinized sparse partially observable tree for online POMDP planning. To achieve this, the authors derive an output-sensitive performance bound, demonstrating that the proposed online approach performs "well" when a simple policy exists. The approach itself involves searching a sparsely sampled belief tree to identify a policy that optimally balances policy size and the accuracy of its value estimate obtained through sampling.  
 
The experimental results indicate that the proposed algorithm scales better than existing online methods.  
However, this work appears incremental and is unlikely to have significant impact, despite being technically sound and well-executed. From the perspective of originality, it primarily represents a technical attempt to create an algorithm that scales better, but this attempt has notable limitations:  
(1) The proposed regularization does not seem to provide benefits for the RockSample(n,k) problem, even though this domain is largely deterministic.  
(2) The initial lower bound, as suggested by the authors, does not meet the requirements for Theorem 1 to hold. It is unclear what practical implications this has.  
(3) The algorithm depends on tree search, which is known to be highly sensitive to the branching factor (i.e., the number of observations). This sensitivity is not addressed either theoretically or experimentally. While the authors state that "Theorem 1 indicates that R-DESPOT may still perform well when the observation space is large," the phrase "MAY STILL" is too vague. A more formal and practical analysis regarding the observation space is necessary.  
Additionally, the content of Table 1 is unclear. If it represents "running time," it is puzzling why some results are shown as -6.03, -6.19, etc.  
The correct reference for AEMS is:  
ROSS, St√©phane, and CHAIB-DRAA, Brahim. AEMS: An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs. In: IJCAI. 2007. p. 2592-2598.  
Reference [2], on the other hand, is a general overview that includes many comparisons of online POMDP approaches.  
The authors frequently state that "the algorithm works well if a simple policy exists." However, the term "simple policy" is not clearly defined. What constitutes a "simple policy"? What should be done if no such policy exists? Should expert input be sought in such cases?  
Overall, this work represents an incremental contribution to online POMDP planning. The branching factor is not adequately addressed, and the experimental results are unconvincing.