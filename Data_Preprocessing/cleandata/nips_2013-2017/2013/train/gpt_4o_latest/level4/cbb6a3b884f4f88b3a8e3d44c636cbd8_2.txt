This paper examines a fundamental tradeoff in hierarchical classification: in deeper hierarchies, classifiers must make decisions at multiple levels, which increases the likelihood of error propagation. Conversely, in flatter hierarchies, fewer decisions are required, reducing the potential for error propagation, but the decisions themselves become more challenging due to the higher cardinality of the decision space.
The authors propose a data-dependent generalization error bound for kernel-based hypotheses. The main result of the paper provides an upper bound on the generalization error of a hierarchical classifier, expressed in terms of the empirical error and the Rademacher complexity of the classifier. The empirical error term favors flatter classifiers, while the Rademacher complexity term incentivizes deeper classifiers.
Building on this theoretical result, the paper develops a pruning strategy for hierarchical classifiers. However, I found the paragraph beginning on line 307 somewhat unclear; specifically, the motivation for employing the metaclassifier and its connection to improving the generalization bound was not well explained.
Overall, the paper is well-written and addresses an important and interesting problem. On the theoretical front, it makes a valuable contribution to the field of hierarchical classification. My primary concern is that the practical implications of the theoretical insights are not sufficiently clear. It remains unclear how practitioners can leverage these insights to perform hierarchical classification effectively. The authors could improve the paper by providing more concrete guidance on how to prune hierarchies to enhance classification performance. In summary, this is a strong theoretical paper on hierarchical classification, but it falls short in translating its theoretical contributions into actionable procedures for practitioners.