This paper explores a distributed version of stochastic dual coordinate ascent (SDCA). The proposed DisDCA algorithm is demonstrated to retain the same convergence guarantees as SDCA, while offering the advantage of requiring no parameter tuning, unlike ADMM. Empirically, it performs on par with a well-tuned ADMM implementation.
The paper is well-written and technically robust. The algorithm is novel yet straightforward, and I particularly appreciated the section discussing the tradeoffs between computation and communication. Distributed learning remains a highly active research area, but few papers delve into analyzing the regimes where algorithms are competitive. Overall, I have no significant criticisms of the paper, though I found the figures—both in the main text and the supplementary material—extremely difficult to read. I recommend that Figure 3 include a sample of the results, with the remaining figures spread across multiple pages in the supplementary material. This is a strong paper that is highly relevant to the NIPS community, and Distributed DCA serves as a compelling alternative to ADMM.