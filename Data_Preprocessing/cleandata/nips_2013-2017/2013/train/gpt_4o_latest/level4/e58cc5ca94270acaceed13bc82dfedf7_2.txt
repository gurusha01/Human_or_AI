The paper investigates the matrix completion problem within the framework of collaborative filtering or multi-label learning. The primary contribution lies in incorporating side information about the rows and columns of the matrix into the model, demonstrating how leveraging this side information can reduce sample complexity compared to the standard matrix completion approach. Specifically, the authors consider a matrix completion model where the target matrix \( M \) is expressed as \( M = A Z0 B^T \), with \( A \) and \( B \) representing the known side information. When \( A \) and \( B \) are low-dimensional, this formulation reduces the degrees of freedom, as the unknown matrix \( Z0 \) is significantly smaller than \( M \). This reduction intuitively explains the improved sample complexity guarantees.
The authors extend the standard nuclear norm minimization framework to analyze this setting. They establish that the sample complexity required to ensure a unique minimizer is \( O(\mu^2 r (ra + rb) \log n) \) for an \( n \times n \) matrix of rank \( r \), where \( Z0 \) has dimensions \( ra \times rb \), and \( \mu \) corresponds to the usual incoherence parameter in the literature. This result indicates that when the side information is low-dimensional (i.e., \( ra, r_b \) are small), the sample complexity becomes sublinear in the matrix dimensions, representing an improvement over traditional matrix completion results.
The theoretical findings are supported by simulations and experiments in the context of multi-label learning. The simulations demonstrate that the proposed algorithm is both computationally and statistically more efficient than singular value thresholding, a widely used matrix completion algorithm. Additionally, the algorithm outperforms several baseline methods across multiple datasets in the multi-label learning application.
The paper addresses an important problem and contributes to the matrix completion literature by incorporating side information. The results are interesting, and the paper is well-written. While the proof techniques are solid, they rely on established tools in matrix completion and convex analysis, making the technical novelty somewhat limited. Consequently, the contribution, though meaningful, is relatively modest. It would have been beneficial to include necessary conditions for the noiseless case and guarantees for the noisy version of the problem. The experiments are compelling but could be better presented; for example, replacing tables with graphs would improve readability and save space.
Questions:
1. The main paper does not discuss the roles of \( \Omega0 \) and \( \Omega1 \) or the condition \( \Omega1 \geq q \Omega0 \). Providing intuition for why this condition is necessary and identifying regimes where it holds would be helpful.
2. In Lemma 4 of the appendix, the condition \( \Omega0 \leq |\Omega| \leq \Omega1 \) is required, but the main theorem does not impose an upper bound on the sample complexity. Is this an oversight? More importantly, why is there an upper bound on \( |\Omega| \) in the first place? What is the intuition behind this requirement?
3. Why are test instances excluded from the experiments? Would it not be more natural to randomly sample the label matrix and evaluate performance on the unobserved entries?
4. The title of the paper is somewhat misleading. The term "speedup" suggests a focus on computational efficiency, which is not the primary emphasis of the paper. I initially expected a more algorithm-centric contribution. While the paper makes a meaningful, albeit modest, contribution by exploring a method to incorporate side information and demonstrating its impact on sample complexity, the analysis relies on standard tools in convex optimization and matrix completion, limiting its technical innovation.