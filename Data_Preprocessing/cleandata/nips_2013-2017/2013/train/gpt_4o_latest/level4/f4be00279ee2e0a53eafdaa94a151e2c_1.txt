-- Updated Review (Paraphrased) --
Just a few additional reflections. Simo Saarka's more recent work on continuous-discrete time systems (which you cited) could provide an interesting point of comparisonâ€”particularly as it employs Gaussian cubature, offering a compelling alternative deterministic approximation technique. This might be worth exploring further in your discussion or as a direction for future research. Additionally, I now wonder whether it might be feasible to derive an algorithm directly using the variational Gaussian approach. Such an approach could be advantageous due to its well-defined objective function, which facilitates optimization, potentially reduces numerical challenges, and provides a clear interpretation in terms of the marginal likelihood. Subsequently, low-order marginal corrections could be incorporated using cumulant perturbations (similar to Opper's work for EP). The only reference I am aware of that explores this is Barber and van de Laar (http://arxiv.org/pdf/1105.5455.pdf). I look forward to seeing the final version of the paper.
-- Original Review (Paraphrased) --
This paper introduces an algorithm for approximate Bayesian inference in models that incorporate both continuous and discrete time observations. The model is framed within the context of latent Gaussian models, and a parallel expectation propagation (EP) algorithm is employed to provide a principled approach for inference and learning in these settings. The EP inference algorithm is integrated into an EM framework, enabling both parameter learning and the computation of marginal distributions. The algorithm is demonstrated to be effective across several experimental scenarios.
Overall, I found the paper enjoyable to read and appreciated its contribution in extending the applicability of approximate message-passing techniques to a broader class of models.
One particularly intriguing aspect was the observation that the EP updates in the continuous-time limit converge to the variational Gaussian updates. While this is tied to the latent Gaussian structure, I am curious whether there is a deeper theoretical explanation for this connection.
The algorithm appears robust, likely due to the implied fractional updating. However, I would appreciate comments on any challenges encountered during implementation, such as issues with slow parameter convergence, numerical stability, or related concerns.
Since the algorithm remains cubic in complexity due to matrix inversions during inference and the M-step updates, it would be helpful to discuss potential strategies for scaling the method to larger datasets or models.
In the experimental section, it would be beneficial to include plots that provide insights into the algorithm's convergence behavior. Additionally, it would be interesting to highlight the advantages of having an estimate of the marginal likelihood. For instance, in Figure 3C, it might be possible to plot individual points with sizes proportional to their marginal likelihood values.
In summary, the paper is well-written and significantly extends the scope of approximate Bayesian inference methods to models with continuous and discrete time settings, which I believe will be of interest to many researchers.