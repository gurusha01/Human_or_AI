This paper addresses the increasingly popular problem of aggregating low-quality answers obtained from crowdsourcing to produce more accurate results.  
The primary focus is on determining the number of control examples (with known ground truth labels) required to achieve the highest accuracy.  
Using a straightforward Gaussian model with worker ability parameters, the authors analyze expected errors for two estimation approaches: two-stage estimation and joint estimation, which are then used to derive the optimal number of control items.  
While I did not identify any significant flaws in the analysis, and the experimental results support the claims under the given assumptions, my main concern lies in the assumption of uniform task assignments to workers.  
In typical crowdsourcing scenarios, this assumption is less realistic, as some workers tend to complete many tasks while most contribute only a few.  
The robustness of the proposed method under such conditions is not evaluated, as all datasets used in the experiments adhere to the uniform task assignment assumption.  
It would be beneficial if the authors discussed extending the method to discrete values.  
Additionally, they should reference existing works that incorporate control items into statistical quality control, such as Tang & Lease (CIR11) and Kajino & Kashima (HCOMP12).  
While the problem studied is compelling, the assumption of random task assignments may limit the practical applicability of the proposed approach.