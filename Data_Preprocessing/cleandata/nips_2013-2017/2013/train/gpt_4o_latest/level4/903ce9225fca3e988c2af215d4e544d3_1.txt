On page 2, you state that it is impossible to use standard reductions to prove NP-hardness. However, I believe this point is more nuanced: it appears that certain classes of reductions do not permit basing the hardness of learning on the assumption RP â‰  NP. (You should cite the work by Applebaum, Barak, and Xiao here.)
You emphasize that your results are not based on cryptographic assumptions. However, it is unclear (at least to me) whether the complexity-theoretic assumption you rely on is inherently weaker than assuming the existence of one-way functions. There might be some evidence suggesting that the assumption of average-case hard functions in NP is "weaker" than the existence of one-way functions. For instance, Noam Livne's results hint at this possibility. It would be beneficial to discuss how the assumptions you use compare in strength to those employed by others. For example, is it evident that your result holds under a sufficiently strong cryptographic assumption, such as the hardness of factoring? I suspect this should be feasible.
Regarding the upper bounds: using your techniques, would it be possible to show that \( H_{n, k+1} \) can be learned using \( \tilde{O}(n^k / \epsilon^2) \) samples? Is that correct? Additionally, do you think it might be possible to strengthen the lower bound (perhaps using a different assumption) to demonstrate that as \( k \) increases, the gap between information-theoretic sample complexity and computational complexity widens?
Minor comments:
---------------
1. Line 130: The proof of item 1.
2. Line 343: Did you mean \( y_k = b \)?
--
Including a discussion about the relative strength of your complexity assumption compared to others would significantly enhance the paper's value, especially for a theoretical audience. Your result suggests that "hardness conjectures" in learning may be weaker than other conjectures that researchers have been willing to adopt in different domains. Highlighting this explicitly would make your contribution more compelling.
I appreciate the simplicity of the concept class you use to demonstrate a separation, which makes the result particularly appealing. That said, it might be helpful to clarify in the text that you are not necessarily claiming your assumption to be weaker than the existence of one-way functions, as the current phrasing might implicitly suggest otherwise.
The paper establishes that under a complexity-theoretic assumption, no polynomial-time algorithm can learn a specific concept class (halfspaces) under a class of distributions (low Hamming weight Boolean vectors) with the sample complexity required for information-theoretic learning. However, polynomial-time learning becomes feasible with a significantly larger sample size. This work builds on a rich history of research demonstrating separations between information-theoretic sample complexity and computational complexity, starting with the work of Decatur, Goldreich, and Ron.