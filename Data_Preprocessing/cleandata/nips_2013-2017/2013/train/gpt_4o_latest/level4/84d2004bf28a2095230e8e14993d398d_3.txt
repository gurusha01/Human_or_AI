This paper presents the GreeDi algorithm, designed to maximize monotone submodular functions under a cardinality constraint within a distributed system framework, specifically leveraging the MapReduce paradigm. The authors conduct extensive experiments across a diverse set of datasets and provide theoretical guarantees, demonstrating that distributing the workload across multiple machines achieves an objective value that remains within a reasonable bound of the centralized algorithm's performance.
The function being optimized must be "decomposable," meaning it can be expressed as the sum of several submodular functions, ensuring that the computation does not rely on the entire dataset.
I would like the following questions to be explicitly addressed:
1) What is the exact amount of communication required between the mappers and reducers in the MapReduce implementation? In other words, how much data needs to be shuffled, representing the communication cost in this setup?
2) What is the maximum number of items that could be reduced to a single key? This would help evaluate the potential overload on individual machines.
3) What is the worst-case number of iterations required for the algorithm to converge?