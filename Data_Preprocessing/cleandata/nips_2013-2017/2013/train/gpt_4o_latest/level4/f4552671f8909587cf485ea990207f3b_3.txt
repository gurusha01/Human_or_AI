Review - SUMMARY
This paper investigates a model of associative memory in which the patterns reside on a low-dimensional manifold. The structure of these patterns is encoded by "constraint-neurons," which, together with the pattern neurons, form the memory network. The authors introduce a recall algorithm for this network and demonstrate that intrinsic noise enhances recall performance.
The manuscript is clearly written and, to the best of my knowledge, technically robust.
MAJOR COMMENTS
1. Biological Plausibility: The paper begins with a discussion of associative memory in the brain, but the proposed associative memory model appears to be far removed from any biologically realistic implementation.  
   - For instance, how could Step 4 in Algorithm 2 be realized in a biologically plausible network? This step assumes that some form of memory is retained across the tmax iterations of Algorithm 1 (see Step 3 in Algorithm 3).  
   - Additionally, how should we interpret the integer-valued state of the neurons? Are these states meant to represent spiking versus non-spiking activity? If so, this binary coding (S = 2) would differ from the coding used in the constraint neurons.
2. Comparison to Restricted Boltzmann Machines (RBM): The authors note that the architecture resembles that of Restricted Boltzmann Machines (RBM), where pattern neurons correspond to visible neurons and constraint neurons correspond to hidden neurons. In RBMs, neurons are also intrinsically noisy, and the level of intrinsic noise is adjusted (via synaptic strengths) to ensure that the marginal distribution over visible units matches the data. Consequently, I would expect RBM performance to degrade if neurons were forced to be deterministic. Therefore, it is unclear to what extent the finding that intrinsic noise improves performance is novel. Moreover, the authors claim that Deep Belief Networks (and, by extension, RBMs) are used for classification rather than pattern completion, but this is incorrect—RBMs can indeed be employed for pattern completion.
3. Comparison to Other Associative Memory Models: Other approaches to associative memory have been proposed. For instance, Lengyel et al. [R1] frame recall as a Bayesian inference process that leverages prior knowledge about the structure of stored patterns, the learning rule, and the corrupting process of the inputs (likelihood). In this framework, the posterior distribution is a deterministic function of the prior and likelihood, and additional noise is not required (except when sampling from the posterior). This raises the question: how can the "optimal" recall rule (in a Bayesian sense) operate without noise, while the recall rule presented in this work depends on noise?  
[R1] Lengyel, M., Kwag, J., Paulsen, O., & Dayan, P. (2005). Matching storage and recall: hippocampal spike timing–dependent plasticity and phase response curves. Nature Neuroscience, 8(12), 1677–1683.
The manuscript is clearly written and, to the best of my knowledge, technically robust.