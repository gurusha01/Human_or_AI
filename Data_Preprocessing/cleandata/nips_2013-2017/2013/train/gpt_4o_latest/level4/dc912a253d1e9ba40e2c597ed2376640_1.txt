The paper introduces a novel parallel dual coordinate descent method aimed at solving regularized risk minimization problems. In the proposed approach, each machine or core updates a subset of dual variables based on the current parameter and subsequently performs a "delayed" update to the shared parameter.
While a similar concept has been employed in Yahoo-LDA and matrix factorization (Hogwild), to the best of my knowledge, this is the first time it has been applied to the dual coordinate descent method. Theoretical guarantees are rigorously established in Theorem 1. Experimental results demonstrate that the proposed method outperforms other parallel algorithms on linear SVM problems. My only critique is that Figure 3 is too small and difficult to interpret; additionally, I recommend using a log-scale for the primal objective and duality gap. Overall, this is a strong paper, and I recommend it for acceptance.