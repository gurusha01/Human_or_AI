Review - Summary:  
The paper, "Predicting Parameters in Deep Learning," investigates the hypothesis that the standard, widely-used representation of neural network parameters contains substantial redundancy.  
Pro:  
- Offers an intriguing observation and interpretation.  
- Experiments provide promising preliminary evidence.  
Con:  
- Empirical results fail to convincingly support the central claims.  
- Lacks comparisons with established techniques such as PCA preprocessing for weight matrix factorization.  
Quality:  
The authors note the challenges of training with a parameter matrix \( W \) factored as a product \( UV \). Could the authors provide further intuition for this difficulty? One possible explanation is that if \( U = V = 0 \) initially, naive backpropagation would fail to update them. Alternatively, training \( W \) normally but constraining it to be a product of \( U \) and \( V \) could trade some computational overhead during updates for reduced communication costs.  
A more significant concern lies in the paper's main observation about weight redundancy. Combined with the use of a spatial exponential kernel, this suggests that similar performance improvements might be achievable through common practices such as PCA of the input or downsampling the images and convolutional kernels. PCA preprocessing, in particular, effectively performs a low-rank factorization of the first layer's weights.  
While maintaining a PCA of intermediate layers is not a standard practice, it could be beneficial and represents an interesting conceptual foundation for the authors' work on "data-driven expander matrices."  
The discussion of columnar architectures and convolutional networks is reasonable but largely speculative. Highlighting various ways to refactor matrices across architectures without more comprehensive empirical validation seems tangential to the paper's core thesis.  
The empirical component of the paper is critical but requires further development. The authors claim that "there exist models whose parameters can be predicted," but this is a relatively weak assertion. Based on the introduction's tone, the authors seem to aim for a stronger claim: "there exist large models that outperform naively down-sized versions, and our techniques cause less degradation to these models than simple down-sizing." While this stronger claim is central to the paper, it is not substantiated by the presented empirical results.  
Additionally, the claim that parallel training can be accelerated using these techniques needs more robust justification. The proposed method introduces new computational overheads, and specific encoder/decoder algorithms for transmitting and reconstructing weight matrices should be implemented and tested to support this argument.  
Clarity:  
- The term "collimation" is misused; while it is a valid word, it does not mean "making columns."  
- Typographical error: "the the."  
- The term "redundancy" may not accurately describe the symmetry/equivalence set discussed around line 115.  
- Section 2, "Low rank weight matrices," is brief background material and might be better integrated into the introduction.  
- Figure 1, referenced only in the introduction, appears to illustrate the method described in Section 3.2. Moving Figure 1 to Section 3.2 would improve clarity.  
- Please number the equations for ease of reference, especially for reviewers.  
Originality:  
The idea of treating weight matrices in neural networks as continuous functions has been explored before, as has the observation of weight redundancy. However, the notion that training could be accelerated by communicating only a small subset of randomly selected matrix values is novel.  
Significance:  
As highlighted in the introduction, this work could be particularly valuable for researchers focused on parallelizing neural network training over low-bandwidth communication channels.  
Edit:  
After reviewing the authors' rebuttal, I have revised my quality score upward. I initially misunderstood the discussion of columns and its connection to the experimental results. I recommend incorporating the authors' defense of their experimental work into future revisions. While the paper presents interesting ideas and encourages further thought, the empirical results remain insufficient to substantiate the most critical and intriguing claims. Additionally, the algorithm for accelerating parallel training is only outlined and requires further elaboration.