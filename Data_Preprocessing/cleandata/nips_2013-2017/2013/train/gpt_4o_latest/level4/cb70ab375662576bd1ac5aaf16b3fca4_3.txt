In this paper, the authors present a spectral learning-based algorithm for learning Markov Models and Hidden Markov Models (HMMs) in a non-sequential setting. The work addresses the challenge of learning these models from sparse realizations observed at random time points.
The paper includes rigorous proofs for empirical moments and provides a sample complexity bound. Overall, the manuscript is well-written and comprehensible. My comments are as follows:
- Learning a sequential model in a non-sequential setting is a concept that may not be familiar to all readers. It would be helpful to include a brief review of existing methods, such as those based on maximum likelihood. While the authors may argue that space constraints limit such a discussion, I believe it would be more effective to motivate the non-sequential sequential model learning approach rather than reproducing the tensor decomposition algorithm of Anandkumar et al. The tensor algorithm could be introduced succinctly by explaining the general idea of expressing the parameters as symmetric observable tensors (as in Equation 2). Consequently, Algorithm 2 and Theorem 1 could be omitted to create space for this discussion.
- On page 5, the proper moment equations for learning a Markov model in a non-sequential setting are defined. Using these equations, the expected transition probability matrix \( T \) and the parameters \( \pi \) of the Dirichlet prior can be recovered via the symmetric orthogonal tensor decomposition algorithm of Anandkumar et al. (2012). However, to recover the actual transition matrix \( P \), the authors propose a search heuristic. To improve clarity, this search heuristic could be presented as pseudo-code. The assumption of a zero entry in \( P \) is reasonable, particularly for Markov chains with a large number of states, which may naturally induce sparsity.  
Minor comment: On page 5, isn't \( \pi = \alpha / \alpha_0 \)? Instead of using the proportionality notation, it would be clearer to provide the exact equality to enhance the reader's understanding of the moment equation proofs.
- The experimental results are limited to synthetic data. Given that the primary motivation of this paper is learning a sequential model in a non-sequential setting, it is crucial to validate the algorithm on real-world data. Additionally, a performance comparison with a more conventional learning algorithm is essential to justify the proposed approach.
- In Section 4, the authors state that as the number of data items \( N \) increases, the takeoff point in Figure 1(a) approaches the true value \( r = 0.3 \). However, I find it unclear why the projection error is nonzero when \( r < 0.3 \) but becomes zero when \( r > 0.3 \). Shouldn't the projection error differ from zero whenever \( r \neq 0.3 \)? A clarification on this point would be helpful.  
Minor comment: In the legend of Figure 1, shouldn't the logarithms be base 10 instead of base 2?
- A key advantage of spectral learning algorithms for latent variable models over maximum likelihood (ML)-based approaches is their computational efficiency. However, the paper does not discuss the speed of the proposed algorithm. A speed comparison with an ML-based approach, such as Expectation-Maximization (EM), would be valuable. Additionally, the authors could consider using the matrix eigendecomposition-based algorithm of Anandkumar et al. (A Method of Moments for Mixture Models and HMMs, 2012), which is computationally less expensive than the tensor decomposition approach.
This paper is well-written and addresses an interesting problem with a solid execution.