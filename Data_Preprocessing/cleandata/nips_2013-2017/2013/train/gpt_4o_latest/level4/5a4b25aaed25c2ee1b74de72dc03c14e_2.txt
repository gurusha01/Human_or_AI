The authors present a method aimed at enhancing the mixing rate of Gibbs sampling in Ising models by reparameterizing the original model into a new parameter space that satisfies the Dobrushin criterion, which is essential for fast mixing. This reparameterization is framed as a constrained optimization problem, where the constraints ensure that the new parameters remain within the same space as the original ones. The authors explore this projection using various distance and divergence measures.
I believe that approaches combining stochastic and deterministic inference principles represent a relatively under-explored area, making this an intriguing contribution. While the concept of modifying the original parameters to accelerate mixing is intuitive, I found the explanation of the dual formulation of the projection problem somewhat unclear. For instance, how do the constraints \(z{ij} \cdot d{ij} = 0\) guarantee that the new parameterization spans the same space as the original? Could it not potentially restrict it to a smaller subspace? Additionally, the overall procedure left me with questionsâ€”do the projected gradient updates need to fully converge before running the Gibbs sampler, or are the sampling and gradient updates interleaved? Including an algorithm box would greatly clarify this process. Furthermore, is there a formal guarantee of convergence for the proposed projected gradient descent approach?
The experimental evaluation also left room for improvement. In the first set of experiments, why was the Gibbs sampler executed for 30,000 iterations? Since the comparison involves both sampling-based and deterministic methods, evaluating performance in terms of runtime would provide a more balanced perspective. Additionally, the absence of error bars on the plots makes it difficult to assess the statistical significance of the results. In the second set of experiments, the authors demonstrate that the proposed projection improves mixing rates compared to the original Gibbs sampler. However, the time required to compute the projection itself is not factored into the comparison. For example, if the projection step takes one minute, and the naive Gibbs sampler could generate 10,000 samples in that same time, the utility of the projection becomes less clear. Lastly, I was surprised that there was no comparison to alternative approaches, such as blocked Gibbs sampling or Gogate's "Lifted Gibbs Sampler."
Overall, this is a promising idea. However, the formulation of the projection problem and the proposed method could benefit from further refinement, and the experimental results need to be strengthened to convincingly demonstrate the practical utility of the approach.