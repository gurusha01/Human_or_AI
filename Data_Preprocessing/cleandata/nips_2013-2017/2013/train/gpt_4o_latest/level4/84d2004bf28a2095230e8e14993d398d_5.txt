This paper addresses the challenge of solving submodular maximization problems at scale. Specifically, it focuses on adapting the classical greedy algorithm for submodular maximization under cardinality constraints to handle massive datasets efficiently.
The problem is well-motivated. There has been prior work on "speeding up" or "parallelizing" the inherently sequential greedy algorithm for submodular maximization. Using MapReduce as a programming paradigm to express the algorithm is also a reasonable and motivated choice.
The primary technical contribution of the paper is the analysis of a two-round algorithm: the input is distributed across machines, where each machine approximates a solution for its subset of the input using a sequential greedy algorithm. These individual solutions are then merged to produce the final solution. A key insight is that each machine outputs a solution of size greater than k/m (where k is the desired solution size and m is the number of machines). The analysis is straightforward, and the results demonstrate an inherent dependence on both k and m. The paper also includes supplementary results for specific cases, such as smooth spaces and decomposable functions.
The experimental results are reasonable and demonstrate the effectiveness of the two-round algorithm compared to the standard greedy approach.
On the positive side, the paper tackles an important problem and proposes a practical modification to the standard algorithm.
However, the paper has several shortcomings:
1. The theoretical contributions are minimal. The analysis is straightforward and lacks depth from a theoretical perspective. Some proofs are repetitive, and the results rely heavily on strong assumptions. For instance, Theorem 4.2 does not leverage randomized input partitioning effectively (nor does it provide a lower bound).
2. The paper does not explore a round-memory-approximation tradeoff. Its results are restrictive, and the proposed approach does not appear to generalize well to multiple rounds. This makes it significantly weaker than prior work, such as the WWW paper by Chierichetti et al. or the SPAA paper by Lattanzi et al.
3. The experimental section includes several unnecessary baselines (e.g., random/random). The authors do not adequately highlight the role of oversampling or explore modifications to the greedy/merge process to make the greedy step dependent on the size of the local solutions.
Additional comments:
1. The paper should explore whether stronger bounds can be established for Theorem 4.2 under randomized input partitioning.
2. The authors should consider the SPAA 2013 paper by Kumar, Moseley, Vassilvitskii, and Vattani, which addresses a similar problem in a more general context and provides a multi-round algorithm with better approximation guarantees.
3. It may be possible to combine the results of Theorems 4.3 and 4.4, as they appear to rely on related assumptions (e.g., neighborhood size in metric spaces vs. growth functions of a metric, which are volume-based).
4. The authors should compare their algorithm against the approaches in the Chierichetti et al. paper and the SPAA 2013 paper.
5. Page 4, line 167: Clarify what is meant by a "suitable choice."
6. Page 5, line 227: The statement about "... unless P = NP" is unclear. Why does this follow?
In summary, this is a theoretically weak paper that addresses an important problem.