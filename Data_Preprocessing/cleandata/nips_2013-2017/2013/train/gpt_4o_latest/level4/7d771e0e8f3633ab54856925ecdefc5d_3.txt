This paper presents a pruning technique that facilitates symbolic modified policy iteration (MPI) in large factored action Markov Decision Processes (MDPs). Similar to standard MPI, their approach generalizes policy iteration and valuation, integrates prior (orthogonal) work on partially bound actions, and is supported by experimental validation.
I am inclined to recommend this paper for acceptance. The primary contribution appears to be a non-trivial implementation detail, but the experimental results demonstrate that (a) pruning alone enhances value iteration, (b) pruning is essential for modified policy iteration, which is often infeasible due to memory constraints, and (c) modified policy iteration accelerates convergence in factored action MDPs.
The paper is well-motivated; however, the notation is inconsistent in certain areas and can be difficult to follow. For instance, Algorithm 3.2 is referred to as "Prune," but it is denoted as \cal P elsewhere. Additionally, it is not immediately clear from the text that T^Q(V) is a function of states and actions, or that the variables are binary.
My primary concern with the paper is that I struggled to fully comprehend the details necessary to understand the statement of Theorem 1. Specifically, it is unclear why \hat T^Q\pi might differ from T^Q\pi. Is pruning required at every step, or is a single pruning step sufficient? Does repeated pruning lead to overestimation, or does the convergence theorem remain consistent for both FA-MPI and OPI?
Proposition 2 appears to be straightforward. Is there any formal guarantee regarding the extent to which the pruned tree is reduced in size?
Overall, I recommend accepting this paper. At a high level, it is well-motivated and clearly articulated, and the experiments effectively demonstrate its capability to address previously intractable problems.