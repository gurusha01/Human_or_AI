This paper introduces a novel method for leveraging human feedback to enhance the performance of a reinforcement-learning agent. The key innovation lies in converting human feedback into a potentially inconsistent estimate of action optimality, rather than treating it as a reward signal, which is the more conventional approach. The proposed algorithm demonstrates superior performance compared to prior state-of-the-art methods in two toy domains. Overall, I found this to be an excellent paper that effectively motivates the problem, clearly presents a new idea, and rigorously compares its performance against other state-of-the-art algorithms (not just weaker baselines). My comments are primarily suggestions for improvement.
- I particularly appreciated the use of a simulated human teacher, which allowed for systematic manipulation of feedback likelihood and consistency. However, I would have liked to see experiments with much lower feedback likelihoods (e.g., < 1%).
- One concern I have is the potential for systematic inconsistency in human feedback. In psychology, reward shaping is often used to train new behaviors through successive approximation. For instance, to train a rat to pull a chain, one might initially reward the rat for approaching the chain, then for touching it, and finally for pulling it—deliberately ceasing to reward earlier behaviors at each step. How would the Advise algorithm handle this type of systematic inconsistency, which might arise from expert human trainers?
- Section 4.2: The assumption that humans know only one optimal action seems overly strong. What happens if this assumption is relaxed? For example, if the human teacher alternates between shaping two different optimal actions, does the algorithm's performance degrade? Should it? A brief discussion of this issue would be helpful.
- Another challenge with human feedback is delay. Inconsistencies might arise simply because humans do not respond at a consistent rate—for instance, providing positive feedback only after an intervening action. This could actually highlight another strength of the Advise approach, as its ability to handle inconsistency might make it more robust to such delays compared to other methods.
Minor points:
- Line 053: The phrase "from MDP reward" is awkward and could be rephrased for clarity.  
- Section 5.1: It is unclear how one wins in Pac-Man. Does this involve eating all the dots? This should be specified.  
- Table 1 and figures: The second column would be clearer if labeled "Reduced Frequency" instead of "Reduced Feedback." Additionally, the ordering of conditions in Table 1 differs from that in the subsequent figures, which could be standardized for consistency.  
- Lines 234–247: The relationship between control sharing and action biasing could be explained more clearly.  
- Line 294: There appears to be a missing word in "prior estimate of the (missing of)."  
- Figure 2: Other than the ideal case, why were only cases where Advise does not help chosen for plotting? This is unclear, especially given the data in Table 1.  
- Lines 369–370: It might be more accurate to suggest taking the closest overestimate of C (i.e., erring on the side of caution by rounding upward).  
- Figures 4 and 5: The text, particularly the axis labels, is too small and should be enlarged for readability.
In summary, this paper presents a novel method for incorporating human feedback into reinforcement learning. The approach is innovative, and the experiments convincingly demonstrate performance improvements over other state-of-the-art methods.