This paper introduces a convex methodology for training a two-layer model in supervised learning. The approach integrates large margin losses into the training objectives, employs indmax transfer for the second layer, and utilizes a multi-label perceptron model with step transfer for the first layer. Convex relaxations are then applied, enabling the global training of the two-layer model.
The manuscript is well-written, and the way the authors connect all components to derive a convex objective is both insightful and compelling. The technical steps leading to the final convex formulation are clearly presented, with some notable intermediate results. From a deep learning standpoint, the proposed global training framework is meaningful, and the experimental results effectively highlight the importance of global training. Overall, the paper offers a robust convex approach for training a conditional two-layer model, with technically sound contributions and significant, insightful conclusions.