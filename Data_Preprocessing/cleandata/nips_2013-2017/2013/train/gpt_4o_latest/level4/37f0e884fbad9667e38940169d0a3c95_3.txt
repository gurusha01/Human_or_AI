The paper introduces a novel hybrid approach to stochastic gradient descent, combining both stochastic and batch gradients. The proposed algorithm is proven to converge to an epsilon-accurate solution using O(log(1/ε)) full gradients and O(k² log(1/ε)) stochastic gradients.
The convergence proof appears both correct and innovative, aside from a few minor errors outlined below.
However, my primary concern lies in the practical relevance of the proposed algorithm within the context of machine learning, which is the central focus of this conference. Specifically, in typical ML scenarios, strong convexity is often induced by the regularizer, leading to a value of μ proportional to the number of samples, N, such that μ = C N, where C is independent of N. Under this assumption, the proposed method outperforms batch gradient descent only when the number of samples is limited to O(C³/L³), which does not seem to represent a practically significant regime. Additionally, the convergence rate of the proposed algorithm is probabilistic (holding with high probability), whereas the convergence rate for batch gradient descent is deterministic. This distinction is critical and must be thoroughly addressed to convincingly demonstrate that the proposed algorithm offers a tangible advantage over batch gradient descent, thereby establishing the paper's relevance to the machine learning community.
Minor comments:
- In equation (6), it should be ||w* - ŵ||².
- Please specify in line 288 the function to which equation (7) is applied.
- The equality in line 286 should be removed, as it adds no value to comprehension and instead detracts from clarity.
- In equation (13), the absolute values should be replaced with norms.
- Although it may seem obvious, please explicitly clarify that L ≥ λ for completeness.
- In equation (4), x should be w, f should be F, and the first term in the max is always greater than the second term, as shown by Lemma 1.
- Please provide a precise definition of the condition number as a function of λ and L.
In summary, while the paper introduces a novel hybrid stochastic/batch gradient descent method, it remains unclear whether the algorithm offers any practical advantage over standard batch gradient descent in typical ML optimization scenarios.