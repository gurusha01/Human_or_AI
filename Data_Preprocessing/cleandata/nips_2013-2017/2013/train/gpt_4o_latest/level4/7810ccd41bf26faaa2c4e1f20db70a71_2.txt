The authors propose the use of a criterion, Σ-Optimality, for active learning in Gauss-Markov random fields. While this criterion was originally introduced by Garnett et al. for active surveying, the submodular property of the criterion does not appear to have been identified in that earlier work.
The framework embeds both labeled and unlabeled data as nodes in a graph, where edge weights, computed using a kernel function, represent pairwise similarities. The motivation for adopting an active learning approach stems from the potential cost associated with labeling the entire dataset (presumably higher than the cost of computing edge weights for all data). To address this, the criterion is used to determine which unlabeled data points should be prioritized for labeling. The authors demonstrate that the criterion satisfies the submodular monotone property, which ensures that greedy selection achieves a (1-1/e) approximation to the optimal selection. However, the authors should clarify that this optimality pertains to the criterion itself and not directly to classification accuracy. While the empirical results show strong classification performance, it is important to emphasize that the criterion serves as a surrogate.
The introduction references several existing criteria (Settles, Krause et al., Ji and Han) against which the proposed criterion is compared. However, the computational complexity of calculating the reward is only briefly discussed. For instance, both V-Optimality and Σ-Optimality require computing the inverse of the graph Laplacian for the remaining unlabeled data, whereas some alternative criteria may have lower computational complexity. Since the problem is framed in terms of costs, and since the performance of all criteria will eventually converge (as they address the same classification problem after selection), a more equitable comparison would account for the computational cost of reward calculation. It is possible that the proposed method performs favorably in this regard, but this aspect warrants further discussion.
The citation of Streeter and Golovin for results on submodular set functions seems misplaced. Nemhauser et al. would be a more appropriate reference.
Section 2.3 contains some inaccuracies. The intractability of subset selection arises from the combinatorial nature of the problem, not from submodularity. Additionally, this does not imply that a greedy solution is "required." Alternative strategies with lower complexity may outperform greedy selection. The significance of establishing submodularity lies in the guarantees it provides for greedy selection relative to the optimal solution, rather than mandating its use.
The fact that the criterion was originally proposed by Garnett et al. in 2012 should be stated earlier in the paper. This would necessitate revising the abstract to reflect the contribution more accurately, such as: "We analyze a previously proposed criterion and demonstrate its utility..."
The establishment of the submodular property and the suppressor-free property is noteworthy. 
The empirical results are sufficient.
Overall, aside from the aforementioned comments, the paper is reasonably clear. The results are interesting, and the analysis, though somewhat limited, constitutes a meaningful contribution.
Minor comments:
- Figure 3 lacks a labeled horizontal axis. It is assumed to represent the probability of correct classification, but the caption's description is ambiguous.  
- The authors demonstrate that a previously proposed criterion is a monotone submodular function, which ensures that greedy selection achieves performance guarantees relative to the optimal selection in an active learning context. Experimental results indicate superior performance compared to previously proposed criteria.