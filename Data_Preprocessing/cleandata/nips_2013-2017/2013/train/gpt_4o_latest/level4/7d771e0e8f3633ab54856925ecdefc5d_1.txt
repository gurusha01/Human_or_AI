This work introduces a novel policy iteration algorithm, MB-OPI, tailored for symbolic MDPs that incorporate factored-action dynamics alongside factored-state dynamics. The proposed algorithm enables a trade-off in representational complexity between value and policy iteration for MDPs represented using algebraic decision diagrams (ADDs), analogous to how Modified Policy Iteration (MPI) balances computational complexity. In doing so, the authors extend and unify prior approaches that separately address factored actions and memory constraints.
A key technical challenge addressed in the paper is the potential for significant growth in the size of the value function when an explicit policy representation is multiplied into it during ADD-based policy iteration. The authors mitigate this issue by introducing a pruning procedure that conservatively combines policy and value diagrams, avoiding the naive multiplication that leads to representational bloat. Empirical results demonstrate a 2-6x improvement in solution time compared to existing methods.
The paper is technically sound and well-articulated. It makes a significant theoretical contribution to symbolic MDP planning by introducing pruning as an alternative to ADD products and proving that this approach satisfies the guarantees of MPI. The work is framed as a generalization of existing symbolic dynamic programming methods and establishes itself as state-of-the-art for planning with factored actions.
Experimental results validate the claim that pruning effectively prevents representational bloat in MPI-based symbolic dynamic programming (SDP) planning, achieving substantial speedups.
The manuscript is generally well-written and accessible. However, I recommend adding more background on SDP solving with ADDs, including their use in representing value functions and dynamic Bayesian networks (DBNs), the standard policy iteration approach using ADD products, and a clear explanation of the distinction between multiplying a policy into a value function versus pruning the value function with the policy.
Additionally, it would be beneficial to include (a) a discussion of practical scenarios involving many parallel actions where action factoring is critical, and (b) a toy example with large parallel actions to illustrate the best-case performance improvement over SPUDD and FAR.
Some suggestions for improving clarity:
- Line 053: "enforcement of policy constraint" – The term 'constraint' is undefined at this point and only becomes clear when policy iteration is viewed as policy-constrained value iteration.
- Line 060, Figure 1: The ordering of state and action nodes could be improved by interleaving or stacking them consistently for better readability.
- Line 060, Figure 1: As state variables are propositions rather than predicates, underscores (e.g., `reboot_c1`) might be more appropriate.
- Line 095: "marginalization operators" – This term should be reworded for accuracy, as it includes operations like maximization in addition to marginalization.
- Line 110, Equation 1: This equation is confusing without referencing the SPUDD paper. Consider adding (a) an explanation of how expectation in factored models translates into a product of DBNs and allows sums to be pushed in, and (b) a simple clarification that "primed" variables are state variables with a prime symbol added, necessary for well-defined ADD operations (e.g., swapping state variables with next-state variables).
- Line 152: For clarity, add a sentence like, "Intuitively, this representation allows 1-step policy backup to be expressed as the ADD product of a policy with a value function."
- Line 179, Figure 4: The caption uses `C` to denote the policy, but the figure uses `\pi`. Ensure consistency throughout.
- Line 206: Clarify that "size of a Bellman backup" refers to the resulting data structure, not computational complexity. An intuitive explanation of why `\pi` is generally larger than the backup would also help (e.g., `\pi` represents joint actions, while the backup represents value and factored actions).
- Line 212: Add "in D" to clarify that the policy is being applied to the value function, avoiding confusion with the original policy definition.
- Line 247, Equation 3: Remove the two extra closing parentheses.
- Line 252: The term "sandwiched" is intuitive but might be too informal; consider rephrasing.
- Line 278: Add a missing comma after "parameterized by k."
- Line 327, Figure 6: Verify the colors for OPI 2 and 5 in panel (b), as they appear to be reversed.
In conclusion, this paper provides a well-defined advancement in decision-diagram-based planning for symbolic MDPs. Both theoretical and empirical results suggest that the proposed algorithm represents the state of the art for planning with factored actions.