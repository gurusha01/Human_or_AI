The authors introduce a method for feature compression by optimizing a formulation that accounts for both the pointer cost and the dictionary cost. To address the non-convex nature of the problem, the binary constraints are relaxed into box constraints within the interval [0, 1], and the Alternating Direction Method of Multipliers (ADMM) is employed to solve the relaxed problem. Experimental comparisons with several existing approaches are provided.
The proposed compressive feature learning framework incorporates a loss-based compression formulation that jointly considers pointer and dictionary costs. The resulting optimization problem is non-convex due to the binary constraints, and the formulation appears to be novel.
To solve the relaxed problem, the authors utilize ADMM. The proposed method involves solving a sequence of relaxed problems that depend on the parameter \(d\), employing a reweighting scheme. However, the efficiency of the proposed compression approach remains unclear. For subsequent classification, the authors report that their method improves classification speed, which seems reasonable.
Following feature extraction, the elastic net method is used for classification. Since the elastic net involves two tuning parameters, it is unclear how these parameters are optimizedâ€”whether through cross-validation, two-dimensional grid search, or another method.
After considering the feedback from other reviewers and the authors' response, I maintain my original recommendation. The authors propose a feature compression approach that optimizes a formulation considering both pointer and dictionary costs. The non-convex formulation is relaxed by replacing binary constraints with box constraints in the interval [0, 1], and ADMM is used to solve the relaxed problem.