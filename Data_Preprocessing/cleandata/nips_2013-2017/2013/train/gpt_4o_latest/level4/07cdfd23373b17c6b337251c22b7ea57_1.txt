This paper introduces the Parsimonious Triangular Model (PTM), which reduces the O(K^3) parameter space of the Mixed-Membership Triangular Model (MMTM) to O(K), enabling faster inference. The authors develop a stochastic variational inference algorithm for PTM and incorporate additional approximation techniques to enhance scalability. Using synthetic data, the paper demonstrates that reducing the number of variables can improve statistical power, while experiments on real-world datasets show that PTM is competitive with existing methods in terms of accuracy.
Quality:
PTM appears to be a compelling specialization of MMTM, but the practical advantage of achieving scalability with respect to K (the number of possible roles) remains unclear. To assess the value of this approach, it is essential to address the question: "How does scalability with large K benefit us in learning MMTM?" Since both MMSB and MMTM are mixed-membership models, the limitations of using small K may not be as severe as in single-membership models. For the real network experiments, it would have been particularly insightful to evaluate the performance of both MMTM and PTM as a function of K. Demonstrating that PTM with a large K outperforms MMTM with a small K would highlight PTM's advantages. Unfortunately, such experiments are absent. Additionally, the paper lacks a clear explanation of how K was selected for both PTM and its competitor MMSB in the real data experiments. This omission raises concerns about potential biases in the experimental setup.
The authors also do not sufficiently discuss the implications of the simplifying assumptions they make. While constraining the parameter space clearly improves scalability, it would be valuable to understand the conditions under which the model might fail. If the parameter matrix B (as per the authors' notation) in MMSB and MMTM truly contains O(K^3) distinct values, PTM's performance would likely degrade. However, the synthetic data experiments do not provide enough detail about the choice of the true B. The authors should have tested PTM with various B configurations, including those that favor PTM and those that challenge it. This would provide readers with a clearer understanding of PTM's strengths and limitations, rather than focusing solely on its successes.
Furthermore, while the approximation techniques introduced to scale PTM are interesting and appear effective, the paper does not adequately describe or analyze their impact. Specifically:  
1) What is the effect of the threshold \(\delta\)? In both synthetic and real-world experiments, does a larger or smaller \(\delta\) improve performance? How much can \(\delta\) be reduced to enhance scalability without significantly compromising accuracy?  
2) The authors claim that a-MMSB fails because downsampling reduces accuracy. Why is PTM immune to this issue? Both MMTM and PTM ignore triplets with only one edge, so why can't non-edges be ignored in MMSB if they can be ignored in PTM?  
3) What is the effect of the O(K) approximation in the local update step (page 5)? How does the quality of the approximation change if 2K, 4K, or 100K triples are sampled instead? Does the approximation degrade as K increases?  
In summary, the experimental setup does not convincingly demonstrate fairness to competing methods, and the discussion of the algorithm's or model's potential weaknesses is insufficient.
Clarity: The paper is well-written and well-organized, making it easy to follow.
Originality: While the modeling contribution is somewhat incremental, the focus on achieving scalability by constraining the parameter space is a novel and promising direction, especially given the recent trend of generalizing existing models in this domain.
Significance: If the simplifying assumptions underlying the model and inference algorithm hold in practice, PTM could have a significant impact on the application of latent-space models to networks, as scalability remains a major challenge for such models when applied to large datasets.
Additional Comments:  
1. The title of the paper feels overly broad. There are other scalable approaches to latent space models, and this work specifically proposes a scalable algorithm for a latent space modelâ€”PTM.  
2. It is unclear whether PTM can truly be considered a probabilistic method. The authors argue in the last paragraph of Section 3 that PTM is analogous to the bag-of-words assumption in LDA, but this claim is unconvincing. In LDA, every bag-of-words corresponds to a document, ensuring a proper likelihood function, even if the model is simplistic. However, in PTM and MMTM, with high probability, no actual network corresponds to a bag-of-triples generated by the model. As a result, these models are not truly generative for networks, and most of the posterior probability mass is allocated to non-network structures. This raises concerns about the probabilistic foundation of PTM. What does probability represent in PTM? Abandoning basic principles of likelihood in "probabilistic" methods could be problematic. I am curious to hear the authors' and other reviewers' perspectives on this issue.  
In conclusion, while the authors propose modeling assumptions and inference techniques that make MMTM scalable, the side effects of these decisions are insufficiently explored.