In this paper, the authors build upon factorized asymptotic Bayesian (FAB) inference to address latent feature models (LFMs). FAB is a recently introduced model selection framework that has demonstrated strong performance for mixture models (MMs) and hidden Markov models. Briefly, the FAB approach optimizes a lower bound of the factorized information criterion (FIC), which asymptotically converges to the marginal log-likelihood. However, a key limitation of FAB is its applicability only to models where the Hessian matrix of the complete likelihood is block diagonal. In this work, the authors successfully extend FAB to LFMs, even though this condition does not hold. They derive an effective lower bound of the FIC for LFMs and demonstrate that it shares the same representation as the FIC for MMs. The authors validate their method on both synthetic and real-world datasets, comparing FAB/LFM against alternative approaches such as fast Gibbs sampling for models based on the Indian Buffet Process, variational Bayes (VB), and maximum-expectation IBP (MEIBP). The results highlight the advantages of FAB/LFM, showing superior performance not only in predictive accuracy but also in computational efficiency.
The paper is well-written, and the authors provide a detailed explanation of the derivation steps for their method.
The proposed model generalizes FAB from MMs to LFMs, which I would consider an incremental contribution. Nonetheless, the approach is compelling, and the results emphasize its effectiveness. While I am not a specialist in this specific area, I believe the work would be of significant interest to the NIPS community. Overall, this is a solid paper that introduces an intriguing idea. The results are persuasive, and I support its acceptance.