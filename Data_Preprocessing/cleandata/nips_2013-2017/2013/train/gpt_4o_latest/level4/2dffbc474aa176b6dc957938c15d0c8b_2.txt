The authors present an application of particle MCMC for inference in Gaussian process state-space models, with a particular emphasis on the recent ancestral sampling particle Gibbs algorithm introduced by Lindsten et al. The paper is well-written, and the proposed approach represents an interesting and original application of particle MCMC. Furthermore, the development of model-specific methodology, such as the sparse GP-SSM, adds value to the work.
However, a notable shortcoming is the absence of comparisons with other particle MCMC methods, specifically the particle marginal Metropolis-Hastings (PMMH) scheme and the particle Gibbs with backward sampling (as described by Whiteley et al.). These methods could have been implemented relatively easily, and such comparisons would provide valuable insights into how the proposed algorithm performs relative to these alternatives.
Additionally, the paper would benefit from the inclusion of graphs illustrating the performance of the algorithms (e.g., in terms of autocorrelation function (ACF) or effective sample size (ESS)) as a function of the number of particles (N) and the time horizon (T). The current results lack sufficient informativeness. For instance, it is unclear whether N needs to scale linearly with T or sublinearly. Based on prior findings (e.g., Whiteley et al.), PMMH is expected to require a number of particles that grows quadratically with T, whereas both particle Gibbs variants likely require sublinear growth in the number of particles with T.
Overall, this is a well-executed application of particle MCMC to GP state-space models. However, the paper would be significantly strengthened by including comparisons with PMMH and particle Gibbs with backward sampling.