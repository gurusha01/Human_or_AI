This paper proposes a general method for parameter tuning of a given training algorithm in a differentially private manner.  
Using a set of examples (for training and validation), a set of model parameters, a training algorithm, and a performance measure, the method produces a differentially private hypothesis that adheres to specified privacy parameters.  
The core concept of the approach is the introduction of (\beta1, \beta2, \delta)-stability, which characterizes the stability of performance when the training and validation sets are altered.  
The procedure essentially employs the exponential mechanism, and a utility bound is provided as part of the analysis.  
At each step, the gradient is structured into a tree, where each node represents the differentially private partial sum of values contributed by its descendant nodes. During the execution of the differentially private FTAL, the learner queries the tree to obtain a differentially private partial sum of prior gradients. Since only O(log T) queries are required to compute the partial sum, the variance of the noise added to each value is O(log T).  
The authors present two algorithms: one for the full information model and another for the bandit setting. In the full information model, the proposed algorithm achieves improved regret bounds. Specifically, in the full information setting with strongly convex and L-Lipschitz cost functions, the regret bound is significant, achieving O(poly log T), similar to FTAL. In the bandit setting, the authors introduce a novel technique called "one-shot gradient" to estimate the gradient and demonstrate regret bounds across multiple scenarios. When the cost function is strongly convex and L-Lipschitz, the regret bound is optimal with respect to T.  
The clarity of the paper could be improved. The problem settings are difficult to follow, and it is not explicitly stated which private instances are protected by differential privacy. Additionally, the definitions of the oblivious adversary and the adaptive adversary in the bandit setting are unclear.  
line 107: What is meant by "stronger setting"? Does this refer to "stronger assumptions"?  
line 229-232: This sentence is difficult to understand.  
line 305: The technique is referred to as "one-point gradient" rather than "one-shot gradient."  
The paper demonstrates a significant improvement in the regret bounds for differentially private online learning in the full information model.