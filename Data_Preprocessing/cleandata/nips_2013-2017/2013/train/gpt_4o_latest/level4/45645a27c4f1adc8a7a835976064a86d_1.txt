This paper introduces a new model selection criterion tailored for binary latent feature models. The approach resembles variational Bayes but diverges by approximately integrating out the parameters using the BIC, rather than assuming a factorized posterior over latent variables and parameters. The authors demonstrate that their method achieves improved held-out likelihood scores compared to several existing IBP implementations.
The proposed method appears reasonable and is supported by a plausible asymptotic argument. Its primary advantage over other IBP inference techniques lies in its computational efficiency: it effectively operates as an EM algorithm with added complexity penalties, avoiding the higher computational costs associated with sampling or variational Bayes methods.
While the technical contributions are novel, they are somewhat incremental, building upon the FAB framework established in [3] and [4].
Some aspects of the exposition are unclear due to imprecise mathematical formulations. For instance, in equation (3), the log sum of the z values becomes infinite if all values are zero. Given that this scenario has nonzero probability under any nondegenerate variational distribution q, it is unclear why the FIC score would not always be infinite. 
Theorem 2 states that the marginal likelihood "can be asymptotically approximated as" a specific expression, but the asymptotic regime and underlying assumptions are not explicitly defined. Specifically, it is unclear how K (the number of components) is treated as N (the number of data points) grows. The theorem seems to rely on a fixed finite K, implying that the activations of each feature would grow unbounded. However, under the IBP model, K is infinite, and new components continue to emerge as more data is observed. Assuming a finite K undermines a key motivation for using the IBP.
Minor issues include the following: In section 2, shouldn't \( pk(X | z{\cdot, k}) \) involve only the data points assigned to mixture component k, rather than the entire dataset? Additionally, in equation (4), the left-hand side should include an expectation.
In the quantitative results presented in Table 1, the proposed method achieves higher predictive likelihood scores in less time compared to alternative approaches. While FAB completes significantly faster in some cases, the interpretation of this result is unclear because the stopping criterion is not specified. For example, the decision of when to stop the Gibbs sampler appears arbitrary.
The improvements in predictive likelihood are noteworthy, but the source of these improvements is unclear. It would strengthen the results if evidence were provided to show that the gains stem from the model selection criterion itself, rather than differences in how algorithms converge to local optima. This concern is particularly relevant given that the Gibbs sampler learns an excessively large number of components in some datasets, which suggests issues with local optima rather than fundamental differences in the methods. In theory, the Gibbs sampler should compute the likelihood exactly, which FAB only approximates.
In summary, the proposed approach is reasonable but somewhat incremental. Certain mathematical aspects lack precision, and while the experimental results indicate improvements in runtime and predictive likelihood, further analysis is needed to better understand and interpret these findings.