The paper introduces a Bayes-optimal framework for incorporating human feedback into reinforcement learning. Specifically, it extends Bayesian Q-learning by integrating binary right/wrong labels provided by humans and evaluates the approach experimentally on two domains (Pac-Man and Frogger).  
---
Quality  
The paper is logically structured and technically sound overall. However, there are a few issues that need to be addressed:  
1) While the authors assert that their method does not convert feedback into rewards or values, the computation of delta_{s,a} based on label counts (right/wrong) effectively translates the labels into values.  
2) The term "policy shaping" used to describe the proposed approach is somewhat misleading. Since the feedback is provided at the action level rather than for an entire policy or episode, a more fitting name might be "action shaping."  
---
Clarity  
The paper is generally well-written and follows a logical progression, but certain sections are somewhat unclear:  
1) In Section 4.2, the authors initially assume that the optimality of an action a in a given state s is independent of the labels assigned to other actions in the same state, leading to formula (1). However, formula (2) contradicts this assumption by incorporating delta_{s,j} values from other actions in the same state.  
2) The description of how human feedback is provided (binary right/wrong labels immediately after an action) appears too late in the paper (around line 112, page 3). This information should be introduced earlier for better clarity.  
3) Section 5.3 lacks sufficient detail on how pre-recorded human demonstrations are utilized to generate a simulated oracle.  
---
Originality  
While the paper presents an interesting approach, some of the most compelling challenges are deferred to future work. For instance, the credit assignment problem (mentioned on line 125) and scenarios involving multiple optimal actions per state remain unexplored.  
Additionally, the proposed method for handling multiple feedback sources appears overly simplistic. By multiplying the two probabilities, both are treated with equal weight, even if one source is less reliable. A more sophisticated approach would involve leveraging the value of C to assess the reliability of human feedback and adjust accordingly.  
---
Significance  
The improvement achieved by incorporating human feedback, while measurable, does not seem substantial enough to justify the significant additional information required by the algorithm. For example, if "oracle" information were provided directly to the agent in the form of demonstrations, the agent could begin learning with a high-return initial policy and refine it further during subsequent episodes.  
---
In summary, the paper presents a Bayes-optimal method for integrating binary right/wrong action labels into reinforcement learning. While the paper is well-written, there is room for improvement in clarity, originality, and significance.