This paper addresses the problem of selecting a subset of informative n-grams for document classification by framing it as a lossless compression task, which is tackled through iterative relaxation of the original hard combinatorial problem. While the concept of unsupervised feature learning as compression is not entirely new, this specific formulation is intriguing, appears novel, and demonstrates reasonable performance in small-scale experiments. The paper is well-written, with clear and adequately motivated ideas. Although the algorithm's presentation is not entirely self-contained due to reliance on a substantial supplement, it is sufficiently understandable as presented. However, I would have appreciated a deeper analysis of the algorithm's computational properties or, at the very least, experiments exploring computational cost-accuracy tradeoffs to better assess the method's scalability (including its claims of parallelizability). On the experimental side, comparisons with widely-used lossy representation methods, such as embeddings (e.g., Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2010, among others), would have been valuable. Additionally, it would be helpful to evaluate the tradeoffs between model size and accuracy achieved by this method in comparison to sparsifying regularization applied to uncompressed n-gram features. Overall, the paper formalizes n-gram feature selection for document classification as a lossless compression problem and proposes an efficient relaxation algorithm for the associated hard combinatorial problem. The approach is elegant and well-motivated, with promising initial results, though it would benefit from further analysis and more extensive experimentation.