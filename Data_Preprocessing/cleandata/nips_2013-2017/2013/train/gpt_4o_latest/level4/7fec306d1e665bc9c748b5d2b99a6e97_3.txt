The paper addresses the challenge of reducing the number of free parameters in neural network architectures, motivated by recent efforts to scale up very large networks. The proposed method leverages the observation that learned parameters often exhibit significant redundancy (e.g., smooth filters in the first layer of image-based NNs). The authors suggest learning only a subset of parameters and predicting the rest through interpolation. The approach is tested on multiple architectures (MLP, convolutional NN, reconstruction-ICA) and vision datasets (MNIST, CIFAR, STL-10). Results indicate that fewer than 50% of the parameters can often be learned without performance degradation, with MNIST showing potential for even greater reductions.
The paper is well-written and easy to follow. The proposed method is conceptually straightforward: it assumes a low-rank decomposition of the weight matrix and fixes one of the matrices using prior knowledge about the data (e.g., spatial correlations in vision tasks). This can be interpreted as predicting "unobserved" parameters from learned weights via kernel ridge regression, where the kernel encodes prior knowledge about weight smoothness or topology. For cases without prior knowledge, the authors propose learning a kernel from the data.
While the idea of parameter reduction through connectivity constraints is not novel, the authors provide a solid discussion of related work in Section 5. The method is closely related to weight matrix factorization approaches, such as those used in 3-way RBMs (e.g., ref [22]) and occasionally in standard RBMs (e.g., [R1], which is missing from the paper). The novelty lies in leveraging prior knowledge to constrain one of the matrices. The method can also be viewed as a specific pooling strategy, commonly used in convolutional networks, or as representing filters as linear combinations of basis functions, where the basis is determined by the kernel. Similar representations have been explored in computer vision and signal processing literature (e.g., [R2, R3, R4, R5]), with [R4, R5] focusing on computational efficiency through basis function representations.
The experimental section demonstrates the method's practical utility and broad applicability across datasets and architectures. However, this section could be strengthened in several ways:
1. Broader Applications: The paper focuses exclusively on vision tasks, where data topology is relatively obvious. It would be valuable to explore scenarios with less apparent topology, where predicting weights is more challenging. The evaluation of the data-dependent kernel is limited and could be expanded.
2. Alternative Basis Representations: For vision tasks, the authors rely on basis functions derived from the kernel regression framework. Exploring alternative linear basis representations, such as PCA, might yield more efficient parameter reductions or other desirable properties (e.g., [R4, R5]).
3. Compelling Use Case: A more impactful demonstration of the method's practical benefits is needed. Since the motivation is to reduce computational and memory complexity, the paper should include a detailed discussion and empirical evidence showing how parameter reduction translates into savings and enables training larger, more performant networks. The current evaluation focuses on moderately large networks, showing modest parameter reductions without performance loss. For MNIST, the reduction potential is significant, but for CIFAR and STL-10, at least 40% of parameters are needed to maintain accuracy. Computational complexity and convergence speed are not addressed, which weakens the practical argument. Fig. 6-right, which compares performance against free parameters, is a step in the right direction but shows clear advantages only for CIFAR, with less convincing results for STL.
Overall, the paper presents an interesting perspective, and the ideas have potential practical impact. While related approaches exist in the literature, they are not widely adopted in the NN/deep learning community. A stronger case for the method could be made by exploring alternative implementations and providing a more compelling evaluation of its effectiveness and versatility.
Additional Comments:
- State-of-the-Art Comparisons: Including state-of-the-art results for the datasets and evaluation regimes would contextualize the reported performance and clarify whether parameter savings are achievable for competitive architectures.
- Data-Dependent Kernel: The evaluation of the data-dependent kernel is limited to MNIST and MLPs (Section 4.1). It would be helpful to test this kernel on other datasets and architectures. For Section 4.1, including the SE-rand baseline would clarify whether the learned kernel outperforms random approaches. Additionally, using the empirical kernel in both layers could serve as a control for its effectiveness.
- Low-Rank Scheme: The naive low-rank scheme, where both matrices are learned, performs poorly. Have the authors attempted to address redundancy in this parameterization? A potential extension could involve starting with a fixed U (e.g., the empirical kernel) and later allowing U to adapt, perhaps through alternating optimization of U and V.
- Pooling in Conv-Nets: The authors do not appear to use pooling layers as described in [18], which could also reduce representation dimensionality in higher layers. Additionally, Fig. 5 suggests a 5-6 percentage point performance drop when predicting 50% of parameters, which seems non-negligible. Including error bars would clarify this impact.
- Hyperparameter Optimization: How were hyperparameters (e.g., learning rate) optimized? Should kernel width vary with the fraction of learned parameters (α)?
- MLP Columns in Section 4.1: How do the columns differ? Do they use the same empirical kernel but differ in sampled pixels? Is the advantage of multiple columns due to better sampling of the parameter space?
- CIFAR vs. STL: Fig. 6-right shows a clear advantage for reduced parameterization on CIFAR but little difference for STL. Given STL's higher resolution, wouldn't the opposite be expected?
- Channel Subsampling: For higher-layer conv-nets with many input channels relative to spatial filter extent, could a data-dependent kernel subsample across channels? Currently, subsampling seems limited to spatial dimensions, with α tied across channels.
- Adaptive Parameter Learning: Could the method be extended to start with a small fraction of learned parameters and gradually increase this fraction until desired performance is achieved?
References:
[R1] Dahl et al., 2012: Training Restricted Boltzmann Machines on Word Observations  
[R2] Roth & Black, IJCV, 2009: Fields of Experts (Section 5.3)  
[R3] Freeman & Adelson, 1991: The design and use of steerable filters  
[R4] Rubinstein et al., 2010: Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation  
[R5] Rigamonti et al., 2013: Learning Separable Filters  
In conclusion, the paper proposes a method for reducing free parameters in neural networks, which could have practical utility. However, there are notable similarities to existing work, and the case for the approach could be strengthened through alternative implementations and a more robust evaluation.