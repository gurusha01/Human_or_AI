Paraphrased Review:
Summary:  
This paper introduces an inference algorithm tailored for the binary latent feature model (LFM), where each observation \( xn \) is represented as a binary vector \( zn \) of size \( K \). Here, \( z{nk} = 1 \) indicates the presence of the \( k \)-th latent feature in \( xn \). The inference task involves learning the \( z \)'s for all observations, alongside other model parameters. The proposed EM-based algorithm incorporates a shrinkage mechanism to infer the \( z \)'s and simultaneously determine the appropriate number of latent features (\( K \)). This approach shares conceptual similarities with nonparametric Bayesian latent feature models like the Indian Buffet Process (IBP). However, unlike IBP, which employs prior distributions to promote model parsimony, the proposed method leverages a model selection criterion—Factorized Asymptotic Bayesian Inference (FAB)—originally developed for mixture models and HMMs. FAB reformulates the log marginal likelihood as the sum of the expected complete log-likelihood and a model selection term, which encourages shrinkage by setting an upper bound on \( K \) and learning the correct value of \( K \) during inference. On large datasets, the FAB-based algorithm is shown to outperform alternative inference methods, including Gibbs sampling, variational inference, and MAP estimation for the IBP, in terms of both runtime and inference quality.
Quality:  
The technical foundation of the paper appears sound, and the algorithmic details seem accurate. The ability to handle large datasets while learning the number of latent features is particularly noteworthy.
Clarity:  
The methodology is explained with reasonable clarity, though certain experimental details lack sufficient explanation (as discussed below).
Originality:  
The proposed method builds on the recently introduced FAB framework for mixture models and HMMs. Its application to latent feature models is novel and represents a meaningful contribution.
Significance:  
The paper addresses an important problem: efficient inference in latent feature models, while simultaneously determining the number of latent features.
Weak Points:  
- The paper does not discuss the limitations of the proposed algorithm. Are there scenarios where it might underperform compared to standard Gibbs sampling?  
- There is no discussion of potential convergence challenges, given the algorithm's EM-like nature.  
- The method is restricted to binary latent feature models and cannot be applied to models like factor analysis or probabilistic PCA.  
- The experimental evaluation is insufficiently thorough and contains some flaws (detailed below).  
- The algorithm's behavior in the small-data regime, where the asymptotic assumptions may not hold, is unclear. This warrants additional discussion or experiments.  
- Some recent works on efficient inference in LFMs have not been cited or discussed (see comments below).  
- The MEIBP algorithm referenced in [22] is designed for non-negative linear Gaussian models, where \( W \) must be positive. Consequently, the artificial simulation and block data experiments are invalid for the MEIBP baseline. To ensure a fair comparison, the synthetic dataset should have been generated with a positive loading matrix \( W \).  
Other Comments:  
- For the block images dataset, since the noise variance is provided to the FAB algorithm, fairness dictates that the IBP Gibbs sampler should also be given the same noise variance value, rather than relying on the 0.75 standard deviation heuristic.  
- For VB-IBP [2], the infinite variational version could have been used instead of the finite version. Additionally, the experimental settings for VB-IBP lack sufficient detail (e.g., the number of restarts is not specified).  
- The accelerated Gibbs sampling reportedly discovered around 70 latent features for the 49-dimensional Sonar dataset. This result seems questionable and may stem from poor initialization or an incorrectly specified noise variance.  
- Line 70: Reference [21] pertains to MAP inference for the IBP, not Gibbs sampling. The text should be revised accordingly.  
- A recent paper, "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes" (ICML 2013), explores efficient inference in nonparametric LFMs using small-variance asymptotics. It would be beneficial to discuss this work in the paper.  
Minor Comments:  
- Line 349: For real datasets, [1,22] recommend setting the standard deviation to \( 0.25 \times \) the standard deviation of the examples across all dimensions, not simply to 0.75.  
Comments After Author Feedback:  
The author feedback addressed some of the raised concerns. However, a few critical issues still need to be resolved before the paper can be accepted:  
- The experimental methodology, for both the proposed method and the baselines, requires clearer explanations. Additionally, the paper should provide justifications for the observed behavior of certain algorithms (e.g., the fixed \( \alpha \) in the Gibbs sampler potentially leading to an overestimated \( K \), or improperly set noise variance hyperparameters).  
- Reference [21] pertains to MAP inference, not sampling. This needs to be corrected.  
- The paper should include a discussion of the MAD-Bayes inference method for the IBP.  
Overall, the proposed inference method for LFMs is compelling, particularly due to its scalability to large datasets and its ability to infer \( K \). However, the experimental evaluation requires greater rigor and careful execution.