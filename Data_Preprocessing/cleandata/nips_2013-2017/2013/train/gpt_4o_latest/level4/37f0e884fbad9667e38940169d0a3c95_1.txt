Review - Quality: 7 (out of 10)  
Clarity: 8  
Originality: 8  
Significance: 7  
SUMMARY:  
The paper addresses the problem of optimizing a smooth, strongly convex function over a convex constraint set, where the gradient mapping update can be computed efficiently. While Nesterov's optimal first-order algorithm achieves linear convergence for this problem, its convergence rate constant depends on the square root of the condition number \( k \). The authors explore a scenario where both an expensive full gradient oracle and a cheaper stochastic gradient oracle are available. They propose a hybrid algorithm, Epoch Mixed Gradient Descent (EMGD), which requires \( O(\log(1/\epsilon)) \) calls to the full gradient oracle (independent of \( k \)) and \( O(k^2 \log(1/\epsilon)) \) calls to the stochastic gradient oracle. This approach could theoretically outperform Nesterov's method when \( k \) is not excessively large.  
The key idea of EMGD is to replace a full gradient step (an epoch) with \( O(k^2) \) mixed gradient steps, combining a single full gradient computation (per epoch) with multiple stochastic gradient updates. By averaging the \( O(k^2) \) iterates within each epoch, the authors demonstrate a constant suboptimality decrease independent of \( k \), leading to a condition-number-independent number of full gradient computations. The paper provides a simple, self-contained proof of the convergence rate but lacks experimental validation.  
I found the paper enjoyable to read and appreciated the novel idea of combining a few full gradient computations with many stochastic gradient steps. This work aligns with recent efforts to leverage cheaper stochastic oracles while maintaining linear convergence rates. For instance, the SAG algorithm [16] achieves linear convergence for smooth, strongly convex objectives expressed as the sum of \( n \) simple functions (e.g., regularized empirical loss minimization, where \( n \) is the number of training examples). In such cases, the full gradient oracle is assumed to be \( n \)-times more expensive than the stochastic gradient oracle. SAG is theoretically faster than Nesterov's algorithm when \( k \leq n/8 \). In contrast, EMGD is faster than Nesterov's algorithm when \( k \leq n^{2/3} \), though it is slower than SAG in all regimes (matching SAG's complexity only when \( k \leq n^{1/2} \)).  
For example, if \( k = n^{1/2} \), both SAG and EMGD achieve \( O(n \log(1/\epsilon)) \), whereas Nesterov's method requires \( O(n^{5/4} \log(1/\epsilon)) \). EMGD has two main advantages over SAG: (1) it applies to constrained optimization problems (assuming efficient gradient mapping updates), whereas SAG is currently limited to unconstrained settings; and (2) its convergence proof is simpler, offering potential insights and making modifications more amenable to provable guarantees. Additionally, the authors' result holds with high probability, unlike SAG, which provides guarantees in expectation. The authors also suggest a potential memory and parallelization advantage for EMGD, though this is less clear since SAG can also leverage mini-batches to reduce memory requirements and enable parallelism.  
EVALUATION SUMMARY:  
Pros:  
- Proposes a novel and interesting algorithmic idea with clean, simple, and solid theoretical guarantees.  
- Achieves linear convergence for regularized empirical loss minimization without the condition number \( k \) scaling with the number of training examples \( n \). The algorithm is more general (applicable to constrained optimization) and has a simpler proof compared to SAG.  
- The paper is well-written, and the proof is self-contained.  
Cons:  
- The paper lacks experimental results to demonstrate the algorithm's practical relevance or provide concrete examples illustrating its theoretical advantages.  
- There is insufficient discussion of the algorithm's limitations and drawbacks, particularly in comparison to existing methods (e.g., SAG and SCDA). Section 3.4 should be expanded to address these points.  
- The proof would benefit from additional high-level commentary to clarify the key insights behind its construction.  
QUALITY:  
The paper is technically sound, though experiments would enhance its impact. The theoretical contribution is valuable on its own, but Section 3.4 should be extended to discuss the algorithm's limitations and drawbacks. Additionally, the authors should provide a more concrete discussion of the \( n \)-function example, highlighting differences with SAG and Nesterov's method (e.g., EMGD underperforms Nesterov for \( k > n^{2/3} \); matches SAG for \( 1 \leq k \leq n^{1/2} \); and is outperformed by SAG for \( n^{1/2} < k \leq n/8 \)). A key drawback for practitioners is that EMGD requires the number of steps per epoch to be fixed in advance (based on knowledge of \( k \)), unlike SCDA and SAG, which adapt dynamically. SCDA also features automatic step-size selection, while SAG employs an adaptive step-size heuristic that performs well in practice. These points should be discussed in the paper.  
[ADDENDUM after discussion with other reviewers:]  
a) As another reviewer noted, in machine learning, the regularization parameter is often \( C/n \), leading to a condition number \( k \approx n/C' \). In this regime, SAG outperforms Nesterov, but EMGD does not. This should be explicitly mentioned, along with examples of practical settings where \( k < n^{2/3} \).  
b) The authors should cite "[Hybrid Deterministic-Stochastic Methods for Data Fitting. M. Friedlander, M. Schmidt. SISC, 2012]," which also proposes a hybrid deterministic-stochastic algorithm with linear convergence. However, that algorithm's rate still depends on the condition number, so the current submission offers a theoretical improvement when \( k \) is not excessively large relative to \( n \).  
CLARITY:  
The paper is clear overall. I appreciated the summary in Table 1, which is presented more clearly than in prior literature. While the proof is rigorous, it would benefit from additional high-level explanations to motivate some of the defined quantities, which currently seem to fit together somewhat "magically."  
ORIGINALITY:  
This work presents a novel combination of known techniques.  
SIGNIFICANCE:  
The practical relevance of the algorithm remains unproven, but the theoretical contribution is significant. The simplicity of EMGD's proof, compared to the complex proof for SAG, is a notable achievement and could inspire further research.  
Detailed Suggestions:  
- Line 050: Clarify that the rates "are summarized in Table 1 and detailed in the related work section."  
- Line 118-119: Specify that SCDA is restricted to a specific form of \( fi \), unlike SAG, which handles any smooth convex \( fi \).  
- Line 132: Explicitly state that the condition holds "for all \( w \)" to avoid ambiguity.  
- Line 139: Explicitly mention that \( \nabla F(w) = \mathbb{E}[\nabla f(w)] \).  
- Line 224: Correct the claim that SAG and SCDA cannot leverage distributed computing; both can use mini-batches, which also reduce memory requirements.  
- Line 288: Explicitly write "(7) on (10) with \( x = w^ \) (feasible by (5)) and \( x^ = w_{t+1} \)."  
- Line 304 (and elsewhere): Add parentheses around \( (F(w_t) - F(w^*)) \) for clarity.  
- Line 366: Note that the second inequality holds for all \( T \geq 1 \) only if \( \delta \leq \exp(-1/2) \).  
Typos:  
- Line 042: "an convex" â†’ "a convex"  
- Line 056: Use "strong convexity" and "condition number" for consistency.  
- Line 088 (and elsewhere): Ensure consistent notation for the domain (e.g., \( D \) vs. \( \Omega \)).  
- Line 307: Use the norm symbol instead of absolute value.  
- Line 366: Replace "log" with "ln" for consistency.  
Update After Rebuttal:  
I am satisfied with the authors' response but will carefully verify that the revised version incorporates the suggested changes. The paper presents a novel and interesting algorithmic idea for smooth convex optimization with solid theoretical guarantees. While it lacks experimental validation, the theoretical contribution stands on its own, provided the authors expand the discussion of the algorithm's properties and limitations as outlined in this review.