This paper investigates convex norm-regularized optimization techniques for the factorization of approximately low-rank tensors. It centers on a recently introduced "latent" norm approach ([25]) and contrasts it with the previously employed "overlap" norm approach. Specifically, the authors derive theoretical bounds on the mean-squared-error behavior of the "latent" norm under various noise assumptions, demonstrating favorable comparisons to the existing analysis of the overlap norm. These results align with the empirically observed superior performance of the latent norm in prior work. The theoretical findings are further validated through numerical simulations. Additionally, the paper introduces a generalization of both norms and establishes a duality theorem for this generalized framework.
The paper is generally well-written, easy to follow, and appears to be technically sound (though I have not rigorously verified the proofs in the supplementary material). The results could be of interest to researchers working on optimization techniques for tensor decompositions. However, I have several concerns:  
(i) The contributions are narrow and highly technical, with only a tangential connection to machine learning. While the authors reasonably argue that tensor factorizations have applications in machine learning, the paper does not explore the proposed methods in the context of machine learning tasks, either theoretically or experimentally.  
(ii) The results feel somewhat incremental and lack a surprising or groundbreaking element (though this is admittedly subjective). They primarily provide additional theoretical justification for a recently proposed method ([25]), which had already been shown to perform well empirically. Furthermore, the techniques employed do not appear particularly novel.  
A minor issue is the absence of lower bounds to complement the derived upper bounds. This omission could be problematic, as the upper bounds are used to compare different algorithms and argue for the superiority of one over the other.  
Additional Comments  
-----------------  
- The abstract begins with "We propose a new class of structured Schatten norms for tensors...". This phrasing seems slightly misleading. The paper primarily focuses on analyzing two existing approaches (overlap and latent norms). The generalization is introduced briefly in Section 2.1, where its dual norm is derived (Lemma 1), but no algorithms are proposed based on these generalized Schatten norms, nor are their behaviors analyzed or tested experimentally.  
- Figure 1 is presented in the introduction but is not interpretable without the background provided in Section 2 and beyond.  
- Line 38: Consider revising "Tensor is" to "Tensor modeling is".  
- Line 40: Add a comma after "Conventionally".  
- Line 395: Consider revising "also in such situations" to "in such situations as well".  
The primary contribution of this paper lies in the theoretical error bounds for a recently proposed low-rank tensor decomposition method. While the work appears technically sound, the results are somewhat incremental and may have limited impact within the scope of NIPS.