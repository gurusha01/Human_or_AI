The paper introduces a criterion termed \(\Sigma\)-optimality for active learning in Gaussian Random Fields (GRFs). While V-optimality, previously proposed, selects query nodes that minimize the L2 loss, the authors argue that V-optimality is suboptimal for classification tasks, where a surrogate for the 0/1 loss would be more appropriate.
Although \(\Sigma\)-optimality has been introduced in prior work, it has not been applied to active learning. The paper demonstrates that \(\Sigma\)-optimality induces a submodular function, enabling a (1 − 1/e) approximation guarantee for the global optimum. Empirical results further indicate that, when using a greedy algorithm, \(\Sigma\)-optimality outperforms V-optimality and other active learning criteria.
Similar to V-optimality, \(\Sigma\)-optimality tends to select nodes with high variance or those correlated with high-variance nodes. However, based on the analysis in Eq. 3.7, the paper observes that \(\Sigma\)-optimality also favors nodes with greater "influence," which are often located near cluster centers.
Experimental results on synthetic datasets (generated under the assumed model) reveal that \(\Sigma\)-optimality outperforms both V-optimality and random selection. The results are presented for specific model parameters \(\beta\) and \(\delta\). It would be valuable to explore scenarios where \(\Sigma\)-optimality underperforms, particularly cases where random selection or V-optimality performs comparably or better. Additionally, the behavior of \(\Sigma\)-optimality on sparse or highly connected graphs warrants investigation.
The poor performance of MIG and, to a lesser extent, uncertainty-based active learning is surprising. The paper would benefit from an explanation of these results. Specifically, it would be helpful to detail how MIG was implemented and include a discussion contrasting these methods (alongside uncertainty-based approaches) with the proposed criterion.
The paper is well-written and presents a modest extension of existing ideas, particularly the application of V-optimality to the same problem and the adaptation of \(\Sigma\)-optimality from prior work. Its primary contribution lies in the improved experimental performance demonstrated. However, the reasons behind the superior performance of \(\Sigma\)-optimality compared to other methods remain unclear. Overall, the work addresses the use of \(\Sigma\)-optimality as an active learning criterion for GRFs in classification tasks—a well-studied problem. While \(\Sigma\)-optimality itself is not novel, its application to active learning represents a mild innovation. The submodularity guarantee provided is an incremental theoretical contribution.