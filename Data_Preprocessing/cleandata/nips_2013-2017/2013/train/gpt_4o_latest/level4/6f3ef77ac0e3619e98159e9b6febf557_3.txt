The paper introduces a novel generative model for images, where low-level features are first shifted and then combined through a nonlinear, stochastic masking process. The authors propose approximate inference and learning algorithms and present experimental results on grayscale image patches.
The manuscript is well-written, logically structured, and easy to follow. While the work builds on two previously explored concepts—translation invariance and occlusive image generation—it represents a somewhat incremental contribution. However, the novel approximations for inferring occluding components lead to a new parameter structure (feature weights and mask probabilities). Although the results are comparable to those from earlier feature learning algorithms, they appear promising, particularly if the model can be extended hierarchically.
The primary concern lies in the paper's focus: is the objective to generate predictions and theories relevant to biological vision, or is it to propose a new set of representations for computer vision applications?
If the focus is computer vision, the authors should clarify why this approach to occlusion is superior to other occlusive models (e.g., max-rule feature combinations, dead leaves models, or masked RBM by Le Roux, Heess, Shotton, Winn, 2011). Additionally, they should explain why translation invariance enhances tractability compared to convolutional models. As it stands, the paper offers an alternative to existing occlusive and translation-invariant models, albeit one that unifies these computations.
If the goal is to provide theoretical insights for neuroscience, the authors should highlight the specific predictions this model makes or what it explains about observed neuronal properties in the visual cortex. The prevalence of center-surround receptive fields has been previously noted and modeled, and theories for translation invariance in complex cells already exist, with some deriving directly from objective functions like information maximization or temporal stability. For this model to be impactful in the context of brain processing, it should offer novel predictions or explanations, and the discussion should address biologically implausible aspects (e.g., complete translation invariance). The neuroscientific claims should be tempered unless they can be sufficiently strengthened to provide value to experimentalists.
Additional Comments:
- What advantage does the stochastic component assignment offer over selecting pixel values using a max rule, as in (Puertas et al., NIPS 2010)? The all-or-none feature activation also seems like a limitation of the proposed model.
- Can the (feed-forward) operations in a convolutional neural network be related to approximate inference with expectation truncation? What are the specific benefits of probabilistic pooling?
- Why is so much effort devoted to computing "estimated Receptive Fields"? For visualizing and interpreting model parameters, the mask-feature product appears sufficient. Regarding biological comparisons, the translation-invariant receptive field is not an accurate characterization of complex cells, as they are not "fully translation invariant" as claimed in the Discussion. For simple-like cells, linear receptive fields are typically estimated using direct regression methods. If the model units are interpreted as populations of cells, wouldn't a convolutional network with replicated receptive fields be a more appropriate model? Additionally, new methods for characterizing translation-invariant neuron features (e.g., Eickenberg, Rowekamp, Kouh, Sharpee, 2012; Vintch, Zaharia, Movshon, 2012) might be worth citing, though data on large neural populations remains limited.
- The final paragraph mentions hierarchical extensions of the model. It would be interesting to know if the authors have specific ideas for constructing multi-layered occlusive models and what kinds of features these might extract from natural images. Would layering and transparency be interpreted similarly at higher levels, or would it simply introduce a nonlinear stochastic component to a deep model? Since the results are not markedly different from those of other learning algorithms, demonstrating the potential of model extensions is crucial.
Minor Comments:
- How are image patch boundaries handled during translation?
- Are the masks constrained to be nonnegative? This is not explicitly stated in the text.
- What is the motivation for prefiltering with center-surround? While this is comparable to the linear transformation performed in the LGN, receptive fields are typically derived experimentally by correlating with pixel stimuli, not LGN outputs.
- A brief summary of all approximations used to make the model tractable (e.g., expectation truncation, independent pixel occlusion) would be helpful.
- Why do all globular components have positive centers?
In summary, this is a clearly written paper describing a somewhat incremental advance that combines two previously developed ideas. The results suggest that the learning algorithms can capture interesting structures, but so far, the authors have primarily replicated features learned by other models.