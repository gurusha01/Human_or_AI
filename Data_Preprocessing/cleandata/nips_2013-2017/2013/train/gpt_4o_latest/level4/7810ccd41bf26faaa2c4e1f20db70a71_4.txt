The paper examines two optimization objectives for active learning on Gaussian Random Fields (GRFs): Sigma-Optimality and a V-Objective (previously proposed but not analyzed in detail). The authors demonstrate that the maximization versions of both objectives exhibit submodularity, enabling approximation guarantees for greedy algorithms. Additionally, they establish that the covariance matrix of GRFs satisfies the suppressor-free condition.
The paper provides some valuable theoretical insights, the most notable being that the covariance matrix of GRFs is suppressor-free. The concept of suppressors adversely affecting subset selection and the role of their absence in enabling strong performance guarantees for greedy algorithms has been explored in prior work. However, it was previously unclear whether there existed a general class of covariance matrices satisfying this condition. The authors' result that GRFs meet this criterion is both theoretically and practically significant.
It is somewhat unclear how novel the Sigma-Optimality criterion is, as it appears to have been introduced earlier in the context of Active Surveying. Nonetheless, I appreciate the authors' rigorous analysis of this optimization problem and their derivation of approximation guarantees through submodularity.
That said, the paper's writing could be improved, as there are instances of technical imprecision (though these do not undermine the correctness of the analysis). Specifically:
- The paper is inconsistent in distinguishing between the minimization and maximization versions of the problem, frequently switching between the two. The greedy multiplicative performance guarantees and submodularity results apply only to the maximization objective, yet the paper occasionally implies they extend to the minimization objective (e.g., at the beginning of Section 2.3).
- In the first paragraph of page 4, the claim that calculating the global optimum is intractable due to submodularity is misleading, as submodularity does not make the problem intractable, nor does it necessitate a greedy solution.
- In the last paragraph of page 4, the suppressor-free property does not "prove" 2.3, as 2.3 is merely a definition.
- The proper citation for the greedy bound in submodular maximization is [Nemhauser et al., 1978], not Streeter and Golovin.
Regarding the experiments, it is surprising that the Mutual-Information criterion performs worse than Random. 
In summary, the authors analyze two active learning criteria for GRFs and provide performance guarantees for greedy algorithms by leveraging the submodularity of the objectives. They also establish that GRFs satisfy the suppressor-free condition. Overall, this is a solid paper with meaningful theoretical contributions, though the imprecise notation and phrasing highlighted above should be addressed.