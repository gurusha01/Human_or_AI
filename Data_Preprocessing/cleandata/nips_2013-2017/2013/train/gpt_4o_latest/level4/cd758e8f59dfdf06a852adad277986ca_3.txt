This paper provides an analysis of a k-nearest neighbor-based algorithm recently introduced by Chaudhuri and Dasgupta, specifically for scenarios where the data lies on or is concentrated near a manifold. The main finding is that the convergence rate is influenced by the dimensionality of the underlying manifold. To extend the results to the manifold setting, the authors adapt the theoretical framework of Chaudhuri and Dasgupta using sampling techniques from the work of Niyogi, Smale, and Weinberger. For the case where the data is concentrated near a manifold, they leverage tools developed across a series of papers by Rinaldi and collaborators.
The theoretical framework appears sound, and the results are compelling. While the algorithm analyzed might be better suited for stratified spaces rather than strictly manifolds, this is a relatively minor critique. The primary drawback of the paper is the lack of novelty in both the algorithm and many of the theoretical tools employed. That said, this limitation is common among many theoretical papers at NIPS. Overall, the authors provide a rigorous theoretical analysis of cluster trees on manifolds, demonstrating that the convergence rate is determined by the manifold's dimension.