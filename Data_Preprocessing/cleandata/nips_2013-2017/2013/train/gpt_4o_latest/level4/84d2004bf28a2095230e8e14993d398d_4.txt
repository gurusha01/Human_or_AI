This paper introduces a novel algorithm for large-scale submodular function maximization. The primary contribution is a distributed algorithm implemented using MapReduce, where the ground set is partitioned, and each machine processes a subset of the ground set. The partial solutions are then merged to produce an approximate subset. The authors provide approximation guarantees for their method and demonstrate that these guarantees are tight. Additionally, they explore the performance of the algorithm under specific assumptions about the datasets or the submodular function.
Overall, I believe the authors tackle a highly challenging problem with significant potential utility in large-scale real-world applications. I also find the experimental validation to be thorough and comprehensive. Furthermore, I appreciate the difficulty of obtaining strong performance guarantees for such problems without introducing additional assumptions.
However, I found the theoretical analysis somewhat underwhelming. While the guarantees are shown to be tight in the worst case, they appear quite weak. This is particularly evident for unfavorable choices of partitions \( V1, V2, \dots \). I was expecting some dependence on the choice of distributions or at least a heuristic for selecting distributions that might work well. The main guarantee (Theorem 5.1) suggests an approximation factor that scales almost linearly with \( m \) and \( k \), which is discouraging. As the proof technique and intuition suggest, a simple modular upper bound approximation already achieves a factor \( k \). Given this, it is unclear how GREEDI performs theoretically compared to the modular upper bound approximation, especially for large \( m \), which is practically relevant. While I suspect the modular upper bound approach would perform poorly in practice, this comparison is not immediately evident from the theoretical results.
One critical issue, which is only loosely addressed in the paper, requires clarification. Many practical submodular functions are graph-based (e.g., the graph cut function and the exemplar-based clustering objective from Section 3.1). A minor clarification here is that the exemplar-based clustering objective is essentially a facility-location objective with a similarity function replacing the distance function. For such functions, evaluating the objective requires summing over all elements in \( V \), even if the subset being evaluated is much smaller. This raises concerns about the feasibility of the algorithm in cases where the dataset is too large to fit into a single machine (as noted in lines 122-127). Each machine would still need to compute the outer sum over \( V \) for graph-based objectives like facility location. It is unclear how this would be handled. While distribution might reduce computation time, it is not evident how it would address memory constraints. One potential solution could involve evaluating the function only on subsets \( Vi \) (i.e., restricting the outer sum to \( Vi \)), but this would alter the submodular function and invalidate the guarantees. This is a significant issue that should be addressed in the rebuttal.
I also have a few minor suggestions. The proof of Theorem 4.1, particularly the tight instance, is difficult to follow and could benefit from better exposition. Additionally, it would be helpful to compare the performance of GREEDI against the standard greedy algorithm on smaller datasets to evaluate its relative performance against the best serial algorithm. Including a timing analysis and memory usage comparison in the experimental results would also strengthen the paper.
In summary, this paper addresses a novel and challenging problem of distributed submodular maximization, with the potential for significant practical impact. However, the theoretical contributions are relatively weak.