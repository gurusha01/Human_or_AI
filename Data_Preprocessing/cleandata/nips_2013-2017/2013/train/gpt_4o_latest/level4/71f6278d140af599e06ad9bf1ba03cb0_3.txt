This paper provides a mathematical analysis of the dropout procedure in deep neural networks. To the best of my knowledge, this is the first formal attempt to rigorously prove the somewhat heuristically employed dropout technique. While prior work has hinted (particularly in the shallow case) that dropout performs a form of averaging—specifically geometric averaging—this paper is the first to establish this property for deep neural networks. Furthermore, it demonstrates that the normalized version of the weighted geometric average yields better approximations compared to the conventional geometric average. Notably, the derivations presented in Equations 11, 12, and 13 represent significant contributions of this work and are likely to have a substantial impact on future theoretical studies of deep neural networks.
Minor Comments
- Eq. 7: The factor \( c \) should appear before the exponential term in the penultimate term. Additionally, there is a missing closing bracket at the end of the expression.
- Eq. 25: Extract the parentheses following the differential for improved clarity.
- Eq. 26: It seems that \( \lambdai \) might actually be \( \deltai \). Please verify this.
- The claim regarding \( p_i = 0.5 \) providing the highest level of regularization was not entirely clear. The authors should elaborate on this point in greater detail, as it is an important observation that supports the current heuristic used in dropout.
- Figure 3 (page 8): The figure appears too small. Consider reorganizing the figures on page 8 to improve their layout and eliminate the unnecessary space between them.