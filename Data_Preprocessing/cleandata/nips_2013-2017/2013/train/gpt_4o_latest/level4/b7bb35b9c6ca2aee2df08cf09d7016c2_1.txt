The paper presents a bounded-staleness parameter server design for iterative distributed learning, leveraging vector clocks. The concept is clear, intuitive, and effectively spans a parameterized spectrum between prior designs for fully asynchronous updates (e.g., Hogwild or Yahoo LDA) and methods with strict synchronization barriers (e.g., recent SGD approaches). While the work does not introduce groundbreaking theoretical insights, it appropriately extends the analysis from [17]. The paper is exceptionally well-written, with the SSPtable design explanation standing out as exemplary.
The experimental results are compelling, particularly the computation-vs-communication trade-offs and clocks/worker/time metrics, which are highly encouraging. However, there are a few areas where further details or improvements would enhance the work:  
- The Lasso results are limited to a synthetic toy dataset, and a more comprehensive evaluation would significantly strengthen the paper.  
- A direct comparison with state-of-the-art distributed learning frameworks (e.g., Yahoo LDA, Hogwild) would provide valuable context.  
- Including a single-worker baseline would offer a useful point of reference.  
- The data partitioning strategy should be explicitly described.  
A couple of additional suggestions:  
- For matrix factorization, consider whether the approach could be integrated with Gemulla et al.'s method, which uses sub-epochs operating on non-overlapping parameter blocks sequentially.  
- It might be worth discussing the feasibility of allowing the slowest stragglers to synchronize with incomplete iterations if doing so avoids blocking under the current staleness level.  
Overall, the paper is well-written, presents an intuitive idea, and delivers convincing experimental results, with only minor areas for improvement.