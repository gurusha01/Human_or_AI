The authors introduce a generalization of the Bayesian nonparametric stochastic block model to a hierarchical framework, enabling the inference of a hierarchy rather than a flat clustering of entities based on observed links between them. They develop a learning algorithm grounded in agglomerative clustering to identify a tree structure that represents valid partitions of the entities. Empirical evaluations on two social connectivity datasets are presented, demonstrating that the proposed method is both faster and more accurate than the Infinite Relational Model (IRM), a widely-used approach for such tasks.
This paper addresses an intriguing problem that is likely to appeal to a substantial portion of the NIPS audience. From a technical perspective, Bayesian nonparametric methods are conceptually appealing but often computationally expensive in practice. From an application standpoint, social scientists are increasingly leveraging sophisticated Bayesian models for data analysis, making a hierarchical extension to a popular network-discovery model particularly relevant. This work effectively intersects both of these important areas.
The central insight of the paper lies in restricting the allowed partitions to conform to a tree structure, enabling efficient computation of the marginal likelihood of the data. In contrast, the IRM must consider an exponential number of possible partitions. However, I found the connection between the tree-learning procedure and the generative Bayesian model somewhat unclear. Given that the learning procedure is greedy, it does not seem to perform true posterior inference over a distribution of trees or yield a point MAP or ML estimate. I may have misunderstood this aspect, but I recommend that the authors clarify this connection in Section 4.
The experimental results are impressive. Across various performance metrics (AUC, predictive log-likelihood, accuracy), the proposed method performs comparably to or slightly better than the IRM, its closest competitor. While the absolute improvements in these metrics are modest, the claimed runtime advantage—orders of magnitude faster than IRM inference—is particularly noteworthy.
That said, the overall clarity and quality of writing could be significantly improved. First, there are several distracting spelling and grammatical errors (e.g., line 126). Second, the prose is occasionally difficult to follow. For example, lines 47–50 in the introduction contain vague phrasing (what exactly are "local changes" in this context? What do "iterations" refer to—iterations of an MCMC algorithm?). Additionally, the datasets used (Monastery networks and NIPS authorship) are not clearly described, which could confuse readers unfamiliar with them. Some mathematical details, such as Equation 5, feel unnecessarily dense, and Figure 3 is difficult to interpret—readers unfamiliar with the monastery dataset may struggle to discern which method performs best. A more familiar dataset might improve accessibility. Lastly, Section 4, which presents the paper's key contribution, required multiple readings to fully grasp.
If these issues are addressed, I believe the paper's novelty and strong results justify its publication. The authors propose an innovative model of graph edge structure that can be interpreted as a Bayesian adaptation of agglomerative clustering inspired by the IRM. While I found the core learning algorithm insightful and the experimental results compelling, the writing quality requires substantial improvement.