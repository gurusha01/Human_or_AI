The paper introduces the first reinforcement learning (RL) controller in the literature that achieves performance comparable to black-box optimization methods in the game of Tetris. This topic is of significant interest to the RL community, as Tetris has long been a challenging benchmark for RL algorithms.
Quality  
The primary contribution of the paper lies in applying a previously established algorithm (CBMPI) to solve the game of Tetris. The authors demonstrate competitive results compared to the prior state-of-the-art optimization algorithm for Tetris, the Cross-Entropy (CE) method. The empirical findings are compelling, and the observed improvement over earlier attempts to apply RL controllers to this domain is noteworthy.
However, the paper lacks an in-depth discussion of the (potential) reasons behind this improvement. While the authors posit in the introduction that the key hypothesis (validated by the paper) is that searching in the policy space rather than the value function space is crucial, further elaboration would enhance the paper. Specifically, the following points could be addressed:
1. In lines 252–253, the authors describe the state distribution used for sampling rollout states, which is derived from executing a high-performing Tetris controller. It would be valuable to discuss how much this choice of distribution impacts CBMPI's performance. If alternative distributions were tested and found less effective, reporting these findings would benefit researchers attempting to replicate the results. For instance, it would be insightful to know whether the states visited by CBMPI during learning form a reasonable rollout distribution.
2. The optimization algorithm employed in the "Classifier" stage of CBMPI is CMA-ES. As noted in "Path Integral Policy Improvement with Covariance Matrix Adaptation" (ICML 2012), CMA-ES belongs to the same family of algorithms as CE. This raises an interesting point: an Approximate Dynamic Programming (ADP) algorithm that incorporates a CE-like algorithm in its "inner loop" achieves performance comparable to CE when directly searching in the policy space. A natural question arises: what would happen if CMA-ES were replaced with other types of optimizers (e.g., gradient-based methods)?
Clarity  
The paper is well-written, easy to follow, and clearly articulates its contributions. However, I have a few minor comments:
- The definition of the "Classifier" in line 222 is unclear. Shouldn't the output be "labels" rather than differences in Q-values?  
- In lines 266–267, CMA-ES is referred to as a "classifier," but it is actually an optimization algorithm.  
- It is unclear why graphs 4a and 4d are disjoint, while Figure 5d includes a performance comparison for all algorithms.  
- In Section 3.1, the word "Experimental" is misspelled.  
Originality and Significance  
The paper addresses a challenging problem in a novel manner and provides valuable insights into applying RL algorithms to non-trivial domains. While it does not introduce new algorithmic or theoretical tools, the results are significant for the RL community, as discussed earlier.
Questions and Suggestions  
Finally, I have a few questions and suggestions for the authors:
1. If the paper is accepted, are the authors willing to share the code to facilitate the reproduction of their experiments?  
2. Given the high variance of scores in Tetris, the graphs should be updated to include confidence intervals for the mean values. Are the differences observed in the graphs (e.g., Figure 4d) statistically significant?  
3. Regarding the claim in lines 393–395, what happens if CBMPI is provided with the same number of samples as CE? Would its performance improve, or is it already at a plateau, where additional samples would not yield further gains?  
4. Concerning the comparison of the best policies, how were the scores of the DU and BDU policies computed? Were these values taken directly from the referenced paper ([20]), or were the policies executed using the authors' codebase? Additionally, what is the performance of the best policy discovered by CE in the authors' experiments? Clarifying this would ensure that the results are thoroughly reported.
Conclusion  
The paper would benefit from a more detailed discussion of the results to maximize its impact and utility for the community. Nonetheless, it represents a meaningful contribution to the literature by shedding light on a previously unexplored phenomenon: the subpar performance of RL algorithms in the game of Tetris.