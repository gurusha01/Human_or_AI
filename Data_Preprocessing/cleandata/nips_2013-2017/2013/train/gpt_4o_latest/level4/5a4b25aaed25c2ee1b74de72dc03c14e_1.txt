The paper aims to enhance the mixing rate of Gibbs sampling in pairwise Ising models with strong interactions, which are notoriously "slow-mixing." The authors propose several projections to "fast-mixing" models, where parameters are adjusted to be as close as possible to the original model while satisfying a spectral bound that guarantees rapid mixing. Experimental results demonstrate certain regimes where this approach improves marginal estimates within a given computation time, outperforming variational methods (mean field and belief propagation variants) and Gibbs sampling applied to the original model.
The technical framework is largely built on prior results that establish conditions for rapid mixing [4,8]. However, the novel contribution lies in the idea of projecting onto the nearest rapidly mixing model as an approximate inference method, which, to my knowledge, has not been explored before.
On the domain of "toy tiny Ising models," the results show notable improvements over strong baselines, including standard sampling and contemporary variational methods. My primary concern is whether these results will scale to larger models. The experiments focus on comparing mixing times across different models, but it is unclear how these comparisons will extend to larger problem sizes. Additionally, the projection step required to construct the fast-mixing model appears computationally expensive, yet its cost does not seem to be accounted for. Finally, generalizing this approach beyond binary states is highly uncertain, as deriving convergence bounds for more complex cases is significantly more challenging.
CLARITY:  
Overall, the paper is clear and reasonably accessible, given the technical depth. However, certain sections require reorganization and clarification:  
- The paper frequently references "standard results" and includes them as theorems (e.g., Theorem 6) without proper citations or proof sketches. Additionally, Lemma 5 is left unproven, with the cited reference only addressing a special case of zero-field.  
- The dependency matrix \( R \) is not adequately discussed. For example, it seems computationally intractable due to the maximization step, but this is not explicitly stated. (This may explain the reliance on Lemma 5.)  
- The second half of Section 4 is difficult to follow. Terms like \( g \), \( M \), and \( \Lambda \) are referenced before being introduced, and Theorem 7 uses unexplained notation. If this optimization is to be included, it requires more thorough explanation. (Reducing the time spent restating results from [8], which are not directly utilized, could help.)  
- In Section 1, the paper uses \( KL(q||p) \) for both directions of KL divergence, which is inconsistent. Furthermore, in the first paragraph, \( q \) represents the true distribution and \( p \) the approximation, which is the opposite of conventions in most related literature.
EXPERIMENTS:  
The time comparison in Figure 2 appears to exclude the computation time for the projection step. While Section 6.1 ambiguously mentions this, it is unclear and needs clarification. If projection time is indeed excluded, results with this time included should be presented. As it stands, the proposed method benefits from the output of a sophisticated variational optimization without penalty, which diminishes the credibility of the improvement over standard Gibbs sampling.
It is also disappointing that no experiments were conducted on larger models, especially since Gibbs mixing times are known to depend on problem size. There are feasible ways to run experiments in regimes where junction tree is intractable, such as using models with symmetries that allow true marginals to be computed or treating the output of a very long sampling run as ground truth.
While \( KL(\theta||\psi) \) is generally intractable, it would be valuable to explore it as a "best-case" scenario for how sampling in an approximate model might perform. For the toy models used in the paper, junction tree could be employed to evaluate this.
The paper notes that mean field is a degenerate case of the reverse KL projection, yet there is a significant gap between mean field error and the error from reverse KL projection. This discrepancy warrants further discussion.
The idea of approximating slow-mixing models by projecting to the nearest fast-mixing model is compelling, and the authors leverage recent advances in mixing bounds in an elegant manner. However, concerns remain about the experimental comparisons and the limited applicability of the approach to a broader range of models.