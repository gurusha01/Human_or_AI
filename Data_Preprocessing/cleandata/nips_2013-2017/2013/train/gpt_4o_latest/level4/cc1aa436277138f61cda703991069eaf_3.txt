This paper addresses the problem of determining the optimal fraction of control questions (with known answers) versus target questions (with unknown answers) when using crowdsourcing to estimate continuous quantities (as opposed to categorical judgments). The authors propose two models for leveraging control questions to estimate worker parameters, such as bias and variance relative to true answers. The first is a two-stage estimator that relies solely on control items to estimate worker parameters, while the second is a joint estimator that uses both control and target items to derive a maximum likelihood (ML) estimate. The paper provides analytical expressions for the optimal number of target items (k) in each case, starting with a clear presentation of the results followed by detailed and transparent derivations. The authors validate their findings empirically using both synthetic and real datasets, demonstrating how the estimates align with the true optimal k in cases where the model perfectly matches the data and exploring the effects of model misspecification. They conclude with practical recommendations for practitioners on selecting between the two models.
This is a highly relevant and practical contribution. As someone with extensive experience in running crowdsourced tasks and engaging with others in the field, I can attest that control questions are a well-established method for estimating bias and variance in worker judgments. However, much of the prior theoretical work has overlooked the availability of control questions, assuming instead that they are absent. While existing methods could theoretically incorporate control questions, I am unaware of any prior work that systematically investigates their value or determines the optimal number to use in specific scenarios.
This question has often arisen in my own experiments, and I have even considered pursuing this line of research myself. I was therefore thrilled to read this comprehensive and thoughtful treatment of the topic. The paper excels in numerous ways: it is exceptionally clear in its writing, from the initial motivation and problem setup to the explanation of the authors' approach and its purpose, through to the derivations, experimental design, and discussion of results. The related work is thoroughly covered, the figures are highly informative, and the notation and methodology are easy to follow. The estimation algorithms and the derivation of the optimal k are well-presented, and the discussion of model misspecification and its practical implications is both insightful and actionable. The recommendations for real-world applications are particularly valuable. 
I would not hesitate to recommend this paper to colleagues working on crowdsourcing analysis, as well as to practitioners who rely on crowdsourcing in their work. This is an outstanding paper that I expect will be highly regarded at the conference and widely cited in the years to come. It provides a novel and practical investigation into the value of control items, a topic of significant importance. The paper is exceptionally well-written, rigorously supported by experiments on both synthetic and real datasets, and offers practical insights that will benefit both researchers and practitioners alike.