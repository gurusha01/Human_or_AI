The authors build upon the work of [Schwing et al., Efficient Structured Prediction with Latent Variables for General Graphical Models; ICML 2012] by incorporating active learning protocols. Specifically, they leverage the entropy of local variable marginals to quantify uncertainty, effectively framing this as local variable uncertainty sampling for structured predictions with partially observed variables. To achieve this, they propose two active learning approaches: (1) separate active, where each active learning round involves a "separate" inference step over the unlabeled and partially labeled variables following training on the observed variables, and (2) joint active, where each active learning round employs a joint learning process over the labeled, partially labeled, and unlabeled instances. Additionally, they investigate the effects of batch sizes and warm-starting the learning process between rounds. Their empirical evaluation focuses on the 3D layout of rooms (a computer vision problem previously explored in [Schwing et al., ICML 2012]), demonstrating that (1) the "joint" active learning approach performs best, achieving annotation savings of approximately 90%, (2) batch mode is effective for reasonably small batch sizes, (3) querying "partial" labels outperforms querying "full" labels (as expected), (4) sensitivity to \(\epsilon\), and (5) computational reuse reduces runtime (also expected).
Overall, this is a well-executed paper that demonstrates the efficacy of active learning in a challenging setting using this specific formulation. Below, I address the components of {quality, clarity, originality, significance} in detail.
Quality:  
The paper is of high quality. The structured learning model with latent variables is state-of-the-art, and the active learning approach achieves significant annotation savings compared to random sampling. This is particularly intuitive in the latent variable setting. However, I have two concerns from an active learning perspective. First, the savings presented might be somewhat misleading (albeit unintentionally). While the "$1 for 1 label" simulations were popular in earlier active learning research (~1995–2008), they are now considered less realistic because active learning often selects the most challenging examples for labeling (e.g., [Kapoor et al., IJCAI 2007], [Settles et al., NIPS WS 2008], [Ringger et al., LREC 2008], [Haertel et al., NIPS WS 2008], [Arora et al., NAACL WS 2009], [Baldridge and Palmer, EMNLP 2009], [Wallace et al., IHI 2010], among others). A discussion of this issue would strengthen the paper, though the experiments involving "full" vs. "partial" labels do hint at this. Second, there is a semantic issue regarding partial labels during querying. Both [Culotta and McCallum, AAAI 2005] and [Roth and Small, ECML 2006] address partial labels during querying, which contrasts with the claim in the last paragraph of page 1. It seems the authors intended to state that the output space is fully observable, rather than requiring "full" labels from the querying function.
Clarity:  
The paper is generally clear, though understanding some details required revisiting [Schwing et al., ICML 2012]. The following points could improve clarity:  
1. The "task-loss" function was not immediately clear until I referred to the prior work. Including examples or additional discussion would help.  
2. The notation for the loss function appears inconsistent, with some confusion between \(s\) and \(y\) (assuming my interpretation is correct). This may stem from copying/pasting from the ICML paper.  
3. The paper does not explain \(\ell^c\), which could be challenging for readers without a background in graphical models.
Originality:  
While the proposed approach is a straightforward extension of uncertainty sampling, it has not been applied in this specific setting before. That said, the combination of these elements could likely be assembled by someone familiar with the relevant areas.
Significance:  
This paper is unlikely to have the same impact as [Schwing et al., ICML 2012], as it serves more as a "companion piece." The ideas are relatively straightforward but are executed well. The lack of theoretical results and the practical challenges of setting \(\epsilon\) in real-world deployments limit its broader significance. However, the results are still interesting, and releasing the code would likely be more impactful than the paper itself.
Minor Comments:  
- Abstract: Replace "∼ 10%" with "\(\sim 10\%\)" to fix the spacing.  
- Page 1: Change "have to deal with exponentially sized output variables" to "have to deal with output spaces that are exponentially sized in the number of local variables."  
- Page 2: The distinction between "labeled" and "observed" could be clarified, though this is at the authors' discretion.  
- Page 5: Correct "as well as the messages \(\lambda\) and believes" to "as well as the messages \(\lambda\) and beliefs."
In summary, the authors extend the work of [Schwing et al., ICML 2012] by introducing active learning protocols based on the entropy of local variable marginals. The approach performs well empirically on a challenging task and is likely to generalize to similar problems. While there are minor issues that could be addressed to strengthen the paper, it provides a useful contribution, even if it is a relatively straightforward extension of [Schwing et al., ICML 2012].