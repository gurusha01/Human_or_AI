This paper investigates the use of a classifier-based approximate policy iteration algorithm, CBMPI, in the context of Tetris, a widely studied benchmark task in reinforcement learning. CBMPI is evaluated against DPI, a variant that omits the regression step for value function estimation in CBMPI; lambda-PI, a related policy iteration method that employs eligibility-trace-like backups and excludes CBMPI's classification step; and CE, a policy-search method that currently represents the state of the art in Tetris. Experiments are conducted on both small and large boards with varying state features, and the best policies for each method are further assessed over 10,000 games.
While the paper does not provide any theoretical or algorithmic innovations, its empirical findings have the potential to be highly impactful. Given the extensive study of this benchmark task, achieving a significant improvement over the state of the art is a challenging and noteworthy accomplishment. Moreover, since the state-of-the-art CE method is known for its high sample complexity, demonstrating strong performance with more sample-efficient value-function-based methods is of considerable interest, particularly if the results offer insights into why prior value-function methods have struggled.
However, I have two primary concerns. First, the paper's attempt to address the "why" behind the results is inadequate. Second, the experimental comparisons suffer from confounding factors that significantly weaken the validity of the conclusions.
Regarding the "why," the authors hypothesize that the success of CE suggests that policies in Tetris may be easier to represent than value functions. This is a plausible hypothesis and aligns with the common belief that good policies can sometimes be simpler to represent than accurate value functions, potentially giving policy-search methods an advantage. However, the authors then argue that this hypothesis implies the need for ADP methods that search in policy space rather than value-function space, proposing CBMPI as an example. This reasoning is flawed because CBMPI, as an actor-critic method, explicitly represents both the value function and the policy, meaning the challenges of representing a value function are not circumvented. (This is true of all ADP methods, as methods that do not explicitly represent the value function are, by definition, policy-search methods.) While CBMPI does perform a search in policy space for a classifier-based policy consistent with estimated Q-values, the paper provides no compelling argument as to why this approach would make Tetris easier to solve. The hypothesis that Tetris policies are easier to represent than value functions does not logically lead to this conclusion. Although the superior performance of CBMPI over lambda-PI, which does not explicitly search in policy space, could support the authors' claim (albeit without addressing the "why"), confounding factors in the experimental comparisons (discussed below) undermine the credibility of this evidence.
Regarding the confounding factors, I have the following concerns about the experiments:
1. In Section 2.2, it is stated that CBMPI relies on rollouts starting from a set of states sampled from a distribution Î¼. This raises concerns about comparisons with CE, as CBMPI requires a generative model, whereas CE only requires a trajectory model. Furthermore, in Section 3.1, it is revealed that the states are sampled from trajectories generated by a strong Tetris policy, DU, assumed to be available as prior knowledge. However, Appendix A indicates that lambda-PI was not given access to this prior knowledge and instead used a Dirac distribution centered on the empty board state. Similarly, CE was also deprived of this prior knowledge. While it is less straightforward to incorporate this initial policy into CE, one could imagine seeding the initial Gaussian distribution based on it. This discrepancy raises serious concerns about the fairness of the comparisons between CBMPI, lambda-PI, and CE.
2. In Section 3.2.1, the small-board experiments using the D-T features reveal that CBMPI's performance was optimized by adding 5 RBF height features to the D-T features. However, as far as I can tell, the other methods were evaluated using only the D-T features, making the comparisons in Figure 4 potentially biased. In Figures 5a-c, where all methods use the same Bertsekas features, CBMPI shows no performance improvement. In Figure 5d, CBMPI again demonstrates an advantage, but it appears (though the paper is not explicit) that CBMPI uses D-T+5 features while the other methods use only D-T.
3. Regarding Figure 5d, while CBMPI outperforms DPI after 4 iterations, DPI performs better between iterations 2 and 4. Arguing that final performance is the most important metric (and thus CBMPI is superior to DPI) is problematic, as CE ultimately surpasses both in the long run. While it is fair to note that CBMPI learns faster than CE in the short term, it is misleading to claim that CBMPI achieves comparable performance with fewer samples, as this is only true at specific points on the learning curve. CBMPI is faster but does not match CE's final performance.
4. Tetris is known for its high variance in scores for a given policy, as a few random events can drastically affect game duration. However, the paper does not include any variance analysis for the experimental results, leaving us unable to assess the statistical significance of the performance differences reported in Figures 4 and 5. This is particularly concerning for the results in Table 1, as policies capable of clearing millions of lines per game likely exhibit substantial cross-game variance.
Minor comment: Figures 4 and 5 would be more interpretable if the x-axis represented samples rather than iterations, allowing for a direct and fair comparison of the methods.
This paper does not present a theoretical or algorithmic contribution but reports strong empirical results for value-function-based methods on Tetris, an important reinforcement-learning benchmark where policy-search methods have traditionally excelled. However, the authors' explanation for these results is unconvincing, and the experimental comparisons are compromised by confounding factors, undermining the validity of the conclusions.