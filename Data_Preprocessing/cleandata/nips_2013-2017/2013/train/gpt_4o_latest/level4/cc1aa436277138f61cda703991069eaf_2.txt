The paper explores various approaches to achieving consensus labeling in a crowdsourcing context, focusing on the specific scenario where a real-valued quantity must be estimated, such as by averaging responses from multiple contributors. 
When individuals exhibit fixed biases and some ground truth values are available, the bias can be estimated either using only the true values or by leveraging all labels provided by the user. 
The authors present theoretical results under this particular data model for these two estimation schemes, aiming to determine the number of true values required for robust estimation. 
The theoretical analysis appears rigorous and aligns reasonably well with empirical results from simulations. Notably, as one might intuitively predict, the joint estimation scheme becomes asymptotically more favorable as the number of questions increases. 
Overall, the paper offers a well-rounded contribution, combining theoretical insights with empirical validation on a timely topic. However, several questions about the model's assumptions and applicability remain, and the authors could benefit from addressing these aspects in the paper. 
Some broader questions about the model include:  
1. Relevance: Are there many real-world "real-valued estimation" problems that could genuinely benefit from crowdsourcing? The authors mention forecasting as a potential application, but would the proposed bias or bias-variance model be empirically valid in such scenarios?  
2. Model Structure: Consider a simplified scenario where all workers share a common bias, meaning the crowd-average is consistently offset by the same amount. In such cases, these estimation schemes may be unnecessary or could be significantly simplified. How would this simplified model perform in practice? For instance, the football dataset suggests that a "variance-only" model might be more suitable.  
3. Model Appropriateness: Determining the right model is inherently challenging. What strategies would the authors recommend for identifying the most appropriate model? Would a larger controlled experiment or alternative methods be more effective?  
The paper provides theoretical insights and empirical validation for two specific models of consensus labeling, addressing the critical question of how many pre-labeled items are necessary for reliable consensus. It represents a strong combination of theoretical and empirical work on a relevant and contemporary problem.