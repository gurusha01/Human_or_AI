The authors propose two equalities to derive estimators for the Radon-Nikodym derivative \( f = p/q \):  
(i) \( \mathbb{E}p[k(x, \cdot) f] = \mathbb{E}q[k(x, \cdot)] \) and  
(ii) \( \mathbb{E}_p[k(x, \cdot) f] \sim q(x) \).  
Empirical versions are used to replace the integral operators. The set of test functions, \( \{k(x, \cdot)\} \), is chosen such that the discrepancy between the left and right sides is minimized. This choice has the advantage of enabling the use of standard kernel methods for optimization.  
The paper provides convergence guarantees for both settings under smoothness and boundedness assumptions on the densities. The convergence rate is shown to depend significantly on the dimensionality of the underlying space, as is typical for density estimation problems.  
Overall, I find this to be a well-executed and valuable contribution.  
A few comments:  
- On convergence rates: Optimizing over the set \( \{k(x, \cdot)\} \) ensures convergence to the ratio. While this is intuitive for (ii) due to the bandwidth dependence, it would be helpful to include a brief intuitive explanation for why this also holds for (i).  
- Choice of cost function: On page 2, optimization is performed over \( L2, p \), but in equation (9), a combination of \( L2, p \) and \( L_2, q \) is used. Some motivation for this choice of cost function would be beneficial. Presumably, the choice is tied to the tasks of interest, so it would be helpful to explicitly link the cost function to specific applications.  
- On applications: One example provided is importance sampling, where \( q/p \) is relevant for transitioning from \( \mathbb{E}q \) to \( \mathbb{E}p \). However, since the empirical cost function involves estimating both \( \mathbb{E}p \) and \( \mathbb{E}q \), why not directly use \( \mathbb{E}q \)? Can \( \mathbb{E}p(q/p) \) provide any advantage over directly using \( \mathbb{E}q \)? A clearer motivation for the setup would be helpful, especially since the authors note on page 5 that sampling from \( \mathbb{E}q \) is challenging. Is this the primary application the authors have in mind?  
- Regularization and smoothness: The regularizer enforces smoothness on the ratio \( q/p \), while the theorem assumes smoothness for \( q \) and \( p \), which seems natural. Is there a relationship between the smoothness of \( q \) and \( p \) and the smoothness of their ratio? For instance, does smoothness of \( q \) and \( p \) (possibly bounded away from zero) imply smoothness of the ratio? Alternatively, does smoothness of the ratio imply the existence of smooth \( q \) and \( p \) that produce it? While the answer may already be implicit (e.g., different \( q \) and \( p \) can yield the same ratio), a brief discussion would be valuable.  
- Dimensional dependence: It would be useful to discuss the dependence of the convergence rates on the dimensionality \( d \). For which dimensions do the authors expect their method to be practically effective?  
- Test function family: Is there a compelling motivation for the choice of the test function family \( \mathcal{U} \)?  
- Broader perspective: In the context of estimating \( q/p \) for covariate shift and related problems, one of the key challenges is designing a generic cost function. The authors' approach involves leveraging expectation operators and applying test functions to extract the desired densities. Do the authors believe that their approach using \( \{k(x, \cdot)\} \) represents the "right" framework for this problem? There is some resemblance to [1], where transformations are used to access the quantities of interest. While the authors' method for estimating the Radon-Nikodym derivative is more elegant due to its use of test functions, the overall approach shares conceptual similarities.  
[1] Smooth Operators; Grünewälder, Gretton, Shawe-Taylor  
In conclusion, I believe this is a timely and robust paper that addresses a relevant problem, provides theoretical convergence guarantees, and includes experimental validation.