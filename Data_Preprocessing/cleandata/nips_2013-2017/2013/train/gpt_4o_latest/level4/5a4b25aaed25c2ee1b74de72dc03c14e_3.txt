The success of inference using Gibbs sampling in Markov Random Fields (MRFs), specifically in the Ising model with two labels, is heavily influenced by the mixing rate of the underlying Markov Chain Monte Carlo process. This paper proposes the following novel approach to inference:
1) Project the input model, which may not mix rapidly, onto a set of models that are guaranteed to mix quickly.  
2) Perform inference on the resulting fast-mixing model.
The space of fast-mixing models is characterized by constraining the spectral norm of the matrix of absolute values of Ising edge strengths. The "projection" is defined in terms of divergences of the Gibbs distribution and is constrained to preserve the graph structure. For Euclidean distance, the projection is achieved by dualizing the original task and employing the LBFGS-B optimization algorithm. For other divergences—such as KL divergence, piecewise KL divergence, and reversed KL divergence—a projected gradient algorithm is utilized. Notably, for reversed KL divergence, Gibbs sampling (applied to a fast-mixing model) is required to compute the projection.
Extensive experiments on small random models are presented to compare the approximated marginals with the true marginals. The methods evaluated include the proposed approach (using all the aforementioned divergences), loopy belief propagation (LBP), tree-reweighted belief propagation (TRW), mean-field (MF) approximation, and Gibbs sampling on the original model. The experiments assess both accuracy and runtime-versus-accuracy trade-offs. Results indicate that the proposed methods consistently outperform TRW, MF, and LBP in terms of accuracy. Furthermore, for a reasonable range of runtimes, the proposed methods also outperform Gibbs sampling on the original model. Among the proposed methods, the approach using reversed KL divergence consistently yields the best performance.
Comments:
- The projected gradient algorithm described in Section 5.1 involves two nested loops, with the inner loop employing LBFGS-B. Please provide details on the stopping criteria for the inner iterations.  
- The horizontal axis in the plots in Figure 2 (and the supplementary material) is labeled "number of samples," but sampling is only utilized for reversed KL divergence. I suspect the horizontal axis should represent the runtime of the algorithm instead. Additionally, why not report the runtime for LBP, TRW, and MF as well? Including these runtimes would ensure a fair comparison of accuracy versus runtime across all tested algorithms. Please clarify this point, as the current presentation makes it difficult to interpret the experimental results. Absolute runtimes in seconds should also be provided.  
- Consider conducting experimental comparisons on larger models. One intriguing option is to use models from the paper:  
  [Boris Flach: A class of random fields on complete graphs with tractable partition function, to appear in TPAMI, available online],  
  which allow polynomial-time inference.  
Minor Comments:  
- Line 222: Replace "onto the tree" with "onto graph T."  
- Line 226: Should the term "subgradient" be used instead of "derivative"?  
Overall, this is an interesting paper with convincing empirical results. However, the practical utility of the proposed methods may be limited due to their high runtime, which requires further clarification in the rebuttal.