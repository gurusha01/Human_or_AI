Paraphrased Review:
Paper Summary:  
This paper addresses a compelling and relatively unexplored challenge of monitoring changes in the dependency structure among variables in a data stream (e.g., time-series data). The authors focus on developing a computationally and memory-efficient solution for simpler SEM models. They propose a novel algorithm, LoSST, to tackle this task. LoSST leverages a standard batch structure learning algorithm (PC by Spirtes et al., 2000) and monitors several statistics from the data stream to determine when to trigger new structure learning. The tracked statistics include the deviation from the weighted mean of each variable, the correlation matrix (Equations 1 and 2), and the weighted sample size. For weighting, the authors introduce a scheme that assigns weights to new data points based on their "surprisingness" (Equations 3 and 4). Specifically, the Mahalanobis distance (MD) of each point is calculated, and the corresponding p-values, estimated using Hotelling's T² distribution, are aggregated using Liptak's weighted pooling method to compute an overall "significance" statistic, denoted as ρ. This statistic is then combined with the previous step in a noisy OR configuration to decide whether to initiate new structure learning. The authors compare LoSST to the vanilla PC batch learning algorithm using synthetic datasets where either parameter changes or structural changes are introduced after a fixed number of data points. They test various configurations, including different numbers of variables and maximum degrees. Additionally, they analyze a real-world dataset from the US price index, consisting of 529 monthly data points (starting in 1967) across six variables. Their results demonstrate that the algorithm identifies known economic changes and trends in the US economy while ignoring transient fluctuations.  
Pros:  
1. The paper addresses an intriguing and underexplored problem. As the authors highlight, there is significant potential for further development of practical solutions for this and related models, along with advancing theoretical understanding and guarantees.  
2. The proposed algorithm is novel, practical, and efficient, creatively combining various methods to achieve its goals.  
3. The experiments are adequate, exploring the impact of different parameters and including an analysis of an interesting real-world dataset.  
4. The authors provide a thorough review of prior work related to this problem.  
5. The paper is generally well-written.  
Cons:  
1. The primary limitation of the paper is the lack of theoretical guarantees or analysis under specific assumptions. While the authors acknowledge the trade-off between convergence and responsiveness (p. 4), it seems feasible to provide some guarantees under simplified conditions. At the very least, this should be mentioned as a direction for future research.  
2. Similarly, the proposed solution, LoSST, relies heavily on heuristics. While this is acceptable and enables a practical implementation, the decisions underlying the heuristics should be more explicitly documented. For instance, while tracking the mean and correlation matrix is straightforward, the key decisions involve reweighting, determining the effective sample size, and deciding when and how to relearn the structure. The authors employ creative methods that likely fall outside the theoretical guarantees of these techniques. For example, the MD p-value is estimated using Hotelling's T² distribution, with the sample size \( N \) replaced by the effective sample size \( S^r \). However, Hotelling's T² distribution assumes complete counts from a given distribution, not partial or reweighted counts. Would the distribution still hold if sampling were performed from these networks/structures with partial weights? Similarly, Liptak's weighted p-values were designed for meta-analysis involving independent tests with varying sample sizes. If these methods are applied outside their intended contexts, the authors should clearly state this to avoid potential misunderstandings by readers.  
Other Comments:  
- The authors make creative use of footnotes to outline proofs, likely exceeding the expectations of Leslie Lamport or the conference organizers. Consider moving these details to a supplementary section.  
- The figures are too small, with excessive spacing between them, missing labels, and insufficient details in the captions and legends. Additionally, the experimental descriptions are unclear (e.g., the top paragraph on p. 7 is difficult to follow).  
- The explanation for why LoSST performs worse on edge addition when only parameters change is unsatisfactory.  
- On a related note, it seems desirable to decouple parameter updates from structure updates.  
Overall, this paper represents an intriguing direction for future research, offering a practical yet heuristic implementation as an initial attempt at solving this problem.