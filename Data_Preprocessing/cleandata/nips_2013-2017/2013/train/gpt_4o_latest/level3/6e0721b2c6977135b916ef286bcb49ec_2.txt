The paper proposes a probabilistic sampling method for selecting matrix entries to construct a sparse sketch \( B \) of a matrix \( A \), aiming to minimize the spectral norm error \( ||B - A|| \). The authors argue that their approach is competitive with the optimal offline distribution while being computationally efficient in the streaming model. The method introduces a sampling distribution dependent on the row norms of \( A \), adapting to the sampling budget, and demonstrates its efficacy through theoretical guarantees and experiments.
Strengths:
1. Technical Soundness: The mathematical foundation of the proposed method is rigorous, leveraging the Matrix-Bernstein inequality to derive near-optimal sampling distributions. The theoretical results are well-supported, and the authors provide clear bounds on the approximation error.
2. Computational Efficiency: The method is designed for the streaming model, requiring \( O(1) \) computation per non-zero entry, making it practical for large-scale matrices.
3. Adaptivity: The approach adapts the sampling distribution based on the sampling budget, which is a novel and insightful contribution.
4. Experimental Validation: The experiments demonstrate that the proposed method outperforms or matches existing techniques, particularly for challenging datasets like Wikipedia.
Weaknesses:
1. Clarity of Presentation: The introduction is confusing and inconsistent, making it difficult to grasp the motivation and scope of the work. Key concepts, such as the relevance of the streaming model to real-world applications, are not clearly articulated.
2. Mathematical Ambiguities: The definitions of probabilities and the sets of matrix elements are unclear, and the claim of unbiased estimation is not convincingly justified. These issues undermine the precision of the theoretical arguments.
3. Relevance to Machine Learning: The paper does not adequately justify the significance of probabilistic sampling for matrix data in the context of machine learning. Deterministic methods, which are simpler and often sufficient, are not compared in depth.
4. Lack of Applications: The paper lacks concrete applications or compelling use cases, which weakens its motivation and potential impact. The experimental section, while thorough, does not tie the results to practical machine learning tasks.
Arguments for Acceptance:
- The paper addresses an interesting and challenging problem in matrix sparsification, with strong theoretical contributions.
- The proposed method is computationally efficient and demonstrates superior performance in experiments.
Arguments Against Acceptance:
- The lack of clarity and mathematical ambiguities detract from the paper's accessibility and rigor.
- The relevance of the work to the machine learning community is questionable without concrete applications or stronger justification.
- Deterministic methods are not sufficiently explored as baselines, leaving a gap in the comparative analysis.
Recommendation:
While the paper makes notable theoretical contributions, its unclear presentation, lack of practical motivation, and limited relevance to machine learning reduce its overall impact. I recommend major revisions to address these issues before acceptance. Specifically, the authors should clarify the introduction, resolve mathematical ambiguities, provide concrete applications, and better position the work within the machine learning context.