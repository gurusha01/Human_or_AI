This paper proposes a variance reduction technique for stochastic gradient descent (SGD) using control variates, which differs from traditional minibatching approaches. The authors demonstrate the utility of their method on two problems: MAP estimation for logistic regression (convex) and stochastic variational inference for latent Dirichlet allocation (non-convex). The key idea is to construct control variates based on low-order moments of the data, which are either pre-computed or estimated online, to reduce the variance of the noisy gradient while maintaining unbiasedness. The paper includes theoretical analysis and experimental results showing faster convergence and improved performance compared to standard SGD.
Strengths:
1. Practical Contribution: The proposed method addresses a fundamental challenge in SGD—variance reduction—without relying on larger minibatches, which can be computationally expensive.
2. Empirical Validation: The experiments on logistic regression and LDA convincingly demonstrate the effectiveness of the approach, with faster convergence and better predictive performance.
3. Theoretical Rigor: The paper provides a solid theoretical foundation for the proposed method, including derivations of the variance reduction mechanism and its impact on convergence rates.
4. Relevance: The method is broadly applicable to both convex and non-convex optimization problems, making it a potentially valuable tool for the machine learning community.
Weaknesses:
1. Generality: While the paper demonstrates the construction of control variates for two specific problems, it lacks clarity on how to generalize this construction to other optimization tasks. This limits the broader applicability of the method.
2. Overstated Comparisons: The authors claim significant advantages over minibatching but do not provide a thorough comparison with state-of-the-art minibatching techniques, which weakens the argument.
3. Experimental Limitations: The choice of hyperparameters (e.g., minibatch size, fixed learning rates) appears arbitrary and is not systematically justified, raising concerns about the generalizability of the results.
4. Novelty Concerns: Similar ideas have been explored in prior work (e.g., Paisley et al., ICML 2012), and the paper does not sufficiently differentiate its contributions from these earlier efforts.
Presentation Feedback:
The paper is generally well-written and organized, but some claims (e.g., the superiority over minibatching) should either be substantiated with additional experiments or toned down. Additionally, the applicability of the method to subgradient-based optimization is unclear and requires clarification. The authors should also ensure that all claims are supported by references or supplementary material.
Overall Impression:
This is a well-executed, implementation-focused paper that introduces a novel application of control variates for variance reduction in SGD. Despite concerns about generality and novelty, the method is likely to interest the NIPS audience due to its practical relevance and demonstrated performance improvements. With revisions to address the weaknesses, this paper could make a meaningful contribution to the field.
Arguments for Acceptance:
- Practical utility and relevance to a wide range of optimization problems.
- Strong experimental results and theoretical grounding.
- Clear writing and presentation.
Arguments Against Acceptance:
- Limited generalizability of the proposed method.
- Insufficient novelty relative to prior work.
- Experimental design could be more rigorous.
Recommendation: Weak Accept.