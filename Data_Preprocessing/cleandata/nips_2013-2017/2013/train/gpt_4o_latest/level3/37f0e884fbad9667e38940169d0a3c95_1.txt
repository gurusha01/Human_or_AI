The paper proposes a novel optimization algorithm, Epoch Mixed Gradient Descent (EMGD), aimed at reducing the computational cost of optimizing smooth, strongly convex functions over convex constraint sets. The key innovation lies in combining full and stochastic gradient steps within epochs to achieve linear convergence while reducing the number of full gradient evaluations to \(O(\log(1/\epsilon))\), independent of the condition number \(\kappa\). This approach addresses a significant limitation of traditional gradient-based methods, which require \(O(\sqrt{\kappa} \log(1/\epsilon))\) full gradient evaluations, making them computationally expensive for ill-conditioned problems.
The theoretical contribution of the paper is robust. EMGD's convergence proof is self-contained, simple, and demonstrates a constant suboptimality decrease independent of \(\kappa\). Compared to the Stochastic Average Gradient (SAG) method, EMGD is more general as it handles constrained optimization and offers stronger theoretical guarantees (high-probability convergence versus expectation). However, EMGD lacks adaptive step-size selection and is slower in some regimes, particularly when \(\kappa\) is large. The paper also highlights EMGD's advantages in terms of storage efficiency and its potential for distributed computing, which are practical considerations for large-scale problems.
Despite its strong theoretical foundation, the paper has notable limitations. The absence of experimental validation is a significant drawback, as it leaves the practical relevance of EMGD untested. The fixed number of steps per epoch, required by the algorithm, may also limit its adaptability compared to methods like SAG or Stochastic Dual Coordinate Ascent (SDCA). Additionally, while the paper is well-written and organized, the high-level insights and motivation behind the proof could be expanded to enhance accessibility for a broader audience.
In terms of originality, EMGD represents a novel combination of existing techniques, contributing a simpler proof framework for constrained optimization. However, its practical significance is limited by the lack of empirical results and unclear advantages over existing methods in real-world scenarios.
Strengths:
- Novel algorithm with reduced dependence on \(\kappa\).
- General applicability to constrained optimization.
- Strong theoretical guarantees with a simple convergence proof.
- Efficient storage requirements and potential for distributed computing.
Weaknesses:
- No experimental results to validate practical utility.
- Fixed steps per epoch reduce adaptability.
- Slower performance in some regimes compared to SAG or SDCA.
- Limited discussion on practical settings and real-world applications.
Suggestions for Improvement:
1. Include experimental results to demonstrate EMGD's practical relevance and compare its performance with existing methods.
2. Discuss strategies for adaptive step-size selection and estimating \(\kappa\) dynamically.
3. Expand the discussion on limitations and provide more motivating insights for the theoretical proof.
4. Address minor typos and inconsistencies in notation.
In conclusion, the paper makes a valuable theoretical contribution to convex optimization but falls short in demonstrating practical impact. Acceptance would depend on the conference's emphasis on theoretical versus empirical contributions. If the authors address the noted limitations, particularly by including experiments, the paper could significantly advance the field.