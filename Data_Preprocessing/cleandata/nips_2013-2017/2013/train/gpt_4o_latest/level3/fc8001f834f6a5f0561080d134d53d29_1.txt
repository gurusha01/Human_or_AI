The paper presents a novel approach to approximate nearest neighbor (ANN) retrieval in both metric and non-metric spaces, leveraging VP-trees with learned pruning functions. The authors propose two simple learning-to-prune methods: density estimation through sampling and piecewise linear approximation of decision functions. The method is evaluated on datasets with Euclidean, KL-divergence, and Itakura-Saito distance functions, and is shown to be competitive with state-of-the-art methods such as multi-probe LSH, permutation-based approaches, and the bbtree. A key theoretical contribution is the proof of applicability conditions (Property 1), which extends the VP-tree's utility to certain non-metric spaces.
Strengths:
1. Clarity and Organization: The paper is well-written and effectively summarizes its contributions in the abstract and introduction. The methodology is clearly explained, with sufficient technical detail to enable reproducibility.
2. Theoretical Contribution: The proof of Property 1 is a significant theoretical advancement, demonstrating the VP-tree's applicability to a broader class of non-metric spaces, including KL-divergence.
3. Empirical Evaluation: The experiments are thorough, comparing the proposed method against multiple baselines across diverse datasets. The results convincingly demonstrate the VP-tree's efficiency and effectiveness, particularly for datasets with low or moderate intrinsic dimensionality.
4. Practical Insights: The authors provide practical implementation details, such as the use of SIMD optimizations and pre-computed logarithms, which enhance the method's computational efficiency.
5. Relevance to ANN Retrieval: The paper addresses an important problem in similarity search, with potential applications in multimedia retrieval, computational biology, and machine learning.
Weaknesses:
1. Limited Role of Learning: While the method employs simple learning techniques (e.g., sampling and piecewise linear approximation), the role of learning is minimal and lacks sophistication. This limits the paper's alignment with the core themes of learning and neural computing typically emphasized at NIPS.
2. Scalability to High-Dimensional Spaces: The method's performance degrades for datasets with high intrinsic dimensionality, as noted in the experiments. This limitation, while common in space-partitioning methods, could have been addressed more explicitly.
3. Novelty of Learning Approach: The learning-to-prune methods are relatively straightforward and do not represent a significant departure from existing techniques. More advanced learning strategies could potentially improve performance further.
4. Out-of-Scope Concerns: The paper's focus on ANN retrieval methods and space-partitioning techniques may be somewhat peripheral to the primary focus of NIPS, which often emphasizes advancements in machine learning and neural networks.
Arguments for Acceptance:
- The paper is technically sound, well-written, and provides a valuable contribution to ANN retrieval, particularly in non-metric spaces.
- The theoretical and empirical results are robust, and the method is competitive with state-of-the-art approaches.
- The insights into VP-tree applicability and the practical implementation details are useful for the broader research community.
Arguments Against Acceptance:
- The minimal role of learning in the proposed method may place the paper slightly out of scope for NIPS.
- The novelty of the learning approach is limited, and the method does not significantly advance the state of the art in machine learning.
Recommendation:
While the paper makes a strong contribution to ANN retrieval and is of high quality, its limited focus on learning and neural computing may make it less suitable for NIPS. If the conference prioritizes broader applicability and relevance to machine learning, the paper may be better suited for a venue specializing in information retrieval or computational geometry. However, if the program committee values methodological rigor and practical impact, the paper could merit acceptance.