The paper presents a novel convex relaxation approach for training two-layer neural networks, addressing the inherent non-convexity of such models. By leveraging semidefinite programming (SDP) and reformulating the problem in terms of a latent kernel, the authors propose a globally trainable framework that extends beyond single-layer models. The paper is well-written, with a clear exposition of the theoretical contributions and experimental results. However, there are several strengths and weaknesses that merit discussion.
Strengths:
1. Clarity and Technical Soundness: The paper is clearly written and well-organized, making it accessible to readers familiar with convex optimization and neural networks. The theoretical derivations are rigorous, and the authors provide detailed proofs to support their claims.
2. Significance: The proposed method addresses a critical challenge in training deep models by providing a convex relaxation that guarantees global optimization. This is a meaningful contribution to the field of deep learning and convex optimization.
3. Experimental Validation: The experiments demonstrate that the proposed method (CVX2) outperforms single-layer models and locally trained two-layer models (LOC2) on synthetic and real datasets. The results highlight the potential of convex relaxations for capturing nonlinearities in two-layer architectures.
4. Extension of Convex Methods: The work extends existing convex approaches to handle more complex latent structures, which is a notable advancement over prior methods like latent clustering.
Weaknesses:
1. Scalability: The use of SDP for convex relaxation is a significant limitation. The quadratic complexity of SDP makes the method computationally expensive, especially for large-scale datasets. While the authors propose a boosting algorithm to mitigate this, the scalability remains a concern.
2. Lack of Novelty in SDP Usage: While the application of SDP to neural networks is interesting, the use of SDP itself is not novel. Previous works have employed similar techniques for latent variable models, and the paper does not sufficiently differentiate its approach from these.
3. Assumption of Low Rank: Algorithm 2 assumes that the latent kernel matrix \(N\) has low rank, but this is not guaranteed in practice. This assumption could limit the applicability of the method to datasets where \(N\) is not inherently low-rank.
4. Unfair Baselines in Synthetic Experiments: The synthetic experiments compare the proposed method against a one-layer linear SVM, which is not a fair baseline. A more appropriate comparison would be against methods like the Nyström approximation to RBF SVMs, which can also capture nonlinearities.
5. Limited Transferability: The proposed method focuses on large-margin losses and specific transfer functions (step and indmax). Extending the approach to more commonly used activation functions like sigmoid or softmax would enhance its practical utility.
Recommendation:
While the paper makes a significant theoretical contribution and demonstrates promising results, the scalability and novelty concerns weigh against its broader applicability. The experimental comparisons could also be improved to provide a fairer evaluation. I recommend acceptance with minor revisions, contingent on addressing the scalability concerns and providing additional comparisons with stronger baselines like Nyström approximations. 
Arguments for Acceptance:
- Clear and rigorous theoretical contributions.
- Demonstrates advantages of convex relaxation over local training methods.
- Extends the scope of convex modeling in deep learning.
Arguments Against Acceptance:
- Scalability issues due to SDP.
- Limited novelty in the use of SDP.
- Assumptions about low rank and unfair baselines in experiments.
Overall, the paper is a valuable contribution to the field, but addressing the highlighted weaknesses would significantly strengthen its impact.