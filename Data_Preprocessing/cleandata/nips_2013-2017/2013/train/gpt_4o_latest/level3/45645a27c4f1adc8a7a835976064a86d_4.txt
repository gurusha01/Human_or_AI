This paper presents an extension of Factorized Asymptotic Bayesian (FAB) inference from mixture models (MMs) to latent feature models (LFMs), addressing a key limitation in FAB's applicability. Specifically, the authors adapt FAB to LFMs by deriving a lower bound of the Factorized Information Criterion (FIC) for LFMs, demonstrating that it shares a similar representation with FIC for MMs despite LFMs not satisfying the block diagonal Hessian condition. This extension enables FAB/LFMs to perform automatic hidden state selection and ensures parameter identifiability, making it a compelling alternative to existing methods like Indian Buffet Processes (IBPs) and Variational Bayesian (VB) approaches.
The paper is technically sound, with rigorous derivations and proofs provided in the supplementary materials. The authors' asymptotic analysis of the Hessian matrix is well-executed, and the theoretical contributions are significant for the Bayesian inference community. The inclusion of a shrinkage acceleration mechanism further enhances the computational efficiency of FAB/LFMs, which is validated through experiments on synthetic and real-world datasets. The results demonstrate that FAB/LFMs achieve superior prediction accuracy and computational efficiency compared to state-of-the-art methods, particularly in large-scale data scenarios.
Strengths of the paper include its clarity and organization. The authors provide a thorough explanation of the FAB framework, the derivation of FIC for LFMs, and the implementation details of the proposed algorithm. The experimental results are comprehensive and convincingly demonstrate the advantages of FAB/LFMs in terms of model selection, prediction performance, and computational cost. Additionally, the paper addresses the identifiability issue in LFMs and proposes a merging post-processing step to further refine the model, which is a thoughtful and practical contribution.
However, the paper has some limitations. While the authors acknowledge that FAB's performance may degrade in small-sample scenarios due to the inaccuracy of FIC approximation, this aspect could be explored further. Additionally, the experiments could include a broader range of real-world datasets to strengthen the generalizability of the results. The reliance on supplementary materials for proofs and some algorithmic details may also hinder accessibility for readers unfamiliar with the FAB framework.
In summary, this paper offers an incremental but meaningful extension of FAB inference to LFMs, with strong theoretical underpinnings and empirical validation. Its contributions are relevant to the NeurIPS community, particularly for researchers working on Bayesian inference and latent variable models. I recommend acceptance of this paper, as it advances the state of the art in model selection and computational efficiency for LFMs.