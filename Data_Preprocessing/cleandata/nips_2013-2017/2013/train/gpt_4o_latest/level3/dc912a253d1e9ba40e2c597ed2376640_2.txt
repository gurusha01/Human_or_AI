The paper presents a novel distributed stochastic dual coordinate ascent (DisDCA) algorithm, which combines stochastic dual coordinate ascent with parallelized training to address regularized loss minimization problems in a distributed framework. The authors focus on the trade-off between computation and communication, providing theoretical bounds and empirical evaluations. While the algorithm and its theoretical contributions are interesting, several critical issues limit the paper's overall impact.
Strengths:  
1. Novelty: The paper introduces a unique combination of stochastic dual coordinate ascent with distributed training, which is relatively underexplored compared to SGD and ADMM-based approaches. The analysis of the trade-off between computation and communication is a significant theoretical contribution, particularly the derivation of an "effective region" for parameters \( K \) (machines) and \( m \) (samples per iteration).  
2. Theoretical Rigor: The convergence bounds for both smooth and Lipschitz continuous loss functions are well-formulated, and the comparison with ADMM provides valuable insights into the algorithm's design and parameter-free nature.  
3. Practical Variant: The practical variant of DisDCA demonstrates improved performance over the basic variant, showing promise for real-world applications.
Weaknesses:  
1. Experimental Design: The experiments are poorly designed and fail to account for critical factors such as input dimensionality and sparsity. Sparse data scenarios, where the computation cost scales with non-zero features (\( d' \)), are not adequately addressed.  
2. Misleading Metrics: The experiments measure only the number of iterations, ignoring total running time and communication latency. This omission undermines the validity of the conclusions, as communication overhead is a key consideration in distributed systems.  
3. Missing Architecture Details: The computational setup (e.g., cluster vs. multicore, shared memory implications) is not clearly described, making it difficult to assess the practical feasibility of the algorithm.  
4. Theorem Validation: While Theorem 1 provides theoretical insights into the effective region of \( K \) and \( m \), the experiments fail to validate these findings with actual training times.  
5. Proof Clarity: The proof of Theorem 1 contains unnecessary and confusing lines, with unclear references to \( T_0 \), which detracts from its readability.  
6. Parameter-Free Claim: The claim that DisDCA is parameter-free is misleading, as the parameter \( \lambda \) plays a critical role in the algorithm's performance.  
7. Terminology and Presentation Issues: The paper suffers from unclear terminology (e.g., \( \alpha \) as dual of \( w \) vs. \( x \)) and typographical errors (e.g., "varing" instead of "varying"). Figure 3 is particularly difficult to interpret due to poor plot clarity.
Arguments for Acceptance:  
- The paper addresses a relevant and challenging problem in distributed optimization with a novel approach.  
- The theoretical analysis of the computation-communication trade-off is a valuable contribution.  
- The practical variant of DisDCA shows promise for improving performance in distributed settings.
Arguments Against Acceptance:  
- The experimental design is inadequate and fails to convincingly demonstrate the algorithm's advantages.  
- Key practical considerations, such as communication latency and sparse data handling, are overlooked.  
- The paper lacks clarity in both theoretical proofs and experimental presentation, reducing its accessibility to readers.
Recommendation: Weak Reject. While the theoretical contributions are significant, the experimental limitations and lack of clarity undermine the paper's overall quality. Addressing these issues in a future revision could make this a strong contribution to the field.