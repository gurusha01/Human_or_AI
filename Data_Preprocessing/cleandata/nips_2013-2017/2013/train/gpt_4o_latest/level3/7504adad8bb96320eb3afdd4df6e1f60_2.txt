The paper investigates the application of Classification-Based Modified Policy Iteration (CBMPI), an approximate dynamic programming (ADP) algorithm, to the game of Tetris and compares its performance to Direct Policy Iteration (DPI), 位-Policy Iteration (位-PI), and Cross-Entropy (CE) methods. While the paper lacks novel theoretical or algorithmic contributions, it provides an extensive empirical analysis that could be valuable for advancing value-function methods in Tetris.
The authors hypothesize that Tetris policies are easier to represent than value functions, which motivates the use of CBMPI. However, their explanation of this hypothesis and its broader implications is insufficiently detailed, leaving the reader unclear about the underlying reasoning. CBMPI demonstrates strong empirical performance, outperforming 位-PI and achieving results comparable to CE on large boards while using significantly fewer samples. However, the experimental design raises concerns. CBMPI relies on prior knowledge, specifically a strong Tetris policy, which creates an unfair advantage over 位-PI and CE, both of which lack this initialization. Additionally, feature optimization for CBMPI in small-board experiments introduces another layer of unfairness, as other methods do not benefit from the same feature set.
The claim of CBMPI's superior sample efficiency is undermined by its failure to match CE's final performance on large boards. While CBMPI learns faster initially, its inability to converge to CE's level makes the sample efficiency argument less compelling. Furthermore, the paper does not analyze the variance in Tetris's high-score variability, which is critical for assessing the statistical significance of the reported performance differences. The lack of variance analysis weakens the reliability of the conclusions.
The clarity of the paper could also be improved. Figures 4 and 5 would benefit from using the number of samples on the x-axis instead of iterations, as this would enable fairer comparisons across methods. Despite these issues, the paper provides valuable insights into the potential of policy-based ADP methods like CBMPI in Tetris, particularly in demonstrating that such methods can rival black-box optimization techniques like CE.
Pros for acceptance:
1. Strong empirical results showcasing CBMPI's potential in Tetris.
2. Novel empirical comparison of policy-based ADP methods with traditional value-function approaches.
3. Highlights the importance of feature selection in Tetris optimization.
Cons for acceptance:
1. Lack of theoretical contributions or rigorous explanation of the central hypothesis.
2. Experimental design introduces fairness concerns due to reliance on prior knowledge and feature optimization.
3. Insufficient statistical analysis of performance variability.
4. Misleading claims about sample efficiency due to CBMPI's failure to match CE's final performance.
In conclusion, while the paper offers valuable empirical insights, its methodological and presentation flaws limit its overall contribution. It may be better suited for a venue focused on empirical studies rather than theoretical advancements.