The paper addresses the problem of submodular maximization under cardinality constraints in distributed settings, proposing a two-round algorithm, GREEDI, to scale the greedy approach for massive datasets. This is a well-motivated problem, as submodular functions appear in numerous machine learning applications, and existing greedy algorithms are inherently sequential, making them unsuitable for distributed architectures. The authors build on prior work in distributed submodular maximization and propose a practical solution that leverages MapReduce-style computations.
The main contribution of the paper is the GREEDI algorithm, which partitions the data across machines, runs local greedy algorithms, and merges the results in a second round to produce a global solution. The theoretical analysis provides performance guarantees under specific assumptions, with dependence on the number of machines (m) and solution size (k). The experimental results demonstrate the efficacy of GREEDI compared to centralized methods and other distributed baselines, showing its scalability on datasets with tens of millions of points.
Strengths:
1. The paper addresses an important and practical problem in distributed machine learning, making it relevant to the community.
2. The GREEDI algorithm is simple, communication-efficient, and easily implementable in distributed frameworks like Hadoop.
3. Theoretical results provide insights into the performance bounds of the algorithm, particularly under geometric and Lipschitz assumptions.
4. Experimental results are extensive, covering multiple applications (e.g., exemplar-based clustering, active set selection) and demonstrating scalability to massive datasets.
Weaknesses:
1. The theoretical contributions are limited, with some results being straightforward extensions of existing work. For example, the dependence on min(m, k) is a known limitation in distributed submodular maximization.
2. The proofs are repetitive and rely on strong assumptions (e.g., uniform random partitioning, Lipschitz continuity), which may not hold in real-world scenarios.
3. The paper does not explore the tradeoff between the number of rounds, memory usage, and approximation quality, which is a critical aspect of distributed algorithms.
4. The experimental section includes unnecessary baselines (e.g., random/random) and does not adequately highlight the importance of oversampling in the first round.
5. The paper lacks comparisons with prior work, such as Chierichetti et al. (SPAA 2013), which could provide a more comprehensive evaluation of its contributions.
Suggestions for Improvement:
1. Strengthen the theoretical analysis by proving tighter bounds for randomized input partitioning and exploring multi-round extensions.
2. Merge related results to improve clarity and reduce redundancy in the proofs.
3. Include comparisons with prior distributed submodular maximization methods, such as Chierichetti et al. (2013), to contextualize the contributions.
4. Revise unclear phrasing (e.g., "suitable choice") and explain ambiguous comments (e.g., "unless P = NP").
5. Highlight the role of oversampling in the experiments and provide a more detailed discussion of its impact.
Conclusion:
Overall, the paper tackles an important problem and proposes a practical algorithm with promising experimental results. However, its theoretical contributions are limited, and it lacks sufficient comparison with prior work. While the paper is a useful step toward scalable submodular maximization, it requires significant revisions to strengthen its theoretical and experimental analysis. I recommend acceptance only if these issues are addressed.