The paper presents the GreeDi algorithm, a distributed approach for maximizing monotone submodular functions under cardinality constraints using a MapReduce framework. Submodular maximization has broad applications in machine learning, including exemplar-based clustering and active set selection in Gaussian processes. Traditional greedy algorithms for this problem are centralized and sequential, making them unsuitable for large-scale datasets. GreeDi addresses this limitation by distributing the computation across multiple machines, enabling scalability while maintaining performance close to centralized solutions.
The authors provide a solid theoretical foundation for GreeDi, demonstrating approximation guarantees under specific conditions. They also extend the analysis to decomposable functions, which are common in practical applications. The experimental results are compelling, showcasing the algorithm's effectiveness on large datasets, including 80 million Tiny Images and 45 million user visits. GreeDi consistently outperforms baseline distributed approaches, achieving near-centralized performance while scaling efficiently in Hadoop environments.
Strengths:
1. Significant Contribution: The paper addresses a critical gap in distributed submodular maximization, a problem of growing importance given the scale of modern datasets.
2. Theoretical Rigor: The authors provide strong theoretical guarantees, including bounds on the approximation ratio and performance under geometric and decomposable settings.
3. Scalability: The experiments demonstrate the algorithm's ability to handle massive datasets effectively, with practical implementation in Hadoop.
4. Relevance: The work is highly relevant to the NeurIPS community, given its focus on scalable machine learning and optimization.
5. Clarity of Experiments: The experimental setup is well-documented, and the results convincingly demonstrate the algorithm's advantages over baselines.
Weaknesses:
1. Communication Cost: The paper does not provide sufficient details on the communication overhead between mappers and reducers. Quantifying this cost is crucial for understanding the algorithm's scalability in bandwidth-constrained environments.
2. Machine Overload: The authors do not discuss how many items can be reduced to a single key, which could lead to potential bottlenecks or overload on specific machines.
3. Worst-Case Iterations: The paper lacks an analysis of the worst-case number of iterations required by GreeDi, which would help assess its efficiency in challenging scenarios.
4. Limited Baselines: While the paper compares GreeDi to several naive baselines, it does not benchmark against more sophisticated distributed optimization methods, which could strengthen its claims.
Recommendation:
Accept with Minor Revisions. The paper makes a significant contribution to distributed submodular maximization, combining theoretical rigor with practical scalability. However, the authors should address the concerns regarding communication costs, machine overload, and worst-case iteration analysis to provide a more comprehensive evaluation. These clarifications would enhance the paper's impact and utility for both researchers and practitioners.
Pro/Con Summary:
Pros:
- Strong theoretical guarantees.
- Demonstrated scalability on massive datasets.
- Practical implementation in Hadoop.
- High relevance to the NeurIPS audience.
Cons:
- Insufficient discussion of communication costs and machine overload.
- Lack of analysis on worst-case iterations.
- Limited comparison to advanced baselines.
In conclusion, the paper is a valuable contribution to the field and is recommended for acceptance, provided the authors address the identified weaknesses.