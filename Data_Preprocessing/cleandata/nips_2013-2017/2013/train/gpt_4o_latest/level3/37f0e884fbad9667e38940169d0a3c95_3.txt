The paper introduces a novel optimization algorithm, Epoch Mixed Gradient Descent (EMGD), which combines stochastic and full gradients to address smooth and strongly convex optimization problems. The authors aim to reduce the computational cost associated with full gradient evaluations, particularly for ill-conditioned problems with large condition numbers (κ). EMGD achieves this by leveraging stochastic gradients, reducing the dependence on κ for full gradient evaluations from \(O(\sqrt{\kappa} \log \frac{1}{\epsilon})\) to \(O(\log \frac{1}{\epsilon})\). The paper provides a convergence proof and demonstrates that EMGD achieves a linear convergence rate with high probability. Comparisons with existing methods, such as SAG and SDCA, highlight EMGD's advantages in constrained optimization and storage efficiency. However, its computational efficiency is limited to specific regimes where \(\kappa \leq n^{2/3}\).
Strengths:
1. Novelty and Theoretical Contribution: The paper presents a novel hybrid algorithm that combines full and stochastic gradients in a structured manner. The convergence proof is rigorous and introduces new insights into probabilistic convergence guarantees for hybrid methods.
2. Reduction in Full Gradient Dependence: EMGD effectively reduces the computational burden of full gradient evaluations, which is a significant improvement for ill-conditioned problems.
3. Applicability to Constrained Optimization: Unlike SAG and SDCA, EMGD can handle constrained optimization problems, broadening its applicability.
4. Storage Efficiency: The algorithm requires \(O(d)\) storage, making it more practical for high-dimensional problems compared to methods like SAG and SDCA, which require \(O(n)\) storage.
5. High-Probability Convergence: The linear convergence guarantee with high probability is stronger than the expectation-based guarantees of some competing methods.
Weaknesses:
1. Relevance to Machine Learning: The strong convexity assumption, while theoretically sound, limits the algorithm's applicability to many machine learning problems where such assumptions are not realistic.
2. Limited Practical Speedup: EMGD is faster than batch gradient descent only in a narrow regime (\(\kappa \leq n^{2/3}\)), which may not cover many practical scenarios. For larger \(\kappa\), other methods like SAG or SDCA are more efficient.
3. Probabilistic Convergence: While the convergence guarantee is high probability, it lacks the deterministic rates provided by batch gradient descent, which may raise concerns about robustness in practice.
4. Dependence on Known \(\kappa\): The algorithm requires prior knowledge of the condition number \(\kappa\), which may not always be available or easy to estimate.
5. Clarity and Notation: Some equations and notations in the paper are unclear, and minor corrections are needed to improve readability.
Suggestions for Improvement:
1. Provide a more detailed discussion on the relevance of EMGD to machine learning problems, particularly those that do not satisfy strong convexity.
2. Address the practical limitations of requiring prior knowledge of \(\kappa\) and suggest methods for estimating it.
3. Expand on the probabilistic nature of the convergence rate and compare it more explicitly with deterministic guarantees.
4. Improve the clarity of equations and notation, particularly in the algorithm description and convergence proof.
Recommendation:
While the paper offers a novel contribution to hybrid optimization methods, its practical relevance to machine learning is limited, and its speedup is constrained to a narrow regime. I recommend acceptance with minor revisions, contingent on addressing the clarity issues and providing a stronger discussion on the algorithm's applicability and limitations.