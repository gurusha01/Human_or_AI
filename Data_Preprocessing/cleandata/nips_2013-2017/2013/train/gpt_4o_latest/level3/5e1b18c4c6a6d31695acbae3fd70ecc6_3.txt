This paper presents a novel approach to unsupervised feature selection for text data by formulating n-gram feature selection as a lossless compression problem. The authors leverage the principle of minimum description length (MDL) and propose a dictionary-based compression scheme that minimizes the cost of reconstructing text. The problem is cast as a binary optimization task, which is approximately solved using iterative reweighting and ADMM. The method is shown to reduce the feature space by two orders of magnitude while maintaining classification accuracy, offering significant computational advantages and interpretability.
Strengths:
1. Novelty and Elegance: The paper introduces an innovative perspective on feature selection by connecting it to lossless compression, which is both conceptually elegant and practically useful. This approach is distinct from traditional feature selection methods and contributes a fresh angle to the field.
2. Clarity and Writing: The paper is well-written, with a clear exposition of the problem, methodology, and experimental results. The authors provide sufficient motivation for their approach and articulate its advantages over existing methods.
3. Performance: The proposed method demonstrates strong performance in small-scale experiments, achieving state-of-the-art classification accuracy on benchmark datasets while drastically reducing the feature space. The results also highlight the method's ability to reveal structure in unsupervised tasks.
4. Scalability: The algorithm is designed to be linear in the input size (for fixed k) and highly parallelizable, making it computationally efficient for large datasets.
Weaknesses:
1. Supplementary Dependence: While the algorithm is understandable, it relies heavily on a substantial supplementary section, which may hinder reproducibility for readers without access to these details.
2. Scalability Analysis: The paper lacks a thorough analysis of the computational properties, particularly regarding the cost-accuracy tradeoffs and scalability claims. A more detailed exploration of runtime and parallelization efficiency would strengthen the work.
3. Comparative Analysis: The paper does not compare its method with popular lossy representation techniques, such as embeddings, which are widely used in text classification tasks. This omission limits the scope of the evaluation.
4. Model Size vs. Accuracy: The tradeoffs between model size and accuracy are not compared with sparsifying regularization techniques applied to uncompressed n-gram features. This comparison would provide a more comprehensive understanding of the method's benefits.
5. Limited Experiments: The experiments are restricted to small-scale datasets. Testing the method on larger and more diverse datasets would better validate its generalizability and scalability.
Recommendation:
This paper is a promising contribution to the field of feature selection and text classification. Its novel formulation and strong experimental results make it a valuable addition to the literature. However, the paper would benefit from additional analysis, particularly on scalability, cost-accuracy tradeoffs, and comparisons with alternative methods. I recommend acceptance with minor revisions to address these gaps. 
Arguments for Acceptance:
- Novel and elegant approach with strong theoretical grounding.
- Demonstrates impressive dimensionality reduction and classification performance.
- Clear and well-organized presentation.
Arguments for Rejection:
- Insufficient analysis of computational properties and scalability.
- Missing comparisons with popular lossy representation methods.
- Limited experimental validation on larger datasets.
Overall, the strengths outweigh the weaknesses, and the paper has the potential to inspire further research in this area.