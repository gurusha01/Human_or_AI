Review
This paper introduces a novel compressive feature learning method for text data, grounded in the principle of Minimum Description Length (MDL). The authors propose a dictionary-based compression scheme that minimizes both dictionary and pointer costs, formulating the problem as a binary optimization task. To address the non-convexity of the problem, the authors relax the binary constraints to box constraints in the interval [0, 1] and solve the relaxed problem using an iterative reweighting scheme coupled with the Alternating Directions Method of Multipliers (ADMM). Experimental results demonstrate that the proposed method achieves state-of-the-art classification accuracy while reducing the feature space by two orders of magnitude, leading to faster training times and improved efficiency.
Strengths:
1. Novelty and Originality: The paper presents a unique approach to unsupervised feature selection by leveraging ideas from data compression. The use of a dictionary-based compression scheme for feature learning is innovative and addresses limitations of existing methods, such as instability in off-the-shelf compression algorithms like LZ77.
2. Technical Soundness: The formulation of the problem and the subsequent relaxation using iterative reweighting and ADMM are well-justified. The authors provide a detailed explanation of their optimization framework, including its computational efficiency (linear in document size for fixed k).
3. Experimental Validation: The method is rigorously evaluated on benchmark datasets (20 Newsgroups and IMDb), demonstrating competitive classification performance while significantly reducing feature space size. The experiments also highlight the robustness of the proposed approach compared to LZ77 and its ability to elucidate class structure in unsupervised settings.
4. Significance: The reduction in feature space size and training time without sacrificing classification accuracy is a meaningful contribution, especially for large-scale text datasets. The method's applicability to sequential data beyond text (e.g., click streams) further enhances its potential impact.
Weaknesses:
1. Parameter Tuning: While the elastic net regularizer is used for classification, the process for tuning its two parameters (via cross-validation or grid search) is not clearly described. This omission may hinder reproducibility.
2. Clarity of Presentation: Although the paper is generally well-written, some sections, particularly the optimization algorithm, are dense and may be challenging for readers unfamiliar with ADMM or reweighting schemes. Simplifying or summarizing key steps could improve accessibility.
3. Limited Scope of Experiments: The experiments focus primarily on text data, and while the authors mention applicability to other sequential data, no empirical results are provided to support this claim. This limits the generalizability of the method as presented.
4. Comparison with Related Work: While the paper references prior work on compression-based methods and feature selection, it could benefit from a more detailed comparison with state-of-the-art unsupervised feature selection techniques beyond LZ77.
Recommendation:
I recommend acceptance of this paper, as it makes a significant contribution to unsupervised feature learning by introducing a novel, efficient, and robust method. The strengths of the paper, particularly its technical rigor and experimental results, outweigh its weaknesses. However, the authors should clarify the parameter tuning process and consider expanding the scope of their experiments in future work.
Arguments for Acceptance:
- Innovative and technically sound approach to feature selection.
- Demonstrated improvements in feature space reduction and training efficiency.
- Strong experimental results on benchmark datasets.
Arguments Against Acceptance:
- Lack of clarity in parameter tuning and some technical details.
- Limited empirical validation on non-text sequential data.
Overall, this paper advances the state of the art in unsupervised feature learning and is a valuable contribution to the field.