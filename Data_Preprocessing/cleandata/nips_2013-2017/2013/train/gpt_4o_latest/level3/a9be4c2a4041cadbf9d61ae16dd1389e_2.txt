This paper introduces a novel convex approach for training two-layer supervised learning models, addressing a key challenge in latent variable models: the difficulty of global optimization due to nested nonlinearities. The authors propose a convex relaxation framework that enables global training while preserving the representational power of two-layer architectures. The model incorporates large-margin losses, indmax transfer for the second layer, and a multi-label perceptron with step transfer for the first layer. These choices are well-motivated and align with the goal of improving training quality over local heuristics.
Strengths:  
The paper is technically sound and well-written, with clear derivations and insightful explanations of the convex objective. The authors effectively demonstrate how the proposed relaxation preserves sufficient structure to transcend one-layer models while enabling global optimization. The use of semidefinite relaxations and latent kernel reformulations is innovative and provides a significant contribution to the field of deep learning. The experimental results are convincing, showing that the proposed method (CVX2) outperforms both one-layer models and locally trained two-layer models (LOC2) on synthetic and real datasets. The theoretical contributions are complemented by a practical algorithmic framework, including an efficient ADMM-based training approach and a boosting algorithm for optimization over the positive semidefinite cone. The conclusions drawn are solid and impactful, highlighting the potential of convex methods in multi-layer architectures.
Weaknesses:  
While the paper is strong overall, there are a few areas for improvement. First, the relaxation introduces some limitations, as the constraints on the latent kernel matrix are not fully convex, leading to potential gaps between the relaxed and original problems. Additionally, the paper focuses on large-margin losses and specific transfer functions (step and indmax), which may limit the generality of the approach. The authors acknowledge this limitation and suggest future work on incorporating sigmoid and softmax transfers, as well as more traditional Bregman divergences. Finally, while the experimental results are promising, the paper could benefit from a broader comparison with other state-of-the-art deep learning methods, particularly those using non-convex optimization.
Arguments for Acceptance:  
1. The paper addresses a fundamental challenge in training latent variable models and provides a novel convex relaxation framework.  
2. The technical contributions are significant, with clear theoretical insights and practical algorithms.  
3. Experimental results demonstrate the effectiveness of the proposed method across diverse datasets.  
4. The work has the potential to inspire further research on convex methods in deep learning.
Arguments Against Acceptance:  
1. The relaxation may not fully capture the original problem's constraints, potentially limiting its applicability.  
2. The focus on specific loss functions and transfer mechanisms may restrict generality.  
3. Broader comparisons with state-of-the-art methods are missing.
Conclusion:  
This paper makes a strong contribution to the field of deep learning by introducing a convex approach to two-layer modeling. While there are some limitations, the strengths far outweigh the weaknesses. I recommend acceptance, as the paper is likely to stimulate further research and has practical implications for improving training methodologies in multi-layer architectures.