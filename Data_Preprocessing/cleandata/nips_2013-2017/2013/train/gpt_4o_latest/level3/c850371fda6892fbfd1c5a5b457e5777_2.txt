This paper introduces new algorithms for differentially private online learning that closely match the best non-private regret bounds, with only minor additional factors. The authors focus on both the full-information and bandit settings, employing a modified "follow the approximate leader" (FTAL) approach with private (noisy) history maintained via standard differentially private counters. The results are compelling, as the algorithms achieve strong regret bounds while maintaining rigorous privacy guarantees. However, the paper has some clarity and presentation issues that could be improved.
Strengths:
1. Strong Theoretical Contributions: The paper provides a comprehensive suite of results, improving upon prior work in differentially private online learning. Notably, the algorithms achieve regret bounds that are competitive with non-private counterparts, up to logarithmic factors in \( T \). This is a significant advancement over prior work, such as Jain et al. (2012), which had suboptimal regret bounds.
2. Novelty: The adaptation of FTAL to the private setting is novel and well-justified. The use of tree-based aggregation for maintaining private gradient sums is a clever and effective technique.
3. Clarity of Core Ideas: The suitability of "follow the leader" algorithms for privacy is well-explored, and the theoretical analysis is rigorous. The paper also provides privacy guarantees (\( \epsilon \)-differential privacy) and regret bounds for both strongly convex and general convex loss functions.
4. Significance: The results are likely to influence future research in private online learning, particularly in the challenging bandit setting, where this work is the first to provide private algorithms.
Weaknesses:
1. Clarity and Presentation: The paper suffers from several clarity issues. For instance, the practice of citing references without listing authors makes it difficult to follow prior work. Additionally, Table 1 should clarify that \( \delta \) refers to \((\epsilon, \delta)\)-differential privacy, and Table 2 incorrectly labels the setting as full-information instead of bandit. These errors detract from the paper's readability.
2. Errors in Results: The adaptive case results in Table 2 appear to have an error, with \( T^{3/4} \) expected instead of \( T^{2/3} \). This discrepancy needs to be addressed.
3. Dimensionality Dependence: While the algorithms achieve strong regret bounds, the explicit dependence on dimensionality \( p \) in the bounds raises questions about scalability. The paper could better justify this dependence or discuss its implications.
4. Redundancy: The second-to-last paragraph on page 6 contains redundant phrasing, which could be streamlined for conciseness.
Arguments for Acceptance:
- The paper provides significant theoretical advancements in private online learning, particularly in the bandit setting.
- The results are novel, rigorous, and likely to inspire further research in the field.
- The approach is simple yet effective, making it accessible to researchers and practitioners.
Arguments Against Acceptance:
- The clarity and presentation issues, including errors in tables and unclear citations, hinder the paper's readability.
- The dimensionality dependence and the provable gap between private and non-private algorithms warrant further discussion.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a substantial contribution to the field, but the authors should address the clarity, presentation, and minor technical issues before publication.