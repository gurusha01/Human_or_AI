This paper presents a novel framework for structured Schatten norms, specifically focusing on tensor decomposition using convex optimization. The authors analyze two approaches—overlapped and latent Schatten norms—and provide theoretical and empirical evidence that the latent approach outperforms the overlapped approach in noisy tensor decomposition settings. The paper's contributions include a duality result between the two norms, theoretical consistency guarantees for the latent approach, and improved recovery bounds that depend on the minimal mode-k rank of the tensor. Numerical experiments further validate the theoretical findings.
Strengths:
1. Novelty and Originality: The paper introduces a new perspective on structured Schatten norms and rigorously analyzes the latent approach, which has been empirically observed to outperform the overlapped approach but lacked theoretical justification. This work bridges that gap and provides a solid theoretical foundation.
2. Theoretical Contributions: The duality between overlapped and latent Schatten norms is an interesting result that connects this work to broader studies on structured sparsity. The recovery guarantees based on the minimal mode-k rank are a significant improvement over previous results that relied on average rank dependencies.
3. Empirical Validation: The numerical experiments are well-designed and confirm the theoretical predictions. The scaling behavior of the mean squared error (MSE) aligns with the derived bounds, reinforcing the validity of the proposed framework.
4. Clarity of Results: The paper clearly demonstrates the advantages of the latent approach over the overlapped approach, particularly in scenarios where the tensor is low-rank in only one mode.
Weaknesses:
1. Motivation and Model Representation: While the paper is well-written, it could better articulate the broader relevance of the proposed methods to machine learning tasks. Including a graphical representation of the latent variable model would enhance clarity, especially for readers less familiar with tensor decomposition.
2. Technical Gaps: Several technical details require clarification:
   - Theorem 2 is incomplete, as components are recovered only up to a permutation.
   - The transition from equations (12) to (13) lacks justification.
   - Proofs of Lemma 4 and Lemma 5 are either missing or insufficiently detailed.
3. Related Work: The discussion of related work, particularly on higher-order SVD (HOSVD) and its variants, is limited. Incorporating references like Kolda's review on tensor decompositions would provide a more comprehensive context.
4. Computational Complexity: The paper does not adequately discuss the computational complexity of the proposed methods, which is crucial for practical applicability.
5. Incoherence Conditions: The comparison of incoherence conditions with prior works ([1,3,10]) is inaccurate and should be removed to avoid misleading conclusions.
Minor Issues:
- Notational inconsistencies (e.g., superscript notation for \(\hat{\cal{W}}^{(k)}\), \(\bar{r}k\), and \(\underline{r}k\)) and typographical errors (e.g., "structures Schatten" in the abstract) should be corrected.
- Missing parentheses in the supplementary material (page 12) need to be addressed.
Recommendation:
The paper makes significant theoretical and empirical contributions to tensor decomposition using structured Schatten norms. However, it requires revisions to address technical gaps, improve clarity, and correct minor issues. I recommend acceptance with major revisions to ensure the paper meets the high standards of the conference.
Arguments for Acceptance:
- Novel contributions to tensor decomposition theory.
- Rigorous theoretical analysis and empirical validation.
- Clear improvement over existing methods in specific scenarios.
Arguments Against Acceptance:
- Technical gaps in proofs and incomplete theorems.
- Limited discussion of related work and computational complexity.
- Minor but numerous notational and typographical issues.
With the suggested revisions, this paper has the potential to make a strong contribution to the field.