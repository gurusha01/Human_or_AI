The paper presents a novel extension of Factorized Asymptotic Bayesian (FAB) inference to binary latent feature models (LFMs), addressing the challenging problem of automatic latent feature selection and dimensionality determination. This is a significant contribution, as FAB inference has previously been limited to simpler models like mixture models (MMs) and hidden Markov models (HMMs). The authors demonstrate that the asymptotic properties of FAB inference can be adapted to LFMs, deriving a Factorized Information Criterion (FIC) for LFMs that mirrors the form used in MMs. This theoretical advancement is complemented by an efficient EM-based algorithm that incorporates shrinkage acceleration and post-processing for parameter identifiability.
Strengths:
The paper's primary strength lies in its theoretical novelty. Extending FAB inference to LFMs is a non-trivial accomplishment, and the authors provide rigorous derivations to justify their claims. The proposed algorithm is computationally efficient, leveraging a shrinkage acceleration mechanism to reduce runtime significantly. Empirical results show that the method outperforms state-of-the-art approaches like Gibbs sampling and variational inference in terms of model selection accuracy, prediction performance, and computational cost. The paper also addresses the issue of parameter identifiability, which is often overlooked in LFM research, and proposes a merging post-processing step to resolve non-identifiability.
Weaknesses:
Despite its strengths, the paper has notable limitations. The experimental evaluation is flawed in several respects. Comparisons with baseline methods, such as the Indian Buffet Process (IBP) Gibbs sampler and MEIBP, appear unfair due to differences in initialization and noise variance settings. Additionally, the authors do not provide sufficient details about hyperparameter tuning or convergence criteria for the baselines, making it difficult to assess the validity of their claims. The paper also omits discussion of recent related work, such as MAD-Bayes, which could provide valuable context and benchmarks. Furthermore, the algorithm is restricted to binary LFMs, and its performance in small-data regimes is not thoroughly analyzed. Finally, minor errors, such as mislabeling references and incorrect experimental settings, detract from the paper's overall clarity.
Clarity:
The paper is well-organized and provides a clear exposition of the methodology and theoretical contributions. However, the experimental section lacks sufficient detail, and some justifications for design choices are missing. Including more comprehensive explanations and addressing the fairness of comparisons would enhance the paper's clarity and credibility.
Originality:
The work is highly original, as it extends FAB inference to a new class of models and introduces novel techniques for computational efficiency and parameter identifiability. However, the lack of discussion on related work limits the contextualization of the contribution.
Significance:
The results are promising and demonstrate the potential for FAB/LFMs to advance the state-of-the-art in scalable inference for latent feature models. The method's ability to automatically select the number of latent features without requiring priors or hyperparameter tuning is particularly impactful. However, the restricted scope to binary LFMs and the experimental shortcomings temper the broader applicability of the findings.
Recommendation:
Borderline Accept. While the paper makes a significant theoretical contribution and demonstrates strong empirical performance, the experimental flaws and lack of discussion on related work must be addressed. The authors should improve the fairness of their evaluations, include missing references, and provide more comprehensive justifications for their experimental methodology. If these issues are resolved, the paper would be a strong candidate for acceptance.