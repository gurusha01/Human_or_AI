This paper addresses the problem of nearest-neighbor (NN) search using binary space-partitioning (BSP) trees, such as kd-trees, principal axis trees, and random projection trees. The authors formalize the intuitive but previously unproven connection between vector quantization performance of BSP-trees and their NN search performance. They introduce the "max-margin" partition tree (MM-tree), which explicitly maximizes partition margins, and demonstrate both theoretically and empirically that trees with better quantization rates and larger margins yield better NN search performance. The paper also provides theoretical guarantees linking search performance to intrinsic data properties, such as the expansion constant, and establishes that large partition margins improve search error bounds.
The paper is technically sound and provides a rigorous theoretical foundation for the relationship between quantization performance and NN search guarantees. Theorem 3.1 is particularly insightful, connecting search performance to the expansion constant, though the bound in Equation 4 could be tighter. The authors' experiments validate their theoretical claims, showing that MM-trees and other trees with good quantization performance (e.g., 2M-trees) achieve strong empirical results. However, the paper could benefit from a tighter analysis of the bound in Equation 4, as the ratio in lines 531–532 should always be less than 1. Additionally, the use of the doubling dimension instead of the expansion dimension could relax the strong assumptions (e.g., Condition C1).
The work is original and significant, offering a formal analysis of BSP-tree-based NN search and introducing the MM-tree as a novel contribution. However, the authors should compare their results to the relevant COLT2013 paper, "Randomized Partition Trees for Exact Nearest Neighbor Search," to contextualize their contributions within prior work. Furthermore, a discussion of scenarios where the input comes from an underlying distribution would enhance the paper's practical relevance.
The writing is clear and well-organized, with sufficient detail to allow reproduction of results. However, the notation for the expansion constant (currently denoted as "\(\tilde{c}\)") could be simplified to "c" for clarity. The paper's empirical validation is convincing, though the computational cost of MM-tree construction is acknowledged as a limitation.
Strengths:
1. Rigorous theoretical analysis linking quantization performance to NN search guarantees.
2. Introduction of the MM-tree, which explicitly maximizes partition margins.
3. Empirical validation demonstrating strong performance of MM-trees and other BSP-trees.
4. Clear and well-written exposition.
Weaknesses:
1. The bound in Equation 4 is looser than expected and could be tightened.
2. Strong assumptions (e.g., Condition C1) could be relaxed by using the doubling dimension.
3. Lack of comparison with the COLT2013 paper.
4. Limited discussion on scenarios with underlying data distributions.
Suggestions for Improvement:
1. Tighten the bound in Equation 4 and clarify the ratio in lines 531–532.
2. Replace the expansion dimension with the doubling dimension to relax assumptions.
3. Compare results with the COLT2013 paper for completeness.
4. Add a discussion on input distributions and their impact on BSP-tree performance.
5. Simplify the notation for the expansion constant.
Overall, this is a high-quality and impactful paper that makes a significant contribution to the theoretical understanding of BSP-tree-based NN search. While there are areas for improvement, particularly in tightening bounds and contextualizing results, the paper is a strong candidate for acceptance.