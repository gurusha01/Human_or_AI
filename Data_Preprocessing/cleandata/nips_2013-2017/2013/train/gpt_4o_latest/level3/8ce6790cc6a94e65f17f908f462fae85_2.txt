The paper presents a novel approach, A Lasso, for learning sparse Bayesian Networks (BNs) in high-dimensional spaces. The authors integrate Lasso as a scoring method within a dynamic programming (DP) framework and enhance it with an A search algorithm to address the computational challenges of large search spaces. A key contribution is the use of an admissible and consistent heuristic in A search, ensuring optimality while significantly pruning the search space. Additionally, the authors propose an approximation of A by limiting the queue size, which trades minor quality degradation for substantial computational efficiency. Unlike traditional two-stage methods that risk pruning optimal solutions upfront, A* Lasso operates as a single-stage algorithm, preserving the integrity of the search space. The experimental results, conducted on both simulated and real-world datasets, demonstrate the method's effectiveness in terms of accuracy, computational efficiency, and scalability.
Strengths:
1. Technical Soundness: The paper is technically robust, with clear theoretical justifications for the proposed methods. The use of an admissible and consistent heuristic in A* search is well-grounded, ensuring optimality while reducing computational overhead.
2. Innovation: The integration of Lasso within A* search and the avoidance of upfront pruning represent a significant advancement over traditional two-stage methods. The heuristic scheme for limiting queue size is a practical innovation that balances accuracy and efficiency.
3. Experimental Rigor: The experimental section is thorough, comparing A Lasso against state-of-the-art methods like DP, L1MB, and SBN. Results consistently show that A Lasso outperforms competitors, particularly in high-dimensional settings and larger networks.
4. Clarity and Organization: The paper is well-structured, with a logical flow from problem formulation to methodology, experiments, and conclusions. The inclusion of detailed algorithmic steps enhances reproducibility.
Weaknesses:
1. Figure 3 Non-Monotonicity: The non-monotonic behavior in Figure 3 raises questions about the stability of the method, particularly when the sparse Bayesian network (SBN) performance drops below the "random guessing" line. This warrants further explanation or analysis.
2. Minor Errors: The manuscript contains minor typos, formatting inconsistencies, and unclear citations, which detract slightly from its overall polish.
3. Scalability Limitations: While the heuristic scheme for limiting queue size improves scalability, the method still struggles with very large networks, as evidenced by the inability to evaluate some larger graphs in the experiments.
Arguments for Acceptance:
- The paper addresses a challenging and important problem in Bayesian network structure learning, advancing the state of the art.
- The proposed method is both theoretically sound and practically impactful, with strong experimental validation.
- The integration of A* search with Lasso is novel and has the potential to influence future research in the field.
Arguments Against Acceptance:
- The scalability of the method, while improved, remains a concern for extremely large networks.
- The observed non-monotonicity in Figure 3 raises questions about the robustness of the approach.
Recommendation:
Overall, this paper makes a significant contribution to the field of Bayesian network learning and is well-suited for the conference. I recommend acceptance, contingent on addressing the issues with Figure 3 and minor corrections to the manuscript.