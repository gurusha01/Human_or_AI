This paper explores the redundancy in neural network parameterization and proposes a method to reduce the number of dynamic parameters by representing weight matrices as low-rank products of smaller matrices. The authors claim that this approach allows for training deep networks with significantly fewer parameters while maintaining performance. They introduce a distinction between static and dynamic parameters, leveraging the former to reduce synchronization overhead in distributed systems. Experimental results on MNIST, TIMIT, CIFAR-10, and STL-10 are presented to validate the proposed techniques.
Strengths:
The paper addresses an important problem in deep learningâ€”reducing the computational and communication costs of training large-scale neural networks. The idea of leveraging structured redundancy in weight matrices is conceptually interesting and has potential applications in distributed training and resource-constrained environments. The experimental results, particularly the ability to predict more than 95% of the weights without accuracy loss in some cases, are promising. The use of kernel-based dictionaries and the exploration of different dictionary construction methods (e.g., autoencoders, random projections) add depth to the work. Furthermore, the proposed method is orthogonal to existing techniques like dropout and maxout, making it complementary rather than competitive.
Weaknesses:
The paper has several shortcomings that limit its impact. The central claim that large model performance can be preserved with fewer parameters lacks robust empirical support. While the authors present results on various datasets, they fail to compare their method with standard dimensionality reduction techniques like PCA or downsampling, which could achieve similar outcomes. The training challenges associated with factored parameter matrices are not adequately addressed, leaving readers without intuition on why naive approaches fail. The discussion on columnar architectures is largely hypothetical, with no empirical follow-through to substantiate its utility. Additionally, the acceleration claims for parallel training are not rigorously tested, and the implementation details of encoder/decoder algorithms are missing.
Clarity and Presentation:
The paper suffers from clarity issues. Misused terms, redundant phrases, and unclear figure placements detract from readability. The background section (Section 2) could be merged into the introduction for better flow, and equations should be numbered for easier reference. While the authors provide some intuition for their approach, the explanations are often dense and lack accessibility for a broader audience.
Originality and Significance:
The idea of reducing dynamic parameters by predicting weights is novel and has potential implications for parallelizing neural network training over low-bandwidth channels. However, the lack of strong empirical evidence and comparisons with baseline methods limits its immediate significance. The novelty of applying PCA-like techniques to intermediate layers is underexplored and could have been a stronger conceptual starting point.
Recommendation:
While the paper introduces a novel idea with potential, its weaknesses in empirical validation, clarity, and comparison with standard methods outweigh its strengths. I recommend rejection in its current form. However, with stronger experimental evidence, clearer exposition, and more rigorous comparisons, this work could make a valuable contribution to the field. 
Pros for acceptance:
- Novel approach to reducing dynamic parameters.
- Promising results on some datasets.
- Potential significance for distributed training.
Cons for acceptance:
- Empirical results do not fully support central claims.
- Lack of comparisons with standard techniques like PCA.
- Clarity and organization issues.
- Hypothetical aspects (e.g., columnar architectures) lack empirical validation.