This paper introduces CLIME-ADMM, a scalable variant of the CLIME estimator, and proposes a distributed computation framework for sparse precision matrix estimation. The authors address the challenge of ultra-large-scale problems, scaling their method to millions of dimensions and trillions of parameters using hundreds of cores. The key innovation lies in solving CLIME in column blocks rather than column-by-column, leveraging matrix-matrix multiplications for improved efficiency. The paper also incorporates block cyclic data distribution to achieve load balancing and optimize memory usage. Empirical results demonstrate the algorithm's scalability and superior performance compared to state-of-the-art methods such as DC-QUIC, Tiger, and Flare.
Strengths:
1. Scalability and Performance: The proposed CLIME-ADMM algorithm demonstrates significant scalability, achieving near-linear speedups on both shared-memory and distributed-memory architectures. The ability to handle one trillion parameters in 11 hours on 400 cores is noteworthy.
2. Novelty: The column-block approach is a novel contribution that improves computational efficiency over traditional column-by-column methods. The use of inexact ADMM and block cyclic distribution further enhances the framework's scalability.
3. Empirical Validation: The paper provides extensive experimental results comparing CLIME-ADMM with state-of-the-art methods on synthetic and real datasets. The results convincingly highlight the algorithm's superior scalability and efficiency.
4. Theoretical Guarantees: The authors establish convergence rates for both the objective and optimality conditions, providing a solid theoretical foundation for their approach.
Weaknesses:
1. Notation and Clarity: The notation is scattered across sections, making the paper harder to follow. A consolidated table of notations is recommended to improve clarity.
2. Justification of Design Choices: The block cyclic data distribution is interesting but lacks sufficient justification for its load-balancing and scalability claims. Empirical results supporting this choice are absent.
3. Ad Hoc Parameter Selection: The choice of column block size (k) appears arbitrary, with no theoretical or intuitive guidance provided. This weakens the generalizability of the method.
4. Distributed vs. Parallel: The algorithm is better described as a parallel implementation rather than a truly distributed algorithm, as it lacks explicit message-passing mechanisms.
5. Minor Typos: Typographical errors such as "tunning" (should be "tuning") and "summerized" (should be "summarized") need correction.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a critical challenge in scaling precision matrix estimation to ultra-large dimensions, providing a novel and empirically validated solution. The work is relevant to the machine learning community and has potential applications in "Big Data" settings.
- Con: The lack of clarity in notation, insufficient justification for design choices, and ad hoc parameter selection detract from the paper's overall quality.
Recommendation: The paper makes a meaningful contribution to scalable precision matrix estimation and is of high relevance to the conference. However, revisions are needed to address clarity, provide better justification for design choices, and correct minor errors. I recommend acceptance, contingent on these improvements.