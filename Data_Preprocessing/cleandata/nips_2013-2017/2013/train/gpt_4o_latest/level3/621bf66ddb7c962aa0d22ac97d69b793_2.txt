The paper presents a novel and efficient algorithm, Subsampled Randomized Hadamard Transform-Dual Ridge Regression (SRHT-DRR), for solving ridge regression in the high-dimensional setting where the number of features (\(p\)) far exceeds the number of observations (\(n\)). By leveraging the Subsampled Randomized Hadamard Transform (SRHT) for dimensionality reduction and computational efficiency, the proposed method achieves a significant speed-up over traditional dual ridge regression, reducing the computational complexity from \(O(n^2p)\) to \(O(np \log(n))\). The algorithm combines randomized Hadamard transformation with uniform feature subsampling, ensuring that the signal is evenly distributed across features to mitigate the risk of losing critical information.
The paper provides a theoretical analysis of the algorithm's risk, establishing that the inflation in risk relative to the true ridge regression solution is bounded and depends on the number of subsampled features (\(p{\text{subs}}\)). The authors also estimate the optimal order of \(p{\text{subs}}\), showing that it suffices to choose \(p_{\text{subs}} = O(n)\) for effective performance. A comparison with principal component regression (PCR) highlights the complementary nature of the two approaches, though the theoretical comparison remains limited.
Strengths of the paper include its simplicity, computational efficiency, and the integration of random projection matrix generation and matrix-product computation into a single step. The theoretical risk bounds are well-supported, and the algorithm is shown to perform competitively in empirical evaluations on synthetic and UCI datasets. Notably, the method achieves comparable accuracy to the true ridge regression solution at a fraction of the computational cost, outperforming both standard and randomized PCA in terms of speed and accuracy.
However, the paper has several weaknesses. First, the empirical results, while promising, are somewhat weak due to limited datasets and a focus on classification accuracy for the UCI dataset, which is less appropriate for evaluating a regression algorithm. A more thorough analysis of regression error would strengthen the empirical validation. Second, the theoretical contributions, while insightful, could be expanded to provide deeper insights into the algorithm's behavior under varying conditions. Additionally, a typo in Eq. (3) regarding the inequality's coefficient should be corrected for clarity.
In summary, the paper makes a valuable contribution to the field of scalable regression algorithms, offering a simple and efficient solution for high-dimensional ridge regression. While the theoretical and empirical results could be improved, the proposed method is likely to inspire further research and practical applications. Arguments for acceptance include the novelty, efficiency, and theoretical grounding of the algorithm. Arguments against acceptance include the limited empirical evaluation and the need for more robust theoretical insights. Overall, the paper is a solid contribution with room for improvement.