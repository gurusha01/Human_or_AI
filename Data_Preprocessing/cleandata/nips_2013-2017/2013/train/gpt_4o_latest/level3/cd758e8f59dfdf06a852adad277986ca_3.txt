This paper investigates the problem of estimating the cluster tree of a density supported on or near a low-dimensional manifold embedded in a high-dimensional space. The authors adapt the k-nearest neighbor (k-NN) algorithm proposed by Chaudhuri and Dasgupta (2010) to the manifold setting, leveraging theoretical tools from Niyogi, Smale, and Weinberger. They demonstrate that the algorithm achieves convergence rates dependent only on the intrinsic manifold dimension \(d\), rather than the ambient dimension \(D\), which is a significant advantage in high-dimensional applications. The paper also introduces a framework for clustering when data is concentrated near a manifold, addressing noise models such as clutter noise and additive noise.
Strengths:
1. Sound Theoretical Contributions: The paper provides rigorous theoretical guarantees for the adapted k-NN algorithm, including consistency and convergence rates. The results are robust and extend the applicability of density clustering to manifold-supported data.
2. Manifold Adaptivity: The algorithm does not require prior estimation of the manifold, which is often a challenging task. This adaptivity makes the approach practical for real-world high-dimensional datasets.
3. Novel Framework for Noisy Data: The extension to noisy data concentrated near a manifold is valuable, as it aligns with the manifold hypothesis often assumed in high-dimensional data analysis.
4. Clarity of Results: The paper clearly links convergence rates to manifold properties such as dimension and condition number, offering insights into the trade-offs between sample complexity and manifold geometry.
Weaknesses:
1. Limited Novelty: While the adaptation of Chaudhuri and Dasgupta's algorithm to manifolds is meaningful, the theoretical tools and algorithmic framework lack significant novelty. The reliance on existing results, such as those by Niyogi et al. and Rinaldo et al., reduces the originality of the contribution.
2. Stratified Spaces vs. Manifolds: The algorithm may be better suited for stratified spaces rather than smooth manifolds, as the curvature and condition number constraints could limit its applicability to certain manifold geometries.
3. Practical Implications: The paper focuses heavily on theoretical analysis, with no experimental validation. This limits the ability to assess the algorithm's performance on real-world datasets or its sensitivity to parameter choices.
Arguments for Acceptance:
- The theoretical analysis is rigorous and addresses an important problem in manifold-based clustering.
- The results are relevant to the NeurIPS community, particularly for researchers working on high-dimensional data and manifold learning.
- The paper extends existing work in a meaningful way by incorporating manifold adaptivity and noise robustness.
Arguments Against Acceptance:
- The lack of novelty in the algorithm and theoretical tools may not meet the bar for groundbreaking contributions.
- The absence of empirical results makes it difficult to evaluate the practical utility of the proposed approach.
Recommendation:
This paper is a solid theoretical contribution to manifold-based clustering, with clear and well-supported results. However, the limited novelty and lack of experimental validation temper its impact. I recommend acceptance as a theoretical contribution, provided the authors address the practical implications and discuss potential extensions to stratified spaces in future work.