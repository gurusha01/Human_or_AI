This paper presents a novel framework for learning on hypergraphs, introducing a total variation (TV) functional specifically defined for hypergraphs. The authors establish its convexity, relate it to hypergraph cuts, and propose a family of regularization functionals that interpolate between TV and Laplacian-type smoothness. They demonstrate applications in semi-supervised learning (SSL) and clustering, providing both theoretical insights and experimental comparisons.
Strengths:
1. Core Contribution: The paper's definition of TV on hypergraphs and its connection to hypergraph cuts is a significant theoretical advancement. The convexity proof and the exploration of its implications for learning tasks are well-grounded.
2. Optimization Framework: The development of proximal algorithms for efficiently solving the proposed optimization problems is technically sound and well-detailed. The complexity analysis, particularly for Proposition 5.1, is insightful.
3. Experimental Results: The proposed methods outperform Zhou (2006) on most datasets, demonstrating the utility of incorporating hypergraph structure. The scalability of the approach is particularly notable, as it handles large datasets where Zhou's method fails.
4. Writing Quality: The paper is generally well-written and focused, with clear motivations and structured arguments. The inclusion of supplementary material for proofs and algorithmic details is appreciated.
Weaknesses:
1. Comparative Analysis: The experimental comparisons are limited to Zhou (2006). Including baselines like k-means or other graph-based methods would provide a more comprehensive evaluation.
2. Clarity Issues: Certain sections, such as Theorem 4.1 and the term "tight relaxation," are not adequately explained. A more intuitive description or examples would enhance understanding.
3. Overstated Claims: The claim of minimizing the normalized cut is overstated, as the method only approximates this objective. Additionally, the assertion of being the first major advance since Zhou (2006) overlooks recent work in hypergraph learning.
4. Hypergraph Construction: The hypergraph construction relies on categorical feature similarity, which may not fully exploit the potential of hypergraphs. Exploring alternative constructions, such as weighted edges for similar feature values, could improve performance.
5. Algorithm Complexity: While the authors propose an O(n log n) complexity for sorting in Proposition 5.1, further optimization to O(n) might be possible.
6. Table Explanation: The table on page 8 lacks sufficient explanation, making it difficult to interpret the results without additional context.
Suggestions for Improvement:
- Expand the experimental section to include comparisons with alternative clustering and SSL methods, such as k-means or graph-based Laplacian regularization.
- Clarify technical terms and theorems with intuitive examples or visualizations.
- Address recent advancements in hypergraph learning to provide a balanced discussion of novelty.
- Explore isotropic TV on hypergraphs as a potential extension.
- Improve the explanation of experimental results, particularly the table on page 8, for better accessibility.
Arguments for Acceptance:
- The paper introduces a significant theoretical contribution by defining TV on hypergraphs and proving its convexity.
- The optimization framework is robust and scalable, with promising experimental results.
- The work addresses a relevant and challenging problem in machine learning, advancing the state of the art in hypergraph methods.
Arguments Against Acceptance:
- The comparative analysis is limited, and recent advancements in hypergraph learning are not adequately acknowledged.
- Certain claims, such as minimizing the normalized cut, are overstated.
- Clarity issues and unexplored directions, such as isotropic TV, limit the paper's accessibility and completeness.
Recommendation:
Overall, this paper makes a meaningful contribution to hypergraph-based learning and optimization. While there are areas for improvement, the strengths outweigh the weaknesses. I recommend acceptance, provided the authors address the clarity issues and expand the comparative analysis in the final version.