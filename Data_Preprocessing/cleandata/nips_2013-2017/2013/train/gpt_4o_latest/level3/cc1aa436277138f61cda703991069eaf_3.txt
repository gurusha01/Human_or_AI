This paper addresses the critical problem of determining the optimal number of control items versus target items in crowdsourcing tasks aimed at estimating continuous quantities. The authors propose two models: a two-stage estimator, which uses only control items to evaluate worker reliability, and a joint estimator, which incorporates both control and target items. They derive theoretical expressions for the optimal number of control items (k) for each model and validate their findings through extensive experiments on both synthetic and real-world datasets.
Strengths:  
1. Novelty and Contribution: The paper fills a significant gap in the literature by focusing on the optimal use of control items, a widely used but underexplored practice in crowdsourcing. The theoretical insights, particularly the scaling laws for optimal k (O(√`) for the two-stage estimator and O(`/√nt`) for the joint estimator), are both novel and practically relevant.  
2. Theoretical Rigor: The derivations are mathematically sound and well-supported by clear assumptions. The authors extend their analysis to more complex models, such as bias-variance and heteroscedastic models, demonstrating the robustness of their approach.  
3. Empirical Validation: The experiments on synthetic data and real-world datasets (price estimation and NFL forecasting) strongly support the theoretical findings. The exploration of model misspecification is particularly insightful, highlighting the robustness of the two-stage estimator compared to the joint estimator.  
4. Practical Implications: The paper provides actionable recommendations for practitioners, such as using minimal control items for the joint estimator when model assumptions hold and switching to the two-stage estimator under model misspecification.  
Weaknesses:  
1. Model Misspecification: While the authors discuss the impact of model misspecification, the joint estimator's sensitivity to such issues is a notable limitation. This could reduce its applicability in real-world scenarios where model assumptions often fail.  
2. Real-World Generalizability: Although the experiments on real datasets are compelling, the price dataset results indicate potential deviations from theoretical predictions, suggesting that further exploration of real-world complexities is needed.  
3. Clarity of Presentation: While the paper is generally well-written, the mathematical sections could benefit from additional intuition to make the results more accessible to a broader audience. For instance, the implications of spectral properties of bipartite graphs could be explained more intuitively.  
Arguments for Acceptance:  
- The paper is technically sound, addresses a significant problem, and provides both theoretical and practical contributions.  
- The results are validated through strong experimental evidence, and the insights are likely to be impactful for both researchers and practitioners in crowdsourcing.  
- The work is original and advances the state of the art in understanding the trade-offs in using control items.  
Arguments against Acceptance:  
- The joint estimator's sensitivity to model misspecification could limit its practical utility.  
- The deviations observed in the price dataset suggest that the theoretical models may not fully capture real-world complexities.  
Recommendation: Accept with minor revisions. The paper is a strong contribution to the field, but addressing the clarity of presentation and further discussing the limitations of the joint estimator under model misspecification would enhance its impact.