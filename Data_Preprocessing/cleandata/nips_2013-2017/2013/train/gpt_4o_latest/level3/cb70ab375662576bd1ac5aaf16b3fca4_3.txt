This paper addresses the challenging problem of learning dynamic models, specifically first-order Markov models (MMs) and hidden Markov models (HMMs), from non-sequential data. The authors propose a novel spectral learning algorithm based on tensor decomposition and the method of moments (MoM), providing the first formal guarantees for this setting. The paper makes a significant contribution by extending spectral learning techniques to a non-sequential context, which is relevant for applications such as modeling galaxies, chronic diseases, and biological processes where sequential data collection is infeasible.
The theoretical foundation of the paper is robust, offering detailed proofs for empirical moments and a sample complexity bound. The derivation of proper moment equations for recovering transition probabilities and Dirichlet prior parameters is a strong point. However, the heuristic for searching parameters, such as the success probability \( r \), would benefit from clearer pseudo-code to aid reproducibility. Additionally, while the theoretical guarantees are compelling, the sample complexity analysis could be refined to explore the trade-offs between the number of sets \( N \) and the size of each set \( n \), which would have practical implications for data collection strategies.
The paper is well-written and addresses an important problem, but there are areas for improvement. The background context could be strengthened by a more thorough review of existing methods, such as maximum likelihood-based approaches, to better motivate the shift to non-sequential learning. The explanation of the tensor decomposition algorithm is dense and could be simplified for clarity. Excluding Algorithm 2 and Theorem 1, as suggested, would improve brevity without sacrificing core contributions.
The experimental validation is a notable weakness. While the simulation results align with theoretical findings, the lack of real-world data experiments limits the practical impact of the work. Furthermore, there is no comparison with conventional algorithms, such as EM-based methods, in terms of accuracy or computational efficiency. Including a speed comparison, especially with computationally cheaper alternatives like matrix eigendecomposition, would strengthen the evaluation.
Figure 1 requires clarification regarding why the projection error is zero only when \( r > 0.3 \), and the logarithm base in the legend should be corrected. These minor issues detract from the overall presentation but are easily addressable.
Strengths:
1. Novel contribution to spectral learning in a non-sequential setting with formal guarantees.
2. Rigorous theoretical analysis, including sample complexity bounds.
3. Well-written and addresses a relevant problem.
Weaknesses:
1. Limited experimental validation with no real-world data or comparison to conventional methods.
2. Dense algorithmic presentation; simplifying tensor decomposition and removing redundant content would help.
3. Insufficient background context on existing methods to motivate the approach.
Recommendation:
This paper makes a valuable theoretical contribution and is well-suited for acceptance, provided the authors address the experimental shortcomings and improve the clarity of the algorithmic presentation.