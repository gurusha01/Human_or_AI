Review
Summary
This paper addresses the intriguing question of whether excess data, beyond the information-theoretic sample complexity limit, can be leveraged to reduce computational complexity in supervised learning tasks. The authors provide a positive answer for the agnostic PAC learning of halfspaces over 3-sparse vectors in \({-1, 1, 0}\), establishing a computational-statistical tradeoff. Conditioning on the hardness of refuting random 3CNF formulas, the paper demonstrates that it is computationally infeasible to efficiently learn this class with \(O(n^2)\) examples, while a new algorithm is proposed that efficiently learns the class with \(\tilde{\Omega}(n^2)\) examples. The paper avoids cryptographic assumptions, instead relying on complexity-theoretic assumptions, and introduces a novel methodology for proving computational-statistical gaps in learning problems. This work contributes to the broader understanding of computational and sample complexity tradeoffs and extends prior work on learning sparse halfspaces.
Strengths
1. Novelty and Originality: The paper introduces a new, non-cryptographic methodology for establishing computational-statistical tradeoffs, which is a significant departure from prior work relying on cryptographic primitives. This approach is both innovative and impactful, as it applies to natural learning problems rather than synthetic constructions.
2. Significance: The results provide a formal separation between information-theoretic and computational sample complexity, a topic of growing interest in the field. The insight that "hardness conjectures" in learning may be weaker than those in other areas is particularly valuable.
3. Technical Depth: The reduction from refuting random 3CNF formulas to learning halfspaces is carefully constructed, and the proofs are rigorous. The use of Feige's hardness assumption and its extensions is well-motivated and aligns with prior complexity-theoretic work.
4. Clarity of Contributions: The paper clearly delineates its contributions, including lower bounds under complexity assumptions and upper bounds for learning sparse halfspaces, making it easy to follow the main results.
5. Relation to Prior Work: The paper situates itself well within the literature, referencing relevant works such as those by Applebaum et al., Hazan et al., and Berthet and Rigollet, while highlighting its distinct contributions.
Weaknesses
1. Assumption Strength: While the paper avoids cryptographic assumptions, the complexity-theoretic assumption used (hardness of refuting random 3CNF formulas) may not necessarily be weaker than the existence of one-way functions. A detailed comparison of the relative hardness of these assumptions would strengthen the theoretical contribution.
2. Upper and Lower Bound Gap: The gap between the upper bound (\(\tilde{O}(n^2)\)) and the lower bound (\(O(n^2)\)) is not fully closed. While the authors conjecture that the gap can be reduced, this remains an open question.
3. Limited Exploration of Stronger Assumptions: The paper could explore whether the results hold under stronger cryptographic assumptions, such as the hardness of factoring, to provide a broader perspective on the generality of the findings.
4. Clarity in Presentation: While the technical content is strong, some aspects of the presentation could be improved. For example, explicitly clarifying that the assumption is not necessarily weaker than the existence of one-way functions would avoid potential misinterpretations (as noted in Keypoint 10).
5. Minor Corrections: Line 130 should reference "proof of item 1," and line 343 likely means \(y_k = b\). These minor errors detract slightly from the polish of the manuscript.
Arguments for Acceptance
- The paper addresses a fundamental and timely question in learning theory, contributing both novel techniques and insights.
- The results are technically sound, with rigorous proofs and a clear connection to prior work.
- The separation between information-theoretic and computational sample complexity is a significant contribution to the field.
Arguments Against Acceptance
- The reliance on the hardness of refuting random 3CNF formulas as an assumption may limit the generality of the results.
- The gap between the upper and lower bounds remains unresolved, leaving room for improvement in the completeness of the work.
Recommendation
This paper makes a strong theoretical contribution to the field of learning theory by introducing a novel methodology for computational-statistical tradeoffs and providing new insights into the role of excess data in learning. While there are some areas for improvement, particularly in the discussion of assumptions and the gap between bounds, the paper is of high quality and should be accepted.