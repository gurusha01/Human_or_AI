This paper presents an innovative extension of Factorized Asymptotic Bayesian (FAB) inference to Latent Feature Models (LFMs), addressing a significant limitation of previous FAB methods that required restrictive conditions on the Hessian matrix of the complete log-likelihood. By generalizing the Factorized Information Criterion (FIC) to LFMs and introducing a mean-field approximation with accelerated feature shrinkage, the authors propose a computationally efficient and automated model selection framework. The method demonstrates strong empirical performance in terms of model selection, prediction accuracy, and computational efficiency, outperforming state-of-the-art approaches like Indian Buffet Processes (IBP) and Variational Bayesian (VB) methods on synthetic and real-world datasets.
Strengths:
1. Technical Contribution: The paper makes a significant theoretical advancement by deriving FIC for LFMs, showing its equivalence to that for mixture models despite LFMs not meeting the original FAB assumptions. This is a notable generalization of the FAB framework.
2. Practical Utility: The proposed method eliminates the need for manual hyperparameter tuning and achieves automatic feature selection, which is a valuable contribution for practitioners.
3. Computational Efficiency: The introduction of an accelerated shrinkage mechanism drastically reduces computational costs, as demonstrated in the experiments.
4. Thorough Evaluation: The method is rigorously compared against competing approaches across multiple datasets, with results showing superior computational efficiency and predictive performance.
5. Clarity and Structure: The paper is well-written, logically structured, and adequately referenced, making it accessible to readers.
6. Theoretical Insights: The analysis of parameter identifiability and the proposed post-processing (e.g., merging latent features) provide additional depth and robustness to the method.
Weaknesses:
1. Limitations of Approximation: While the authors acknowledge that the mean-field approximation and other assumptions may introduce inaccuracies, there is no detailed analysis of when and how these approximations might lead to suboptimal performance. For instance, scenarios with small sample sizes or highly complex data distributions could be problematic.
2. Scalability to Large Datasets: Although the method is computationally efficient, the experiments do not extensively evaluate scalability to extremely large datasets or high-dimensional feature spaces.
3. Sensitivity Analysis: The paper lacks a sensitivity analysis of key parameters (e.g., initialization of latent features) and their impact on the algorithm's convergence and performance.
Arguments for Acceptance:
- The paper addresses a challenging and relevant problem in Bayesian inference and latent feature modeling.
- It provides a novel and well-supported extension of FAB inference, advancing the state of the art.
- The empirical results are compelling, demonstrating both theoretical and practical contributions.
Arguments Against Acceptance:
- The lack of a detailed analysis of the limitations of the approximations used could hinder reproducibility and applicability in certain scenarios.
- The scalability of the method to very large datasets remains somewhat unclear.
Recommendation:
Overall, this paper makes a strong contribution to the field of Bayesian inference and latent feature modeling. While there are minor limitations, they do not significantly detract from the quality and impact of the work. I recommend acceptance, with a suggestion to include a more detailed discussion of the limitations and potential failure cases of the proposed approach.