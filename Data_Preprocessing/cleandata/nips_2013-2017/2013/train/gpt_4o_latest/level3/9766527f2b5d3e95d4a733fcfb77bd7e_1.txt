The paper addresses an important issue in stochastic gradient descent (SGD) optimization: the high variance of noisy gradients, which can hinder convergence and performance. The authors propose a general approach using control variates to reduce this variance, leveraging low-order moments of the data to construct control variates that are computationally efficient and highly correlated with the noisy gradient. They demonstrate the approach on two problems: MAP estimation for logistic regression (convex) and stochastic variational inference (SVI) for latent Dirichlet allocation (LDA, non-convex). Experimental results show improved convergence and predictive performance compared to standard SGD.
Strengths:
1. Relevance and Scope: The paper tackles a significant problem in stochastic optimization, making it relevant to the broader machine learning community.
2. Theoretical Foundation: The authors provide a solid theoretical framework, demonstrating that their variance-reduced gradient remains unbiased and leads to faster convergence rates.
3. Practical Examples: The application of the method to both convex (logistic regression) and non-convex (LDA) problems illustrates the versatility of the approach.
4. Empirical Results: The experiments show promising improvements in convergence speed and predictive performance, particularly for LDA.
Weaknesses:
1. Novelty: While the use of control variates in variance reduction is well-established, the paper primarily extends prior work rather than introducing a fundamentally new method. The authors should explicitly clarify the novelty of their contributions and how they differ from existing approaches, such as those in [Paisley et al., 2012], which is notably missing from the citations.
2. Experimental Design: The comparison between variance-reduced SGD and standard SGD is flawed due to the use of the same fixed step size for both methods. Optimal step sizes should be tuned separately to ensure a fair comparison.
3. Computational Overhead: The paper does not analyze the computational complexity of the proposed method or provide comparisons in terms of CPU or wall-clock time. This omission makes it difficult to assess the practical trade-offs of using control variates.
4. Clarity of Presentation: While the theoretical sections are rigorous, the paper could benefit from clearer explanations of the practical implementation details, particularly for constructing control variates in LDA.
Pro and Con Arguments for Acceptance:
- Pro: The paper addresses a critical problem, provides a sound theoretical basis, and demonstrates empirical improvements on real-world problems.
- Con: The lack of novelty, unfair experimental comparisons, and missing computational complexity analysis detract from its overall contribution.
Recommendations for Improvement:
1. Include a detailed comparison with prior work, particularly [Paisley et al., 2012], and explicitly highlight the novel aspects of the proposed approach.
2. Redesign the experiments to use optimal step sizes for each method and include computational time comparisons to evaluate overhead.
3. Add a computational complexity analysis to quantify the cost of using control variates.
4. Improve clarity by providing more intuitive explanations and practical implementation details.
Conclusion: The paper makes a meaningful contribution to variance reduction in stochastic optimization, but its lack of novelty, experimental design flaws, and missing computational analysis limit its impact. With revisions, it could become a stronger submission.