The paper proposes a novel method, FIRE (Fredholm Inverse Regularized Estimator), for estimating the ratio of two probability density functions by formulating the problem as an inverse problem using a Fredholm integral equation. This approach is grounded in classical operator theory and leverages regularization techniques within the Reproducing Kernel Hilbert Space (RKHS) framework. The authors provide theoretical guarantees, including concentration bounds and convergence rates for Gaussian kernels, and introduce an unsupervised parameter selection method, CD-CV (Cross-Density Cross-Validation). Experimental results demonstrate the method's effectiveness in density ratio estimation and its applications to tasks such as classification under covariate shift.
Strengths:
1. Novelty: The paper introduces a principled framework for density ratio estimation by connecting it to classical inverse problems, which is a fresh perspective compared to existing methods like Kernel Mean Matching (KMM) and Least Squares Importance Sampling (LSIF). The use of Fredholm integral equations is particularly innovative.
2. Theoretical Rigor: The authors provide detailed theoretical analysis, including error bounds and convergence rates, for both Euclidean spaces and sub-manifolds. This adds significant credibility to the proposed method.
3. Flexibility: The FIRE framework supports multiple norms and regularization techniques, making it adaptable to various settings. The ability to handle both density functions and arbitrary functions (Type I and Type II settings) is a notable advantage.
4. Unsupervised Model Selection: The CD-CV method addresses a critical challenge in unsupervised/semi-supervised learning by providing a practical mechanism for parameter tuning.
5. Experimental Validation: The paper includes extensive experiments comparing FIRE to baseline methods (e.g., TIKDE, LSIF) across multiple datasets and tasks, demonstrating its superior performance and computational efficiency.
Weaknesses:
1. Lack of Clarity in Assumptions: The paper does not clearly specify the assumptions regarding the nature of the space \(x\) belongs to (e.g., \(R^d\) vs. general topological spaces) or whether the density \(p(x)\) is positive everywhere. This omission raises questions about the generality and applicability of the results.
2. Dominating Measure Ambiguity: The choice of the dominating measure \(\nu\) (e.g., Lebesgue or counting measure) is not explicitly stated, which could impact the rigor and reproducibility of the theoretical results.
3. Experimental Comparisons: While the experiments are comprehensive, the paper does not compare FIRE to KMM in scenarios where out-of-sample extensions are not required. This limits the scope of the evaluation.
4. Complexity of Presentation: The paper is dense and highly technical, which may hinder accessibility for readers unfamiliar with inverse problems or RKHS theory. Simplifying the exposition or providing more intuitive explanations could improve clarity.
Recommendation:
The paper makes a significant contribution to the field of density ratio estimation and its applications in machine learning. Its theoretical rigor, innovative approach, and practical utility make it a strong candidate for acceptance. However, the authors should address the ambiguities in assumptions and provide clearer explanations to enhance the paper's accessibility and impact.
Arguments for Acceptance:
- Novel and principled approach to a challenging problem.
- Strong theoretical foundations with detailed analysis.
- Practical utility demonstrated through experiments.
Arguments Against Acceptance:
- Lack of clarity in assumptions and dominating measure.
- Limited comparison to certain baseline methods.
- Dense and technical presentation.
Overall, the strengths outweigh the weaknesses, and I recommend acceptance with minor revisions to address the noted concerns.