The paper introduces a novel inference approach for Ising models by projecting parameters onto a fast-mixing model space, enabling Gibbs sampling to converge rapidly. The authors define fast-mixing models by bounding the spectral norm of the Ising edge strength matrix while preserving the graph structure. They propose projection methods based on various divergences (KL, piecewise KL, reversed KL) and Euclidean distance, utilizing algorithms like LBFGS-B and projected gradient descent. Notably, the reversed KL divergence requires Gibbs sampling on the fast-mixing model to compute the projection. Experimental results on small random models demonstrate that the proposed methods outperform traditional inference techniques like TRW, MF, LBP, and Gibbs sampling in accuracy, with reversed KL divergence yielding the best performance. Additionally, the methods achieve favorable accuracy-runtime trade-offs compared to Gibbs sampling on the original model.
Strengths:
1. Novelty and Originality: The paper introduces a new notion of tractability by focusing on fast-mixing models rather than traditional tree-based approximations. This is a significant departure from existing methods and provides a fresh perspective on approximate inference.
2. Technical Soundness: The theoretical foundation, particularly the spectral norm-based guarantee for fast mixing, is well-grounded. The use of dual optimization for Euclidean projection and the exploration of multiple divergences add depth to the methodology.
3. Empirical Validation: The experiments convincingly demonstrate the superiority of the proposed methods in accuracy over standard inference techniques. The reversed KL divergence, in particular, shows strong empirical performance.
4. Practical Relevance: The approach addresses a critical limitation of Gibbs sampling—its slow convergence for strong interactions—making it a potentially valuable tool for practitioners dealing with high-treewidth models.
Weaknesses:
1. Runtime and Practical Utility: While the proposed methods improve accuracy, the high computational cost of projection (e.g., 30,000 Gibbs iterations for KL(ψ||θ)) raises concerns about scalability and practical utility for larger models. This limitation is acknowledged but not thoroughly addressed.
2. Experimental Scope: The experiments are limited to small graphs (e.g., 8×8 grids and 10-node random graphs). Extending the evaluation to larger models, such as those in Boris Flach's work, would strengthen the paper's claims.
3. Algorithmic Details: The stopping criteria for the inner LBFGS-B loop in the projected gradient algorithm are unclear, which may hinder reproducibility.
4. Plot and Reporting Issues: The horizontal axis in Figure 2 is ambiguous, and absolute runtimes for all algorithms (LBP, TRW, MF) are not reported, making runtime comparisons less transparent.
5. Conservativeness of Spectral Norm Bound: The spectral norm bound is overly conservative, as highlighted by the discrepancy with the known threshold for the Ising model. A tighter bound would enhance the theoretical rigor and practical applicability of the method.
Suggestions for Improvement:
1. Clarify the stopping criteria for the LBFGS-B loop and provide more details on parameter tuning for the stochastic gradient descent algorithm.
2. Include experiments on larger models to demonstrate scalability and broader applicability.
3. Report absolute runtimes for all algorithms to enable fair comparisons.
4. Address the ambiguity in Figure 2 and clarify whether the horizontal axis represents runtime or "number of samples."
5. Explore tighter bounds for the spectral norm to improve the theoretical guarantees of the method.
Recommendation:
This paper presents a novel and technically sound approach to inference in Ising models, with strong empirical results on small graphs. However, concerns about scalability, runtime, and the conservativeness of the spectral norm bound limit its immediate practical impact. I recommend acceptance with minor revisions, provided the authors address the runtime reporting, clarify algorithmic details, and expand the experimental scope.