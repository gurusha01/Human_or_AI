The paper addresses the problem of minimizing a convex smooth loss function with trace norm regularization, a topic of significant interest in machine learning due to its applications in areas like matrix completion and multi-task learning. The authors establish a novel result: the proximal gradient method (PGM) achieves linear convergence for a broad class of loss functions, even without the strong convexity assumption on the loss. This is achieved through a new Lipschitzian error bound, which is a key theoretical contribution and resolves an open question in the field.
Strengths:
1. Theoretical Contribution: The paper extends prior work on linear convergence of PGM, previously limited to lasso and group lasso, to trace norm regularization. This generalization is significant given the non-polyhedral nature of the trace norm's epigraph, which complicates analysis.
2. Error Bound: The introduction of a Lipschitzian error bound for trace norm–regularized problems is a major technical achievement and could have broader implications for other matrix norm–regularized problems.
3. Clarity of Proofs: The proofs are generally clear and well-structured, leveraging the framework of Luo and Tseng to establish linear convergence rigorously.
4. Numerical Validation: The inclusion of numerical experiments, such as semi-log plots for matrix completion and classification, empirically supports the theoretical claims of linear convergence.
Weaknesses:
1. Limited Novelty: While the extension to trace norm regularization is valuable, the contribution is primarily technical and builds heavily on existing frameworks (e.g., Tseng et al.). The novelty may be perceived as incremental rather than groundbreaking.
2. Constant Dependencies: The paper does not adequately explain how the constants (\(\kappa1, \kappa2, \kappa3, \kappa4\)) depend on key parameters (\(\underbar{\alpha}, \bar{\alpha}\)). This lack of clarity may hinder the practical applicability of the results.
3. Proof Referencing: The proofs closely resemble prior work, and more precise referencing is needed to delineate the authors' contributions from existing literature.
4. Numerical Example: While numerical experiments are provided, a more detailed discussion of the results and their implications would strengthen the paper. Additionally, the semi-log plots could be better annotated for clarity.
5. Minor Issues: The sequence of inequalities leading to inequality (13) is confusing and requires clarification. This could improve the paper's overall readability.
Suggestions for Improvement:
1. Include a simulation where the loss is not strongly convex but PGM converges linearly, and discuss when the theory fails. This would provide deeper insights into the limitations of the proposed approach.
2. Extend the analysis to the alternating direction method of multipliers (ADMM), as requested, to broaden the paper's applicability.
3. Clarify the dependencies of the constants on problem parameters and improve the exposition of inequality (13).
4. Provide more precise citations to differentiate the authors' contributions from prior work.
Recommendation:
The paper makes a solid theoretical contribution by extending linear convergence results to trace norm regularization and introducing a Lipschitzian error bound. However, the limited novelty, lack of clarity in some aspects, and the need for additional simulations and discussions temper its impact. I recommend acceptance with minor revisions, provided the authors address the issues outlined above.