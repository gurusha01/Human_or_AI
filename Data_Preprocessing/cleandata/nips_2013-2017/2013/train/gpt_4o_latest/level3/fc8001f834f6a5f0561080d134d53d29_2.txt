The paper introduces a novel "learning-to-prune" approach for VP-trees in both metric and non-metric spaces, leveraging a piecewise linear approximation of the decision function to enhance nearest neighbor (NN) search efficiency. The authors evaluate their method on datasets with Euclidean, KL-divergence, and Itakura-Saito distance functions, comparing it against state-of-the-art methods like multi-probe LSH, permutation methods, and the bbtree. The proposed method demonstrates competitive performance, particularly for datasets with low intrinsic dimensionality, achieving significant speed-ups while maintaining rank approximation quality. The authors also discuss the theoretical applicability of their method to non-metric spaces, supported by a formal proof.
Strengths:
1. Originality: The "learning-to-prune" approach for VP-trees is novel, particularly in its application to non-metric spaces, where few methods exist. This work represents a meaningful contribution to the field of approximate NN search.
2. Performance: The method demonstrates strong performance on datasets with low or moderate intrinsic dimensionality, often outperforming existing methods in terms of speed for comparable rank approximation quality.
3. Clarity of Applicability: The authors provide a theoretical foundation for the applicability of their method to non-metric spaces, supported by a proof, which enhances the scientific rigor of the work.
4. Implementation and Reproducibility: The availability of the code and datasets ensures reproducibility, which is a commendable effort by the authors.
Weaknesses:
1. Dataset Limitations: The evaluation is conducted on low-dimensional or low-intrinsic-dimensional datasets, which limits the generalizability of the results to high-dimensional, real-world datasets. The method's performance in such scenarios remains unclear.
2. Metric Space Comparisons: The paper does not adequately justify the focus on non-metric spaces while omitting comparisons with strong metric-space baselines like LSH embeddings. This raises questions about the broader applicability of the method.
3. Parameter Choices: The paper lacks detailed explanations for key parameter choices (e.g., \(K=100\), \(m=7\), \(\rho=8\), bucket size=\(10^5\)), making it harder for readers to assess the robustness of the method.
4. Scalability: While the method performs well on datasets with low intrinsic dimensionality, its scalability to high-dimensional datasets, where the curse of dimensionality is more pronounced, is not convincingly addressed.
Pro and Con Arguments for Acceptance:
- Pro: The paper introduces a novel and well-motivated approach to a fundamental problem, supported by theoretical analysis and competitive experimental results.
- Con: The limited scope of datasets and the lack of comparisons with relevant baselines in metric spaces reduce the paper's significance and generalizability.
Suggestions for Improvement:
1. Include experiments on high-dimensional datasets to better assess scalability and robustness.
2. Provide detailed justifications for parameter choices and discuss their impact on performance.
3. Compare the method against stronger metric-space baselines to contextualize its advantages and limitations.
Conclusion:
The paper is a valuable contribution to the field of approximate NN search, particularly in non-metric spaces. However, its limited evaluation scope and lack of detailed parameter analysis temper its significance. I recommend acceptance with minor revisions to address the noted weaknesses.