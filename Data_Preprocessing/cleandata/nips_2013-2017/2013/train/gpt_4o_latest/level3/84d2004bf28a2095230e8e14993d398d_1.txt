This paper presents GREEDI, a distributed protocol for submodular function maximization under cardinality constraints, addressing the challenges of handling massive datasets in a distributed setting. The authors provide theoretical guarantees for the performance of GREEDI and demonstrate its effectiveness through large-scale experiments on exemplar-based clustering and active set selection. The study is a valuable contribution to distributed data mining, particularly for applications requiring scalable solutions to submodular optimization problems.
Strengths:
1. Relevance and Novelty: The paper tackles a significant problem in distributed machine learning, proposing a novel two-stage protocol that leverages MapReduce-style computations. The theoretical analysis of GREEDI, including bounds under natural conditions, adds rigor to the contribution.
2. Scalability: The experiments on large datasets, including 80 million Tiny Images and Yahoo!'s Today Module, convincingly demonstrate the scalability of the method.
3. Practical Applications: The paper effectively connects GREEDI to real-world applications like exemplar-based clustering and sparse Gaussian process inference, showcasing its utility.
4. Theoretical Insights: The authors provide detailed theoretical guarantees, including worst-case bounds and performance improvements under geometric assumptions, which are valuable for understanding the algorithm's behavior.
5. Experimental Rigor: The experiments are extensive, comparing GREEDI against multiple baselines and demonstrating its superiority in terms of performance.
Weaknesses:
1. Notation Clarity: The use of similar symbols (kappa and k) in Theorem 4.2 is confusing and should be revised for better clarity. This issue could hinder comprehension for readers unfamiliar with the context.
2. Tightness of Bounds: The tightness of the bound in Theorem 4.2, particularly the factor min(m, k), is not sufficiently discussed. Addressing this would strengthen the theoretical contribution.
3. Experimental Metrics: The experiments lack generalized performance metrics like negative log predictive probability, which would better assess the generalization capabilities of GREEDI.
4. Decomposable Functions: It is unclear which experiments specifically address decomposable functions in Section 5. This omission makes it harder to evaluate the claims in Section 4.5.
5. Figure Issues: There are inconsistencies in the figures. For instance, the x-axis label in Figure 1(e) should be corrected to "k," and the authors should clarify why the ratio at the smallest m starts below 1 in Figure 1(f) but starts from 1 in Figures 1(a)-1(d). Additionally, the dip observed when k=10 in Figure 1(c) warrants explanation.
6. Missing References: Adding references to Yahoo!'s Today Module would help readers explore related research and contextualize the experiments.
Recommendation:
While the paper makes a strong contribution to distributed submodular maximization, the issues with clarity, notation, and experimental details need to be addressed. The theoretical and experimental results are compelling, but improving the presentation and addressing the weaknesses would make the paper more robust. I recommend acceptance, provided the authors revise the manuscript to address the above concerns.