This paper presents an application of the Classification-Based Modified Policy Iteration (CBMPI) algorithm to the game of Tetris, a challenging benchmark in reinforcement learning (RL) and approximate dynamic programming (ADP). The authors demonstrate that CBMPI achieves competitive results with the state-of-the-art Cross-Entropy (CE) method, while using significantly fewer samples. This marks a notable advancement, as traditional ADP methods have historically underperformed in Tetris compared to black-box optimization techniques.
Strengths:
1. Relevance and Contribution: The paper addresses a long-standing challenge in RL by applying CBMPI to Tetris, achieving results comparable to CE. This is a significant contribution, as it demonstrates that ADP methods can be competitive with black-box optimizers in complex domains.
2. Empirical Results: The experimental results are robust and convincing. CBMPI outperforms previous RL attempts and achieves state-of-the-art performance in the small 10×10 board while matching CE in the large 10×20 board with only 1/6th of the sample budget.
3. Clarity and Writing: The paper is well-written and organized, making it accessible to readers. The detailed experimental setup and comparisons with existing methods enhance its reproducibility and credibility.
4. Significance: The work highlights the potential of policy-space search methods over value-function-based approaches in Tetris, providing valuable insights for future research in RL and ADP.
Weaknesses:
1. Discussion Gap: The paper lacks a thorough explanation of the observed performance improvements. Specifically, the role of the state distribution (biased towards small board heights) and the choice of the CMA-ES optimizer in CBMPI are not adequately analyzed. Exploring alternative state distributions and optimizers could strengthen the conclusions.
2. Statistical Relevance: Given the high variance in Tetris scores, the absence of confidence intervals in the performance graphs is a notable omission. This weakens the statistical rigor of the results.
3. Policy Comparison: The method for computing scores for baseline policies like DU and BDU is not clearly explained. Additionally, the performance of CE in the authors' experiments should be explicitly reported for direct comparison.
4. Sample Efficiency: While CBMPI is more sample-efficient than CE, it is unclear whether its performance would improve with additional samples or if it has reached a plateau. This could be explored further.
5. Code Sharing: The authors do not clarify whether they will share their code, which is critical for reproducibility and community adoption.
Arguments for Acceptance:
- The paper addresses a challenging problem and achieves state-of-the-art results, making it a valuable contribution to the field.
- The insights into policy-space search methods are significant and could inspire further research in RL and ADP.
Arguments Against Acceptance:
- The lack of detailed discussion on key design choices and performance factors limits the interpretability and generalizability of the results.
- Statistical rigor could be improved with confidence intervals and clearer reporting of baseline comparisons.
Recommendation: Accept with minor revisions. Addressing the discussion gaps, clarifying policy comparisons, and including confidence intervals would significantly enhance the paper's impact.