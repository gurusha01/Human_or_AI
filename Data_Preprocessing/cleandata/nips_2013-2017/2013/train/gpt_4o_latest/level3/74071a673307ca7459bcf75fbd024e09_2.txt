This paper addresses the problem of asymmetric regret guarantees in online learning, specifically in the context of the absolute loss game. The authors provide a detailed characterization of regret trade-offs between two experts (K=2) over a known horizon \( T \), and introduce an algorithm that achieves any trade-off on the Pareto frontier. The primary contributions, Theorems 6 and 8, are significant. Theorem 6 offers a tight minimax characterization of the achievable trade-offs, while Theorem 8 extends this to asymptotic regret rates, providing a smooth limit curve for large \( T \). Additionally, Theorem 10 generalizes the results to the 2-dimensional expert problem under dot loss, suggesting broader applicability.
Strengths:
1. Theoretical Contribution: The exact characterization of the Pareto frontier for \( K=2 \) experts is a valuable contribution to the field. The derivation of the optimal strategies and the asymptotic analysis are rigorous and insightful.
2. Novelty: The paper addresses an underexplored area by focusing on asymmetric regret guarantees and multi-objective optimization, which are less commonly studied compared to uniform regret bounds.
3. Broader Implications: The generalization to dot loss (Theorem 10) and the discussion on extending the results to \( K > 2 \) experts hint at potential future applications and extensions.
4. Clarity in Key Results: The Pareto frontier characterization and its connection to random walks are well-explained, and the figures effectively illustrate the theoretical findings.
Weaknesses:
1. Limited Scope: The results are restricted to \( K=2 \) experts, which limits their practical applicability. While Theorem 10 generalizes to dot loss, the paper does not provide concrete results for \( K > 2 \), which is more relevant for real-world scenarios.
2. Unclear Motivation: The motivation behind the key function \( f_T(i) \) is not adequately explained. A deeper discussion on its intuition and relevance would enhance the paper's accessibility.
3. Section 5.2 Clarity: The discussion on recovering standard \( \sqrt{T \log K} \) bounds from the \( K=2 \) algorithm is unclear and lacks justification for its utility. This section could benefit from a more structured explanation and examples.
4. Practical Relevance: The paper does not address modern regret budgets (e.g., \( \sqrt{L_k T} \), \( \sqrt{\text{Var}} \)), which are increasingly important in online learning. Incorporating these would make the results more impactful.
5. Horizon Dependence: The results rely on a known horizon \( T \), which is a limitation in settings where \( T \) is unknown or dynamic.
Recommendation:
While the paper makes significant theoretical contributions, its practical impact is limited due to the focus on \( K=2 \) experts and the lack of results for \( K > 2 \). The unclear motivation for \( f_T(i) \) and the weak justification in Section 5.2 further detract from its clarity. However, the novelty and rigor of the results warrant acceptance, provided the authors address these concerns in a revised version. 
Arguments for Acceptance:
- Rigorous theoretical contributions (Theorems 6, 8, and 10).
- Novel focus on asymmetric regret guarantees and multi-objective optimization.
- Potential for future extensions to \( K > 2 \) and modern regret budgets.
Arguments Against Acceptance:
- Limited practical relevance due to the restriction to \( K=2 \).
- Lack of clarity in key motivations and certain sections.
- No concrete results for \( K > 2 \), which is a significant limitation.
Final Decision: Weak Accept