The paper presents a spectral algorithm for learning Hidden Markov Models (HMMs) from non-sequential observations, a problem relevant to scientific domains such as modeling galaxies, chronic diseases, and biological processes. The authors propose a method inspired by Latent Dirichlet Allocation (LDA) and moment-matching techniques, leveraging tensor decomposition to recover HMM parameters. They provide theoretical guarantees, including finite-sample bounds, and demonstrate the algorithm's behavior on synthetic data. However, the experimental evaluation is limited, leaving real-world applicability unexplored.
Strengths:
1. Novelty and Scope: The paper addresses a challenging and underexplored problem: learning dynamic models from non-sequential data. The generative model and its analysis show novelty, particularly in adapting tensor decomposition methods for this setting. The connection to LDA-like models is insightful and well-motivated.
2. Theoretical Contributions: The authors provide finite-sample bounds and formal guarantees for parameter recovery, which is a significant theoretical advancement for non-sequential HMM learning.
3. Clarity: The paper is well-written, with clear intuitions and detailed explanations of the generative process and algorithm. The mathematical exposition, though dry, is rigorous.
4. Simulation Results: The synthetic experiments confirm the theoretical findings, demonstrating convergence of parameter estimates with increasing sample size.
Weaknesses:
1. Limited Experimental Validation: The paper lacks evaluation on real-world datasets, which is critical for assessing the practical significance of the method. The authors acknowledge this as future work, but it limits the immediate impact of the paper.
2. Comparative Analysis: The paper does not compare its method to existing approaches for learning from non-sequential data, such as those based on Expectation-Maximization (EM). This omission makes it difficult to assess the relative performance and advantages of the proposed method.
3. Interpretability of Bounds: While finite-sample bounds are provided, they are complex and hard to interpret, limiting their practical utility for practitioners.
4. Prior Work: The problem of learning HMMs from non-sequential data has been studied before (e.g., ICML 2013), and the proof techniques used here are similar to prior spectral learning work. The paper could better differentiate its contributions from existing literature.
5. Scalability: The sample complexity bounds suggest a high polynomial dependence on parameters like the number of states and observations, raising concerns about scalability to large datasets.
Pro vs. Con Arguments:
- Pro: The paper makes a novel theoretical contribution to a challenging problem, with rigorous analysis and promising preliminary results.
- Con: The lack of real-world experiments and comparative analysis limits the practical and empirical significance of the work.
Recommendation: While the paper makes a solid theoretical contribution, its practical significance remains uncertain due to the lack of real-world validation and comparative analysis. I recommend acceptance only if the conference prioritizes theoretical advancements. Otherwise, the paper would benefit from additional experiments and clearer differentiation from prior work.
Minor Issue: Typo on line 107: "V2" should be "X2."