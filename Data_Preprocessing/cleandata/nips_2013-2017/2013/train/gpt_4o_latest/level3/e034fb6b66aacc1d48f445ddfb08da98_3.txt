The paper introduces a novel approach, Advise, which employs a Bayesian framework for integrating binary human feedback into reinforcement learning (RL). The authors argue for a paradigm shift from reward/value shaping to "Policy Shaping," where feedback is treated as direct policy advice rather than being converted into rewards. The proposed method extends Bayesian Q-learning (BQL) to incorporate human feedback, aiming to maximize its utility while being robust to inconsistencies and infrequent feedback. The paper compares Advise to state-of-the-art methods like Action Biasing, Control Sharing, and Reward Shaping, demonstrating its robustness and competitive performance across various experimental conditions.
Strengths  
The paper addresses an important problem in interactive RL by proposing a theoretically grounded and practical method for leveraging human feedback. The use of Bayesian principles to model feedback consistency and likelihood is a notable strength, as it provides a principled way to integrate human input without relying on ad hoc parameter tuning. The experimental results convincingly demonstrate Advise's robustness to noisy and sparse feedback, outperforming or matching state-of-the-art approaches. Additionally, the authors highlight limitations of existing methods, such as their dependence on domain-specific tuning parameters, and show how Advise avoids these pitfalls. The paper is well-situated in the literature, with appropriate references to related work in reward shaping, feedback-based RL, and transfer learning.
Weaknesses  
Despite its strengths, the paper has several issues. First, the claim that Advise does not convert feedback into values is misleading, as the calculation of Î”_{s,a} effectively assigns numerical values to feedback. The term "Policy Shaping" is also somewhat misleading, as the feedback is action-specific rather than policy-wide; "Action Shaping" would be more accurate. Clarity is another concern: Formula (2) contradicts the stated assumption of action optimality being independent of other labels in the same state, and the explanation of how feedback is provided (binary labels after actions) appears too late in the text. Additionally, the use of pre-recorded human demonstrations to construct a simulated oracle is insufficiently detailed, leaving readers unclear about its implementation. The paper defers critical challenges, such as credit assignment and handling multiple optimal actions per state, to future work, which limits its originality. Finally, the improvement in performance from human feedback is not substantial enough to justify the additional complexity introduced by the algorithm, raising concerns about its practical significance.
Arguments for Acceptance  
- The paper provides a theoretically sound and novel approach to integrating human feedback into RL.  
- It highlights and addresses key limitations of existing methods, such as reliance on domain-specific parameters.  
- The experimental results demonstrate robustness to noisy and infrequent feedback, a critical challenge in real-world applications.  
Arguments Against Acceptance  
- The paper's claims about not converting feedback into values and its use of "Policy Shaping" terminology are misleading.  
- Clarity issues, particularly around Formula (2) and the simulated oracle, hinder reproducibility.  
- The method's practical significance is limited, as the performance gains from human feedback are modest.  
- Key challenges, such as credit assignment and multiple optimal actions, are deferred to future work, reducing the paper's originality.  
Recommendation  
While the paper makes a valuable contribution to the field of interactive RL, its clarity issues, overstated claims, and limited practical significance suggest it is not yet ready for acceptance. A major revision addressing these concerns would strengthen its impact.