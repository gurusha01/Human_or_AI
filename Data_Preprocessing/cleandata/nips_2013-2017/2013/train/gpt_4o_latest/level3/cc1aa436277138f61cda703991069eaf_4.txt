This paper addresses the critical problem of aggregating low-quality crowdsourced answers to achieve more accurate results, specifically focusing on determining the optimal number of control examples required for accurate estimation. The authors analyze two estimation strategies—two-stage and joint estimation—using a Gaussian model with worker ability parameters. They provide theoretical insights and empirical results, offering practical rules of thumb for crowdsourcing practitioners. While the paper makes a valuable contribution to understanding the trade-offs in using control items, it has notable strengths and weaknesses.
Strengths:
1. Novelty and Practical Relevance: The paper tackles a significant problem in crowdsourcing, offering actionable guidance on the optimal allocation of control items. The theoretical results, such as the scaling laws for the two-stage and joint estimators, are insightful and align well with empirical findings.
2. Theoretical Rigor: The analysis is mathematically sound under the Gaussian model assumptions, and the authors provide clear derivations for the expected mean square error (MSE) of the two estimators.
3. Comprehensive Experiments: The experiments on both simulated and real-world datasets (e.g., price estimation and NFL forecasting) validate the theoretical predictions and highlight practical considerations, such as model misspecification and heteroskedasticity.
4. Practical Guidance: The paper offers clear recommendations for practitioners, such as using minimal control items for joint estimators when assumptions hold and switching to two-stage estimators in cases of model misspecification.
Weaknesses:
1. Unrealistic Assumptions: The analysis assumes uniform task assignments to workers, which is rarely the case in real-world crowdsourcing scenarios. The lack of evaluation under non-uniform task assignments limits the practical applicability of the results.
2. Limited Robustness: The joint estimator, while efficient under ideal conditions, is shown to be highly sensitive to model misspecification. This issue is acknowledged but not sufficiently addressed in the paper.
3. Lack of Discussion on Discrete Models: The paper focuses exclusively on continuous quantities and Gaussian models, without extending the method to discrete tasks, which are common in crowdsourcing.
4. Missing References: The authors fail to cite relevant prior work on statistical quality control with control items, such as Tang & Lease (CIR11) and Kajino & Kashima (HCOMP12), which could provide additional context and connections to existing literature.
Arguments for Acceptance:
- The paper provides a solid theoretical foundation and practical insights for a relevant crowdsourcing problem.
- The experiments are thorough and support the theoretical claims.
Arguments Against Acceptance:
- The reliance on unrealistic assumptions (e.g., uniform task assignments) and the lack of robustness to model misspecification reduce the practical utility of the proposed methods.
- The omission of related work and discrete models leaves the contribution somewhat incomplete.
Recommendation: Conditional acceptance. The paper is a strong contribution but would benefit from addressing robustness to non-uniform task assignments, discussing extensions to discrete models, and incorporating missing references. These improvements would enhance its impact and practical relevance.