The paper introduces Advise, a novel method for interactive reinforcement learning (IRL) that leverages human feedback as direct policy labels rather than converting it into rewards or values. This approach, termed Policy Shaping, is a significant departure from traditional methods that rely on reward shaping or value-based interpretations of feedback. The authors argue that treating human feedback as policy advice is more effective, particularly in scenarios with infrequent or inconsistent feedback. The method is grounded in Bayesian principles and combines human feedback with the agent's learned policy using Bayesian Q-learning (BQL). Experimental results demonstrate that Advise performs on par with or better than state-of-the-art methods such as Action Biasing and Control Sharing, while being more robust to noisy or sparse feedback.
Strengths:
1. Clarity and Writing: The paper is well-written, with a clear exposition of the problem, methodology, and experimental results. The authors effectively situate their work within the existing literature, referencing relevant prior work and highlighting the limitations of current approaches.
2. Simplicity and Robustness: Advise is a simple yet effective method that requires fewer meta-parameters than competing approaches. Its single meta-parameter, the estimated feedback consistency (Äˆ), is intuitive and domain-independent, making the method robust to variations in feedback quality and domain size.
3. Experimental Validation: The experiments are thorough, comparing Advise to state-of-the-art methods across multiple scenarios with varying feedback consistency and frequency. Results consistently show that Advise is more robust and achieves higher cumulative rewards in challenging conditions.
4. Practical Contributions: The method's robustness and simplicity make it a compelling choice for real-world applications where human feedback is often noisy or sparse.
Weaknesses:
1. Limited Benchmarks: The experimental evaluation is restricted to two relatively simple domains (Pac-Man and Frogger). While these benchmarks are useful for proof-of-concept, they do not fully demonstrate the scalability or applicability of Advise to more complex, real-world tasks.
2. Theoretical Innovation: While the use of Bayesian principles is well-justified, the theoretical contributions are incremental compared to existing methods. The primary novelty lies in the interpretation of human feedback as policy advice, which, while impactful, is not a fundamentally new algorithmic framework.
3. Minor Presentation Issues: Some figures and tables are small and difficult to interpret. Additionally, there are minor grammatical errors and instances of unclear phrasing that could benefit from revision.
Recommendation:
I lean slightly toward acceptance. Advise is a compelling method that advances the state of the art in IRL by addressing key limitations of existing approaches. Its simplicity, robustness, and practical utility outweigh its weaknesses, such as limited benchmarks and incremental theoretical contributions. To strengthen the paper, the authors could expand the experimental evaluation to include more complex domains and address the minor presentation issues noted above.
Arguments for Acceptance:
- Clear and well-grounded methodology.
- Strong experimental results demonstrating robustness and effectiveness.
- Practical contributions that make the method suitable for real-world applications.
Arguments Against Acceptance:
- Limited evaluation on simple benchmarks.
- Incremental theoretical contributions relative to existing methods.
Overall, Advise represents a meaningful contribution to the field of interactive reinforcement learning and has the potential to inspire further research on leveraging human feedback effectively.