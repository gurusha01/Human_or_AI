This paper proposes an extension of Factorized Asymptotic Bayesian (FAB) inference to binary latent feature models (LFMs), addressing a key limitation of prior FAB approaches that required restrictive assumptions on the Hessian matrix of the complete log-likelihood. By deriving a Factorized Information Criterion (FIC) for LFMs, the authors demonstrate that the model complexity term has the same form as that for mixture models (MMs), enabling automatic feature selection and parameter identifiability. The proposed FAB/LFM algorithm is computationally efficient, resembling an EM algorithm with complexity penalties, and introduces a shrinkage acceleration mechanism to further reduce runtime. Empirical results show that FAB/LFMs outperform state-of-the-art Indian Buffet Process (IBP) implementations in terms of predictive likelihood and computational efficiency.
Strengths:
1. Novelty and Contribution: The paper extends FAB inference to LFMs, a significant step given the broader applicability of LFMs in real-world problems. The derivation of FIC for LFMs is theoretically interesting and demonstrates the generalizability of FAB approaches beyond MMs.
2. Computational Efficiency: The proposed shrinkage acceleration mechanism is a practical contribution, significantly reducing computational costs while maintaining performance. This is particularly valuable for large-scale datasets.
3. Empirical Validation: The experimental results convincingly demonstrate the advantages of FAB/LFMs over IBP and other methods, particularly in predictive likelihood and runtime. The approach achieves compact models with fewer latent features, mitigating overfitting.
4. Identifiability: The authors address a critical issue in LFMs by showing that FAB inference resolves parameter non-identifiability, a theoretical insight that strengthens the paper's contribution.
Weaknesses:
1. Incremental Nature: While the extension to LFMs is novel, the technical contributions are incremental, building directly on prior FAB work. The paper does not introduce fundamentally new concepts but rather adapts existing methods to a new class of models.
2. Theoretical Assumptions: The assumption of a fixed finite number of latent features (K) conflicts with the infinite nature of the IBP model, raising questions about the theoretical consistency of the approach.
3. Mathematical Clarity: Some parts of the mathematical exposition are imprecise, particularly equation (3) and Theorem 2. These issues may confuse readers and hinder reproducibility.
4. Experimental Interpretation: While the results are promising, the stopping criteria and potential sensitivity to local optima are not thoroughly analyzed. Additionally, the improvements in predictive likelihood may stem from implementation details rather than the proposed model selection criterion itself.
Recommendation:
The paper is a solid contribution to the field of Bayesian inference for latent feature models. It provides a computationally efficient alternative to IBP-based methods, with strong empirical results and theoretical insights. However, the incremental nature of the contributions, combined with some clarity and consistency issues, limits its impact. I recommend acceptance, provided the authors address the mathematical exposition issues and clarify the theoretical assumptions in the final version.
Arguments for Acceptance:
- Extends FAB inference to a broader class of models (LFMs).
- Demonstrates superior empirical performance and computational efficiency.
- Provides theoretical insights into parameter identifiability and model complexity.
Arguments Against Acceptance:
- Incremental technical contributions.
- Theoretical assumptions conflict with the infinite nature of IBP.
- Clarity issues in mathematical derivations.
Overall, the paper is a valuable addition to the conference, with practical and theoretical contributions that are likely to interest the community.