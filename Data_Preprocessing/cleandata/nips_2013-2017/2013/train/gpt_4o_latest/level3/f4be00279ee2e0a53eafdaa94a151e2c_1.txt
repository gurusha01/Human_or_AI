This paper presents a novel algorithm for approximate Bayesian inference in continuous-time Gaussian Markov process models with both discrete and continuous time observations. The authors leverage latent Gaussian models and extend the expectation propagation (EP) framework to the continuous-time domain, introducing a hybrid fixed-point iteration that combines EP updates for discrete-time terms with variational updates for continuous-time terms. The algorithm is further integrated into an EM framework to enable effective learning of model parameters and marginal distributions. This approach extends classical Kalman-Bucy smoothing to non-Gaussian observations, broadening the applicability of approximate message-passing techniques to a wider class of models.
Strengths
The paper makes a significant contribution by addressing the challenging problem of continuous-time inference with non-Gaussian observations, a topic that has seen limited exploration in the machine learning community. The connection between EP updates in the continuous-time limit and variational Gaussian updates is particularly intriguing and suggests a deeper theoretical relationship that could inspire future work. The proposed method demonstrates robustness and computational efficiency across diverse experimental settings, including spiking neuronal models and point process applications, showcasing its practical utility. The inclusion of post-inference correction methods further enhances the accuracy of marginal approximations, as evidenced by comparisons with MCMC benchmarks. The paper is well-written, with clear derivations and detailed experimental results that validate the algorithm's effectiveness.
Weaknesses
Despite its strengths, the paper has some limitations. The cubic complexity of the algorithm, due to matrix inversions and M-step updates, raises concerns about scalability to high-dimensional problems. A discussion on potential strategies to mitigate this complexity, such as low-rank approximations or sparse representations, would strengthen the paper. Additionally, the experimental section could benefit from convergence plots and visualizations of marginal likelihood estimates to better illustrate the advantages of the proposed method. Implementation challenges, such as slow parameter learning convergence and numerical stability, are acknowledged but not thoroughly addressed. Exploring recent work on Gaussian cubature for deterministic approximation (e.g., Simo Särkkä) could provide valuable contrasts or extensions. Finally, deriving an alternative algorithm using the variational Gaussian approach might offer benefits like a well-defined objective function and improved numerical stability, which are worth investigating.
Recommendation
Overall, this paper represents a valuable contribution to the field of approximate Bayesian inference, extending EP methods to continuous-time settings and demonstrating their applicability to real-world problems. While the scalability and implementation challenges limit its immediate applicability to large-scale problems, the theoretical insights and practical results make it a strong candidate for acceptance. I recommend acceptance, provided the authors address the scalability concerns and enhance the experimental section with additional visualizations and discussions.