The paper introduces a Determinized Sparse Partially Observable Tree (DESPOT) for online POMDP planning, along with its regularized variant, R-DESPOT, and an anytime approximation, AR-DESPOT. The authors aim to address the computational challenges of POMDPs by focusing on sampled scenarios and balancing policy size with value estimate accuracy. The proposed approach is evaluated against state-of-the-art online POMDP algorithms, demonstrating scalability and competitive performance in large state and observation spaces.
Strengths:
1. Technical Correctness: The paper is technically sound, with a solid theoretical foundation. The output-sensitive performance bound provided for DESPOT-derived policies is a noteworthy contribution.
2. Scalability: The experiments demonstrate that AR-DESPOT scales well to large state and observation spaces, outperforming existing methods like POMCP and AEMS2 in certain domains.
3. Practical Implementation: The availability of source code and experimental settings enhances reproducibility and practical utility for the community.
4. Clear Motivation: The paper clearly identifies the limitations of existing approaches (e.g., POMCP's poor worst-case behavior) and positions DESPOT as a solution.
Weaknesses:
1. Incremental Contribution: While the approach builds on existing ideas like Monte Carlo sampling and belief tree search, the novelty is limited. The work appears incremental rather than groundbreaking.
2. Unconvincing Experiments: The experimental results, though promising, are not comprehensive. For example, the claim that "R-DESPOT may still perform well with large observation spaces" is not rigorously analyzed. Additionally, the proposed regularization does not show benefits in the RockSample domain, raising questions about its general applicability.
3. Theoretical Gaps: The initial lower bound does not satisfy the requirements of Theorem 1, and its practical importance remains unclear. Moreover, the sensitivity of the tree search to the branching factor is neither theoretically nor experimentally addressed.
4. Clarity Issues: Table 1 is unclear, with unexplained results such as negative running times. The repeated claim about the algorithm working well with a "simple policy" is vague and lacks clarity on cases without such policies.
5. Missing References: The correct reference for AEMS is missing, which detracts from the paper's thoroughness in situating itself within the literature.
Pro vs. Con for Acceptance:
Pros:
- Technically sound and well-motivated.
- Demonstrates scalability to large POMDPs.
- Provides a practical implementation for reproducibility.
Cons:
- Limited originality and incremental contribution.
- Experimental results are not fully convincing or comprehensive.
- Some theoretical and clarity issues remain unresolved.
Recommendation:
While the paper provides a technically correct and scalable approach to online POMDP planning, its incremental nature and lack of rigorous experimental validation limit its potential impact. I recommend weak rejection unless the authors address the clarity issues, provide stronger experimental evidence, and better justify the theoretical and practical significance of their contributions.