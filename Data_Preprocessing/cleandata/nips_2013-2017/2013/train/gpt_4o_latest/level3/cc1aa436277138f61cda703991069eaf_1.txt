The paper addresses the critical problem of determining the optimal number of control questions required to evaluate workers in crowdsourcing tasks, balancing the trade-off between evaluating worker reliability and maximizing resources for target items. It introduces two methods: a two-stage estimator and a joint estimator, both grounded in theoretical analysis and validated through experiments. The two-stage estimator requires \(O(\sqrt{L})\) control questions per worker to minimize mean squared error (MSE), while the joint estimator ties MSE to the eigenvalues of the bipartite graph's adjacency matrix, requiring \(O(L/\sqrt{n})\) control items, which simplifies to \(O(\sqrt{L})\) as \(n \to \infty\). These findings provide practical rules of thumb for crowdsourcing practitioners.
The paper is technically sound, with well-supported claims through rigorous theoretical derivations and empirical validation. The authors clearly articulate the assumptions underlying their models, such as Gaussian noise and the use of random L-regular graphs to ensure the assignment graph is an expander. The experiments on both synthetic and real-world datasets (e.g., price estimation and NFL forecasting) validate the theoretical predictions, demonstrating the strengths and limitations of each method. Notably, the joint estimator is shown to be more efficient in terms of control question usage but less robust under model misspecification, highlighting the practical trade-offs between the two approaches.
The paper is well-written and organized, with clear explanations of the problem, methods, and results. However, some sections, particularly the derivations, are dense and may require familiarity with advanced statistical and graph-theoretic concepts. The authors could improve accessibility by providing more intuition behind the mathematical results. A minor typo in the conclusion ("O(L)" should be corrected to "O(\sqrt{L})") should also be addressed.
The work is original and builds upon prior research in crowdsourcing and consensus algorithms, such as Dawid and Skene (1979) and Karger et al. (2011). It extends these methods by providing a detailed analysis of control item usage and its impact on estimator performance, advancing the state of the art in crowdsourcing theory.
The paper's significance lies in its practical implications for crowdsourcing practitioners, offering actionable guidance on control question allocation. It also opens avenues for future research, such as exploring model selection using control items and addressing robustness under real-world complexities.
Pros for acceptance:
- Rigorous theoretical analysis and empirical validation.
- Practical relevance and actionable insights for crowdsourcing practitioners.
- Clear articulation of trade-offs between methods.
Cons for acceptance:
- Dense mathematical exposition may limit accessibility for non-experts.
- Sensitivity of the joint estimator to model misspecification warrants further exploration.
Overall, the paper is a strong contribution to the field of crowdsourcing and consensus modeling, and I recommend its acceptance.