The paper introduces MB-OPI, a novel policy iteration algorithm designed for symbolic Markov Decision Processes (MDPs) with factored states and actions. It builds on Modified Policy Iteration (MPI) and addresses key scalability challenges in symbolic dynamic programming (SDP) for large action spaces. The authors propose Opportunistic Policy Iteration (OPI), which balances the trade-off between policy evaluation and value iteration by opportunistically enforcing policy constraints only when they do not increase the size of the value function representation. This is achieved through a pruning procedure that conservatively combines policy and value diagrams, avoiding representational bloat. Additionally, the authors extend OPI to a memory-bounded version (MB-OPI), enabling a space-time tradeoff for resource-constrained scenarios.
The paper makes significant contributions both theoretically and empirically. Theoretically, it introduces pruning as an alternative to algebraic decision diagram (ADD) products and proves its compatibility with MPI guarantees. This is a novel and impactful contribution to the field, as it addresses a critical bottleneck in symbolic planning for factored-action MDPs. Empirically, the results demonstrate a 2-6x improvement in solution time over existing approaches, with OPI consistently outperforming FA-MPI and symbolic value iteration (VI) in terms of scalability and efficiency. The experiments span diverse domains, including inventory control, SysAdmin, and elevator control, showcasing the algorithm's versatility and robustness.
Strengths:  
1. Technical Novelty: The introduction of OPI and its pruning-based approach is a clear advancement over existing methods.  
2. Scalability: The empirical results convincingly show that OPI and MB-OPI scale better than state-of-the-art methods, particularly in large action spaces.  
3. Clarity and Organization: The paper is well-written and structured, making the technical content accessible to readers.  
4. Theoretical Rigor: The proofs of convergence and bounds on the policy backup are solid and well-justified.  
Weaknesses:  
1. Background Context: While the paper is generally clear, it could benefit from additional background on SDP with ADDs and the differences between policy iteration methods. This would make it more accessible to a broader audience.  
2. Clarity Issues: Some terms and notations in the figures and equations are inconsistent or insufficiently explained, which could hinder understanding.  
3. Practical Case Studies: The paper could include more discussion on real-world applications with many parallel actions to better contextualize its contributions.  
Pro and Con Arguments for Acceptance:  
- Pro: The paper addresses a critical problem in symbolic MDP planning, introduces a novel algorithm with strong theoretical guarantees, and demonstrates significant empirical improvements.  
- Con: Minor clarity and background issues could limit accessibility for non-expert readers.  
Recommendation: Accept. The paper represents a meaningful advancement in symbolic MDP planning, both theoretically and practically, and is likely to inspire further research in the field. Addressing the minor clarity and background issues in a revision would enhance its impact.