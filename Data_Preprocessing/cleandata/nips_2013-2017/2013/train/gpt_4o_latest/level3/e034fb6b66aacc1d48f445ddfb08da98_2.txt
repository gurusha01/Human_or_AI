The paper introduces a novel approach, Advise, which utilizes human feedback as policy advice rather than converting it into reward signals, addressing a key limitation in existing Interactive Reinforcement Learning (IRL) techniques. By framing feedback as a Bayesian estimate of action optimality, the authors propose a robust method for integrating human input into reinforcement learning. The experimental results demonstrate that Advise outperforms state-of-the-art methods, particularly in scenarios with infrequent or inconsistent feedback, making it a significant contribution to the field.
Strengths:
1. Novelty and Originality: The paper presents a fresh perspective on human feedback integration by treating it as policy advice rather than a reward signal. This approach avoids the pitfalls of reward shaping, such as reliance on ad hoc parameter tuning, and provides a principled Bayesian framework.
2. Technical Soundness: The derivation of the feedback policy and its integration with Bayesian Q-Learning is well-grounded in theory. The use of simulated human feedback allows for systematic exploration of feedback consistency and frequency, which is a thoughtful experimental design.
3. Performance: Advise consistently outperforms or matches existing methods like Action Biasing and Control Sharing, particularly under challenging conditions (e.g., low feedback frequency or high inconsistency). Its robustness to these real-world challenges is a notable strength.
4. Clarity of Contribution: The paper clearly articulates the limitations of prior work and positions Advise as a solution. The experiments are well-designed to highlight the advantages of the proposed method.
Weaknesses:
1. Strong Assumptions: The assumption that humans know only one optimal action per state is overly restrictive. Real-world tasks often involve multiple equally optimal actions, and the impact of relaxing this assumption is not explored.
2. Limited Exploration of Feedback Likelihood: While the authors vary feedback consistency and frequency, likelihoods below 1% are not tested. This omission leaves questions about the method's performance in extremely sparse feedback scenarios.
3. Systematic Inconsistencies: The paper does not adequately address how Advise handles systematic inconsistencies in human feedback, such as biases introduced during reward shaping or behavior training.
4. Clarity and Presentation: Minor issues, such as awkward phrasing, unclear game objectives (e.g., Pac-Man), inconsistent table/figure labeling, and small figure text, detract from the paper's readability. Additionally, the relationship between control sharing and action biasing could be better clarified.
5. Practicality: While the use of a simulated oracle is useful for controlled experiments, the paper does not discuss how well Advise generalizes to real human feedback, which often involves delays and more complex inconsistencies.
Arguments for Acceptance:
- The paper addresses a critical limitation in IRL by proposing a novel and theoretically sound approach.
- Advise demonstrates robust performance across a range of feedback conditions, advancing the state of the art.
- The Bayesian framework provides a principled way to model feedback consistency, which could inspire further research.
Arguments Against Acceptance:
- The strong assumption about single optimal actions limits the method's applicability to more complex, real-world tasks.
- The lack of exploration of extreme feedback sparsity and systematic inconsistencies leaves gaps in the evaluation.
- Presentation issues, while minor, could hinder comprehension for readers unfamiliar with the domain.
Recommendation:
Overall, the paper makes a meaningful contribution to the field of IRL by introducing a novel framework for leveraging human feedback. While there are some limitations in assumptions and evaluation, the strengths of the proposed method outweigh these concerns. I recommend acceptance with minor revisions to address clarity issues and provide additional discussion on the assumptions and their implications.