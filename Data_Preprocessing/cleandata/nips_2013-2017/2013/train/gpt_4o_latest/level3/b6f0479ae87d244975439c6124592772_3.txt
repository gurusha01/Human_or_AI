The paper presents an active learning framework tailored for structured prediction problems, leveraging entropy-based measures to selectively query labels. Building on Schwing et al.'s work on efficient inference for structured learning with missing labels, the authors propose two algorithmic variants. The first, "Separate Active," queries nodes with the highest entropy after inference, while the second, "Joint Active," integrates unlabeled examples during learning without requiring additional inference steps. The proposed methods are validated on the task of 3D room layout prediction from single images, demonstrating competitive performance with significantly reduced labeling effort (~10% of the random variables).
Strengths:  
The paper addresses an important challenge in structured prediction—reducing labeling costs—by proposing computationally efficient active learning strategies. The use of entropy as a surrogate for uncertainty is intuitive and aligns well with the structured prediction context. The experimental results are promising, showing that the proposed methods outperform random baselines and achieve state-of-the-art performance with minimal labeling. The authors also address computational considerations effectively, such as warm-starting the learning process and reusing computations, which enhances the practicality of the approach. The paper is well-organized, with clear explanations of the algorithms and thorough experimental evaluation. The inclusion of batch-mode active learning and analysis of the epsilon factor adds depth to the study.
Weaknesses:  
While the results are promising, the experimental comparisons are limited to a weak random baseline, which undermines the strength of the claims. The paper does not explore alternative active learning strategies, such as expected error reduction or variance-based approaches, which could provide a more comprehensive evaluation. Additionally, the impact of entropy approximation errors introduced by convex belief propagation is not thoroughly discussed, leaving a gap in understanding the robustness of the method. The novelty of the work is somewhat incremental, as it builds on existing frameworks and primarily focuses on efficiency improvements rather than introducing fundamentally new concepts. Finally, the experiments are restricted to a single application (3D room layout prediction), which limits the generalizability of the findings.
Pro and Con Acceptance Arguments:  
Pros:  
- Clear and well-executed methodology with strong experimental results.  
- Addresses a practical problem in structured prediction with computational efficiency.  
- Demonstrates significant reduction in labeling effort while maintaining performance.  
Cons:  
- Limited novelty and lack of comparison to stronger baselines or alternative strategies.  
- Insufficient discussion on the implications of entropy approximation errors.  
- Experiments are confined to a single domain, restricting generalizability.  
Recommendation:  
Overall, the paper is a solid contribution to the field of active learning for structured prediction, with practical implications for reducing labeling costs. However, the limited scope of experimental comparisons and incremental novelty temper its impact. I recommend acceptance with minor revisions, encouraging the authors to expand their comparative analysis and discuss the limitations of their entropy approximations in greater detail.