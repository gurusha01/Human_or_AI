This paper introduces a novel approach to learning chordal Markov networks by reformulating the problem as a constraint satisfaction task. The authors leverage Boolean constraint-solving techniques, including SAT, MAXSAT, SMT, and ASP, to compute optimal network structures. A key contribution is the development of a balancing condition for separators in clique graphs, which enables efficient encoding of the learning problem. The authors demonstrate the method's ability to solve previously unresolved problems, such as proving the optimality of a network for a dataset where stochastic search methods had been inconclusive.
The paper is well-written and technically sound, with clear explanations of the proposed method and its theoretical underpinnings. The authors provide detailed descriptions of the encoding process and experimental results using real-world datasets. The results highlight the method's ability to compute globally optimal structures, advancing the state of the art in exact structure learning for Markov networks.
However, the paper has several limitations. The primary concern is scalability. The method's reliance on exponential growth in propositional variables makes it impractical for larger datasets, as evidenced by the 14-hour runtime and additional proof-of-optimality time for the "econ" dataset. While the authors acknowledge this limitation, further discussion on potential strategies to improve scalability, such as hybrid approaches combining constraint-solving with heuristic methods, would strengthen the paper.
Another area for improvement is the lack of comparison to related work on learning Bayesian networks, particularly the literature on learning directed acyclic graphs (DAGs), such as the work by Koivisto and Sood. Including such comparisons would provide a more comprehensive evaluation of the method's relative strengths and weaknesses. Additionally, while the authors briefly mention stochastic search methods, a deeper analysis of how their approach compares in terms of computational efficiency and solution quality would be valuable.
Strengths:
- Reformulates Markov network learning as a constraint satisfaction problem, a novel perspective.
- Demonstrates the ability to compute globally optimal structures for previously unresolved problems.
- Provides detailed theoretical and experimental analysis.
Weaknesses:
- Poor scalability to larger datasets due to exponential growth in variables.
- Limited comparison to related work, particularly on Bayesian network learning and DAGs.
- Lack of discussion on potential scalability improvements.
Recommendation:
This paper makes a significant contribution to exact structure learning for Markov networks and opens new avenues for applying constraint-solving techniques in this domain. However, its scalability issues and limited comparative analysis temper its impact. I recommend acceptance with minor revisions to address the aforementioned concerns, particularly the inclusion of comparisons to Bayesian network learning methods and a discussion on scalability improvements.