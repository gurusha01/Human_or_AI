The paper introduces decision Directed Acyclic Graphs (DAGs), termed "decision jungles," as a compact and powerful alternative to traditional decision trees for classification tasks. By allowing multiple paths to each node, decision DAGs address the exponential growth in memory usage that plagues decision trees as depth increases. The authors propose two layer-wise algorithms, LSearch and ClusterSearch, to jointly optimize the structure and parameters of the DAGs. Experimental results demonstrate that ensembles of decision DAGs outperform fixed-size decision tree ensembles in terms of generalization, particularly under memory constraints, making the approach highly relevant for resource-limited environments like mobile or embedded systems.
Strengths:
1. Novelty and Contribution: While the underlying concepts of decision DAGs and node merging are not entirely new, the paper's main contribution lies in integrating these ideas into a cohesive model with intuitive learning algorithms. The proposed methods are well-motivated and address a critical limitation of decision trees—memory inefficiency—while improving generalization.
2. Experimental Rigor: The paper provides extensive experimental results across diverse datasets, including semantic segmentation tasks and UCI datasets. The results consistently show that decision jungles achieve better memory efficiency and generalization compared to standard decision forests and their variants.
3. Impact and Practical Relevance: The proposed method is particularly significant for practitioners working with memory-constrained hardware. The ability to achieve compact models without sacrificing accuracy is a valuable contribution to the field.
4. Clarity in Methodology: The paper clearly explains the optimization process for decision jungles, particularly the LSearch and ClusterSearch algorithms. The comparison between these methods is thorough, with LSearch emerging as the more effective approach.
Weaknesses:
1. Training and Evaluation Time: While the paper highlights the memory efficiency and generalization benefits of decision jungles, it lacks a detailed comparison of training and evaluation times between decision DAGs and traditional decision trees. This omission is critical for practitioners who may prioritize computational efficiency.
2. Clarity Issues: There are minor ambiguities in the paper, such as the use of the term "energy" in the context of optimization and the description of bagging. Additionally, a grammatical error in the LSearch description detracts slightly from the paper's overall polish.
3. Limited Exploration of Alternatives: While the paper compares decision jungles to decision forests and a few tree-based baselines, it does not explore comparisons with other compact classification models, such as gradient-boosted trees or neural networks designed for memory efficiency.
Recommendation:
I recommend acceptance of this paper, as its contributions are both novel and impactful. The proposed decision jungles offer a compelling solution to a well-recognized problem in decision tree-based models, and the experimental results strongly support the authors' claims. However, the authors should address the missing analysis of training and evaluation times and clarify the minor ambiguities in the final version. These improvements would enhance the paper's accessibility and practical utility.