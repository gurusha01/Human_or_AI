This paper presents a novel approach to training two-layer networks for classification with latent variables using a convex semidefinite programming (SDP) relaxation of the inherently non-convex problem. The authors propose an approximate optimization algorithm to solve the SDP formulation and demonstrate its effectiveness through experiments. The results show that the proposed method outperforms globally optimized single-layer models and locally optimized two-layer models, making a significant contribution to the field of latent variable modeling and deep learning.
Strengths:
1. Novelty and Technical Soundness: The use of SDP relaxation to train two-layer networks with latent variables is innovative and extends existing convex modeling approaches. The theoretical foundation is solid, and the authors provide detailed derivations to support their claims.
2. Experimental Results: The experimental evaluation is thorough, comparing the proposed method (CVX2) against multiple baselines, including single-layer models and locally optimized two-layer models. The results on synthetic and real-world datasets convincingly demonstrate the advantages of the proposed approach.
3. Significance: The work addresses a challenging problem in training latent variable models, offering a global training method that could inspire further research into more efficient and scalable approaches.
4. Potential Impact: The ability to train two-layer models globally using convex relaxation is a meaningful step forward, particularly in scenarios where local optimization methods struggle.
Weaknesses:
1. Complexity of the SDP Problem: The primary limitation of the proposed method is the computational complexity of solving the SDP problem. While the authors propose an approximate algorithm, they do not provide sufficient details on computational time, scalability, or stopping conditions. This raises concerns about the practicality of the approach for large-scale problems.
2. Clarity Issues: Parts of the text, particularly the technical derivations, are dense and challenging to follow. This may hinder accessibility for readers unfamiliar with SDP or convex relaxation techniques.
3. Experimental Details: The experiments lack clarity in some aspects:
   - The sizes of problem instances are not reported, making it difficult to assess scalability.
   - Variables are missing in equations (17) and (18), and the transductive evaluation method described in line 356 is unclear.
4. Intractability in Practice: While the convex relaxation is theoretically appealing, its practical intractability for large datasets is a significant drawback. A more detailed discussion of this limitation and potential mitigations would strengthen the paper.
Arguments for Acceptance:
- The paper introduces a novel and technically sound approach to a challenging problem.
- Experimental results demonstrate clear advantages over existing methods.
- The work has the potential to inspire further research into scalable convex relaxation techniques.
Arguments Against Acceptance:
- The computational complexity of the proposed method limits its practical applicability.
- Clarity issues and missing details in the experimental setup reduce the paper's accessibility and reproducibility.
- The scalability of the approach is not adequately addressed.
Recommendation:
Overall, this paper makes a valuable contribution to the field of latent variable modeling and deep learning. While there are concerns about the practicality and clarity of the approach, the novelty and significance of the work warrant acceptance. However, the authors should address the noted weaknesses, particularly the computational complexity and clarity issues, in a revised version.