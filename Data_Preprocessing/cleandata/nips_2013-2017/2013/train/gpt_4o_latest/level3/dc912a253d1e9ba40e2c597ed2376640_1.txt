The paper presents a novel distributed stochastic dual coordinate ascent (DisDCA) algorithm for solving regularized risk minimization problems in a distributed framework. The authors leverage the strengths of dual coordinate ascent (DCA) methods, which often outperform stochastic gradient descent (SGD) methods in certain regimes, and extend them to a distributed setting. The algorithm employs parallel computation across multiple machines or cores, with each machine updating a subset of dual variables followed by a "reduce" step to synchronize updates. The paper provides theoretical guarantees for convergence (Theorem 1) and analyzes the tradeoff between computation and communication. Experimental results demonstrate the algorithm's superior performance on linear SVM problems compared to SGD-based and ADMM-based distributed algorithms.
Strengths:
1. Novelty and Contribution: The paper addresses a significant gap in the literature by extending DCA methods to a distributed framework. The approach is inspired by ideas from Yahoo-LDA and Hogwild but is novel in its application to dual coordinate ascent.
2. Theoretical Rigor: The authors provide strong theoretical guarantees for convergence, including an analysis of the tradeoff between computation and communication. This is a valuable addition, as it offers practical insights into parameter tuning (e.g., number of machines and samples per iteration).
3. Practical Relevance: The practical variant of DisDCA demonstrates substantial improvements over the basic variant and other competitive algorithms. The parameter-free nature of DisDCA is a notable advantage over ADMM, which requires careful tuning of penalty parameters.
4. Empirical Validation: The experimental results are robust, showing clear advantages of DisDCA over SGD-based and ADMM-based distributed algorithms on large-scale SVM problems. The results also validate the theoretical analysis of computation-communication tradeoffs.
Weaknesses:
1. Clarity of Presentation: While the paper is technically sound, some sections, particularly the theoretical analysis, could benefit from clearer explanations and better organization. For example, the derivation of Theorem 1 is dense and might be challenging for readers unfamiliar with the topic.
2. Figure Quality: Figure 3 is too small and difficult to interpret. Using a log-scale for the primal objective and duality gap, as suggested, would enhance readability and better illustrate the results.
3. Limited Scope of Comparisons: Although DisDCA is compared against several baselines, the paper could include more diverse datasets or problem types to further validate its generalizability.
Arguments for Acceptance:
- The paper makes a significant contribution to distributed optimization by extending DCA methods to a distributed setting, which is both novel and impactful.
- Theoretical guarantees and empirical results are strong, demonstrating the algorithm's effectiveness and practical utility.
- The practical variant of DisDCA offers meaningful improvements, making the approach highly relevant for real-world applications.
Arguments Against Acceptance:
- The clarity of the theoretical sections and figures could be improved, which might hinder accessibility for a broader audience.
- The experimental evaluation, while strong, could be expanded to include more diverse datasets or scenarios.
Recommendation: Strong accept. The paper is a well-rounded contribution to the field of distributed optimization, with both theoretical and practical significance. Addressing the clarity issues and improving figure quality in the final version would further enhance its impact.