The paper introduces a novel method to improve Gibbs sampling for inference in Ising models by projecting parameters onto a fast-mixing set, satisfying the Dobrushin criterion. This is achieved through a constrained optimization framework, which ensures the new parameters remain within the same parameter space as the original ones. The authors explore various divergences, including Euclidean distance, piecewise KL-divergence, and reversed KL-divergence, to define the projection. The proposed approach is particularly relevant for high-treewidth graphical models, where exact inference is intractable, and Gibbs sampling can suffer from slow convergence due to strong interactions.
Strengths:
1. Novelty: The combination of stochastic and deterministic inference principles to enforce fast mixing is an under-explored area, making this work a meaningful contribution. The use of spectral norm constraints to guarantee rapid mixing is a creative approach that extends the applicability of Gibbs sampling.
2. Theoretical Rigor: The paper provides a solid theoretical foundation, including a dual formulation for the Euclidean projection and a detailed discussion of divergence measures. The iterative thresholding of singular value decomposition for parameter projection is well-motivated and mathematically sound.
3. Experimental Comparisons: The authors compare their method against standard variational inference techniques (e.g., loopy belief propagation, mean-field) and Gibbs sampling on original parameters. The results demonstrate that the proposed method achieves better accuracy under limited computational budgets.
Weaknesses:
1. Clarity Issues: The dual formulation of the projection problem, particularly the constraints (e.g., \(z{ij}d{ij} = 0\)), is not clearly explained. This raises questions about the interpretability and reproducibility of the parameter space constraints.
2. Algorithm Description: The procedure for interleaving projected gradient updates with Gibbs sampling is ambiguous. Including a pseudocode box or algorithm description would significantly improve clarity.
3. Convergence Guarantee: The paper does not provide a formal guarantee for the convergence of the projected gradient descent strategy. This omission weakens the theoretical completeness of the method.
4. Experimental Concerns: The choice of 30K iterations for the Gibbs sampler seems arbitrary and lacks justification. Additionally, comparisons based on computational time rather than iterations would provide a more practical evaluation.
5. Error Bars: The absence of error bars in experimental results makes it difficult to assess the statistical significance of the reported improvements.
6. Projection Overhead: The computational cost of performing the projection is not accounted for in the comparisons, which could impact the method's practicality for larger models.
7. Missing Baselines: The paper does not compare its approach to blocked Gibbs samplers or Gogate's "Lifted Gibbs Sampler," which are relevant baselines for inference in high-treewidth models.
Overall Assessment:
The paper presents a promising idea with a solid theoretical foundation, but its practical and experimental aspects require further refinement. The method is novel and addresses an important problem in probabilistic inference, but the lack of clarity in some sections, missing baselines, and incomplete experimental validation limit its impact. 
Arguments for Acceptance:
- Novel approach combining stochastic and deterministic inference principles.
- Theoretical contributions to fast-mixing parameter projection.
- Promising experimental results showing improved accuracy over variational methods.
Arguments Against Acceptance:
- Lack of clarity in key formulations and algorithmic details.
- Missing convergence guarantees for the proposed optimization strategy.
- Incomplete experimental evaluation, including missing baselines and error bars.
Recommendation: Weak Accept. The paper has significant potential but requires revisions to address clarity, experimental rigor, and missing comparisons.