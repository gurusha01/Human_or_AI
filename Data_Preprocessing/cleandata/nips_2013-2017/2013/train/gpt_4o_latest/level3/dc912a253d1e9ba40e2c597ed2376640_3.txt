The paper introduces DisDCA, a distributed implementation of stochastic dual coordinate ascent (SDCA), specifically designed for solving regularized loss minimization problems in a distributed framework. The authors address a significant gap in distributed optimization by extending SDCA, which is known for its strong theoretical guarantees and superior convergence properties, to a distributed setting. The proposed algorithm requires no parameter tuning, unlike ADMM-based methods, and achieves comparable performance. The paper also provides a detailed analysis of the tradeoff between computation and communication, which is both theoretically rigorous and empirically validated.
Strengths:
1. Novelty and Relevance: The paper makes a meaningful contribution by introducing a distributed variant of SDCA, filling a notable gap in distributed machine learning. The focus on computation-communication tradeoffs is particularly relevant to the NIPS community, given the increasing importance of scalable algorithms for large-scale data.
2. Technical Soundness: The algorithm is well-motivated, and its theoretical guarantees are clearly articulated. The convergence analysis is thorough, with insightful discussions about the effective regions of computation and communication parameters.
3. Empirical Validation: The experiments are comprehensive, comparing DisDCA to SGD-based and ADMM-based distributed algorithms. The results demonstrate that DisDCA is competitive, with the practical variant outperforming the basic variant and other baselines in many scenarios.
4. Clarity: The paper is well-written and logically organized. The authors provide sufficient background on related work and clearly differentiate their contributions.
5. Significance: By eliminating the need for parameter tuning and achieving competitive performance, DisDCA positions itself as a strong alternative to ADMM for distributed optimization.
Weaknesses:
1. Figures and Supplementary Material: The figures in the main paper and supplementary material are difficult to read and poorly organized. This detracts from the clarity of the empirical results. The authors should improve the resolution and layout of the figures, ensuring that key insights are easily interpretable.
2. Limited Discussion on Practical Variant: While the practical variant of DisDCA shows significant improvements, its convergence properties remain unexplored. A theoretical analysis of this variant would strengthen the paper.
3. Scope of Experiments: The experiments focus primarily on SVM formulations. While this is a reasonable starting point, additional experiments on other machine learning tasks or datasets would demonstrate the broader applicability of DisDCA.
Recommendation:
I recommend acceptance of this paper. The work is technically sound, addresses a critical problem in distributed learning, and provides both theoretical and empirical contributions. However, the authors should address the issues with figure readability and consider expanding the discussion of the practical variant in the final version.
Arguments for Acceptance:
- Novel and relevant contribution to distributed optimization.
- Strong theoretical foundation and empirical validation.
- Insightful analysis of computation-communication tradeoffs.
- Competitive performance compared to well-tuned baselines.
Arguments Against Acceptance:
- Poorly presented figures and supplementary material.
- Lack of theoretical guarantees for the practical variant.
- Limited diversity in experimental tasks.
Overall, the strengths of the paper far outweigh its weaknesses, and it is a valuable addition to the NIPS community.