The paper introduces "decision jungles," a modification of random forests that replaces decision trees with directed acyclic graphs (DAGs) to address memory efficiency challenges in memory-constrained environments. The authors propose two algorithms, LSearch and ClusterSearch, to optimize the structure and features of the DAGs during training. The paper claims that decision jungles reduce memory usage while improving generalization compared to traditional decision forests. Experimental results are presented across multiple datasets, including semantic image segmentation tasks and UCI datasets, to evaluate the proposed method.
Strengths
1. Problem Identification: The paper addresses a relevant issue in decision tree-based models—the exponential growth of memory usage with tree depth—particularly for applications on embedded or mobile devices.
2. Conceptual Clarity: The idea of replacing trees with DAGs is well-motivated, and the authors provide a clear explanation of how DAGs can reduce redundancy and improve generalization.
3. Comparative Baselines: The paper compares decision jungles against several baselines, including standard decision forests and width-limited tree variants, providing a broad experimental context.
4. Experimental Scope: The experiments span diverse datasets and tasks, including image segmentation and UCI datasets, which highlights the general applicability of the proposed approach.
Weaknesses
1. Limited Novelty: While the use of DAGs in decision models is not new, the paper does not sufficiently differentiate its contributions from prior work. The proposed methods (LSearch and ClusterSearch) appear incremental and lack significant innovation compared to existing approaches.
2. Insufficient Evidence for Memory Efficiency: The memory efficiency claims are primarily based on node count rather than overall model size or runtime memory usage. While node count is a proxy, it does not convincingly demonstrate practical benefits, especially for real-world applications.
3. Ad-hoc Optimization: The optimization approach in Section 3 is ad-hoc and lacks theoretical rigor. The algorithms are described in a way that makes them difficult to reproduce or verify, particularly for readers unfamiliar with the domain.
4. Experimental Limitations: The experimental results fail to convincingly establish the superiority of decision jungles. While there are gains in memory efficiency and generalization in some cases, these improvements are modest and dataset-dependent. The UCI results, for instance, show limited generalization benefits.
5. Clarity Issues: The paper is dense and could benefit from better organization and clearer descriptions of the algorithms. For example, the explanation of the optimization process is overly technical and lacks intuitive insights.
Recommendation
While the paper addresses an important problem and provides a novel perspective by leveraging DAGs, the contributions are incremental, and the results do not convincingly demonstrate significant practical benefits. The optimization methods are ad-hoc, and the memory efficiency claims are inadequately supported. Additionally, the work lacks sufficient novelty to meet the standards of a top-tier conference like NeurIPS. I recommend rejection in its current form.
Arguments for Acceptance
- Addresses a relevant problem in memory-constrained machine learning.
- Provides a novel perspective by using DAGs in decision forests.
- Includes a broad experimental evaluation.
Arguments for Rejection
- Limited novelty compared to prior work on DAGs and decision forests.
- Insufficient evidence to support memory efficiency claims.
- Ad-hoc optimization methods that lack clarity and reproducibility.
- Modest and dataset-dependent improvements in experimental results.
- Does not meet the standard of significant impact required for NeurIPS.
Overall Rating: 4/10 (Weak Reject)