This paper presents a novel unsupervised method for feature selection in text data, leveraging the principle of Minimum Description Length (MDL) and a dictionary-based compression scheme. The authors formulate document compression as a binary optimization problem, solved approximately using iterative reweighting and ADMM. The resulting features, derived from k-grams, are shown to be compact, interpretable, and effective across a range of tasks, including text categorization and exploratory analysis. The authors demonstrate that their method reduces the feature space by two orders of magnitude while maintaining state-of-the-art classification performance, offering significant computational advantages.
Strengths:
1. Novelty and Innovation: The paper introduces a unique approach to feature selection by framing it as a compression problem, which is both theoretically grounded and practically impactful. The method addresses limitations of existing compression-based approaches, such as sensitivity to document order (e.g., LZ77), and provides a robust, order-invariant solution.
2. Technical Soundness: The optimization framework is well-justified, and the iterative reweighting scheme is inspired by established methods (e.g., Candes et al.). The use of ADMM for efficient, parallelizable optimization is a strength, enabling scalability to large datasets.
3. Practical Utility: The method achieves significant dimensionality reduction without sacrificing accuracy, which is highly valuable for tasks with limited computational resources or training data. The reduction in training time (e.g., from 8–16 hours to 1 hour for 20 Newsgroups) is a compelling result.
4. Comprehensive Evaluation: The experiments are thorough, covering both binary and multiclass classification, PCA-based exploratory analysis, and sensitivity to training set size. Comparisons to baseline methods (e.g., full 5-grams, LZ77) and state-of-the-art results on benchmark datasets (20 Newsgroups, IMDb) demonstrate the method's effectiveness.
5. Clarity: The paper is well-written, with clear explanations of the problem formulation, optimization algorithm, and experimental setup. The inclusion of visualizations (e.g., PCA plots) aids in understanding the results.
Weaknesses:
1. Limited Scope of Applications: While the method is theoretically applicable to any sequential data, the experiments focus solely on text. Extending the evaluation to other domains (e.g., genomic sequences or clickstream data) would strengthen the claim of general applicability.
2. Sensitivity to Hyperparameters: The paper briefly mentions the impact of the pointer cost parameter (λ) but does not provide a systematic analysis of its sensitivity or guidance on how to select it in practice.
3. Reproducibility: While the optimization algorithm is described in detail, the paper lacks sufficient information on implementation specifics (e.g., initialization, stopping criteria) and does not provide code or pseudocode, which may hinder reproducibility.
4. Acknowledgment of Limitations: The authors do not explicitly discuss potential limitations of their approach, such as the trade-off between compression quality and feature interpretability or the impact of lossy approximations in future work.
Recommendation:
Overall, this paper makes a significant contribution to unsupervised feature selection for text data, combining theoretical rigor with practical utility. The method is novel, technically sound, and demonstrates strong empirical performance. While there are minor concerns regarding generalizability and reproducibility, these do not detract substantially from the paper's merits. I recommend acceptance, with the suggestion to include a more detailed discussion of limitations and provide implementation details to enhance reproducibility.
Pro Arguments:
- Novel and robust approach to feature selection.
- Significant dimensionality reduction with no loss in accuracy.
- Thorough experimental validation and state-of-the-art results.
Con Arguments:
- Limited evaluation beyond text data.
- Insufficient discussion of parameter sensitivity and implementation details.
Rating: 8/10