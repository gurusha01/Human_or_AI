The paper presents GREEDI, a distributed protocol for submodular function maximization under cardinality constraints, designed to address the challenges of large-scale machine learning problems. The authors focus on scenarios where centralized solutions are impractical due to the size of the data, and they propose a two-stage MapReduce-compatible approach. The main contributions include theoretical guarantees for GREEDI's performance, experimental validation on tasks like exemplar-based clustering and active set selection, and scalability demonstrations on datasets with tens of millions of points using Hadoop.
Strengths:
1. Relevance and Novelty: The paper addresses a critical challenge in large-scale machine learningâ€”efficiently solving submodular maximization problems in distributed settings. While prior work has explored centralized or streaming approaches, GREEDI introduces a novel distributed framework with theoretical guarantees, filling a significant gap in the literature.
2. Theoretical Analysis: The authors provide rigorous theoretical bounds for GREEDI's performance, demonstrating that it achieves solutions competitive with centralized methods under certain conditions. The analysis of Lipschitz functions and decomposable objectives is particularly insightful and adds depth to the work.
3. Scalability: The experimental results convincingly show that GREEDI scales to massive datasets, such as 80 million Tiny Images and 45 million user visits, with practical runtimes on Hadoop clusters. This demonstrates the method's applicability to real-world problems.
4. Experimental Validation: The paper includes extensive experiments comparing GREEDI to several baselines. The results consistently show that GREEDI outperforms naive distributed approaches and achieves near-centralized performance, even with decomposable objective functions.
5. Clarity of Problem Definition: The paper clearly defines the problem of distributed submodular maximization and situates it within the broader context of machine learning applications, such as Gaussian process inference and clustering.
Weaknesses:
1. Limited Practical Comparisons: While the paper compares GREEDI to naive baselines, it does not benchmark against other state-of-the-art distributed submodular maximization methods (e.g., [18], [19], [20]). This omission makes it harder to contextualize GREEDI's performance relative to the broader field.
2. Assumptions on Data Structure: The theoretical guarantees rely on assumptions about data geometry (e.g., Lipschitz continuity, density of neighborhoods), which may not hold in all practical scenarios. A discussion of how GREEDI performs when these assumptions are violated would strengthen the paper.
3. Clarity of Presentation: While the technical content is strong, the paper is dense and could benefit from clearer explanations of key concepts, particularly for readers less familiar with submodular optimization. For example, the distinction between GREEDI and naive approaches could be more explicitly highlighted in the main text.
4. Reproducibility: Although the experiments are impressive, the paper does not provide sufficient implementation details (e.g., parameter settings, hardware configurations) to ensure reproducibility. Including a public code repository would enhance the paper's impact.
Recommendation:
I recommend acceptance of this paper, as it makes a significant contribution to the field of distributed optimization and demonstrates strong theoretical and experimental results. However, the authors should address the weaknesses by benchmarking against more advanced baselines, clarifying the presentation, and providing additional implementation details. These improvements would make the paper even more impactful.
Pro and Con Arguments:
Pros:
- Novel and practical solution to distributed submodular maximization.
- Strong theoretical guarantees and rigorous analysis.
- Demonstrated scalability on massive datasets.
- Broad applicability to real-world machine learning problems.
Cons:
- Limited comparisons to state-of-the-art methods.
- Reliance on assumptions that may not generalize.
- Dense presentation that could be more accessible.
Overall, the paper is a valuable contribution to the NeurIPS community and advances the state of the art in distributed machine learning.