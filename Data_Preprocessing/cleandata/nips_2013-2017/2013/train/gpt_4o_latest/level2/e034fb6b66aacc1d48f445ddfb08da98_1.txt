The paper presents a novel approach, Advise, for integrating human feedback into Reinforcement Learning (RL) through a Policy Shaping paradigm. Unlike traditional methods that map human feedback to rewards or values, Advise treats feedback as direct policy labels, leveraging a Bayesian framework to maximize the utility of human input. The authors demonstrate that Advise outperforms state-of-the-art methods, such as Action Biasing, Control Sharing, and Reward Shaping, particularly in scenarios with infrequent or inconsistent human feedback. The paper also highlights Advise's robustness and its ability to avoid ad hoc parameter tuning, a common limitation in prior approaches.
Strengths:
1. Novelty and Originality: The paper introduces a significant shift in how human feedback is utilized in RL by formalizing it as policy advice rather than reward shaping. This is a meaningful contribution to the field of Interactive Reinforcement Learning.
2. Technical Soundness: The Bayesian formulation for estimating the feedback policy and combining it with the RL policy is well-grounded. The derivations and experimental results are consistent with the claims made.
3. Robustness: Advise demonstrates resilience to noisy and sparse feedback, a critical challenge in real-world interactive systems. This robustness is convincingly validated through experiments in Pac-Man and Frogger domains.
4. Comprehensive Evaluation: The authors compare Advise against multiple state-of-the-art methods under varying feedback conditions (frequency and consistency). The results are presented clearly, with quantitative metrics and learning curves that support the claims.
5. Practical Implications: The reduced reliance on domain-specific parameter tuning makes Advise more applicable to real-world scenarios, where human feedback is often inconsistent and sparse.
Weaknesses:
1. Limited Real-World Validation: While the use of simulated oracles allows for systematic testing, the absence of experiments with actual human participants limits the paper's practical impact. Real-world feedback often involves additional complexities, such as credit assignment errors, which are not fully addressed.
2. Static Feedback Consistency: The assumption of a static feedback consistency parameter (C) may not hold in dynamic human-agent interactions. While the authors acknowledge this and propose future work, it remains a limitation of the current approach.
3. Domain-Specific Evaluation: The experiments are restricted to two relatively simple game domains. Testing Advise on more complex, high-dimensional tasks would strengthen its generalizability.
Suggestions for Improvement:
1. Incorporate experiments with real human participants to validate the approach in practical settings.
2. Explore methods for dynamically estimating feedback consistency (C) during learning, as suggested in the discussion.
3. Extend the evaluation to more complex domains, such as robotics or real-world decision-making tasks, to demonstrate scalability.
Recommendation:
I recommend acceptance of this paper, as it introduces a novel and robust approach to integrating human feedback into RL, addresses key limitations of existing methods, and provides a solid foundation for future work. While there are areas for improvement, the contributions are significant and relevant to the NIPS community.