The paper presents a detailed analysis of regret trade-offs in online learning, specifically focusing on the absolute loss game with two experts. The authors address the problem of achieving skewed regret guarantees, where the learner prioritizes minimizing regret against certain experts over others. The main contributions include a characterization of the Pareto-optimal regret trade-offs, the construction of optimal strategies for finite horizons, and an asymptotic analysis of regret trade-offs as the time horizon grows. The work also extends its findings to general linear losses and scenarios with more than two experts.
Strengths:
1. Novelty and Originality: The paper tackles a relatively unexplored aspect of online learning by focusing on asymmetric regret guarantees. The characterization of Pareto-optimal trade-offs and the identification of sub-optimality in prior-based approaches (e.g., square-root log-prior bounds) are significant contributions that advance the state of the art.
2. Technical Depth: The authors provide rigorous theoretical analysis, including exact characterizations for finite horizons and asymptotic results. The connection between regret trade-offs and random walks is particularly insightful and provides a novel perspective on strategy design.
3. Clarity of Results: The paper is well-organized, with clear definitions, theorems, and proofs. The visual representation of Pareto frontiers for finite and asymptotic settings (e.g., Figure 1 and Figure 2) aids in understanding the results.
4. Broader Applicability: The extension of results to general linear losses and multi-expert settings demonstrates the versatility of the proposed framework. The discussion of modern regret budgets (e.g., √LkT, √VarmaxT) suggests promising directions for future work.
Weaknesses:
1. Limited Practical Evaluation: While the theoretical contributions are strong, the paper lacks empirical validation. Demonstrating the practical utility of the proposed strategies in real-world online learning scenarios would strengthen the paper's impact.
2. Complexity of Algorithms: The proposed strategies, especially for finite horizons, involve intricate calculations (e.g., random walk-based probability assignments). This may limit their practical adoption without further simplifications or approximations.
3. Scalability to K > 2 Experts: Although the paper briefly discusses extensions to more than two experts, the analysis remains preliminary. The counter-intuitive result regarding unbalanced binary trees is intriguing but requires more detailed exploration and validation.
Suggestions for Improvement:
1. Include experimental results to validate the theoretical findings and demonstrate the practical performance of the proposed algorithms.
2. Simplify the implementation of the optimal strategies, particularly for finite horizons, to make them more accessible to practitioners.
3. Expand the analysis for K > 2 experts, providing concrete algorithms and regret guarantees for this more general setting.
Recommendation:
The paper makes significant theoretical contributions to the field of online learning and regret analysis. While it could benefit from empirical validation and further exploration of multi-expert settings, its originality and technical depth make it a valuable addition to the conference. I recommend acceptance with minor revisions to address the practical and scalability concerns. 
Arguments for Acceptance:
- Strong theoretical contributions with novel insights into asymmetric regret trade-offs.
- Rigorous analysis and clear presentation of results.
- Potential for significant impact on both theory and practice in online learning.
Arguments Against Acceptance:
- Lack of empirical validation limits practical relevance.
- Complexity in implementation may hinder adoption.
- Preliminary treatment of extensions to K > 2 experts.