The paper introduces decision jungles, a novel approach to classification that extends decision trees and forests by employing rooted decision directed acyclic graphs (DAGs). The authors address a key limitation of decision trees: their exponential growth in memory with depth, which restricts their applicability on memory-constrained hardware like mobile devices. By allowing multiple paths from the root to each leaf, decision jungles achieve compactness and improved generalization. The paper proposes two node-merging algorithms, LSearch and ClusterSearch, to jointly optimize the features and structure of the DAGs during training. Experimental results demonstrate that decision jungles require significantly less memory than decision forests while achieving superior generalization across diverse datasets.
Strengths:
1. Novelty and Significance: The paper presents a compelling innovation by introducing decision jungles, which address a fundamental limitation of decision trees. The use of DAGs for classification is well-motivated, and the results demonstrate clear advantages in terms of memory efficiency and generalization.
2. Technical Rigor: The proposed algorithms (LSearch and ClusterSearch) are well-defined, and the optimization framework is grounded in minimizing a weighted entropy objective. The experiments are thorough, covering multiple datasets and comparing against strong baselines.
3. Practical Impact: The work has practical implications for deploying machine learning models on resource-constrained devices, a growing area of interest in the field.
4. Comprehensive Evaluation: The authors evaluate their method on diverse datasets, including semantic segmentation and UCI classification tasks, and provide detailed comparisons with decision forests and other baselines. The results consistently show the advantages of decision jungles in terms of memory usage and generalization.
5. Clarity: The paper is well-organized and clearly written, with a logical flow from problem motivation to experimental results. The figures and tables effectively support the claims.
Weaknesses:
1. Limited Scope of Applications: While the paper demonstrates strong results on classification tasks, it does not explore regression tasks, which are mentioned as future work. This limits the immediate applicability of the method.
2. Optimization Complexity: The proposed training algorithms, particularly LSearch, involve iterative optimization steps that may increase training time. Although the authors acknowledge this, a more detailed discussion of computational trade-offs would strengthen the paper.
3. Baseline Comparisons: While the baselines are well-chosen, additional comparisons with non-tree-based methods (e.g., neural networks or gradient boosting) could provide a broader context for the performance of decision jungles.
4. Generalization Gains: The improvement in generalization, while consistent, is modest on some datasets (e.g., UCI datasets). Further analysis of when and why decision jungles generalize better would be valuable.
Recommendation:
I recommend acceptance of this paper. Its contributions are novel, well-supported, and practically significant. The proposed decision jungles offer a meaningful advancement over decision forests, particularly for memory-constrained applications. While there are areas for improvement, such as broader comparisons and exploration of regression tasks, the paper provides a solid foundation for future work in this direction.
Pro and Con Arguments:
Pros:
- Novel and impactful contribution to decision tree/forest models.
- Strong experimental validation across diverse datasets.
- Clear and well-written presentation.
Cons:
- Limited exploration of regression tasks and broader baselines.
- Modest generalization improvements on some datasets.
- Potential computational overhead during training.
Overall, the paper is a valuable contribution to the field and aligns well with the goals of the conference.