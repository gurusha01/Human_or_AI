This paper presents an extension of Factorized Asymptotic Bayesian (FAB) inference to Latent Feature Models (LFMs), addressing a significant limitation in its prior applicability. The authors' main claim is that their asymptotic analysis of the Hessian matrix reveals that the Factorized Information Criterion (FIC) for LFMs has the same form as for Mixture Models (MMs), enabling FAB inference to be applied to LFMs. They further claim that FAB/LFMs exhibit desirable properties such as automatic hidden state selection, parameter identifiability, and superior performance compared to state-of-the-art Indian Buffet Process (IBP) methods in terms of model selection, prediction, and computational efficiency.
Strengths:
1. Novelty and Contribution: The extension of FAB inference to LFMs is a notable contribution, as it overcomes a key limitation of FAB's prior applicability. The derivation of FIC for LFMs and its equivalence to FIC for MMs is theoretically significant and broadens the scope of FAB inference.
2. Empirical Validation: The paper provides extensive experimental results, demonstrating FAB/LFMs' superior performance in model selection, prediction, and computational efficiency compared to IBP, VB, and MEIBP methods. The inclusion of real-world datasets strengthens the practical relevance of the work.
3. Theoretical Insights: The authors provide rigorous proofs, including an analysis of parameter identifiability and the introduction of a shrinkage acceleration mechanism, which significantly reduces computational costs.
4. Clarity of Contributions: The paper is well-organized, with clear explanations of the FAB/LFM algorithm, its theoretical underpinnings, and its practical advantages. The use of supplementary materials for detailed proofs is commendable.
Weaknesses:
1. Limited Discussion of Limitations: While the authors acknowledge that FIC approximation may be less accurate for small sample sizes, a more detailed discussion of other potential limitations (e.g., sensitivity to initialization or assumptions in Lemma 1) would strengthen the paper.
2. Comparative Analysis: Although the paper compares FAB/LFMs with existing methods, the computational advantage of MEIBP in certain scenarios is not fully explored. A deeper analysis of trade-offs between computational efficiency and predictive performance would be valuable.
3. Reproducibility: While the paper provides algorithmic details, some implementation aspects (e.g., parameter tuning for FAB and baseline methods) could be clarified further to enhance reproducibility.
Recommendation:
This paper makes a strong contribution to the field of Bayesian inference and latent feature modeling by extending FAB inference to LFMs. Its theoretical rigor, empirical validation, and practical utility make it a valuable addition to the NIPS community. However, addressing the identified weaknesses, particularly a more comprehensive discussion of limitations and reproducibility, would further strengthen the work. Overall, I recommend acceptance with minor revisions.
Arguments for Acceptance:
- Significant theoretical contribution by extending FAB inference to LFMs.
- Strong empirical results demonstrating practical utility and performance gains.
- Clear and well-structured presentation of methods and results.
Arguments Against Acceptance:
- Limited discussion of limitations and trade-offs.
- Some aspects of reproducibility and parameter tuning could be clarified.
In conclusion, the paper is a high-quality contribution that advances the state of the art in Bayesian inference for LFMs and aligns well with the goals of the NIPS conference.