The paper presents a compelling study on applying classification-based modified policy iteration (CBMPI) to the game of Tetris, addressing a long-standing challenge in approximate dynamic programming (ADP). The authors hypothesize that Tetris is better suited to policy-space exploration rather than value-function-based approaches, a conjecture supported by their experimental results. The paper's primary claim is that CBMPI achieves state-of-the-art performance in Tetris, surpassing traditional ADP methods and rivaling the cross-entropy (CE) method, while using significantly fewer samples.
Strengths  
The paper is technically sound and well-supported by extensive experiments. The authors provide a thorough review of prior work, situating their contributions within the broader context of Tetris optimization research. The novelty of applying CBMPI to Tetris is evident, and the results are impressive: CBMPI achieves the best-reported performance in the small 10×10 board and matches CE in the large 10×20 board while using only 1/6 of the samples. The authors also explore the trade-offs between rollout length and sample efficiency, providing valuable insights into the algorithm's behavior. The inclusion of multiple feature sets (e.g., Dellacherie-Thiery and Bertsekas) and their impact on performance further strengthens the study. Additionally, the paper is well-organized, with clear explanations of the algorithms and experimental setup, making it accessible to readers familiar with reinforcement learning and ADP.
Weaknesses  
While the results are impressive, the paper lacks a deeper theoretical analysis of why policy-space exploration outperforms value-function-based methods in Tetris. The conjecture that policies are easier to represent than value functions is plausible but not rigorously proven. Additionally, the reliance on specific feature sets (e.g., Dellacherie-Thiery) raises questions about the generalizability of the approach to other domains. The experiments focus heavily on Tetris, and it would be beneficial to discuss how CBMPI might perform in other complex environments. Finally, while the paper acknowledges the sensitivity of Tetris controllers to implementation details, it does not provide a comprehensive analysis of the robustness of CBMPI under varying conditions.
Pro and Con Arguments for Acceptance  
Pro:  
1. The paper demonstrates a significant improvement in ADP performance for Tetris, achieving state-of-the-art results.  
2. CBMPI's sample efficiency is a notable contribution, addressing a critical limitation of CE.  
3. The experimental methodology is rigorous, with comprehensive comparisons and ablation studies.  
Con:  
1. The theoretical justification for the superiority of policy-space exploration is underdeveloped.  
2. The results are domain-specific, and the broader applicability of CBMPI remains unclear.  
3. The paper does not fully address the robustness of the proposed approach.
Recommendation  
I recommend acceptance of this paper, as it provides a meaningful contribution to the field of ADP and reinforcement learning. The results are significant, and the insights into policy-space exploration are valuable, even if the theoretical underpinnings could be strengthened. The work is likely to inspire further research into CBMPI and its application to other domains.