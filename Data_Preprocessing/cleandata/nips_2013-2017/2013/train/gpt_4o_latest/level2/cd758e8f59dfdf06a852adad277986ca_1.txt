This paper addresses the problem of estimating the cluster tree of a density supported on or near a low-dimensional manifold embedded in high-dimensional space. The authors extend the k-nearest neighbor-based robust single linkage (RSL) algorithm proposed by Chaudhuri and Dasgupta (2010) to adapt to manifold settings. The key contributions include: (1) proving that the RSL algorithm achieves consistency and fast convergence rates independent of the ambient dimension, (2) establishing a sample complexity lower bound for manifold-oblivious clustering algorithms, and (3) introducing a framework for clustering in the presence of noise near manifolds. These results are significant for high-dimensional data applications where the manifold hypothesis holds.
Strengths
1. Novelty and Significance: The paper makes a notable contribution by extending density clustering to manifold-supported distributions, addressing the curse of dimensionality. The independence of sample complexity from the ambient dimension is a critical advancement for high-dimensional data analysis.
2. Theoretical Rigor: The authors provide strong theoretical guarantees, including consistency proofs and sample complexity bounds. The use of manifold-specific parameters like condition number and intrinsic dimensionality is well-motivated and insightful.
3. Noise Robustness: The extension to noisy settings, including clutter noise and additive noise, demonstrates the practical applicability of the proposed approach. The ability to recover the latent cluster tree under noise is particularly compelling.
4. Relation to Prior Work: The paper builds directly on Chaudhuri and Dasgupta (2010) while addressing its limitations in manifold settings. The discussion of related work is thorough, situating the contributions within the broader literature on density clustering and manifold learning.
Weaknesses
1. Empirical Validation: The paper lacks experimental results to validate the theoretical findings. While the theoretical analysis is robust, empirical demonstrations on synthetic or real-world datasets would strengthen the paper and provide practical insights.
2. Complexity of Assumptions: The manifold assumptions, such as bounded condition number and smoothness, may limit the applicability of the results to real-world datasets where these assumptions do not hold exactly. A discussion of how the method performs under weaker assumptions would be beneficial.
3. Clarity of Presentation: While the theoretical results are detailed, the paper is dense and may be challenging for readers unfamiliar with manifold learning or density clustering. Simplifying the exposition and providing more intuitive explanations of key results would improve accessibility.
4. Lower Bound Construction: The lower bound instance is only sketched and not fully formalized. A more rigorous treatment of this construction would strengthen the claim that the dependence on 1/(ετ)^d is unavoidable.
Pro and Con Arguments for Acceptance
Pros:
- The paper addresses a significant and challenging problem in clustering on manifolds.
- The theoretical contributions are novel, rigorous, and advance the state of the art.
- The results have potential applications in high-dimensional data analysis and manifold learning.
Cons:
- The lack of empirical validation limits the practical impact of the work.
- The assumptions and technical complexity may restrict accessibility and applicability.
- The lower bound construction could be more rigorously developed.
Recommendation
I recommend acceptance with minor revisions. The paper makes a strong theoretical contribution to density clustering on manifolds, but the authors should address the lack of empirical validation and improve the clarity of presentation. Including experiments and simplifying the exposition would significantly enhance the paper's impact and accessibility.