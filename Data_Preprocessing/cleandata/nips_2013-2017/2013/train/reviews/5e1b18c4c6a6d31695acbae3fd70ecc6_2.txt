The authors proposed to compress the features by optimizing a formulation that takes into consideration both the pointer cost and the dictionary cost. The non-convex formulation is relaxed by replacing the binary constraints with box constraints in the interval of 0 and 1, and ADMM is applied for solving the relaxed problem. Experimental results were reported in comparison with several approaches. 
The proposed compressive feature learning utilizes a losses compression formulation that considers both the pointer cost and the dictionary cost. The resulting problem is non-convex due to the binary constraints. The proposed formulation seems new. 
In solving the relaxation, ADMM is adopted. The proposed approach requires solving a series of relaxed problems that are dependent on the parameter d. These series of solutions adopt a reweighting scheme. How about the efficiency of the proposed compression approach? For subsequent classification, it was reported that the classification speed can be improved with the proposed approach, which is reasonable. 
After the features are extracted, the elastic net method is used for classification. The elastic net method has two tuning parameters, how are these two parameters tuned via cross validation? Two-dimensional grid search? 
After reading other reviewers' comments and the author response, the reviewer would like to keep the original recommendation. The authors proposed to compress the features by optimizing a formulation that takes into consideration both the pointer cost and the dictionary cost. The non-convex formulation is relaxed by replacing the binary constraints with box constraints in the interval of 0 and 1, and ADMM is applied for solving the relaxed problem.