This paper proposes a modication to the random forests to make the models more memory-efficient. Instead of using trees, it use DAGs. The paper proposes two methods for automatically learning the DAGs. 
The overall quality of this paper is clearly below the threshold for NIPS. First of all, the novelty wrt random forests is very limited. The only modification is that this paper uses DAGs instead trees. Although this paper claims that the advantage of DAG is that it is more memory efficent, but this point is not sufficently demonstrated (more on this later). 
Second, the proposed technique is very ad-hoc. Although Section 3 starts with an optimization problem in Eq 2-4. The optimization in Sec 3.1 ends up being some ad-hoc local search methods. The description of the algorithms in Sec 3.1 is extremely hand-waving. I doubt anyone can implement the algorithm with the given description. Also, does the algorithm converge at all (even to some local minimum)? 
The experimental results did not really demonstrate the benefit of the proposed approach. The main claim of this paper is that it is more memory efficent. However, the experiments in the paper only try to demonstrate that the proposed method generates models with less nodes. But the number of nodes is only one of the factors of model size (you also need to store other information of the model, e.g. the spliting function, the class distribution at leaf nodes, etc). It is not clear to me how the number of nodes will translate to memory efficency. For instance, if the number of nodes only takes a small percetange of the overall model size, perhapse 3000 nodes and 22000 nodes will take rougly the same amount of memory. In addition, memories are getting cheaper everyday. If the proposed method only reduces the memory by a small amount (say a few KB), it is not relevant for real applications. 
 This paper seems to be trivial modification of existing techniques. It is unlikely to have any impact and is clearly below the NIPS standard.