The paper presents a new online approach for solving POMDPs. The paper builds on some solid work, including POMCP, AEMS2, which are some of the state-of-the-art methods for this problem. The key new insight presented is to prune the forward search tree using regularization of the policy size. Another contributions is a performance bound on the value estimate; this is used in the algorithm to direct the search. The paper includes a number of empirical results, comparing with other recent POMDP methods (online and offline). The new LaserTag domain is also a good contribution, which would likely be re-used by other researchers. 
The core idea of controlling the complexity of the online search using constraints on the policy size is an interesting one, which has not been exploited in online POMDP methods. It provides good insight into the key question of balancing computation and solution quality in POMDPs. This is probably the most interesting aspect of the paper, which could have a major impact on solving large POMDPs. The authors suggest they will share the code - this would definitely be a plus. 
Even though I liked this idea, it's hard to predict what will be the impact of the work. In particular, looking at the empirical results, it's not clear to me what aspects of the proposed R-DESPOT framework have the most impact in practice. Is is the particle filtering? The DESPOT structure? Or the regularization itself? These aspects really need to be teased out carefully so we have a clear picture of what is going on. 
I have some concerns about the experiments. Why not re-compute the AEMS2 results? The algorithm should be simple to implement, given a DESPOT implementation. The results for AEMS2 are now 5 years out of date, and seem very competitive with what you report for DESPOT/R-DESPOT, at least on small domains. In the case of the larger domains, why not try AEMS2 with particle filtering belief representation, to see whether it's the belief filtering or the value approximation that really matters? This is a really important/interesting question, and the paper would be much stronger if it provided good evidence on this. Also, can you speculate on why your POMCP results are poorer than those in [3]? Overall, a more detailed discussion (Sec. 6) would be very useful to understand the impact of the work. 
The paper is for the most part well written, especially the early parts, including the intro, related work, and basic DESPOT framework. In the latter parts, there are a number of technical details that should be clarified. 
- Be very clear about what are the differences between DESPOT and R-DESPOT. Is it only the PRUNE function (line 301)? Can you give pseudo-code for this function? 
- Summarize the key differences between POMCP and DESPOT. I think both use the particle filtering representation of the belief. So it is mostly the use of the lower/upper bounds in DESPOT? This can be in Sec. 3 or Sec.6. 
- How loose is the lower bound? Do you use the actual bound in expression (1), or just some hand-tuned parameter L? 
- Add more detail on how the key parameters alpha and K (or L) are picked experimentally. Did you use the same ones for all the experiments? How much offline time was used to select them? I understand this is a side issue, but I need to understand the procedure and the amount of work required for this (at least order of magnitude). 
- How do you impose the time constraint (1 sec / action) in practice? Do you ignore the parameters (e.g. K) if they exceed the time limit? 
 This could be a very strong paper. It makes an potentially very interesting contribution to POMDP solving. But there is a lack of technical detail, which probably makes the results difficult to reproduce. And there are some non-trivial limitations in the results and discussion as presented, which make it difficult to assess whether the work will have a major impact or not. I look forward to the authors' response.