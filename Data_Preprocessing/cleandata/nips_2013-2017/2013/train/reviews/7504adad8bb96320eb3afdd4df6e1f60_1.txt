The paper empirically evaluates classification based policy iteration (CBPI) on the Tetris benchmark problem. There are no new technical results; instead the authors analyze the impact of various parameters of the algorithm on its performance. 
The results show that direct policy iteration (CBPI that does not use value functions) performs essentially as well as the cross-entropy method and classification-based policy iteration. In some sense, I do not find this too surprising since the DPI is very similar to the cross-entropy method in the way that policies are represented and optimized. 
I do find the analysis and comparison convincing in that the standard Tetris features are not suitable for representing value functions optimized by approximate policy iteration. However, the paper comes short of identifying why this may be the case or how one can design better features. This limits the importance of the results and their implication for other problems. In addition, I would like to see included other algorithms that compute approximate value functions in a different way from policy iteration. As it stands, the results show that good value functions cannot be computed using policy iteration, but that does not mean that other algorithms cannot find a good representation. For example, Smoothed Approximate Linear Programming [Desai, Farias, et al, 2012] has been used to obtain moderately encouraging results on this test domain and should be included in the comparison. The paper presents thorough evaluation of some major algorithms on Tetris, which is an important RL benchmark problem. While the evaluation shows that classification-based policy iteration performs better than DP, the paper lacks additional insights into the reasons for the difference in solution quality which limits the applicability of the results to other (practical) domains.