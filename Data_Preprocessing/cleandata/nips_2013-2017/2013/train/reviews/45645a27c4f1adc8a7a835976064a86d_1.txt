This paper proposes a novel model selection criterion for binary latent feature models. It is like variational Bayes, except that rather than assuming a factorized posterior over latent variables and parameters, it approximately integrates out the parameters using the BIC. They demonstrate improved held-out likelihood scores compared to several existing IBP implementations. 
The proposed approach seems like a reasonable thing to do, and is motivated by a plausible asymptotic argument. The main advantage relative to other methods for IBP inference is that computationally, it corresponds to an EM algorithm with some additional complexity penalties, rather than the more expensive sampling or variational Bayes algorithms. 
The technical contributions seem novel but incremental: they are essentially an extension of the FAB work of [3] and [4]. 
Some parts of the exposition were confusing because the math seems imprecise. In equation (3), the log sum of the z values is infinite if all of the values are zero. Since this has nonzero probability under any nondegenerate variational distribution q, why isn't the FIC score always infinite? 
Theorem 2 states that the marginal likelihood "can be asymptotically approximated as" something, but it's not stated what the asymptotic regime is or what the assumptions are. In particular, how is K assumed to behave as N gets larger? Since the BIC is applied to each component individually, the theorem seems to require a fixed finite K, so that the number of activations of every feature would approach infinity. Under the IBP model, the number of components is infinite, and new components would continue to be observed as the amount of data increases, so assuming finite K removes one of the motivations for using the IBP. 
More minor points: in section 2, shouldn't pk(X | z{\cdot, k}) include only the data points assigned to mixture component k, rather than the entire dataset? In equation (4), there should be an expectation on the left hand side. 
In the quantiative results in Table 1, the proposed method achieves higher predictive likelihood scores in less time compared to alternative methods. While FAB finishes faster by orders of magnitude in some cases, it's not clear how to interpret this result because the stopping criterion isn't specified. It seems like an arbitrary decision when to stop the Gibbs sampler, in particular. 
The improvements in predictive likelihood are significant, but where does the difference come from? The results would be more convincing if there's evidence that the difference is due to the model selection criterion rather than which algorithms get stuck in worse local optima, since the latter probably depends more on the details of the implementation (initialization strategies, etc.). (The fact that the Gibbs sampler learns an excessively large number of components in some of the datasets suggests that it's a problem with local optima, since in theory it should be exactly computing the likelihood which FAB is approximating.) 
 The proposed approach seems to make sense, but is somewhat incremental. Some of the math seems a bit imprecise. The experimental results show improvements in running time and predictive likelihood compared to previous approaches, but as stated above, I don't think enough analysis was done to be able to interpret these results.