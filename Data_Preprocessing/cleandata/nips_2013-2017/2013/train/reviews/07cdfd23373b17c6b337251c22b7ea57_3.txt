The authors develop a scalable approximate inference algorithm for the 
"triangle model" of networks. They develop a powerful subclass of the 
model that shares parameters in a clever way, and they use stochastic 
variational inference to infer the posterior with very large networks. 
The paper is well written, and well exexcuted. I think it is acceptable. 
p1: What is "parsimonious" modeling? I don't think this is a 
run-of-the-mill expression in the context of probabilistic models. (I 
might be wrong about that.) I suggest unpacking that term in a 
sentence since it is important to the paper. 
p1: From my reading of Gopalan et al., down-sampling the zeros does 
not comprimise accuracy. Your sentence in the introduction is 
misleading. 
p4: I'm sympathetic to the triangle model being OK from a pragmatic 
perspective, but I don't think appealing to LDA is fair. If we run the 
generative process of LDA, we will obtain something that "is" a 
document, albeit a silly one, even if it's not the original. Running 
your model, however, can produce objects that are not networks. Again, 
this does not bother me---it's a good and practical model---but the 
defense by means of LDA is not appropriate. 
p5: You should cite Hoffman et al. with Amari and Sato. In your 
scenario, where there are "local variables" (in Hoffman's language), 
Sato's algorithm does not apply. The needed theory is in Hoffman. 
p6: How did you generate power law networks in the latent space? The 
paper is vague around this point. 
p6: Earlier you cite Gopalan et al. as a way of speeding up MMSB. You 
should compare to their algorithm, in addition to (or instead of) MMSB 
batch. 
Small comments 
p2: mixture-membership -> mixed-membership 
p8: I suggest removing the last sentence. The second-to-last sentence 
is a stronger way to end the paper. 
In the bibliography, cite the JMLR version of "Stochastic Variational 
Inference". 
 The authors develop a scalable approximate inference algorithm for the"triangle model" of networks. They develop a powerful subclass of themodel that shares parameters in a clever way, and they use stochasticvariational inference to infer the posterior with very large networks.The paper is well written, and well exexcuted. I think it is acceptable.