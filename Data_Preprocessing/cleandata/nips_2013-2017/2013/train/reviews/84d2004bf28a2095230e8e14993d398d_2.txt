I think this is a really nice paper. It's addressing an important problem area, gives a simple practical algorithm that's easily implemented, and the empirical results are good. The theoretical analysis is well done and honest (this approach won't always work well, but the assumptions that need to be made are reasonable for learning tasks). 
My main criticism of the paper is that it feels squeezed for space. The experimental section, in particular, is much too terse around explanations of baseline comparison methods and explication of the results. (Figure 1 has a ton going on in it.) I find the strong performance of greedy/max interesting in itself any maybe worth a little discussion. Also, there's no discussion of run-time or cpu cost for any of the methods -- odd for a paper that's pushing on scalability, and should definitely be addressed. 
I'm surprised that the idea of lazy evaluation wasn't discussed more in this paper. This seems to give huge wins for efficiency, and should certainly be mentioned as a practical trick for a scalability paper, since some of the folks looking at this may be more engineering-oriented and not know about this. I also wonder if lazy-evaluation + mapreduce is a useful way to get by without the approximation method of this paper -- if you can eat the cost of the first round to select the first element, subsequent rounds will be really very cheap. (You can do things like "evaluate next 100 from the list" in parallel; if you hit on 55 on the list you've wasted some computation but are still doing fine compared to a full evaluation of all candidates). The first paragraph of 3.2 suggests that this is impractical for large k, but for large k things are expensive anyway. 
The phrase "diminishing returns" should be mentioned in the introductory part of section 3 (or in the introduction). I feel this is the most intuitive way to understand submodularity, for those who are not familiar with it. 
In the paper, it's not clear what's meant by "fit on one machine". Is the issue what can fit in RAM? On disk? The amount of CPU processing available? The first paragraph of section 2 makes it seem like CPU cost is the main bottleneck, but often times disk i/o is equally large an issue. What benefits (if any) can we get from a large-RAM machine with 16 or more cores? 
Since I'm asking for more detail in some places, I need to propose some things to cut: 
-- I'd definitely cut section 6 -- it doesn't add anything to the paper. 
-- I think we can also live without pseudo-code for algorithm 1. 
-- The first two sentences of paragraph 3 of section 1, and the last sentence of this paragraph, can be cut. The remaining sentence can be merged with the one above it. 
-- The first paragraph of section 2 can be significantly shortened. 
-- Paragraphs 2 and 3 of section 3.2 can be cut. (You might also mention the naive approach of sampling the data set down to a size that can fit on one machine and run the standard algorithm here.) 
 The paper proposes a simple but novel method for submodular maximization in a distributed framework. They show that it is a sound approach with theoretical analysis under some reasonable assumptions, and report good results across several possible applications.