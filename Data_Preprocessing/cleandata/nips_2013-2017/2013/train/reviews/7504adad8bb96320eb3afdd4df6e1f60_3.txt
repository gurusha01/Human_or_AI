The paper describes the first RL controller in the literature that has comparable performance to black-box optimization techniques in the game of Tetris. The topic is relevant to the RL community, as Tetris has always been a difficult benchmark domain for RL algorithms. 
Quality 
The contribution of the paper is to apply a previously published algorithm (CBMPI) for solving the game of Tetris. The authors present competitive results with the previous state of the art optimization algorithm for Tetris - the Cross-Entropy method (CE). The empirical results are convincing and the boost in performance as compared to previous attempts to apply RL controllers to this domain is remarkable. 
That being said, the paper lacks a detailed discussion regarding the (possible) reasons for this improvement. While the authors mention in the introduction that the key conjecture (that was positively tested by the paper) is that one should use techniques that search in the policy space instead of the value function space, more details would improve the paper. To be concrete, here are two examples of topics: 
1. In lines 252-253 the authors describe the state distribution used for sampling rollout states. This state distribution comes from executing a very good Tetris controller. It would be interesting to describe how strongly this choice of distribution influenced the performance of CBMPI. If the authors tried other solutions that didn't work well, reporting it would be useful for anybody trying to replicate the results. For example, it would be interesting to know whether the states CBMPI visits while learning constitute a reasonable rollout distribution. 
2. The optimization algorithm used for the "Classifier" stage of CBMPI is CMA-ES. As discussed in "Path Integral Policy Improvement with Covariance Matrix Adaptation" (ICML 2012) for example, CMA-ES is part of the same family of algorithms as CE. It is thus interesting that an ADP algorithm that uses a CE-type algorithm in its "inner loop" becomes comparable to CE as applied to searching directly in the policy space. So one natural question is: what would happen if CMA-ES would be replaced by other types of optimizers (gradient-based for instance). 
Clarity 
The paper is well written, easy to follow and the contributions are clearly stated. 
I have several minor comments: 
- The definition of the "Classifier" in line 222 is confusing: shouldn't the output be "labels" and not differences of Q values? 
- In line 266/267, CMA-ES is named "classifier" - but it is optimization algorithm. 
- It's not clear why graphs 4a and 4d are disjoint but 5d contains the performance comparison for all algorithms. 
- Section 3.1 - "Experimental" is spelled incorrectly. 
Originality and Significance 
The paper solves a difficult problem in a novel way and provides interesting insights about how to apply RL algorithms to non-toy domains. It doesn't introduce new algorithmic or theoretical tools. As discussed above, I think the results are significant for the RL community. 
Finally, I have several questions: 
1. If the paper is accepted, are the authors willing to share the code for reproducing their experiments? 
2. The variance of scores tends to be high in Tetris so the graphs should be modified to include confidence intervals for the mean values. So, are the differences we see in the graphs (Figure 4D for example) statistically relevant? 
3. Regarding the claim in the sentence in lines 393-395, what happens if CBMPI receives the same number of samples as CE? Will it improve, or is the learning curve already at a plateau so more samples wouldn't help? 
4. Regarding the comparison of the best policies - how did you compute the scores of the policies DU and BDU? Did you take the values from the referred paper ([20]), or did you execute those policies using your code base? I'm wondering what is the performance of the best policy discovered by CE in your experiments, and I wanted to make sure such a result is reported. The paper needs a more detailed discussion of the results to maximize its impact and usefulness to the community. But it is a valuable contribution to the literature that sheds new light on a previously unknown phenomenon: the poor performance of RL algorithms in the game Tetris.