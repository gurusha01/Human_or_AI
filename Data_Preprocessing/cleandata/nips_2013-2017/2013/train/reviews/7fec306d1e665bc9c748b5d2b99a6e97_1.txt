Summary: 
"Predicting Parameters in Deep Learning" explores the hypothesis that there is 
significant redundancy in the naive (and universally used) representation of 
the parameters of neural networks. 
Pro: 
* Interesting observation and interpretation 
* Experiments show promising evidence 
Con: 
* Empirical results do not support central claims 
* Missing comparisons with common weight matrix factoring like PCA preprocessing 
Quality: 
At a few points in the paper the authors remark on how difficult it is to 
train with a parameter matrix W factored as a product UV. Could the authors 
offer some intuition for why? One reason might be that if U = V = 0 at the outset, 
absolutely naive backprop will not move them. On the other hand if W is 
trained normally but simply constrained to be a product of some U and V, some 
computational overhead during updates can be traded for lower communication 
cost. 
A more fundamental concern is that the paper's central observation that 
weights are redundant, together with the use of a spatial exponential kernel 
suggest that similar gains could be had by either doing a PCA of the input, or 
downsampling the images and the kernels for convolution. These are both common 
practice. PCA pre-processing also represents a low-rank factorization of the 
first layer's weights. 
The technique of maintaining a PCA of intermediate layers is not common practice, but 
it might be helpful, and it certainly represents a conceptual starting point for your 
work on "data driven expander matrices". 
The discussion of columnar architectures and convolutional nets seems 
reasonable, but largely hypothetical. Pointing out various ways to 
refactor matrices in various architectures without more extensive empirical 
follow-through seems like a departure from the core thesis of the paper. 
The empirical aspect of this paper is important, and I think it needs more 
work. The claim that the authors test is that "there exists models whose 
parameters can be predicted", but that is a weak claim. The claim the authors are aiming for 
from the tone of their introduction and which I agree they should be aiming for is that: 
"there exist large models that work better than naively down-sized versions 
and using our techniques we do less damage to the large models than simply 
down-sizing them". This second point is the central claim of the text of the 
paper, but it is not supported by empirical results. 
The claim that parallel training can actually be accelerated by the use of 
these techniques also requires more justification. The proposed method 
introduces a few new sorts of computational overhead. Particular 
encoder/decoder algorithms for transmitting and rebuilding weight matrices 
would need to be implemented and tested to complete this argument. 
Clarity: 
"collimation" - mis-used word; it is actually a word, but not about making columns 
"the the" 
"redundancy" is probably the wrong term to describe the symmetry / equivalence 
set around line 115. 
Section 2 "Low rank weight matrices" is a short bit of background material, it 
should probably be merged into the introduction. 
Figure 1 is only referenced in the introduction, but it actually illustrates 
the method of section 3.2 in action right? It would be clearer if Figure 
1 were put into Section 3.2. 
Please number your equations, if only for the sake of your reviewers. 
Originality: 
The idea of looking at weight matrices in neural networks as a continuous 
function has precedent, as does the observation that weights have redundancy, 
but the idea that training could be accelerated by communicating a small fraction of 
randomly chosen matrix values is new. 
Significance: 
As the authors point out in their introduction, this work could be very 
interesting to researchers trying to parallelize neural network training across 
very low-bandwidth channels. 
Edit: After reading the authors' rebuttal I have raised my quality score. I apparently did not understand the discussion of columns, and how it related to the experimental work. I hope the authors' defense of their experimental work is included into future revisions. The paper has some good ideas and gets you thinking, but the empirical resultsdo not really support the most important and interesting claims. The algorithmfor actually accelerating parallel training is only sketched.