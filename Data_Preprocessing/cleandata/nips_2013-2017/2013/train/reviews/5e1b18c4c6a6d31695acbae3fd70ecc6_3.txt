This paper formulates the problem of selecting a subset of informative n-grams for document classfication as a lossless compression problem solved by iterative relaxation of he original hard combinatorial problem. While unsupervised feature learning as compression is not a new idea, this particular formulation is interesting, seems novel, and performs fairly well in small-scale experiments. The paper is very well written, the ideas are clear and reasonably motivated. The algorithm presentation, while not fully self-contained (there's a substantial supplement), is understandable as given. I would have liked more analysis of the algorithm's computational properties, or at least some experiments on computational cost-accuracy tradeoffs to help understand the scalability of the method (including the claims of parallelizability). Still in the experimental side, I'd also liked to see comparisons with popular lossy representation methods such as embeddings (Bengio et all 2006, Collobert and Weston 2008, Mikolov et al 2010, inter alia). And I'd like to see the tradeoffs between model size and accuracy obtained with this method compared with sparsifying regularization over the uncompressed n-gram features.  Formalizes n-gram feature selection for document classification as lossless compression, with an efficient relaxation algorithm for the original hard combinatorial problem. Elegant and well-motivated approach, with basic positive results but in some need of more analysis and experimentation.