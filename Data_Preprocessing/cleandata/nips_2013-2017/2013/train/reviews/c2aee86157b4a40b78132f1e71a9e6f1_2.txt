This paper presents a determinized sparse partially observable tree for online POMDP planning. For that, it derives an ouput-sensitive performance bound, showing thus that the online proposed approach works "well" if a simple policy exists. The online approach itself searches in a sparsely sampled belief tree for a policy that optimally balances between the size of the policy and the accuracy on its value estimate obtained through sampling. 
Experiments show that the proposed algorithm scales up better than existing online methods. 
This work appears however incremental and unlikely to have much impact even though it is technically correct and well executed. Indeed from the originality point of view, it is just a technical attempt to have an algorithm which scales up better. Unfortunately this attempt presents some limitations 
(1) The proposed regularization does not appear beneficial to the RockSample(n,k) even if this is mostly deterministic 
(2) The initial lower bound as suggested by the authors does not satisfy the requirements for Theorem 1 to hold and we do not what's the importance of that in practice. 
(3) The proposed algorithm relies on tree search and we know that this type of search is very sensitive to the branching factor (which is the number of observation) and this is not taken into consideration neither theoretically nor experimentally. Although the authors stated " Theorem 1 indicates that R-DESPOT may still perform well when the observation space is large". However "MAY STILL" remains (for me) too fuzzy and I think a formal and practical analysis in terms of observations is required here. 
It is not clear what is represented in Table 1, if it is a "running time" then why we have some results as -6.03; -6.19, etc. 
The right reference for AEMS is : 
ROSS, St√©phane et CHAIB-DRAA, Brahim. AEMS: An Anytime Online Search Algorithm for Approximate Policy Refinement in Large POMDPs. In : IJCAI. 2007. p. 2592-2598. 
For its part [2] is a general overview containing many comparisons on online POMDP approaches. 
The authors say and repeat all the time "the algorithm works well if simple policy exists". What does this mean? What does ``simple policy" mean? what can we do if we do not have a simple policy? should we ask some expert? 
 An incremental online POMDP planning of which the branching factor is not taken into consideration and the experiments are not convincing.