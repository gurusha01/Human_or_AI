The paper proves linear convergence of the standard proximal gradient method for trace norm regularized problems, under weaker assumptions than strong convexity of the loss function, which is a crucial problem for many ML and signal processing applications. 
The paper is carefully and clearly written. It closely follows the Lipschizian error bound technique of Tseng [20], whose application to the nuclear norm case was still left as an open question. The main part of the proof seems correct as up to my understanding. 
* What I miss in this paper is a clear comparison/analogy to the vector case of l1-norm regularization, as e.g. in [20,22] and related work. 
It would be very helpful to relate to the results and techniques used in that case, and to explain if/why the l1-case techniques do apparently not directly translate to trace norm. (Also now the l1-norm case should follow from the trace norm one as a corollary). 
As for judging the value of the paper, this should be taken into account, as some people might find it not extremely surprising that the l1-result also holds for trace norm (I still find it a very valuable contribution though). 
 Independent of that I would suggest a slight change of the title of the paper, as the main new contribution affects the loss part more than making the trace norm regularization a structured* one. (Since the newly gained flexibility by the linear operator A is on the loss side, not in the regularizer). 
* Minor issues 
- Notation: Calling P a sampling or mask operator? (sampling might hint at randomness, but here P is fixed) 
-l337: Broken reference 
- The meanings of "Q-" and "R-" linear convergence are maybe not that widely known, and could be repeated for clarity. 
- the residual R(X) defined in (6) could be additionally explained in relation to the original definition (46) by [20] for better interpretation. 
In general, I'd vote to include a bit more discussion and maybe conclusions in favour of the long technical part of the proof of Lemma 3.2 in Section 3. 
* Update after author feedback: 
Thanks for answering most questions. As for the title, i was referring to the word structured, which i found unnecessary in the context here, but of course i leave that to the authors. The paper proves linear convergence of the standard proximal gradient method for trace norm regularized problems, under weaker assumptions than strong convexity of the loss function, which is a crucial problem for many ML and signal processing applications.