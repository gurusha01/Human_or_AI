Added after authors' feedback * 
Please include a simulation where the loss is not strongly convex but PG algorithm converges linearly (for different levels of stability), and importantly discuss when the theory fails. Please also discuss on how this can be extended to the analysis of ADMM. 
* 
This paper proves the linear convergence of the proximal gradient method applied to trace-norm regularized learning problem when the loss function has the form of f(X)=h(A(X)), where A is linear and h is strongly convex on any compact set and has Lipschitz gradient. 
This paper is an extension of Tseng [20], Tseng and Yun "A coordinate gradient descent method for nonsmooth separable minimization" and Zhang et al. [22], which established the same result using the "error-bound condition" for lasso and group lasso, to the trace norm. This is a non-trivial extension but the contribution seems purely technical. 
The presentation of the proofs is mostly clear. 
Strength: 
- Shows the linear convergence of the proximal gradient algorithm extending the result of Tseng et al. 
Weakness: 
- The contribution is purely technical. 
- I would like to see a numerical example showing the linear convergence. 
More details: 
1. The outlines of the proofs of Theorem 3.1 and Lemma 3.2 seem very similar to those of Tseng. The authors should refer to the original work more precisely to make their contribution clearer. 
2. I would like to see a numerical example that indeed shows the linear convergence (a semilog plot of function value vs. the number of iterations). 
3. It is not clear how the constants \kappa1, ..., \kappa4 depend on the choice of \underbar{\alpha} and \bar{\alpha}. 
Minor issue: 
4. The sequence of inequalities before inequality (13) is confusing. The last inequality follows due to the convexity of the trace norm and the fact that -\bar{G}\in \tau\partial\|X\|_{\ast} and not because of the intermediate inequality (see p289 in [20]). 
 Extension of previous linear convergence result based on the "error-bound condition" from lasso and group lasso to the trace norm.