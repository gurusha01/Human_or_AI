The paper considers the problem of learning sparse bayesian networks. 
The paper follows the literature in addressing the problem through a dynamic programming based approach to finding the optimal ordering to determine the network 
and learn the parameters of the distribution. 
The contributions of the paper seem to be the use of a consistent and admissible heuristic inside A search. The paper also proposes heuristic schemes to improve scalability of the DP A search based approach. 
The scalability gains are observable for small data sets in the numerical experiments. However, the complexity of the algorithm in the worst-case still seems exponential. So, for problems with large number of nodes in the network, the proposed algorithm doesn't seem practical. On the other hand, the SBN algorithm of Huang et al 
avoids DP and guarantees to find the optimal solution in polynomial time. 
The SBN algorithm is significantly slower than the algorithm in the paper. 
However, the SBN algorithm comes with a polynomial time guarantee and the algorithm in the paper, though fast in experiments, is based on heuristics without guarantees. 
In summary, the paper proposes fast heuristics inside a DP approach to learn sparse bayesian networks. However these heuristics don't come with any guarantees of optimality and the overall algorithm is also not guaranteed to be polynomial time. 
The algorithm in the paper would be attractive if there are some theoretical results 
to back up the performance of the fast heuristics in the algorithm. Also, the experiments could be more convincing when the algorithms are compared on data sets with large number of nodes. The paper proposes a DP-based approach with fast heuristics embedded that are consistent and admissible. While the algorithm is empirically faster than state of the art, the heuristics used for scalability in the algorithm doesn't come with any theoretical guarantees.