The authors present an algorithm for matrix reconstruction under noisy observations. The particular setting looks at low-rank matrices with additional assumptions on the factors and uses an Approximate Message Passing approach in order to speed up the classical, computationally expensive, Bayesian approach. The authors also connect matrix reconstruction with K-means clustering, which is an interesting application domain for the proposed algorithms. 
To the best of my knowledge, the Approximate Message Passing approach for matrix reconstruction is novel and interesting. The connections between low-rank matrix factorizations and K-means are fairly well-known (e.g., PCA provides a factor two approximation algorithm for K-means). However, this allows the authors to provide a nice experimental evaluation of their algorithms and compare them to k-means and k-means ++. Interestingly, their approach seems faster and more efficient than classical k-means and k-means ++ according to their empirical data. The authors compare both the Frobenius norm residual, as well as the actual clusterings, which is a nice feature of their experimental evaluations. 
The main weak point of the paper is that the proposed algorithm comes with few theoretical guarantees in terms of convergence. This is to be expected, since many other algorithms for K-means also have only weak properties. The authors might want to at least cite more papers in Theoretical Computer Science that provide provably accurate algorithms for the K-means objective function. A solid paper presenting Approximate Message Passing algorithms for low-rank matrix reconstruction and k-means. Promising experimental evaluation, somewhat weak theoretical results.