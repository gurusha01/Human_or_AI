The paper propose a new parallel dual coordinate descent method for solving regularized risk minimization problems. In the proposed algorithm, each machine/core updates a subset of dual variables using the current parameter, and then conduct the "delayed" update to the shared parameter. 
The similar idea has been used in yahoo-LDA and matrix factorization (Hogwild), but to my knowledge this is the first applying to dual coordinate descent method. Nice Theoretical guarantee is provided in Theorem 1. Experiments show that the method outperforms other parallel algorithms on linear SVM problems. My only complain is that Figure 3 is too small and hard to read; also, I suggest to use log-scale on primal obj and duality gap.  This is a good paper and I vote for acceptance.