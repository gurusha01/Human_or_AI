This paper investigates a distributed implementation of stochastic dual coordinate ascent (SDCA). The DisDCA algorithm is shown to enjoy the same convergence guarrantees as SDCA, and compared to ADMM it has no parameters that need tuning. Empirically it performs as well as a well-tuned ADMM implementation. 
The paper is well-written and technically sound. The algorithm is novel but straightforward and I really enjoyed the section on the tradeoffs between computation and communication. Distributed learning is a very active topic yet not many papers try to analyze the regime at which the algorithms are competitive. In general I don't have any major complaints with the paper except that the figures, both in the main paper, as well as the supplementary material are extremely hard to read. I suggest that Figure 3 contains a sample of the results and the rest are over multiple pages in the supplementary material. This is a solid paper and relevant to the NIPS community. Distributed DCA is a nice alternative to ADMM.