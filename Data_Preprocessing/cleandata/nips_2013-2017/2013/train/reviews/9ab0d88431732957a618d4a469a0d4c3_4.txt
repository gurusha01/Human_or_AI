This work proposes a new robust method and a new unified framework 
for some learning task generalizing clustering models. It 
allows to robustly deal with problems such as clustering, subspace 
clustering or multiple regression by encompassing weights on the data 
samples. 
To better handle outliers, probability distributions over the data 
(in classical settings Dirac distributions are mostly considered) 
are attached to each "model". Moreover, the distributions are obtained 
by minimizing a trade-off between two terms. 
The first one is the (Euclidean) 
distance between the average weight distribution to the uniform distribution. 
It aims at producing spread out weights inside each 
model (it is a regularization term). 
The second one is a weighted loss function that take into account the 
importance of each samples (it is a data fitting term). 
Globally the notation, formulae and mathematics details 
are really loose. The English level is also rather poor. 
Worst, very often the clarity of the paper is at stake. 
Though the motivation and the method proposed are interesting, 
I feel that this version of the paper is rather a preliminary investigation, 
than a camera-ready paper. More work is needed to make it clear. 
For instance, to clarify their point, the authors should 
provide, with careful details, what the proposed method does in the simplest 
case of their framework, i.e., in the context of clustering (with possibly experiments 
in the presence of outliers). 
This could be better developed by moving Section 2.1 into the Appendix. 
The experiment section is even worse than the theoretical part. 
Not enough details are provided to understand the figures, and section 
3.1 is particularly uninformative in term of practical performance. 
Can the author compare with other methods for the specific task proposed, 
as, for instance, for the clustering task mentioned earlier. 
Last but not least, a longer discussion on the way the trade-off parameter 
\alpha is chosen (both in theory and in practice) should be given. 
Additional line by line comments: 
l086: other generalizations of the the k-means algorithm could be referenced 
here as the framework proposed by 
Banerjee et al. "Clustering with Bregman divergences", JMLR, 2005. 
l103--106: the notations are not introduced before they are used: X, M, Delta_n^k, etc. 
l109: \Delta^{n 1} is not consistent with the notation defined just before. 
l147: a a 
l161: the footnote should be given on the previous page. 
l185: mention that u=(1/n, ... , 1/n) (if I understand correctly) 
l186: indices in the sum are not consistent... 
l194: is the optimization over W or over one single w_j. Please remove 
this ambiguity. 
l200: so why using an unusual closeness index? can you motivate your choice? 
l207: "average penalty" what is this referring to precisely? 
l232: " has 1/3 the point"? I don't understand this sentence, neither do I get the 
figure. 
l254: P_\Delta^n: is never defined (only afterwards in the Appendix!!!). 
l299-314: What is MAD referring to in the legend? it is nowhere defined before. 
l368: the rhs depends on i, the lhs does not. Please correct. 
l402: requires->required 
l403: converge->converges 
l457: Candes -> Cand\`es 
l468: what is the nature of this work?article, conference, etc... 
l553: "with the corresponding variable ...": this sentence is unclear 
l778: is the equality true for all U? It seems false without more assumptions 
that should be reminded. 
 This papers proposes a general robust formulation for handling multiple model learning (e.g. extending clustering algorithms) by considering optimized data distributions.