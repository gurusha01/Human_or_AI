The authors study space partioning trees with the nearest-neighbor search. More 
specifically, they study the error caused by dfs search (without backtracking) 
and show connection between the error and quantization error of the tree. 
The authors work is interesting, however, there seems to be some weaknesses. 
Authors demonstate the connection between the search error and the quantization 
error by bounding the search error with a (rather complex) bound that has 
parameters derived from the quantization error. However, this connection 
is implied only if the bound is tight, which doesn't seem to be the case: 
Consider the expansion coefficient c in in Theorem 3.1. This coefficient 
depends on q. If we select q to be far enough from S: let's say that that 
min_{x \in S} |x - p| is bigger than the distance of any two points in S. This 
implies that expansion coefficient is n, which forces L in C2 to be 0, making 
Theorem 3.1. unusable for this case. The problem with this one is that Theorem 
3.1. analysies only upstairs of the distance error. I think a more solid result 
could be obtained is the downstairs term is also taken into account 
simultaneosly. 
The fact that c depends on q implies that Theorem 3.1 cannot be used in 
practice to obtain the actual error bounds while doing the actual search since 
computing expansion coefficient seems to be very computationally expensive. 
Theorem 3.1. can be only used as a theoretical evidence for the connection 
between the search error and the quantization error. 
Let's make an assumption now that q is actually "within" S, that the expansion 
coefficient doesn't change that much when we add q into S. Consider the 
following case: copy S and transpose the copy from S such that they both copies 
are fully separated. The expansion coefficient should stay about the same, as 
well as \omega (since any reasonable tree would first separate the copies from 
each other, giving \omega = 0, for the first level). While the quantization 
improvement rate will be excellent on the first level, it will be as bad on the 
next levels as with original S. Consequently, \beta \beta will stay the same. 
As we move the copies from each other away. \psi will get larger, and the bound 
will get loose. I think a better approach would not to consider \psi and \beta 
globally but instead try to work on a "local level". 
While these cases may not happen in practice, I would like to see authors 
demonstrating how tight is the bound, for example, empirically, similar to 
Figure 2. 
Techical writing could be improved. The authors tend to use accents and 
indices in cases where they are not really needed. For example, 
\tilde{c} -> c 
B{l2} -> B 
use less exotic caligraphy for intrinsic dimenstion d and full dimension D 
027: - -> --- 
Definition 2.1. is a bit messy. It would be better to just say that 
quantization error improvement rate is equal to 
beta = VS(A) / (VS(A) - VS(Al, A_r)) 
C2: what do you mean by complete? 
C4: in Theorem 3.1. are all nodes in T included or only leaves? 
 The authors bound the search error of non-backtracking dfs of space-partitioning tree (for NN search) with a quantization error.Interesting paper and while the connection makes sense, I am not convinced that the provided bound is tight, which dilutes the impact of the paper.