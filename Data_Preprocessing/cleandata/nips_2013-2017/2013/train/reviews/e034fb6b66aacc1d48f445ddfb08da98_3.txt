The paper proposes a Bayes-optimal approach for integrating human feedback with reinforcement learning. The method extends and is compared to the baseline of Bayesian Q-learning. The approach is tested experimentally on two domains (Pac-Man and Frogger). 
Quality 
------- 
Overall, the paper is sound and logical. However, there are a few problems: 
1) The authors claim that their approach does not convert the feedback to rewards or values. But, by calculating delta_{s,a} from the count of the labels (right/wrong) they essentially convert the labels to values. 
2) The name of the proposed approach (policy shaping) is a bit misleading. In fact, the feedback is given per action, not per whole policy/episode during the learning. Therefore, a more appropriate name would have been, maybe, "action shaping". 
Clarity 
------- 
The paper is generally well written and flows logically. There are a few parts, though, that are a bit confusing: 
1) Section 4.2. In it, the authors at first state the assumption that the optimality of an action a in a given state s is independent of the labels provided to other actions in the same state. This leads to formula (1). However, the following formula (2) violates this assumption by relying on the values delta_{s,j} from other actions in the same state. 
2) The first time the authors clearly state how the human feedback is given (as binary right/wrong labels immediately following an action) comes too late in the text (around line 112, on page 3). It should have been much earlier in the text. 
3) Section 5.3. It is not entirely clear to me how the pre-recorded human demonstrations are used to produce a simulated oracle. 
Originality 
----------- 
Unfortunately, some of the most interesting problems are left for future work (e.g. the credit assignment problem, mentioned on line 125, as well as the case when there is more than one optimal action per state). 
The proposed method for resolution of the multiple sources does not seem to be elaborate enough. By multiplying the two probabilities, both of them are taken into account with equal weight, even if one of them is less reliable than the other. A better approach would have been to use the value of C to evaluate the reliability of the human feedback and take this into consideration. 
Significance 
------------ 
In my opinion, the demonstrated improvement by using the additional human feedback is not sufficiently significant to justify the large amount of additional information needed by the algorithm. In fact, if the "oracle" information is directly provided to the agent in the form of "demonstrations", the agent would be able to "jump-start" the learning from a very high-return initial policy, and further improve it during the episodes. 
 The paper proposes a Bayes-optimal method for inclusion of binary right/wrong action labels provided by human into reinforcement learning. The paper is well written, but could be further improved in terms of clarity, originality and significance.