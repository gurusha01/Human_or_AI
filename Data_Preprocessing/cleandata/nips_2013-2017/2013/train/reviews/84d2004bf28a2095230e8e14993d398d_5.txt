This paper is on solving submodular maximization problems at scale. 
In particular, the paper looks at the classical greedy algorithm for 
submodular maximization under cardinality constraints and offers 
modifications to run these algorithms on massive data. 
The problem itself is quite motivated. There have been a few earlier 
work on trying to "speed up" or "parallelize" the inherently 
sequential greedy algorithm for the submodular maximization problem. 
MapReduce as a programming paradigm to express the algorithm is also 
well motivated. 
The main technical contribution of the paper is an analysis of the 
following two-round algorithm: the input is split across the machines 
and each machine approximates the solution to its part of the input 
(by running a sequential greedy algorithm) and then the individual 
solutions are combined to obtain the final algorithm. The key point 
here is for each machine to output a solution of size more than k/m (k 
= desired solution size, m = number of machines). The analysis itself 
is quite simple and the paper shows inherent dependence on both k and 
m. The paper also has sundry results for special cases, for eg, 
smooth spaces and for decomposable functions. 
The experimental results are reasonable showing the efficacy of the 
two-round algorithm when compared to standard greedy. 
On the positive side, the paper addresses an important problem and 
proposes a practical modification of the standard algorithm. 
The main negatives of the paper are the following: 
1. The paper is near-trivial on the theory front. The analysis is so 
obvious from a theoretical perspective. Some of the proofs are 
repetitive in nature and the seemingly strong results stem from quite 
strong assumptions. Randomized input partition is not taken advantage 
of in Theorem 4.2 (or show a lower bound). 
2. There is no round-memory-approximation tradeoff. The paper is 
restrictive in its results and the approach itself is unclear to be 
generalized to multiple rounds. In this sense, it is significantly 
weaker than the earlier work (WWW paper of Chierichetti et al or the 
SPAA paper of Lattanzi et al). 
3. The experimental results contain several needless baselines 
(random/random, for example). The authors do not try to bring out the 
importance of oversampling by modifying the greedy/merge to make 
greedy as a function of the size of the local solutions. 
Additional comments: 
1. The paper should investigate if slightly stronger bounds can be 
proved for Theorem 4.2 when the inputs are randomly partitioned. 
2. The authors may want to look at the SPAA 2013 paper of Kumar, 
Moseley, Vassilvitskii, and Vattani that addresses a similar problem 
but in more general context and provides a multi-round and better 
approximation algorithm. 
3. It might be possible to "merge" the results in Theorem 4.3 and 4.4 
since they seem to be using related assumptions (neighborhood size for 
metric spaces vs. growth function of a metric, which is the volume). 
4. The authors may want to compare their algorithm with the Chierichetti 
et al paper and the SPAA 2013 paper. 
5. page 4, line 167: "explain what is "suitable choice"" 
6. page 5, line 227: didnt get the comment about "... unless P = NP". why does it follow? 
 A theoretically weak paper addressing an important problem.