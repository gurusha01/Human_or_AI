Classic work on associative memories following Hopfield 1982 focused on issues of capacity and performance, usually considering random memories embedded as stable attractors of a dynamical system. Such work usually led to capacities which scale linearly with the size of the network. The present work proposes a neural architecture which is able to reach exponential capacities at the cost of introducing specific low-dimensional structure into the stored patterns. 
The authors propose a bi-partite architecture of pattern and constraint neurons corresponding to patterns and clusters, and a two-tiered algorithm, based on within and between-module processing aimed at retrieving clean versions of noise corrupted the patterns. The intra-module algorithm operates iteratively based on forward and backward iterations, based on a belief variable. The inter-module algorithm is based on iterations taking place until all constraints are satisfied. The authors present theoretical results relating to convergence, showing that there is a noise range for the internal dynamics, which leads to potentially improved performance relative to noise-free dynamics. Simulation results seem to corroborate these theoretical claims, also demonstrating an interesting threshold effect as a function of the noise parameter. 
The results presented here are interesting and, to the best of my knowledge, novel. The fact that noise improves performance in such a nonlinear dynamical system is indeed surprising and non-trivial. While the authors have tried to present some intuition about their results (the proof of which appears in the appendix), I was not able to get a good feeling for 'what makes the system tick'. 
Specific comments: The authors do not seem to be aware that a great deal of early work on associative memories dealt with stochastic retrieval dynamics, as well as in structured patterns leading to higher capacities. AN early review of many of these results can be found in Daniel Amit's "Modeling Brain Function", and later work. In fact, the advantageous effect of noisy dynamics on improving retrieval by eliminating so-called spurious states was noted there. 
Although the authors have shown that noise may assist in improving retrieval, it would be nice to understand the robustness of this result. For example, what would happen if the weights W_{ij} were corrupted by noise? Such robustness had been shown in early neural network studies (again, see Amit's book). 
Finally, the dynamics given by eq. (2) and (3) seem to be at odds with the requirement that the patterns be restricted to a fixed finite range {0,â€¦,S-1}. How is this guaranteed by this update rule? 
 A work which suggests how to increase the capacity of associative memories exponentially at the price of introducing specific structure into the stored patters. The authors do not relate to earlier work on the effect of noise, but present an interesting result on possible merits of noise.