The authors present a hierarchical extension of the IRM for network modelling using the key ideas from the Bayesian rose tree paper: 1) that the hierarchy is used to specify a mixture over consistent partitions of the nodes 2) that this hierarchy can be learnt using an efficient greedy agglomerative procedure. Qualitative results on the Sampson's monks dataset, and full NIPS dataset, and quantitative results on the NIPS-234 dataset are presented. The proposed inference is computational much cheaper than the IRM, whilst obtaining similar predictive performance. 
Quality/clarity. The paper is very well written and the exposition of the key ideas is clear. 
Originality. The main ideas are taken from the Bayesian rose tree work, but there was some work to do in adapting these ideas to the relational data setting, particularly in maintaining the same computational complexity. 
Significance. Scaling BNP models to, especially network models, is an important challenge if these methods are to be used by practioners. While the n^2 scaling of the basic algorithm may prove prohibitive for larger networks, the sparse version should presumably remain feasible. It would have been nice to see some timings on larger real world networks. NIPS-234 takes only tens of seconds: how long does full NIPS take? Gene networks would typically have tens of thousands of nodes, and real social networks are much larger still: how would inference scale? 
In addition it would be nice to have some feel how close to the optimal tree the greedy method gets. This could be investigated at the largest scale that you can still find the optimal tree by exhaustive search. Is it possible to given any theoretical support to the method? e.g. in terms of submodularity? 
Minor corrections: 
Line 48 page 1: whilst... but 
pair of treeS 
A fruitful next steps 
 While the ideas presented here are familiar from the BRT work the contribution is still significant and this paper should be accepted.