The paper presents an approach to interactive reinforcement learning there human feddback is interpreted directly as policy labels. 
The paper is clearly written and easy to understand. The method is sound and based (to my understanding) well on the existing literature. 
In my opinion the papers strongest point is that the method presented (named Advise) is simple, needs less meta parameters than state of the art methods and this single meta parameter C (that depicts the estimated consistency of the human feedback) is also not very sensitiv. In combination with the results that show that Advise performs better or equal to the state of the art approaches, Advise seems to me to be an very interesting method. 
But the paper has also some weaknesses, especially for a NIPS submission: 
The examples that were used as benchmarks seem too easy. 
Also the theoretical delta of the method to the state of the art is not very large. 
Because the idea is interesting and the method itself is compelling I still tend, however, slightly to suggesting acceptance of the paper. 
There are also some minor points: 
Page 1, line 32 or 33 (the numbering is a bit off in the PDF): "In this paper WE introduce..." 
Page 2, line 75 or 76: "This is THE most common..." 
Page 5, Table 1: This table is in my opinion too small. 
Pages 6-8, Figures 2-5: This figures are definitively too small (at least in the printout). I know its hard to meet the page-limit in NIPS, but the ticks are not readable and the plots themselves are too close on top of each other. 
Page 7, Line 373 or 374: "interpret feedback is as a direction" - please rephrase. The paper presents an interesting method for interactive reinforcement learning that is simpler, with less meta parameters by showing equal or better performance than state of the art methods.It lacks however involved theoretical innovation and demonstrates the performance only on two simple benchmarks.