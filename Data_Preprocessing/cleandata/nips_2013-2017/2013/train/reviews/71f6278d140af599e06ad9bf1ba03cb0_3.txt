This paper presents a mathematical analysis of the dropout procedure in deep neural networks. As far as I know, this is the first attempt to prove the some what heuristically used dropout procedure. There have been some suggestions in prior work (at least for the shallow case) that dropout performs some form of an averaging (geometric to be precise). But this is the first attempt to prove this property for deep neural networks and show that the normalized version of the weighted geometric version provides better approximations than the traditional geometric average. In particular, the three equations 11, 12, 13 are important contributions of this paper and will have a greater impact on future work on deep neural networks, especially in their theoretical analysis. 
Minor comments 
----------------------- 
Eq. 7: factor c should appear before exp term in one before the last term. Missing closing bracket at the end of the expression. 
Extract parenthesis in Eq 25 after the differential 
Perhaps lambdai in Eq. 26 is actually deltai? 
It wasn't clear to me the claim about p_i = 0.5 giving the highest level of regularization. Authors could clarify this point in bit more detail in the paper because it is an important observation that justifies the current heuristic in the dropout. 
Figure 3 (page 8) appears too small. You can organize the figures in page 8 in a better way and avoid the space between the two figures. This paper presents a mathematical analysis of the dropout procedure in deep neural networks. As far as I know, this is the first attempt to prove the some what heuristically used dropout procedure. There have been some suggestions in prior work (at least for the shallow case) that dropout performs some form of an averaging (geometric to be precise). But this is the first attempt to prove this property for deep neural networks and show that the normalized version of the weighted geometric version provides better approximations than the traditional geometric average. In particular, the three equations 11, 12, 13 are important contributions of this paper and will have a greater impact on future work on deep neural networks, especially in their theoretical analysis.