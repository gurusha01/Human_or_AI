Well written paper with good spelling, grammar, correct citations, etc. Well above average here. Though try not to begin a sentence with a citation in the NIPS style. Theory is succinct in parts, for instance using f() and g() throughout the paper. 
The paper looks at stochastic block models (SBM) and extends the simple single table instance of an IRM by adding the theory of Bayesian Rose trees. This allows the CRP formulation of IRM to be replaced by the simpler eq (7). The factoring over subtrees follows the work on learning Bayesian network structures, e.g., Koivisto and Sood JMLR 2004 section 3, where the posterior factorises neatly based on the class of constrainted structures chosen and thus greedy search, marginal likelihoods, handling of sufficient statistics, link prediction, etc., can all be neatly done. 
Evaluation for SBM is often done against "gold standard" data where the partition structure is known. Scores like NMI and other clustering metrics can be used for this where you compare against the known clusterings. You report using AUC and I had to think for a while how this could be done, since am familar with NMI evaluations for SBM. I presume you delete a few links and then guess if they should be added back. Is this right? Anyway, badly needs an explanation. Recent evaluations of SBM run on bigger data sets, bigger than your full NIPS, so the small evaluation shown in Figure 4 is good but should be presented for a larger dataset too. Its not good enough just to compare against Bayesian non-parametric methods as there are many published semi-parametric methods, some you mention. 
The contribution of this algorithm is the agglomerative method and Table 1 sums it up well. 
 The paper extends the simple single table instance of an IRM by adding the theory of Bayesian Rose trees. Good clear theory and algorithms, though a simple concept and algorithms. The experimental work is illustrative rather than comparative so not really adequate.