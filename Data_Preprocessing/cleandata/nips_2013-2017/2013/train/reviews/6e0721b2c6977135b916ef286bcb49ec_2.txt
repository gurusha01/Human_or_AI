The paper introduces a new sampling method for sampling the entries of large matrices. Unlike deterministic approaches, which only select matrix entries that exceed a given threshold, the authors propose a probabilistic approach for selecting entries of a matrix that are candidates for storage and further analysis. From that perspective, the authors are looking for a matrix B that approximates an input matrix A in an optimal way, where optimality is measured in terms of the loss ||B-A||. 
The approach of the authors is supported by precise mathematical arguments, but I was not convinced by this paper. I found the introduction utterly confusing. For example, the simple formula in (1) does not lead to a probability, in the sense that it yields a score between zero and one. Maybe I am totally confused, but I also did not get why Bij should be in the set (-thetai, 0, thetai). I guess it should be in the set (-1/thetai, 0, 1/theta_i). Moreover, I also did not get why B should be an unbiased estimator of A. The same small mathematical inconsistencies continue to appear in later sections. For example, in Theorem 1.1, the expected value of a matrix is zero. This should probably be the n x m zero matrix? 
Most importantly, the motivation for this work was not convincing to me. Why should the machine learning community care about sampling models for matrix data? The authors mention the "streaming model" motivation, where one must decide for every entry whether to keep it or not upon presentation. In such a scenario, probably a deterministic solution based on retaining entries that exceed a threshold would suffice. From that perspective, the last paragraph of Section 3 could not convince me that such a deterministic approach is less useful. I would like to read at least one concrete application where one would need sampling models for matrix data. Probably, such applications exist, but then they should be mentioned in a paper of this kind. 
 Interesting problem, but a better justification for this work is needed.