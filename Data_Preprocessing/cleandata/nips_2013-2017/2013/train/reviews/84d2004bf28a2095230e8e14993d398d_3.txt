This paper introduces the GreeDi algorithm to maximize monotone submodular functions subject to a cardinality constraint in a distributed system environment, in particular mapreduce. The authors experiment against an exhaustive array of datasets and also prove that the distributing of work across many machines maintains the objective to within a reasonable bound of a centralized algorithm. 
The function optimized must be "decomposable" i.e. composed of the sum of many submodular functions, so that the function does not depend on the entire dataset. 
I would like to see the following questions explicitly answered: 
1) Exactly How much communication is required between the mappers and reducers in the mapreduce implementation? i.e. how much data needs to be shuffled? this is the communication cost in this setup. 
2) Exactly how many items could be reduced to a single key? This measures how overloaded a single machine may become. 
3) How many iterations needed in the worst case? This paper introduces the GreeDi algorithm to maximize monotone submodular functions subject to a cardinality constraint in a distributed system environment, in particular mapreduce. The authors experiment against an exhaustive array of datasets and also prove that the distributing of work across many machines maintains the objective to within a reasonable bound of a centralized algorithm.