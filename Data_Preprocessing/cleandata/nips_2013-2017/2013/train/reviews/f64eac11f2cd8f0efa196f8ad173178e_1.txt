This paper provides a theoretically grounded adaptive step size for policy gradient methods in which the lower bound of the expected policy improvement is maximized. Approximated version of step size can be computed only by estimated gradient and thus the computational complexity would be same as ordinary policy gradient methods. There is a mild assumption that the standard deviation of Gaussian policy is fixed for deriving the lower bound. Experimental results show that the proposed method works well when the fixed SD is large enough. 
Quality, clarity and originality: 
The motivation of the paper is clearly described and its organization is reasonable. The paper provides potentially useful theoretical analyses such as Theorem 4.3 and Corollary 4.4 although I cannot check the derivation of lemmas and theorems in detail. The originality is definitely high. 
Significance: 
Newton method would be a standard approach for adaptive step-size but the second-order derivative of expected return with Gaussian policy is not guaranteed to be semi-positive definite. Thus, adaptive step-size based on the maximization of lower-bound would be a good alternative of ordinary policy gradient methods. 
However, as the author(s) review in the introduction, there are several extensions of policy gradients for stable policy update, such as EM policy search. Thus, it would be much more convincing if there are more experimental comparisons with these methods. 
Small comments and questions: 
* The section of conclusion should be added. 
* Is it really possible to converge to the optimal policy with only one update for \sigma=5 (in Table 1)? The paper provides potentially useful theoretical analyses and good alternative of policy gradient methods. It would be much more convincing if there are more experimental comparisons with state-of-art methods such as EM policy search.