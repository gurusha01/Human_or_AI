The paper develops new theories that use side information to reduce sample complexity, and demonstrates the approach's effectiveness in transductive multi-label learning. 
For the proof, it is assumed that both A and B are orthonormal matrices (top paragraph in page 4). It would be better to clarify how the results will change when A and B are not orthogonal. Also since U and V are linear combinations of column vectors in A and B, the proof may be simplified. 
The unconstrained optimization problem (3) is very similar to the problem solved in [1]. Actually, if we index the set of observation \Omega from 1 to k such that $\Omegak = (ik, jk)$, and let $Ck = A{ik, .} B{jk, .}^T$. The problem (3) becomes $minZ \sumk \|Mk - tr( Z Ck)\|^2+ \lambda |Z|_{tr}$ which is exactly the problem solved in [1]. Similarly, [1] also provides consistency results and an efficient algorithm. It would be useful to compare to the algorithm for multi-label learning experiment. 
In the end of second paragraph, it is said that recent efforts try to address the issue but at a price of losing performance guarantee. It's not clear as paper [5] cited in the original paper gives convergence guarantee. Actually there are several papers that provide efficient algorithms that don't require updating full matrix and also have convergence guarantees such as paper [2] below. 
In proof of lemma 1 in supplementary material, there is a typo in line 063 that it should be $(1 - ||P_{T^{\perp}||)$ where the $\perp$ is missed. 
[1] Francis R. Bach, Consistency of Trace Norm Minimization, JMLR 2008. 
[2] M. Jaggi, M. Sulovsky. A Simple Algorithm for Nuclear Norm Regularized Problems, ICML 2010 
 The authors provide new theoretical and empirical results of sample complexity for matrix completion. The results should be compared to Bach(2008) that analyzed a very similar problem.