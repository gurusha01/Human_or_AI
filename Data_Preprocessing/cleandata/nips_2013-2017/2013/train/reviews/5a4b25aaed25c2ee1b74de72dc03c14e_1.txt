Aims to improve the mixing rate of Gibbs sampling in pairwise Ising models with strong interactions, which are known to be "slow-mixing". Several projections to "fast-mixing" models are proposed; essentially, parameters are identified which are as close as possible to the original model, but weak enough to satisfy a spectral bound establishing rapid mixing. Experiments show some regimes where this leads to improved marginal estimates for a given computation time, compared to variational methods (mean field and belief propagation variants) and Gibbs sampling in the original model. 
The technical content builds heavily on prior results establishing conditions for rapid mixing [4,8]. But I haven't seen the idea of projecting onto the nearest rapidly mixing model, as an approximate inference method, explored before. 
On the domain of "toy tiny Ising models", results offer some nice improvements over a good set of baselines (standard sampling and contemporary variational methods). My main concern is that I think there should be more than the usual amount of skepticism as to whether these results will scale to larger models. Experiments involve comparisons of mixing times of samplers on different models, and it is hard to judge how these will scale with problem size. Also there is a (from all appearances) computationally expensive projection step required to build the fast-mixing model, the cost of which seems not to be accounted for. Finally, it is not at all clear that generalization beyond binary states will be possible, since establishing convergence bounds more generally is far more challenging. 
CLARITY: 
In general the presentation is clear and reasonably accessible, given the technical content. But there are some places where reorganization and clarification is needed: 
* The paper refers several times to "standard results" and even states these in the form of a theorem (e.g. Theorem 6) without reference or proof sketch. Lemma 5 is left unproven, the reference proves for a special case of zero-field. 
* Discuss properties of the dependency matrix R. For instance, it does not appear to be tractable due to the maximization, please state whether this is the case. (I presume this is why lemma 5 is invoked.) 
* The second half of Sec. 4 is nearly impossible to follow. Before Theorem 7 the text references g, M, and Lambda before they are introduced. Then the statement of Theorem 4 includes notation that is not really explained. If this optimization is going to be discussed, more explanation is needed. (Perhaps less time could be spent restating results from [8] which are not really used.) 
* In Sec. 1, KL(q||p) is used for both directions of KL divergence when the notation needs to be shifted for one. In the first paragraph, q is used for the true distribution and p for the tractable approximation; this is the opposite of almost all related literature. 
EXPERIMENTS: 
It appears that the time comparison in Figure 2 does not include the computation required for projection. A somewhat ambiguous statement to this effect appears in Sec 6.1 but is unclear; please clarify and if it is the case, show results with projection time included. As it stands, the proposed methods essentially get to use the output of a sophisticated variational optimization without penalty, which certainly makes the improvement over standard Gibbs less convincing. 
It was disappointing not to see experiments on larger models, given that Gibbs mixing times often depend on problem size. There are certainly options for running experiments in regimes where junction tree is intractable, like using a model where symmetries let the true marginals be computed, or taking the output of a very long sampling run as truth. 
I understand that KL(\theta||\psi) is intractable in general, but it would still be interesting to explore here as a potential "best case" for how sampling in an approximate model would perform. (Junction tree could be used for the toy models used in the submission.) 
Mean field is a degenerate case of the reverse KL projection, as the paper points out, yet there is a large difference between mean field error and the error from reverse KL projection. This deserves discussion. 
 The idea of approximating slow-mixing models by projecting to the closest fast-mixing model is a nice one, and recent work on mixing bounds is leveraged in an elegant way. But there are some concerns about experimental comparisons, and the limited range of models to which this approach is potentially applicable.