Variance Reduction for Stochastic Gradient Optimization 
NOTE: Due to the number of papers to review and the short reviewing time, to maintain consistency I did not seek to verify the correctness of the results. As a result, I generally ignored any proofs or additional details in the supplementary material. This review should be considered as a review of the main paper alone. 
This paper proposes a trick using control variates to help reduce the variance of the gradient step in stochastic gradient descent procedures. The idea is to use a structured correlation to reduce the variance, which is different than the averaging approach used in minibatching. The intuition is clear and the paper is generally well written. I believe that the contribution is somewhat novel (although the concept is not that new) and the audience at NIPS will appreciate it. 
STRENGTHS: 
- develops a trick to help improve the performance of SGD that is different from minibatching and applies it to specific problem 
- the experiments show that this trick works, so this is an important tool to add to the arsenal 
WEAKNESSES: 
- it's hard to tell how one can construct control variates for other problems 
COMMENTS: This is a rather nice implementation-driven paper which tries to address one of the big problems with SGD -- it is great on paper and rather finicky in practice. I found the presentation convincing, but some of the claims feel a bit overstated to me. 
One issue is the concept of novelty. They do point out that control variates have been used elsewhere, but they are missing a reference to a recent work by Paisely et al (ICML 2012) which also uses control variates for variance reduction in gradient estimation and applies it to HDPs... while it's hard to keep on top of all of the literature, the novelty with respect to this previous approach is unclear. 
Because many of the experimental details seemed a bit arbitrary (e.g. mini batch size of 100), the comparisons between this approach and minibatching felt overstated. The approach of "Algorithm A does better than Algorithm B on theses datasets" doesn't tell me why, especially, when it was just one run with a fixed set of learning rates. Will hand-tuning the learning rates help? Who knows? I think the authors should instead focus on exploring how much this trick can help and when/where there may be diminishing returns or interesting practical tradeoffs to explore. 
"It can be shown that, this simpler surrogate to the Aâˆ— due to Eq. 6 still leads to a better convergence rate." -- for this and other comments I would prefer that there be explicit references to the supplementary material (if a proof exists) or omitted (if it does not). 
Does this work with proper gradients only, or can it be applied to subgradients as well? 
TYPOS/SMALL ITEMS: 
058: "discussion on" 
before (4): be clear that $h$ can depend on $g$ here. 
ADDITIONAL COMMENTS AFTER THE REBUTTAL: 
* With regards to novelty, it's more about tone than substance -- the idea of using control variates to help reduce variance is not new (as the authors note). I agree that the current application is sufficiently different than the ICML paper referenced above. 
* Since I didn't have as extensive comments, I am happy with the response and am modifying my score. 
 This paper proposes a trick using control variates to help reduce the variance of the gradient step in stochastic gradient descent procedures. I believe that the contribution is novel (although the approach is known in the control literature) and the audience at NIPS should appreciate it.