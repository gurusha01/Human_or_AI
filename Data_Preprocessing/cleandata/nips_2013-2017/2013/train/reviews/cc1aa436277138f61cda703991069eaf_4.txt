This paper considers the recently popular problem of aggregating low-quality answers collected from crowds to obtain more accurate results. 
The central question here is how many control examples (whose ground truth labels are known) are required to obtain the most accurate results. 
With a simple Gaussian model with worker ability parameters, the authors evaluate expected errors for two estimation strategies: two-stage estimation and joint estimation, from which the optimal numbers of control items are derived. 
Although I found no apparent flaw in the analysis and the experiments support the claims as far as several assumptions hold, the main concern is the assumption of uniform task assignments to workers. 
In most crowdsourcing situations, the assumption is not so realistic; some workers complete many tasks, but most workers do only a few. 
Whether or not the proposed method is robust to such situations is not evaluated in the experiments since all of the datasets used in the experiments follow the assumption. 
It would be nice if extension to discrete values were discussed. 
Also, the authors should mention several existing work incorporating control items into statistical quality control such as Tang&Lease(CIR11) and Kajino&Kashima(HCOMP12), 
 The problem is interesting, but the assumption of random task assignments might limit the applicability of the proposed method.