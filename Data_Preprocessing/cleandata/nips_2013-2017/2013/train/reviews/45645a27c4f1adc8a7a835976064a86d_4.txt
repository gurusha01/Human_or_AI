In this paper, the authors extend factorized asymptotic Bayesian (FAB) inference for latent feature models. FAB is a recently-developed model selection method with really good results for mixture models (MMs) and hidden Markov models. In short, this method maximizes a lower bound of a factorized information criterion (FIC) which converges to marginal log-likelihood. The limitation of the FAB is that it has only been applicable to models satisfying the condition that the Hessian matrix of a complete likelihood should be block diagonal. The authors extend FAB to latent feature models (LFMs) despite the fact the condition is not satisfied. They effectively derive a lower bound of FIC for LFMS and show that it has the same representation ad the FIC for MMs. They provide results on both synthetic and real world datasets and compare FAB/LFM to other methods such as fast GIbbs sampling in models that use Indian Bufffet process, models that use variational Bayes (VB) and maximum-expectation IBP (MEIBP). The results illustrate the superiority of FAB?LFM; the proposed method claimed better performance not only in the prediction task but also in terms of computational cost. 
The paper is well written and the authors thoroughly describe the derivation steps of their method. 
The proposed model extends FAB/MMs to FAB/LFMs and would consider it incremental. It is quite interesting and the results underline it's efficiency. Although I am not an expert on this, I think that it would be of great interest to the NIPS community.  All in all, it is a nice paper that presents a really interesting idea. The results are convincing and I support its acceptance.