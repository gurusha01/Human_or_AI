The authors propose to apply particle MCMC to perform inference in Gaussian process state-space models. In particular, they focus on the recent ancestral sampling particle Gibbs algorithm of Lindsten et al. The paper is clear and it is an interesting and original application of particle MCMC. There are also some useful model-specific methodology developed in the paper, namely sparse GP-SSM. 
One thing I find truly regrettable is the lack of comparisons to other particle MCMC schemes, in particular the particle marginal MH (PMMH) scheme and the particle Gibbs with backward sampling (as in Whiteley et al.). They could have been straightforwardly implemented and it would be of interest to know how those variants compared to the proposed scheme (and it would not be much work for the authors either). 
Additionally I would like to see graphs displaying the performance of the algorithms (e.g. in terms of ACF or ESS) as a function of N and T. As they stand the results are not very informative. Do I need to scale N linearly with T, sublinearly? 
I believe that for such models the PMMH would require a number of particles increasing quadratically with T as observed in Whiteley et al. whereas both particle Gibbs require a number of particle growing sublinearly with T. 
 A well-written application of particle MCMC to GP state-space models. The paper could be significantly improved if the proposed algorithm was compared to the PMMH and the particle Gibbs with backward sampling.