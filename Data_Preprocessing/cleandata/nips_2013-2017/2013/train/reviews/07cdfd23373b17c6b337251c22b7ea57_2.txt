This paper proposes a technique for learning latent structure in social network data. The model is fit to triangle count statistics obtained from a preprocessing step, analogous to a bag-of-words representation for text documents. The main modeling advance is motivated by aMMSB: rather than parameterize all between-"community" interactions they consider the configurations of the triangles to identify a set of K roles to parameterize. This results in fewer parameters in the model and, in turn, a more scalable implementation. They apply recent advances stochastic variational inference -- deriving a global update and local updates for the proposed model. 
The novel modeling contribution is a straightforward combination of the aMMSB with the MMTM, though somewhat tedious bookkeeping was required. The presentation of this material could be improved. 
Though most of the learning algorithm is a direct application of stochastic variational inference, the authors provide an intuitive and intriguing approximation for the local update rule. 
Given the prevalence of directed networks, a detailed discussion of how these methods might extend would be welcome. 
Ongoing concerns: 
- "our Parsimonious Triangular Model (PTM) is essentially a latent space counterpart to ERGMs, that represents the processes of generating triangular motifs". This seems like a curious statement: 1) ERGMs can include much more varied statistics than just triangles and 2) I don't see how the PTM is a "latent space counterpart", unless all latent space network models are "latent space counterparts". 
- Are the inferred latent space interpretable? If so, what do we learn about the similarities and differences among these graphs? 
The difference between Delta^1 and Delta^2 is not clear (at least to this reader). 
- It is not made clear (in this paper) why subsampling the triangles for each node (using delta) provides an adequate approximation. 
- If the goal is to recover the "set of non-zero roles" then perhaps there should be more of a binary notion of "role" in the model. 
- How robust are the latent space recovery results to the threshold of .125 (which seems somewhat arbitrary)? 
- A variety of latent space models can produce power-law networks. Which one did you use for the experiments? 
- Why is PTM CGS have K^3 complexity? Didn't you argue that there are O(K) parameters rather than O(K^3)? 
- One concern is how this will perform when fewer edges are observed. In the experiments they only hold out 10% of the edges, presumably because the distribution of adjacent triangles will be quite biased if too many edges are removed prior to the preprocessing step. 
- It is counterintuitive that the method beats MMSB on a synthetic network generated via MMSB (Figure 1, left 4 panels). 
 This work uses stochastic variational inference to learn a mixed-membership blockmodel (with a restricted set of parameters) on the triangle counts in a graph. Their modeling choices and approximations allow for a more scalable O(NK) algorithm.