The paper describes a bounded-staleness design for a parameter server for iterative distributed learning based on vector clocks. The idea is clear, intuitive and provides a nice parameterized range between the previous designed for pure asynchronous updates (e.g., Hogwild or Yahoo LDA) and approaches that have a hard synchronization barrier (e.g., most recent SGD papers). While there are no breaking theoretical insights, the paper correctly adapts the analysis from [17]. THe paper is very well-written, and the SSPtable design description is exemplary. 
The experimental results are convincing, with computation-vs-communication and clocks/worker/time results particularly encouraging . A few areas for improvement/details that would help: 
- Lasso results are painfully limited to a toy synthetic dataset -- a more comprehensive evaluation on it would be quite useful. 
- A direct comparison with state-of-the-art distributed learning packages (Yahoo LDA, Hogwild) would be illustrative. 
- There should be a single-worker baseline. 
- How exactly is data partitioned? This should be mentioned. 
A couple suggestions: 
- For matrix factorization, could the idea be combined with Gemulla et al's approach for sub-epochs that operate on non-overlapping parameter blocks sequentially? 
- It would be helpful to discuss the possibility of forcing the slowest stragglers to sync with an incomplete iteration if that prevents blocking for current staleness level. 
 The paper is well-written, describes an intuitive idea, and provides convincing experimental results with some relatively minor areas for improvement.