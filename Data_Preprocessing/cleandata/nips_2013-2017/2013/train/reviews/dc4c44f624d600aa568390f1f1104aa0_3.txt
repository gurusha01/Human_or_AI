The authors propose a generalized method of moments (GMM) approach for computing parameters of the Plackett-Luce model. The method first 'breaks' the complete rankings into pairwise comparisons which are then used to estimate the necessary quantities at each iteration of the algorithm. 
I find the proposed approach to be interesting, I especially like the analysis of the relationship between the breaking type and consistency/quality of found solutions which leads to a trade-off between time and efficiency. 
The major drawback of the GMM methods is its applicability. All results in the paper hold for full rankings only while in practice most preference aggregation problems typically have partial rankings/preferences. The authors mention extension to the partial case in future work but I think having at least some results on that would significantly increase the impact of the paper. 
I also have some comments regarding the experiments which are summarized below. 
-The real data experiment is not very convincing. It seems strange to compare models on a dataset where neither model provides a good fit, especially since solution from one of these models is used as ground truth. Why was this data chosen? 
-I would want to see a comparison with a gradient descent procedure on the PL log likelihood. In my experience gradient descent works well on PL models especially in settings where relative order of \gamma's is more important than the absolute magnitude (i.e. Kendall correlation criteria). Careful implementation should also be more efficient than GMM per iteration and require less storage. Do you have any results on this? I find the proposed approach to be interesting and promising but in the current state it will have limited impact on the relevant research area.