The paper analyses two optimization objectives for active learning on GRFs - Sigma-Optimality and a V-Objective (that was proposed before, but not analyzed). The authors show that the maximization versions of both the objectives are submodular and hence obtain approximation guarantees on the performance of greedy algorithms. Furthermore, they show that the covariance matrix for GRFs satisfy the Suppressor-free condition. 
The paper has some nice theoretical insights, the most interesting of which being that the covariance matrix of GRFs is suppressor-free. The notion of how suppressors play an adverse role in subset selection, and how the absence of suppressors enables good performance guarantees for greedy algorithms has been studied previously, but it was not known whether there was a general class of covariance matrices that satisfied this condition. The authors' result that GRFs do satisfy this condition is quite interesting, both theoretically and practically. 
It was not clear how novel the Sigma-Optimality criteria is, given that it seems to have been proposed before in an Active Surveying context. But I liked the fact that the authors analyzed this optimization problem rigorously, and provide approximation guarantees using submodularity. 
On the other hand, I think the paper could improve in its writing - there were a few places where it was technically imprecise (though these do not affect correctness of their analysis) 
-I found it annoying that the paper was imprecise in distinguishing between minimization and maximization versions of the problem, and kept switching back and forth. The greedy multiplicative performance guarantees and submodularity of the objective are only for the maximization objective, whereas the paper seemed to suggest several times that it worked for the minimization objective (eg. beginning of section 2.3). 
-In the first para of page 4, calculating the global optimum is not intractable because of submodularity, and a greedy-solution is not "required". 
-In the last para of page 4, the suppressor-free property does not "prove" 2.3 (2.3 is just a definition) 
-The proper citation for the greedy bound for submodular-maximization is [Nemhauser et al 1978], not Streeter and Golovin. 
Regarding the experiments section, it is very surprising that the Mutual-Information criteria performs even worse than Random. 
 The authors analyze two active-learning criteria for GRFs and show performance bounds of greedy algorithms using submodularity of the objectives. They also show that GRFs obey the supressor-free condition. Overall it is a nice paper, and has useful theoretical contributions - though it should fix the imprecise notation/sentences mentioned above.