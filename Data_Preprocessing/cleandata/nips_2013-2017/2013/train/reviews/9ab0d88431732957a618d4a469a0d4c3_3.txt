Multiple model learning is a generalization of clustering, and in this framework cluster centers can be learning models. 
For each learning model, weighted data are assigned, and the weights averaged over learning models are restricted uniform weights with l2 regularization. 
Thanks to this regularization term, a simple alternating optimization algorithm is derived. 
Also some theoretical bounds of performance are obtained, which support robustness to outliers. 
The motivation is clearly stated. 
Theoretical analysis looks mathematically sound. 
(some notations such as P_delta is defined in appendix, 
but not defined in the main body. therefore the authors should carefully check the main manuscript is self-contained.) 
Each set of weighted data can be related with (empirical) distribution, so it might be nice to discuss the properties of regularization from the viewpoint of probabilistic mixture models, not only from optimization perspective. 
 Based on l2 regularization of average weights of data, a new method of multiple model learning is proposed.Numerical experiments support its efficiency and good performance.