Success of inference by Gibbs sampling in MRF (here, only with two 
labels, ie, ising model) depends strongly on the mixing rate of the 
underlying Monte Carlo Markov chain. The paper suggests the following 
approach to inference: 
1) Project the input model (which possibly does not mix fast) on the set 
of models that do mix fast. 
2) Do inference on the obtained model that mixes fast. 
The space of fast-mixing models is defined by bounding the spectral 
norm of the matrix of absolute values of Ising edge strengths. 
"Projection" is defined by divergences of Gibbs distribution. It is 
forced to preserve the graph structure. Projection in Euclid distance 
is obtained by dualizing the initial task and using LBFGS-B. For 
other divergences (KL, piecewise KL, and reversed KL divergences are 
implemented), projected gradient algorithm is used. In reversed KL 
divergence, Gibbs sampling (but on a fast mixing model) must be done 
to compute the projection. 
Extensive experiments on small random models are presented compare the 
approximated marginals with the true marginals. The methods tested are 
the proposed one (with all the above divergences) and loopy BP, TRW, 
MF and Gibbs sampling on the original model. Not only accuracy but also 
runtime-vs-accuracy evaluation is done. The experiments show that the 
proposed methods consistently outperform TRW, MF and LBP in accuracy, 
and for reasonable range of runtimes also Gibbs sampling on the 
original model. Of the proposed methods, the one with reversed KL 
divergence performs consistently best. 
Comments: 
The projected gradient algorithm from section 5.1 n fact has two 
nested loops, the inner loop being LBFGS-B. Pls give details on when 
the inner iterations are stopped. 
It is not clear what the horizontal axis in the plots in Figure 2 (and 
the supplement) means. It is titled "number of samples" but sampling 
is used only for reversed KL divergence. I believe the horizontal 
axis should be runtime of the algorithm. Similarly, why not to report 
also runtime of LBP, TRW and MF. This would ensure fair 
accuracy-runtime comparison of all tested algorithms. Please, clarify 
this issue - without that it is hard to interpret the experiments. Give absolute running time in seconds. 
Please consider experimental comparison with larger models. An interesting option is to use models from the paper 
[Boris Flach: A class of random fields on complete graphs with tractable partition function, 
to appear in TPAMI, available online] 
which allow polynomial inference. 
222: replace "onto the tree" with "onto graph T" 
226: Shouldn't we ssy "subgradient" rather than "derivative"? Interesting paper, convincing empirical results. Practical utility can be limited though due to high runtime (this needs clarification in rebuttal).