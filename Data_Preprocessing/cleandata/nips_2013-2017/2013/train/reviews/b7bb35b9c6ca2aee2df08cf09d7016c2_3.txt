The paper proposes Stale Synchronous Parallel(SSP) computation for machine learning. It also discusses an implementation thereof and, most importantly, provides the first discussion of its relationship to machine learning quantities. The latter analysis is done for SGD. Comprehensive experiments show the empirical validity of the approach. 
The paper is very well written and easy to follow. It has been a pleasure to read and review. Thanks for that! 
While my overall verdict is to accept the paper, I'd like point out a few things that might improve the clarity of exposition and the embedding into the related work. To my mind, the main contribution of the work is not to suggest to use SSP for machine learning. That (arguably somewhat obvious) connection has been made before, if only amongst practitioners in the field and not in the academic discourse. 
The key contribution in my mind is to (a) have done it with rigor and (b), more importantly, to provide analysis that connects systems level parameters like stale-ness with machine learning concepts such as convergence of the learner. If at all possible, more space should be devoted to that analysis, as there is a suggestion in the paper that space was a consideration when leaving examples beyond SGD out. Please add them to the paper or to the appendix. 
Lastly, some nitpicking: The statement that SSPtable is more general than Hadoop on page 7, Line 376 is a bit naive in the following sense: SSPtable is optimized for small incremental updates, while Hadoop can deal with massive objects being aggregated. Also, Hadoop has a (rather good) shuffle phase, while SSPtable doesn't need one. I'd thus suggest to weaken that statement a bit as it does stand out a bit in an otherwise clean paper. Excellent paper that makes the connection between SSP and machine learning. The presentation and content are great, an easy accept.