Summary: 
The authors propose an algorithm/approximation method for efficient model selection for latent feature models by using factorized asymptotic bayesian (FAB) inference and the factorized information criterion (FIC). FAB and FIC are well known for mixture models, where the latter approximates a model's log likelihood via factorization and Laplace approximation for tractable model selection. The contribution is the generalization of FAB to latent feature models, which uses a mean field approximation for inference of the latents and accelerated shrinkage of the global number of features selected. 
A feature of the approach is more automaticity of the model selection, with little hand tweaking. Results are shown on synthetic and real data, and the gain in computational efficiency is significant. 
Quality: 
Presented results involve thorough analysis and the method is evaluated in comparison to a wide range of competing methods on multiple data sets. References are sufficient. 
Clarity: 
The paper is clearly written and well structured. 
Originality/Significance: 
Seems to be a logical extension and solid generalization of previous work in the area of model selection in LFMs using FAB inference and FIC. Namely, the using a combination of several approximations into one novel method that works well. 
It would be nice to understand when/how these approximations may lead to poor performance, i.e. "push" the method until it breaks, given that there are several approximations (FIC, mean field, shrinkage) playing a roll at the same time. The authors propose an algorithm/approximation method for efficient model selection for latent feature models by using factorized asymptotic bayesian (FAB) inference and the factorized information criterion (FIC). Results are shown on synthetic and real data, and the gain in computational efficiency is significant.