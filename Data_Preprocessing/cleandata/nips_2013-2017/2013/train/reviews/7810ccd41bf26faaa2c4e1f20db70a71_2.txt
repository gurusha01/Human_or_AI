The authors suggest the use of a criterion, Î£-Optimality, for active 
learning in Gauss-Markov random fields. The criterion itself was 
originally proposed by Garnett et al for active surveying, but it does 
not appear that the submodular property was recognized in that 
previous work. 
Labeled and unlabeled are embedded in a graph nodes represent both 
labeled and unlabled data and edge weights, computed via a kernel, 
capture similarity. The motivation for an active approach is that 
acquiring labels on the full data set may incur some cost (presumably 
greater than computing the edge weights over all data) so a criterion 
is used to determine which of the remaining unlabeled data should be 
labeled. The authors establish that the criterion satifies the 
submodular monotone property and as such greedy selection achieve 
(1-1/e) performance relative to optimal selection. The authors should 
be careful to note that this optimality is with respect to the 
criterion itself and not with respect to classification 
accuracy. While the empirical results do give good classification 
performance, the criterion itself is only a surrogate. 
Several existing criterion are mentioned in the introduction (Settles, 
Krause et al, Ji and Han) to which the authors compare their 
criterion. One item which is only given limited discusion is the 
computational complexity of computing the reward. For example, both 
V-optimality and Sigma-optimality require computing the inverse of the 
graph Laplacian over the remaining unlabeled data. Some of the other 
criterion have lower complexity. This is an important issue since the 
problem is cast in terms of costs and since performance of all 
criterion will eventually approach each other (since they are 
presumably solving the same classification problem following 
selection) a fairer comparison would include the cost of computing the 
reward. It may be that the proposed method wins here, but it at least 
bears some mention. 
Why do the authors cite Streeter and Golovin for results regarding 
submodular set functions? Shouldn't this be Nemhauser et al? 
Section 2.3 misstates several things. The intractability of subset 
selection is not a consequence of submodularity, it is simply a 
consequence of the combinatorics. Furthermore, this does not mean 
that a greedy solution is "required". Other strategies may have lower 
complexity and outperform the greedy selection. It is merely, that by 
establishing the property, greedy selection has certain guarantees 
relative to the optimal selection. 
The fact that the criterion of interest was originally proposed by 
Garnett et al in 2012 should be mentioned much earlier. This changes 
the abstract from "We propos a new criterion" to something more 
accurate, such as "We analyze a previously suggested criterion and 
demonstrate its utility...". 
Establishing the sumodular property as well as the suppressor free 
properties are interesting. 
Empirical results are suffcient. 
Modulo the comments above, the paper is fairly clear. The results are 
interesting and the analysis, though limited, represents a 
contribution. 
minor comments: 
Figure 3 lacks a horizontal axis. I assume it is probability of 
correct classification, but the authors give an ambiguous description 
in the caption. 
 The authors establish that a previously proposed criterion is a montonoe submodular function and as such greedy selection achieves performance guarantees relative to optimal selection for an acitve learning problem. Experimental results demonstrate superior performance as compared to previously proposed criterion.