Paper Summary: 
The paper tackles an interesting and under explored problem of tracking changes in the underlying dependency structure between variables in a stream (e.g. time based) of data points. 
The author focus on a efficient (computational/memory wise) solution for the simpler SEM models. They develop a new algorithm for this task, named LoSST. LoSST uses a standard batch learning algorithm for the underlying structure learning (PC by Spirtes et al, 2000) and tracks several statistics from the stream of data to decide whether to initiate new structure learning. The tracked statistics involve the distance from the weighted mean of each variable, the correlation matrix (eq. 1,2) and the weighted sample size. For the sample weight there is a weighting scheme for each new point based on how "surprising" the new data point is (Eq. 3,4). Each point's Mahalanobis distance (MD) is computed and the estimated p-values for the MDs by a Hotelling's T2 distribution is combined using Liptak's weighted pooling of p-values to compute an overall "significance" statistic \rho. This stat is then used in a noisy OR configuration with the previous step to decide wether to start a new structure learning. The authors compare to vanilla PC batch learning on synthetic data sets where either a change in params or a change of structure is introduced after a fixed set of datapoints, testing for different number of variables and max degree configurations. Finally, they perform a short analysis of real life data from US price index with 529 data points (every month, starting at 1967) and six variables. They show the algorithm detects some known changes/trends in the US economy while ignore short lived changes. 
Pros: 
1. The paper tackles an interesting and under explored problem. As the author point out there is a lot of room for additional development of practical solutions for these and other models, as well as matching theory and theoretical guarantees. 
2. The algorithm is novel and practical/efficient. It makes creative use of a host of different methods for this. 
3. Experiments seem adequate trying to track the effect of the various params and include an interesting real life data set. 
4. Authors give a good review about previous work on this problem. 
5. Overall well written. 
Cons: 
1. The biggest lack in the paper with respect to this interesting problem is that of theory or any guarantees under some assumptions. Convergence and diligence are like opposing forces as the authors point out (p. 4) but it seems like some form of guarantees under simplifying conditions should be plausible. At least mention as another direction for future work. 
2. Similarly, the solution suggested by the authors, LoSST, is heavy on the heuristic/hack side. That's OK and enables the authors to get a practical solution, but the decisions involved should be more readably noted. Since tracking the mean and correlation matrix is straightforward, these involve mostly decisions on reweighing, effective sample size, and when/how to re learn. The authors make creative use of some methods under conditions that probably void guarantees of these methods. For example, they estimate the MD p-value using Hotelling's T2 distribution. They set the sample size N to be the effective sample size S^r. However, that distribution was developed for complete counts from a given distribution, not for partial/re weighted counts. If they actually sampled from these nets/structures with these partial weights would the distribution follow this form? Similarly, Liptak's weighted p-values was developed for meta analysis using independent tests with different sample sizes. At least if the methods are used outside their guarantees/context the authors should clearly state that, so readers don't get the wrong impression/understanding. 
Other comments: 
- The authors make creative use of footnotes to outline proofs, probably beyond what Leslie Lamport or the conference organizers intended. Consider using supp for that and other missing details. 
- Figures are way too small, with unnecessary space between them, missing labels, missing details in the captions, legends etc. Description of the experiments is not clear enough (e.g. top paragraph p. 7 is cryptic). 
- The explanation why LoSST does worse on edge addition when only the params are changed is not satisfactory. 
- With regards to the above, it seems desirable that sparking params update and structure updates should be decoupled. 
 Really interesting direction for future research with a practical though heuristic implementation as a first stab at it.