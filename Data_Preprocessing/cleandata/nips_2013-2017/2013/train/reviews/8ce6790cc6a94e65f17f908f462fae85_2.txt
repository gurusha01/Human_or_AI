The authors put forward a new class of approaches for learning sparse Bayesian Networks. Their first method is a DP approach that integrates the Lasso as a scoring method. Since this approach requires searching over a prohibitively large space, they modify it with an A search (that still uses the Lasso scoring method). This method uses a heuristic function that is admissible and consistent (and thus is guaranteed to find the optimum and allows pruning). Their final method is an approximation of A that works by limiting the queue size. While very small queues degrade the quality of solution, they find that for moderate limits, great speed improvements are possible with little degradation of quality. 
The authors note that many other methods prune the search space in a first step and then find the best DAG in this reduced space as a separate step. This practice may exclude the optimum. Therefore, their approach seems like a good way to avoid doing this. 
The experimental section seems pretty thorough. 
This paper is well-written and well-organized. 
In Figure 3, what was varied to generate the curves? Lambda? I ask because the non-monotonicity in the Hailfinder 2 plot is a little surprising. Also, does SBN really drop below the "random guessing" line in four places? This might indicate something fishy. 
Minor comments: 
- Line 21: Missing space: "two-stageapproach" 
- Line 130: "U\in V" should be "U\subseteq V" 
- Line 140: empty citation? [] 
- Lines 157 and 175: "and and" 
- Line 169 and 171: Don't start both with "On the other hand" 
- Line 180: heuristic misspelled 
- Line 258: CLOSE should be CLOSED 
- Line 356: least square misspelled 
- Line 357: no "a" before prediction 
- Line 430: Not a sentence. This paper presents a promising set of methods for learning Bayesian Networks through an A* search with Lasso-based scoring function.