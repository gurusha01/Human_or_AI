The authors use two equalities to derive two estimators of the Radon-Nikodym derivative f= p/q: 
(i) Ep ( k(x,.) f) = E_q k(x,.) and (ii) Ep (k(x,.) f ) ~ q(x) 
The integral operators are replaced with empirical versions. {k(x,.)} can be seen as a set of test functions over which the difference between the right and left side is minimised. The choice of {k(x,.)} as the family of test functions has the advantage that standard kernel methods can be used for optimisation. 
Convergence guarantees are given for both settings under smoothness & boundedness assumptions on the densities. The rate of convergence depends crucially on the dimension of the underlying space -- as one would expect for a density estimate. 
I think it is nice and well executed work. 
A couple of comments: 
- For the convergence rates. Optimising over the set {k(x,.)} is sufficient to guarantee convergence to the ratio. For (ii) this is intuitive given the bandwidth dependence. Would be nice to have a short intuitive argument why this also works out in (i). 
-You optimise on page 2 over L2,p. Later in (9) you have a combination of L2,p and L_2,q. Some motivation for the choice of cost function would be nice. I guess ultimately these need to stem from the tasks one is interested in. So would be nice to link the cost function choice to certain important applications. 
- Speaking of applications, one example you got is importance sampling. Ie q/p is relevant for this to move from Eq to Ep. I'm wondering here: you are effectively estimating Ep and Eq to get your empirical cost function. So why not use directly the Eq you got there? Can Ep (q/p) be any better than directly Eq? I guess I'm asking here for a motivation of the setup since you need to produce estimates Ep and Eq to estimate q/p. You have a sentence on p 5 about it. That is sampling is difficult from Eq. Is this the kind of main application you have in mind? 
- The regulariser: You enforce smoothness for the ratio q/p. Later in the theorem you got smoothness assumptions on q and p ( which I would say is the natural thing). So I'm wondering if there is any sort of relation between smoothness on q and p and smoothness of the ratio? I guess an answer is here already that you have many different q and p which produce the same ratio; ie just change on countable or on uncountable many points q and p to make it extremely irregular. So it might be more p ,q smooth (possibly bounded away from 0) => ratio smooth (?) or ratio smooth => there exist a smooth p and q which produces the ratio. 
- Would be nice to discuss the d dependence of your rates -- for what dimensions would you expect your method to be useful? 
- Is there any good motivation for your family of test functions U? 
- A more principled question: For this q/p ; covariate shift etc setting one of the main difficulties seems to be to come up with a generic cost function. One approach is to go through expectation operators and throw different test functions at these to extract the underlying densities. Do you feel that you got here "the right" approach with the {k(x,.)} ? There is also some similarity to [1] in terms of playing around with transformations to get hold of the quantities of interest. Certainly, your approach for the Radon Nikodym derivative is nicer by working with test functions. But the overall approach has some similarity. 
[1] Smooth Operators; Grunewalder, Gretton, Shawe-Taylor 
 I think it is a timely and strong paper addressing a relevant problem, providing a convergence study and experimental results.