This paper considers convex norm-regularized optimization approaches for factorization of approximately low-rank tensors. It focuses on a recently proposed "latent" norm approach ([25]), and its comparison to a previously used "overlap" norm approach. Specifically, the paper derives theoretical bounds on the "latent" norm mean-squared-error behavior under several noise assumptions, which compare favorably to the existing analysis on the overlap norm, supporting its better empirically-observed performance in a previous paper. The theory is also confirmed via some numerical simulations. Finally, the paper considers a generalization of both these norms, and proves a duality theorem for it. 
The paper is generally easy to follow, and appears to be technically sound (although I haven't checked the proofs in the supplementary material carefully). The results may also be useful for researchers focusing on optimization methods for tensor decompositions. My main issues are that 
(i) The results are narrow and of a technical nature, only peripherally related to machine learning, and probably of limited potential impact at NIPS. While the authors make the (justified) case that tensor factorizations can crop up in machine learning, the paper does not attempt to examine the proposed approaches in the context of machine learning problems, either theoretically or experimentally. 
(ii) The results seem a bit incremental, and I didn't find them to be particularly surprising (admittedly this is a subjective notion). They seem to provide some additional theoretical backing to a specific recently proposed approach [25], which was already argued empirically to work well. The techniques used also don't seem particularly novel. 
A more minor defect is that the upper bounds do not come with any lower bounds. This is potentially dangerous, since the upper bounds are used to compare between different algorithms and argue for the superiority of one over the other. 
Additional Comments 
----------------- 
- The abstract begins with "We propose a new class of structured Schatten norms for tensors...". However, I think this is slightly misleading. The focus on the paper is devoted to analyzing two existing approaches (overlapped and latent). Unless I missed something, the generalization only comes up by defining it in section 2.1 and proving what is the dual norm (Lemma 1). In particular, no particular algorithms are proposed based on this generalized Schatten norms, 
the behavior of such algorithms is not analyzed, nor are they tested experimentally. 
- Figure 1 is presented in the introduction, but at this stage is not interpretable without the background appearing in section 2 onwards. 
- Line 38: "Tensor is" -> "Tensor modelling is"(?) 
- Line 40: Missing comma after "Conventionally" 
- Line 395: "also in such situations" -> "in such situations as well" The paper's main contribution are theoretical error bounds for a recently proposed low-rank tensor decomposition approach. The paper seems technically sound, but the results are somewhat incremental and may suffer from limited impact at NIPS.