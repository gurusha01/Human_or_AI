DETAILED COMMENTS: 
This work considers using control variates to reduce the variations of stochastic gradient descent algorithms, a very important problem for all stochastic approximation methods. This approach is not new to the machine learning community, see e.g. "John Paisley, David M. Blei and Michael I. Jordan, Variational Bayesian Inference with Stochastic Search, in: Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012", but the authors provide more examples for popular models such as LDA and NMF. The paper also includes some experimental results in the context of logistic regression and LDA. 
The applications of control variates in logistic regression and variational inference were presented in [Paisley'2012], although they are not exactly the same as the ones used in this paper. The authors should at least cite this paper, elaborate the novelties of this work, and compare with the choices of control variates therein. 
In the experiments for logistic regression, the comparisons between using variance reduction and standard sgd is based on the SAME fixed stepsize. This does not sound reasonable to me. A fair comparison should choose the best stepsizes separately for variance reduction and standard sgd. 
What is the additional overhead for using control variates? Since there is no computational complexity analysis, comparisons using CPU time or wall-clock time are desired. 
 This work uses control variates to reduce the variations of stochastic gradient descent algorithms. The novelty of this paper has to be clarified and choice of fixed stepsizes for both variance reduction and standard sgd in logistic regression needs justification.