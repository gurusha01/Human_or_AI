This work aims to shed light to the novel neural network algorithm of dropout, but grounding it to theory and through this provide guidance of how relevant parameters should be chosen to achieve a good performance. 
Quality: The quality of the work is high, the quality of the manuscript can be however, improved: there are several typos, or missing symbol definitions. A summary of the findings at the end would also enhance the readability of the manuscript. 
Clarity: The paper is overall clearly written subject to typos, definitions and summary missing (see above). A non-exhaustive list of suggested corrections is given below: 
-Introduction, line 7, there may be a typo/ ambiguous 
-Eq (1): l needs to be defined 
-3.1, n needs to be defined. Is stochasticity is S due to dropout? Please explain m=2^n (sems to be the number of different inputs in the network). Typo in equation 7 
-5.1 typo in eq 25, typo right after eq 28 (an extra fullstop) 
-5.2 first line: in the definition of the sigmoidal a + is missing. 
-the caption layout of the figures is non consistent 
-references in the text start from number [6] 
-A summary /conclusions section is missing 
Originality: To the best of my knowledge, the paper is original, providing answers to some key questions regarding the setup of a network with dropout. 
Significance: In my view the article is significant, and I was wondering if parallels could be drawn between the dropout algorithm, applied to the connecting weights rather than the nodes, with the release probability of biological synapses, as those may be unreliable, effectively implementing a "dropout" mechanism. This is an interesting work that helps grounding the dropout algorithm to solid mathematical foundations. The manuscript though has a few typos, and its readability can be improved.