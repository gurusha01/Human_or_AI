A convex relaxation of two-layer neural network is proposed in the 
paper. This paper is well-written. The experiments show good 
performance on the "real" datasets. But my main concern is the 
scalability of this approach. 
The approach of using SDP for convex relaxation was widely used more 
than 10 years ago, nothing new here. Though it has a nice form, the 
scalability is the major issue of this type of relaxation. Here, we 
need to optimize a problem of t^2, the square of the size of 
instances, which is probably only feasible for toy datasets. For the scalability issue, it would be better to compare the training time among algorithms. 
Algorithm 2 takes advantage of low rank of N. However, the rank of N 
is not guaranteed to be small. 
For those synthetic experiments, RBF SVM probably can achieve a quite 
good. A fair comparison would be Nystrom approximation to RBF SVM with 
random selected bases, instead of one-layer linear SVM. 
 Well-written. But SDP convex relaxation is not novel.