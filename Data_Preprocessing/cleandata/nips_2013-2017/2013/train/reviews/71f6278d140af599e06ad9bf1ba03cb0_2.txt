Summary. 
Dropout is an algorithm that was recently introduced by Krizhevsky et al. This paper introduces a general formalism for studying and understanding dropout. It provides a useful analysis of what exactly dropout does, and how it regularizes a deep network. 
Quality. 
Very good writeup. The paper first summarizes what dropout is, and how its formulation is exact for linear networks. It then goes on by demonstrating what happens (and what approximations occur) with neural networks. It concludes with simulation results which empirically confirm most of the theory introduced in the previous sections. 
The authors also describe the 3 typical learning phases that occur while using dropout, and also support these claims with simulation data. This is a quite interesting analsyis. 
Disclaimer: I haven't gone through all the math, 
Clarity. 
Very clear overall. 
Originality / Significance. 
The Dropout algorithm itself was quite original; I expect to see quite a few papers (at NIPS) trying to understand and analyze it. Nevertheless, given the impact that dropout had, it is very important to understand why and how it works, and this paper can draw attention, given its usefulness. A good paper whose aim is to understand what and how dropout works. This type of work is needed, and nicely excecuted here.