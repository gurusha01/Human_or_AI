The method consists of compressing documents for accelerating the 
document processing in other tasks, such as classification. The 
coding scheme is related to the one used in the Lempel-Ziv algorithm, 
storing pointers of substrings appearing at several locations in the 
document. The proposed approach is formulated as a combinatorial 
optimization problem, whose solution is approximated by a sequence 
of convex problems solved by ADMM. 
The experiments are carried on text classification problems, where 
compressing the documents leads to some gains in memory and 
computational efficiency, at a minor loss in terms of precision. 
I found the approach interesting, even though I am not familiar enough 
with the NLP literature to exactly judge the novelty of the approach. 
I have only a few minor remarks to make 
- the sentence ``an optimal lossless compression of D...'' requires some 
clarifiation. Is the coding scheme optimal in terms of minimum entropy? 
- it is not clear that the reweighting scheme can be interpreted here as 
a majorization-minimization procedure. Is it really the case here? 
- minimizing (4) with respect to w amounts to computing the Fenchel conjugate 
of the (weighted) l_infty-norm, which involves a projection on a weighted 
l1-norm (the dual norm of the l_infty-norm). When adding the non-negativity 
constraint, this involves a projection on the simplex. Algorithms for 
projecting on the simplex have been known for a while, and are similar to 
the approach described in the paper. See 
Brucker. An O(n) algorithm for quadratic knapsack problems. 1984. 
see also 
Bach et al. Optimization with Sparsity-Inducing Penalties. 2012, 
for the computation of Fenchel conjugates for norms. 
 The paper proposes a way to compress documents to accelerate the document processing in various tasks. It seems that the approach is novel and performs well.