Summary: 
This paper proposes an interesting greedy algorithm called SWAP for sparse recovery from correlated measurements. The proposed algorithm iteratively swaps variables until the desired loss function cannot be decreased. The implementation is quite easy to understand and implement. The paper also presents detailed theoretical analysis that guarantees exact sparse recovery with high probability. Experimental results on both synthetic and real-world data sets show the effectiveness of the proposed algorithm. 
Major Comments: 
1. The paper argues that the proposed algorithm is very effective in handling measurement matrices with high correlations. The question is that why the algorithm has advantages over standard sparse learning algorithms in handling sparse problems with high correlations? Is there any intuitive explanation? How do you show the advantages of the proposed algorithm in the theoretical analysis for solving high correlated sparse learning problems? 
2. Does the theoretical analysis show any advantages over other existing sparse learning algorithms with support recovery analysis? Are the assumptions weaker? 
3. It seems that the assumptions in Eqs (3),(4),(7),(8) are not standard in existing sparse learning algorithms. It would be good to give intuitive explanations for these assumptions. That would be better if the authors compare the assumptions in this paper with some other commonly used assumptions. 
4. Many existing sparse learning algorithms [1] [2] [3] are designed to deal with high correlated sparse learning problems. The authors should compare the proposed algorithm with those algorithms. 
5. The description of the proposed algorithm in line 104 is not clear. For example, what is the definition of $L^{(1)}_{I,i^\prime}$ ? 
References: 
[1] Zou, H. and Hastie, T. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society. Series B, 67(2):301, 2005. 
[2] Bondell, H.D. and Reich, B.J. Simultaneous regression shrinkage, variable selection and clustering of predictors with OSCAR. Biometrics, 64(1):115, 2008. 
[3] Shen, X. and Huang, H.C. Grouping pursuit through a regularization solution surface. Journal of the American Statistical Association, 105(490):727â€“739, 2010. 
 This paper proposes an interesting greedy algorithm called SWAP for sparse recovery from correlated measurements. The paper also presents detailed theoretical analysis that guarantees exact sparse recovery with high probability.