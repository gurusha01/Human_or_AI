-- Update 
Just a few other thoughts. Simo Saarka has more recent work on continuous-discrete time systems (as you referenced) that might be interesting to contrast against - there, using Gaussian cubature that provides a nice alternative deterministic approximation method. This might be useful for additional discussion and future work. In addition I also now wondered if it is possible to derive an algorithm directly using the variational Gaussian approach. This would be more appealing from the point of having a well defined objective function with which to optimise, potentially fewer numerical issues and interpretation directly in terms of the marginal likelihood. We could afterwards add low-order marginal corrections using cumulant perturbations (like those of Opper for EP) - the only place I think shows this is Barber and van de Laar (http://arxiv.org/pdf/1105.5455.pdf). I do look forward to reading the final version of the paper. 
-- Original 
The paper presents an algorithm for approximate Bayesian inference in models 
with continuous and discrete time observations. The model can be 
cast in the framework of latent Gaussian models and a parallel 
expectation propagation algorithm can be used to derive a principled approach for 
inference and learning dealing with both continuous and discrete time. This EP inference algorithm is 
embedded within an EM algorithm to both learn parameters of the model as 
well as marginal distributions. The algorithm is shown to be effective in 
the number of experimental settings. 
Overall I enjoyed the paper and thoughts that it extends the 
applicability of approximately message passing to a wider class of models. 
In particular I thought it was interesting that EP updates for a 
continuous time limit collapsed to the variational Gaussian updates. This is 
related to the latent Gaussian structure but I wondered if there is a 
deeper reason underlying this connection. 
The algorithm seems robust due to be implied fractional updating but 
I wondered if you could comment on any experienced 
difficulties in implementation, such as issues of slow convergence of parameter learning, 
numerical stability, etc. 
The algorithm is still cubic due to the inverses is in the inference 
as well as the M-step updating - could comment on approaches to scaling up such algorithms. 
In the experimental section it would be nice to see plots giving 
insight into the convergence of the algorithm. Can we also demonstrate the advantages 
obtained by of having an estimate of the marginal likelihood. 
For example, it could be possible in figure 3C to plot 
each of the individual points with a size proportional the marginal likelihood value. 
 Overall the paper is well written and extend the applicability ofapproximate Bayesian inference methods to the class of continuous anddiscrete time settings, which many will find interesting.