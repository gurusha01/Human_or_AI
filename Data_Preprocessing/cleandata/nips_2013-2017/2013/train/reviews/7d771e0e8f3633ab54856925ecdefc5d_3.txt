This paper describes a pruning technique that enables symbolic modified policy 
iteration in large factored action MDPs. Their technique, like regular MPI, 
generalizes policy iteration and valuation, incorporates prior (orthogonal) 
work on partially bound actions, and is experimentally validated. 
I weakly recommend this paper for acceptance. The main contribution appears to 
be a non-trivial implementation detail, but their experiments show that (a) 
pruning by itself is helpful for value iteration, (b) pruning is required for 
modified policy iteration, which is often not possible for memory reasons, and 
(c) that modified policy iteration improves convergence in factored action 
MDPs. 
The paper is well motivated, but the notation is inconsistent in places and 
often hard to follow. e.g., Alg 3.2 is called Prune, but it is used as \cal P 
elsewhere, it is not obvious from the text that T^Q(V) is a function of 
states and actions, or even that the variables are binary. 
My main concern with the paper is that I could not follow the details enough to 
completely understand the statement of theorem 1. In particular, it is not 
clear why \hat T^Q\pi can be different than T^Q\pi. Is it necessary to prune 
at every step, or is it sufficient to prune only once? Is it the repeated 
pruning that causes the overestimation? or is the convergence theorem the same 
for FA-MPI and OPI? 
Proposition 2 seems trivial. Is there any guarantee on how much smaller the 
pruned tree will be? 
 I recommend that this paper be accepted. From a high level it is well motivated and clearly written, and the experiments demonstrate its ability to tackle previously intractable problems.