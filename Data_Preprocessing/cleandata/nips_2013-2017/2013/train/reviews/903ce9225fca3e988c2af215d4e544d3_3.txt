Summary of paper : 
The paper considers the problem learning half-spaces over 3 sparse input in {+1,-1,0}^n. While the information theoretic sample complexity for this problem is of order n/eps^2, assuming the hardness of refuting random 3CNF formulas, it is shown that one cannot efficiently learn using only order n/eps^2 samples. In fact under a stronger version of the same hardness assumption it is shown that it is not possible to efficiently learn using order n^{1+mu}/eps^2 samples for appropriate mu in [0,0.5). 
The hardness result is shown to hold for improper learning. 
On the other hand it is shown that one can efficiently learn these half spaces with sample of size n^2/eps^2. Thus a true gap in statistical versus computational complexity in learning is shown. It is also shown that the gap between sample complexity of learning information theoretically and sample complexity of efficiently learning is not present while learning 2 sparse vectors. 
Comments : 
What can be said about extending the positive result of efficiently learning H{n,3} in n^2/eps^2 to learning H{n,k}. The result seems to rely mainly on using Hazan et al algorithm to learning H{n,2} efficiently by using lemma A.4. Is is possible to extend something like lemma A.4 to deal with H{n,k} ? Whats is the dependence on k in such a case ? 
 Overall the paper is well-written and a nice read. While the reduction/proof is fairly straightforward it is quiet insightful. Providing hardness results for improper learning has been notoriously hard as the usual NP hardness type assumptions cant yield results for improper learning problems. This paper provides a new hardness result for improper learning without using the usual cryptographic hardness assumptions and reduction to lattice problems. The simplicity of the hardness assumption and reduction makes me believe that the result could be useful to other learning scenarios too. I believe the work is definitely worth publishing.