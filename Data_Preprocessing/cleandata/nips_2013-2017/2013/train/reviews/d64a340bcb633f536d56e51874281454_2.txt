Summary: 
The paper deals with a very important problem that arises in linear regression modeling when estimating high dimensional, sparse coefficient vectors (referred to as the 'sparse recovery'), namely correlations in the covariates. In this problem, one seeks a sparse set of non-zero coefficients, called the support of the coefficient vector. The true support identifies those covariates that the response variable depends on. It is well known that correlations among the covariates present a major challenge for correct identification of the support. 
The authors developed a wrapper algorithm around standard sparse recovery algorithms, such as the Lasso, to improve the selected set of covariates. This wrapper, called SWAP, optimizes a desired objective function in a greedy fashion by iteratively swapping covariates. The main result of this paper is that the SWAP can recover the true set with high probability under some conditions. The authors also showed the superior performance of SWAP over the state of the art sparse recovery algorithms on synthetic and real data. There are, however, two main reasons for little satisfaction from this work: 
1) Mild innovation in the actual procedure. This is foremost due to the fact that the procedure very strongly depends on initialization, i.e., on how good the initial support is in terms of its size and overlap with the true support. Given a good initial support, by the trivial swap steps the algorithm can replace the irrelevant covariates and bring in the true ones. 
Thus, the authors suggest using well recognized and highly performing algorithms for sparse recovery to do the initialization. In practice, the size of the true support is not known, and to the knowledge of this reviewer there is no guarantee from the known algorithms to always recover support of correct size. This, again in practice, leads to increased run times where one has to try a (limited) range of possible support sizes. This of course leads to errors if the true support size is outside of the explored range. Also, the "quality" of the initial support is decisive of the performance, as illustrated with low true positive rates when the initial supports (of correct size) are chosen at random (figure 2). 
To summarize this point, it seems the procedure itself is efficient, but seems to be designed as a very simple post-processing of the outcome of other sparse recovery algorithms, which themselves need to be quite sophisticated for the entire framework to work. 
2) Mess in the results. Figure 1 has a confusing description in the text. A lot of things that are required to understand the figure are mentioned in the end of the entire discussion. Some are not formally introduced at all, like the "sparsity level" which is expressed in some numbers from 1 to 10. It would be much easier if what appears in the figure was explained right in front of the paragraphs that refer to it, and not in last remarks. Most of all, figure 2 seems to have wrong captions related to the sub-panels (a to i), which makes it really impossible to understand and to relate to conclusions made in the text. For example, in the text it is implied that Figure 2 a) and b) show numbers of iterations, where in fact a) is a legend and b) has y axis titled "Mean TPR". This makes it hard to read through and make any sense of these results. 
In addition, it would be interesting to know if the "Sparse Recovery" probability is high enough in the usual settings (theorem 4.1). In particular, please report the "Sparse Recovery" probabilities for parameters that describe all the applications mentioned in the results (including the gene expression application in lines 139-146). 
Finally, a comparison to other wrapper methods is missing. 
Minor comments: 
Line 158 With just trying several values of k and observing a linear relation between number of iterations and k, it is not possible to generalize that the run-time complexity of the SWAP is O(k). 
LIne 163 please remove "roughly", the O notation is understood as "rough" already 
Remark 3.2 about empirical convergence rate seems to contradict the theoretical considerations in Proposition 4.1 
Line 374 "types cancers" --> "types of cancers" 
Line 316 "that the Lasso" --> "than the Lasso" 
Line 317 "a backwards algorithm" --> "a backward algorithms" 
Line 321 "Figures 1" --> "Figure 1" 
The paper is mostly nicely written and the organization of the paper is good. Howevere. the English in Section 5.1 is worse than in the rest of the paper and could be improved. Particularly, please rewrite the description for the example A1 (lines 302-308). 
 This work deals with an extremely important problem in high-dimensional statistics, and proposes a simple wrapper algorithm around standard sparse recovery algorithms. While this method seems to work well in practice, this paper is not novel enough (or does not bring a significant contribution) to be considered at a conference like NIPS.