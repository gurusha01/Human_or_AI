The paper presents a "hybrid" deterministic-stochastic method for first-order optimization. The algorithm mixes calls to a deterministic oracle (querying full gradients) and stochastic oracle (querying stochastic gradients). The authors show that the approach allows to drop the dependency in the condition number in the rate of convergence. 
The authors present an interesting and striking theoretical result. Basically, assuming the condition number is known (i.e. both strong convexity and strong smoothness constants of the objective are known), assuming first-order hybrid oracle (both stochastic and deterministic gradient can be queried) is available, then with a suitably chosen mix of deterministic gradient and stochastic gradient steps, one can achieve with high-probability a O(log(1/epsilon)) convergence rate for batch optimization. 
However, there are several concerns with the current state of the paper. First, the theoretical analysis only covers the case where the condition number is perfectly known beforehand, and does not cover the behavior of the algorithm when this hyper-parameter of the algorithm is misspecified. Basically, the current resuIt says that the dependence on the condition number (kappa) can be removed from the convergence rate of a first-order optimization algorithm when this condition number is known before hand to the algorithm. Maybe, the theoretical analysis in these other cases (when kappa is unknown) is challenging. If so, then this analysis could have been conducted through experiments. But the paper has no experiments section, which is the second major concern. Since the main contribution of the paper is a new algorithm, then I guess it would make sense to at least perform some experiments to assess the theoretical results presented in the paper, and study their relevance wrt the actual behavior of the algorithm on empirical data. 
Detailed comments 
The proposed algorithm (EGD) relies on the update rule defined by Eq. 8 (page 4) using the so-called "mixed gradient" defined in Eq. 7. Therefore, EGD requires $\eta$ as a hyper-parameter to be set (or estimated). Setting $\eta$ boils down to knowing the condition number $\kappa$ ("conditional number" in the paper), that is both the "strong convexity modulus" $\lambda$ and the "strong convexity modulus" $L$. As far as I understood the paper, the authors do not provide any guideline or theoretical argument allowing to set $\kappa$ beforehand. 
So, there are two possibilities. Either this parameter has to be estimated, and the corresponding estimation procedure is missing (just a heuristic procedure would be fine, as long as it is supported by numerical experiments) . Or, this parameter is assumed known, because the point of the paper is mainly theoretical. But then the theoretical analysis/experimental section should cover the cases where this hyper-parameter is misspecified. This implies studying the convergence rate in cases where the hyper-parameter is set too large or set too small. 
Although popular in theoretical analysis, and realistic in many situations, the smooth and strongly convex case can be too restrictive, and other settings (non-strongly convex) are also interesting. In particular, the non-strongly convex case is important as well, as it also arises in several situations. See Bach & Moulines, 2011, for a theoretical analysis of the different behaviors depending on the cases (convex vs strongly convex). 
There are other concerns. Blending deterministic gradient steps and stochastic gradient steps in a first-order optimization algorithm is not a new idea. Actually, the algorithm presented in the paper is not written this way, that is as an alternation of deterministic gradient steps and stochastic gradient steps, with different of frequencies for each type of steps. It is written as one "mixed" update per iteration (within an epoch), and then a gradient-like update step. It would be interesting to discuss how the proposed algorithm relates and compares with a similar-in-spirit algorithm where one would interleave deterministic gradient steps and stochastic gradient steps (at least in the "unconstrained" case). 
The authors do not review a related line of work, namely so-called hybrid deterministic-stochastic optimization algorithms; see [Hybrid Deterministic-Stochastic Methods for Data Fitting. M. Friedlander, M. Schmidt. SISC, 2012]. Discussing and comparing the convergence rates would be valuable here. See also the above ref. for a review of older works on that topic. 
Finally, a thorough experimental study would be a valuable addition to the paper, including a detailed comparison with regular SGD, averaged SGD, and recent proposals for stochastic first-order optimization (SAG, etc.). 
A more minor concern, the optimization setting considered in the paper is not clearly stated. The purpose of the paper is to get the best of both worlds (deterministic optimization and stochastic optimization), namely exponential rate of convergence from the deterministic world and dependence on the condition number from the stochastic world. The authors do not specify clearly what they intend to solve: the deterministic optimization problem [Minw F(w)=1/n \sum{i=1}^n Fi(w)] with a "stochastic" (or more precisely, "randomized") algorithm, or the stochastic approximation problem [Minw E(F(w))]. Note in passing that both SAG and SDCA are randomized algorithms for solving the deterministic optimization problem [Minw F(w)=1/n \sum{i=1}^n F_i(w)]. The theoretical setup stated in Section 3.1 is misleading from this respect, and none of the claims made later in the paper clarifies which setting is considered. This could easily be fixed. 
 The authors present an interesting and striking theoretical result. Basically, assuming the condition number is known (i.e. both strong convexity and strong smoothness constants of the objective are known), assuming first-order hybrid oracle (both stochastic and deterministic gradient can be queried) is available, then with a suitably chosen mix of deterministic gradient and stochastic gradient steps, one can achieve with high-probability a O(log(1/epsilon)) convergence rate for batch optimization.However, there are several concerns with the current state of the paper. First, the theoretical analysis only covers the case where the condition number is perfectly known beforehand. The behavior of the algorithm when this hyper-parameter of the algorithm is misspecified is not discussed. Basically, the current resuIt says that the dependence on the condition number (kappa) can be removed from the convergence rate of a first-order optimization algorithm when this condition number is known before hand to the algorithm. Maybe, the theoretical analysis in these other cases (when kappa is unknown) is challenging. If so, then this analysis could have been conducted through experiments. But the paper has no experiments section, which is the second major concern. Since the main contribution of the paper is a new algorithm, then I guess it would make sense to at least perform some experiments to assess the theoretical results presented in the paper, and study their relevance wrt the actual behavior of the algorithm on empirical data.