The paper describes a new generative model of images, in which low-level features are first shifted and then combined according to a nonlinear, stochastic, masking process. The authors develop approximate inference and learning algorithms, and demonstrate results on grayscale image patches. 
The paper is clearly written, well organized, and easy to follow. It introduces a combination of two previously explored ideas (translation invariance and occlusive image generation), so conceptually it is somewhat of an incremental advance, but the approximations to inference of occluding components are novel and lead to a new structure for model parameters (feature weights and mask probabilities). Although the results are very similar to previously reported feature learning algorithms, they seem promising, especially if such a model could be extended hierarchically. 
My main concern is with the focus of the paper: is the goal to generate predictions and theories for biological processing, or is it to propose a new set of representations more useful for computer vision? 
If the focus is on computer vision, the authors should explain why this solution to occlusion is better than other occlusive models (including max- rule for feature combinations, dead leaves models, and masked RBM by Le Roux, Heess, Shotton, Winn, 2011), and also why translation invariance makes the model more tractable than convolutional models. As it is, this paper presents another alternative to occlusive and translating models (though it unifies the two computations). 
If the aim is to provide a theoretical result for neuroscience, the authors should emphasize what kind of predictions this model makes (or what it explains about observed properties of neurons in visual cortex). The prevalence of center-surround receptive fields has been noted and modeled previously. Several theories have been proposed for translation invariance in complex cells, and some models even derive this directly from objective functions like information maximization or temporal stability of the representation. If this model is to be taken seriously in the context of brain processing, specific, novel predictions or explanations should be offered, and aspects of the model that are not biologically plausible (like the complete translation invariance) should be addressed in the discussion. I recommend backing off the neuroscientific claims unless these can be strengthened sufficiently to be useful to experimentalists. 
Other comments: 
What is the benefit of the stochastic component assignment over choosing pixel value with a max rule, as in (Puertas et al, NIPS2010)? Also, the all-or- none activation of the features seems like a limitation of the proposed model. 
Is it possible to relate the (feed-forward) operations in a convolutional neural network to performing approximate inference with expectation truncation? What exactly are the benefits of probabilistic pooling? 
Why all the work to compute the "estimated Receptive Fields"? For visualizing and interpreting model parameters, the mask-feature product seems to work quite well. As a comparison to biology, the translation invariant receptive field is not very appropriate: complex cells are not "fully translation invariant" as claimed in the Discussion (so it's not a good characterization of a complex cell's behavior), and for simple-like cells, linear receptive fields are estimated using direct regression methods. If model units are to be interpreted as populations of cells, then wouldn't a convolutional network with replicated receptive fields be a better model? As an aside, new methods are being developed to characterize the features encoded by translation invariant neurons (e.g., Eickenberg, Rowekamp, Kouh, Sharpee, 2012; Vintch, Zaharia, Movshon, 2012). These might be worth citing, though there isn't much data analyzing large neural populations yet. 
In the last paragraph, there is a mention of building hierarchical versions of this model. I am curious if the authors have more specific ideas of how multi-layered occlusive models can be constructed, and what kind of features they will extract from natural images. Specifically, would the layering/transparency be interpreted similarly at higher levels of the hierarchy, or would it simply add a nonlinear stochastic component to a deep model? Results presented here are not strikingly different from many other learning algorithms, so it is important to show that extensions to the model have promise. 
Minor comments: 
How are image patch boundaries handled during translation? 
I am assuming the masks are constrained to be nonnegative, but the text does not specify. 
What is the motivation for prefiltering with center-surround? It's true that this is comparable to the (linear component of the) transformation performed in LGN, but receptive fields are experimentally derived by correlating to pixel stimuli on the screen, not LGN outputs. 
It would be helpful if a sentence or two in the paper listed all the approximations required to make the model tractable (expectation truncation, independent pixel occlusion). 
Do you have any insight as to why all globular components have positive centers? 
 This is a clearly written paper describing that decsribes a somewhat incremental advance: the combination of two previously developed ideas. The results suggest that the learning algorithms can learn interesting structure, but so far the authors have only replicated features learned with other models.