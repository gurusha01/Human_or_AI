This paper provides a generic way to perform parameter tuning of a given training algorithm in a differentially private manner. 
Given a set of examples (for training and validation), a set of model parameters, a training algorithm, and a performance measure, the proposed procedure outputs a differentially private hypothesis with respect to prescribed privacy parameters. 
The basic idea behind the procedure is the definition of (\beta1, \beta2, \delta)-stability; which describes the stability of the performance with respect to change in the training set and the validation set. 
The procedure basically follows the exponential mechanism, and the utility bound is also provided. 
At each step, the gradient is set to a tree such that each node of the tree represents the differentially private partial sum of values held by the descendent nodes. In execution of the differentially private FTAL, the learner issues a query to the tree to learn a differentially private parietal sum of previous gradients. Noting the fact that only O(log T) accesses are needed to learn the partial sum, the variance of the noise added to each value is O(log T). 
The authors provides two algorithms, one is for the full information model and the other is for the bandit setting. In the full information setting, the proposed algorithm improves the regret bounds. The regret bound in the full information model with strongly convex and L-Lipschitz cost functions is significant; O(poly log T) is achieved as FTAL does. In the bandit setting, the authors introduces a technique called "one-shot gradient" to evaluate the gradient and shows regret bounds in several settings; the regret bound is optimal with respect to T when the cost function is strongly convex and L-Lipschitz. 
The clarity can be improved. It is hard to follows the problem settings. It is not clearly stated what are private instances to be protected by differential privacy. Definitions of the oblivious adversary and the adaptive adversary in the bandit setting is unclear, too. 
line 107. What is "stronger setting"? Does this mean "with stronger assumptions"? 
line 229-232. The sentence was hard to follow. 
line 305. This technique was referred to as one-point gradient, not as one-shot gradient. 
 Significant improvement of regret bounds of differentially private online learning in the full information model is shown.