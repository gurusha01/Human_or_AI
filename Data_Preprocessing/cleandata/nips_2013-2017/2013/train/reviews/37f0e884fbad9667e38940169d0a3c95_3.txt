The paper presents a new hybrid strategy for stochastic gradient descent, that employs both stochastic and batch gradients. The algorithm is guaranteed to converge to an epsilon accurate solution using O(log(1/eps)) full gradients, and O(k^2 log(1/eps)) stochastic gradients. 
The convergence proof appears correct and novel to me, a part for some minor mistakes detailed below. 
My main concern is about the relevance of the proposed algorithm in the machine learning setting, that is the focus of the conference. 
In fact, in usual ML algorithms the strong convexity is given by the regularizer. Hence, the value of mu is of the order of the number of samples N, that is something like mu = C N, where C does not depend on N. With this assumption, the proposed method is faster than batch gradient only if the number of samples is bounded by O(C^3/L^3), that does not seem to me an interesting regime. Moreover the convergence rate for the proposed algorithm holds only in high probability, while the ones for batch gradient descent is deterministic. 
This point is very important and it must be carefully discussed, to actually show that the algorithms has a real advantage over batch gradient descent, and to prove the relevance of the paper for the ML community. 
Minor comments: 
- equation (6) should be ||w^*-\hat{w}||^2 
- please specify in 288 on which function you use (7) 
- the equality in 286 should be removed: it adds nothing to the comprehension, rather it decreases it 
- in (13) the absolute values should be norms 
- Please explain somewhere the fact that L \geq lambda, even if it is obvious, it is better to state it more clearly 
- In (4) x^ should be w^ and f should be F, and the first term in the max is always bigger than the second one, by Lemma 1 
- Please precisely define the condition number as a function of lambda and L Novel hybrid stochastic/batch gradient descent. Not clear if the algorithm has any advantage over standard batch gradient descent in practical ML optimization problems.