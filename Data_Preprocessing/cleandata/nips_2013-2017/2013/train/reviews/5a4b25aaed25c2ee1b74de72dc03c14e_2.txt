The authors propose a method for improving the mixing rate of Gibbs sampling in Ising models by projecting the models original parameterization onto a new parameter setting that satisfies the Dobrushin criterion needed for fast mixing. They formulate the projection as a constrained optimization problem, where the constraints are needed to ensure that the new parameters are defined over the same space as the original parameters, and examine this projection under several distance/divergence measures. 
In my opinion, methods that combine principles from stochastic and deterministic inference is an under-explored area and as a result, I think this is an interesting idea. While the idea of augmenting the original parameters to improve mixing time is straightforward, I found the description of the dual of the projection problem to be a little unclear - e.g. how do the zij*dij =0 constraints ensure that the new parameterization is over the same space (can't it be over a smaller space)? I also was unsure of the overall procedure - do you perform the projected gradient operations to completion and then run a Gibbs sampler, or do you somehow interleave sampling with the gradient updates? An algorithm description box would help to clarify. Also, is the proposed projected gradient descent strategy guaranteed to converge? 
I also found the experiments to be a little unconvincing. In the first set of experiments, why was the Gibbs sampler run for 30K iterations? Since you are comparing a sampling method to deterministic methods, a comparison on the basis of time would seem more fair. Also, where are the error bars on these charts - the reader cannot tell if these results are significant. The second set of results compare the original/naive gibbs sampler with gibbs samplers under different projections and show that the projection does lead to faster mixing. However, it takes time to performing the projection operation and this is not accounted for in this comparison (e.g. if the projection takes 1 minute, and the naive Gibbs sampler can generate 10k samples in that time, then the projection might not be worthwhile). Last, why was there no comparison to blocked Gibbs samplers or Gogate's "Lifted Gibbs Sampler"? 
 All in all, a very nice idea. However, the development of the projection problem and proposed method could use a little work and a bolstered set of experiments are needed to convince me of the utility of the method.