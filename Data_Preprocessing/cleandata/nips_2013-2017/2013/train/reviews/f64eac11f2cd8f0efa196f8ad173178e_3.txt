Overview 
======== 
The paper proposes a lower bound on the expected performance gain for policy gradient methods. This lower bound can then used to optimize the step-size. The bound is given in general, specialized for Gaussian policies, and finally a version for REINFORCE and G(PO)MDP/PGT is given, which is also evaluated empirically on a simple LQG problem. 
Quality 
======= 
The theoretical part is very nice with a lot of material. The experiment nicely illustrates the approach. It would be highly interesting to evaluate how well the adaptive step-size performs in motor control and robotic applications (but that is clearly out of the scope of this paper). 
Clarity 
======= 
The paper reads well. The paper itself nicely manages to convey a notion of where the theorems and lemmas come from/how they are proven. I like the structure of the paper that goes from more theoretical to more and more applied. However, towards the end (Sect. 6) I got the feeling that it gets a bit crammed... 
Originality 
=========== 
To my knowledge nobody has looked into adaptive step-sizes for policy gradient methods from a theoretical point of view yet. 
Significance 
============ 
Even though tighter bounds would be nice (as mentioned in Sect. 6), the results are interesting from a more theoretical point of view and even show promise in experiments. 
Minor Comments: 
=============== 
Table 1: maybe you could add another line with the optimal step-size (determined by line search as discussed in Sect. 1.) 
l 75: "do not need to tune a free parameter" is a bit misleading. It is true that this type of algorithms does not have a step-size but still e.g. the exploration magnitude needs tuning... 
Table 1: maybe you could mark the "winners" 
Table 1: 1 step for alpha* with sigma=5. is hard to believe/a very lucky coincidence 
Table 2: How do you count the iterations here? The number of update steps or (number of update steps)*(number of trajectories)? 
Table 2: maybe use e notation instead of >100,000 
Table 2: especially here a comparison to the heuristics would have been interesting 
Supplementary Material Fig. 1&2: maybe you could use a logarithmic iteration axis (such that we can see better what happens at the beginning with alpha=1e-5/t). Axis labels would also be nice. 
Public Review: 
============== 
The author response addresses most of the above points. The notes on spelling and grammar mistakes as well as minor formatting/LaTeX suggestions were removed. The paper tackles a very interesting unsolved problem from a theoretical point of view and manges to get results that have great potential for practical applications.