This paper presents a tensor factorization approach for parameter 
estimation in settings where there is an underlying hidden Markov 
model, but we only see a small random fraction of the observations. 
The contribution of the paper involves identifying the tensor 
factorization structure in the problem (which extends ideas of 
Anandkumar et al.), which is relatively straightforward. An 
interesting part is showing that you can estimate recover the 
transition distribution from an expectation over the sums of the 
transition distribution. Sample complexity results and some toy 
simulations are provided as well. Overall, a reasonable paper with 
some new ideas. 
Presentation suggestion: I would suggest defining the actual 
model/problem (which is currently in section 3) much earlier - for 
example, one shouldn't be subjected to Theorem 1 before even having a 
formal definition of what the paper is trying to do. Too much space is 
spent reviewing the tensor decomposition framework; a citation to that 
work and a brief description of the key ideas suffices, or relegate to 
the appendix. 
In the definition of the model in section 3.1, please make it explicit 
that we are getting $N$ i.i.d. replicates of this model. For a while, 
I was confused at how this was even possible if you only have one 
random draw from $\pi_0$. 
Currently, the theorems very dryly write down true facts, but the text 
doesn't really provide any guidance about what are the important 
properties to watch out for. For example, line 062 of Appendix A.1 
nicely lays out the factorization structure, which I think can be 
imported into the main body of the paper. Also, I'd appreciate more 
intuition about Theorem 3. 
Experiments: it's nice to see some relationship (even if it's just 
qualitative) between the empirical findings and the theoretical bounds; 
the fact that $U$ is easier to learn is not surprising. I'd be curious 
to see how this algorithm compares with EM, since the original 
motivation is that EM presumably gets stuck in local optima and this 
method does not suffer from that problem. And of course, needless to 
say, experiments on real data would be good too, especially taken from 
the cited that look at learning from non-sequence data. 
318,320: primes should be placed above the subscripts in $M2$ and $M3$ 
 This paper applies recently developed tensor factorization techniquesto the new setting of learning a HMM from non-sequence data. The papercould be written to convey more insight, but overall it's a reasonablepaper.