The paper presents a comprehensive exploration of the Optimistic Mirror Descent (OMD) algorithm and its applications across various domains, including offline optimization, zero-sum games, and convex programming. The authors effectively leverage the concept of predictable sequences to improve regret bounds and convergence rates, demonstrating the algorithm's versatility and theoretical significance. The paper builds on prior work in online learning and optimization, such as Mirror Prox [9], Exponential Weights, and the excessive gap technique [6], while extending these methods to new settings, including Hölder-smooth functions and partial information games.
Strengths:
1. Technical Depth and Rigor: The paper provides a detailed theoretical analysis of OMD, including regret bounds, convergence rates, and extensions to structured optimization problems. The results are well-supported by rigorous proofs and lemmas, showcasing the authors' deep understanding of the topic.
2. Novel Contributions: The extension of OMD to Hölder-smooth functions and its application to zero-sum games with uncoupled dynamics addresses open questions in the literature, such as those posed by Daskalakis et al. [6]. The proposed algorithm achieves a faster convergence rate of \(O(\log T / T)\) in zero-sum games, which is a significant improvement over standard no-regret methods.
3. Practical Relevance: The application of OMD to convex programming, particularly the Max Flow problem, demonstrates its utility in practical optimization tasks. The algorithm achieves a time complexity of \(O(d^{3/2} / \epsilon)\), matching prior results [8] while simplifying the procedure.
4. Clarity of Results: The paper provides clear theoretical guarantees for each application, such as regret bounds and convergence rates, making it easy for readers to understand the implications of the proposed methods.
Weaknesses:
1. Clarity and Organization: While the technical content is strong, the paper is dense and challenging to follow in places. The presentation could benefit from clearer explanations of key concepts, particularly for readers less familiar with OMD or predictable sequences. For example, the transition between sections (e.g., from zero-sum games to convex programming) could be smoother.
2. Experimental Validation: The paper lacks empirical results to complement the theoretical findings. While the focus is on theory, numerical experiments could provide additional evidence of the algorithm's practical performance and robustness.
3. Scope of Predictability: The discussion on generalizing predictability beyond smooth functions is limited to high-level remarks. A more concrete exploration of non-smooth settings or adaptive methods would strengthen the paper's impact.
4. Dependence on Known Parameters: Some results, such as those for the Max Flow problem, assume prior knowledge of optimal values (e.g., \(F^*\)), which may not always be practical. Addressing this limitation through adaptive techniques or alternative approaches would enhance applicability.
Arguments for Acceptance:
- The paper addresses significant open questions in online learning and optimization, offering novel theoretical insights and practical algorithms.
- The results are well-grounded in theory and extend existing methods in meaningful ways.
- The applications to zero-sum games and convex programming are both timely and impactful.
Arguments Against Acceptance:
- The lack of experimental validation limits the paper's practical relevance.
- The dense presentation may hinder accessibility for a broader audience.
- The reliance on smoothness and known parameters restricts the generality of the results.
Recommendation:
I recommend acceptance of this paper, as it makes substantial theoretical contributions and advances the state of the art in online learning and optimization. However, I encourage the authors to improve the clarity of the presentation and consider adding empirical results in future work to strengthen the paper further.