Review
Summary and Context
This paper addresses the problem of transfer learning in long-lived agents tasked with solving a sequence of Markov Decision Processes (MDPs) over a finite lifetime. The novel contribution lies in the reuse of reward functions—specifically, leveraging good guidance reward functions from prior tasks to train a reward mapping function that initializes guidance reward functions for subsequent tasks. The authors build on the Optimal Rewards Problem (ORP) framework, which separates the roles of reward functions into "guidance" and "evaluation," and demonstrate that this approach can mitigate computational limitations in bounded agents. The proposed Reward-Mapping-Transfer ORP Agent is compared against three baselines: a Conventional Agent, a Non-Transfer ORP Agent, and a Sequential-Transfer ORP Agent. Empirical evaluations in grid world and network routing domains show that the reward mapping approach outperforms these baselines, as well as a competing policy transfer method, particularly in settings where tasks vary in both reward and transition dynamics.
The paper builds on prior work in transfer learning, including the reuse of policies, value functions, and models [9, 16, 17], and extends the ORP framework introduced by Singh et al. [13] and Sorg et al. [14]. It also connects to broader research on reward shaping [10] and inverse reinforcement learning [11]. The authors' focus on reward transfer as a generalizable mechanism is a significant departure from traditional transfer learning approaches.
Strengths
1. Novelty: The paper introduces a unique perspective on transfer learning by focusing on reward function reuse, which is less explored compared to policy or value function transfer. This approach is particularly compelling because reward functions are task-agnostic and can generalize across changes in transition dynamics.
2. Technical Soundness: The proposed method is grounded in the ORP framework and uses well-established techniques like UCT planning and policy gradient reward design (PGRD). The empirical results are robust, with clear performance improvements over baselines and a competing method.
3. Significance: The work addresses a critical challenge in reinforcement learning—how to enable efficient transfer in bounded agents operating in dynamic environments. The demonstrated performance gains suggest that the reward mapping approach has practical implications for real-world applications.
4. Clarity: The paper is well-organized, with detailed descriptions of the problem, methodology, and experimental setup. The pseudocode for the reward mapping agent and visualizations of learned reward mappings enhance reproducibility and understanding.
Weaknesses
1. Theoretical Analysis: While the empirical results are strong, the theoretical understanding of when and why the reward mapping approach outperforms alternatives is limited. The authors acknowledge this as a direction for future work, but a preliminary theoretical analysis would strengthen the paper.
2. Scalability: The reward mapping function is learned using kernel regression, which may not scale well to high-dimensional reward spaces or large task sequences. This limitation is not discussed in detail.
3. Comparison with Other Transfer Mechanisms: The paper primarily compares reward transfer with policy transfer but does not explore hybrid approaches (e.g., combining reward and policy transfer). Such comparisons could provide a more comprehensive evaluation.
4. Task Similarity Assumptions: The method implicitly assumes some degree of similarity between tasks (e.g., shared reward features). The paper does not discuss how performance might degrade in highly dissimilar task sequences.
Arguments for Acceptance
- The paper presents a novel and impactful contribution to transfer learning in reinforcement learning, with strong empirical evidence supporting its claims.
- The focus on reward function reuse is a fresh perspective that could inspire future research in this area.
- The experiments are thorough, covering both simple and complex domains, and demonstrate clear advantages over existing methods.
Arguments Against Acceptance
- The lack of theoretical analysis limits the generalizability of the findings.
- Scalability concerns and the absence of discussion on task dissimilarity could hinder practical applicability in more complex settings.
Recommendation
Overall, this paper makes a significant contribution to the field of reinforcement learning and transfer learning. While some limitations exist, they do not detract from the novelty and practical relevance of the work. I recommend acceptance, with the suggestion that the authors address scalability and theoretical analysis in future iterations. 
Score: 8/10