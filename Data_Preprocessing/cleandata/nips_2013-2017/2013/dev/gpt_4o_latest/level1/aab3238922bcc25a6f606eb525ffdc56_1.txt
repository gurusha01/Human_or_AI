The paper introduces the Randomized Dependence Coefficient (RDC), a novel measure of nonlinear dependence between random variables of arbitrary dimensions, inspired by the Hirschfeld-Gebelein-Rényi (HGR) Maximum Correlation Coefficient. RDC is defined as the largest canonical correlation between random nonlinear projections of copula-transformed data. It is computationally efficient (O(n log n)), invariant to marginal distribution transformations, and easy to implement, with a concise R implementation provided. The authors validate RDC through theoretical analysis and empirical experiments, demonstrating its scalability and effectiveness in detecting nonlinear dependencies across synthetic and real-world datasets. The paper positions RDC as a lightweight alternative to existing methods like HSIC, MIC, and KCCA, addressing their limitations in computational cost, scalability, and ease of implementation.
Strengths
1. Technical Soundness: The paper provides a rigorous theoretical foundation, including a detailed derivation of RDC and its relationship to HGR. The authors also establish RDC's computational complexity and its approximation bounds relative to HGR, ensuring clarity about its strengths and limitations.
2. Empirical Validation: The experiments are comprehensive, covering synthetic data, real-world feature selection tasks, and comparisons with state-of-the-art methods. RDC consistently demonstrates competitive or superior performance in detecting nonlinear dependencies and feature selection tasks.
3. Scalability: The computational efficiency of RDC (O(n log n)) makes it suitable for large datasets, a significant improvement over methods like KCCA and MIC, which have higher computational demands.
4. Practicality: The inclusion of a concise R implementation highlights the method's simplicity and ease of adoption, which is likely to appeal to practitioners.
5. Originality: The use of random nonlinear projections and copula transformations to approximate HGR is a novel and elegant approach, distinguishing RDC from existing methods.
Weaknesses
1. Parameter Sensitivity: The performance of RDC depends on hyperparameters like the number of random features (k) and the scaling factor (s). While the authors provide heuristics, a more systematic approach to parameter tuning would enhance usability.
2. Limited Theoretical Guarantees: While RDC approximates HGR, the paper does not establish whether RDC satisfies all of Rényi's desirable properties in practice, especially under finite sample conditions.
3. Overfitting in Specific Cases: The authors acknowledge that RDC may overfit in certain scenarios, such as linear associations, due to the use of sinusoidal features. This limitation could impact its robustness.
4. Comparison Scope: While the paper compares RDC to several methods, some comparisons (e.g., KCCA) are incomplete due to challenges in hyperparameter tuning. A broader evaluation across diverse datasets and parameter settings would strengthen the results.
Arguments for Acceptance
- The paper addresses a fundamental problem in statistics and machine learning: measuring nonlinear dependencies efficiently and effectively.
- RDC is a significant contribution due to its scalability, simplicity, and empirical performance.
- The theoretical and experimental analyses are thorough and well-executed, demonstrating the method's potential impact on both research and practice.
Arguments Against Acceptance
- The reliance on heuristic parameter tuning and the lack of a systematic approach may limit the method's applicability in practice.
- Some theoretical aspects, such as RDC's adherence to all of Rényi's properties, remain underexplored.
- The experimental comparisons, while strong, could be more exhaustive, particularly for methods like KCCA.
Recommendation
I recommend acceptance with minor revisions. The paper makes a strong contribution to the field, but addressing the parameter sensitivity and providing additional theoretical guarantees or empirical comparisons would further solidify its impact.