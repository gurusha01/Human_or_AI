This paper proposes a straightforward form of nonlinearity for neural network architectures, where groups of neurons (typically two) are formed, and all neurons in a group are zeroed out except the one with the highest value. Despite its simplicity, the approach appears to yield strong classification performance. The authors also present evidence suggesting that this nonlinearity reduces forgetting compared to networks employing more conventional nonlinearities, though there may be issues with the experimental setup (discussed below).
1) The results on permutation-invariant MNIST are promising, but there are better results reported in Table 1 of http://arxiv.org/pdf/1302.4389v3.pdf, some of which involve simple feedforward networks. Additionally, I disagree with the characterization of dropout in the input layer as a form of data augmentation, as it does not rely on any assumptions about the input structure. It would have been valuable to include experiments with input dropout for comparison.
2) The claim that the network forgets less is an interesting property, but there are potential flaws in the experimental design. Specifically, the authors wait for the network to reach a certain likelihood threshold before changing the data/labels. Since the proposed nonlinearity performs better on recognition tasks, it may require less effort to reach the likelihood threshold, resulting in less training overall. Consequently, the reduced forgetting could simply be a byproduct of less training rather than an inherent property of the nonlinearity.
3) A related experiment would have strengthened the paper. After training the network on all digits simultaneously (to convergence), the authors could have conducted a two-phase experiment: first training on digits 1-5 until convergence, then adding digits 6-10 and training on all digits. This would test how much the network is constrained by the local minimum found during the first phase of training. Ideally, the performance in this two-phase experiment should match the performance of training on all digits from the start. It is well-documented that sigmoid networks struggle with this, and it would be interesting to see how the proposed nonlinearity compares.
Quality: The paper is good, but it would benefit from additional experiments, and the forgetting experiment should be improved.  
Clarity: Very good.  
Originality: The idea is novel; while it is essentially another nonlinearity (related to max pooling), it is simple and achieves promising results.  
Significance: Moderately significant, as it adds a new nonlinearity to the neural network toolbox.  
In summary, this paper presents a simple idea that performs well for classification tasks. The reduced forgetting is an important property, but the experimental evidence supporting it is not entirely convincing. Additional experiments, such as the suggested two-phase training test, would enhance the paper's contributions.