Review - Paraphrased Version
Paper Summary:  
This paper introduces two innovative entropy estimators tailored for binary neural spike words. These estimators adopt a Bayesian framework, leveraging a mixture-of-Dirichlet prior. The hierarchical distribution employs a count distribution as the base measure for the Dirichlet distribution. The authors validate their methods using both synthetic data and neural recordings from retinal ganglion cells, comparing their performance against established entropy estimators. They demonstrate that their estimators achieve accurate results with fewer samples. Additionally, the method is applied to measure temporal synergy in retinal ganglion cell data.
Quality:  
The Bayesian entropy estimators are robust and skillfully address the curse of dimensionality. By incorporating prior knowledge about the problem's structure, the approach significantly reduces the number of samples required for accurate estimation.  
While it is expected that the proposed estimators outperform existing methods on synthetic data—given that the word distributions align with the model structure—their performance on retinal ganglion cell recordings is particularly noteworthy.  
However, it remains uncertain how well these methods would generalize to other types of neural data. The approach relies on the critical assumption that the word distribution is adequately captured by the overall count distribution. While this assumption holds true for the retinal ganglion cell data, further testing is needed to determine its applicability to other datasets. Including a sentence in the discussion to address this limitation would strengthen the paper. Regardless, the proposed estimators represent a valuable contribution.  
Clarity:  
The manuscript is well-written and clear.  
Originality:  
The entropy estimators build upon the work of Nemenman et al. (NIPS 2002) by incorporating prior knowledge about the structure of spike trains. The overarching concept is reminiscent of the raster marginals model (Okun et al., J Neurosci 2012), as both approaches utilize the total spike count distribution to mitigate the curse of dimensionality.  
Significance:  
Entropy estimation is a critical challenge, as information quantification is central to neural coding research. The demonstrated improvements in performance over alternative methods are impressive and noteworthy.  
Minor Points:  
- Line 080: "In Section 3 introduce" → "In Section 3, we introduce"  
- Line 163: Missing citation  
- Line 232: Incomplete sentence  
- Line 234: "the estimator fast-to-compute" → "the estimator is fast-to-compute"  
- Line 290: "We only then need only" → "We then need only"  
- Line 295: "for" → "form"  
- Figures 4 and 5: "DCnt" → "DCt"  
Summary:  
This paper presents entropy estimators for neural spike trains that achieve accurate results with fewer samples. The contribution is significant, well-executed, and addresses an important problem in neural coding research.