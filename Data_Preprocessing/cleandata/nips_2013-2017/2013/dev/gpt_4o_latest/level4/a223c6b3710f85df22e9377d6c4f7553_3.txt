This manuscript presents a detailed and comprehensive analysis of the post-Lasso estimator, which is obtained by fitting a least squares model to the variables selected by the Lasso, in the context of high-dimensional sparse regression.
The authors provide a novel theoretical guarantee for the post-Lasso estimator and complement their theoretical results with simulated experiments on a toy example that clearly demonstrate the advantages of the procedure.
However, there are a few areas that could benefit from improvement.
First, some key references are missing regarding prior works that have studied the post-Lasso estimator (or its variations) from a theoretical perspective:
- "Pivotal Estimation of Nonparametric Functions via Square-root Lasso," Alexandre Belloni, Victor Chernozhukov, Lie Wang (see Theorem 7, for example).  
- "L1-Penalized Quantile Regression in High-Dimensional Sparse Models," Alexandre Belloni and Victor Chernozhukov, 2011 (see post-l1-QR Theorem 5).  
Additionally, a more recent study has examined the practical properties of the post-Lasso estimator on a specific set of simulated data. This work raises concerns about the theoretical benefits highlighted by the authors in certain scenarios. Could the authors provide their perspective on this?  
- "Trust, but verify: benefits and pitfalls of least-squares refitting in high dimensions," Johannes Lederer, 2013.  
Points to address:
- Line 134: Without additional assumptions, the minimizers of \( G \) may not be unique. The results hold under further assumptions on the Gram matrix, as previously established in:  
  - "On Sparse Representations in Arbitrary Redundant Bases," J.-J. Fuchs, 2004.  
  - "The Lasso Problem and Uniqueness," Ryan J. Tibshirani, 2013.  
  The issue of uniqueness arises multiple times in the proofs. For example, uniqueness is invoked before being formally established (e.g., line 1034, where a strict inequality is stated without justification; see also lines 1071 and 1113). I recommend that the authors adapt results from the aforementioned references to demonstrate that uniqueness holds under suitable assumptions. This would enhance the clarity and rigor of the proofs. Additionally, it may be helpful to explicitly state an assumption requiring the extracted Gram matrix to be invertible, where necessary.
- Line 303: The parameter \( t_0 \) is defined but not used in this section. It is later reused in the next section. Please remove this definition from the current section to avoid confusion.  
- Line 307: A comment on the dependency of the parameter \( \lambda \) on the unknown quantity \( \eta \) would be useful.  
- Line 317: Could the authors clarify the advantages of their assumptions compared to those in [23]? It is not immediately clear which set of assumptions is weaker. On one hand, the proposed work requires a larger invertible matrix (since \( T_0 \) contains \( S \)), but on the other hand, only the sup-norm of a vector needs to be controlled.  
- Section 3.1: This section appears redundant, as the results are identical to those in Section 2.1. I suggest removing Section 3.1 to make room for additional comments and the inclusion of the missing references. For example, further discussion on the differences between the deterministic and random cases could be explored.  
- Line 408: Did the authors attempt to improve the term involving \( \exp(-t_0/2) \)? This term seems to be the weakest part of the probability control in Theorem 3.4.  
- Lines 441/443: The spelling of "Cand√®s" is inconsistent. It is written as "Candes" and "Cand\'es," but it should be "Cand\`es."  
- Line 553: There appears to be a sign error in this line.  
General questions:
- Could the authors comment on the fact that their procedure requires the sparsity level to be known a priori (i.e., there is no adaptivity)?  
- When is \( T_0 = S \) (line 267)? It would be helpful to understand the conditions under which these two sets are identical.  
Overall assessment:  
The paper is clear, precise, and highly relevant for both statisticians and practitioners.