Review - Optimization, Learning, and Games with Predictable Sequences  
Summary:  
The paper explores Optimistic Mirror Descent (OMD), a variant of the classical algorithm for online convex optimization that excels when the sequence of loss function gradients exhibits predictable regularity that the learner can leverage.  
The authors first present the specific substitution under which OMD simplifies to Mirror Prox and extend the known bounds to Hölder-smooth functions. The paper then transitions to analyzing objectives with a saddle-point structure. It begins by addressing plain min-max optimization using a single algorithm and subsequently adopts a game-theoretic perspective. The authors demonstrate how two opponents can achieve faster convergence to a saddle point when both employ the proposed algorithm, all while maintaining the worst-case √T regret in the absence of cooperation. The paper concludes with an exploration of approximate convex optimization, including an application to approximate max flow.  
Discussion:  
This paper on leveraging predictable sequences is well-crafted and comprehensive. The exposition is exceptionally clear, and the results are robust. I particularly appreciate the application to saddle-point finding, where the absence of cooperation is assumed, but a speedup is observed when cooperation occurs.  
Minor points:  
- Page 5, Proposition 6: The notation references x0 and f0, but these are not defined.  
- Page 7, Equation (11): The notation x[i] should be corrected to x_i.  
This excellent paper delves into prediction with predictable sequences (of gradients) and achieves significant results with compelling technical applications.