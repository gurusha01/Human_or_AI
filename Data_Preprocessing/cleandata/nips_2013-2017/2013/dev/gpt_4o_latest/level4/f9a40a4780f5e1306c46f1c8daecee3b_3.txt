This paper introduces a clear and practical enhancement to existing entropy estimators for multidimensional binary distributions. Estimating entropy (and related metrics such as mutual information) plays a crucial role in deepening our understanding of the neural code. The study is particularly relevant given the growing prevalence of multi-electrode recordings, whose interpretation depends on improved analytical tools like the one proposed here.
The proposed method leverages the sparsity of population responses, especially when analyzed over short time intervals. In such cases, the most common pattern is typically one where no neurons fire, with the frequency of patterns decreasing rapidly as the number of spikes (or ones) in a pattern increases. By integrating this prior knowledge into their Bayesian estimator, the authors develop a method that achieves a bias comparable to existing estimators while requiring several orders of magnitude less dataâ€”a critical advantage for empirical research.
In a practical application of their method to real data, the authors report that retinal ganglion cells exhibit "synergistic coding" over time. While this finding is intriguing, the analysis in this section would benefit from further elaboration to provide more robust and compelling insights.
The derivation and results are presented in a clear and accessible manner, and I have only minor suggestions for improvement.  
[deleted]  
This work represents a significant advancement in entropy estimation techniques, achieved through the use of a more sophisticated prior compared to previous approaches.