In this paper, the authors present efficient methods to compute non-linear kernels between non-negative vectors, with a particular focus on chi_2 kernels.
The work builds on the established concept that the dot product <x, y> of two vectors can be derived as the cosine of the collision probability P(sign(xj) ≠ sign(yj)), where xj and yj represent the dot products of x and y with a shared vector aj. The coordinates of aj are randomly sampled from a stable distribution with scale parameter α=2. The authors explore what happens when α < 2 and investigate whether meaningful approximations of other kernels can be derived. After providing a motivating example in Section 2 and some preliminary results in Section 3, the authors introduce their main objective in Section 4 and, in Section 5, propose two formulas to approximate the collision probability as a function of the chi_2 similarity.
I found this paper interesting and believe it contains enough novel contributions to merit consideration for NIPS. However, I have several concerns that prevent me from fully endorsing it. Fundamentally, the paper feels disorganized. It appears that the authors themselves are still grappling with the material and are uncertain about the most effective structure for the paper. Phrases like "(perhaps surprising)" and the simplicity of some results (e.g., upper bounds) suggest that the work is still in progress. This lack of clarity makes the paper challenging to follow. Below are some specific issues:
- The authors justify their focus on chi2 similarity by noting its popularity for comparing histograms. However, they then use time series data (UCI-PEMS) with non-normalized measurements as a primary example, which seems inconsistent. Additionally, the dataset is relatively small (440 points), which may not be sufficient to support their claims. Furthermore, the introduction of the "acos-χ2" kernel on line 82 lacks sufficient context, leaving the reader confused. I recommend removing Section 2 and instead focusing on the experimental results toward the end of the paper. As a side note, the reported accuracies on MNIST are quite low, and it would be interesting to see how a Hellinger kernel or Gaussian kernel would perform in comparison.
- The paper is structured around demonstrating how chi2 similarity can approximate collision probability. However, in practical applications, the flow often works in the opposite direction: empirical collision probabilities are computed to approximate chi2 similarities. This raises questions about the utility of the second approximation in Equation 12 for machine learning, especially without a clear inversion formula for the arctan integral.
- The paper has the feel of a work-in-progress. Including an explicit algorithm summarizing the authors' key contribution could greatly improve clarity and readability.
- Lemma 4 is presented without proof, and the authors' suggestion to proceed by analogy with the proof of Lemma 2 is unconvincing. In Lemma 2, the correspondence between features, feature products (collisions), and the kernel 1 - 1/π acos(p2) was exact when taking the expectation. However, for pχ2, no such exact formula exists, as the authors themselves acknowledge. Therefore, it is necessary to prove that, because ρχ2 is a positive semi-definite (psd) kernel, 1 - 1/π acos(ρχ_2) is also psd. This is not "trivial" and requires a formal proof. Using Corollary 1.14 from Chapter 3 (p.70) of the Berg/Christensen/Ressel reference ("Harmonic Analysis on Semigroups") should suffice, as it only requires showing that acos is holomorphic with positive coefficients. The following reference may also be helpful:  
http://mizugadro.mydns.jp/t/index.php/ArcCosRangeofholomorphism  
As it stands, Lemma 4 remains a conjecture.
Update after rebuttal:  
The authors' response regarding the positive semi-definiteness of the acos-χ2 kernel did not convince me. Their proof is unclear. As far as I understand, the feature representations for the χ2 kernel are infinite-dimensional. The authors then propose using random Gaussian approximations on these infinite-dimensional vectors, but I find this approach difficult to follow. This part of the paper requires a more rigorous and transparent explanation.  
Overall, this paper presents an intriguing attempt to generalize simhash (α=2) to stable distributions with α < 2 and to explore the resulting implications. While some of the ideas are promising, the presentation needs significant improvement to meet the standards of NIPS.