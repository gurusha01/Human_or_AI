AIS has emerged as a crucial tool for assessing the performance of learning algorithms for MRFs. Typically, practitioners employ geometric averaged models as intermediate models. This paper poses an intriguing question regarding the possibility of identifying a superior alternative to the standard annealing path. It centers on the bias of the expected log-weight, which is monotonically linked to the variance of the AIS output, and examines the asymptotic behavior of various annealing paths.
Overall, I find this paper to be well-written. The explanations are clear, and the examples provided—such as the two paths between Gaussian distributions in Figure 1 and the visualization of intermediate RBMs in Figure 4—are particularly illustrative. Additionally, the analysis appears robust (although I did not verify the proof provided in the supplementary material).
My primary concern pertains to the process of obtaining the moment-averaged models. The authors employ PCD with a relatively short training duration for moment matching. How sensitive is the AIS outcome to the accuracy of the moments of the target distribution and the parameters of the intermediate models? Furthermore, how does the total computational cost of the proposed method compare to that of the geometric averaging approach?
In the geometric averaging path, it has been observed for RBMs that allocating more models at the low-temperature end often results in significantly lower variance compared to a linear schedule in practice. Was this phenomenon considered or evaluated in the experiments?
This paper introduces an alternative to the conventional geometric average annealing schedule for AIS and demonstrates reduced variance and increased effective sample size in the output weights. The paper is well-written, and the results are compelling.