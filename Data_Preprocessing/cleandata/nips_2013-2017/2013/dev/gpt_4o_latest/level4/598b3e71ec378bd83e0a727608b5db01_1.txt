The paper investigates the feasibility of parallelizing the best arm identification problem. Specifically, it explores how and at what rate speed-up can be achieved by running \(k\) instances of an algorithm in parallel, provided these instances are allowed to share limited information. The information sharing mechanism involves each instance broadcasting some data to the others (typically at the end of its execution), with the constraint that the number of broadcasted bits is limited (usually of size \(\tilde{O}(n)\) per instance).
The authors benchmark their algorithms against three approaches: the full-communication method, the no-communication method, and the majority vote (MV) method, where the latter aggregates the individual recommendations of the instances. The full-communication method generates excessive network traffic, the no-communication method achieves no speed-up, and, as the authors argue, the MV method also fails to achieve speed-up. In contrast, the proposed algorithm achieves a speed-up of \(\sqrt{k}\), which the authors prove to be optimal under the constraint of a single communication round.
Additionally, the authors demonstrate that allowing multiple rounds of communication introduces a trade-off between the achievable speed-up (up to \(k\)) and the amount of information exchanged.
---
Evaluation
The paper is well-written overall (with some exceptions noted below), the proposed solutions are both simple and innovative, and the problem tackled is non-trivial and well-motivated (e.g., by the distributed computation model of MapReduce).
However, there are two key points that require clarification:
1. Definition of "speed-up" and formalization of goals: The authors should explicitly define what they mean by "speed-up" and clearly state their objectives. For instance, it is intuitive to consider speed-up in the following sense: if an algorithm \(A\) achieves confidence \(1-\delta\) in time \(T\), while a distributed version achieves confidence \(1-\delta'\) in the same time \(T\) (with \(\delta' \ll \delta\)), this could be interpreted as a form of speed-up. After all, achieving the same confidence \(1-\delta'\) with the stand-alone version of \(A\) might take significantly longer than \(T\). Under this interpretation, the MV method would indeed achieve a speed-up: if \(T\) suffices for \(A\) to output the best arm with probability \(2/3\), then running \(k\) instances of \(A\) in parallel and taking the majority vote would yield a probability of success approaching 1 as \(k \to \infty\).
2. Performance of the MV method: The authors' treatment of the MV method requires more nuance. For the MV method to succeed (in expectation) with at least half of the instances returning the best arm, it is true that the individual algorithms must output the best arm with probability at least \(1/2\). However, this condition is unnecessarily strict. In fact, it suffices for the individual algorithms to output the best arm with a probability higher than that of any suboptimal arm. This distinction should be addressed and clarified.
---
Additional Questions
1. In the second line of the proof of Lemma 3.3, why is there a factor of 6 in front of \(H/\sqrt{k}\)?
2. In the second line of the proof of Lemma 3.4, shouldn't \(12n\) in the logarithm be divided by \(\Delta^\star\)?
---
In summary, the problem addressed by the paper is both significant and challenging, and the proposed solutions are elegant and insightful. However, the authors need to clarify their definition of "speed-up" and provide a more careful analysis of the MV method's performance. Additionally, minor issues in the proofs of Lemmas 3.3 and 3.4 should be resolved.