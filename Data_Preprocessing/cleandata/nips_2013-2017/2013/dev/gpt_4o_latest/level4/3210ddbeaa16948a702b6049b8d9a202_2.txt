This paper investigates a novel hashing scheme that establishes a relationship between the chi-squared similarity of two high-dimensional vectors, x and y, and the Hamming distance between the sign vectors of their randomly projected versions, sign(xR) and sign(yR). Here, R is a projection matrix whose elements are sampled from an alpha-stable distribution. The authors provide a theoretical result showing that the chi-squared similarity measure is lower-bounded by the cosine of the (normalized) Hamming distance between the sign vectors. Additionally, for the special case where alpha=1 and the projection matrix is i.i.d. Cauchy, they empirically demonstrate that this bound serves as a good approximation of the actual similarity measure. Leveraging this relationship, the authors achieve competitive performance compared to existing kernel-based methods for classification.
The ideas presented in this paper are intriguing. As highlighted in the conclusions, the chi-squared similarity measure has potential applications in various domains, and an efficient method to compute this measure could prove highly valuable. However, I have several concerns that the authors may wish to address:
- The paper would benefit from more careful editing. While much of the content focuses on bounding the collision probabilities of the dimensionality-reduced representations, the primary emphasis (particularly in practical contexts) appears to be on the chi-squared similarity measure. This central message is not clearly emphasized until Section 7. Furthermore, the experimental results are scattered throughout the paper, disrupting its overall flow.
- A notable feature of many hashing schemes in this area (e.g., sim-hash, LSH) is their ability to provide both lower and upper bound-type relations between pairwise metrics in the original high-dimensional space and the Hamming space. Consequently, this scheme cannot be described as a rigorous similarity estimation technique in the same vein as prior approaches. It would be helpful if the authors could discuss the conceptual challenges in proving the complementary bound.
- While the proposed approach is space-efficient, the authors do not explicitly address its time efficiency for the case where alpha=1 and the random projection matrix is Cauchy. For alpha=2, it is well-known that fast transforms exist for dimensionality reduction, but this aspect is not explored for the Cauchy case.
- Finally, the authors should normalize the x-axis to consistent limits in Figures 2 and 3. The differing visualization scales make it challenging to assess the empirical tightness of the proposed bounds.
--------
Post-rebuttal edit: I will maintain my original score. My overall impression of the paper remains positive, as it introduces interesting and novel ideas with potentially significant impact. However, I share several concerns raised by other reviewers: (i) The paper requires a thorough re-organization to improve clarity. (ii) There are notable gaps in the narrative, particularly concerning the relationship between the chi-squared similarity and the approximations presented in Theorem 2 and Lemma 5, which need to be addressed. (iii) The experimental results should be better organized, clearly explained, and made more compelling for machine learning practitioners. 
This is an interesting paper that proposes an efficient method for estimating the chi-squared similarity between high-dimensional vectors by measuring the Hamming distances between the signs of their alpha-stable random projections. While the paper falls short of establishing a fully rigorous metric equivalence between the original space and its low-dimensional embedding, the extensive empirical evaluations help mitigate this limitation.