The paper proposes a method to "retain" prior experiences in solving a reinforcement learning (RL) problem and leverage them in subsequent runs with varying reward functions. A key aspect of the approach is the assumption of constrained learning capacity, which motivates the use of a surrogate reward function to enable the agent to effectively learn the actual reward. The core idea involves equipping the agent with the ability to internalize an intermediate reward function that is derived from the external reward function.
The concept is both innovative and appealing, and the implicit adoption of a "bounded rationality"-like assumption underlying the algorithm (though the authors prudently avoid explicitly framing it this way) makes the method a valuable contribution to a more realistic and applicable perspective on reinforcement learning in general settings.
While the paper is generally well-written, the reviewer found that clarity diminished in the discussion of the network routing domain. Specific issues are outlined below.
From a methodological standpoint, the work aligns with the broader "reward-shaping" framework. The demonstrated success of the approach in the provided examples is compelling, though it should be noted that this methodological class is already transitioning from a nascent stage to a more established and consolidated domain.
- Line 323: Could you clarify what is meant by "trajectory-count" parameters for UCT? Does this refer to the number of sample runs?
- Line 332: There appears to be an inconsistency between the text and Figure 4 regarding the colors, coefficient signs, or the semantics of the coefficients. The text states: "negative/dark/discouraging exploration," but this does not align with the figure.
- Line 370: The purpose of the decomposition into G1, G2, and G_3 is unclear. Could you elaborate on its significance?
- Line 403: The modeling of the transition function is not well-explained. Are you no longer using reward mapping at this stage? If reward mapping is still employed alongside modifications to the transition function, how are these changes implemented? This section requires rephrasing for better clarity.
- Line 418: What exactly is the "competing policy transfer agent"? What model underpins it?
Overall, this is an intriguing approach to transfer learning under resource constraints. While situated within the established "reward-shaping" methodology, the method demonstrates sufficient originality and efficacy to merit publication. However, there are minor weaknesses in the explanation of the second example that should be addressed.