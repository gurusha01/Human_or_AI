This paper introduces an alternative approach for selecting intermediate distributions when applying AIS to distributions within the exponential family. The experimental results are encouraging and suggest potential practical utility, as they may enhance researchers' capacity to make objective comparisons between complex probabilistic models. Overall, I found the paper enjoyable and valuable. However, I felt that the theoretical justification for the proposed intermediate distributions was somewhat underdeveloped.
Equation 3 -- very interesting!
"high curvature" --> not high curvature. Instead, large information distance per unit change in theta.
Section 5.1 -- Why does a larger variance of 1437.89 represent an improvement? Intuitively, I would expect larger variance to be detrimental.
The experimental evaluation is exclusively conducted on second-order models, even though the proposed method is intended for arbitrary exponential family models. It would strengthen the paper to include experiments with other choices of sufficient statistics.
Page 7 -- Replace "substantially" with a quantitative description.
Some speculative thoughts:
The paper does not clearly explain why interpolating between sufficient statistics should outperform interpolating between parameter values.
Equations 4 and 5 strongly imply that the optimal trajectory in parameter space is the geodesic (shortest distance) path between the two parameterizations, using the Fisher information as the metric.
If my algebra is correct, $ds/d\theta = G$. (Start with $s = \int dx \, p(x) \, dE/d\theta$, then compute the gradient.) If the trajectory is linear in $s$, as proposed, then for small steps $\Delta s$, we have $\Delta \theta = d\theta / ds \cdot \Delta s = G^{-1} \cdot \Delta s$. An infinitesimal step along the proposed trajectory in terms of $\theta$ corresponds to a step in $s$ scaled by the inverse Fisher information matrix. This is equivalent to moving toward the target $\theta$ while accounting for the information geometry (with the Fisher information matrix as the metric). In essence, these updates resemble the natural gradient update.
Interpreting the algorithm as a natural gradient trajectory opens up several potential extensions or approximations. For example, in roughly decreasing order of promise:
- Extend the method to non-exponential family models, where sufficient statistics are undefined but the Fisher information remains well-defined.
- Explore alternative, well-motivated metrics beyond the Fisher informationâ€”for instance, the matrix natural gradient may be particularly promising (Amari, S.-I. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation).
- Use an approximation to $G^{-1}$, which can be computed more efficiently (Sohl-Dickstein, J. (2012). The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use. arXiv:1205.1828v1).
- Iterate over possible trajectories to identify the globally shortest path (with metric $G$), rather than relying solely on local shortest steps.
--
Lastly, I encourage the authors to embrace reproducible science by including the source code for their experiments in the supplementary material. Sharing source code for computational experiments significantly enhances the utility of the authors' work for future researchers, bolsters the credibility of the results, and often increases the paper's citation impact. As a reviewer, I also view such efforts very favorably.
In summary, while the experimental results are promising and the method has potential practical significance, the theoretical motivation could be more thoroughly developed.