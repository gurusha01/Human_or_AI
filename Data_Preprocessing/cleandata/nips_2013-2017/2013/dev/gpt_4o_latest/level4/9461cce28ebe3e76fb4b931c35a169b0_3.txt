This paper introduces a boosting approach that directly minimizes the empirical classification error, defined using the indicator function for whether the predicted label matches the observed label, commonly referred to as the 0-1 loss. The proposed method employs a two-step process: first, a greedy coordinate descent is used to identify the coordinatewise minimum loss, followed by coordinate ascent to expand the margin. The method is innovative and offers certain advantages; however, several concerns arise:
1. The authors appear to assume a finite hypothesis space, as Algorithms 1 and 2 iterate over all hypotheses \( h \) at each step. For example, in Algorithm 1, all weak learners that yield the largest reduction in classification error are identified at each iteration. How would the method handle hypothesis spaces that are not finite, such as spaces of linear functions?
2. At the conclusion of Algorithm 1, the weight of the weak learner with the smallest exponential loss is updated. What is the justification for this step? Does this not effectively revert to using a convex loss function, despite the earlier use of the 0-1 loss in the algorithm?
3. The method assumes that the data is separable, provided the combined strong classifier is sufficiently strong. What happens if this assumption does not hold? Specifically, what if the chosen hypothesis space lacks the complexity to separate the dataset? The paper claims that Algorithm 1 achieves a coordinatewise minimum and that the 0-1 loss reaches zero. What exactly is meant by "coordinatewise minimum"? Does this imply a global minimum, given that the lowest possible error is zero?
4. When the second step (margin maximization) begins, it starts from a region constrained to remain within the zero-loss region, determined by the value of \( d \). However, the method for calculating \( d \) is not clearly explained, beyond the statement that it is determined by "the lowest sample whose margin is decreasing."
5. Since the method assumes separable cases, the margin is always positive in the formulation. What happens in scenarios where negative margins arise due to inseparable data? Would the second part of the algorithm fail to execute in such cases?
6. On page 2, the paper states that "it might escape the region of minimum training error in order to achieve a larger margin." However, in a later section, the design of Algorithm 2 ensures that the margin maximization step does not leave the zero 0-1 loss region. This apparent contradiction requires clarification.
7. Assuming that weak learners can be obtained at a similar computational cost, DirectBoost appears to be more computationally intensive per iteration compared to AdaBoost. In the experiments, what stopping criterion was used for AdaBoost? Why does AdaBoost require so many iterations in the reported results?
8. The paper does not clearly articulate a compelling advantage of the proposed method over standard boosting methods.
In summary, this paper presents a boosting method that directly minimizes the empirical classification error based on the 0-1 loss. While the method is novel and interesting, the concerns outlined above warrant further clarification and discussion.