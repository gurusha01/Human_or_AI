Paraphrased Review:
Summary:  
The manuscript is structured into two main sections. In the first section, the authors highlight the benefits of principal component regression (PCR) compared to traditional linear regression. After introducing the problem, they present novel theoretical findings that explicitly demonstrate PCR's robustness to collinearity and its ability to leverage low-rank structures in the covariance matrix. These results are framed in a setting where both the dimensionality \(d\) and the sample size \(n\) grow. The second section introduces a new PCR algorithm designed for scenarios where \(d > n\) and the predictors \(X\) follow an elliptical distribution. The proposed method is straightforward: replacing the sample covariance matrix with Kendall's tau accommodates elliptical distributions (building on recent work by Oja), while a sparsity constraint addresses the high-dimensional case where \(d > n\). The authors validate the advantages of their method using both simulated and real-world datasets.  
Quality:  
The primary theoretical contributions (Theorems 2.2 and 3.3) are technically robust and represent significant advancements. The simulated and experimental results, presented in Figures 1, 2, and 3, further enhance the paper's overall impact.  
One notable concern is the reliance on the principal component model (Equation 2.1) in the first section and the assumption in Equation 3.5 in the second section. Prior works by Artemiou and Li (2009) and Cook (2007), both cited in this paper, emphasize PCR's advantages over linear regression in more general contexts. However, this paper explicitly assumes that the regression coefficient aligns with the first principal component, which does not encompass all scenarios where PCR outperforms linear regression, as suggested by these references. Consequently, it is unclear whether the results in Figures 1 and 2 are non-trivial or if PCR is merely outperforming linear regression due to the assumed principal component model.  
The application to real-world data is a strong aspect of the paper, and the positive results in Figure 3 partially mitigate the above concern. However, the authors only analyze a single dataset, raising questions about whether RPCR's performance depends on specific analysis choices, such as their focus on the financial subcategory within the dataset.  
Clarity:  
The manuscript is well-written, with an excellent organizational structure that makes it easy to follow. However, the presentation of the main theorems (2.2 and 3.3) is less clear, likely because their proofs are relegated to the supplementary materials. Certain assumptions, such as the condition in Theorem 2.2 (\(r^*(\Sigma)\log d / n = o(1)\)) and the constraints in Theorems 3.2 and 3.3, are not adequately explained or intuitively justified. Providing more exposition on the theorems themselves, rather than solely focusing on their implications, would improve clarity.  
Originality:  
The paper's originality primarily stems from its two main theoretical contributions, Theorems 2.2 and 3.3. While the development of the RPCR algorithm heavily relies on recent advances, the integration of these results into a novel algorithm adds to the paper's overall originality.  
Significance:  
The two main theorems are important contributions to the field. As the authors note, Theorem 2.2 is the first explicit characterization of certain observations in PCR. Additionally, the newly proposed RPCR method shows potential for broader adoption in future research.  
Other Notes:  
- Line 43: \(R^{n \times d}\) should be \(R^{d \times d}\).  
- Line 358: The first \(wd\) should be \(w3\).  
- Line 361: The definition of \(m\) is missing, but it seems \(m = 2\) in this context.  
- There is limited discussion regarding the constant \(\alpha\). For instance, linear regression outperforms PCR for large \(\alpha\). Additionally, was simplicity the sole reason for setting \(\alpha = 1\) in the simulation studies?  
This paper makes significant contributions to the understanding of principal component regression and is well-presented overall. However, the results would be more compelling with a stronger justification for the assumption of a principal component model and a more comprehensive analysis of real-world datasets.