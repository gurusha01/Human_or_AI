The authors investigate an agent designed to undertake a sequence of sequential decision-making tasks throughout its lifetime within the same or a similar environment. They introduce a method for transferring knowledge gained from previous tasks to the current task, specifically focusing on transferring knowledge related to learning optimal reward functions. The utility of the proposed approach is demonstrated through two illustrative examples.
This work builds on two key prior contributions. The first is the optimal rewards framework introduced by Singh et al. (2010), which defines an internal reward function aimed at maximizing external reward. This internal reward function may differ from the external reward function, particularly when the agent is constrained in its capabilities, such as having a limited planning horizon. The second is the incremental learning algorithm proposed by Sorg et al. (2010), which enables learning of such internal reward functions in a single-task setting.
The primary contribution of this paper is extending these earlier ideas to a multi-task setting. Similar to Sorg et al. (2010), the agent learns an internal reward function for each task. Additionally, the agent constructs a mapping from the external reward functions of previously encountered tasks to the internal reward functions it learned by the end of those tasks. This mapping is then used to initialize the agent's internal reward function at the start of each new task.
While the examples provided are relatively small in scale, they effectively illustrate the potential utility of the proposed approach. However, the practical success of the algorithm will hinge on the availability of high-quality features for constructing the mapping between external and internal rewards.
The paper is well-written and easy to comprehend. The empirical results are well-executed, insightful, and valuable. Figure 4, in particular, provides a clear and concrete visualization of the algorithm's behavior. Including a similar figure or detailed description for the network example would enhance the paper further. 
Although the paper is not highly innovative, it represents a well-executed and meaningful contribution to the field of transfer learning.