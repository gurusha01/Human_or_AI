The authors utilize the Optimistic Mirror Descent algorithm to address a range of problems where smoothness introduces a degree of predictability in the sequences of the loss functions. Specifically, improved rates are achievable under two scenarios: 1) when the objective function \( G \) is fixed across all \( t \) and exhibits smoothness, or 2) when the problem involves a saddle point on a function \( \phi(f, x) \) that is individually smooth in each of its arguments, and the optimistic mirror descent algorithm is applied in self-play. While these are fairly specialized forms of predictability, they yield some intriguing results. Section 4 extends these findings to zero-sum games, and Section 5 explores approximate smooth convex programming.
In contrast to many typical applications of online algorithms, the primary objective in most cases presented in this paper is to solve a batch optimization problem. This distinction should be made more explicit in the introduction.
Section 3: The authors need to clarify why this section offers an improvement over the result in Lemma 3. The main insight (presumably) is that the smoothness conditions in Corollary 5 can hold even if \( G(f) = \inf_x \phi(f, x) \) is not smooth. An alternative approach to addressing the problem in Section 3 would be to run an online algorithm against a best-response adversary. It is worth investigating whether any online algorithm paired with a best-response oracle could achieve convergence rates similar to the \( 1/T \) rate derived in this paper. (This may not be feasible, as it could undermine the predictability assumptions.)
Section 4: Greater clarity is needed on how this section builds upon the results of Section 3. The key contribution appears to be the addition of robustness against arbitrary adversaries, while still maintaining strong bounds for predictable ones. However, the results in Section 4.2 are somewhat weaker than the section title suggests. It would be more accurate to describe this setting as multi-point bandit feedback and to cite "Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback." Given the notational similarities, it seems likely the authors are familiar with this work, and it should have been cited in the submission. Additionally, the need for 4 points (rather than just 2) is surprising and warrants further discussion.
Section 5: It is unclear whether the assumption that \( F^ \) is known can be made without loss of generality. At a minimum, the binary search procedure requires more precision: Are the authors running the algorithm from Section 5 at each step of the binary search? If so, how is it determined whether the current guess for \( F^ \) is too high or too low? Furthermore, how does an error of \( \delta \) in the guess for \( F^* \) affect the final bound in Lemma 8? These concerns also extend to Section 5.1.
Additionally, Equation (11) seems to align with the setup in Section 3 (Corollary 5). Why is the machinery from Section 4.2 (Lemma 7) necessary here? Is Line 368 a typographical error?
Overall, the paper contains several typographical errors that hindered the review process. It would also benefit from a more thorough discussion of related work, with specific citations recommended below.
Local points:
- Line 112: The simple intuition and analysis for Mirror Prox are commendable.  
- Line 93: This bound can be interpreted as a direct generalization of the result that the "Be The Leader" (BTL) algorithm incurs no regret, as the BTL algorithm is obtained by setting \( \Deltat = Mt \). This should be mentioned, along with a citation to "Efficient algorithms for online decision problems" (Adam Kalai and Santosh Vempala).  
- Line 131: The section title "Saddle-Point Optimization" would be clearer.  
- Lines 154-155: Citing "Adaptive game playing using multiplicative weights" (Yoav Freund and Robert E. Schapire) is appropriate here, as well as at lines 208-209.  
- Line 223: Provide more detail on why Section 3 does not yield this result (is it because it cannot handle arbitrary adversaries?), and why a direct application of Mirror Prox does not achieve strong decoupling.  
- Lines 225, 265, 268: Referring to regret bounds as \( 1/T \) or \( 1/\sqrt{T} \) is imprecise; these should be multiplied by \( T \) to align with the definition of regret on Line 65. Alternatively, describe these as convergence rates.  
- Lines 312-314: There are typos in both "Build estimates" sections of the pseudo-code. The correct expression should involve \( r^+ - r^- \); as written, all \( a \)'s and \( b \)'s are zero.  
- Line 366: \( \epsilon \) has not been defined yet. Introduce it earlier as an arbitrary constant \( \epsilon > 0 \).  
- Line 368: Do you mean Corollary 5 instead of Lemma 7? If Lemma 7 is indeed intended, the procedure requires clarification, e.g., what is the matrix \( A \) here?  
- Appendix: The proofs rely heavily on copy-pasting, which makes them unnecessarily verbose and harder to follow. Restructure the proofs to reduce repetition and highlight the main arguments more clearly.
The paper presents novel and interesting applications of the Optimistic Mirror Descent algorithm, including a simple analysis for fixed smooth functions and an analysis for self-play of the algorithm. However, the presentation could be improved in several areas to enhance clarity and rigor.