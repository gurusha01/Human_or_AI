Review - Summary  
=======  
Annealed importance sampling (AIS) relies on a sequence of intermediate distributions bridging an initial distribution and a target distribution. While prior AIS applications predominantly utilized paths in parameter space based on geometric averages of the two distributions, this paper introduces an alternative approach using paths derived from moment averaging. The authors substantiate their method with both theoretical arguments and empirical evidence, highlighting its potential advantages.  
Quality  
=======  
The paper provides a thorough and rigorous analysis of the moment averaging (MA) path. The theoretical claims appear to be well-founded, and I did not identify any errors in the provided proofs, aside from minor typographical issues.  
The empirical evaluation on intractable models employs a linear annealing schedule for the geometric averaging (GA) path. Given that prior studies (e.g., Neal, 1998; Salakhutdinov & Murray, 2008) have demonstrated the effectiveness of alternative schedules, it would be valuable to explore how GA performs with these schedules on larger RBMs. This is particularly relevant in light of Figures 2 and 3, which suggest that the choice of schedule may not significantly impact performance. Using these alternative schedules, which allocate more time near the target distribution, could potentially yield more plausible samples in Figure 4.  
Clarity  
=======  
The paper is clearly written and well-organized, making the main text easy to follow.  
The supplementary material, however, could be improved by explicitly detailing a few more calculations, such as the Fisher information matrix used in the integral at the top of page 4.  
Originality  
===========  
This work represents a highly original contribution. As noted by the authors, nearly all prior AIS applications have relied on geometric paths, underscoring the novelty and technical complexity of the proposed alternative.  
Significance  
============  
AIS is a widely used method in statistical machine learning. While the MA approach appears effective for estimating partition functions in certain models (e.g., RBMs), applying it to more complex models may pose challenges. Improved annealing strategies, such as the one proposed here, have the potential to address these challenges and enable better model comparisons. Additionally, as the authors note, many sampling methods critical for inference and learning could benefit from advancements in annealing strategies.  
Although I am skeptical that the proposed strategy will achieve widespread adoption—primarily due to the added complexity of estimating intermediate distributions and the observation that GA can still compete with or outperform MA in terms of ESS for intractable RBMs—I believe this work has significant potential to inspire further research in this area.  
Minor Comments  
==============  
– Supp. 1: Consider using $\theta0 = \theta$ instead of $\theta = \theta0$.  
– Supp. 2: It might be clearer to use $pa$ and $pb$ in place of $p0$ and $p1$ throughout.  
– Supp. 2.1: The first term of the Lagrangian should be written as $\lambda ( 1 - \sumx q(x) )$ or $\lambda ( \sumx q(x) - 1 )$.  
– Supp. 3.1: The equation $\dot s(\beta) = t$ should instead be $\dot s(\beta) = 2\beta$.  
– Supp. 3.2: A factor of 2 appears to be missing in the derivative of $\lambda(\beta)$.  
This paper is a highly original contribution with the potential to influence future research in learning, inference, and model comparison. While I remain unconvinced of its immediate adoption for partition function estimation, I believe it will serve as a catalyst for further theoretical advancements in this direction.