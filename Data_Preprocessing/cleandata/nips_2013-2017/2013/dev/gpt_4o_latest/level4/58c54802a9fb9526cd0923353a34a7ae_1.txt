This paper introduces a model for transfer learning in which synthetic reward functions are derived from prior tasks and adapted to new tasks to accelerate learning. The approach leverages the relatively recent concept of synthetic reward functions, which enhance the performance of an underlying "utility" reward function, and applies it to the transfer learning context. This application is both novel and intriguing. The experiments are well-designed, comprehensive, and reasonably persuasive.
My primary critique of the paper is its lack of comparison to other transfer methods. The authors' claim that policy transfer is not feasible in their setting is inaccurate; this is precisely why sub-policies, such as options, are often transferred instead of full policies. (On this note, the authors should consider citing Lisa Torrey's work on policy transfer.) That said, omitting this comparison is acceptable in this case since policy transfer and synthetic reward transfer can be used independently, together, or not at all. While such a comparison would add value, it is not essential.
A more significant issue, however, is the absence of a comparison with similar reward shaping techniques. Both synthetic reward functions and reward shaping address the same fundamental problem, with the key distinction being that reward shaping does not alter the final solution, whereas synthetic reward functions do. In my view, the broader comparison between these two approaches (even outside the transfer setting) has been insufficiently explored. This omission is surprising, given that learning a shaping function is straightforward—it is essentially a value function initialization and involves learning a value function. Without such a comparison, I have lingering concerns about the overall validity of the synthetic reward function framework. Including an experimental comparison—where the shaping function is adapted in the same manner as the synthetic reward function—would significantly strengthen the paper. However, I acknowledge that this might be beyond the scope of a single paper, especially since adapting shaping functions could itself be considered a novel (albeit somewhat obvious) contribution.
The paper is well-written and enjoyable to read, though the frequent use of parenthetical citations as nouns detracted from the experience. Additionally, some references were poorly formatted (e.g., inconsistent capitalization of "mdps"). Overall, this paper presents an innovative method for transfer in reinforcement learning domains and is both well-executed and well-articulated. While a stronger experimental comparison to reward shaping methods would have been beneficial, it is not strictly necessary.