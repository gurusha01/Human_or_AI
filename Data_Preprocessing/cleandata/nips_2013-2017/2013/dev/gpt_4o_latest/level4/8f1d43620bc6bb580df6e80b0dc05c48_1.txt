Winner-take-all modules are integrated into an otherwise feedforward architecture and trained using backpropagation. The implementation is straightforward, and the observed performance is remarkable.
The simplicity of the approach and the absence of detailed mathematical analysis or theoretical grounding should not deter its acceptance. This work may represent a significant discovery, with numerous clear opportunities for further analysis. For instance, the activations in such a network are likely to be highly sparse, suggesting that mathematical tools from the study of sparsity could be applied. Additionally, several practical extensions are immediately apparent.
Line 141: Grammar issue, "only subset."  
Incorporates winner-take-all modules into a feedforward architecture, yielding notable performance gains.