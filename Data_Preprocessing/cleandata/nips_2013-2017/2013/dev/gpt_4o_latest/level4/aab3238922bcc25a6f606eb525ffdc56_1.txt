The paper presents a novel approach, termed RDC, for measuring statistical dependence between random variables. This method integrates a copula transformation with a modified kernel CCA that leverages random projections, achieving a computational complexity of O(n log n). Experimental results on both synthetic and real-world benchmark datasets demonstrate its potential for feature selection.
Overall, the paper is well-written, clear, and engaging. The simplicity of the proposed method and its strong experimental performance suggest that it could serve as a valuable complement to existing techniques.
However, the originality of the work is somewhat limited, as it primarily combines established techniques. Specifically, the copula transformation, which ensures invariance to monotonic transformations, has been explored in prior work (e.g., Conover and Iman, The American Statistician, 35(3):124-129, 1981, as well as reference [15] cited by the authors). Additionally, the use of random projections for low-rank kernel matrix approximations is a well-known "trick."
While the methodology appears technically sound, the following points require further clarification:
- The authors emphasize that RDC is significantly faster than kCCA, which they claim has a complexity of O(nÂ³) and takes 166 seconds for 1,000 data points in their experiments. This claim is surprising, as the original kCCA paper by Bach and Jordan describes an O(n) implementation using incomplete Cholesky decomposition, which reportedly processes 1,000 points in 0.5 seconds (more than a decade ago). Since incomplete Cholesky decomposition is a widely used approach for scaling kernel methods to large datasets, similar to the random projection technique employed by the authors, a natural question arises: for large sample sizes, is the random projection approach truly superior to incomplete Cholesky decomposition for performing kCCA?
- The authors note that as the dimensionality \( k \) of the random projections increases, RDC converges to unregularized kCCA. This implies that \( k \) plays a critical role in regularization, analogous to the regularization parameter in kCCA. Are there theoretical or empirical justifications for preferring regularization via \( k \) over the standard kCCA regularization? A potential drawback of relying on \( k \) is that when \( k \) is small, random fluctuations may introduce significant variability in the final score, which is undesirable. Indeed, while RDC satisfies all conditions listed in Table 1, it does not fulfill the property of determinism (i.e., repeated computations yield different values). 
In summary, the paper represents an interesting combination of established ideas, but the comparison with its closest counterpart, kCCA, is neither comprehensive nor entirely fair.