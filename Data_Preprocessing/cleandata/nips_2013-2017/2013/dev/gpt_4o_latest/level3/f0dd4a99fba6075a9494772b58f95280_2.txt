The paper introduces Optimistic Mirror Descent (OMD), a versatile algorithm for online convex optimization that leverages predictable gradient sequences to improve performance. The authors extend the theoretical framework of OMD to Hölder-smooth functions, demonstrating its connection to the Mirror Prox method through a specific substitution. This generalization bridges offline and online optimization techniques and provides tighter regret bounds under smoothness assumptions. The paper also explores OMD's application to saddle-point objectives, including both single-algorithm min-max optimization and game-theoretic scenarios, such as zero-sum games. Notably, the authors show that when both players in a zero-sum game employ OMD, the algorithm achieves faster convergence to the minimax equilibrium while maintaining the standard \(\sqrt{T}\) regret in adversarial settings. Additionally, the paper applies OMD to approximate convex optimization problems, including a simplified algorithm for approximate max flow, achieving comparable complexity to prior methods but with a more straightforward approach.
Strengths:
1. Technical Depth and Novelty: The extension of OMD to Hölder-smooth functions and its connection to Mirror Prox is a significant theoretical contribution. The analysis of saddle-point problems and game-theoretic applications is robust and insightful.
2. Practical Relevance: The application to approximate max flow demonstrates the practical utility of the proposed methods, achieving state-of-the-art complexity with a simpler procedure.
3. Clarity and Organization: The exposition is clear, with a logical progression from theoretical results to applications. The authors provide detailed proofs and explanations, making the paper accessible to experts in optimization and game theory.
4. Significance: The results address important open questions, such as the existence of simple algorithms for uncoupled zero-sum games with fast convergence rates. The work has potential implications for both theoretical research and practical applications in optimization and game theory.
Weaknesses:
1. Minor Presentation Issues: Some notation is undefined on page 5, and there is a typographical error in equation (11) on page 7. These issues, while minor, could hinder readability and should be addressed.
2. Limited Experimental Validation: While the theoretical results are strong, the paper lacks empirical experiments to validate the practical performance of OMD in real-world scenarios, particularly for the max flow application.
3. Scope of Predictability: The discussion on predictability is limited to smooth functions. Exploring broader conditions for predictability, such as non-smooth settings or adaptive methods, could enhance the generality of the results.
Arguments for Acceptance:
- The paper makes significant theoretical contributions by extending OMD to new settings and connecting it to established methods like Mirror Prox.
- The results are impactful, addressing open questions in game theory and optimization.
- The application to approximate max flow is compelling and demonstrates the algorithm's practical relevance.
Arguments Against Acceptance:
- The lack of experimental validation limits the paper's practical impact.
- Minor presentation issues could confuse readers unfamiliar with the topic.
Recommendation:
I recommend acceptance with minor revisions. The paper is a strong theoretical contribution to the field of optimization and game theory, with clear potential for practical impact. Addressing the presentation issues and including a brief discussion or experiment on real-world performance would further strengthen the work.