The paper proposes a novel approach to sparse PCA for estimating multiple components, addressing the limitations of the deflation step in standard methods. By leveraging the Fantope, a convex relaxation of orthogonal matrices, the authors formulate a semidefinite program (SDP) for sparse principal subspace estimation. The proposed method is computationally efficient, utilizing an ADMM-based algorithm, and achieves near-optimal convergence rates under general conditions. The theoretical framework extends to various input matrices, including Kendall's tau correlation, broadening its applicability. While the method builds on DSPCA, it generalizes to higher dimensions (d > 1), introducing the Fantope as a novel constraint in sparse PCA.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous statistical analysis, demonstrating near-optimal convergence rates for subspace estimation without restrictive assumptions like the spiked covariance model. This is a notable advancement over existing methods.
2. Algorithmic Development: The ADMM-based algorithm is efficient and well-suited for the proposed SDP, making the method computationally practical for high-dimensional settings.
3. General Framework: The extension of the theoretical guarantees to input matrices beyond sample covariance, such as Kendall's tau, enhances the method's versatility and relevance to nonparametric and robust statistical applications.
4. Clarity and Writing: The paper is well-written, with clear explanations of the problem, methodology, and theoretical results. The use of the Fantope as a convex relaxation is novel and conceptually appealing.
Weaknesses:
1. Experimental Validation: The experimental section is underwhelming and lacks sufficient evidence to support the proposed method's practical advantages. Key comparisons to existing methods, such as Journee et al., are missing, and there are no phase-transition diagrams to illustrate performance under varying conditions.
2. Limited Novelty: While the use of the Fantope is novel, the method and theoretical results are incremental updates to DSPCA. The orthogonality constraint, while natural in standard PCA, is less compelling in the sparse PCA context and could benefit from further justification.
3. Numerical Comparisons: The paper does not include comparisons with sparse matrix factorization methods or real-world datasets, which would strengthen its empirical claims.
4. Significance: The practical impact of the method is limited by the lack of robust experimental evidence and real-world applications. The theoretical contributions, while solid, may not be groundbreaking enough to significantly advance the state of the art.
Arguments for Acceptance:
- The paper introduces a novel convex relaxation for sparse PCA and provides strong theoretical guarantees.
- The ADMM algorithm is efficient and relevant for high-dimensional applications.
- The work addresses an important problem in sparse PCA and extends existing methods to higher dimensions.
Arguments Against Acceptance:
- The experimental section is insufficiently developed, with no comparisons to key baselines or real-world datasets.
- The contributions, while solid, are incremental and may not represent a significant leap forward.
- The orthogonality constraint, a central aspect of the method, lacks compelling justification in the sparse PCA context.
Recommendation:
While the paper makes meaningful contributions to the sparse PCA literature, particularly in its theoretical analysis and algorithmic development, the lack of robust experimental validation and limited novelty reduce its overall impact. I recommend weak rejection unless the authors can provide stronger empirical evidence and address the concerns about the method's practical significance.