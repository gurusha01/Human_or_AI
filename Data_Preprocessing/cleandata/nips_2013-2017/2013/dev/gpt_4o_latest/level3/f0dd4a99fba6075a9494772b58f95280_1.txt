The paper presents a series of applications of the Optimistic Mirror Descent (OMD) algorithm, leveraging the concept of predictable sequences to achieve improved performance in various optimization and game-theoretic settings. The authors recover the Mirror Prox algorithm for offline optimization, extend it to Hölder-smooth functions, and apply it to saddle-point problems. They also address convergence to minimax equilibrium in zero-sum games and propose a simple algorithm for approximate Max Flow in convex programming. The work is novel and demonstrates the versatility of OMD in structured optimization problems, particularly under smoothness assumptions.
Strengths:
1. Novelty and Contribution: The paper introduces innovative applications of OMD, particularly in fixed smooth objectives and self-play scenarios. The extension to Hölder-smooth functions and the demonstration of faster convergence rates in zero-sum games are significant contributions.
2. Theoretical Insights: The analysis is rigorous, with clear derivations of regret bounds and convergence rates. The results are well-supported by theoretical guarantees, such as the adaptive step-size mechanism and its impact on regret minimization.
3. Practical Implications: The application to Max Flow provides a simpler alternative to existing methods while matching their complexity, showcasing the practical relevance of the proposed approach.
Weaknesses:
1. Clarity: The paper suffers from clarity issues, particularly in Sections 3, 4, and 5. For example, the improvement on Lemma 3 in Section 3 is not well-explained, and the binary search procedure in Section 5 requires more detail regarding its impact on bounds and assumptions about \( F^* \).
2. Related Work: The discussion of related work is insufficient. Key citations, such as those on multi-point bandit feedback and alternative approaches to zero-sum games, are missing. This weakens the contextualization of the contributions.
3. Typos and Presentation: The manuscript contains numerous typos (e.g., Line 368) and errors in pseudo-code (Lines 312-314), which hinder readability. Undefined constants and verbose appendix proofs further detract from the paper's clarity.
4. Section Dependencies: The necessity of Section 4.2's machinery for Equation (11) is unclear, and the robustness claims in Section 4 are weaker than suggested. These points need better justification and refinement.
Arguments for Acceptance:
- The paper provides novel insights into the use of predictable sequences in optimization, advancing the state of the art in both theory and applications.
- The results are theoretically sound and have practical implications, particularly in convex programming and game theory.
Arguments Against Acceptance:
- The clarity and organization of the paper need significant improvement, particularly in explaining key contributions and dependencies between sections.
- The lack of sufficient discussion on related work and missing citations undermines the positioning of the paper within the broader literature.
Recommendations:
1. Improve the clarity of Section 3 by explicitly detailing how it builds on Lemma 3 and discussing alternative approaches.
2. Expand the related work section to include key citations and comparisons with existing methods.
3. Address the typos, pseudo-code errors, and verbosity in the appendix to enhance readability.
4. Clarify the role of Section 4.2's machinery in Equation (11) and provide stronger justification for robustness claims.
In conclusion, the paper presents significant contributions but requires substantial revisions to improve clarity, contextualization, and presentation. With these improvements, it would be a strong candidate for acceptance.