The paper introduces a novel approach to transfer learning by leveraging synthetic reward functions to accelerate learning in sequential decision-making tasks for long-lived agents. Building on the Optimal Rewards Problem (ORP), the authors propose a reward-mapping mechanism that uses guidance reward functions learned in prior tasks to initialize reward functions for new tasks. This approach is demonstrated to significantly improve agent performance compared to conventional methods, including policy transfer and sequential transfer, across two domains: a grid-world environment and a network routing problem.
Strengths:
The application of synthetic reward functions in transfer learning is innovative and well-motivated. The authors effectively argue that reward functions, as a transferable component, can adapt across tasks with varying dynamics, unlike policies or value functions. The experiments are well-designed and comprehensive, systematically comparing the proposed reward-mapping agent with baseline agents and a competing policy transfer method. The results convincingly demonstrate the superiority of the reward-mapping approach, particularly in scenarios with short task durations or changing transition functions. Additionally, the visualization of the learned reward mapping function provides valuable insight into the mechanism's behavior and effectiveness.
The paper is well-written and organized, with a clear exposition of the problem, methodology, and results. The authors also address computational overhead, showing that the reward-mapping agent remains efficient even when considering the additional cost of learning the mapping function.
Weaknesses:
While the paper makes a strong case for the utility of synthetic reward functions, it lacks a direct comparison with reward shaping methods, which are a closely related area of research. This omission raises questions about the broader applicability of the proposed approach. Additionally, the authors dismiss policy transfer as infeasible in their setting without providing sufficient justification or empirical evidence, which weakens their argument. Including a more thorough discussion or experimental comparison with policy transfer methods would strengthen the paper.
Minor issues include overuse of parenthetical citations, which disrupt the flow of reading, and inconsistent reference formatting, which detracts from the paper's overall polish.
Recommendation:
Pros for acceptance: The paper presents a novel and impactful contribution to transfer learning, with strong experimental results and clear writing. The reward-mapping approach is both innovative and practical, with potential applications in various sequential decision-making domains.
Cons for acceptance: The lack of comparison with reward shaping methods and insufficient justification for dismissing policy transfer limit the paper's scope and contextual grounding.
Overall, the paper is a high-quality submission with significant contributions to the field. While addressing the noted weaknesses would enhance its impact, they do not critically undermine the paper's core contributions. I recommend acceptance, with minor revisions to address the clarity and formatting issues.