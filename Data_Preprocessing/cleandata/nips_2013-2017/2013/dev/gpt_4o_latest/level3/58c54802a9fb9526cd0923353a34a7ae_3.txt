The paper presents a novel method for conserving and transferring knowledge in reinforcement learning (RL) through the reuse of reward functions, specifically within the context of long-lived, bounded agents solving sequential tasks. Building on the Optimal Rewards Problem (ORP), the authors propose a reward-mapping approach that incrementally learns a mapping function to initialize internal reward functions for new tasks based on prior experiences. This method is shown to improve agent performance compared to conventional RL approaches and competing transfer methods, particularly in computationally bounded settings.
The concept of reusing reward functions as a transfer mechanism is both original and compelling. Unlike traditional approaches that focus on transferring policies, value functions, or models, this work highlights the potential of reward functions as a more adaptable and generalizable locus for knowledge transfer. The alignment with principles of bounded rationality adds a practical dimension to the study, addressing real-world constraints on computational resources. The experimental results in both the grid world and network routing domains convincingly demonstrate the efficacy of the reward-mapping approach, particularly in scenarios where tasks vary in reward functions, transition functions, or both.
However, the paper has several weaknesses that merit attention. While the methodology is well-grounded, the explanations of certain technical aspects lack clarity. For instance, the trajectory-count parameters (line 323), coefficient semantics (line 332), and the purpose of decomposition (line 370) are insufficiently detailed. Similarly, the modeling of the transition function and its interaction with reward mapping (line 403) is vague, which could hinder reproducibility. The "competing policy transfer agent" (line 418) also requires further elaboration to clarify its role and comparison with the proposed method. Additionally, the network routing domain example, while valuable, is less clearly articulated than the grid world example, making it harder to follow.
Strengths of the paper include its originality, technical rigor, and alignment with ongoing research in transfer learning and bounded rationality. The reward-mapping approach represents a significant contribution to the field, offering a novel perspective on reward shaping and transfer in RL. The results are robust and demonstrate the practical utility of the method across diverse domains.
In contrast, the weaknesses primarily lie in the clarity of exposition and the depth of explanation for certain technical details. Addressing these issues would make the paper more accessible and impactful.
Arguments for Acceptance:
1. The paper introduces a novel and effective method for knowledge transfer in RL, leveraging reward functions in a unique way.
2. The approach is well-motivated, aligns with bounded rationality principles, and demonstrates clear empirical benefits.
3. The results are robust, with convincing performance improvements over baseline and competing methods.
Arguments Against Acceptance:
1. Certain technical details are underexplained, which may hinder reproducibility and understanding.
2. The network routing domain example lacks clarity compared to the grid world example.
3. Some methodological components, such as the trajectory-count parameters and transition function modeling, are insufficiently detailed.
Overall, this paper makes a significant contribution to reinforcement learning and transfer learning research. While minor weaknesses in clarity exist, they do not detract from the originality and impact of the proposed method. I recommend acceptance, contingent on the authors addressing the identified clarity issues.