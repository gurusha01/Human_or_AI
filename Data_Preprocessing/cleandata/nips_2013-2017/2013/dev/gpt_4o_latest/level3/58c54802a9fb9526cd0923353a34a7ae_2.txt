The paper addresses the problem of transferring knowledge in sequential decision-making tasks by reusing reward functions, a novel approach that builds on the Optimal Rewards Problem (ORP) framework introduced by Singh et al. (2010) and the incremental learning of reward functions by Sorg et al. (2010). Specifically, the authors extend these ideas to a multi-task setting, proposing a reward mapping function that translates task-specific reward parameters into internal reward parameters for initializing subsequent tasks. This approach is demonstrated to improve the performance of bounded agents in two domains: a grid-world environment and a network routing problem.
Strengths:
1. Technical Soundness: The paper is technically solid, with a clear theoretical foundation rooted in ORP and incremental reward learning. The use of UCT for planning and PGRD for reward adaptation is well-justified and effectively implemented.
2. Empirical Evaluation: The experiments are thorough, systematically comparing four agent architectures and demonstrating the advantages of the reward mapping approach. The results convincingly show that reward mapping improves performance, particularly in short-duration tasks or when task transitions are significant.
3. Clarity and Presentation: The paper is well-written and organized, with a logical flow from problem definition to results. Figure 4, which visualizes the learned reward mapping function, is particularly illustrative and helps clarify the approach. However, similar clarity could be applied to the network routing example, where the complexity of the domain might overwhelm readers unfamiliar with it.
4. Significance: The work contributes to the transfer learning literature by highlighting the utility of reward functions as a transferable component, especially in scenarios where other forms of transfer (e.g., policies or value functions) may not generalize across tasks with changing dynamics.
Weaknesses:
1. Innovation: While the paper extends existing work to a multi-task setting, the novelty is incremental rather than groundbreaking. The core idea of reusing reward functions and learning mappings builds directly on prior work, and the contribution lies more in execution than conceptual innovation.
2. Feature Dependence: The authors emphasize the importance of good features for reward mapping, but the paper lacks a detailed discussion on how feature selection impacts performance or how robust the method is to suboptimal feature choices.
3. Scalability: The experiments are conducted on relatively small-scale domains. While the results are promising, it is unclear how well the approach scales to more complex environments or higher-dimensional reward spaces.
Arguments for Acceptance:
- The paper is technically sound and provides a well-executed extension of ORP to multi-task settings.
- The empirical results are robust and demonstrate clear advantages of the proposed reward mapping approach.
- The work is a useful contribution to the transfer learning literature, particularly for bounded agents in sequential decision-making tasks.
Arguments Against Acceptance:
- The novelty is limited, as the work primarily builds on existing frameworks without introducing fundamentally new concepts.
- The scalability and generalizability of the approach to more complex domains remain unaddressed.
- The dependence on good feature selection is a potential limitation that is not thoroughly explored.
Recommendation:
Overall, this paper represents a solid and well-executed contribution to the field of transfer learning, albeit with incremental novelty. It is recommended for acceptance, provided the authors address the scalability and feature-dependence concerns in future work.