This paper introduces a novel nonlinearity for neural networks, termed Local Winner-Take-All (LWTA), inspired by biological neural circuits. The LWTA mechanism groups neurons into blocks and retains only the neuron with the highest activation, effectively creating sparse, modular subnetworks. This approach is shown to yield competitive results in classification tasks, such as permutation-invariant MNIST and sentiment analysis, while also mitigating catastrophic forgetting—a common issue in neural networks trained sequentially on different tasks. The authors argue that LWTA networks self-modularize, enabling better handling of multimodal data distributions and improved retention of learned representations.
Strengths:  
The paper presents a simple yet effective idea that is both biologically inspired and computationally grounded. The LWTA mechanism is novel and distinct from related techniques like max-pooling and dropout, offering a unique contribution to the field. The experiments demonstrate that LWTA achieves competitive performance on benchmark datasets without relying on data augmentation or unsupervised pretraining. The results on catastrophic forgetting are particularly promising, suggesting LWTA could be valuable for continual learning scenarios. The paper is well-written and clearly organized, making it accessible to readers with varying levels of expertise. The authors also provide a thorough comparison to related methods, such as ReLU and maxout, situating their work within the broader context of neural network research.
Weaknesses:  
While the LWTA mechanism shows promise, the experimental evaluation is somewhat limited. For instance, the results on permutation-invariant MNIST, while good, do not surpass state-of-the-art methods that incorporate dropout or data augmentation. The authors' dismissal of dropout in input layers as data augmentation seems overly simplistic, and additional experiments incorporating such techniques would strengthen their claims. The catastrophic forgetting experiments, though intriguing, may suffer from methodological issues. Specifically, the reduced forgetting could be partially attributed to LWTA's better recognition performance rather than its inherent ability to retain knowledge. A suggested experiment—training sequentially on subsets of digits—could provide deeper insights into LWTA's behavior in overcoming initial minima and retaining learned representations.
Pro and Con Arguments for Acceptance:  
Pro:  
- Novel and biologically inspired nonlinearity with clear computational benefits.  
- Promising results on catastrophic forgetting, a significant challenge in neural network research.  
- Clear and well-structured presentation of ideas and experiments.  
Con:  
- Limited experimental scope; better results exist for MNIST with standard techniques.  
- Methodological concerns in the forgetting experiments.  
- Lack of exploration of LWTA's performance in more complex or real-world datasets.  
Overall Recommendation:  
This paper introduces an interesting and original idea with potential significance for neural network research, particularly in the context of continual learning. However, the experimental evaluation requires more depth and rigor to fully substantiate the claims. I recommend acceptance contingent on the authors addressing the methodological concerns and expanding the experimental scope in a future revision.