This paper introduces two novel Bayesian entropy estimators, ĤDBer and ĤDSyn, specifically designed for binary neural spike words. By leveraging a mixture-of-Dirichlet prior, these methods incorporate prior knowledge about the statistical structure of spike trains, enabling accurate entropy estimation with fewer samples compared to traditional methods. The work builds on prior contributions by Nemenman et al. (2002) and Okun et al. (2012), extending these approaches with innovative use of count distributions and hierarchical priors. The authors demonstrate the effectiveness of their methods on both synthetic data and retinal ganglion cell recordings, showing substantial performance improvements over existing estimators like NSB and BUB. However, the generalizability of these estimators to other types of neural data remains uncertain.
Strengths
The paper addresses a critical problem in neural coding analyses—accurate entropy estimation in high-dimensional and sparse data settings. The proposed methods effectively mitigate the curse of dimensionality by incorporating prior information, such as the synchrony distribution, into the estimation process. This is a significant advancement over existing Bayesian estimators, which often assume uniform priors that fail to capture the sparsity of neural spike words. The computational efficiency of the proposed estimators, achieved through careful exploitation of symmetries in the Dirichlet and Bernoulli distributions, is another notable strength. The clarity of the manuscript is commendable, with well-structured explanations and illustrative figures that make the methodology accessible. Furthermore, the empirical results convincingly demonstrate the superiority of the proposed methods, particularly ĤDSyn, in both simulated and real-world scenarios.
Weaknesses
While the paper presents strong results for retinal ganglion cell data, its applicability to other neural datasets with different statistical structures is not thoroughly explored. This limits the broader impact of the work. Additionally, while the authors reference prior work adequately, a few citations are missing, and some sentences are incomplete, which detracts slightly from the overall polish of the manuscript. Minor grammatical errors also need correction. Lastly, the reliance on empirical synchrony distributions for ĤDSyn may introduce challenges in cases where the sample size is too small to estimate these distributions reliably.
Pro and Con Arguments for Acceptance
Pro:  
1. Significant methodological contribution to entropy estimation in neuroscience.  
2. Demonstrates impressive performance gains over existing methods.  
3. Computationally efficient and well-suited for high-dimensional data.  
Con:  
1. Limited evidence of generalizability to diverse neural datasets.  
2. Minor issues with manuscript clarity and completeness.  
Recommendation
This paper makes a meaningful contribution to the field of neural coding and Bayesian inference. Despite some limitations in generalizability and minor issues in presentation, the methodological advancements and empirical results are compelling. I recommend acceptance, provided the authors address the minor issues and discuss the potential applicability of their methods to other types of neural data.