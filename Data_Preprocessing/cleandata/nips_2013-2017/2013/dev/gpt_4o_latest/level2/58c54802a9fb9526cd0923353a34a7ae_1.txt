The paper presents a novel approach to transfer learning in long-lived agents by reusing reward functions rather than policies, value functions, or models. The authors propose a reward mapping function that leverages guidance reward functions learned from previous tasks to initialize internal reward functions for subsequent tasks. This approach builds on the Optimal Rewards Problem (ORP) framework, which separates the roles of reward functions into evaluation and guidance. The paper demonstrates the effectiveness of the proposed method through experiments in two domains: a grid-world "Food-and-Shelter" task and a network routing domain. Results show that the reward mapping agent outperforms conventional agents, non-transfer ORP agents, and competing policy transfer methods, particularly in scenarios where tasks vary in reward functions, transition functions, or both.
Strengths:
1. Novelty: The paper introduces a unique perspective on transfer learning by focusing on reward function reuse, which is less explored compared to policy or value function transfer. This approach is particularly compelling as reward functions are more adaptable across tasks with varying dynamics.
2. Theoretical Foundation: The work is grounded in the Optimal Rewards Problem framework, providing a solid theoretical basis for the proposed method.
3. Empirical Validation: The experiments are comprehensive, spanning two distinct domains with varying task complexities. The results convincingly demonstrate the superiority of the reward mapping approach over baseline and competing methods.
4. Practical Implications: The method is computationally efficient, as shown by the analysis of overhead costs, and it generalizes well to multi-agent settings and tasks with changing transition functions.
5. Clarity of Results: The visualizations, such as the learned reward mapping function and performance comparisons, effectively communicate the benefits of the proposed approach.
Weaknesses:
1. Limited Theoretical Analysis: While the empirical results are strong, the paper lacks a deeper theoretical exploration of the conditions under which reward mapping is most effective. For instance, the impact of task similarity on transfer performance could be better formalized.
2. Scalability Concerns: The experiments are conducted on relatively small-scale domains. It is unclear how the approach would scale to more complex, high-dimensional environments.
3. Comparison to Broader Methods: The paper primarily compares its approach to a single policy transfer method. Broader comparisons with other state-of-the-art transfer learning techniques, such as meta-RL or hierarchical RL, would strengthen the evaluation.
4. Reproducibility: While the pseudocode for the reward mapping agent is provided, some implementation details, such as kernel regression parameters, are not fully specified, which may hinder reproducibility.
Pro/Con Arguments for Acceptance:
- Pro: The paper addresses a novel and underexplored aspect of transfer learning, presents strong empirical results, and has practical implications for long-lived agents in sequential decision-making tasks.
- Con: The lack of broader comparisons and limited theoretical insights may reduce the generalizability and impact of the proposed method.
Recommendation: Accept with minor revisions. The paper makes a significant contribution to the field of transfer learning in reinforcement learning, particularly in the context of long-lived agents. Addressing the theoretical gaps and expanding the experimental scope would further enhance its impact.