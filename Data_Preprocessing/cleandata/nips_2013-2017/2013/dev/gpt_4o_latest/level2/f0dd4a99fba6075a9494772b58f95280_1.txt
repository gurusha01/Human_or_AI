The paper presents several applications of the Optimistic Mirror Descent (OMD) algorithm, leveraging the concept of predictable sequences to improve performance in online learning, optimization, and game theory. The authors recover and extend the Mirror Prox algorithm for offline optimization to Hölder-smooth functions, address a question by Daskalakis et al. regarding convergence rates in zero-sum games, and propose a simple algorithm for approximate Max Flow in convex programming. The work demonstrates the versatility of OMD in structured optimization problems and highlights its potential for achieving faster convergence rates under specific assumptions.
Strengths:
1. Novelty and Contributions: The paper makes significant theoretical contributions by extending the Mirror Prox algorithm to Hölder-smooth functions and addressing open questions in game theory regarding uncoupled dynamics in zero-sum games. The application to Max Flow also simplifies existing approaches while maintaining competitive time complexity.
2. Technical Rigor: The paper is mathematically rigorous, providing detailed proofs and regret bounds for OMD in various settings. The results are well-supported by theoretical analysis, and the adaptive step-size techniques are particularly noteworthy for ensuring robustness and flexibility.
3. Breadth of Applications: The paper demonstrates the utility of OMD across diverse domains, including optimization, game theory, and convex programming. This breadth underscores the generality of the proposed approach.
4. Clarity in Key Results: The regret bounds and convergence rates are clearly stated, and the authors provide insightful discussions on how predictable sequences can improve performance compared to worst-case assumptions.
Weaknesses:
1. Experimental Validation: The paper lacks empirical results to complement the theoretical findings. While the theoretical contributions are strong, experimental validation would enhance the practical relevance and demonstrate the real-world applicability of the proposed methods.
2. Complexity of Presentation: The paper is dense and assumes a high level of familiarity with advanced optimization and game theory concepts. While this is appropriate for the target audience, some sections, such as the derivation of regret bounds, could benefit from additional intuition or examples to improve accessibility.
3. Limited Discussion of Limitations: While the authors acknowledge the reliance on smoothness assumptions, the discussion of limitations is brief. For example, the practicality of the adaptive step-size methods in large-scale problems or the challenges of implementing the proposed algorithms in real-world settings could be explored further.
4. Open Problems: While the paper identifies open problems, such as achieving faster rates in partial information settings, it does not provide concrete directions or preliminary insights into how these challenges might be addressed.
Arguments for Acceptance:
- The paper addresses important and timely problems in optimization and game theory, providing novel theoretical insights and practical algorithms.
- The extension of OMD to Hölder-smooth functions and its application to Max Flow are significant contributions that advance the state of the art.
- The work is technically sound, with rigorous proofs and clear theoretical guarantees.
Arguments Against Acceptance:
- The lack of experimental results limits the paper's practical impact and leaves questions about the real-world performance of the proposed methods.
- The dense presentation may hinder accessibility for a broader audience, even within the specialized community.
Recommendation:
I recommend acceptance with minor revisions. The paper makes substantial theoretical contributions and is relevant to the NIPS community, but it would benefit from the inclusion of experimental results and a more detailed discussion of limitations and practical considerations. These additions would strengthen the paper's impact and accessibility.