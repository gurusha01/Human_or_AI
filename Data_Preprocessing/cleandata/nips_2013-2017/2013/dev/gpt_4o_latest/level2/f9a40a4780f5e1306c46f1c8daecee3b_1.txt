This paper introduces two novel Bayesian entropy estimators, ĤDBer and ĤDSyn, specifically designed to estimate the entropy of binary spike trains in neural data. The authors address a critical limitation of existing entropy estimators, which fail to exploit the sparse and structured nature of neural spike responses. By incorporating prior knowledge about the statistical structure of spike trains, the proposed methods achieve more accurate and efficient entropy estimation. The paper demonstrates the superiority of these estimators over traditional methods, such as the Nemenman–Shafee–Bialek (NSB) and Best Upper Bound (BUB) estimators, through simulations and real-world neural data analysis.
Strengths:
1. Novelty and Significance: The paper makes a significant contribution by introducing entropy estimators tailored to neural data. The use of hierarchical mixture-of-Dirichlets priors, informed by synchrony distributions, represents a novel and meaningful improvement over existing approaches.
2. Theoretical Rigor: The authors provide a thorough mathematical foundation for their methods, including derivations and computational strategies to handle the high-dimensional space of spike words efficiently.
3. Practical Utility: The proposed estimators, particularly ĤDSyn, demonstrate strong performance on both simulated and real neural data. The ability to incorporate empirical synchrony distributions as priors is a practical and flexible approach that adapts well to different datasets.
4. Clarity of Results: The paper includes detailed comparisons with existing methods and demonstrates the faster convergence and higher accuracy of the proposed estimators, especially in scenarios with sparse data.
5. Reproducibility: The authors provide a clear computational framework and promise to release a MATLAB implementation, which enhances the reproducibility and accessibility of their work.
Weaknesses:
1. Limited Scope of Applications: While the estimators perform well on neural data, the paper does not explore their applicability to other domains where entropy estimation is critical. A broader discussion of potential use cases would strengthen the paper's impact.
2. Assumptions in Prior Design: The reliance on synchrony distributions as priors, while effective, may not generalize to neural populations with more complex or unknown correlation structures. This limitation is acknowledged but not deeply explored.
3. Empirical Bayes Dependency: The ĤDSyn estimator depends on an accurate estimate of the empirical synchrony distribution, which may be challenging to obtain in datasets with very small sample sizes.
4. Clarity of Presentation: While the theoretical sections are rigorous, some parts of the paper, particularly the derivations, may be difficult for readers unfamiliar with Bayesian methods or information theory. Additional explanatory figures or examples could improve accessibility.
Recommendation:
This paper is a strong candidate for acceptance at NIPS. Its contributions are both novel and significant, advancing the state of the art in entropy estimation for neural data. The proposed methods are theoretically sound, practically useful, and well-supported by experimental results. However, the authors should consider expanding the discussion of limitations and potential applications beyond neuroscience. Additionally, improving the clarity of some technical sections would make the paper more accessible to a broader audience.
Pro Arguments:
- Novel and tailored entropy estimators for neural data.
- Strong theoretical foundation and practical performance.
- Promises reproducible results via code release.
Con Arguments:
- Limited exploration of generalizability to other domains.
- Dependence on accurate synchrony distribution estimates.
Overall, the paper is a valuable scientific contribution and is recommended for acceptance with minor revisions.