Simple idea with nice results. 
If this is effectively new, then the paper should be accepted. 
Therefore the novelty aspect should be carefully checked. 
Is it really a boosting algorithm? To me it should be called a pursuit algorithm because it does not apparently rely on manipulating the distribution of examples. This is important because there are closely related works (see kernel matching pursuit, http://www.iro.umontreal.ca/~vincentp/Publications/kmp_techreport2000.pdf, section 3) found in the pursuit literature instead of the boosting literature. This is close, but not exactly the same, though... 
About the overlap with papers 539 and 956. I was initially a reviewer of paper 956 which describes a multiclass variant of the same idea. Paper 956 is harder to understand and could have been a paragraph in paper 481. Paper 539 is a semi-supervised variant od the same idea and could have been a paragraph in paper 481. This splitting of a work into minimal publishable units is unwise. I think that the community will be better served by accepting 481, rejecting 539 and 956, and letting the author know that he can refer to these extensions in the final version of the Nips paper and defer the full discussion to the future journal paper. 
Detailed comments: 
[page 1, line 44] -- Even with surrogate losses, the problem is not always convex. This depends on the family of functions. Recommend accepting only 481 which contains the important idea.