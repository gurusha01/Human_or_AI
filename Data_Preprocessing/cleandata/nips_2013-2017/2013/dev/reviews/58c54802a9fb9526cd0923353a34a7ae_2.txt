The authors consider an agent that will experience a series of sequential decision-making tasks over its lifetime in the same environment (or a similar one). They propose a method for transfering knowledge acquired in previous problems to the current problem. Specifically, they consider transfering knowledge acquired in learning optimal reward functions. The authors demonstrate the utility of their approach in two examples. 
The paper builds primarily on two pieces of earlier work. The first is the optimal rewards formulation in Singh et al. (2010), where the authors define an internal reward function as one that maximizes external reward (this internal reward function may differ from the external reward function if the agent is bounded in its capabilities, for instance if it has a small planning horizon). The second is the algorithm by Sorg et al. (2010) for incremental learning of such a reward function in a single task setting. 
The contribution of the current paper is to place this earlier work in a multi-task setting. As in Sorg et al (2010), the agent learns an internal reward function in each task. In addition, the agent learns a mapping from the external reward functions of the tasks it has experienced in the past to the internal reward functions it has learned by the end of those tasks. The agent uses this mapping to initialize its internal reward function at the beginning of each task. 
The examples in the paper are small but sufficient to demonstrate the potential utility of the approach. In practice, the success of the algorithm will depend on the availability of good features for the mapping from external rewards to internal rewards. 
The paper is well written and easy to follow. The empirical evaluation is well done, informative and useful. Figure 4, in particular, is helpful in concretely showing what the algorithm has done. Including a similar figure (or description) for the network example would be a useful addition to the paper. 
 The paper is not particularly innovative but it is a well-executed, useful addition to the literature on transfer learning.