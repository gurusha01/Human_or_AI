Summary: 
The paper is primarily divided into two parts. The authors first discuss the advantages of principal component regression (PCR) over classical linear regression. After giving an overview of the problem, they provide new theoretical results explicitly showing how principal component regression is insensitive to collinearity, and how it can take advantage of low rank structure in the covariance matrix. These results are taken in the setting where both dimension d and sample size n can increase. The second part of the paper develops a new PCR algorithm to handle the case where d > n, and when the predictors X are from an elliptical distribution. The new method is straightforward: the use of Kendall's tau in place of the sample covariance matrix handles the generalization to elliptical distributions (utilizing recent work by Oja), while the sparsity constraint handles the setting where d > n. The authors then confirm the advantages of the new method in both simulated and real-world data. 
Quality: 
The primary theoretical results (theorem 2.2 and 3.3) are strong contributions and technically sound. The simulated and experimental results, seen in figures 1, 2 and 3, add significantly to the paper's strength. 
One primary quibble is the assumption of the principal component model (equation 2.1) in the first part of the paper, and the assumption of equation 3.5 in the second part. Discussions from Artemiou and Li (2009) and Cook (2007) -- both referenced in this paper -- focus on the advantages of PCR over linear regression in a much more general context. In the current paper, the regression coefficient is explicitly assumed to be aligned with the first principal component, which (based on the aforementioned references) does not characterize all scenarios where PCR outperforms LR. It is thus unclear to what extent the results in figure 1 and 2 are trivial -- is PCR outperforming LR merely because a principal component model was assumed? 
The application to real-world data is a strong point of the paper, and the positive result seen in figure 3 helps address the above concern. However, the dataset they chose is one example, and it is unclear whether the strength of RPCR depends on analysis choices, such as the authors' choice to focus on the financial subcategory in their dataset. 
Clarity: 
The paper is well-written. The organization structure is exceptional, making the paper easy to read. The presentations of the main theorems (2.2 and 3.3) are less clear, understandably due to the fact that their proofs are delegated to the supplementary materials. Some choices, such as the supposition in theorem 2.2 (r*(Sigma)logd/n = o(1)), or the conditions of theorem 3.2/3.3, are not made clear or are not self-evident. Some exposition of the theorems themselves -- and not just their consequences -- would be helpful. 
Originality: 
The originality of the paper is largely tied to the two main theoretical contributions, theorem 2.2 and theorem 3.3. The development of the RPCR algorithm builds heavily from recent results; however, the synthesis of these results into a novel algorithm contributes to the overall originality of the paper. 
Significance: 
The two main theorems are significant contributions to the field. As the authors mentioned, theorem 2.2 is the first time observations in PCR have been explicitly characterized. The new method, RPCR, also shows promise to be used by others. 
Other notes: 
- Line 43: R^{nxd} should be R^{dxd} 
- Line 358: The first wd should be w3 
- Line 361: I could not find a definition for m, but I assume m = 2 in this context. 
- There is limited discussion of the constant alpha. For example, LR outperforms PCR for large alpha. Secondly, was there a reason, aside from simplicity, to set alpha = 1 in the simulation studies? 
 This paper signifies important contributions to our understanding of principal component regression, and is well-presented. Results would be strengthened by a clearer justification of the choice to assume a principal component model, and by a more thorough analysis of real-world datasets.