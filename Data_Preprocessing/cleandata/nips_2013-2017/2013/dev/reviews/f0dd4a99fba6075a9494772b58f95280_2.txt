Optimization, Learning, and Games with Predictable Sequences 
Summary: 
The paper discusses Optimistic Mirror Descent (OMD). This is a variant of the classical algorithm for online convex optimisation that is especially good when the sequence of loss function gradients exhibits some predictable regularity that can be exploited by the learner. 
The authors first give the specific substitution for which OMD reduces to Mirror Prox, and generalise the known bounds to Hoelder smooth functions. The paper then proceeds with the analysis of objectives with saddle-point structure. It first discusses plain min-max optimisation by a single algorithm. It then turns to a game-theoretic viewpoint, and discusses how two opponents can arrive at a saddle point faster when both players use the proposed algorithm, while maintaining the worst-case sqrt(T) regret without cooperation. The paper concludes by considering approximate convex optimisation with an application to approximate max flow. 
Discussion: 
This paper on prediction of predictable sequences is well executed and rich. The exposition is very clear, and the results are strong. I very much like the application to saddle-point finding, where the players cannot assume cooperation but there is a speedup if it happens. 
Small things: 
Page 5, proposition 6. In your notation you access x0 and f0, but these are undefined. 
page 7, (11). Here x[i] should be x_i. 
 This beautiful paper investigates prediction with predictable sequences (of gradients) and obtains strong results with interesting technical applications.