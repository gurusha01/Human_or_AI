The paper deals with model selection properties of Gauss-Lasso procedure. This is a two step procedure. In the first step a lasso estimator is fitted. In the second step, the OLS estimator is fitted on the subset of selected variables. The estimated support is given by choosing the largest s components (in absolute value) of the OLS estimator. 
The main contribution of the paper is providing the generalized irrepresentability condition and showing that the Gauss-Lasso procedure correctly recovers the support of the parameter vector under this condition. 
First, I believe that exposition of the material could be dramatically improved. Section 3.1 does not seem to be needed. One could formulate an optimization procedure that generalizes both (5) and (16). Then state results for this optimization procedure. The way material is currently presented, you are repeating the same things twice. More importantly, you should try to explain how do you improve the results of [1]. In particular, without going through the details of the proof, a reader should get a sense, at a higher level, of what novel tools does one need to use to improve existing results. Maybe provide an outline of the proof and point out where does your work differ from [1]. 
There are other two step procedures that are able to select variables consistently. See for example [2] and [3]. Both papers discuss variable selection under weaker conditions than irrepresentable condition. How does generalized irrepresentable condition compare to conditions imposed in that work. 
I believe that there is another question worth answering. How does Gauss-Lasso procedure perform when the unknown parameter vector is approximately sparse? Would the procedure still select the s largest in absolute value components? 
[1] M.J. Wainwright, Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming, IEEE Trans. on Inform. Theory 55 (2009) 
[2] Fei Ye, Cun-Hui Zhang. Rate Minimaxity of the Lasso and Dantzig Selector for the lq Loss in lr Balls. 11(Dec):3519−3540, 2010. 
[3] Sara van de Geer, Peter Bühlmann, and Shuheng Zhou. The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the Lasso). Electron. J. Statist. Volume 5 (2011), 688-749. 
 The paper studies an important problem. Exposition of the material could be dramatically improved. The authors should also compare their results to other work on two step procedures.