The paper introduced a novel formulation of sparse subspace discovery problem as one of finding sparse matrices in a fanotope, a convex hull of rank-d projection matrices. The proposed formulation gives rise to a convex problem that is solved using an ADMM algorithm. Guarantees for support recovery and error in frobenius norm are provided. Finally, a set of illustrative synthetic experiments are provided demonstrating improved performance, in terms of frobenius norm, in recovering a sparse factorization of a target matrix. 
The new formulation is elegant and provides a novel and intuitive replacement for the sparse PCA objective. The drop-in replacements for a covariance matrix, Kendal and Pearson correlation, can also be nearly-optimally decomposed enabling an analog of non-linear sparse PCA with the usual constraints familiar from the nonparanormal work. 
A bit lengthier discussion of the synthetic experiments would be helpful. The numbers of selected variables are quite high compared to what they should be for matrix Pi, which seems to be of rank 5 and fairly sparse itself. So, what is the optimal sparse decomposition for the overlap and non-overlap examples and does the method achieve this decomposition with 100s of selected variables? How well does DSPCA work in this respect? The Frobenius norm as an error term is just a part of the story. The claim that FPS gives rise to estimates that "are typically sparser" is not supported by the results. Please provide more information here. 
Notation: Please define what \vee stands for. The meaning can be gleaned from the context, but it is sufficiently non-standard that it would benefit from a clear definition. 
Couple of minor comments: "difficult to interepretation" -> "difficult to interpret", Figure 1 use the same notation for Frobenius norm as in the rest of the paper 
 Clearly written paper with novel contribution in recasting sparse PCA problem and providing a straightforward new method for estimating the sparse subspaces.