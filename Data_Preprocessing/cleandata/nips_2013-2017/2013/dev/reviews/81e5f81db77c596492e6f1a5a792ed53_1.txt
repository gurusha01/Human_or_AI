The authors address the problem of sparse principal component analysis when more than a single component is to be estimated. The standard method computes the principal components one-by-one and uses a heuristic step often called "deflation" to switch to the next component. This step-by-step sequential method is fragile and criticizing it is absolutely natural. The authors suggest to estimate the matrix using projection to the Fantope: the convex relaxation of the orthogonal matrices. 
The authors provide an efficient algorithmic scheme to solve the problem and analyse the solution statistically. The paper is well written and will interest people who are working on this very popular topic. 
An important question: why is the orthogonality constraint crucial in sparse PCA? in fact in standard (desne / full) PCA it is a consequence of the fact that the low-rank approximation of a matrix is provided by the eigenvalue decomposition which has orthogonal factors. Somehow when an extra sparsity assumption is maid why should we keep the orthogonality? 
The experimental section is not convincing. I would have wished to see a phase-transition type of diagram to see when the performance really outperforms the rival as the factors supports overlap. The idea of computing multiple principal components at once had already been tackled by Journee et al. (ref [8] in the paper, please name all the authors by the way in [8]). Why is there no comparison with their method? the code for their method is available online. There should be a numerical comparison with a sparse matrix factorization method as well. Comparison of the supports found by each method on real data may be interesting. 
 The paper is about a relevant problem, is well written and suggests a method which seems to be efficient for the goal fixed by the authors. I would like however a more convincing discussion on the orthogonality constraints which seem more embarrassing than useful. The algorithm is an incremental update of DSPCA [1] using Fantope constraint, and theoretical results are also mostly incremental, and not so exciting.