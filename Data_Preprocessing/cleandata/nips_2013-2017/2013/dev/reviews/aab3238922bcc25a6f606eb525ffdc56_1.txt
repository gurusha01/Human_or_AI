The paper introduces a new method called RDC to measure the statistical dependence between random variables. It combines a copula transform to a variant of kernel CCA using random projections, resulting in a O(n log n) complexity. Results on synthetic and real benchmark data show promising results for feature selection. 
The paper is overall clear and pleasant to read. The good experimental results and simplicity of implementation suggest that the proposed method may be useful in complement to other existing methods. 
The originality is limited, since it mostly combines several "tricks" that have been used in the past, namely the copula transformation to make the measure invariant to monotonic transformations (see eg Conover and Iman, The American Statistician, 35(3):124-129, 1981 or more recently the reference [15] cited by the authors), and the random projection trick to define a low-rank approximation of a kernel matrix. 
Although the work is technically correct, the following points would require clarification: 
- the authors insist that RDC is much faster to implement than kCCA, claimed to be in O(n^3) and taking 166s for 1000 points in the experiments. I am surprised by this, since in the original kCCA paper of Bach and Jordan an implementation in O(n) using incomplete Cholevski decomposition is proposed, and the authors claim there that it takes 0.5 seconds on 1000 points (more than 10 years ago). In fact, the incomplete Cholevski decomposition of Bach is a very popular approach to run kernel methods on large numbers of samples, similar to the random projection trick used by the authors. A natural question is then: to perform kCCA with large n, is it really better to use the random projection trick compared to the incomplete Cholevski decomposition? 
- as pointed out by the authors, when the dimension k of random projections gets large the method converges to unregularized kCCA, so it is important that k is not too large because "some" regularization of kCCA is needed. Hence k seems to play a crucial regularization role, akin to the regularization parameter in kCCA. Is there any theoretical or empirical argument in favor of a regularization by k, compared to the classical kCCA regularization? An argument against using k is that, when k is not too large, the random fluctuations may lead to significant fluctuations in the final score, which is not a good property. In fact, although RDC fulfills all conditions in Table 1, one that is not fulfilled is that it is a stochastic number (ie, compute it twice and you get different values). An interesting work combining several known ideas, but the comparison with the nearest cousin kCCA is not really fair and well-studied.