This paper explores an novel hashing scheme that relates the chi-squared similarity between two high-dimensional vectors x and y, and the Hamming distance between the sign vectors of their randomly projected versions, sign(xR) and sign(yR). Here R is chosen to be a projection matrix whose elements are drawn from an alpha-stable distribution. The authors theoretically demonstrate that the chi-squared similarity measure is lower-bounded by the cosine of the (normalized) Hamming distance between the sign vectors. Moreover, for the case where alpha=1 and the matrix is i.i.d. Cauchy, they empirically demonstrate this bound is a good approximation to the actual similarity measures. The authors utilize this relation to achieve competitive performance with existing kernel-based methods for classification. 
The ideas introduced in this paper are interesting. As the authors point out in the conclusions, the chi-squared similarity measure potentially lends itself to several important applications and therefore, a method that efficiently computes this measure can be very beneficial. I have a few concerns that the authors can hopefully address: 
- The paper can benefit from some careful editing. Most of the paper concerns itself with carefully bounding the collision probabilities of the dimensionality reduced representations. However, it looks like the true focus instead (especially in practice) is the chi-squared similarity measure and this message is not really hammered home until Section 7. The experimental results are also scattered and this impedes the flow of the paper somewhat. 
- A feature of most hashing schemes in this line of work (such as sim-hash, LSH, etc) is that it is possible to provide concrete lower as well as upper bound-type relations between pairwise metrics measured in the original high-dimensional space and the Hamming space. Therefore, one cannot really describe this scheme as a rigorous similarity estimation technique in the vein of the previous approaches. It would be nice if the authors comment upon the conceptual difficulties involved in proving the other half of the problem. 
- While the approach presented in the paper is space-efficient, the authors do not also explicitly address the time efficiency for the case alpha = 1 where the random projection matrix is Cauchy (for alpha = 2, it is known that there exist fast transforms that achieve the dimensionality reduction). 
- Lastly, it would be good if the authors normalized the x-axis to the same limits in Figs 2 and 3. It is difficult to visualize the empirical tightness of the proposed bounds when the scales of visualization are very different. 
-------- 
Post-rebuttal edit: I would leave my score unchanged. My opinion of the paper continues to be positive, since the ideas are interesting, novel, and might have considerable potential impact. However, I echo several concerns of the other reviewers: (i) The papers needs a careful and thorough re-organization. (ii) The paper needs to address the (fairly substantial) gaps in the narrative -- particularly, the relationship between the chi-squared similarity and the approximations proposed in Theorem 2 and Lemma 5. (iii) The experimental results need to be re-organized, explained clearly, and made more convincing to a machine learning practitioner. I hope that the authors can address some of these in future revisions.  This is an interesting paper that proposes an efficient method to estimate the chi-squared similarity between high-dimensional vectors via the method of measuring Hamming distances between the signs of their alpha-stable random projections. The paper falls short of establishing a fully rigorous metric equivalence between the original space and its low-dimensional embedding, but the (fairly extensive) empirical evaluations offsets this drawback.