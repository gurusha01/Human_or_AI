The paper provides applications of online learning with predictable loss sequences. First earlier results are generalized to Holder-smooth loss functions. The framework is then applied to compute solutions to saddle-point type problems, as well as to efficient computation of equilibria in two-player zero-sum games (for cooperative players, with guarantees if one of the players does not cooperate). Finally, convex programming problems are considered, but here I find the underlying assumptions strange. 
I think the problems considered are of natural interest, and the approach is novel. It is somewhat misleading though, in some sense, that the "predictable sequence framework" uses, as prediction, past observations, and smoothness to quantify how good the predictions are. 
The paper is generally well-written and clear. However, I have two major criticisms which need to be addressed in the authors' response: 
1. All learning rates in the paper are set with peeking into the future: \etat is a function of \Nablat, which only becomes available after an update with \etat is performed, and the next gradient (loss) \Nablat is revealed. 
2. Section 5: I do not understand the assumption that F^* can be known in advance, or can be found using binary search. Why would verifying the existence of a solution with a given value be any easier than actually finding such a solution? So why is it a valid assumption to know the exact (not approximate) value of the maximum without actually knowing an optimal solution? (In which case the whole following optimization process is pointless.) 
Minor comments: 
- 1. It does not seem to be necessary to give the proof of Lemma 1, copied verbatim from [9], except for the unproven first line in the statement. Instead, Lemma 3 of [9] should be referred to. 
- 2. Proof of Lemma 2: line571: it seems that 1/eta1 \le 1/Rmax is implicitly assumed, which is the same as assuming the norm of the gradient is bounded by 1. Please make this explicit (or avoid this bound). 
- 3. line 11: It seems that \rho should be 1/\sqrt{H} to cancel the first term in (3), deteriorating the bound in line 113 worse. 
- 4. Explain the inequalities in the derivations, such as lines 587 and 590, 637, 645, 655, 659, etc. These are not necessarily hard, but sometimes simply the inequalities are so long that it is hard to find the differences... 
- 5. Corollary 5: define the norms F and X 
- Proposition 6: equation (9) bounds the average (or normalized) regret. The same for lemma 7. 
The authors might also be interested in the recent ICML paper Dynamical Models and tracking regret in online convex programming by Eric Hall and Rebecca Willett (JMLR W&CP 28(1):579-587, 2013), which considers predictability in a related, albeit somewhat different setting. 
ADDED AFTER THE REBUTTAL PERIOD: 
- learning rates: the corrections seems ok 
- Section 5: knowing F^ in advance: as I understand the response, F^ can only be known approximately, and this approximation has some price in the bounds. So I would not say knowing F^* can be assumed without loss of generality. However, the explanation given would be sufficient. 
- bound in line 113: you are right, it was my mistake. I think this is a solid paper with several interesting results.