The paper studies the idea of using a boosting-like approach to directly minimize the training error and various functions of the training margins. The algorithms are explained in detail, and decent experimental results are presented. Theory is fairly minimal. 
In a way, this is a fairly obvious idea, but conventional wisdom says that the idea should not work since classification error is not convex or even continuous. It is great to see someone try it, and to spell out all the issues involved, and the details of how to implement it. I thought the experimental results were especially strong, a nice comparison on 10 datasets against several boosting algorithms. The new method works surprisingly well, especially in the presence of noise. 
The presentation is generally good. I thought the descriptions of algorithms could have been a bit more precise, but the paper gives examples that really help to illustrate what is going on. A good idea presented with algorithmic details, and strong experimental results.