In this paper, the authors propose fast approaches to compute non-linear kernels between non-negative vectors. They focus in particular on chi_2 kernels. 
Building upon the known fact that the dot-product < x, y > of two vectors can be recovered as the cosine of the collision probability P(sign(xj) \ne sign(yj)) where xj and yj are equal to the dot product of x and y with a same vector a_j whose coordinates are drawn randomly from a stable distribution with scale alpha=2. The topic of the paper is to check what happens when the scale alpha < 2 and see whether one can recover interesting approximations of other kernels. After a motivating example in Section 2 and preliminary results in Section 3, the authors introduce their real aim in Section 4 and provide in Section 5 two formulas to approximate the collision probability as a function of the chi2 similarity. 
I have liked this paper, and I think it contains sufficient novel material to be a worthwhile addition to NIPS, but I have a few concerns which prevent me from being more positive about it. Basically, the paper is messy. It feels as if the authors themselves have not digested the content they propose and are still hesitating about which form the paper should take. Some expressions such as "(perhaps surprising)", and the simplicity of some of the results (upper bounds) suggest that this is still a work in progress. This makes the paper difficult to read. Here are a few examples: 
= the authors motivate their interest for the chi2 similarity by fact that it's popular to compare histograms. In that case why consider time series (UCI-PEMS) of non-normalized measurements as one of their main examples? This is confusing. Besides, that dataset itself is not that large (in the context of what the authors are trying to show) since it only contains 440 points. Finally, the reader will have a hard time trying to understand why the "acos-\chi_2" kernel is introduced in l.82 without more context. I'd suggest to drop section 2 and focus more cleanly on experimental results at the end of the paper. As a side note, the accuracies on MNIST seem very low... I wonder what a Hellinger kernel or a Gaussian kernel would do. 
= the whole paper is constructed to show how the chi2 similarity can be used to approximate the collision probability. However, if I understand correctly, the natural flow in applications goes the other way round: empirical collision probabilities can be computed, and hence chi2 similarities can be approximated. In that sense, I wonder whether the second approximation of Equation 12 has any sense in machine learning, unless there's an obvious inversion formula for arctan integral. 
= the paper has reads like a hack in progress. Maybe the explicit layout of an algorithm at the end of what the authors think is their more important contribution could help the paper be more understandable. 
= Lemma 4 is provided without a proof, and I don't think one can proceed by analogy with the proof of Lemma 2, as suggested by the authors). In the case of Lemma 2 the correspondence between features and feature products (collisions) / the kernel 1-1/pi acos(p2) was, when taking the expectation, exact. There is not such an exact formula in the case of p\chi2, as stated repeatedly by the authors. Therefore, one needs to prove that, because \rho\chi2 is a psd kernel, 1-1/\pi acos \rho\chi_2 is psd too. This is not "trivial" but must be proved. Using corollary 1.14 chapter 3 (p.70) of the Berg/Christensen/Ressel reference (Harmonic Analysis on Semigroups) should suffice, since we only need to show that acos is holomorphic and with positive coefficients. The following reference might be a good starting point: 
http://mizugadro.mydns.jp/t/index.php/ArcCosRangeofholomorphism 
At this point, however, lemma 4 is just a conjecture. 
update after rebuttal: 
The answer from the authors on the psdness of the acos chi2 did not convince me. I don't understand their proof. The feature representations for the chi2 kernel are, as far as I know, infinite dimensional. Then the authors propose to use random Gaussian approximations on infinite dimensional vectors? I am sorry, I can't follow this. This part needs to be proved more clearly. This nice paper reads like a mishmash of ideas that try to generalize simhash (alpha=2) for stable distributions with alpha < 2 and see what can be taken out of it. Some of these ideas are promising, but the presentation needs to be improved to be suitable for NIPS