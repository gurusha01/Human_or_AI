The paper describes a method to "conserve" some earlier experiences 
in solving a RL problem to later runs using different reward 
functions. Importantly, the system assumes a limited power for 
learning, which makes it advantageous to use a surrogate reward 
function that allows the agent to learn the concrete reward. The 
idea is to provide the opportunity to learn an inner reward function 
as a function of the external reward function. 
The idea is interesting and attractive, and the prospect of a 
"bounded rationality"-type assumption behind the algorithm (although 
the authors studiously - and wisely - avoid using it) renders the 
method a welcome approach to a more practical (and plausible) 
perspective on reinforcement learning in general scenarios. 
Generally well readable, the reviewer found that the paper lost 
clarity in the network routing domain. I'll mention some of the 
issues in the details below. 
In terms of methodology, the paper falls into the general category 
of the "reward-shaping" methodology, the success of the methodology 
in the examples is convincing, the general method class is, of 
course, already, if not maturing, but consolidating. 
- line 323: what are "trajectory-count" parameters for UCT? Number 
of sample runs? 
- line 332: it seems that either colours, coefficient signs or 
semantics of the coefficent are inconsistent here. The text says: 
"negative/dark/discouraging exploration", but that does not fit 
with figure 4. 
- line 370: I do not understand the point of the decomposition into 
G1, G2, G_3? What's the purpose of it? 
- line 403: I do not understand how the transition function is 
modeled. Don't you use reward mapping anymore here? If you use it, 
and you modify the transition function, how does that happen? 
Please reformulate this section, it is completely unclear to me. 
- line 418: What is the "competing policy transfer agent"? What 
model does it use? 
 An interesting method for transfer learning under limitedresources. Settled in existing "reward shaping" methodologyterritory, the method itself looks sufficiently original andeffective to warrant publication. Some (minor) weaknesses in thedescription of the second example.