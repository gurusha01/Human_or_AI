This paper presents a local winner take all approach, where units are grouped in small sets (2 in most of the paper) and only the max of them gets to output something, all the other ones output 0. Experiments on MNIST and a sentiment analysis task show improvements relative to other approaches. 
There are connections to max-pooling (only one unit out of a subset fires) and dropout (some units are shut down), but it is indeed different from both and provides a new nonlinearity to consider in the deep learning toolbox. 
Results on MNIST are not very impressive. Either the approach gets 0.02% better than the competitor, which means having correctly classified 2 more images out of 10000, or it is simply not as good as normal CNN (but again, by only 0.02%...). [I must say I stopped being impressed by any results on MNIST long time ago...]. The only good result is on the amazon sentiment analysis task, it seems. 
I also liked the experiment of section 6, but wondered if it could be shown to improve performance on a real task instead of this artificial setting. 
I would have liked the authors to experiment with variying the "pool" size and try to understand when and how it would help: is it better when the model is overfitting, underfitting, noisy, etc. A local winner take all technique is described for deep learning algorithms. It provides yet another simple non-linearity to consider in the toolbox. Results on MNIST are not very impressive, but results on sentiment analysis are.