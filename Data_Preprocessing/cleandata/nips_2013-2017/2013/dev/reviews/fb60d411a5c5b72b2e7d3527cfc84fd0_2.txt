Summary 
======= 
Annealed importance sampling (AIS) depends on a sequence of distributions between an initial and a target distribution. While most previous applications of AIS have used distributions along paths in parameter space based on geometric averages of the two distributions, the present article proposes using alternative paths based on averaging moments. The authors provide theoretical and empirical evidence for the advantages of their approach. 
Quality 
======= 
The paper gives a rigorous analysis of the proposed moment averaging (MA) path. The theoretical claims appear to be sound. Except for typographical errors, I did not find any mistakes in the proofs provided. 
The empirical results on intractable models used a linear annealing schedule for the geometric averaging (GA) path. Since previous work seems to have found other schedules to be more effective (Neal, 1998; Salakhutdinov & Murray, 2008), it would be interesting to also see how well GA does with a different schedule when applied to the larger RBMs – despite or especially because of figures 2 and 3 suggesting that it will not make a difference. With the schedules used in the mentioned work, the sampler will spend more time close to the target distribution, so that I suspect that using them will at least lead to more reasonable samples in Figure 4. 
Clarity 
======= 
The paper is well written and well structured. I had no problems following the main text. 
The supplementary material might benefit from making a few more calculations explicit (for example, the Fisher information matrix used in the integral at the top of page 4). 
Originality 
=========== 
This paper represents a highly original contribution. As pointed out by the authors, virtually all previous applications of AIS used geometric paths. This shows that the proposed alternative is not at all obvious and technically challenging. 
Significance 
============ 
AIS is a widely used technique in statistical machine learning. While the MA approach seems to work reasonably well for estimating partition functions of some models (such as RBMs), getting it to produce useful results with more complex models can be difficult. Better annealing strategies have the potential to improve this situation and to allow for better model comparisons. In addition, as also mentioned by the authors, many sampling strategies important for inference and learning can benefit from better annealing strategies. 
Although I am not convinced that the proposed annealing strategy will find widespread adoption (mainly because of the added complexity of having to estimate intermediate distributions and because the results on intractable RBMs suggest that GA can still compete with or even outperform MA, as measured by the ESS), I think it has great potential to inspire further work in this direction. 
Minor comments 
============== 
– Supp. 1: better use $\theta0 = \theta$ instead of $\theta = \theta0$ 
– Supp. 2: better use $pa$ and $pb$ for $p0$ and $p1$ throughout 
– Supp. 2.1: the first term of the Lagrangian should be $\lambda ( 1 - \sumx q(x) )$ or $\lambda ( \sumx q(x) - 1 )$ 
– Supp. 3.1: $\dot s(\beta) = t$ should be $\dot s(\beta) = 2\beta$ 
– Supp. 3.2: a factor 2 is missing in the derivative of $\lambda(\beta)$ This is a highly original contribution with potential future impact on learning, inference, and model comparison. While I am not convinced that it will find immediate adoption for the purpose of partition function estimation, I think it will inspire more theoretical work in the direction taken by the authors.