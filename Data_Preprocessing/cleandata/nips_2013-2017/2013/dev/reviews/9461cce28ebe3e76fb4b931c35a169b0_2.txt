In this paper the authors provide an algorithm for directly minimzing 0-1 loss and margin maximization. Most existing machine learning techniques have relied on minimizing a convex upper bound on the 0-1 loss in classification problems. In contrast, in this paper the authors propose a simple greedy algorithm for directly minimizing the 0-1 loss via a combination of weak learners. This is followed by a few steps of direct maximization of margin. The proposed algorithm is then evaluated on a few small low dimensional datasets. 
I have the following major concerns about this paper: 
The authors claim that their algorithm has a much favorable run-time compared to AdaBoost (Table 2). I do not understand how this is possible. In AdaBoost, in each round, a weight is given to a training example and a weak learner such as a decision tree can be found quite efficiently to minimize a weighted loss (for example using CART as the weak learner). Thus, in each step of boosting one needs to find only one decision tree. However, for the proposed algorithm, one has to iterate over all possible weak learners. I just don't see how the proposed algorithm can be computationally more efficient unless the number of weak learners is really small. In many applications with large datasets it is typical to consider decision trees of depth 5 or 10 and I do not see how the proposed method can be efficient in that case if one has to enumerate all weak learners. I am concerned that the proposed algorithm is limited to small datasets with low dimensions and weak learners with very few instances. 
From the standard deviations in Table 1, I am not sure that the proposed method results in statistically significant results in Table 1 compared to AdaBoost. If we take the standard error most of the results seem to overlap. 
The proposed method consists of two steps: first, 0-1 loss is minimized using greedy co-ordinate descent. Once a few weak learners are selected, a few more weak learners are added to maximize the average margin of bottom n' examples greedily. How much of the claimed benefit is due to each step? For example, what happens if you run your algorithm 2 after a few iterations of AdaBoost? "Direct 0-1 loss minimization" is a bit of a misnomer since it is followed up with a few steps of margin maximization. I am not sure such natural questions are adequately answered in this paper. 
 While this work could be potentially interesting, I am not completely convinced about this paper at this point.