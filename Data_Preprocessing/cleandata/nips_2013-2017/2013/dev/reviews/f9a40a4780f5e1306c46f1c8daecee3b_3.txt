This submission presents a straightforward improvement on current estimators of entropy in multidimensional binary distributions. Estimating entropy (and related measures like mutual information) is considered important in advancing our understanding of the neural code. This work is especially timely in that an increasing number of multi-electrode recordings are currently taking place whose interpretability depends on better analysis methods like the one presented here. 
The work presented here relies on the fact that population responses, esp. when binned in short time intervals, are very sparse: the most frequent word is one in which no neuron spikes, with a word frequency rapidly decreasing with the number of spikes, or ones, in a word. By incorporating this prior knowledge into their Bayesian estimator, the authors derive an estimator that achieve a bias comparable to that of existing estimators using several orders of magnitude less data - a critical constraint in empirical studies. 
In an example application of their method to real data, the authors find that retinal ganglion cells "code synergistically" in time. While interesting, this section would need to be expanded to yield robust and convincing insights. 
The presentation of the derivation and the results is quite clear and I only have minor comments. 
[deleted] 
 Important improvement on entropy estimation methods based on a more sophisticated prior than used previously.