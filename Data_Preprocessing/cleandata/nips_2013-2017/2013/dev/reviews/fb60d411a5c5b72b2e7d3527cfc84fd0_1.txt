AIS has become an important tool to evaluate the performance of a learning algorithm for MRFs. In common practice, people use geometric averaged models as intermediate models. This paper raises an interesting question of finding a better alternative to the regular annealing path. It focuses on the bias of the expected log-weight, on which the variance of the AIS output is monotonically related, and compares the asymptotical behavior of different annealing paths. 
Overall, I think this paper is well written. The explanation is clear, the example of the two paths between Gaussian distributions in Figure 1 and the visualization of intermediate RBMs in Figure 4 are very helpful. Also the analysis is sound (I didn't check the proof in the supplementary). 
My only concern is about how to obtain the moment averaged models. The authors use PCD with a relatively short training time to do the moment matching. How sensitive is the outcome of AIS wrt the accuracy of the moments of the target distribution as well as the parameters of those intermediate models? How is the total computational cost of the new method compared to method based on geometric average? 
In the geometric averaging path, it is observed for RBMs that assigning more models at the low temperature end usually obtains much smaller variance than the linear schedule in practice. Is this phenomenon considered or compared in the experiments? 
 This paper proposes an alternative to the regular geometric average annealing schedule for AIS, and shows a smaller variance and higher effective sample size on the output weights. The paper is well written and the result is convincing.