This paper presents a thorough study on Bayesian entropy estimation for neural spike trains, providing a new approach to estimating the entropy of discrete distributions from samples. The authors introduce two novel Bayesian entropy estimators, ĤDBer and ĤDSyn, which utilize a hierarchical mixture-of-Dirichlets prior with a base measure designed to integrate prior knowledge about spike trains into the model.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and methodology. The use of a synchrony distribution as a base measure is a clever idea, allowing the authors to capture the sparsity structure of multi-neuron recordings. The empirical Bayes procedure using the estimated synchrony distribution (ESD) as a base measure is also a nice touch, providing a more flexible and data-driven approach to entropy estimation.
The simulation results and comparisons with other estimators, such as the Nemenman-Shafee-Bialek (NSB) and Best Upper Bound (BUB) entropy estimators, demonstrate the effectiveness of the proposed estimators, particularly ĤDSyn. The application of the estimators to real neural data also shows promising results, providing insights into the coding of single neural time-series and the quantification of temporal dependence.
However, there are some areas that require improvement. The paper could benefit from a more detailed discussion of the relationship between the proposed estimators and existing methods, such as the NSB estimator. Additionally, the authors could provide more justification for the choice of hyper-priors on α and p, and explore the sensitivity of the results to these choices.
Some technical errors and areas for improvement include the uniqueness of minimizers, the invertibility of the Gram matrix, and the dependence of the lambda parameter on an unknown quantity. The authors should also consider removing unnecessary sections, such as Section 3.1, and adding comments and assumptions to clarify the results and improve readability.
Overall, the paper is of high interest to statisticians and practitioners working in the field of neural data analysis, and the proposed estimators have the potential to make a significant impact in the field. With some revisions to address the mentioned areas for improvement, the paper could be even stronger and more convincing. 
Arguments pro acceptance:
- The paper presents a novel and well-motivated approach to Bayesian entropy estimation for neural spike trains.
- The proposed estimators, ĤDBer and ĤDSyn, demonstrate excellent performance in simulations and real data applications.
- The use of a synchrony distribution as a base measure is a clever idea, allowing the authors to capture the sparsity structure of multi-neuron recordings.
Arguments con acceptance:
- The paper could benefit from a more detailed discussion of the relationship between the proposed estimators and existing methods.
- The authors should provide more justification for the choice of hyper-priors on α and p, and explore the sensitivity of the results to these choices.
- Some technical errors and areas for improvement, such as the uniqueness of minimizers and the invertibility of the Gram matrix, need to be addressed.