This paper proposes a general formalism of iterated random functions with semigroup property, which can be used to analyze the convergence of Bayesian posterior updates in complex models. The authors provide a convergence theory for these iterated random functions and apply it to the analysis of exact and approximate message-passing algorithms for sequential change point detection problems. The paper is well-written, and the results are well-executed, making it a rich and beautiful contribution to the field of optimization and learning.
The paper discusses Optimistic Mirror Descent (OMD), a variant of the classical algorithm for online convex optimization that exploits predictable regularity in the sequence of loss function gradients. The authors analyze OMD's performance on objectives with saddle-point structure, including min-max optimization and game-theoretic scenarios where two opponents can arrive at a saddle point faster without cooperation. The paper provides strong results and interesting technical applications, including an application to approximate convex optimization and approximate max flow.
The authors generalize known bounds to Hoelder smooth functions and provide a specific substitution for which OMD reduces to Mirror Prox. The paper's exposition is clear, and the results are well-executed, making it a valuable contribution to the field.
The strengths of the paper include:
* The proposal of a general formalism of iterated random functions with semigroup property, which can be used to analyze the convergence of Bayesian posterior updates in complex models.
* The provision of a convergence theory for these iterated random functions, which can be applied to various problems, including sequential change point detection.
* The application of the convergence theory to the analysis of exact and approximate message-passing algorithms for sequential change point detection problems.
* The generalization of known bounds to Hoelder smooth functions and the provision of a specific substitution for which OMD reduces to Mirror Prox.
The weaknesses of the paper include:
* The paper assumes that the sequence of loss function gradients has predictable regularity, which may not always be the case in practice.
* The paper focuses on the analysis of OMD in the context of online convex optimization, and it is not clear how the results can be extended to other optimization problems.
* The paper does not provide extensive experimental results to demonstrate the effectiveness of the proposed algorithms in practice.
Overall, the paper is well-written, and the results are well-executed, making it a valuable contribution to the field of optimization and learning. The proposal of a general formalism of iterated random functions with semigroup property and the provision of a convergence theory for these functions are significant contributions to the field.
Arguments for acceptance:
* The paper proposes a new and interesting formalism for analyzing the convergence of Bayesian posterior updates in complex models.
* The paper provides a convergence theory for iterated random functions, which can be applied to various problems, including sequential change point detection.
* The paper generalizes known bounds to Hoelder smooth functions and provides a specific substitution for which OMD reduces to Mirror Prox.
* The paper is well-written, and the results are well-executed, making it a valuable contribution to the field of optimization and learning.
Arguments against acceptance:
* The paper assumes that the sequence of loss function gradients has predictable regularity, which may not always be the case in practice.
* The paper focuses on the analysis of OMD in the context of online convex optimization, and it is not clear how the results can be extended to other optimization problems.
* The paper does not provide extensive experimental results to demonstrate the effectiveness of the proposed algorithms in practice.