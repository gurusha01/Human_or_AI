This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, which is a departure from traditional transfer learning methods that focus on reusing value functions, policies, or models. The authors build on the optimal rewards problem (ORP) framework, which separates the task-specifying objective reward function from the internal reward function used to guide agent behavior.
The paper is well-written and clear, effectively presenting its contributions and ideas. The authors provide a thorough background on the ORP and its application to single-task settings, and then extend this work to the long-lived agent setting. They propose a reward mapping approach that learns a mapping from objective reward function parameters to internal reward function parameters, which is used to initialize the internal reward function for subsequent tasks.
The empirical evaluation of the approach is thorough and convincing, with experiments on both a grid world domain and a networking routing domain. The results demonstrate that the reward mapping approach can substantially improve the performance of a bounded agent relative to other approaches, including a competing policy transfer method.
The strengths of the paper include its novel approach to transfer learning, its thorough empirical evaluation, and its clear presentation. The weaknesses of the paper include the lack of theoretical analysis of the reward mapping approach and the limited exploration of the conditions under which this approach is effective.
Here is a list of arguments pro and con acceptance:
Pros:
* The paper presents a novel and interesting approach to transfer learning in long-lived agents.
* The empirical evaluation is thorough and convincing, demonstrating the effectiveness of the reward mapping approach.
* The paper is well-written and clear, making it easy to follow and understand.
Cons:
* The paper lacks theoretical analysis of the reward mapping approach, which makes it difficult to understand the underlying mechanisms and limitations of the method.
* The paper does not explore the conditions under which the reward mapping approach is effective, which limits its applicability and generality.
* The competing policy transfer method is not thoroughly evaluated, which makes it difficult to compare the performance of the reward mapping approach to other state-of-the-art methods.
Overall, I believe that the paper is a good scientific contribution to the field, and its strengths outweigh its weaknesses. I recommend acceptance, but suggest that the authors address the limitations of the paper, including the lack of theoretical analysis and the limited exploration of the conditions under which the reward mapping approach is effective. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by experimental results. However, the lack of theoretical analysis and the limited exploration of the conditions under which the reward mapping approach is effective reduce the quality of the paper.
Clarity: 9/10
The paper is well-written and clear, making it easy to follow and understand. The authors provide a thorough background on the ORP and its application to single-task settings, and then extend this work to the long-lived agent setting.
Originality: 8/10
The paper presents a novel approach to transfer learning in long-lived agents, which is a departure from traditional transfer learning methods. However, the paper builds on existing work on the ORP, which reduces its originality.
Significance: 8/10
The paper demonstrates the effectiveness of the reward mapping approach in improving the performance of a bounded agent, which is a significant contribution to the field. However, the limited exploration of the conditions under which the reward mapping approach is effective reduces its significance.