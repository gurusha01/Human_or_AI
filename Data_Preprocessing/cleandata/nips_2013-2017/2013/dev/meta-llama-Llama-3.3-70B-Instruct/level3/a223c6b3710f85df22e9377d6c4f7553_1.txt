This paper presents a novel approach to estimating the entropy of discrete distributions, specifically in the context of neural spike trains. The authors introduce two Bayesian entropy estimators, ĤDBer and ĤDSyn, which utilize a hierarchical mixture-of-Dirichlets prior with a base measure designed to incorporate prior knowledge about spike trains. The key innovation is the use of a synchrony distribution as the base measure, which captures the statistical structure of simultaneously recorded spike responses.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and methodology. The introduction provides a thorough background on the problem of entropy estimation and the limitations of existing methods. The authors also provide a detailed explanation of their approach, including the derivation of the estimators and the computational implementation.
The strengths of the paper include the novelty of the approach, the thorough evaluation of the estimators using simulated and real neural data, and the potential impact of the work on the field of neuroscience. The authors demonstrate that their estimators outperform existing methods, such as the Nemenman-Shafee-Bialek (NSB) and Best Upper Bound (BUB) entropy estimators, particularly in cases where the sample size is small.
However, there are some weaknesses and areas for improvement. One potential concern is the assumption of sparsity in the spike trains, which may not always hold in practice. The authors acknowledge this limitation and suggest that their approach can be extended to more general cases. Additionally, the computational implementation of the estimators may be challenging for very large datasets, and the authors could provide more details on the scalability of their approach.
In terms of originality, the paper presents a significant contribution to the field of entropy estimation and neuroscience. The use of a synchrony distribution as the base measure is a novel idea, and the authors demonstrate its effectiveness in capturing the statistical structure of spike trains. The paper also provides a thorough comparison with existing methods, which helps to establish the significance of the contribution.
The significance of the paper lies in its potential to improve our understanding of neural coding and the analysis of neural data. The authors demonstrate the applicability of their approach to real neural data and show that it can provide new insights into the temporal dependence of neural signals. The paper also has implications for other fields, such as information theory and machine learning, where entropy estimation is a crucial task.
Overall, I would recommend accepting this paper for publication, as it presents a significant contribution to the field of entropy estimation and neuroscience. The authors provide a thorough and well-written paper that demonstrates the effectiveness of their approach and its potential impact on the field.
Arguments pro acceptance:
* Novel approach to entropy estimation using a synchrony distribution as the base measure
* Thorough evaluation of the estimators using simulated and real neural data
* Potential impact on the field of neuroscience and other related fields
* Well-organized and clearly written paper
Arguments con acceptance:
* Assumption of sparsity in spike trains may not always hold in practice
* Computational implementation may be challenging for very large datasets
* Limited discussion of the scalability of the approach
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 9/10