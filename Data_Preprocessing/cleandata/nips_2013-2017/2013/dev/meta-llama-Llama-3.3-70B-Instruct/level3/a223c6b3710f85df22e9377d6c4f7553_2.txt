This paper proposes two novel Bayesian entropy estimators, ĤDBer and ĤDSyn, for estimating the entropy of discrete distributions from samples, with a focus on neural spike train data. The authors introduce a hierarchical mixture-of-Dirichlets prior with a base measure designed to integrate prior knowledge about spike trains into the model. The key idea is to use a synchrony distribution as the base measure, which captures the statistical structure of simultaneously recorded spike responses.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of entropy estimation and the limitations of existing methods. The proposed estimators are technically sound, and the authors provide a thorough analysis of their properties and performance. The use of a synchrony distribution as a base measure is a novel and interesting approach, and the authors demonstrate its effectiveness in capturing the statistical structure of neural spike train data.
The paper has several strengths. Firstly, the proposed estimators are shown to outperform existing methods, such as the Nemenman-Shafee-Bialek (NSB) estimator, in simulations and on real neural data. Secondly, the authors provide a detailed analysis of the properties of the estimators, including their bias and variance, and demonstrate their robustness to different synchrony distributions. Finally, the authors provide a clear and concise implementation of the estimators, which will be made available online.
However, there are some weaknesses to the paper. Firstly, the authors could provide more discussion on the choice of hyper-priors for the concentration parameters α and p. While the authors provide some justification for their choices, it would be helpful to see a more detailed analysis of the sensitivity of the estimators to these hyper-priors. Secondly, the authors could provide more comparison with other existing entropy estimators, such as the Best Upper Bound (BUB) estimator. Finally, the authors could provide more discussion on the potential applications of the proposed estimators in neuroscience and other fields.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, and the authors provide a clear and concise introduction to the problem and the proposed solution. The paper is well-organized, and the authors provide a detailed analysis of the properties and performance of the proposed estimators. The paper is also original, and the authors provide a novel and interesting approach to entropy estimation.
Arguments for acceptance:
* The paper proposes a novel and interesting approach to entropy estimation, which is shown to outperform existing methods in simulations and on real neural data.
* The authors provide a detailed analysis of the properties and performance of the proposed estimators, including their bias and variance.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem and the proposed solution.
Arguments against acceptance:
* The authors could provide more discussion on the choice of hyper-priors for the concentration parameters α and p.
* The authors could provide more comparison with other existing entropy estimators.
* The authors could provide more discussion on the potential applications of the proposed estimators in neuroscience and other fields.
Overall, I would recommend accepting the paper, as it provides a novel and interesting approach to entropy estimation, and the authors provide a detailed analysis of the properties and performance of the proposed estimators. However, the authors could address some of the weaknesses mentioned above to improve the paper.