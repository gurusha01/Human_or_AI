This paper proposes a novel approach to estimating the entropy of discrete distributions, specifically in the context of neural spike trains. The authors introduce two Bayesian entropy estimators, ĤDBer and ĤDSyn, which utilize a hierarchical mixture-of-Dirichlets prior with a base measure designed to incorporate prior knowledge about spike trains. The key idea is to use a synchrony distribution as the base measure, which captures the statistical structure of simultaneously recorded spike responses.
The paper is well-written, and the authors provide a clear and detailed explanation of their approach. The use of a synchrony distribution as a base measure is a novel and interesting idea, and the authors demonstrate its effectiveness in estimating entropy in several simulated and real neural datasets. The results show that ĤDBer and ĤDSyn outperform existing entropy estimators, such as the Nemenman-Shafee-Bialek (NSB) and Best Upper Bound (BUB) estimators.
The strengths of the paper include its technical soundness, clear exposition, and demonstration of the effectiveness of the proposed approach. The authors also provide a detailed analysis of the computational complexity of their approach and demonstrate its feasibility for large-scale neural data.
However, there are some weaknesses to the paper. One minor issue is that there are some minor errors in English and a lack of clear intuition in certain sections, particularly in Section 4. Additionally, the paper could benefit from a more detailed discussion of the relationship between the proposed approach and existing work in the field. Furthermore, the authors could provide more insight into the choice of hyper-priors and the sensitivity of the results to these choices.
In terms of originality, the paper proposes a novel approach to entropy estimation that builds on existing work in the field. The use of a synchrony distribution as a base measure is a new and interesting idea, and the authors demonstrate its effectiveness in several examples. However, the paper does not provide a significant breakthrough or a fundamentally new idea that challenges existing paradigms in the field.
Overall, the paper is a solid contribution to the field of neural data analysis and entropy estimation. The proposed approach is technically sound, and the results demonstrate its effectiveness in several examples. While there are some minor weaknesses to the paper, it is a well-written and well-organized contribution that advances the state of the art in the field.
Arguments for acceptance:
* The paper proposes a novel approach to entropy estimation that builds on existing work in the field.
* The use of a synchrony distribution as a base measure is a new and interesting idea.
* The authors demonstrate the effectiveness of the proposed approach in several simulated and real neural datasets.
* The paper is well-written and well-organized, with a clear and detailed explanation of the approach.
Arguments against acceptance:
* The paper does not provide a significant breakthrough or a fundamentally new idea that challenges existing paradigms in the field.
* There are some minor errors in English and a lack of clear intuition in certain sections.
* The paper could benefit from a more detailed discussion of the relationship between the proposed approach and existing work in the field.
* The authors could provide more insight into the choice of hyper-priors and the sensitivity of the results to these choices.