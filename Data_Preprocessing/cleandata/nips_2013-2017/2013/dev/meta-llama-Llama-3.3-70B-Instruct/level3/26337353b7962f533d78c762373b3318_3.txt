This paper proposes a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The authors establish a near-optimal convergence rate for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. The paper also provides a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices, including sample covariance and Kendall's tau correlation matrices.
The strengths of the paper include its novel approach to sparse principal subspace estimation, its ability to handle high-dimensional data, and its strong statistical guarantees. The authors also provide an efficient ADMM algorithm to solve the resulting semidefinite program, which is a significant improvement over existing methods.
However, there are some weaknesses to the paper. The algorithm's efficiency may be a concern for very large datasets, as the authors note that the computational cost of the ADMM algorithm can be high. Additionally, the paper assumes that the input matrix is symmetric, which may not always be the case in practice. The authors also do not provide a clear comparison to other existing methods for sparse principal subspace estimation, which would be helpful in evaluating the performance of their approach.
In terms of clarity, the paper is well-organized and easy to follow, but some of the notation and technical details may be challenging for non-experts to understand. The authors provide a clear explanation of the Fantope and its properties, but some of the proofs and technical results may require a significant amount of background knowledge in convex optimization and statistical theory.
The originality of the paper is high, as the authors propose a novel approach to sparse principal subspace estimation that is different from existing methods. The paper also provides a significant contribution to the field of statistical theory, as it establishes a near-optimal convergence rate for estimation of the principal subspace of a general covariance matrix.
The significance of the paper is also high, as it provides a new tool for analyzing high-dimensional data and estimating sparse principal subspaces. The paper has the potential to impact a wide range of fields, including statistics, machine learning, and data science.
Overall, I would recommend accepting this paper for publication, as it provides a significant contribution to the field of statistical theory and proposes a novel approach to sparse principal subspace estimation. However, I would suggest that the authors provide more comparisons to existing methods and address the potential concerns about the algorithm's efficiency and the assumption of symmetry.
Arguments for acceptance:
* The paper proposes a novel approach to sparse principal subspace estimation that is different from existing methods.
* The paper establishes a near-optimal convergence rate for estimation of the principal subspace of a general covariance matrix.
* The paper provides a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices.
* The paper has the potential to impact a wide range of fields, including statistics, machine learning, and data science.
Arguments against acceptance:
* The algorithm's efficiency may be a concern for very large datasets.
* The paper assumes that the input matrix is symmetric, which may not always be the case in practice.
* The paper does not provide a clear comparison to other existing methods for sparse principal subspace estimation.
* Some of the notation and technical details may be challenging for non-experts to understand.