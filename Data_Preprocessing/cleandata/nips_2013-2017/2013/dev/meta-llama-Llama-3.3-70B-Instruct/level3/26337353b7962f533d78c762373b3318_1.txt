This paper proposes a novel method called multi-level sparse matrix factorization, which decomposes a matrix into a product of sparse matrices in a hierarchical manner with decreasing rank. The approach allows for different levels of regularization to be performed at different levels, providing a benefit over traditional decomposition methods. The authors demonstrate the effectiveness of their method, called bilevel SHMF (sparse heterarchical matrix factorization), on both synthetic and real-world calcium imaging data.
The paper is well-written and clearly explains the motivation behind the proposed method. The authors provide a thorough review of related work and highlight the advantages of their approach. The experimental results are impressive, showing that the method can accurately identify neurons, assemblies, and their temporal activities.
One of the strengths of the paper is its ability to handle hierarchical and heterarchical relations between concepts at different levels. The authors also propose a joint optimization scheme that allows for the simultaneous estimation of the relation between concepts at different levels and the sparse approximation of the raw data.
However, there are some potential weaknesses and areas for improvement. The convergence properties of the proposed learning algorithm, including convergence speed and computational complexity, are not thoroughly discussed. Additionally, the choice of hyperparameters, such as the tradeoff parameters η and λ, seems to be ad-hoc and could be improved using data-driven tuning methods.
The significance of the paper lies in its ability to provide a novel framework for unsupervised learning that can handle complex hierarchical data. The results on calcium imaging data demonstrate the potential of the method to advance our understanding of neural circuits and behavior.
Arguments pro acceptance:
* The paper proposes a novel and well-motivated method for multi-level sparse matrix factorization.
* The experimental results are impressive and demonstrate the effectiveness of the method.
* The paper provides a thorough review of related work and highlights the advantages of the proposed approach.
Arguments con acceptance:
* The convergence properties of the proposed learning algorithm are not thoroughly discussed.
* The choice of hyperparameters seems to be ad-hoc and could be improved using data-driven tuning methods.
* The paper could benefit from a more detailed analysis of the computational complexity and scalability of the method.
Overall, I believe that the paper makes a significant contribution to the field of unsupervised learning and neural data analysis. With some revisions to address the potential weaknesses, the paper has the potential to be a strong contribution to the conference.