This paper presents a novel approach to transfer learning in long-lived agents, where the agent must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, rather than learning a new policy or value function for each task. The authors build on the optimal rewards problem (ORP) framework, which separates the task-specifying objective reward function from the internal reward function used to guide agent behavior.
The paper is well-written and clearly explains the underlying algebraic proofs. The authors propose a reward-mapping agent that learns a mapping from objective reward function parameters to internal reward function parameters, which is used to initialize the internal reward function for each new task. The results show that this approach can substantially improve the agent's performance relative to other approaches, including a competing policy transfer method.
The paper has several strengths. First, the idea of reusing reward functions is novel and has promising applications in bioinformatics and other domains. Second, the authors provide a clear and well-motivated explanation of the ORP framework and its relevance to long-lived agents. Third, the empirical evaluation is thorough and demonstrates the effectiveness of the reward-mapping approach in two different domains.
However, there are also some weaknesses. First, the paper lacks experiments on non-synthetic data, which would be necessary to fully demonstrate the practical applicability of the approach. Second, the comparison to other genomics methods is limited, and it would be useful to see a more comprehensive evaluation of the approach in relation to other state-of-the-art methods. Third, the paper raises the possibility of extending the test to more than 3 variables, but this is not fully explored, and the behavior of the method in large-dimensional spaces is unclear.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, well-written, and presents a novel approach to transfer learning in long-lived agents. The results are well-supported by theoretical analysis and experimental results, and the authors are careful to evaluate both the strengths and weaknesses of the work.
Arguments pro acceptance:
* The paper presents a novel and promising approach to transfer learning in long-lived agents.
* The authors provide a clear and well-motivated explanation of the ORP framework and its relevance to long-lived agents.
* The empirical evaluation is thorough and demonstrates the effectiveness of the reward-mapping approach in two different domains.
Arguments con acceptance:
* The paper lacks experiments on non-synthetic data, which would be necessary to fully demonstrate the practical applicability of the approach.
* The comparison to other genomics methods is limited, and it would be useful to see a more comprehensive evaluation of the approach in relation to other state-of-the-art methods.
* The paper raises the possibility of extending the test to more than 3 variables, but this is not fully explored, and the behavior of the method in large-dimensional spaces is unclear.
Overall, I would recommend accepting the paper, but with the suggestion that the authors address the weaknesses mentioned above in a revised version.