This paper presents a novel approach to estimating the entropy of neural spike trains using Bayesian methods. The authors introduce two new estimators, ĤDBer and ĤDSyn, which utilize a hierarchical mixture-of-Dirichlets prior with a base measure designed to incorporate prior knowledge about spike trains. The key idea is to use the synchrony distribution, which characterizes the statistical structure of spike patterns, as a base measure for the Dirichlet distribution.
The paper is well-written, and the approach is clearly demonstrated through simulations and applications to real neural data. The authors show that their estimators outperform traditional methods, such as the Nemenman-Shafee-Bialek (NSB) and Best Upper Bound (BUB) entropy estimators. The use of a hierarchical mixture-of-Dirichlets prior allows for flexible modeling of the spike train distribution, and the authors demonstrate the effectiveness of their approach in capturing higher-order dependencies in neural data.
One of the strengths of the paper is its ability to balance theoretical rigor with practical applications. The authors provide a clear and concise explanation of the underlying mathematical framework, while also demonstrating the efficacy of their approach in real-world scenarios. The use of simulations and real data examples helps to illustrate the advantages of the proposed estimators and provides a convincing case for their adoption.
However, there are some areas where the paper could be improved. For example, the authors could provide a more detailed comparison with other methods, such as the BUB estimator, particularly in situations with spike-trains generated according to a Generalized Linear Model (GLM). Additionally, the authors could further evaluate the performance of their estimators in situations with higher-order dependencies, as the current results are mainly based on spike count.
Some minor errors were also noted, including a missing reference on line 163, a clarification needed on line 221 regarding the implication of α=0, and a typo on line 296. Nevertheless, these errors do not detract from the overall quality of the paper.
In terms of the conference guidelines, this paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-organized, and provides sufficient information for the expert reader to reproduce the results. The approach is novel and differs from previous contributions, and the authors provide a clear explanation of how their work advances the state of the art in entropy estimation for neural spike trains.
Overall, I would recommend accepting this paper for publication, as it presents a significant contribution to the field of neural coding and entropy estimation. The authors' approach has the potential to improve our understanding of neural coding and could lead to new insights into the statistical structure of spike trains.
Arguments pro acceptance:
* The paper presents a novel approach to entropy estimation for neural spike trains
* The authors provide a clear and concise explanation of the underlying mathematical framework
* The approach is demonstrated to be effective in simulations and real data examples
* The paper meets the conference guidelines for quality, clarity, originality, and significance
Arguments con acceptance:
* The paper could benefit from a more detailed comparison with other methods
* The authors could further evaluate the performance of their estimators in situations with higher-order dependencies
* Some minor errors were noted, but these do not detract from the overall quality of the paper.