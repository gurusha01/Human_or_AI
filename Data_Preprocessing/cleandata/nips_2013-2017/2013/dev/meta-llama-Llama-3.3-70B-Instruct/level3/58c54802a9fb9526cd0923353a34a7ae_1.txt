This paper presents a novel approach to transfer learning in reinforcement learning domains, where a long-lived agent must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions learned from previous tasks to speed up learning in new tasks. The authors build on the insight of recent work on the optimal rewards problem, which shows that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational bounds of the agent.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach. The experimental evaluation is thorough, and the results demonstrate the effectiveness of the proposed reward mapping approach in improving the agent's performance relative to other approaches, including a competing policy transfer method.
However, there are some limitations to the paper. One major concern is the lack of comparison to other transfer learning methods, such as policy transfer and reward shaping schemes. The authors only compare their approach to a conventional agent that ignores the possibility of transfer and to a non-transfer ORP agent that exploits the optimal rewards problem but ignores transfer. This leaves doubts about the effectiveness of synthetic reward functions compared to other approaches.
Additionally, the paper could benefit from a more detailed analysis of the reward mapping function and its properties. The authors provide some visualizations of the learned reward mapping function, but a more thorough understanding of its behavior and limitations would be useful.
In terms of clarity, the paper is generally well-organized, but there are some minor issues with formatting and citation style. The use of parenthetical citations as nouns can be distracting, and the reference formatting could be improved.
Overall, the paper presents a novel and interesting approach to transfer learning in reinforcement learning domains. While there are some limitations and areas for improvement, the paper is well-written, and the results are promising. With some additional comparisons to other transfer learning methods and a more detailed analysis of the reward mapping function, this paper could make a significant contribution to the field.
Arguments pro acceptance:
* The paper presents a novel and interesting approach to transfer learning in reinforcement learning domains.
* The experimental evaluation is thorough, and the results demonstrate the effectiveness of the proposed reward mapping approach.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach.
Arguments con acceptance:
* The paper lacks comparison to other transfer learning methods, such as policy transfer and reward shaping schemes.
* The analysis of the reward mapping function and its properties could be more detailed.
* There are some minor issues with formatting and citation style.