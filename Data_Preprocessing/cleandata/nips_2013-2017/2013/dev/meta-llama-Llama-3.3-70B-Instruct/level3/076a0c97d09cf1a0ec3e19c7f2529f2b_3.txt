This paper proposes a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, which is a departure from traditional transfer learning approaches that focus on reusing value functions, policies, or models. The authors build on the optimal rewards problem (ORP) framework, which separates the evaluation and guidance roles of reward functions, and propose a reward mapping function that maps task-specifying reward functions to good initial guidance reward functions for subsequent tasks.
The paper is well-written, and the authors provide a clear and concise introduction to the problem, related work, and their approach. The technical contributions are sound, and the empirical evaluation is thorough and well-designed. The results demonstrate that the proposed reward mapping approach can substantially improve the performance of a bounded agent relative to other approaches, including a competing policy transfer method.
The strengths of the paper include:
* A clear and well-motivated introduction to the problem and the proposed approach
* A thorough and well-designed empirical evaluation
* A sound technical contribution that builds on existing work in the optimal rewards problem framework
* A clear and concise writing style
The weaknesses of the paper include:
* The paper assumes that the agent has perfect knowledge of the task specifications and the environment dynamics, which may not always be the case in practice
* The paper focuses on a specific type of transfer learning, namely reusing reward functions, and it is not clear how the approach would generalize to other types of transfer learning
* The paper could benefit from a more detailed analysis of the computational overhead of the proposed approach and its potential scalability to larger and more complex tasks
Overall, I believe that this paper makes a significant contribution to the field of transfer learning and reinforcement learning, and it has the potential to be of interest to a wide range of researchers and practitioners. The paper is well-written, and the technical contributions are sound, making it a strong candidate for acceptance.
Arguments pro acceptance:
* The paper proposes a novel and well-motivated approach to transfer learning in long-lived agents
* The empirical evaluation is thorough and well-designed, and the results demonstrate the effectiveness of the proposed approach
* The paper builds on existing work in the optimal rewards problem framework and makes a significant contribution to the field
Arguments con acceptance:
* The paper assumes perfect knowledge of the task specifications and environment dynamics, which may not always be the case in practice
* The paper focuses on a specific type of transfer learning, and it is not clear how the approach would generalize to other types of transfer learning
* The paper could benefit from a more detailed analysis of the computational overhead and scalability of the proposed approach.