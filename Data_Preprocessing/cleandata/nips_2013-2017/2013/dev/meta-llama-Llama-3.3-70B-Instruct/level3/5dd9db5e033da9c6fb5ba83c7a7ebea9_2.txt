This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key insight is to reuse reward functions across tasks, which is counterintuitive as rewards are typically considered immutable parts of the task description. The authors build on the optimal rewards problem (ORP) framework, which separates the evaluation and guidance roles of reward functions. They propose a reward-mapping agent that learns a mapping from objective reward function parameters to internal reward function parameters, allowing for effective initialization of the guidance reward function for subsequent tasks.
The paper is well-written and clear, with a thorough introduction to the background and related work. The authors provide a detailed description of their approach, including the reward-mapping agent and its components. The empirical evaluation is comprehensive, with experiments on two domains: a grid world and a networking routing domain. The results demonstrate the performance advantage of the reward-mapping agent over other approaches, including a competing policy transfer method.
However, there are some areas that could be improved. The paper lacks sufficient experimental evaluation details for reproducibility, such as the specific hyperparameters used for the UCT algorithm and the reward-mapping function. Additionally, the theoretical guarantees for the proposed method's running time are not provided. The paper could also benefit from additional clarification on certain points, such as the solution method via level sets and the definition of y(A).
The approach is original in its combination of known techniques, but relies on specialized methods for projecting onto certain subproblems. The paper has significant potential impact due to the pervasiveness of submodular functions in machine learning, but could be improved with additional details and comparisons to existing methods.
Arguments pro acceptance:
* The paper presents a novel and original approach to transfer learning in long-lived agents.
* The empirical evaluation is comprehensive and demonstrates the performance advantage of the proposed method.
* The paper has significant potential impact due to the pervasiveness of submodular functions in machine learning.
Arguments con acceptance:
* The paper lacks sufficient experimental evaluation details for reproducibility.
* The theoretical guarantees for the proposed method's running time are not provided.
* The paper could benefit from additional clarification on certain points, such as the solution method via level sets and the definition of y(A).
Overall, I recommend accepting this paper, but with revisions to address the areas mentioned above. The paper has the potential to make a significant contribution to the field of transfer learning and long-lived agents, and with some revisions, it can be even stronger.