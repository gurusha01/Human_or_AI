This paper proposes two novel Bayesian entropy estimators, ĤDBer and ĤDSyn, for binary neural spike words. The estimators utilize a mixture-of-Dirichlet prior and outperform established estimators with fewer samples. The key strength of these estimators is their ability to incorporate prior information about the structure of the problem, reducing the number of required samples and evading the curse of dimensionality.
The paper is well-written, and the proposed entropy estimators extend existing work by incorporating prior knowledge about the structure of spike trains. The method's performance on synthetic and retinal ganglion cell data is impressive, demonstrating a significant improvement over traditional methods. The use of a synchrony distribution as a base measure allows the estimators to capture the sparsity structure of multi-neuron recordings and provides a flexible framework for incorporating prior knowledge.
However, the generalizability of the method to other neural data is unclear due to a critical underlying assumption about the word distribution. The paper assumes that the word distribution can be captured by a synchrony distribution, which may not hold for all types of neural data. Additionally, the choice of hyper-priors on α and p may affect the performance of the estimators, and further investigation is needed to determine the optimal choice of hyper-priors.
The significance of this work lies in its potential to advance the state of the art in neural coding analyses. The proposed estimators provide a powerful tool for information quantification in neural coding analyses, and their ability to capture higher-order dependencies in neural data makes them a valuable contribution to the field. The application of the estimators to real neural data demonstrates their potential to provide new insights into the coding of neural time-series, and the availability of a MATLAB implementation will facilitate their adoption by the research community.
Arguments pro acceptance:
* The paper proposes novel and powerful Bayesian entropy estimators that outperform established methods.
* The estimators incorporate prior knowledge about the structure of spike trains, reducing the number of required samples.
* The method's performance on synthetic and retinal ganglion cell data is impressive.
* The paper is well-written, and the proposed estimators extend existing work in a significant way.
Arguments con acceptance:
* The generalizability of the method to other neural data is unclear due to a critical underlying assumption about the word distribution.
* The choice of hyper-priors on α and p may affect the performance of the estimators, and further investigation is needed.
* The paper could benefit from additional analysis and discussion of the limitations and potential applications of the proposed estimators.