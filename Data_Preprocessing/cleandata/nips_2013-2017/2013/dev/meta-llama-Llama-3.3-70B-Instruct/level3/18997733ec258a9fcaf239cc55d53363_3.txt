This paper proposes a novel approach to estimating the entropy of discrete distributions, specifically in the context of neural spike trains. The authors introduce two Bayesian entropy estimators, ĤDBer and ĤDSyn, which utilize a hierarchical mixture-of-Dirichlets prior with a base measure designed to incorporate prior knowledge about spike trains. The paper is well-written, and the ideas are clearly presented.
The strengths of the paper include the development of a novel approach to entropy estimation, which is shown to outperform existing methods in simulations and real-data experiments. The use of a hierarchical mixture-of-Dirichlets prior with a base measure designed to capture the statistical structure of spike trains is a key innovation. The paper also provides a detailed analysis of the properties of the estimators and demonstrates their effectiveness in capturing higher-order dependencies in neural data.
However, there are some weaknesses to the paper. The regret analysis is disappointing, as it is provided for a different algorithm than the one proposed and evaluated, with no clear explanation or discussion on the differences. The analysis lacks clarity on the behavior of certain terms and their dependence on variables, making it difficult to gain insight from the provided regret bound. Additionally, the paper lacks a guideline for setting the exploration parameter, and the dependence of the upper bound on this parameter is unclear.
The experiment section is well-done, with detailed descriptions and real-data experiments, making it reproducible. However, the use of graph and Laplacian regularization is questioned, with suggestions for alternative approaches, such as regularizing the Laplacian instead of using clustering as a regularizer. The inclusion of theoretical analysis for a changed algorithm is confusing and may not be useful, and removing it could improve the paper's score.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and significance. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, and the ideas are well-organized and easy to follow. The paper addresses a difficult problem in a better way than previous research and provides unique insights and approaches to entropy estimation.
Arguments for acceptance:
* The paper proposes a novel approach to entropy estimation, which is shown to outperform existing methods.
* The use of a hierarchical mixture-of-Dirichlets prior with a base measure designed to capture the statistical structure of spike trains is a key innovation.
* The paper provides a detailed analysis of the properties of the estimators and demonstrates their effectiveness in capturing higher-order dependencies in neural data.
Arguments against acceptance:
* The regret analysis is disappointing and lacks clarity.
* The paper lacks a guideline for setting the exploration parameter, and the dependence of the upper bound on this parameter is unclear.
* The use of graph and Laplacian regularization is questioned, and alternative approaches may be more effective.
Overall, I would recommend accepting the paper, as it provides a significant contribution to the field of entropy estimation and neural data analysis. However, the authors should address the weaknesses mentioned above, particularly the regret analysis and the lack of guidelines for setting the exploration parameter.