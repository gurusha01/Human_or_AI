This paper proposes a novel method for conserving earlier experiences in solving reinforcement learning (RL) problems using different reward functions, assuming limited learning power. The approach falls under the category of "reward-shaping" and is considered a welcome perspective on reinforcement learning. The authors introduce a surrogate reward function to learn the concrete reward, allowing the agent to learn an inner reward function as a function of the external reward function.
The paper's strengths include its originality and effectiveness in addressing a difficult problem in RL. The methodology is well-motivated, and the use of a surrogate reward function is a clever approach to learning the inner reward function. The experimental results demonstrate the success of the proposed method in various examples, including network routing domains.
However, the paper has some clarity issues, particularly in the network routing domain, where specific lines and sections require reformulation for better understanding. Additionally, there are minor weaknesses in the description of the second example, which could be improved for clarity.
The key contributions of this paper are:
* A novel approach to conserving earlier experiences in RL problems using different reward functions
* The introduction of a surrogate reward function to learn the concrete reward
* The use of a reward-shaping methodology to address a difficult problem in RL
The paper's weaknesses include:
* Clarity issues in certain sections, particularly in the network routing domain
* Minor weaknesses in the description of the second example
Overall, I believe that this paper is a significant contribution to the field of reinforcement learning and warrants publication. The proposed method is original, effective, and addresses a challenging problem in RL. While there are some clarity issues and minor weaknesses, these can be addressed through revisions to improve the paper's overall quality.
Arguments pro acceptance:
* Original and effective approach to conserving earlier experiences in RL problems
* Well-motivated methodology with a clear contribution to the field
* Experimental results demonstrate the success of the proposed method
Arguments con acceptance:
* Clarity issues in certain sections, particularly in the network routing domain
* Minor weaknesses in the description of the second example
Recommendation: Accept with minor revisions to address clarity issues and improve the overall quality of the paper.