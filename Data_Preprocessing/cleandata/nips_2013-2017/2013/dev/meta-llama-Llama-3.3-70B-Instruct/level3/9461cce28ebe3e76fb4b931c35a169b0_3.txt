This paper proposes a novel method for unsupervised learning, called sparse heterarchical matrix factorization (SHMF), which decomposes a matrix of observations into a product of multiple sparse matrices. The method is motivated by the analysis of calcium imaging data in neuroscience, where there is a natural hierarchy of concepts, such as pixels, neurons, and assemblies. The authors demonstrate the effectiveness of their method on both synthetic and real-world data, showing that it can recover the structure of the data and provide plausible interpretations.
The strengths of the paper include its clear motivation, well-organized structure, and convincing experimental results. The authors provide a thorough review of related work and clearly explain the limitations of existing methods. The proposed method is novel and has the potential to be applied to a wide range of problems.
However, there are several weaknesses and limitations of the paper. Firstly, the method assumes a finite hypothesis space, which may not be applicable to infinite hypothesis spaces. Additionally, the algorithm's update rule for the weight of the weak learner seems to use a convex loss function, contradicting the initial use of 0-1 loss. The method also assumes that the data is always separable, which may not be the case if the chosen hypothesis space is not complex enough.
Furthermore, the calculation of the value of d, which determines the starting point for margin maximization, is unclear and lacks a specific definition. The algorithm does not account for negative margins in inseparable cases, which could prevent the second part of the algorithm from running. There is also a contradiction between the statement that the algorithm may escape the region of minimum training error and the design of algorithm 2, which does not allow this.
The computational cost of DirectBoost is higher than AdaBoost, and the stopping criterion used for AdaBoost in the experiments is unclear. The advantages of the proposed method over regular boosting methods are not clearly demonstrated and lack convincing evidence.
In terms of quality, the paper is well-written and easy to follow, but there are some areas that require improvement. The notation is clear, but some of the equations are difficult to understand without a background in matrix factorization. The experimental results are convincing, but more analysis is needed to fully understand the performance of the method.
In terms of originality, the paper proposes a novel method that combines sparse matrix factorization with hierarchical clustering. The method is novel and has the potential to be applied to a wide range of problems. However, the paper could benefit from more discussion of related work and how the proposed method differs from existing methods.
In terms of significance, the paper addresses an important problem in neuroscience and provides a novel solution. The method has the potential to be applied to a wide range of problems and could have a significant impact on the field. However, more analysis is needed to fully understand the performance of the method and its limitations.
Overall, I would recommend accepting this paper, but with some revisions to address the weaknesses and limitations mentioned above. The paper has the potential to make a significant contribution to the field, but more work is needed to fully realize its potential.
Arguments pro acceptance:
* The paper proposes a novel method that combines sparse matrix factorization with hierarchical clustering.
* The method is motivated by a real-world problem in neuroscience and has the potential to be applied to a wide range of problems.
* The experimental results are convincing and demonstrate the effectiveness of the method.
Arguments con acceptance:
* The method assumes a finite hypothesis space, which may not be applicable to infinite hypothesis spaces.
* The algorithm's update rule for the weight of the weak learner seems to use a convex loss function, contradicting the initial use of 0-1 loss.
* The method does not account for negative margins in inseparable cases, which could prevent the second part of the algorithm from running.
* The computational cost of DirectBoost is higher than AdaBoost, and the stopping criterion used for AdaBoost in the experiments is unclear.