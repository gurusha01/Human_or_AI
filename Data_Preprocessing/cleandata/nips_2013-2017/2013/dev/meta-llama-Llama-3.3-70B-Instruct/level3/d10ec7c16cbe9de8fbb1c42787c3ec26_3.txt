This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, which is a significant departure from traditional approaches that focus on reusing value functions, policies, or models. The authors build on the insight from the optimal rewards problem that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational bounds in the agent architecture.
The paper is of excellent quality, with clear and well-organized writing that effectively conveys the main ideas and contributions. The authors provide a thorough background on the optimal rewards problem and its application to single-task settings, which serves as a foundation for their extension to long-lived agents. The proposed reward mapping approach is well-motivated, and the empirical evaluation on two domains (grid world and network routing) demonstrates its effectiveness in improving the agent's performance.
The strengths of the paper include its originality, technical soundness, and significance. The authors address a important problem in transfer learning, and their approach has the potential to advance the state of the art in this area. The paper is also well-supported by theoretical analysis and experimental results, which provides a clear understanding of the benefits and limitations of the proposed approach.
One potential weakness of the paper is the lack of a more detailed analysis of the conditions under which the reward mapping approach is effective. The authors provide some insights into the factors that influence the performance of the approach, such as the mean task duration and the similarity between tasks, but a more thorough investigation of these factors would be beneficial.
Some questions that arise from reading the paper include: What are the typical values of the internal reward function parameters, and how do they relate to the objective reward function parameters? Under what conditions is the reward mapping approach more effective than other transfer learning methods, such as policy reuse or value function reuse? Can the reward mapping approach be extended to more complex domains, such as those with high-dimensional state and action spaces?
Overall, the paper makes a significant contribution to the field of transfer learning and reinforcement learning, and its ideas and approaches have the potential to be applied to a wide range of problems. The authors' writing is clear and concise, making the paper accessible to a broad audience. 
Arguments pro acceptance:
- The paper presents a novel and original approach to transfer learning in long-lived agents.
- The approach is well-motivated and supported by theoretical analysis and experimental results.
- The paper addresses an important problem in transfer learning and has the potential to advance the state of the art in this area.
Arguments con acceptance:
- The paper could benefit from a more detailed analysis of the conditions under which the reward mapping approach is effective.
- The authors could provide more insights into the factors that influence the performance of the approach, such as the mean task duration and the similarity between tasks.
- The paper may not be accessible to readers without a strong background in reinforcement learning and transfer learning.