This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, which is a departure from traditional transfer learning methods that focus on reusing value functions, policies, or models. The authors build on the optimal rewards problem (ORP) framework, which separates the task-specifying objective reward function from the internal reward function used to guide agent behavior.
The paper introduces a new agent architecture, called the Reward-Mapping-Transfer ORP Agent, which learns a reward mapping function that maps objective reward function parameters to internal reward function parameters. This mapping function is used to initialize the internal reward function at the beginning of each new task, and is updated using the policy gradient reward design (PGRD) method.
The authors evaluate their approach on two domains: a grid world domain and a networking routing domain. The results show that the Reward-Mapping-Transfer ORP Agent outperforms other agent architectures, including a conventional agent that ignores transfer and ORP, a non-transfer ORP agent, and a sequential-transfer ORP agent. The authors also demonstrate that their approach can be extended to handle changes in the transition function and to multi-agent settings.
The strengths of this paper include its novel approach to transfer learning, its thorough evaluation on two domains, and its potential to advance the state of the art in long-lived agent learning. The weaknesses of the paper include the lack of clear descriptions of the novel techniques, such as the application of the DR variant to dual problems with more than two functions, and the unclear presentation of the algorithms. Additionally, the iteration complexity of the algorithms is not well understood, particularly with respect to the number of functions being decomposed.
Overall, I would argue in favor of accepting this paper, as it presents a significant contribution to the field of transfer learning and long-lived agent learning. However, I would suggest that the authors clarify their notation, provide more explicit details on the novel aspects of their approach, and address the issues with iteration complexity and algorithm presentation.
Pros:
* Novel approach to transfer learning that reuses reward functions across tasks
* Thorough evaluation on two domains
* Potential to advance the state of the art in long-lived agent learning
Cons:
* Lack of clear descriptions of novel techniques
* Unclear presentation of algorithms
* Iteration complexity of algorithms not well understood
* Missing references and inconsistencies in figures and empirical evaluation
Recommendations for improvement:
* Clarify notation and provide more explicit details on novel aspects of the approach
* Address issues with iteration complexity and algorithm presentation
* Provide more references to related work and ensure consistency in figures and empirical evaluation.