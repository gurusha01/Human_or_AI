This paper proposes a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The authors establish a near-optimal convergence rate for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. The paper is well-written, with a clear and concise introduction to the problem and a thorough explanation of the proposed method.
The strengths of the paper include its novel and practically relevant framework, elegant results, and efficient algorithm for solving the proposed semidefinite program. The authors provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices, which extends the applicability and provable guarantees to a wide array of settings. The simulation results demonstrate the effectiveness of the proposed method compared to existing deflation-based methods.
One potential weakness of the paper is that the proof of Theorem 3.1 relies on a restricted sub-Gaussian deviation inequality, which may not hold in all cases. Additionally, the authors assume that the input matrix S satisfies certain conditions, such as symmetry and identifiability, which may not always be true in practice.
To improve the paper, the authors could consider providing more discussion on the choice of regularization parameter λ and the dimension d, as these are crucial in practice. They could also explore extensions of the method to other types of penalties and provide more numerical examples to demonstrate the robustness of the method.
Overall, the paper makes a significant contribution to the field of sparse principal subspace estimation and provides a valuable tool for practitioners. The results are important, and the paper is well-written and easy to follow. I would recommend accepting this paper for publication.
Arguments pro acceptance:
* The paper proposes a novel and elegant method for sparse principal subspace estimation.
* The authors provide a thorough theoretical analysis of the method, including a near-optimal convergence rate.
* The simulation results demonstrate the effectiveness of the proposed method compared to existing methods.
* The paper is well-written and easy to follow.
Arguments con acceptance:
* The proof of Theorem 3.1 relies on a restricted sub-Gaussian deviation inequality, which may not hold in all cases.
* The authors assume that the input matrix S satisfies certain conditions, such as symmetry and identifiability, which may not always be true in practice.
* The paper could benefit from more discussion on the choice of regularization parameter λ and the dimension d.