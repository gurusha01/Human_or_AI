This paper presents a novel approach to transfer learning in long-lived agents, where the agent must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, rather than relearning them from scratch. The authors build on the optimal rewards problem (ORP) framework, which separates the task-specifying objective reward function from the internal reward function used to guide agent behavior. The main contribution is a reward mapping function that maps objective reward function parameters to internal reward function parameters, allowing for efficient transfer of knowledge across tasks.
The paper is well-organized and clearly written, with a thorough introduction to the background and related work. The authors provide a detailed description of the four agent architectures, including the conventional agent, non-transfer ORP agent, reward-mapping-transfer ORP agent, and sequential-transfer ORP agent. The empirical evaluation is comprehensive, with experiments on both a grid world domain and a networking routing domain.
The results demonstrate that the reward mapping approach can substantially improve the performance of the bounded agent, especially when task durations are short. The authors also show that the reward mapping function can be learned effectively using nonparametric kernel-regression, and that the approach can be extended to handle changes in the transition function as well as the objective reward function.
The paper has several strengths, including its novel approach to transfer learning, its thorough empirical evaluation, and its clear and well-organized writing. However, there are also some weaknesses. One potential limitation is that the approach relies on the availability of a good guidance reward function for the previous tasks, which may not always be the case. Additionally, the authors could provide more discussion on the implications and insights of the theory for obtaining efficient algorithms, as well as more context and explanation for readers unfamiliar with the tools and settings used.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, with well-supported claims and a complete piece of work. The writing is clear and well-organized, making it easy to follow and understand. The approach is novel and original, with a unique combination of familiar techniques. The results are significant, with important implications for the field of reinforcement learning and transfer learning.
Overall, I would recommend accepting this paper, with some minor revisions to address the weaknesses mentioned above. The paper makes a significant contribution to the field, and its strengths outweigh its weaknesses. 
Arguments pro acceptance:
- Novel approach to transfer learning
- Comprehensive empirical evaluation
- Clear and well-organized writing
- Significant results with important implications for the field
Arguments con acceptance:
- Relies on availability of good guidance reward function
- Could provide more discussion on implications and insights of theory
- Could provide more context and explanation for unfamiliar readers