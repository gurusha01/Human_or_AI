The authors utilize the Optimistic Mirror Descent algorithm to tackle various problems where the smoothness of the loss function sequences induces a certain level of predictability. Specifically, improved rates can be achieved in two scenarios: 1) when the objective function G remains fixed and smooth for all iterations, or 2) in saddle point problems involving a function phi(f, x) that is smooth with respect to each argument, and the optimistic mirror descent algorithm is applied against itself. Although these notions of predictability are somewhat specialized, they yield intriguing results. The paper extends this concept to zero-sum games in Section 4 and explores approximate smooth convex programming in Section 5.
Unlike many online algorithm applications, the primary objective in most cases presented in this paper is to solve a batch optimization problem, which should be explicitly stated in the introduction.
In Section 3, the authors need to clarify why this section improves upon the result from Lemma 3. The key insight appears to be that the smoothness assumptions in Corollary 5 may be satisfied even if G(f) = inf_x phi(f, x) is not smooth. An alternative approach to solving the problem in Section 3 is to employ an online algorithm against a best-response adversary. It is worth investigating whether any online algorithm paired with a best-response oracle can achieve convergence rates similar to 1/T produced by the procedure introduced here.
Section 4 should provide clearer explanations of how it improves upon Section 3. The main contribution seems to be the addition of robustness to an arbitrary adversary, along with good bounds for the predictable one. The results in Section 4.2 are somewhat weaker than the section title suggests. It would be more accurate to describe this setting as multi-point bandit feedback and cite the relevant work "Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback". The notational similarities suggest that the authors are familiar with this work, and it should have been cited in the submission. Furthermore, it is surprising that four points are required, rather than just two, which warrants discussion.
In Section 5, it is unclear whether assuming F^ is known without loss of generality is justified. At the very least, the binary search procedure needs to be precisely defined: Is the algorithm from Section 5 run at each step of the binary search? If so, how is it determined whether the guess of F^ is too high or too low? How does using a guess of F^* that is off by some amount delta impact the final bound of Lemma 8? This also applies to Section 5.1.
Additionally, Equation (11) appears to fit into the setup of Section 3 (Corollary 5), so it is unclear why the machinery of Section 4.2 (Lemma 7) is necessary. It is also possible that Line 368 contains a typo.
The paper suffers from several typos that made the review process more challenging and could benefit from a more comprehensive discussion of related work, including several specific citations.
Some local points to consider:
- Line 112: The simple intuition and analysis for Mirror Prox are well-presented.
- Line 93: This bound can be seen as a direct generalization of the result that the "Be The Leader" (BTL) algorithm suffers no regret, as it arrives at a BTL algorithm by taking Deltat = Mt. This is worth mentioning, along with a citation to "Efficient algorithms for online decision problems" (Adam Kalai and Santosh Vempala).
- Line 131: The section title "Saddle-Point Optimization" would be more descriptive.
- Lines 154-155: Citing "Adaptive game playing using multiplicative weights" (Yoav Freund and Robert E. Schapire) is likely appropriate, and this citation is also relevant at lines 208-209.
- Line 223: More detail is needed to explain why Section 3 does not lead to this result (possibly due to its inability to handle an arbitrary adversary) and why a direct application of mirror-prox does not provide strong decoupling.
- Line 225, as well as lines 265 and 268: It is imprecise to refer to regret bounds like 1/T or 1/Sqrt(T) without multiplying by T to obtain regret as defined on line 65. Alternatively, these can be referred to as convergence rates.
- Lines 312-314: There are typos in both "Build estimates" sections of the pseudo-code, which should be corrected to r^+ - r^-.
- Line 366: Epsilon should be introduced as an arbitrary constant eps > 0 earlier in the statement.
- Line 368: It is possible that Corollary 5 is intended instead of Lemma 7. If Lemma 7 is indeed intended, the procedure needs to be clarified, including the definition of the matrix A.
- Appendix: The proofs rely heavily on copy-paste, making them harder to read due to repetition. The proofs should be restructured to be less verbose and more clearly present the main arguments. The paper explores new applications of Optimistic Mirror Descent, including a simple analysis and intuition for fixed smooth functions and an analysis applicable to self-play of the algorithm. The results appear novel and interesting.