Summary of ideas: 
A known proximal problem, whose minimization yields at least as much information as minimizing the Lovasz extension, has been previously identified. 
When a submodular function can be expressed as a sum of simpler submodular functions, each of which is relatively easy to solve, the dual of the proximal problem can be solved using projections rather than non-smooth functions, as was the case with previous methods.
Evaluation: 
This paper presents an improved approach to handling submodular functions that can be decomposed into simpler, additively separable functions, leveraging novel dual decompositions based on a lesser-known application of the Lovasz extension. 
These decompositions enable the process to proceed without the complications associated with non-smoothness that arose in previous approaches. The empirical evaluation, which utilizes examples from image processing, demonstrates promising results. 
However, the paper's clarity in describing its predecessors comes at the cost of providing only a sketchy overview of the novel techniques. 
For instance, a key technical contribution is the application of the DR variant from reference [5] to dual (9), which is specialized for the sum of two functions. Nevertheless, since [5] also focuses on two functions, it is unclear how to apply these algorithms when dealing with more than two functions (r > 2), despite the reported usefulness of such results in the experimental evaluation section under the name DR-para. 
Furthermore, the iteration complexity's dependence on r remains unknown. 
In another example, the detailed description of the third main contribution is essentially relegated to the supplementary material, which neither formally states an algorithm nor a theorem. 
Pros: 
- Submodular functions that are decomposable arise in various applications, and developing more suitable methods for their minimization is beneficial. 
- The novel duals may inspire further research on algorithms for their solution, extending beyond the algorithms proposed in this work. 
Cons: 
- While the paper discusses iteration and computational complexity bounds as limitations of previous results, it does not provide corresponding iteration complexity results for the novel optimization problems and algorithms. Although a comprehensive analysis can be deferred to future work, some understanding of how r affects complexity under the different algorithms is lacking. 
- The presentation of the algorithms, as opposed to the duals, is not sufficiently clear. 
Detailed comments: 
- On page 072, the phrase "the level set {k, xk^ \ge 0} of ... x^" is unclear; it is presumed that {k | xk^* \ge 0} is intended. Reference [2] refers to multiple problems, and a more specific reference for the mentioned fact would be helpful. 
- At 088, it would be beneficial to reference section 3, where examples of this decomposition are provided, and offer insight into the types of simplicity that might be encountered for the summand functions. 
- The notation a(A) where a\in R^n and A\subseteq V appears to be introduced without prior explanation. 
- The statement at 117 is misleading, as Section 2 defers to the Supplementary material, which also lacks explicit detail. 
- At 138, "computed the" should be "computed by the". 
- The term "discrete gaps" used at 156 and elsewhere is not clearly defined; it is presumed to refer to the duality gaps of F(A) and a particular dual problem, similarly for smooth gaps. 
- The statement "In the appendix.." at 170 should be accompanied by an explicit algorithm, along with a lemma and proof providing its complexity, neither of which is present in the paper or the supplementary material. 
- The problems' sizes are stated as 640x427, implying r = 640 + 247, but either a typo exists (427 to 247) or the relationship is not clearly explained. Additionally, it is worth clarifying how and why the problems are "solved as r=2/3". 
- Figure 2: 
  - The inconsistencies in the vertical and horizontal axes make comparisons challenging. 
  - The phrase "From top to bottom: four different images" likely refers to 2 different images. A broader benchmark would be beneficial. 
- Figure 3: 
  - It is unclear whether "non-smooth problems" corresponds to the previously existing duals, as the legends suggest that the new duals are being used. 
- The empirical evaluation in general: comparisons of convergence in terms of iteration counts are informative only when iteration costs are roughly comparable; it is unclear whether they are. 
The interesting duals presented are accompanied by algorithms that are not well-presented. The presentation should focus on providing explicit detail on the novel aspects, potentially at the expense of some detail on existing methods.