The authors tackle the challenge of sparse principal component analysis when estimating multiple components, a problem with the conventional sequential approach that employs a "deflation" step to transition between components. This method's fragility is a valid concern. The proposed solution involves estimating the matrix by projecting onto the Fantope, a convex relaxation of orthogonal matrices.
The paper presents an efficient algorithmic framework for solving the problem and provides a statistical analysis of the solution. The writing is clear, and the topic's popularity will likely garner interest. However, a crucial question arises: what is the rationale behind the orthogonality constraint in sparse PCA? In standard PCA, orthogonality is a natural consequence of the eigenvalue decomposition providing a low-rank matrix approximation with orthogonal factors. The introduction of a sparsity assumption does not necessarily justify retaining orthogonality.
The experimental section falls short of expectations. A phase transition diagram illustrating the performance advantages over rival methods as the factor supports overlap would be more convincing. Notably, Journee et al. (including all authors listed in reference [8]) have previously addressed the estimation of multiple principal components simultaneously. A comparison with their method, for which code is available online, is surprisingly absent. Additionally, a numerical comparison with a sparse matrix factorization method and an analysis of the supports found by each method on real data would be valuable.
Overall, the paper addresses a relevant problem, is well-written, and proposes an efficient method. Nevertheless, a more compelling discussion on the orthogonality constraints, which seem more limiting than beneficial, is needed. The algorithm builds upon DSPCA [1] by incorporating the Fantope constraint, and while the theoretical results are mostly incremental, they are not particularly groundbreaking.