This manuscript proposes an alternative approach to selecting intermediate distributions for Annealed Importance Sampling (AIS) on exponential family distributions. The experimental findings are encouraging and may have substantial practical implications, as they could enhance the ability of researchers to make objective comparisons between complex probabilistic models. Overall, the paper is well-received, although the theoretical justification for the proposed intermediate distributions could be more comprehensive.
The derivation in Equation 3 is noteworthy. However, the terminology "high curvature" is misleading; instead, it refers to a large information distance per unit change in the parameter theta. 
In Section 5.1, it is unclear why a larger variance of 1437.89 is considered superior, as one would expect larger variance to be detrimental. 
The experimental results are limited to second-order models, whereas the technique is intended for arbitrary exponential family models. It would be beneficial to include experiments with different sufficient statistic choices.
On page 7, the term "substantially" should be quantified to provide a clearer understanding.
Some speculative ideas are presented below:
The manuscript does not provide a clear explanation for why interpolating between sufficient statistics is preferable to interpolating between parameter values. 
Equations 4 and 5 suggest that the optimal path in parameter space is the geodesic path (minimal distance) between the two parameterizations, with the Fisher information matrix serving as the metric. 
If the algebra is correct, then $ds/d\theta = G$ (obtained by writing $s = \int dx p(x) dE/d\theta$ and taking the gradient). If the path is linear in $s$, as proposed, then for small steps $\Delta s$, $\Delta \theta = d\theta / ds \Delta s = G^{-1} \Delta s$. An infinitesimal step along the proposed trajectory in terms of $\theta$ resembles the step in $s$ scaled by the inverse Fisher information matrix, which is analogous to the natural gradient update.
Interpreting the algorithm in terms of a natural gradient trajectory opens up possibilities for approximations and extensions, such as:
- Applying the technique to non-exponential family models, where sufficient statistics are not defined, but the Fisher information is still available.
- Utilizing well-motivated metrics other than the Fisher information, such as the matrix natural gradient (Amari, S.-I. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation).
- Employing an approximation to $G^{-1}$, which can be computed more efficiently (Sohl-Dickstein, J. (2012). The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use. arXiv:1205.1828v1).
- Iterating over choices for the entire trajectory to find the global shortest trajectory (with metric $G$), rather than just taking local shortest steps.
Finally, sharing source code for experiments in the supplemental material can significantly enhance the usefulness and credibility of the work, and increase the number of citations earned. As a reviewer, it also positively influences my assessment of the paper. 
In summary, the experimental results are promising, but the theoretical motivation is not fully satisfying, and the results may have significant practical benefits for comparing complex probabilistic models.