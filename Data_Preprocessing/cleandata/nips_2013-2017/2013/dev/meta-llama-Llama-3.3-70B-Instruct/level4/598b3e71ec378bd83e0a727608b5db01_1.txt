The paper explores the potential for parallelizing the best arm identification problem, specifically examining how running k instances of an algorithm in parallel, with limited information sharing between them, can achieve speed-up. The information sharing process involves each instance broadcasting a restricted amount of information, typically of size \tilde{O}(n), to the other algorithms, often at the end of the run. 
The authors compare their proposed algorithm to three existing methods: full-communication, no-communication, and the majority vote (MV) method. They argue that the full-communication method generates excessive network data, the no-communication method yields no speed-up, and the MV method also achieves no speed-up. In contrast, their algorithm achieves a speed-up of \sqrt{k}, which they demonstrate is optimal when only one round of communication is permitted. 
Furthermore, they show that allowing multiple rounds of communication introduces a trade-off between achievable speed-up, up to k, and the amount of information exchanged. 
EVALUATION 
The paper's writing style is generally clear, the proposed solutions are clever yet simple, and the problem tackled is both non-trivial and well-motivated, with a compelling example in the distributed computation model of MapReduce. 
However, two key clarifications are necessary. 
Firstly, the authors should provide a precise definition of "speed-up" and formally state their objectives. A natural interpretation of speed-up could be when a distributed algorithm achieves a certain confidence level in the same time as a standalone algorithm, but with a significantly lower error probability. Under this definition, the MV method could indeed achieve speed-up, as increasing the number of instances and taking the majority vote could yield a much higher confidence level than a single instance. 
Additionally, the performance of the MV method deserves more nuanced treatment. While individual algorithms may need to output the best arm with a probability of at least 1/2 for the MV method to work in expectation, it is actually sufficient for the individual algorithms to output the best arm with a higher probability than any suboptimal arm. 
Two minor questions also arise: 
1. In the proof of Lemma 3.3, the multiplicator 6 in front of H/\sqrt{k} requires explanation. 
2. In the proof of Lemma 3.4, it appears that 12n should be divided by \Delta^\star in the logarithm. 
Overall, the paper presents a well-motivated problem, clever solutions, and a clear writing style, but requires clarification on the definition of speed-up and the treatment of the MV method.