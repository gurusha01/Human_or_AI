This manuscript introduces a boosting approach that directly minimizes empirical classification error, defined by the indicator of whether predicted and observed values of y match, known as 0-1 loss. The proposed method employs a two-step process, first utilizing greedy coordinate descent to find the coordinatewise minimum loss, followed by coordinate ascent to maximize the margin. The approach is novel and presents several advantages, but raises several concerns.
1. The authors appear to assume a hypothesis space with a finite number of hypotheses, as algorithms 1 and 2 iterate over all hypotheses at each step. For instance, algorithm 1 identifies all weak learners that yield the largest classification error reduction at each iteration. However, this may not be feasible if the hypothesis space is infinite, such as the space of linear functions.
2. The rationale behind updating the weight of the weak learner with the smallest exponential loss at the end of algorithm 1 is unclear. Although the algorithm initially uses 0-1 loss, it seems to revert to a convex loss function, which may not be consistent with the initial approach.
3. The method assumes that the data is separable as long as the combined strong classifier is robust. Nevertheless, this assumption may not always hold, particularly if the chosen hypothesis space is not complex enough to separate the given dataset. The paper claims that algorithm 1 reaches a coordinatewise minimum solution, with 0-1 loss reaching 0. However, the definition of coordinatewise minimum is unclear, and it is uncertain whether this represents a global minimum.
4. When initiating the second step of margin maximization, the algorithm starts from a region that does not exit the 0-loss region, characterized by the value of d. The calculation of this value is not clearly explained, and it is unclear why a simpler approach, such as determining the lowest sample with a decreasing margin, is not used.
5. The formulation assumes separable cases, resulting in positive margins. However, if there are negative margins for inseparable cases, the second part of the algorithm may not function as intended.
6. The manuscript states that the algorithm may escape the region of minimum training error to achieve a larger margin, but the design of algorithm 2 restricts the margin maximization step to the 0-1 loss region, which may limit its ability to generalize.
7. Compared to AdaBoost, DirectBoost appears to be computationally heavier per iteration, assuming similar computational costs for weak learners. The stopping criterion used for AdaBoost in the experiments is unclear, and it is surprising that AdaBoost requires so many iterations.
8. The advantages of the proposed method over regular boosting methods are not entirely convincing, and further clarification is needed to demonstrate its superiority. Overall, while the paper presents an interesting and novel approach, it raises several concerns that require addressing to fully appreciate its potential benefits.