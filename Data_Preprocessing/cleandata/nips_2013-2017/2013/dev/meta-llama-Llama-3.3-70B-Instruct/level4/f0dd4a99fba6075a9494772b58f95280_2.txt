Review- Optimization, Learning, and Games with Predictable Sequences 
Summary: 
This manuscript explores the Optimistic Mirror Descent (OMD) algorithm, a variant of the traditional online convex optimization method that leverages predictable patterns in the sequence of loss function gradients to enhance performance. 
The authors initially establish a specific substitution that reduces OMD to Mirror Prox and extend existing bounds to encompass Hoelder smooth functions. The paper then delves into the analysis of objectives characterized by a saddle-point structure, first examining straightforward min-max optimization using a single algorithm. Subsequently, it adopts a game-theoretic perspective, demonstrating how two opposing players can converge to a saddle point more rapidly when both employ the proposed algorithm, while maintaining a worst-case sqrt(T) regret in the absence of cooperation. The manuscript concludes by exploring approximate convex optimization, with a notable application to approximate max flow. 
Discussion: 
This investigation into the prediction of predictable sequences is meticulously executed and offers substantial contributions. The presentation is remarkably clear, and the results are compelling. A particularly noteworthy aspect is the application to saddle-point identification, where players cannot assume cooperative behavior yet still achieve a speedup when cooperation occurs. 
Minor corrections: 
On page 5, in Proposition 6, the notation references x0 and f0, which are not defined. 
On page 7, equation (11) contains a typographical error, where x[i] should be replaced with x_i. 
Overall, this outstanding paper examines the prediction of predictable sequences (of gradients) and yields significant results with intriguing technical applications.