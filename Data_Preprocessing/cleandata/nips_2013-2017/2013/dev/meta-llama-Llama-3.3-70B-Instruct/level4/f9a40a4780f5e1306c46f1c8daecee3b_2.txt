Paper summary: 
This manuscript presents two innovative entropy estimators tailored for binary neural spike words, leveraging a Bayesian approach with a mixture-of-Dirichlet prior. The hierarchical distribution utilizes a count distribution as the base measure for the Dirichlet distribution. The authors assess the efficacy of their methods using both artificial data and real data recorded from retinal ganglion cells, comparing their performance to established entropy estimators. The results indicate that their estimators achieve accurate estimations with fewer samples. Furthermore, the authors apply their method to quantify temporal synergy in retinal ganglion cell data, yielding promising outcomes.
Quality: 
The proposed Bayesian entropy estimators demonstrate considerable power by adeptly circumventing the curse of dimensionality. By incorporating prior knowledge about the problem's structure, the method significantly reduces the required number of samples. The superior performance of the proposed estimators over alternative methods on synthetic data is expected, given that the word data distributions align with the model structure of the estimators. However, their impressive performance on real data from retinal ganglion cells is particularly noteworthy. Nevertheless, it remains unclear how these methods would perform on other types of neural data, as the approach relies on the critical assumption that the word distribution can be accurately characterized by the overall count distribution. While this assumption appears to hold for retinal ganglion cell data, further evaluations are necessary to determine its general applicability. Acknowledging this limitation in the discussion would be beneficial. Nonetheless, the new estimators are undoubtedly highly useful.
Clarity: 
The paper is well-written and clear in its presentation.
Originality: 
The entropy estimators proposed in this work build upon the foundation established by Nemenman et al. (NIPS 2002) by integrating prior knowledge about the structure of spike trains. The underlying concept shares similarities with the raster marginals model (Okun et al., J Neurosci 2012), as both utilize the total spike count distribution to simplify and evade the curse of dimensionality.
Significance: 
Entropy estimation is a crucial problem in the field of neural coding analyses, where information quantification plays a central role. The demonstrated performance gain of the proposed methods compared to alternative approaches is impressive and highlights the significance of this contribution.
Minor points: 
080: The sentence "In Section 3 introduce" should be revised to "In Section 3, we introduce" for clarity.
163: A citation is missing and should be added for completeness.
232: The sentence is incomplete and requires revision for proper comprehension.
234: The phrase "the estimator fast-to-compute" should be corrected to "the estimator is fast-to-compute" for grammatical accuracy.
290: The sentence "We only then need only" contains redundant wording and should be simplified to "We then need only".
295: The word "for" should be replaced with "form" to correct a minor typo.
In Figures 4 and 5, the notation "DCnt" should be corrected to "DCt" for consistency. Overall, the paper introduces innovative entropy estimators for neural spike trains that require fewer samples for accurate estimations, making an important and well-implemented contribution to the field.