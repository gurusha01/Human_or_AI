The authors present a framework where an agent encounters a sequence of decision-making tasks within the same or similar environment, and propose a knowledge transfer method to leverage experience from previous tasks to inform the current one. Specifically, they focus on transferring learned optimal reward functions. The approach is demonstrated through two illustrative examples.
The work primarily extends two foundational studies: the optimal rewards formulation by Singh et al. (2010), which defines an internal reward function that maximizes external reward, considering potential limitations such as bounded capabilities; and the incremental learning algorithm by Sorg et al. (2010) for learning such reward functions in a single-task context.
The current paper's contribution lies in adapting these concepts to a multi-task setting. Similar to Sorg et al. (2010), the agent learns an internal reward function for each task. Additionally, it learns a mapping between the external reward functions of previously experienced tasks and their corresponding learned internal reward functions, utilizing this mapping to initialize the internal reward function for each new task.
Although the provided examples are modest, they adequately showcase the approach's potential. The algorithm's practical success will hinge on the availability of suitable features for mapping external to internal rewards. 
The paper is well-structured and clear, with a thorough and informative empirical evaluation. Figure 4 is particularly noteworthy for its concrete illustration of the algorithm's performance. Including a similar figure or description for the network example would enhance the paper's clarity. 
While the paper may not introduce highly novel concepts, it represents a well-executed and valuable contribution to the transfer learning literature.