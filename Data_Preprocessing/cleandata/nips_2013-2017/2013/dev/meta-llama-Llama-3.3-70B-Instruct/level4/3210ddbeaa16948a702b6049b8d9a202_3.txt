This paper presents a novel approach to efficiently compute non-linear kernels, specifically chi_2 kernels, between non-negative vectors. The authors leverage the fact that the dot product of two vectors can be recovered from the collision probability of their signs, where the collision probability is defined as the probability that the signs of the corresponding components of the two vectors are different. This collision probability can be estimated using a stable distribution with scale alpha=2, and the authors explore what happens when alpha < 2.
The paper begins with a motivating example in Section 2, followed by preliminary results in Section 3. The authors then introduce their main contribution in Section 4 and provide two formulas to approximate the collision probability as a function of the chi2 similarity in Section 5. While I find the paper to be interesting and containing sufficient novel material to warrant inclusion in NIPS, I have several concerns that prevent me from being more enthusiastic.
One major issue is that the paper feels disjointed and lacks a clear narrative thread. The authors seem to be still exploring different ideas and have not fully digested the content they are presenting. This is evident in the inclusion of phrases such as "(perhaps surprising)" and the simplicity of some of the results, which suggest that the paper is still a work in progress. For instance, the authors motivate their interest in the chi2 similarity by its popularity in comparing histograms, but then use time series data from the UCI-PEMS dataset as one of their main examples, which is confusing. Furthermore, the dataset is relatively small, with only 440 points, and the introduction of the "acos-\chi_2" kernel in line 82 is unclear without more context.
Another concern is that the paper is constructed to show how the chi2 similarity can be used to approximate the collision probability, but in practice, the natural flow is the other way around: empirical collision probabilities can be computed, and hence chi2 similarities can be approximated. I question the relevance of the second approximation in Equation 12 in machine learning, unless there is an obvious inversion formula for the arctan integral.
The paper reads like a work in progress, and the lack of a clear algorithm or explicit layout of the authors' contributions makes it difficult to follow. Additionally, Lemma 4 is provided without a proof, and the authors' suggestion to proceed by analogy with the proof of Lemma 2 is unconvincing. The correspondence between features and feature products (collisions) / the kernel 1-1/pi acos(p2) is exact when taking the expectation, but there is no such exact formula in the case of p\chi2. Therefore, it is necessary to prove that 1-1/\pi acos \rho\chi_2 is a positive semi-definite (psd) kernel, which is not trivial and requires a rigorous proof.
After the rebuttal, I remain unconvinced by the authors' response regarding the psdness of the acos chi2 kernel. The feature representations for the chi2 kernel are infinite-dimensional, and the authors' proposal to use random Gaussian approximations on infinite-dimensional vectors is unclear. This part of the paper needs to be proved more clearly. Overall, while the paper contains some promising ideas, the presentation needs to be improved to make it suitable for NIPS.