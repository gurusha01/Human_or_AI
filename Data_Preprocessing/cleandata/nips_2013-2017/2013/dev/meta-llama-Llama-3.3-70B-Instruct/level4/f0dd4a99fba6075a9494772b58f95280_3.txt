The manuscript presents an exploration of online learning applications with predictable loss sequences, initially extending earlier findings to Holder-smooth loss functions. This framework is subsequently applied to solve saddle-point problems and efficiently compute equilibria in two-player zero-sum games, including scenarios with cooperative players and providing guarantees when one player does not cooperate. Additionally, the paper examines convex programming problems, although the underlying assumptions in this context seem unusual.
The problems addressed are inherently interesting, and the approach undertaken is innovative. However, it's somewhat deceptive that the "predictable sequence framework" relies on past observations and smoothness to quantify prediction quality, using these as predictions.
The paper is, for the most part, well-written and clear. Nonetheless, there are two significant criticisms that the authors must address in their response:
1. The learning rates throughout the paper are determined by peeking into the future, where $\etat$ is a function of $\Nablat$, which becomes available only after an update with $\etat$ has been performed, and the subsequent gradient (loss) $\Nablat$ is revealed.
2. In Section 5, the assumption that $F^*$ can be known in advance or found using binary search is perplexing. It is unclear why verifying the existence of a solution with a given value would be easier than finding such a solution. Thus, assuming knowledge of the exact value of the maximum without knowing an optimal solution seems unjustified, rendering the subsequent optimization process pointless.
Minor comments include:
- The proof of Lemma 1, copied from [9], could be replaced with a reference to Lemma 3 of [9], as the first line of the statement lacks proof.
- In the proof of Lemma 2, line 571 implies an assumption that $1/\eta1 \le 1/R{max}$, equivalent to assuming the gradient norm is bounded by 1, which should be made explicit or avoided.
- Line 11 suggests $\rho$ should be $1/\sqrt{H}$ to cancel the first term in (3), potentially worsening the bound in line 113.
- The inequalities in derivations, such as lines 587, 590, 637, 645, 655, and 659, should be explained, as their complexity can make them difficult to follow.
- In Corollary 5, the norms $F$ and $X$ should be defined.
- Proposition 6 and Lemma 7 bound the average (or normalized) regret, which should be clarified.
The authors may also find the ICML paper "Dynamical Models and tracking regret in online convex programming" by Eric Hall and Rebecca Willett relevant, as it explores predictability in a related setting.
Following the rebuttal period, it's noted that:
- The corrections to the learning rates seem acceptable.
- Regarding Section 5, knowing $F^$ in advance is clarified to be approximate, with a cost in the bounds. Thus, assuming knowledge of $F^$ without loss of generality may not be entirely justified, but the provided explanation suffices.
- The bound in line 113 was a mistake, as acknowledged. Overall, the paper presents solid and interesting results.