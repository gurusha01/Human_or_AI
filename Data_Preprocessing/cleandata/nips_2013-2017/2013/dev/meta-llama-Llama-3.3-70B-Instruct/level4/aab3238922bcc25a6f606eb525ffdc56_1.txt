This paper presents a novel approach, termed RDC, for measuring statistical dependence between random variables, which integrates a copula transformation with a variant of kernel Canonical Correlation Analysis (CCA) utilizing random projections. This combination yields a computational complexity of O(n log n), making it efficient for large datasets. The experimental results on both synthetic and real-world benchmark data demonstrate the method's potential, particularly in feature selection applications.
The manuscript is well-organized and enjoyable to read, with clear explanations that facilitate understanding. The simplicity of the method's implementation, coupled with its promising experimental results, suggests that RDC could serve as a useful complement to existing methods in the field.
However, the originality of the work is somewhat limited, as it primarily combines previously used techniques. Specifically, the application of the copula transformation to achieve invariance to monotonic transformations has been explored in earlier studies, such as the work by Conover and Iman in 1981, and more recently, reference [15] cited by the authors. Additionally, the use of random projections to define a low-rank approximation of a kernel matrix is another technique that has been employed in the past.
From a technical standpoint, the work is correct, but several points require further clarification. Firstly, the authors claim that RDC is significantly faster to implement than kernel CCA (kCCA), stating that kCCA has a complexity of O(n^3) and requires 166 seconds for 1000 points. This assertion is surprising, given that the original kCCA paper by Bach and Jordan proposed an implementation with a complexity of O(n) using incomplete Cholesky decomposition, which they reported to take only 0.5 seconds for 1000 points over a decade ago. The incomplete Cholesky decomposition has become a popular approach for running kernel methods on large sample sizes, similar to the random projection trick used in this work. Therefore, it is natural to question whether the random projection trick offers a significant advantage over the incomplete Cholesky decomposition for performing kCCA with large datasets.
Furthermore, as the authors note, when the dimension of random projections (k) increases, the method converges to unregularized kCCA. This implies that k should not be too large, as some regularization of kCCA is necessary. Consequently, k appears to play a crucial role in regularization, similar to the regularization parameter in kCCA. It would be beneficial to have theoretical or empirical evidence supporting the use of k as a regularization mechanism compared to the traditional kCCA regularization. An argument against using k for regularization is that, for smaller values of k, random fluctuations may lead to significant variability in the final score, which is an undesirable property. Notably, although RDC satisfies all conditions listed in Table 1, it does not fulfill the condition of being a deterministic measure, as repeated computations may yield different values due to its stochastic nature.
In conclusion, while this work combines several established ideas in an interesting way, the comparison with its closest relative, kCCA, is not thoroughly explored. A more detailed analysis of the advantages and limitations of RDC in relation to kCCA would strengthen the manuscript and provide a clearer understanding of its contributions to the field.