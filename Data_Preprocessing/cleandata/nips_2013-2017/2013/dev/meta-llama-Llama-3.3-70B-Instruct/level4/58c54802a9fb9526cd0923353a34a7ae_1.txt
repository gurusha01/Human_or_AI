This manuscript proposes a transfer learning model that leverages synthetic reward functions learned from prior tasks and adapts them to new tasks, thereby accelerating the learning process. The application of synthetic reward functions, a relatively novel concept aimed at enhancing the performance of underlying utility reward functions, to the transfer learning scenario is both innovative and intriguing. The experimental design is thorough, well-considered, and presents reasonably convincing evidence.
A significant critique of the paper is the lack of comparison with other transfer methods. The authors' assertion that policy transfer is not feasible in their context is incorrect, as sub-policies, often in the form of options, can be transferred instead of entire policies. Notably, the work of Lisa Torrey on policy transfer is relevant and should be cited. Although omitting this comparison is understandable, as both types of transfer can be used in conjunction, it would have provided additional insights.
More critically, the paper's failure to compare its approach to similar reward shaping schemes is a significant oversight. Both methods aim to solve the same problem but differ in their impact on the ultimate solution. The absence of a comprehensive comparison between these methods, even beyond the transfer learning context, raises substantial doubts about the efficacy of synthetic reward functions. Including a comparison where a shaping function is mapped similarly to the reward function would substantially enhance the paper's validity. However, this might be an ambitious expectation for a single paper, especially considering that the mapped shaping function could be viewed as a novel, albeit straightforward, method.
The manuscript is well-written and engaging, although the frequent use of parenthetical citations as nouns detracts from the reading experience. Additionally, some references are poorly formatted, such as inconsistent capitalization. Overall, the paper presents a novel transfer method for reinforcement learning domains, is well-executed, and clearly described. While a more comprehensive experimental comparison to reward shaping methods would have been beneficial, it is not essential to the paper's contribution.