This manuscript presents a novel, straightforward nonlinearity technique for neural network architectures, which involves grouping neurons (usually in pairs) and setting all but the highest-valued neuron in each group to zero. Despite its simplicity, this approach yields promising results in classification tasks. The authors also provide evidence suggesting that their proposed network exhibits less forgetting compared to traditional nonlinearities, although potential issues with the experimental design are noted.
The paper's key contributions can be summarized as follows:
1. The introduction of a new nonlinearity technique, which, although simple, demonstrates good performance in classification. However, it is worth noting that better results have been achieved on permutation-invariant MNIST (as seen in Table 1 of http://arxiv.org/pdf/1302.4389v3.pdf), including those obtained by feedforward networks. Furthermore, the consideration of dropout in the input layer as a form of data augmentation is questionable, as it does not account for the input structure. An additional experiment exploring this aspect would be beneficial.
2. The proposed network's ability to mitigate forgetting is a desirable property. Nevertheless, the experimental methodology raises some concerns. The network is trained until it reaches a certain likelihood threshold, at which point the data or labels are modified. Given that the new nonlinearity performs better in recognition tasks, it may not require as extensive training to achieve the desired likelihood, potentially resulting in less forgetting due to reduced training. Therefore, the experiment's design may not accurately capture the network's ability to resist forgetting.
3. An additional, related experiment would be valuable in assessing the network's behavior. The authors have already evaluated the network's performance when trained on all digits simultaneously until convergence. A complementary approach would involve training the network on digits 1-5 until convergence, then adding the remaining digits and continuing training on all digits. This would help determine how easily the network becomes stuck in a local minimum during the initial training phase, a known issue with sigmoid networks.
In terms of overall quality, the manuscript is good but would benefit from additional experiments and a revised forgetting experiment. The clarity of the presentation is excellent, and the idea, although simple, is novel and effective. The significance of the work is moderate, as it introduces another nonlinearity to the neural network toolbox, albeit one related to max pooling. Ultimately, the manuscript presents a straightforward yet effective idea for classification, with importance placed on the network's ability to resist forgetting, although further experimentation is necessary to fully validate the results.