This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The key idea is to reuse reward functions across tasks, which is counterintuitive as rewards are typically considered task-specific. However, the authors build on recent work on the optimal rewards problem, which shows that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational bounds.
The paper's main contribution is a new approach to transfer in long-lived agents, where good guidance reward functions learned on previous tasks are used to incrementally train a reward mapping function. This function maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. The authors demonstrate that their approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.
The paper is well-written, and the authors provide a clear and concise overview of the background and related work. The experimental evaluation is thorough, and the results are impressive, showing that the reward mapping approach can outperform other methods in various settings.
The strengths of the paper include:
* The novel idea of reusing reward functions across tasks, which challenges the conventional wisdom in reinforcement learning.
* The thorough experimental evaluation, which demonstrates the effectiveness of the approach in various settings.
* The clear and concise writing style, which makes the paper easy to follow.
The weaknesses of the paper include:
* The assumption that the agent has perfect knowledge of the task specifications and the environment, which may not be realistic in many scenarios.
* The lack of theoretical analysis, which would provide a deeper understanding of the approach and its limitations.
* The comparison to only one competing method, which may not be representative of the state-of-the-art in transfer learning.
Arguments pro acceptance:
* The paper presents a novel and innovative approach to transfer learning in long-lived agents.
* The experimental evaluation is thorough and demonstrates the effectiveness of the approach.
* The paper is well-written and easy to follow.
Arguments con acceptance:
* The assumption of perfect knowledge of the task specifications and the environment may not be realistic.
* The lack of theoretical analysis may limit the understanding of the approach and its limitations.
* The comparison to only one competing method may not be representative of the state-of-the-art.
Overall, I believe that the paper is a good scientific contribution to the field, and I would recommend acceptance. However, I would like to see the authors address the weaknesses mentioned above, particularly the lack of theoretical analysis and the comparison to other competing methods.