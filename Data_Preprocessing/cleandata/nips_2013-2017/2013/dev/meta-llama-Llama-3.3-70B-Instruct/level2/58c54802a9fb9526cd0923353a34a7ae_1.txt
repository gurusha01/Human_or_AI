This paper presents a novel approach to transfer learning in long-lived agents that must solve a sequence of tasks over a finite lifetime. The main contribution is a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. The authors demonstrate that their approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.
The paper is well-written and clearly explains the problem, the proposed solution, and the experimental results. The authors provide a thorough background on the optimal rewards problem and its relevance to transfer learning. The experimental evaluation is comprehensive, covering two domains: a grid world domain and a networking routing domain. The results show that the reward mapping approach outperforms other methods, including a competing policy transfer method.
The strengths of the paper include:
* A clear and well-motivated problem statement
* A novel and well-explained solution
* Comprehensive experimental evaluation
* Comparison to a competing policy transfer method
The weaknesses of the paper include:
* The paper assumes that the agent has perfect knowledge of the controlled Markov process (CMP) and the task specifications, which may not always be the case in practice.
* The paper does not provide a thorough analysis of the computational complexity of the reward mapping function.
* The paper could benefit from more discussion on the potential applications and limitations of the proposed approach.
Arguments pro acceptance:
* The paper presents a novel and well-motivated approach to transfer learning in long-lived agents.
* The experimental evaluation is comprehensive and demonstrates the effectiveness of the proposed approach.
* The paper provides a clear and well-explained solution that can be easily implemented and extended.
Arguments con acceptance:
* The paper assumes perfect knowledge of the CMP and task specifications, which may not always be the case in practice.
* The paper does not provide a thorough analysis of the computational complexity of the reward mapping function.
* The paper could benefit from more discussion on the potential applications and limitations of the proposed approach.
Overall, I recommend accepting this paper as it presents a novel and well-motivated approach to transfer learning in long-lived agents, with comprehensive experimental evaluation and clear explanations. However, the authors should address the weaknesses mentioned above to improve the paper's quality and impact.