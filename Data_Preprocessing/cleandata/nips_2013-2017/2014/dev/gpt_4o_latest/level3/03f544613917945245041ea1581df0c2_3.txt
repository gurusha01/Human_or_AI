This paper presents a novel family of second-order optimization algorithms tailored for high-dimensional statistical estimation problems with superposition-structured regularizers. The authors extend prior work ([11], [12]) by developing a proximal Newton framework that incorporates active subspace selection for decomposable norms, enabling efficient handling of strongly convex, twice-differentiable loss functions and hybrid regularization. The proposed method demonstrates significant speedups (over 10x) compared to state-of-the-art first-order methods, particularly for latent variable graphical model selection and multi-task learning problems. The theoretical contributions include convergence guarantees and asymptotic quadratic convergence under certain conditions, addressing challenges posed by the lack of positive definiteness in the Hessian and the use of active subspace selection.
Strengths:
1. Theoretical Rigor: The paper provides a solid theoretical foundation for the proposed framework, including global convergence and super-linear convergence guarantees. The relaxation of assumptions on the Hessian's positive definiteness is a notable advancement over prior work.
2. Algorithmic Innovation: The active subspace selection method is generalized to decomposable norms, offering computational efficiency without compromising accuracy. This is particularly impactful for large-scale problems.
3. Empirical Validation: The numerical experiments convincingly demonstrate the algorithm's efficiency and scalability. The results on real-world datasets (e.g., gene expression and multi-task learning problems) highlight the practical utility of the method.
4. Significance: The work addresses a challenging and relevant problem in high-dimensional statistical estimation, with potential applications in areas such as graphical models and multi-task learning. The ability to handle multiple regularizers simultaneously is a significant step forward.
Weaknesses:
1. Restrictive Assumptions: The reliance on strongly convex loss functions may limit the applicability of the method to sparse learning problems, where non-strongly convex losses are common. This could restrict the broader adoption of the approach.
2. Clarity of Assumptions: Assumption (16) appears strong and is not adequately justified or contextualized. Providing concrete examples or intuition for this assumption would improve clarity.
3. Comparison with Alternatives: While the authors compare their method to first-order approaches, block coordinate descent algorithms are competitive in this domain. A detailed comparison with such methods would strengthen the empirical evaluation.
4. Rebuttal Considerations: The authors argue that statistical error considerations may reduce the need for highly accurate solutions. However, this claim would benefit from additional experiments exploring the trade-off between computational time and classification error.
Suggestions for Improvement:
- Provide examples or intuition for Assumption (16) to clarify its implications and necessity.
- Include a comparison with block coordinate descent methods to contextualize the performance gains.
- Conduct experiments that explicitly analyze the trade-off between timing and statistical error, as suggested in the rebuttal.
Recommendation:
Overall, this paper is a strong contribution to the field of high-dimensional optimization and statistical learning. Despite some limitations, the theoretical and empirical advancements are significant, and the proposed framework has the potential to inspire further research. I recommend acceptance at NeurIPS, with minor revisions to address the concerns outlined above.