The paper introduces a novel method, Calibrated Multivariate Regression (CMR), which replaces the Frobenius norm with the L_{2,1} norm in multivariate regression to address task-specific noise calibration. The authors claim that CMR achieves tuning insensitivity and improved finite-sample performance compared to ordinary multivariate regression (OMR). The method is computationally efficient, leveraging a smoothed proximal gradient (SPG) algorithm, and is theoretically supported to achieve optimal convergence rates. Numerical experiments and a brain activity prediction task demonstrate its effectiveness.
Strengths:
1. Clarity and Writing: The paper is well-written, with clear explanations of the problem, methodology, and experiments. The mathematical rigor and algorithmic details are presented comprehensively, making the work reproducible for experts in the field.
   
2. Theoretical Contributions: The authors provide sufficient theoretical guarantees for CMR, including optimal convergence rates and tuning insensitivity. The use of the L_{2,1} norm for calibration is an interesting extension of existing multivariate regression techniques.
3. Computational Efficiency: The proposed SPG algorithm demonstrates significant computational advantages over ADMM in numerical experiments, which is a practical contribution.
4. Empirical Results: The numerical experiments and real-world application (brain activity prediction) show that CMR consistently outperforms OMR and is competitive with handcrafted models, indicating its potential utility in practice.
Weaknesses:
1. Lack of Theoretical Justification for L{2,1} Norm: While the authors highlight the benefits of the L{2,1} norm over the Frobenius norm, the paper lacks a strong theoretical or intuitive justification for why standard deviation (L_{2,1}) outperforms variance (Frobenius norm) in this context. This omission weakens the foundational argument for the proposed method.
2. Limited Experimental Scope: The numerical experiments are constrained to a narrow range of DI (noise variance) and lambda (regularization parameter) values. It is unclear how CMR performs under broader conditions or extreme parameter variations, which limits the generalizability of the results.
3. Novelty Concerns: The paper builds on existing multivariate regression frameworks, and while the calibration aspect is novel, the overall methodological contribution feels incremental rather than groundbreaking. Related work, such as square-root sparse multivariate regression, is not sufficiently contrasted to highlight the distinctiveness of CMR.
4. Minor Errors: Typographical issues, such as the missing lambda in Equation (1.2), detract from the overall polish of the manuscript.
Pro/Con Arguments for Acceptance:
- Pro: The paper is well-executed, with strong theoretical and empirical results. The computational efficiency of the SPG algorithm and the demonstrated real-world applicability make it a valuable contribution.
- Con: The lack of novelty, limited experimental validation, and insufficient justification for the L_{2,1} norm reduce its impact as a significant advancement in the field.
Recommendation: Weak Reject. While the paper is technically sound and well-written, the incremental nature of the contribution and the lack of comprehensive experimental validation limit its significance. Addressing the theoretical gap regarding the L_{2,1} norm and expanding the experimental scope could strengthen its case for acceptance in future iterations.