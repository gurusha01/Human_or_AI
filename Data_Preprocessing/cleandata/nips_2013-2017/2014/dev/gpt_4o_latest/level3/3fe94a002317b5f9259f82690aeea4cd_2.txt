This paper introduces the PLACES database, a large-scale, scene-centric dataset designed to address the limitations of object-centric datasets like ImageNet for scene recognition tasks. The authors present a comprehensive methodology for constructing PLACES, which includes scraping images using composite queries and validating them via Amazon Mechanical Turk (AMT). The dataset comprises over 7 million labeled images across 476 scene categories, making it the largest scene-centric dataset to date. The authors also propose novel metrics for comparing dataset density and diversity, demonstrating that PLACES surpasses existing scene datasets (e.g., SUN) in diversity while maintaining comparable density. Additionally, the paper highlights the complementary nature of PLACES and ImageNet, with the former excelling in scene recognition and the latter in object recognition.
The strengths of the paper lie in its robust dataset creation process and the empirical results that underscore the importance of large-scale, diverse data for scene recognition. The authors convincingly show that training convolutional neural networks (CNNs) on PLACES leads to state-of-the-art performance on several scene-centric benchmarks, such as SUN397 and MIT Indoor67. The visualization of CNN layers further illustrates the differences in feature representations learned from scene-centric versus object-centric datasets, providing valuable insights into the role of training data in shaping neural network representations.
However, the paper has some limitations. First, the algorithmic contributions are limited, as the work primarily focuses on dataset creation and evaluation rather than proposing novel methods or architectures. Second, the proposed density and diversity metrics, while useful, rely on subjective judgments and feature-based similarity measures, which may introduce biases. These limitations could be addressed by exploring alternative, more objective metrics or by providing a deeper theoretical analysis of the proposed measures.
In terms of future directions, the authors suggest combining PLACES and ImageNet to train a unified model capable of excelling in both object and scene recognition. This is a promising avenue that could further advance the field. Additionally, making the PLACES dataset publicly available, as implied, would likely have a significant impact on scene recognition research by enabling broader experimentation and benchmarking.
In conclusion, this paper makes a valuable contribution to the field by introducing a high-quality, large-scale scene-centric dataset and demonstrating its utility for advancing scene recognition. While the work is more data-centric than algorithmic, its potential impact on the community is substantial. I recommend acceptance, with the caveat that future iterations should address the noted limitations to maximize the paper's scientific contribution.