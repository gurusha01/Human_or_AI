The paper introduces Probabilistic Differential Dynamic Programming (PDDP), a novel trajectory optimization framework that combines Differential Dynamic Programming (DDP) with Gaussian Processes (GPs) to address systems with unknown dynamics. By leveraging GPs, PDDP explicitly incorporates model uncertainty and performs local dynamic programming in Gaussian belief spaces. Unlike gradient-based policy search methods like PILCO, PDDP does not require policy parameterization and instead learns a linear, time-varying control policy. The authors demonstrate PDDP's performance on two simulated tasks: cart-double inverted pendulum swing-up and six-link robotic arm reaching, comparing it to classical DDP and PILCO.
Strengths:
1. Novelty and Relevance: The integration of GPs into DDP to handle model uncertainty is a significant contribution, making PDDP relevant for model-based optimization and control tasks. The approach is particularly appealing for practitioners in DDP/iLQG.
2. Efficiency: PDDP offers faster solution times compared to PILCO, a state-of-the-art GP-based policy search method, while maintaining comparable data efficiency. The analytical computation of Jacobians reduces computational overhead compared to finite difference methods.
3. Practical Applicability: The framework is well-suited for high-dimensional systems, as demonstrated in the experiments, and provides a scalable alternative to traditional DDP.
Weaknesses:
1. Presentation and Clarity: The paper suffers from shallow results presentation. Figures lack sufficient discussion, and claims such as "safe exploration" and "keeping the system stable" are not clearly substantiated. The computational complexity analysis is incomplete, as it does not fully address the relationship between control inputs and state dimensions.
2. Motivation and Comparison: The motivation for introducing PDDP as a new method is unclear, especially given its marginal improvements in data efficiency over PILCO. A deeper analysis comparing PDDP and PILCO, as well as traditional DDP with fully known dynamics, is necessary to highlight PDDP's unique advantages.
3. Local vs. Global Optimization: The paper does not adequately address the trade-offs between local trajectory optimization (scalable but prone to poor local optima) and global optimization approaches.
4. Control Distribution Assumptions: The assumption of neglecting control distribution variance is not well-justified and requires further explanation.
5. Writing Quality: The writing lacks polish and coherence, making it difficult to follow the technical details. Improvements in organization and clarity are needed.
Arguments for Acceptance:
- The paper proposes a novel framework that combines established techniques (DDP and GPs) in an innovative way.
- PDDP's faster solution times and analytical Jacobian computation are valuable contributions for the community.
- The work is relevant and impactful for trajectory optimization and model-based control.
Arguments Against Acceptance:
- The paper lacks clarity and depth in presenting results and justifying claims.
- The computational complexity analysis and comparisons with existing methods are insufficient.
- The motivation for PDDP over PILCO and traditional DDP is not convincingly articulated.
Recommendations:
The paper has potential but requires significant revisions. The authors should:
1. Provide a more detailed and critical comparison with PILCO and traditional DDP.
2. Clarify and substantiate claims about "safe exploration" and computational complexity.
3. Improve the writing quality and presentation of results, including better figure legends and axis consistency.
4. Explore and visualize variance dynamics during GP learning iterations to provide deeper insights.
In summary, while the framework is of interest to the DDP/iLQG community, the paper lacks the clarity and thorough evaluation necessary for acceptance in its current form.