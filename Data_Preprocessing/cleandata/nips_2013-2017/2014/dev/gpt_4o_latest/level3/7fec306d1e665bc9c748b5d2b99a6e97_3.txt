The paper introduces Probabilistic Differential Dynamic Programming (PDDP), a trajectory optimization framework combining Gaussian Processes (GPs) for modeling system dynamics with a DDP-like trajectory optimization algorithm. The approach explicitly incorporates model uncertainty into the optimization process by operating in Gaussian belief spaces. Unlike gradient-based policy search methods, PDDP avoids policy parameterization, instead learning a locally optimal, time-varying control policy. The authors evaluate PDDP on two challenging tasks—a cart-double inverted pendulum swing-up and a six-link robotic arm—and compare its performance to classical DDP and PILCO, a state-of-the-art GP-based policy search method.
Strengths:  
The paper addresses an important problem in model-based reinforcement learning (RL) by integrating probabilistic dynamics modeling with trajectory optimization. The use of GPs to handle model uncertainty is well-motivated, and the analytical computation of Jacobians improves computational efficiency compared to finite-difference methods. The method demonstrates significant improvements in computational time over PILCO, which is a key contribution for scaling to higher-dimensional systems. Additionally, the use of Gaussian belief spaces for trajectory optimization is a promising direction, and the experimental results show that PDDP achieves comparable data efficiency to PILCO while maintaining reasonable control performance.
Weaknesses:  
The paper has several shortcomings that limit its impact. First, the novelty of the proposed method is questionable, as it primarily combines existing techniques (DDP-based belief space planning and GP models) without introducing substantial innovation. The connection to Gaussian belief space planning is not acknowledged, which diminishes the originality of the approach. Second, the experimental evaluation is limited, with comparisons restricted to PILCO and classical DDP. This lack of broader benchmarks against other belief space planning or model-based RL methods undermines the claimed computational advantages. Additionally, the choice of PI² as a baseline is suboptimal, as it is not well-suited for the tasks considered. The experimental results, while promising, fail to convincingly demonstrate the tradeoff between computational cost and sample complexity due to limited and suboptimal comparisons. Lastly, the title of the paper is overly ambitious and does not accurately reflect the contribution, which is primarily a combination of existing methods.
Clarity and Presentation:  
The paper is generally well-written and organized, with detailed descriptions of the methodology and experiments. However, the distinction between PDDP and belief space planning is not clearly articulated, and the lack of explicit experimental validation of this distinction weakens the paper's clarity. The title could also be revised to better align with the actual contribution.
Pro and Con Arguments for Acceptance:  
Pros:  
1. Significant computational efficiency compared to PILCO.  
2. Promising integration of GPs with trajectory optimization in belief space.  
3. Analytical computation of Jacobians reduces computational overhead.  
Cons:  
1. Limited novelty, as the method combines existing techniques without substantial innovation.  
2. Insufficient experimental comparisons with a broader range of methods.  
3. Failure to acknowledge connections to Gaussian belief space planning reduces the perceived originality.  
4. Experimental results do not convincingly demonstrate the claimed tradeoffs.  
Recommendation:  
While the paper presents an interesting combination of techniques and demonstrates computational efficiency, the lack of novelty, limited experimental validation, and insufficient comparisons weaken its contribution. I recommend rejection in its current form but encourage the authors to address these issues in a future submission. Specifically, acknowledging related work, broadening experimental comparisons, and clearly articulating the novelty would significantly strengthen the paper.