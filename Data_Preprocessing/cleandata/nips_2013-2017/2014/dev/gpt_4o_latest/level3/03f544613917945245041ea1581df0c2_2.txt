The paper presents a novel approach for optimizing "dirty models" in high-dimensional statistical estimation by leveraging a proximal Newton framework with quadratic approximations and active subspace selection. The authors extend the concept of decomposable norms, which have been previously applied in contexts like nuclear norm minimization and sparse inverse covariance estimation, to a broader setting. Their algorithm demonstrates a significant 10-fold speedup over state-of-the-art first-order methods for problems such as latent variable graphical model selection and multi-task learning. However, the practical significance of this speedup, particularly in terms of ease of optimization or parallelization, remains unclear.
Strengths:
1. Theoretical Contributions: The paper provides a clear theoretical foundation, addressing challenges such as the lack of positive-definiteness in the Hessian and the use of active subspace selection. The convergence guarantees and super-linear convergence rate are notable contributions.
2. Algorithmic Innovation: The active subspace selection strategy is a key highlight, enabling efficient optimization by reducing the search space. This generalization to multiple regularizers is a meaningful extension of prior work.
3. Empirical Results: The algorithm achieves impressive speedups in real-world applications, such as Gaussian Markov Random Fields with latent variables and multi-task learning, demonstrating its potential for large-scale problems.
4. Broader Applicability: The framework is versatile, accommodating a variety of decomposable norms and hybrid regularizers, making it relevant for a wide range of high-dimensional estimation problems.
Weaknesses:
1. Writing and Presentation: The paper suffers from poor writing quality, including language errors, inconsistent grammar, and issues with inline formulas. These detract from readability and professionalism.
2. Formatting Issues: Figures and formulas breach margins, and the lack of a conclusion or discussion section leaves the paper feeling incomplete. Future directions and broader implications are not adequately addressed.
3. Algorithmic Clarity: Specific errors in Algorithm 1, such as superscript notation and variable updates, raise concerns about correctness and reproducibility.
4. Significance of Speedup: While the 10-fold speedup is empirically demonstrated, the paper does not convincingly argue its practical significance, particularly in scenarios where parallelization or simpler first-order methods might suffice.
5. Relation to Prior Work: Although the theoretical contributions are interesting, they appear to be incremental rather than groundbreaking, building on well-established ideas like proximal Newton methods and decomposable norms.
Recommendation:
While the paper offers valuable theoretical and algorithmic insights, its poor presentation and lack of clarity hinder its impact. The work is a meaningful extension of prior research but does not represent a transformative breakthrough. The paper would benefit from substantial revisions, including improved writing, clearer algorithmic descriptions, and a stronger emphasis on the practical significance of the contributions. If these issues are addressed, the paper could make a solid contribution to the field. 
Arguments for Acceptance:
- Strong theoretical foundation and convergence guarantees.
- Significant empirical speedup in challenging applications.
- Generalization of active subspace selection to multiple regularizers.
Arguments Against Acceptance:
- Poor writing and formatting detract from clarity and professionalism.
- Incremental contributions relative to prior work.
- Insufficient discussion of practical implications and future directions.
Final Decision: Weak Reject (with potential for reconsideration after major revisions).