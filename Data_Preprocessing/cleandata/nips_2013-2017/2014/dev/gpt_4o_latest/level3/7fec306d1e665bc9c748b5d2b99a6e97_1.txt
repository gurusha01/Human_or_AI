This paper introduces Probabilistic Differential Dynamic Programming (PDDP), a data-driven trajectory optimization framework that combines Gaussian Processes (GPs) with Differential Dynamic Programming (DDP). The authors aim to address the challenge of unknown dynamics in control systems by leveraging GPs for probabilistic modeling and explicitly accounting for model uncertainty. PDDP is positioned as an alternative to classical DDP and PILCO, offering a locally optimal, linear, time-varying control policy without requiring policy parameterization. The paper evaluates PDDP on two tasks: a cart-double inverted pendulum swing-up and a six-link robotic arm, comparing its performance with DDP and PILCO.
Strengths
1. Conceptual Elegance: The proposed approach is straightforward yet effective, combining GPs and DDP in a novel way to address model uncertainty. The integration of Gaussian belief spaces into trajectory optimization is well-motivated and builds on existing work in Bayesian machine learning and control theory.
2. Clarity of Presentation: The authors provide a detailed explanation of the methodology, including analytical derivations of Jacobians and the quadratic approximation of the value function. The algorithm is clearly summarized in a step-by-step format, aiding reproducibility.
3. Computational Efficiency: The paper highlights PDDP's advantage in computational efficiency compared to PILCO, particularly in terms of learning speed, due to its avoidance of iterative solvers for policy optimization.
4. Novelty: The combination of GPs with DDP for probabilistic trajectory optimization is a novel contribution, and the paper adequately situates this work within the context of prior research.
Weaknesses
1. Experimental Validation: The experimental results are limited to two tasks, and the comparisons with PILCO and DDP lack depth. Notably, the paper does not compare the total cost between PILCO and PDDP, which would provide a more comprehensive evaluation.
2. Unsubstantiated Claims: Several claims require clarification or are inaccurate. For instance, the assertion that PDDP avoids non-convex optimization is questionable, as hyperparameter tuning for GPs involves non-convex optimization. Similarly, the claim that PDDP's complexity does not scale with state dimensionality is misleading and needs revision.
3. Algorithmic Discussion: The discussion of noise in Algorithm 1 (step 8) is insufficient. The authors should analyze how noise impacts performance and provide experimental evidence to support their approach.
4. Comparison with GPDDP: The paper does not address why GPDDP outperforms PDDP in data and computational efficiency, which undermines PDDP's claim of "safe exploration."
5. Typographical Error: There is a typo on line 47 where "draw" should be corrected to "drawn."
Arguments for Acceptance
- The paper presents a novel and conceptually sound approach to probabilistic trajectory optimization.
- It offers computational advantages over PILCO and builds on established methods in the field.
Arguments Against Acceptance
- The experimental results are weak and lack sufficient validation to support the claims.
- Several inaccuracies in the theoretical claims need to be addressed.
- The paper does not provide a thorough comparison with state-of-the-art methods like GPDDP.
Recommendation
While the proposed approach is promising, the lack of robust experimental validation and the need for clarification of several claims prevent this paper from being a clear-cut acceptance. I recommend a weak reject with encouragement to address the identified weaknesses for resubmission.