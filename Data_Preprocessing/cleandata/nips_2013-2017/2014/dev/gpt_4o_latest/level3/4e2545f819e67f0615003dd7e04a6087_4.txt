This paper presents a novel recurrent spiking neural network (RSNN) model for Markov Chain Monte Carlo (MCMC) sampling, which separates computation from representation to encode multidimensional real-valued probability distributions using a spatio-temporal code. The key contribution lies in its ability to represent uncertainty flexibly and efficiently, leveraging parallel MCMC chains to approximate target distributions nearly instantaneously. The authors validate their approach through simulations, demonstrating its inference capabilities and aligning neural response characteristics with experimental neuroscience data. Additionally, the exploration of decoding strategies for identifying properties of the underlying distribution without a known linear decoder is particularly relevant for experimental applications.
The manuscript is well-written, particularly in the introduction and discussion sections, and includes clear, illustrative figures. The work aligns well with the conference's scope, addressing a critical problem in Bayesian neural computation. The proposed model offers a significant advancement over existing probabilistic codes by enabling faster sampling and a more flexible representation of distributions. Importantly, the separation of computation and representation enhances robustness to neural damage and scalability for higher-dimensional distributions.
However, several issues limit the paper's clarity and accessibility. First, the writing contains minor errors (e.g., missing or extra words in lines 46, 47, 142) that detract from readability. Key terms such as "spike trains" (line 80) and "reset mechanism" (line 140) are not adequately defined, which could confuse readers unfamiliar with the terminology. The derivations presented are highly compressed, requiring prior knowledge of referenced works, and critical simulation parameters are missing, making it difficult to verify results. Additionally, the paper does not explicitly discuss the symmetry limitation of the recurrent weight matrices in the Gaussian case, which could impact the generalizability of the model. Concerns about specific equations (e.g., summation in line 141, sparseness parameter λ, and time constant τ_slow) also require clarification.
Another limitation is the manuscript's overloaded scope, as it attempts to address multiple research questions simultaneously. While the multi-chain sampler is an intriguing aspect, it is underexplored, and the breadth of topics reduces the paper's overall focus.
In summary, this paper makes a valuable contribution to the field of probabilistic neural computation, introducing a promising framework for distributed MCMC sampling. Despite its presentation deficits, the work is technically sound, original, and significant. The proposed ideas warrant follow-up research, particularly to refine the model and explore its experimental implications further. Arguments for acceptance include the novelty of the approach, its alignment with the conference's scope, and its potential impact on neuroscience and machine learning. Arguments against acceptance include the lack of clarity in derivations, insufficient exploration of key aspects, and presentation issues. With revisions to address these concerns, the paper could become a strong addition to the conference proceedings.