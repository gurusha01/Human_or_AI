Review of "Saddle-Free Newton Method for Optimization in High-Dimensional Non-Convex Spaces"
This paper addresses the challenges of optimizing high-dimensional non-convex error functions, particularly the prevalence of saddle points, which the authors argue are a more significant impediment to optimization than local minima. The proposed Saddle-Free Newton (SFN) method leverages second-order curvature information to escape saddle points by rescaling gradients using the absolute value of the Hessian's eigenvalues. The authors provide theoretical justification, experimental validation, and comparisons with other optimization methods, demonstrating the potential of SFN in training deep and recurrent neural networks.
Strengths:
1. Novelty and Originality: The paper introduces a novel approach to optimization by addressing saddle points explicitly, which is a departure from traditional methods that focus on local minima. The use of the absolute value of the Hessian eigenvalues is an innovative idea, and the theoretical framework of generalized trust region methods is well-constructed.
2. Theoretical Contributions: The authors draw from diverse fields, including statistical physics and random matrix theory, to provide a compelling argument for the prevalence of saddle points in high-dimensional error surfaces. This interdisciplinary approach enriches the paper's theoretical foundation.
3. Empirical Validation: The experiments confirm the existence of saddle points in neural network error surfaces and demonstrate that SFN can escape saddle points more effectively than SGD and Newton methods. The method achieves state-of-the-art performance on benchmarks like deep autoencoders for MNIST.
4. Clarity of Problem Statement: The paper clearly identifies the limitations of existing optimization methods near saddle points and provides a well-motivated solution.
Weaknesses:
1. Lack of Direct Comparisons: The paper does not include a direct comparison between SFN and the Fisher/Natural Gradient or Gauss-Newton methods under identical setups. This omission makes it difficult to establish the superiority of SFN over these established methods.
2. Experimental Limitations: While the experiments show promising results, they fail to convincingly demonstrate that SFN consistently outperforms Natural Gradient or Gauss-Newton methods. The reliance on SGD for initialization raises concerns about whether the observed improvements are due to SFN or SGD pretraining.
3. Excessive Formalism: The extensive formal justification, including the introduction of a new trust region method, detracts from the main contributions. This section could be streamlined to focus more on the practical implications of SFN.
4. Convex Approximation Assumptions: The assumption of convex approximations is challenged by the presence of saddle points. While the authors address this issue, the treatment could be more comprehensive.
5. Scalability Concerns: The reliance on Krylov subspaces for approximating the Hessian limits the scalability of SFN to very high-dimensional problems, which is acknowledged but not adequately addressed.
Arguments for Acceptance:
- The paper proposes a novel and theoretically sound approach to a well-recognized challenge in optimization.
- It provides empirical evidence that SFN can escape saddle points and improve optimization in deep learning tasks.
- The interdisciplinary theoretical framework is a valuable contribution to the field.
Arguments Against Acceptance:
- The lack of direct comparisons with Natural Gradient and Gauss-Newton methods weakens the empirical claims.
- The experiments do not convincingly demonstrate that SFN consistently outperforms other methods.
- The paper's excessive formalism detracts from its practical contributions.
Recommendation:
While the paper makes significant theoretical and methodological contributions, its empirical validation is incomplete, and key comparisons are missing. I recommend acceptance conditional on the authors addressing the experimental shortcomings and providing direct comparisons with Natural Gradient and Gauss-Newton methods.