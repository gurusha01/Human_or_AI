The paper introduces a novel exact algorithm for structure learning of chordal Markov networks using dynamic programming and recursive partition tree (RPT) structures. This approach is a significant contribution to the field of graphical models, as it provides a computationally efficient method for solving a previously challenging problem. The authors rigorously prove the correctness of their algorithm and demonstrate its superiority over GOBNILP, a state-of-the-art method for Bayesian network structure learning. The experimental results are compelling, showing that the proposed algorithm is orders of magnitude faster than existing methods for moderate-sized instances.
Strengths:
1. Technical Soundness: The paper is technically robust, with well-defined theoretical foundations. The authors provide clear proofs of correctness and complexity analysis, ensuring the algorithm's reliability.
2. Efficiency: The proposed algorithm demonstrates remarkable computational efficiency, solving instances with up to 18 vertices within hours, a significant improvement over prior methods.
3. Clarity and Organization: The paper is well-written and logically organized, making it accessible to readers familiar with graphical models and dynamic programming.
4. Relevance and Contribution: The work is highly relevant to NIPS, addressing a challenging problem in learning graphical models of bounded complexity. The contribution is significant, advancing the state-of-the-art in exact structure learning for chordal Markov networks.
Weaknesses:
1. Motivation for Chordal Markov Networks: The paper lacks a thorough discussion on why chordal Markov networks are a focal point. A detailed explanation of their advantages in terms of inference efficiency and sample complexity would strengthen the paper.
2. Related Work: The authors omit recent related work on bounded treewidth Bayesian networks, such as those by Parviainen et al. (2014) and Berg et al. (2014). Including these references would provide a more comprehensive context for the contribution.
3. Experimental Design: The experiments could be improved by using distributions drawn from a symmetric Dirichlet with hyper-parameter < 1, which would better reflect real-world scenarios compared to uniform random sampling.
4. Clarity in Definitions and Notation: Naming the conditions in Definition 1 (e.g., RPT1, RPT2, RPT3) and renumbering conditions on page 5 (line 245) would enhance clarity and reduce confusion in the proofs.
5. Assumptions and Comparisons: The assumption of small decomposition width (w) should be better justified, and experiments with w set to infinity seem less relevant. Additionally, a comparison with algorithms for learning maximum likelihood Bayesian networks of bounded treewidth would provide a broader perspective.
Suggestions for Improvement:
1. Include a discussion on the practical benefits of chordal Markov networks, such as their impact on inference and sample complexity.
2. Cite and discuss recent related work on bounded treewidth Bayesian networks.
3. Use more realistic data generation methods in the experiments.
4. Improve clarity in the proofs by naming and renumbering conditions.
5. Justify the assumption of small decomposition width and consider omitting experiments with w set to infinity.
Recommendation:
This paper is a strong candidate for acceptance at NIPS due to its significant contribution to the field, technical rigor, and computational efficiency. However, addressing the weaknesses outlined above would further enhance its impact and clarity.