This paper introduces a novel Newton-like optimization method tailored for high-dimensional problems involving superposition-structured statistical estimators. The proposed approach combines a smooth convex loss function with multiple decomposable norms, leveraging quadratic approximations and an innovative active subspace selection technique. The authors provide convergence guarantees and demonstrate significant empirical improvements over state-of-the-art methods.
Strengths:
1. Novelty: The active subspace selection framework is a key innovation, extending prior work on single regularizers to multiple decomposable norms. This is a meaningful contribution to optimization techniques for high-dimensional problems.
2. Theoretical Rigor: The paper provides a solid theoretical foundation, addressing challenges such as the non-positive-definiteness of the Hessian and the dynamic nature of the active subspace. The convergence proofs are robust and well-argued, ensuring both global and super-linear convergence under reasonable assumptions.
3. Empirical Performance: The method achieves a 10x speed-up over existing approaches for latent variable Gaussian graphical model selection and multi-task learning problems. This is a substantial improvement, especially for large-scale applications. The breakdown of the speed-up sources in the appendix is a valuable addition.
4. Practical Relevance: The ability to efficiently handle problems with multiple regularizers, such as sparse + low-rank or sparse + group-sparse structures, is highly relevant for practitioners in machine learning and statistics.
Weaknesses:
1. Clarity Issues: While the paper is generally well-organized, there are minor clarity issues. Some notations are introduced without sufficient explanation, and typos detract from readability. Additionally, inconsistent referencing (e.g., missing citations for certain claims) should be addressed.
2. Appendix Dependency: Critical details, such as the breakdown of speed-up sources and implementation specifics, are relegated to the appendix. While this level of detail is appreciated, it may be more appropriate for a journal submission rather than a conference paper.
3. Scope of Numerical Experiments: Although the results are impressive, the experiments are limited to two applications. Expanding the empirical evaluation to additional domains or datasets would strengthen the paper's generalizability claims.
Arguments for Acceptance:
- The paper addresses a challenging and impactful problem, providing both theoretical and practical advancements.
- The 10x speed-up over state-of-the-art methods is a significant contribution, with clear implications for large-scale optimization tasks.
- The active subspace selection framework is novel and well-supported by both theory and experiments.
Arguments Against Acceptance:
- The paper's reliance on the appendix for critical details may hinder its accessibility to a broader audience.
- Minor clarity and presentation issues reduce the overall polish of the work.
Recommendation: Accept with minor revisions. The paper's contributions are substantial and align well with the conference's focus on advancing the state of the art in machine learning and optimization. Addressing clarity issues and refining the presentation will further enhance its impact.