This paper introduces a novel approach to discriminative clustering by leveraging generalization error bounds derived from unsupervised nonparametric classifiers. The authors propose a framework that bridges clustering and multi-class classification, utilizing kernel density estimates to define a new similarity measure. This reformulation reduces the complexity of parameter tuning and clarifies the optimization objective, which is a notable contribution to the field of clustering. The theoretical connection to Low Density Separation and the derivation of pairwise similarity measures from generalization error bounds are well-motivated and provide a fresh perspective on kernel-based clustering methods.
Strengths:  
The paper's primary strength lies in its theoretical contributions. By deriving generalization error bounds for the plug-in and nearest neighbor classifiers, the authors provide a principled explanation for the widely used kernel similarity in clustering. The connection between the error bounds and the weighted volume of cluster boundaries is particularly insightful, as it aligns with established criteria like Low Density Separation. Additionally, the proposed Plug-In Exemplar Clustering (PIEC) algorithm demonstrates improved discriminative capability compared to existing exemplar-based methods, supported by empirical results on benchmark datasets. The reduction in parameter complexity is another practical advantage, making the method potentially more scalable to high-dimensional data.
Weaknesses:  
Despite its theoretical rigor, the paper has several limitations. The experimental evaluation is limited in scope, focusing primarily on comparisons with exemplar-based methods rather than other discriminative clustering approaches. This narrow evaluation undermines the practical significance of the proposed method. Furthermore, while the consistency results for kernel density estimates are technically sound, they occupy substantial space that could have been better utilized for additional experiments or comparisons. The paper also suffers from excessive and dense notation, making it difficult to follow. A clearer, consolidated explanation of the overall algorithm and its steps would significantly improve readability.
Arguments for Acceptance:  
- The paper presents a novel and theoretically grounded approach to discriminative clustering.  
- It provides a new perspective on kernel similarity and its connection to generalization error bounds.  
- The proposed method reduces parameter complexity and demonstrates empirical improvements over exemplar-based methods.  
Arguments Against Acceptance:  
- The experimental evaluation is insufficient, with limited comparisons to relevant state-of-the-art methods.  
- The paper is difficult to read due to excessive notation and a lack of clear exposition.  
- Practical significance is not convincingly demonstrated, especially for high-dimensional or real-world datasets.  
Recommendation:  
While the paper makes a promising theoretical contribution, it requires a more robust experimental evaluation and significant improvements in clarity. I recommend a weak reject, encouraging the authors to address these issues in a future submission.