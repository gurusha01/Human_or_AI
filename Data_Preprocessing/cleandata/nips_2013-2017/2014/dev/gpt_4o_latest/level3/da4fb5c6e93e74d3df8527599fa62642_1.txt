The paper introduces Calibrated Multivariate Regression (CMR), a novel method for high-dimensional multivariate regression that addresses limitations in existing approaches. Specifically, CMR calibrates the regularization for each regression task based on its noise level, improving finite-sample performance and reducing sensitivity to tuning parameters. The authors propose a smoothed proximal gradient (SPG) algorithm to efficiently compute optimal solutions, achieving a worst-case iteration complexity of \(O(1/\epsilon)\). Theoretical analysis demonstrates that CMR achieves the same convergence rates as its non-calibrated counterpart, Ordinary Multivariate Regression (OMR), while offering practical advantages in tuning insensitivity. Empirical results on synthetic data and a brain activity prediction task show that CMR consistently outperforms OMR and is competitive with handcrafted models created by human experts.
Strengths:
1. Novelty and Contribution: The introduction of a calibration mechanism for task-specific noise levels is a significant advancement over existing methods. By addressing the tuning sensitivity of OMR, CMR provides a practical and theoretically sound alternative.
2. Theoretical Rigor: The paper provides a comprehensive theoretical analysis, proving that CMR achieves optimal convergence rates and demonstrating its robustness to tuning parameter selection.
3. Computational Efficiency: The proposed SPG algorithm is well-designed, with clear advantages over ADMM in terms of speed, as demonstrated in the experiments.
4. Empirical Validation: The extensive numerical simulations and real-world application to brain activity prediction convincingly illustrate the practical utility of CMR. The results highlight its superior performance over OMR and its robustness across different noise structures.
5. Clarity of Presentation: The paper is well-organized, with detailed explanations of the methodology, theoretical results, and experimental setup.
Weaknesses:
1. Intuitive Explanation: While the calibration mechanism is mathematically sound, the paper could benefit from a more intuitive explanation of how the loss term adapts to noise levels. This would make the method more accessible to a broader audience.
2. Comparison with OMR: Although CMR outperforms OMR in most cases, the paper acknowledges that OMR with a differentiable loss term may have computational advantages. Clearer guidance on when to prefer CMR over OMR would enhance the paper's practical relevance.
3. Limited Real-World Applications: While the brain activity prediction task is compelling, additional real-world applications could strengthen the case for CMR's generalizability.
4. Tuning Parameter Selection: Although CMR reduces sensitivity to tuning, the paper does not fully address how practitioners should select the regularization parameter in practice, especially in scenarios with limited validation data.
Arguments for Acceptance:
- The paper addresses a critical limitation in multivariate regression and provides both theoretical and empirical evidence for its claims.
- The proposed SPG algorithm is computationally efficient and well-suited for high-dimensional data.
- The method has potential applications in various domains, as demonstrated by the brain activity prediction task.
Arguments Against Acceptance:
- The lack of intuitive explanations and practical guidance may limit the accessibility and usability of the method.
- The paper could explore additional real-world applications to better demonstrate generalizability.
Recommendation:
Overall, this paper makes a significant contribution to high-dimensional regression and is well-suited for the conference. While some areas could benefit from further elaboration, the strengths outweigh the weaknesses. I recommend acceptance with minor revisions to improve clarity and practical guidance.