The paper introduces the Places database, a large-scale, scene-centric dataset with over 7 million labeled images across 476 categories, designed to address the limitations of existing datasets like ImageNet and SUN for scene recognition tasks. The authors propose novel measures of dataset density and diversity, demonstrating that Places is both dense and more diverse compared to its counterparts. Using Convolutional Neural Networks (CNNs) trained on Places, the authors establish state-of-the-art performance on multiple scene recognition benchmarks, such as SUN397 and MIT Indoor67. The paper also visualizes differences in internal representations learned by object-centric (ImageNet) and scene-centric (Places) networks, highlighting their complementary strengths. Additionally, the authors explore hybrid training by combining Places and ImageNet datasets, further improving performance on certain benchmarks.
Strengths:
1. Significant Contribution to Dataset Development: The Places database represents a substantial advancement in scene-centric datasets, addressing the limitations of existing datasets in terms of scale, diversity, and density.
2. State-of-the-Art Performance: The CNNs trained on Places achieve new benchmarks in scene recognition tasks, demonstrating the utility of the dataset.
3. Novel Dataset Comparison Metrics: The introduction of relative density and diversity measures provides a valuable framework for comparing datasets, which could have broader applications in computer vision.
4. Visualization of Learned Representations: The visualization of CNN layers' responses offers insights into the differences between object-centric and scene-centric networks, contributing to a better understanding of deep learning models.
5. Comprehensive Experiments: The paper evaluates the Places-CNN on a wide range of benchmarks, showing its generalizability and effectiveness.
Weaknesses:
1. Limited Theoretical Analysis: While the empirical results are strong, the paper lacks deeper theoretical insights into why scene-centric features differ fundamentally from object-centric ones, beyond the observed differences in receptive fields.
2. Dataset Bias: The paper acknowledges dataset bias but does not fully explore its implications or propose methods to mitigate it, especially when combining datasets like Places and ImageNet.
3. Reproducibility Concerns: Although the dataset and pre-trained models are made available, the computational resources required (e.g., 6 days on a Tesla K40 GPU) may limit accessibility for smaller research groups.
4. Hybrid-CNN Results: While the hybrid training approach is interesting, the performance gains are modest, and the paper does not provide a detailed analysis of why combining datasets yields only incremental improvements.
Arguments for Acceptance:
- The paper presents a significant contribution to the field by introducing a large-scale, diverse dataset that advances the state of the art in scene recognition.
- The novel metrics for dataset comparison and the visualization of learned representations are valuable contributions with potential impact beyond this specific work.
- The experimental results are robust, demonstrating clear improvements over existing methods and datasets.
Arguments Against Acceptance:
- The paper could benefit from a more thorough theoretical exploration of the observed phenomena, such as the differences in learned features between object-centric and scene-centric networks.
- The computational demands for training and evaluation may limit the accessibility of the proposed methods and dataset.
Recommendation:
Overall, this paper represents a strong contribution to the field of computer vision, particularly in the area of scene recognition. Its introduction of the Places database and associated insights into dataset diversity and CNN representations make it a valuable resource for the community. While there are minor weaknesses, they do not detract significantly from the overall quality and impact of the work. I recommend acceptance.