This paper addresses the challenge posed by the existence of saddle points, which represent a significant obstacle for first-order methods (such as SGD) and certain second-order methods (such as those derived from the Newton method) in achieving efficient optimization.
Specifically, the authors argue that when an algorithm encounters a saddle point, it should leverage the direction of negative curvature to make progress. First-order methods like SGD fail in this regard, as they are ineffective in directions of low curvature (as previously established in the literature). Similarly, second-order methods based on trust regions generally do not exploit directions of significant negative curvature, along which rapid progress could otherwise be achieved.
The authors propose a novel method inspired by the work of Vinyals and Povey. However, instead of relying on the Gauss-Newton matrix or the standard Newton method applied to the Krylov subspace, they introduce a new trust region approach that extends to non-PSD Hessian matrices. Their method uses the absolute values of eigenvalues to define the trust region, enabling it to handle saddle points more effectively than SGD (by incorporating curvature) and outperform Newton-based methods (by naturally escaping saddle points).
Comments for the authors:
- Around line 315, could you clarify what is meant by the "k biggest eigenvectors"? Are these the eigenvectors corresponding to the largest eigenvalues in absolute value? If not, how does the Krylov approximation capture negative curvature to be exploited by your algorithm?
- The improvement achieved with SFN appears substantial, particularly in datasets like PTB. Could the authors provide perplexities or other standard metrics on the held-out set for comparison? Carefully tuned SGD is known to perform well on PTB, so such a comparison would be valuable.
- Implementing these methods can be challenging. Do the authors plan to release the code or provide a detailed set of instructions, including all the heuristics used, to facilitate reproducibility?
- From a practical perspective, how do regularization techniques (e.g., dropout) influence saddle points? It would be helpful to discuss this in the context of your method.
- What network architecture was used in the experiments? Additionally, how would the proposed method handle ReLU networks? Can the authors provide insights into the second-order behavior of such architectures?
In summary, this is a strong paper that tackles an important and broadly relevant problem in machine learning and deep neural network optimization. The authors convincingly argue that saddle points are a key factor causing first-order methods (e.g., SGD) to suffer from inefficiency and stagnation. The paper provides sufficient evidence and conducts a thorough analysis across multiple machine learning tasks to substantiate its central claims.