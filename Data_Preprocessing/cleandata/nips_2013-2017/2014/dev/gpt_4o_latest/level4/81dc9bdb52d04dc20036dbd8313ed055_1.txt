This paper introduces a novel exact algorithm for structure learning of chordal Markov networks (MN) under decomposable score functions. The proposed algorithm employs a dynamic programming approach by utilizing recursive partition tree structures. These structures, which are equivalent to junction trees, facilitate the decomposition of the problem into smaller subproblems, thereby enabling dynamic programming. The authors provide a thorough literature review, prove the correctness of their algorithm, and benchmark it against a modified version of GOBNILP, a state-of-the-art method for exact structure learning of Bayesian networks.
The manuscript is well-written, relevant to NIPS, and technically rigorous. To the best of my knowledge, the work is novel and represents a significant contribution to the field. However, the primary concern is the lack of a clear motivation for focusing on learning chordal Markov networks. I assume the restriction to chordal networks is intended to ensure efficient exact inference and reduce sample complexity. A discussion clarifying this motivation would be valuable.
Below, I outline some minor issues:
Page 2, Line 67: Recent works on learning Bayesian networks of bounded treewidth should be cited as related work:  
- P. Parviainen et al., "Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming," AISTATS 2014.  
- J. Berg et al., "Learning Optimal Bounded Treewidth Bayesian Networks via Maximum Satisfiability," AISTATS 2014.  
The first paper, in particular, presents an integer programming formulation that could potentially be adapted for learning chordal MNs, similar to how the authors adapted the work of Bartlett and Cussens. Since the implementation of this code is publicly available, the authors are encouraged to compare their method against it.
There is also a conceptual similarity between learning maximum likelihood Bayesian networks of bounded treewidth and learning chordal maximum likelihood MNs of bounded treewidth. If this connection holds, algorithms for the former could also be tested against the proposed approach, which would strengthen the link between these two tasks.
Page 3, Lines 108-110: By assuming that the scores are provided as input, the authors implicitly assume that the width (w) of the network decomposition is small. Consequently, the experimental results for cases where w approaches infinity seem less relevant. The current approach appears to be tailored for scenarios with "small" w (e.g., up to 30). A note clarifying this limitation would be helpful.
Same Page, Definition 1: It would be clearer to assign names to the conditions (1), (2), and (3) in Definition 1. This would prevent confusion when following the proofs, which also include enumerations. For example, naming them RPT1, RPT2, and RPT3 would improve readability.
Page 5, Line 245: The enumeration (0), (1), and (3) may confuse readers, as they might expect a (2) to appear. Renumbering these conditions would enhance clarity.
Page 7, Line 370: Additional or alternative references for score pruning rules include:  
- C.P. de Campos, Q. Ji, "Properties of Bayesian Dirichlet Scores to Learn Bayesian Network Structures," AAAI 2010.  
- C.P. de Campos, Q. Ji, "Efficient Structure Learning of Bayesian Networks using Constraints," Journal of Machine Learning Research 12: 663-689 (2011).  
Pages 7-8, Experiments Section: Drawing conditional probability tables (CPTs) uniformly at random seems unrealistic. A better approach would be to sample distributions from a symmetric Dirichlet distribution with a hyperparameter < 1, which would result in relatively high-entropy distributions that better mimic real-world scenarios. Since the performance of GOBNILP is sensitive to the parameters, using unrealistic models might bias the comparison.
Page 9: In Reference 13, the full name of JMLR should be written out.
Overall, the paper is well-written, relevant to NIPS, and technically sound. It appears to be novel and provides a meaningful contribution to the study of graphical models with bounded complexity.