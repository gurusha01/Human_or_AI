The paper introduces a novel regression approach, termed calibrated multivariate regression (CMR), tailored for high-dimensional data analysis. In addition to presenting the CMR framework, the study emphasizes two key aspects: (1) employing a smoothed proximal gradient method to compute the optimal solutions of CMR, and (2) conducting a theoretical analysis of CMR's statistical properties.
A notable contribution of the work lies in the development of the CMR formulation, where the loss term is designed to calibrate each regression task's loss based on its corresponding noise level. However, I am curious if there is a more intuitive explanation for leveraging the noise level in this calibration process. The authors are encouraged to elaborate further on this aspect.
The findings in Theorem 3.2 demonstrate that CMR attains convergence rates comparable to its non-calibrated counterpart, OMR. Given that OMR employs a differentiable loss term, it appears to offer computational advantages over CMR. It would be helpful if the authors could provide practical guidance on how to choose between OMR and CMR in different scenarios.
In summary, the paper presents a new regression method, CMR, for high-dimensional data analysis, utilizing the smoothed proximal gradient method for solution computation and offering a theoretical examination of its statistical properties.