The paper introduces a CRF model trained via auto-encoding through latent variables \( y \). Specifically:  
(a) \( P(y|x) \) is parameterized in the same way as traditional CRFs.  
(b) The regeneration \( P(\hat{x}|y) \) is modeled as a categorical distribution, independent for each \( \hat{x}i \) and \( yi \) pair, where \( \hat{x}i \) can be a transformation of \( xi \), such as Brown cluster IDs.
The paper is well-conceived and admirably straightforward (in a positive sense). It belongs to the class of work that prompts the reader to think, "Why hasn't this been done before? I wish I had thought of it."
I found the paper compelling. The experimental results are promising, and I am leaning toward recommending acceptance.
The proposed approach appears to be extendable to semi-supervised settings. It would be valuable to see results exploring this direction.
In this context, loosely related work that combines conditional and generative training with log-linear models (though not necessarily in a structured output setting) includes:  
(1) Tom Minka. Discriminative models, not discriminative training. MSR-TR-2005-144, 2005.  
(2) Andrew McCallum, Chris Pal, Greg Druck, and Xuerui Wang. Multi-Conditional Learning: Generative/Discriminative Training for Clustering and Classification. AAAI, 2006.
On page 5, you discuss alternative methods that would require approximations of \( Z \) in the reconstruction phase. However, I did not see empirical comparisons with these methods. Are they outperformed by the methods you do compare against?
Additionally, after Equation (5) and in the discussion of the "Feature HMM," what is the trade-off when applying a "multi-feature view" of \( x \) only to \( P(y|x) \) and not to \( P(\hat{x}|y) \)? It would be helpful to include a discussion on this point.
In Section 3.2, you state that the goal is to achieve "coherence and interpretability." Why is this the goal, and how is interpretability evaluated? It seems that interpretability is not explicitly assessed in the paper.
Regarding the CoNLL-X shared task, you selected only a subset of the languages. This raises questions about the selection criteria. Why not include all the languages? This is particularly relevant given that scalability is presented as a key advantage of your approach.
Since scalability is emphasized, I would have appreciated a graph showing test accuracy as a function of the amount of training data.
The objective function for this model is non-convex. A discussion of local minima would be beneficial, including any empirical evidence of issues related to local minima. Additionally, what initialization strategy do you employ?
---
Minor writing issues:  
- Page 2: "offers two ways locations to impose" → "offers two ways for locations to impose"  
- "condition on on side information" → "condition on side information"  
- Page 6: "and parsing Though" → "and parsing. Though"  
---
This is a well-written paper presenting a clean and impactful idea with positive experimental results. Its relative simplicity—in a good way—suggests it has the potential for widespread use and significant impact.