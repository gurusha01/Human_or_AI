In this paper, the authors present a novel perspective on discriminative clustering. Their core concept is as follows: they envision a classifier that distinguishes between the different (cluster) classes and derive a bound on the generalization error associated with this classifier. This bound is subsequently optimized using a clustering algorithm, specifically a belief-propagation-based method.
The key innovation of this work lies in reformulating the cost function as a novel similarity measure that leverages a kernel density estimate, thereby circumventing the need for an extensive parameter search. This is not to suggest that their approach is entirely free of parameters; rather, the parameters they introduce are either balance parameters or a variance parameter associated with the kernel density function they estimate.
In general, I find this to be a reasonable and promising idea, particularly in terms of reducing the parameter space for discriminative clustering. Additionally, it offers a clearer narrative about the objective being optimized by the clustering algorithm.
However, when it comes to practical applicability, the paper appears somewhat lacking. The primary comparison presented is against a different exemplar-based clustering method. This choice seems somewhat peculiar, given that one of the paper's stated advantages is its reduced reliance on parameters compared to other discriminative clustering approaches. Would it not have been more insightful to compare the method against MM clustering or even information-theoretic approaches? The space allocated to proving the consistency of the kernel density estimate could have been better utilized for such comparisons, as the consistency results largely follow directly from established findings.
I also found the paper somewhat challenging to follow, primarily due to the heavy use of notation in explaining the main ideas. A consolidated "here is our algorithm" section that synthesizes all the components would have been highly beneficial. While I was ultimately able to piece together how the components interact, I spent considerable time tracing various parameters (such as the variance parameter \( h \)) and their roles within the overall algorithm.
In summary, this is a reasonable and worthwhile idea that merits consideration. However, it would benefit from a more comprehensive experimental evaluation and improved clarity in its presentation.