The proposed method, though relatively simple, addresses the problem at hand with notable elegance. However, the primary factor preventing this paper from being a definitive acceptance is the insufficient experimental validation.
Typographical error on line 47: "draw" should be corrected to "drawn."
A more detailed discussion regarding the role of noise in the exploration phase of Algorithm 1 (step 8) would be valuable. This aspect is also not adequately addressed in the experiments section (e.g., what level of noise was used?).
I also encountered some concerns with certain claimed advantages in the paper. Specifically:  
(1) The assertion that PDDP has an edge over PILCO due to avoiding non-convex optimization problems appears questionable, given that the optimization problem in the hyper-parameter tuning step is itself non-convex.  
(2) The claim that PDDP's complexity is independent of the dimensionality of the state seems misleading. While the dimensionality may not directly influence the computational complexity of the policy learning step, it clearly impacts the computation of kernel matrices required for Gaussian Process inferences. This point should be clarified in the final version.  
(3) The statement on line 364 that PDDP maintains a fixed data size appears accurate only in the specific case where I_max = 1 (i.e., when trajectory collection and optimization are not iterated).  
The most significant shortcoming of this paper lies in the experimental results, which are not particularly compelling. First, there is no comparison of the total cost between PILCO and PDDP. Second, is it truly the case that there are no straightforward ways to accelerate PILCO? A discussion on this topic would be beneficial. Finally, GPDDP consistently matches or outperforms PDDP across all dimensions (data efficiency and computational efficiency). This observation seems to challenge the claim that PDDP offers the advantage of "safe exploration." Addressing this point would enhance the experiments section.  
Overall, the authors propose an elegant approach that combines Gaussian Process Regression with Differential Dynamic Programming to effectively balance the exploration/exploitation tradeoff. However, the primary weakness of the paper lies in the limited experimental validation.