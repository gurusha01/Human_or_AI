Spectral methods have recently emerged as an efficient framework for inference by leveraging moment characterization to identify model parameters. In this work, the authors extend this framework to the Indian Buffet Process (IBP) and two related models: the Linear Gaussian Latent Factor Model and the Infinite Sparse Factor Analysis model, which are based on Laplace and Gaussian priors, yielding sparse PCA and ICA-like decompositions, respectively. The paper explores the multilinearity of higher-order moments and addresses non-identifiability issues in the third-order moment by analyzing the structure of the fourth-order moment. A spectral inference algorithm is proposed, and the framework is demonstrated to be more computationally efficient than Variational and MCMC inference methods. Detailed derivations of moments and proofs of bounds are included in the appendix.
The paper builds upon prior work on spectral approaches (ref. 5-8) and extends these methods to the Indian Buffet Process. This extension, along with the accompanying proofs of bounds, is non-trivial, and the paper presents a rigorous and highly technical mathematical analysis that justifies its potential for publication. From an application standpoint, the method is straightforward to implement and performs well in practice, which could encourage adoption by other researchers and lead to significant impact.
While the experiments suggest that the proposed method is more efficient than a variational approach, the paper would benefit from a more thorough comparison of its accuracy relative to a full MCMC procedure. A comparison to MCMC is provided for one synthetic example, demonstrating better reconstruction in less computational time. However, it remains unclear how the method would perform if the MCMC procedure were run for a significantly longer duration. Similarly, while the results for real gene expression data appear meaningful, it would be valuable to compare these results to those obtained using Variational and MCMC inference. Do these alternative approaches also identify approximately 10 similar components? The spectral approach appears accurate and highly efficient, but it would be insightful to assess its practical accuracy against more exhaustive MCMC methods.
In Algorithm 1, the dimensionality \( K \) is determined by truncating eigenvalues larger than a threshold \( \epsilon \) to estimate the rank of \( S_2 \). Since determining \( K \) is central to non-parametric approaches, the paper would benefit from a deeper discussion of how truncation influences the estimation of \( K \) and the criteria used to select \( \epsilon \). The authors mention that \( K \) can also be determined by identifying the largest slope in the eigenvalue spectrum, but it would be helpful to elaborate on the reliability of this approach in general.
The orthogonal tensor decomposition relies on multiple random initializations to ensure reliable estimation. The paper does not clarify how many random initializations are typically required in practice or how significantly local optima affect the results. Addressing this would provide a clearer understanding of the robustness of the proposed method.
Minor comments:
- In the synthetic dataset figures, should "0101" not be "0110" and "1001" be "1010"? If not, please clarify how the binary notation reflects the symbols learned.
- The notation is occasionally unclear regarding scalars versus vectors. For example, \( \pi \) is a vector, but \( \pi^2 \) appears to involve element-wise exponentiation. Similarly, \( C_x \) is sometimes treated as a vector and other times as a scalar. Clarifying this notation would improve the paper's accessibility. Additionally, "diag" is used inconsistently—sometimes to form a diagonal matrix and other times to form a fourth-order diagonal tensor.
Minor comments on the appendix:
- Line 511: \( x^T A \) → \( v^T A \)
- Equation 32: "top" → \( \top \)
- Equation 36: Please clarify how \( Ex \) becomes \( Ez \) in line 36 and then \( Ez, Ey \) in line 37.
- Line 669: "In this section, we provides bounds for moments of linear gaussian latent feature model" → "In this section, we provide bounds for moments of the linear Gaussian latent feature model."
- Line 968: "Before starting the put everything together" → "Before starting to put everything together."
- Line 1126: "in order to" – Do you mean "in order for" or "in order to assure"?
- Please explain the notation \( \text{Poly}(.) \).
- Line 1126: "Lamma 11" → "Lemma 11."
The authors extend spectral approaches to the Indian Buffet Process, including the Linear Gaussian Latent Feature Model and Infinite Sparse Factor Analysis, providing a promising and efficient inference framework. However, for non-parametric models, the paper could better address the challenges of reliably determining model orders.