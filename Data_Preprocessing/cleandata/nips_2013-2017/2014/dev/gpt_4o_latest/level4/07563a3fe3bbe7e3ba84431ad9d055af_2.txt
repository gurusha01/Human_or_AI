This paper introduces a novel method for discriminative unsupervised feature learning. Starting with an unlabeled image dataset, the authors extract 'seed' patches that serve as surrogate classes for training. Various transformations are applied to each seed patch to create training examples within these surrogate classes. A convolutional network is then trained to predict the seed classes. After training, features are extracted by forward propagating new images, obtaining feature maps at each layer, and pooling over them. The approach is evaluated on multiple standard image classification benchmarks, with additional experiments analyzing the impact of transformations and the invariance properties of the learned features.
The proposed method is straightforward and achieves strong performance across all classification tasks. The additional analyses, particularly the exploration of removing transformations, are insightful. The paper is generally well-written and provides sufficient details for reproducibility.
Detailed comments/questions:
- Could you elaborate on the patch extraction process? Are you sampling a large number of patches, calculating gradients for each, and selecting the top-N?
- The paper should mention earlier that the STL-10 dataset uses 96x96 images to provide context for the choice of a 32x32 patch size.
- Was the 32x32 patch size chosen arbitrarily? Do you have any intuition on how to determine patch sizes relative to image sizes?
- Section 4.1: Including a couple of diagrams—one illustrating the convolutional network during surrogate training and another showing the feature extraction process—would be helpful. While the description is clear for readers familiar with this area, it may be challenging for those less experienced.
- From a practical standpoint, a common baseline for datasets with limited labels is to use features extracted from a pre-trained convolutional network (e.g., on ImageNet) and train a linear classifier on top. Including these results in Table 1, even in a separate block, would be valuable. It would be interesting to compare your approach to this baseline, especially since your results on Caltech 101 are comparable to DeCAF.
- How well does validation on the surrogate task correlate with validation on the classification task? Does the network with the best surrogate validation performance also achieve the best classification validation performance? Figure 3 suggests this might be the case. Table 1 in the supplementary material shows classification accuracies for various networks. I recommend including (perhaps in a separate table) both surrogate validation scores and classification validation scores to better understand this relationship.
- Supplementary, Table 2: Why does the diagonal differ from the results in Table 1 for the same network?
- To the best of my knowledge, the highest published result on STL-10 is 70.1% ± 0.6% from "Multi-Task Bayesian Optimization" (Swersky et al., NIPS 2013). Their approach involved extracting k-means feature maps and training a convolutional network on top, which reduced overfitting by fixing the k-means features. A strong baseline for the authors to consider, if time permits, would be to adopt a similar approach while incorporating the transformations described in this paper. This could provide a straightforward and competitive method for applying convolutional networks to small labeled datasets.
In summary, this is a well-executed paper presenting a simple yet effective algorithm that achieves impressive results on standard benchmarks. The generality of the method is promising and could inspire further research directions. For these reasons, I recommend acceptance.