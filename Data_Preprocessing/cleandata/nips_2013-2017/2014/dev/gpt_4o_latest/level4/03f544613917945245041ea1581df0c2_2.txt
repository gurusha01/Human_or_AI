The paper tackles the problem of estimating so-called dirty models, where the parameter is expressed as a sum of "simple parameters," each of which is individually regularized for complexity. The authors introduce a subspace selection strategy that facilitates the use of quadratic approximation. The theoretical framework builds upon the concept of decomposable norms.
I must admit that I have not been closely following this particular line of research, so my feedback comes from the perspective of a generally informed reader in the fields of machine learning and model selection. The work appears to be a thoughtful synthesis of previously published ideas, such as minimizing the nuclear norm via active subspace selection (ICML 2014) and employing quadratic approximation for sparse inverse covariance estimation (NIPS 2011). However, this paper extends these ideas to a broader context involving decomposable norms. While this strikes me as an elegant contribution, certain questions remain.
The reported algorithmic improvement of achieving a 10-fold speedup is not particularly striking, as such gains are often attainable through implementation optimizations. This raises the question of how straightforward it would be to further optimize or parallelize the computationally intensive components of the algorithm. For practitioners with a strong focus on practical applications, this achievement may not seem particularly compelling.
The theoretical exposition is generally clear, albeit somewhat dense, likely due to space constraints. However, the quality of the writing leaves room for improvement. Several language issues are distracting to the point of impairing comprehension. For instance, integrating citations directly into sentences is generally discouraged, as it introduces grammatical ambiguities: should one write "[7, 5] consider the estimation ..." or "[7, 5] considers the estimation ..."? Additionally, poor punctuation can lead to confusion, as seen in examples like "[14] in turn use a superposition ..." or "we consider instead a proximal ...". In some cases, the sentences are so poorly constructed that they become incomprehensible, such as the sentence beginning with "Overall, our algorithmic ..." on page 2.
Embedding formulas within the text creates additional challenges, often resulting in awkward formatting. For example, lines frequently end with an equal sign (e.g., 0 = \newline). On page 7, a formula exceeds the page margin, and both Figure 1 and Table 1 on page 8 similarly breach the margins.
It is also customary for papers to include a conclusion or discussion section, but this paper lacks one. A discussion of future directions would be particularly valuable, as it is unclear where this line of research could lead next.
In Algorithm 1, line 7, the superscript (t) seems like it should be (r), and the update to the sum on line 8 appears unusualâ€”it would be more standard to update variables rather than expressions.
While the theoretical development of quadratic approximation for dirty models with decomposable regularizers is intriguing, its practical significance remains unclear. To some extent, this work feels more like the culmination of an existing research trajectory rather than the beginning of a novel direction. Additionally, the quality of editing does not meet the expected standard.