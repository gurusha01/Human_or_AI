Certainly, here's a paraphrased version of your review while maintaining the original structure, meaning, and technical content:
---
Fundamentally, I believe it is crucial to demonstrate that |H| outperforms the Fisher/Natural Gradient matrix in a direct "apples-to-apples" comparison. This means using the same experimental setup, identical learning rate heuristics, and differing only in the curvature matrix. Such a comparison is essential because the Fisher matrix is widely recognized as being significantly better than the Hessian for nonlinear optimization. While the experiments presented indicate that |H| is also much better than the Hessian, the key question remains: which is superior, |H| or F, when both are tested under identical conditions?
---
Optimization methods typically assume access to convex approximations of the objective function. However, this assumption is clearly invalid in high-dimensional spaces due to the prevalence of saddle points. The idea of explicitly modeling and addressing saddle points is compelling, and the results are intriguing: leveraging the absolute value of the eigenvalues does appear to improve performance on the tasks studied.
That said, I feel the experiments fall short of conclusively supporting the paper's main claim. Specifically, the experiments do not convincingly establish that using the absolute value of the Hessian outperforms the Natural Gradient or the Gauss-Newton matrix. To elaborate, I observed that Saddle-Free Newton (SFN) was initialized using SGD. In contrast, prior work on second-order methods, such as Martens (2010), did not rely on SGD for initialization. While the reported results are better, it is plausible that much of the improvement stems from the use of SGD as a "pretraining" phase for the second-order method. To strengthen the paper, it would be highly beneficial to include a direct comparison where the Gauss-Newton matrix is used with the same optimizer (Krylov Subspace Descent) and experimental setup as SFN. Currently, the only comparison provided is with damped Newton.
A secondary but notable issue is the disproportionate focus on formal justification of the method, including the introduction of a new trust region approach. However, this is not novelâ€”classical derivations of natural gradients already minimize the linear gradient under a quadratic constraint, which is mathematically equivalent to an elliptical trust region method. Similarly, since the |H| matrix has positive eigenvalues, dividing by this positive definite matrix constitutes a valid optimization procedure.
I strongly encourage the authors to replace the Hessian with the Gauss-Newton or Natural Gradient matrix while keeping all other factors constant (and dedicating sufficient effort to tuning the new setup). This would help determine whether the observed performance gains are due to the use of the Hessian or simply the SGD pretraining phase.
---
The paper introduces an innovative and promising approach to optimizing high-dimensional objectives by explicitly addressing saddle points and negative curvature. The experiments are interesting, and the results are promising. However, the current experiments do not sufficiently validate the paper's central claim.
---