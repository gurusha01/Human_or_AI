The paper introduces two related algorithms for clustering under the stochastic block model assumption, addressing scenarios with partial information (where only a subset of the adjacency matrix columns is revealed) or in a data streaming context (where the computation must be performed while retaining fewer bits than the size of the input).
Pros:
The paper contributes to an area of growing interest, particularly in the context of data-constrained computation and clustering of matrices.
The primary algorithm divides the problem into two stages, which appears to be the appropriate approach for streaming clustering under assumptions such as the stochastic block model. This explicit formulation effectively emphasizes the key aspects of the problem.
Cons:
The algorithm is primarily of theoretical interest, as it relies on assumptions that are overly strong for practical networks of even moderate size. Additionally, it fails to perform well at critical "edge density" thresholds, such as in extremely sparse graphs.
The claims regarding the paper's positioning within the streaming and community detection literature are significantly overstated, as elaborated below.
The algorithm's description is somewhat unclear. My interpretation is that under the stochastic block model with large blocks, the input exhibits a well-structured form. The algorithm's primary objective seems to ensure that sampling does not disproportionately concentrate certain quantities in the "empirical" estimates. For instance, this appears to be the purpose of the trimming step, which is reminiscent of techniques used by Feige et al., Montanari et al., and others.
Additional Comments:
1. The problem, as formulated, is not representative of what most researchers would consider realistic community detection scenarios. For example, the clusters are assumed to be non-overlapping, and the results do not hold for extremely sparse graphs (a common case) or extremely dense graphs (which may occasionally arise). Consequently, the work is best viewed as a contribution to the growing body of research on spectral-like algorithms for stochastic block models, which also face similar limitations, and related models under computational constraints.
2. The paper lacks empirical evaluation. While this is not inherently problematic, any such evaluation would likely be trivial (e.g., demonstrating a phase transition already established theoretically) or overly idealized (e.g., simulating memory limitations, as handling realistic memory management is highly nontrivial).
3. The assumption that columns are revealed sequentially is quite strong in terms of practical motivation. This makes it easier to achieve stronger results compared to scenarios where elements or blocks are revealed. For instance, in low-rank matrix approximation, sampling columns yields significantly stronger results than sampling individual elements. More formal data streaming models, such as those in theoretical computer science, often consider alternative data presentation formats.
4. The authors appear to have overlooked related work on clustering, partitioning, or community detection. A brief search reveals several relevant papers, including:
   - FENNEL: Streaming Graph Partitioning for Massive Scale Graphs by Tsourakaki et al.
   - Online Analysis of Community Evolution in Data Streams by Aggarwal and Yu
   - Streaming Graph Partitioning for Large Distributed Graphs by Stanton and Kliot
   - Sparse Cut Projections in Graph Streams by Das Sarma, Gollapudi, and Panigrahy
   While these papers differ sufficiently from the present work to preserve its novelty, they demonstrate that the paper's claims are overstated. Rather than being the first community detection algorithm in the data stream model or the first to address community detection with partial information (as claimed), this paper represents an incremental but interesting contribution to recent work on stochastic block modeling under memory constraints.
Overall:
This is a reasonable paper on a relevant topic. If accepted, I recommend that the authors temper their claims to be more accurate and modest. Additionally, they should revise the presentation to better emphasize their key contributions.