The authors investigate the relatively limited success of CNNs in scene recognition compared to their notable achievements in visual object recognition. They attribute this disparity to insufficient data and address the issue by assembling the largest scene dataset to date, comprising over 6 million images. Using the same convolutional architecture that has excelled in object recognition, they demonstrate improved state-of-the-art performance in scene recognition with the newly curated dataset.
The paper effectively provides additional insights by evaluating scene datasets based on density and diversity metrics. In these terms, the proposed dataset also stands out in terms of quality.
The main conclusion of the paper is that the type of input data determines whether the network learns an object-centric or scene-centric representation. While both representations perform well on object and scene recognition tasks, achieving state-of-the-art results requires a large dataset tailored to the specific task (object or scene). Although this conclusion is somewhat intuitive, it could not be empirically validated until now due to the absence of a sufficiently large scene dataset.
However, the distinction between the proposed visualization and prior approaches remains unclear, as the paper does not explicitly contrast them. The authors should clarify this in their rebuttal.
It is somewhat disappointing that the authors did not conduct the straightforward experiment of combining ImageNet with their new scene dataset to train a unified network for both objects and scenes. Their conclusion implies a trade-off, suggesting that one must choose between learning a strong scene representation or a strong object representation. It remains uncertain whether both can be achieved simultaneously by training a joint network. Nonetheless, the authors successfully demonstrate that the advances of CNNs in object recognition can extend to scene recognition, leveraging their newly collected scene dataset, which is approximately 60 times larger than any existing scene dataset. While the dataset and results are highly valuable, the paper lacks significant technical innovation.