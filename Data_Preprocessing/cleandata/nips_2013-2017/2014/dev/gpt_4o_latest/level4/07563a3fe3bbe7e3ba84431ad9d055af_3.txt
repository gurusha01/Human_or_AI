In this paper, the authors present an intriguing approach to learning invariances within the unsupervised learning framework for image classification tasks. The central concept involves constructing surrogate classes by grouping transformations of randomly chosen image patches. These transformations include 2D operations such as rotations, scaling, translations, and color shifts. The formal analysis is compelling and provides valuable insights into the potential effectiveness of this method.
The manuscript is well-written and straightforward to comprehend. The experiments address several facets of the problem, including the impact of the number of transformations and the network size.
One limitation of the proposed method is that two randomly selected patches with highly similar content may be "forced apart" by being assigned to different surrogate classes. The authors appear to mitigate this issue by restricting the number of patches to a maximum of 32,000 from large datasets. However, this limitation raises concerns about generalization when dealing with larger datasets. Could the authors provide further clarification or comments on this aspect?
Additionally, it would have been interesting to see an experiment where the same set of surrogate classes was used for unsupervised learning, followed by training dataset-specific classifiers. For instance, given the close relationship between STL-10 and CIFAR-10, why was it necessary to retrain on each dataset separately?
Finally, experiments exploring the effect of varying patch sizes would have added further depth to the analysis.