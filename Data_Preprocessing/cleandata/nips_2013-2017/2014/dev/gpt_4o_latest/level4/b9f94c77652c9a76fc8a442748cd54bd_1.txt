REPLY TO AUTHOR RESPONSE
Thank you for your response. You wrote: "It is possible we missed a related paper in the neural network literature. We would be grateful if you could provide citations of neural architectures which used hand-crafted features for unsupervised learning of structured outputs."
I believe this may have misunderstood my point. My intention was to echo the concerns raised by reviewer_26 (to whom you have already responded).
That said, I am aware of a line of work that might be loosely relevant to your question. Stoyanov et al. (AISTATS 2011, NAACL 2012) demonstrated how to derive the structure of a sum-product feed-forward network by unrolling loopy sum-product belief propagation on a predefined graphical model. In this framework, each potential function in the graphical model can be parameterized using hand-crafted features of its incident variables (e.g., via log-linear models). These potential functions translate into parametric weights of the network, incorporating the hand-crafted features. Additionally, the network's structure mirrors that of the graphical model. The network is subsequently trained using backpropagation to minimize prediction error at designated output variables. Is this "unsupervised"? Similar to the original graphical model, the resulting feed-forward network learns to predict the latent variables of the graphical model from the input variables in a way that aids in predicting the output variables. This could be interpreted as "unsupervised" learning of a structure over the latent variables given the (input, output) variables. If the graphical model is structured as an autoencoder where output = input, their method would yield an unsupervised system closely resembling yours.
---
SUMMARY
The method focuses on unsupervised learning of latent structure \( y \) given input \( x \), employing a bottleneck approach using an autoencoder \( p(x | y) p(y | x) \). Specifically, \( p(y | x) \) is a domain-specific CRF, while \( p(x | y) \) appears to be a simple independent model \( \prodi p(xi | y_i) \). The latent variables \( y \) are compared against human annotations.
The objective (Equation 2) and its gradient are computationally efficient. The objective corresponds to the regularized log-loss of the reconstruction, marginalized over \( y \).
More generally, the autoencoder is expressed as \( p(\hat{x} | y, \phi) p(y | \hat{x}, x, \phi) \), where \( \hat{x} \) is an ad hoc simplified version of \( x \), and \( \phi \) represents side information.
---
COMMENTS
The results are promising, particularly as the method demonstrates consistent improvements across multiple languages (8 for POS tagging, 3 for alignment in MT evaluation). However, the paper and supplementary material appear to have been prepared in haste, making it difficult to discern the experimental details.
The method is theoretically appealing and could serve as a significant building block for learning certain types of latent structure. (This is especially true if reconstructing the input \( x \) is just one task in a multi-task learning setting, where \( y \) could also be useful for predicting other supervised properties of \( x \). The authors do not explore this possibility.)
However, the authors fail to adequately explain why their method is well-suited for unsupervised learning. Their objective is to match human annotations, rather than to learn representations that are generally useful for other tasks. A common challenge in such settings is that the latent variables often capture "incorrect" properties of the dataâ€”those not aligned with human annotations. While the authors briefly acknowledge this issue at L067, they do not provide a compelling explanation of why their method would mitigate this problem. Additionally, there is no analysis of the experimental results to clarify what was learned and why.
The paper primarily provides single-number comparisons with other methods, without any breakdowns or targeted experiments to elucidate the underlying mechanisms. This leaves the results as an unexplained success. (There are no learning curves provided, either.)
It would also be valuable to investigate whether the improved results stem from reduced model error or reduced search error. For instance, if method A (previous) and method B (proposed) yield latent distributions \( pA \) and \( pB \) over \( y \), initializing both methods separately at \( pA \) and \( pB \) could reveal whether method A's objective prefers \( pA \) and method B's prefers \( pB \) (indicating a better objective), or whether both prefer the same distribution (suggesting a search bias).
The authors claim (L262) that their autoencoder architecture is more appropriate than neural autoencoders in some respects, but this assertion is not substantiated with discussion, empirical comparisons, or error analysis. It seems the authors are primarily motivated by the fact that the \( p(y | x) \) model can leverage a rich history of efficient, feature-engineered work on supervised structured prediction in NLP. However, many neural architectures could also incorporate such features.
---
DETAILED COMMENTS TO AUTHOR
- The \( p(\hat{x} | y) \) model is only described indirectly. While I can infer its details, please provide an explicit description.
- What exactly are you evaluating? At L172, you mention the distribution \( p(y | x, \hat{x}, \phi) \), under which the probability of a given latent \( y \) is proportional to \( p(\hat{x} | y, \phi) p(y | x, \phi) \). Are you evaluating the 1-best \( y \) from this distribution? One or multiple samples? Something else?
- The features are not clearly described. For instance, supplement L103 states that auto+full features include functions of \( yi \), but supplement Table 2 does not specify these features. Additionally, it is unclear whether "xi, x_{i-1}" in the "full" column refers to conjoined features or a list of features. It is also ambiguous whether the full model includes all h&k features.
- L147: Even before introducing \( \phi \), side information was already allowed during the encoding phase (e.g., any part of \( x \) not included in \( \hat{x} \)). Perhaps clarify that \( \phi \) is side information specifically available during the reconstruction phase.
- A potentially relevant reference is http://www.cs.cmu.edu/~nasmith/papers/gimpel+smith.naacl12b.pdf.
- Regarding the supplementary material: Currently, it is formatted as a separate document, which is confusing. Consider integrating it as appendices to the main paper, following the bibliography, without duplicating content. This would allow for a unified bibliography, figure/equation numbering, and cross-references. After compiling with pdflatex, you can use pdftk to split the document into the main paper and supplementary material.
---
This is an appealing method, and the core idea is well-suited for venues like NIPS or ACL. The results appear strong. However, the paper would benefit from deeper analysis of why the method should work and why it does work, as well as a more precise description of the experiments, which I trust the authors can address.