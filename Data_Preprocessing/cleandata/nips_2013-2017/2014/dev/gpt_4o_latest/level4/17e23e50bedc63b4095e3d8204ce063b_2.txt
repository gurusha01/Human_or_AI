Review - Summary:
This paper posits that saddle points, rather than local minima, are the primary challenge in optimizing high-dimensional, non-convex functions such as those encountered in deep neural networks, when using gradient-based or Newton's methods. The authors provide a review of relevant theoretical findings, empirically validate the issue in deep neural networks performing visual object recognition tasks, and propose a novel solution, which is demonstrated to be effective for training deep neural networks.
Main Comments:
This is a novel and impactful paper that has the potential to significantly alter prevailing intuitions about learning in deep networks, as well as other non-convex optimization problems. The review of theoretical work highlights results that are not widely recognized within the deep learning community, and a key contribution of the paper is the empirical validation on MNIST and CIFAR datasets. This validation establishes a relationship between training error and the index of critical points, demonstrating that the theoretical results hold even without the simplifying assumptions used in prior work.
However, is there a clear justification for using only the top-k eigenvectors of the Hessian? It appears that the Krylov method might overlook directions with small or negative curvature that are characteristic of saddle points. Specifically, it seems plausible that the Hessian could appear fully positive definite within the Krylov subspace, even when descent directions with negative eigenvalues exist outside of it. For instance, in the classical saddle point depicted in Fig. 2a, if the Krylov subspace had a dimension of 1, it would align with the high positive curvature direction, ignoring the negative curvature direction. This raises concerns that the proposed approximate SFN method might also become trapped at saddle points where the number of large positive eigenvalues exceeds the dimension of the Krylov subspace.
Additionally, it might be insightful to visualize the weights at the end of the MSGD epoch compared to a few iterations after SFN takes over. Are there discernible patterns in the changes, such as previously inactive units with small norm weights becoming active? While there may not be any clear patterns, identifying such changes could provide valuable intuition. For instance, does this primarily address scaling symmetries across layers, permutation symmetries among hidden units, or other phenomena?
The paper does not place much emphasis on its result of achieving lower error for the deep autoencoder compared to Hessian-free optimization. However, it is worth noting that the Hessian-free optimization result did not leverage a carefully tuned MSGD initialization, which may have influenced the comparison.
Overall, the paper is well-written and clearly presented.
Minor Comments:
- Adding a vertical line in Fig. 4b to indicate the point where the switch to SFN occurs would be helpful.  
- The paper effectively argues that saddle points, rather than local minima, are a critical obstacle for current learning algorithms applied to non-convex models like deep neural networks. It also highlights an algorithm capable of escaping saddle points more efficiently.