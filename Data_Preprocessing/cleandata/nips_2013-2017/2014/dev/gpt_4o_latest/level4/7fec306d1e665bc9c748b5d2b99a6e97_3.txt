The authors present a method that integrates a Gaussian Process (GP) dynamics model, learned from data, with a trajectory optimization algorithm resembling Differential Dynamic Programming (DDP). This optimization operates on the mean and covariance of the state distribution. The proposed approach is benchmarked against PILCO and an ablated version (using only the mean as the state), achieving slightly inferior performance but offering a significant reduction in computation time.
However, the proposed method appears to align closely with Gaussian belief space planning, a connection that the authors do not explicitly acknowledge. For instance, the paper "Motion planning under uncertainty using iterative local optimization in belief space" describes a method that, aside from the GP learning phase, is strikingly similar. Numerous other techniques within the broader "family" of belief space planning methods also share this resemblance. Consequently, the novelty of the proposed approach seems limited, as it essentially combines belief space planning with learned GP models. Since belief space planning algorithms are typically agnostic to the choice of simulator, incorporating a GP-based dynamics model into any existing belief space planning framework would yield a comparable method.
There are additional concerns with this work. Among prior methods, the authors only compare their approach to PILCO, while entirely overlooking the extensive body of belief space planning literature. This omission may stem from the fact that belief space planning often does not explicitly address model learning, as any model learning technique can be paired with a belief space planner. Nevertheless, there are other model-based reinforcement learning (RL) methods that warrant comparison. Given the authors' emphasis on computational efficiency as a key advantage, the choice of PILCO as a baseline is questionable, as PILCO is known for its exceptionally high computational demands, which are atypical for model-based RL. Without comparisons to other methods, it remains unclear whether the proposed method offers a meaningful improvement in the tradeoff between computation and sample efficiency.
The title of the paper also appears overly ambitious. Numerous prior works address the "probabilistic" aspects of DDP, including papers like "Stochastic DDP," as well as research on handling noise and uncertainty (e.g., state- and action-dependent noise). Additionally, the field of belief space planning inherently deals with such probabilistic considerations. A more appropriate title might be "Gaussian Process DDP," or, given the paper's primary contribution of integrating belief space planning with learned GP models, something along the lines of "GP Models for Belief Space Planning."
---
Regarding the rebuttal: The distinction the authors draw between their method and belief space planning remains unclear. In belief space planning, the true underlying dynamics are sometimes assumed to be Gaussian (not equivalent to deterministic, though E[f(x)] = f(E[x]) holds). The resulting mean and covariance propagation equations closely resemble those presented in this paper, where stochastic transitions in the state space are transformed into deterministic transitions in the space of state mean and covariance. If there is a meaningful distinction, it should be demonstrated experimentally, as it is not evident from the description whether one method outperforms the other.
I appreciate the clarification regarding PI^2, though I question whether this algorithm is the most suitable comparison. Requiring 2500 iterations to solve the pendulum swing-up task seems excessive (see, for example, NP-ALP, "Reinforcement Learning in Continuous Time and Space" by Doya, 2000, and other works that benchmark on the pendulum task). A comparison with another model-based method would be more appropriate, given that the focus is on model-based approaches with specific inductive biases (e.g., smooth dynamics functions). The proposed method essentially combines DDP-based belief space planning with a learned GP dynamics model. In light of existing literature, this contribution does not appear particularly novel. Furthermore, the experimental results fail to convincingly demonstrate a clear advantage in the computation cost-sample complexity tradeoff, as the only baseline (PILCO) is an outlier in terms of computational cost compared to most model-based RL methods.