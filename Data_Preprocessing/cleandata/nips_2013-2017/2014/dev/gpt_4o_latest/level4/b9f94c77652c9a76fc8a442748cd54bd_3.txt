This paper proposes an efficient approach to unsupervised structured prediction by incorporating a CRF as the first layer of an autoencoder. The manuscript is exceptionally well-written, presenting an idea that is both elegant and highly appealingâ€”one of those "I wish I had thought of this" moments. The experimental results, demonstrated through POS tagging and MT alignment tasks, are robust and thoughtfully analyzed. The discussion of related work is generally fair, with one significant omission: the 2008 ACL paper by Suzuki and Isosaki on semisupervised CRF training. Upon closer inspection, their method bears notable similarities to the one proposed here. In essence (paraphrasing their more complex exposition), Suzuki and Isosaki jointly train a CRF p(y|x) and a generative model p(x'|y) using a loss function that combines labeled data error with the disagreement between the two models on unlabeled data. While their approach is considerably more intricate than yours, and your presentation is much more streamlined and effective, it is important to carefully articulate the connection between the two methods and also consider more recent extensions of their work.
The claim that neural autoencoders cannot infer latent sequential structure without labeled data seems somewhat overstated. While it may be true that we currently lack a clear method for achieving this, models like RNNs and LSTMs theoretically possess the capability to do so. A more nuanced comparison between these different types of autoencoders would strengthen the argument.
Overall, this is a clearly written and compelling presentation of a simple yet powerful idea for unsupervised learning of structured predictors. I am highly impressed by the paper, but addressing the missing citation and its implications is essential.