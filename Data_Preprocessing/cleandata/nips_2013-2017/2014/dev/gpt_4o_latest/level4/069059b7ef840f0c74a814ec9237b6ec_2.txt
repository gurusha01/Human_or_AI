This paper introduces a clustering framework based on a pairwise similarity function model. Different data partitions are utilized to construct various nonparametric classifiers, and the optimal clustering is achieved by minimizing the generalization error of the classifiers associated with these partitions. The framework incorporates two nonparametric classifiers: the plug-in classifier and the nearest-neighbor classifier. Additionally, the authors establish an intriguing result, showing that the generalization error bound for unsupervised plug-in clustering asymptotically equals the weighted volume of the cluster boundary under the Low Density Separation assumption.
The paper effectively connects two significant topics in machine learning: multi-class classification and clustering, and it is grounded in strong theoretical foundations. However, the paper is poorly written. While the introduction is well-structured, subsequent sections delve into technical details without summarizing the results in a concise and comprehensible manner. The technical lemmas in Section 2 exacerbate the confusion, making it challenging to assess the paper's true impact. Section 2, in particular, overwhelms the reader with technical details before providing any intuition. Moreover, the lemma statements are unclear and poorly presented. For example:
Lemma 1:
- It is unclear how \( n0 \) depends on \( \sigma0 \) and the VC characteristics of \( K \).
- The statement is not grammatically coherent and requires revision.
Lemma 2:
- The authors introduce numerous interdependent parameters, making it nearly impossible to discern the lemma's actual meaning from the assumptions provided.
Lemma 3:
- Similar issues to those mentioned above are present.
The lemma statements are difficult to read, and some are introduced too early in the paper. The authors should invest more effort in improving the presentation of their results. A summary of the theoretical findings should precede the detailed statements, and the authors should provide intuition for each lemma discussed in the main text.
The authors demonstrate how their techniques can be applied to exemplar-based clustering, but they fail to compare their method with existing state-of-the-art approaches. The experimental section is minimal and does not provide sufficient evidence of the advantages of the proposed technique over other clustering algorithms. The difficulty of clustering depends heavily on the specific setting, such as whether the data is high-dimensional or lies on low-dimensional manifolds, whether data points are sparse or dense, whether the ground truth clusters are convex or non-convex, and the objective function being optimized. Without a detailed analysis of the algorithm's performance across these scenarios, it is difficult to evaluate its practical applicability or its ability to address nontrivial problems.
The computational complexity of the proposed algorithms is not discussed, which is a significant omission. Additionally, it is unclear whether the analysis can be extended to other classifiers beyond the nearest-neighbor and plug-in classifiers. The authors should also consider releasing the source code used in their experiments to facilitate reproducibility, though this is not a critical requirement.
In summary, this paper addresses an important machine learning problem and introduces innovative ideas that, as the authors claim, "bridge the gap between clustering and multi-label classification." To some extent, I agree with this claim. However, the paper suffers from poor organization and presentation, making it difficult to grasp the main theoretical contributions. The experimental section is incomplete, and there is no comparison with existing methods. I strongly recommend that the authors invest significant effort in reorganizing and rewriting the paper, particularly Sections 2 and 3, based on the points raised above.