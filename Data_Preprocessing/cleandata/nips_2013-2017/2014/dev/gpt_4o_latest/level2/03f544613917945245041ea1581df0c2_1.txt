The paper presents a novel family of second-order optimization algorithms tailored for high-dimensional statistical estimators with "superposition-structured" or "dirty" regularization. The authors propose a proximal Newton framework that leverages quadratic approximations of the loss function and incorporates an active subspace selection strategy to reduce computational complexity. The key contributions include theoretical convergence guarantees under challenging conditions (e.g., non-positive-definite Hessians) and empirical demonstrations of the algorithm's efficiency, achieving over a 10x speedup compared to state-of-the-art first-order methods in applications like latent Gaussian Markov random field (GMRF) structure learning and multi-task learning.
Strengths:
1. Novelty and Significance: The paper addresses a critical gap in optimizing dirty statistical models, which are increasingly relevant in high-dimensional settings. The extension of active subspace selection to multiple regularizers is a notable innovation.
2. Technical Soundness: The theoretical contributions, including global convergence and asymptotic quadratic convergence guarantees, are rigorous and well-supported. The relaxation of assumptions (e.g., self-concordance) to accommodate the superposition structure is particularly commendable.
3. Practical Impact: The empirical results convincingly demonstrate the algorithm's efficiency and scalability. The speedup over existing methods, especially in large-scale problems like RCV1, highlights the practical utility of the proposed approach.
4. Clarity of Applications: The paper effectively contextualizes its contributions by applying the algorithm to real-world problems, such as GMRF structure learning and multi-task learning, showcasing its versatility.
Weaknesses:
1. Clarity: While the paper is technically dense, some sections, particularly those detailing the active subspace selection and quadratic approximation framework, could benefit from clearer exposition. For example, the derivation of subspace selection criteria is mathematically rigorous but may be difficult for non-experts to follow.
2. Experimental Scope: The experiments focus on two applications, which, while relevant, may not fully capture the breadth of potential use cases for the proposed framework. Additional benchmarks on diverse datasets or problem domains would strengthen the empirical evaluation.
3. Limitations Discussion: The paper does not explicitly discuss potential limitations, such as the computational overhead of Hessian-based methods in extremely high-dimensional settings or the sensitivity of the active subspace selection to parameter tuning.
Suggestions for Improvement:
1. Enhance the clarity of the active subspace selection methodology by including more intuitive explanations or visual aids.
2. Expand the experimental evaluation to include additional datasets or problem types, such as image or time-series data, to demonstrate broader applicability.
3. Include a discussion of limitations and potential extensions, such as adapting the framework for non-convex loss functions or exploring approximate solutions for the quadratic subproblem.
Recommendation:
This paper makes a significant contribution to the field of high-dimensional optimization and dirty statistical models. Its combination of theoretical rigor and practical relevance aligns well with the goals of NIPS. While some improvements in clarity and experimental breadth are desirable, the strengths of the paper outweigh its weaknesses. I recommend acceptance, with minor revisions to address the clarity and scope concerns.