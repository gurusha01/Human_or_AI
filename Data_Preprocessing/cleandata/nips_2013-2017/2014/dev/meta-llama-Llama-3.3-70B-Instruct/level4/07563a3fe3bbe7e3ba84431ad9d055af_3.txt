This manuscript presents a compelling concept for acquiring invariances in the context of unsupervised learning for image classification tasks. The central notion involves constructing surrogate classes comprised of transformation groups applied to randomly chosen patches, with transformations including rotations, scaling, translations, and color shifts. The accompanying formal analysis provides insightful explanations for the potential efficacy of this methodology.
The manuscript is well-structured and clear, facilitating easy comprehension. The experimental section thoroughly explores various facets of the problem, including the impact of transformation numbers and network size.
However, a potential limitation of the proposed approach is that it may forcibly distinguish between two randomly selected patches with highly similar content, treating them as distinct surrogate classes. The authors seem to address this issue indirectly by limiting the number of patches to 32,000, even from large datasets. Nevertheless, utilizing a small set of surrogate patches might hinder generalization, particularly with larger datasets. It would be beneficial to hear the authors' thoughts on this matter.
An additional experiment that could provide further insight would involve using the same set of surrogate classes for unsupervised learning and then training dataset-specific classifiers. For instance, given the close relationship between STL-10 and CIFAR-10, it is unclear why the authors opted to retrain on each dataset separately.
Lastly, investigating the effect of varying patch sizes on the outcomes could yield interesting results, offering a more comprehensive understanding of the proposed methodology.