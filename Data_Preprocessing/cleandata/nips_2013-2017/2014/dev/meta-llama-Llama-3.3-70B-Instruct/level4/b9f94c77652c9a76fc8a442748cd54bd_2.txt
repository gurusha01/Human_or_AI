The paper proposes a conditional random field (CRF) model trained using auto-encoding via latent variables y, where (a) the conditional probability P(y|x) is parameterized similarly to traditional CRFs, and (b) the regeneration probability P(\hat{x}|y) is a categorical distribution that is independent for each \hat{x}i and yi pair, with \hat{x}i potentially being a transformation of xi, such as Brown cluster ids.
This paper is remarkably straightforward and sensible, making it a pleasure to read. It falls into the category of works that prompt the reader to think, "Why hasn't this been done before?" The simplicity of the idea is a significant strength. 
I am inclined to accept this paper, given its positive experimental results. One potential direction for future work is exploring the application of this approach in a semi-supervised setting, which could yield interesting results.
Related work that combines conditional and generative training with log-linear models, although not necessarily in a structured output setting, includes Tom Minka's "Discriminative models, not discriminative training" (MSR-TR-2005-144, 2005) and Andrew McCallum et al.'s "Multi-Conditional Learning: Generative/Discriminative Training for Clustering and Classification" (AAAI, 2006).
On page 5, alternative methods are discussed that would require approximations on Z in the reconstruction phase, but empirical comparisons with these methods are not provided. It would be beneficial to know whether the proposed method outperforms these alternative approaches.
Following Equation (5) and the description of the "Feature HMM," it is unclear what is lost by only applying a "multi-feature view" of x to the P(y|x) side and not to the P(\hat{x}|y) side. A discussion on this point would be valuable.
In Section 3.2, the goal of being "coherent and interpretable" is stated, but the rationale behind this goal is not explained, and interpretability is not evaluated. 
The selection of languages from the CoNLL-X shared task seems arbitrary, and it would be more convincing to include results for all languages. Given that scalability is a claimed advantage of this approach, a graph showing test accuracy as the amount of training data increases would be a useful addition.
The objective function of this model is non-convex, which raises concerns about local minima. A discussion on the empirical evidence regarding local minima and the initialization method used would be beneficial.
Minor writing issues include:
- Page 2: "offers two ways locations to impose" should be "offers two ways for locations to impose," and "condition on on side information" should be "condition on side information."
- Page 6: "and parsing Though" should be "and parsing. Though."
Overall, the paper presents a well-written and clean idea with positive experimental results, and its relative simplicity makes it likely to be used and have a significant impact.