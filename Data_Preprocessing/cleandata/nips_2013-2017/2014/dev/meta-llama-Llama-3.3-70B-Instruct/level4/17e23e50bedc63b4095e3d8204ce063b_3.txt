This paper presents a compelling argument regarding the limitations of first-order methods, such as Stochastic Gradient Descent (SGD), and certain second-order methods, including those derived from the Newton method, in navigating saddle points. The primary challenge posed by saddle points is that they hinder the progress of these algorithms, which fail to effectively exploit the direction of negative curvature.
Specifically, when encountering a saddle point, an optimal algorithm should leverage the direction of negative curvature to achieve progress. However, SGD methods are inadequate in this regard, as they struggle to make significant progress in directions of low curvature, a limitation that has been previously established. Meanwhile, second-order methods based on trust regions often fail to exploit directions of large negative curvature, which could facilitate rapid progress.
The authors propose a novel method that builds upon the work of Vinyals and Povey, with a key distinction being the introduction of a new trust region method that can handle non-positive semi-definite (PSD) Hessian matrices. This approach utilizes the absolute value of eigenvalues to define the trust region, enabling the algorithm to cope with saddle points more effectively than SGD, which neglects curvature, and Newton-based methods, which can become trapped in saddles.
Several suggestions and questions are posed to the authors:
- Around line 315, it would be beneficial to clarify the definition of the "k biggest eigenvectors." Are these determined by the absolute value of their corresponding eigenvalues? If not, how can the Krylov approximation capture negative curvature to inform the algorithm's decisions?
- The reported improvement with SFN appears substantial, particularly in the context of the PTB dataset. To facilitate comparison, it would be helpful if the authors could provide perplexities or other standard metrics on the held-out set, considering that carefully tuned SGD can achieve impressive results on PTB.
- Implementing these methods can be complex, so it would be appreciated if the authors could release code or provide detailed instructions, including any heuristics employed.
- From a practical perspective, it is essential to understand how regularization techniques, such as dropout, impact saddle points.
- The network architecture used in the study is not specified, and it would be valuable to discuss how the authors' approach would handle ReLU nets, potentially shedding light on their second-order behavior.
Overall, this paper tackles a critical and general problem in machine learning and deep neural networks, namely optimization, and presents a well-supported argument regarding the importance of addressing saddle points. The in-depth analysis and evidence provided across various machine learning tasks effectively substantiate the paper's central claim.