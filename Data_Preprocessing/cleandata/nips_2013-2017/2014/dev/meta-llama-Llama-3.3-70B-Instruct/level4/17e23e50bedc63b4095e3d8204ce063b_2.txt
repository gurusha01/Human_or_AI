Review - Summary:
This manuscript posits that saddle points, rather than local minima, are the primary obstacle in optimizing high-dimensional non-convex functions, such as deep neural networks, using gradient- or Newton's method-based approaches. The paper provides a comprehensive review of relevant theoretical results, presents empirical evidence of this issue in deep neural networks applied to visual object recognition, and proposes an effective solution for training deep neural networks.
Main comments:
This original paper has the potential to significantly alter our understanding of learning in deep networks and other non-convex optimization problems. The review of theoretical work focuses on lesser-known results in the deep learning community, and the paper's empirical verification on MNIST and CIFAR datasets demonstrates the relationship between training error and index, showing that the results are not dependent on simplifying theoretical assumptions. 
However, it is unclear whether using only the k largest eigenvectors of the Hessian is justified. The Krylov method may overlook directions with low or negative curvature near saddle points. Specifically, the Hessian could appear fully positive definite in the Krylov subspace despite the presence of other descent directions with negative eigenvalues. Considering the classical saddle in Fig 2a, if the Krylov subspace had a dimension of 1, it would point in the direction of high positive curvature, ignoring the existing negative curvature direction. Therefore, the proposed approximate SFN method may also be trapped by saddle points with more positive, large eigenvalues than the Krylov subspace dimension.
Visualizing the weights at the end of the MSGD epoch and after SFN takes over could provide valuable insights. Are there any noticeable patterns in the changes, such as an unused unit with small norm weights becoming active? If such patterns exist, they could help overcome scaling symmetries across layers or permutation symmetries among hidden units. 
The paper does not emphasize its result of lower error for the deep autoencoder compared to Hessian-free optimization. However, it is worth noting that the Hessian-free result did not utilize carefully tuned MSGD for initialization, which may have impacted the comparison.
The paper is well-written and clear.
Minor:
Adding a vertical line to Fig 4b to indicate the switch to SFN would be helpful.
The paper convincingly argues that saddle points, rather than local minima, are a key challenge for current learning algorithms applied to non-convex models like deep neural networks. It also presents an algorithm capable of rapidly escaping saddle points.