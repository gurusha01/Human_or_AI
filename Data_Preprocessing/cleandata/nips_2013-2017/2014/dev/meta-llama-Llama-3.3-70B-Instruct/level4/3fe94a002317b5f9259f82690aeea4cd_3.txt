The authors investigate the relatively limited success of Convolutional Neural Networks (CNNs) in scene recognition compared to visual recognition, attributing this disparity to a scarcity of data. To address this, they compile the largest scene dataset to date, comprising over 6 million images. By training the same convolutional architecture that has been highly effective for object recognition on this new dataset, they achieve state-of-the-art results in scene recognition.
The authors provide additional valuable insights through a comparative analysis of scene datasets based on density and diversity metrics, demonstrating that their new dataset excels in terms of quality as well. A key takeaway from the paper is that the type of input data dictates whether an object-centric or scene-centric representation is learned. While both representations yield good performance on their respective recognition tasks, achieving state-of-the-art performance requires a large, specialized dataset. Although this conclusion may not be entirely unexpected, its validation was previously hindered by the lack of a substantial scene dataset.
However, the distinctiveness of the proposed visualization method from existing ones is not clearly articulated in the paper, and a detailed comparison is needed to understand its novelty. Furthermore, it is surprising that the authors did not explore the straightforward approach of merging ImageNet with their new scene database to train a joint network for object and scene recognition. Their conclusion implies a binary choice between learning a good scene or object representation, without considering the potential of jointly training a network to achieve both. The authors successfully demonstrate that the effectiveness of CNNs in object recognition can be extended to scene recognition using their newly assembled, vastly larger scene dataset. While the dataset and results are highly valuable, the technical approach lacks innovation.