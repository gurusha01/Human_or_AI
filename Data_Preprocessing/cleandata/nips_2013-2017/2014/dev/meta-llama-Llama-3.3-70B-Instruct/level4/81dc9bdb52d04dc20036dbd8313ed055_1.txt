This study presents a novel exact algorithm for learning the structure of chordal Markov networks (MNs) under decomposable score functions, utilizing a dynamic programming approach based on recursive partition tree structures. These structures, equivalent to junction trees, effectively decompose the problem into smaller instances, enabling dynamic programming. The authors provide a thorough literature review, prove the algorithm's correctness, and compare it to a modified version of GOBNILP, a state-of-the-art method for exact Bayesian network structure learning.
The paper is well-structured, relevant to the NIPS conference, and technically sound. It appears to be novel and makes a significant contribution to the field. However, one major issue is the lack of motivation for focusing on chordal Markov networks. It is assumed that this restriction allows for efficient exact inference and smaller sample complexity, but a discussion on this topic would be beneficial.
Several minor issues were identified:
On page 2, lines 67, recent publications on learning Bayesian networks of bounded treewidth should be included as related work, such as P. Parkaviainen et al. (AISTATS 2014) and J. Berg et al. (AISTATS 2014). The first publication's integer programming formulation could be adapted to learn chordal MNs, and comparing against this method would be valuable.
There is a similarity between learning maximum likelihood BNs of bounded treewidth and learning chordal maximum likelihood MNs of bounded treewidth. If this is the case, algorithms for the former could be tested against the proposed method, providing a better connection between the two tasks.
On page 3, lines 108-110, assuming the scores are given as input implies that the network decomposition width (w) is small. Therefore, taking w to infinity in the experiments section may not be valuable, and a warning note would be helpful. Additionally, definition 1 would be clearer if the conditions (1), (2), and (3) were named (e.g., RPT1, RPT2, RPT3).
On page 5, line 245, renumbering the conditions would avoid confusion. On page 7, line 370, additional references for score pruning rules, such as C.P. de Campos and Q. Ji (AAAI 2010 and Journal of Machine Learning Research 2011), would be beneficial.
In the experiments section (pages 7-8), drawing CPTs uniformly at random may not be realistic. Instead, drawing distributions from a symmetric Dirichlet with a hyper-parameter < 1 could mimic real-world distributions and provide a more accurate comparison. On page 9, the full name of the Journal of Machine Learning Research should be written in reference 13.
Overall, the paper is well-written, relevant to NIPS, and technically sound, presenting an important contribution to learning graphical models of bounded complexity.