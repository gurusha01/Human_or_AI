This paper presents a innovative approach to training convolutional neural networks (CNNs) in an unsupervised manner, enabling the model to learn invariance to common transformations. The proposed method, although straightforward, exhibits a certain elegance in its simplicity. By training the CNN to differentiate between image patches and their transformations, the model develops the ability to recognize and be invariant to these transformations.
However, as acknowledged by the authors, this approach is susceptible to "collisions" where distinct image patches with identical content are incorrectly distinguished by the model. Despite this limitation, the method demonstrates novelty and cleverness, making it a worthy candidate for publication with some minor revisions.
The authors also provide theoretical results intended to support the argument that the model learns invariant features. Nevertheless, this analysis appears to merely restate the obvious, namely that achieving a perfect invariant feature representation would result in the global optimum of the objective function, which seems inherent to the definition of the objective function itself. It is possible that a more subtle aspect of the analysis may have been overlooked.
The empirical examination of the proposed approach is thorough and insightful, including experiments that carefully investigate the potential vulnerability of the method as the number of surrogate "classes" or base training patches increases. The experimental results also evaluate the invariance properties of the learned representation, demonstrating a relatively solid and mature empirical foundation for this work.
One notable concern is the presentation of state-of-the-art results for the STL-10, CIFAR-10, and Caltech-101 datasets, where the authors fail to provide a comparison with the actual state-of-the-art, particularly for CIFAR-10. The justification for excluding standard discriminatively-trained CNN models from the comparison seems arbitrary and self-serving. The paper's presentation may lead readers to incorrectly assume that this method approaches the state-of-the-art for all these datasets, which is not the case. To address this, the authors should provide the true state-of-the-art results for each dataset, including comparisons with and without dataset transformations.
Overall, this is a well-written paper that explores a simple yet clever idea through thorough empirical analysis, making it a valuable contribution to the field.