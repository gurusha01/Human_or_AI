This manuscript introduces a novel method for discriminative unsupervised feature learning, leveraging a set of unlabelled images to extract 'seed' patches that serve as surrogate classes for training purposes. These seed images undergo various transformations to generate diverse training examples within each seed class. A convolutional neural network is then trained on these patches to predict the corresponding seed classes. Following training, features are extracted by forwarding a new image through the network, obtaining feature maps at each layer, and applying pooling operations. The approach is evaluated on several standard image classification benchmarks, with additional experiments assessing the impact of transformations and the invariance properties of the learned features.
The proposed method is straightforward and demonstrates robust performance across all classification tasks. The supplementary analysis, particularly the investigation into the effects of removing transformations, is a valuable addition. The manuscript is well-written, providing sufficient information for readers to replicate the results.
Detailed comments and questions:
- Could you provide a more detailed explanation of the patch extraction process? Does it involve sampling a large number of patches, computing gradients for each, and selecting the top-N patches based on certain criteria?
- It would be beneficial to mention the 96x96 image size used in the STL-10 dataset earlier in the manuscript to provide context for the choice of 32x32 patch size.
- Was the 32x32 patch size chosen arbitrarily, or is there an underlying intuition for selecting patch sizes as a function of image sizes?
- In section 4.1, including visual aids such as diagrams illustrating the convolutional network during surrogate training and feature extraction would enhance understanding, particularly for readers unfamiliar with this line of work.
- From a practical perspective, when dealing with a dataset containing a small number of labels, a common approach is to utilize a pre-trained convolutional network on ImageNet and train a linear classifier on top of the extracted features. Including these results in table 1, possibly in a separate section, would provide a useful comparison, even though this approach falls under a different category of algorithms. It would be interesting to see how the proposed method's results compare, especially considering that the performance on Caltech 101 is comparable to DeCAF.
- To what extent does validation on the surrogate task serve as a proxy for validation on the classification task? Does the network with the best validation performance on the surrogate task also exhibit the best validation performance on the classification task? Figure 3 suggests a positive correlation, and including both surrogate validation scores and classification validation scores for each task in a separate table would help clarify this relationship.
- In the supplementary material, table 2, the diagonal elements differ from the results in table 1, which use the same network. Could you explain this discrepancy?
- The current state-of-the-art result on STL-10, to the best of my knowledge, is 70.1% Â± 0.6% achieved by "Multi-Task Bayesian Optimization" (Swersky et al, NIPS 2013), which involves extracting k-means feature maps and training a convolutional network on top of these features. This approach reduces overfitting by fixing the k-means features. A strong baseline to consider, if time permits, would be to apply the transformations used in this manuscript to the k-means features, providing a relatively straightforward method for running convolutional networks on small labelled datasets, potentially yielding competitive results. In summary, this is a well-written paper presenting a simple yet effective algorithm that achieves excellent results on standard benchmarks. The generality of this method is likely to lead to several interesting research directions, and for these reasons, I recommend acceptance.