This manuscript presents a novel formulation of Differential Dynamic Programming (DDP) for models represented as Gaussian processes, providing the necessary derivations to connect the Gaussian Process model with the DDP framework. This approach enables the solution of local optimal trajectory problems for systems with unknown dynamics, sharing similarities with PILCO. The simulated results demonstrate that the proposed method yields solutions significantly faster than PILCO.
Quality and Clarity:
Although the paper's concept is robust, it requires further refinement, particularly in the presentation of results. The current discussion of the results is superficial, with figures 1 and 2 lacking any meaningful analysis. Claims such as "safe exploration" are unsubstantiated, and the conclusion regarding computational complexity is questionable. The writing needs refinement, suggesting that additional polishing iterations are necessary to enhance clarity.
Originality and Significance:
The application of DDP as an optimization method has implications for both DDP/iLQG practitioners and the model-based optimization community. While the work bears similarities to PILCO in philosophy and methodology, it provides a self-contained solution by proposing DDP and developing a comprehensive framework. However, a thorough comparison between the proposed method and PILCO is essential to strengthen the manuscript.
Questions:
1. The statement in Line 33 regarding the scalability of local optimal DDP to high-dimensional problems is incomplete, as it fails to acknowledge the trade-off between local optimality and the potential for poor local optima.
2. The assertion in Line 212 that the variance of control distribution and off-diagonal entries of the state covariance matrix can be neglected is not obvious, particularly regarding control distribution, as noise in control can significantly impact state integration and final cost.
3. The conclusion on computational complexity in Line 294 is premature, as the complexity of the learning policy may not directly relate to the number of states, but the number of control inputs is often proportional to the number of states, resulting in a complexity of O3.
4. The formulation of "safe exploration" is unclear, and its implementation in the algorithm is not evident, despite the intuitive cartoon in Figure 3(a).
5. The sentence in Line 322, "To keep the system stable," is ambiguous and potentially misleading, unless referring to unstable systems like an inverted pendulum.
6. The variance in Figure (a) appears excessively small, suggesting a nearly deterministic system, which is inconsistent with the expected large variance during initial iterations of GP learning.
7. The introduction of GPDDP as a separate method seems unnecessary, as its benefits are not clearly demonstrated, and the effect of uncertainty propagation could be shown without proposing a new method.
8. The manuscript should compare traditional DDP results with fully known dynamics in Figures 1 and 2 to provide a more comprehensive evaluation.
Minor Questions:
1. The legends in Figure 1 are too small, and the y-axes of (a) and (c) should have the same range.
2. The costs for PILCO should be included in Figures 1(b) and 2(b) for a more thorough comparison.
The manuscript provides a self-contained framework for GP-based DDP, which is of significant interest to the DDP/iLQG learning community. However, the paper requires improvement in clarity and evaluation, particularly in comparison to PILCO.