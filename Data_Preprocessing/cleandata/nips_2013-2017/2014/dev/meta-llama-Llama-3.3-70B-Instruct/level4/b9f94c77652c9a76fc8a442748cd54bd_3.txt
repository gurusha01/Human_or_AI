This manuscript proposes a novel approach to unsupervised structured prediction by leveraging a Conditional Random Field (CRF) as the initial layer of an autoencoder, yielding an efficient solution. The paper is exceptionally well-written, presenting an innovative idea that is both intuitive and compelling, with robust experimental results in POS tagging and MT alignment that are thoroughly discussed. The authors provide a fair overview of related work, with a notable omission being the research by Suzuki and Isosaki on semi-supervised CRF training, presented at ACL 2008. Upon closer examination, their proposal bears resemblance to the one presented in this paper, as they jointly train a CRF and a generative model using a loss function that combines labeled data error and model disagreement on unlabeled data. Although their approach is more complex and less elegantly presented than the current work, it is essential to provide a detailed comparison between the two methods and explore more recent related research.
The claim that neural autoencoders are incapable of learning latent sequential structure without labeled data seems to be an argument based on current limitations rather than a fundamental constraint. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks, in particular, possess the potential to learn such structures. A more nuanced comparison between different types of autoencoders would be beneficial. Overall, this paper presents a straightforward yet effective idea for unsupervised learning of structured predictors, with a clear and convincing presentation. However, the connection to prior research that is not currently cited needs to be addressed to further strengthen the manuscript.