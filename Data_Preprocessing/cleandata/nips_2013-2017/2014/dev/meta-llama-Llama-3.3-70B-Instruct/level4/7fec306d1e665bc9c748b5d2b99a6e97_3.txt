The authors suggest utilizing a Gaussian Process (GP) dynamics model, learned from data, in conjunction with a DDP-like trajectory optimization algorithm that operates on the mean and covariance of the state distribution. This proposed method is compared to PILCO and an ablated variant, which uses only the mean as the state, resulting in slightly worse performance but with a significant improvement in computation time.
Upon examination, it appears that the proposed method is essentially performing Gaussian belief space planning, aside from learning a GP model of the dynamics. This connection is not acknowledged by the authors, but it seems evident. For instance, the paper "Motion planning under uncertainty using iterative local optimization in belief space" describes a method that, excluding the GP learning phase, bears a strong resemblance to the proposed approach. There are numerous other methods within the broader category of belief space planning techniques that share similarities. Consequently, the novelty of the proposed approach seems limited, as it primarily combines belief space planning with learned GP models. Given that belief space planning algorithms are generally agnostic to the simulator's form, integrating a GP-based dynamics model with any existing belief space planning algorithm would yield a similar approach.
Several issues are present in this work. The authors only compare their method to PILCO, neglecting the extensive body of belief space planning research. Although this might be due to the fact that belief space planning often does not involve model learning, other model-based reinforcement learning (RL) methods are also overlooked. The choice of PILCO for comparison is questionable, as it has exceptionally high computational requirements that are atypical for model-based RL. Without comparing to other methods, it is unclear whether the improvement in the tradeoff between computation and sample complexity is notable.
The paper's title is somewhat overly ambitious. Various papers discuss the probabilistic aspects of DDP, including those that address noise and uncertainty, as well as the field of belief space planning. A more suitable title might be "Gaussian Process DDP" or "GP models for belief space planning," as the primary contribution appears to be the combination of belief space planning with learned GP models.
Regarding the rebuttal, the distinction between the authors' approach and belief-space planning is not entirely clear. In belief-space planning, the underlying true dynamics is sometimes assumed to be Gaussian, and the resulting mean and covariance propagation equations resemble those presented in this paper. If a distinction exists, it should be demonstrated experimentally, as it is not obvious from the description whether one method is superior to the other.
The clarification regarding PI^2 is appreciated, but it is uncertain whether this algorithm is the best candidate for comparison, given its requirement of 2500 iterations to solve the pendulum swingup problem. Other model-based methods, such as NP-ALP or those discussed in "Reinforcement Learning In Continuous Time and Space" by Doya, might be more suitable for comparison. The proposed method appears to be DDP-based belief space planning combined with a learned GP dynamics model, which, in light of prior work, does not seem particularly novel. The experimental results also fail to convincingly demonstrate the advantage of the proposed method in terms of the computation cost-sample complexity tradeoff, as the only prior method compared (PILCO) has unusually high computational costs compared to most model-based RL methods.