This manuscript presents the development of a scene-centric database, termed PLACES, comprising over 6 million labeled images for scene recognition tasks. The primary motivation behind constructing this database is to serve as a complement to the object-centric ImageNet database, with the goal of enhancing scene recognition performance through the learning of deep features using PLACES. The database was constructed by leveraging search engines with composite queries and validating the results using Amazon Mechanical Turk (AMT). To assess the density and diversity of PLACES in comparison to existing databases such as SUN and ImageNet, the authors propose two novel metrics and employ AMT to obtain these measures, thereby demonstrating the advantages of PLACES. Furthermore, the authors compare the features learned from PLACES and ImageNet, illustrating that the two databases are indeed complementary, with PLACES yielding superior performance in scene recognition and ImageNet exceling in object recognition.
The notable strengths of this work include:
1. A rigorous and comprehensive approach to dataset construction.
2. Detailed and persuasive results that surpass those of existing scene databases, highlighting the significance of data quality.
Some minor concerns and suggestions for improvement are:
1. The algorithmic contribution of this work is somewhat limited.
2. The method used to calculate density, which relies on nearest neighbors based on the GIST feature, may potentially introduce bias into the density metric, despite being consistently applied across all databases.
3. The diversity metric, which involves human judgment of image similarity, may be challenging to evaluate when judges are presented with unrelated random images, potentially leading to inconsistent assessments; it would be interesting to know if the authors observed similar challenges in their AMT experiments.
4. Given the complementary nature of PLACES and ImageNet, exploring the possibility of combining these databases to train a model capable of exceling in both object and scene recognition could be a fruitful avenue for future research.
Overall, this is a robust and well-executed work on database construction, with a clearly written manuscript and convincing results demonstrating the value of the developed database. It is assumed that the database will be made publicly available, and it is anticipated that it will have a positive impact on scene image parsing and scene recognition research.