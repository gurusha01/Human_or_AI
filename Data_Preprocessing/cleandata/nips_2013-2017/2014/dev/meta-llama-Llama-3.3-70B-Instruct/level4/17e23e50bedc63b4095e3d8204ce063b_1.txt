In my opinion, a direct comparison between |H| and the Fisher/Natural Gradient matrix is crucial, as it would provide an "apples to apples" evaluation of their relative performance. This is particularly important given the Fisher matrix's known superiority in nonlinear optimization. The experiments presented demonstrate the superiority of |H| over the Hessian, but the key question remains: how does |H| compare to the Fisher matrix, F, when both are used in an identical setup?
Optimization methods typically rely on convex approximations of the objective function, but this assumption is flawed due to the prevalence of saddle points in high-dimensional spaces. The idea of explicitly modeling and addressing saddle points is intriguing, and the results suggest that working with the absolute value of the eigenvalues yields improved performance.
However, I believe the experiments fall short of convincingly demonstrating the superiority of |H| over the Natural Gradient or Gauss-Newton methods. Specifically, the use of SGD for initialization in Saddle Free Newton raises questions about the source of the observed improvement. Previous work, such as Martens (2010), did not employ SGD for initialization, and it is possible that the "pretraining" stage contributed significantly to the results. To strengthen the paper, I recommend including an additional experiment that uses the Gauss-Newton matrix alongside SFN, with the same optimizer and Krylov Subspace Descent.
Furthermore, I think the paper devotes excessive space to unnecessary formal justifications, such as the introduction of a new trust region method, which is essentially equivalent to the classical derivation of natural gradients. The fact that the |H| matrix has positive eigenvalues renders the optimization procedure proper, making further justification unnecessary.
To address these concerns, I strongly suggest replacing the Hessian with the Gauss-Newton/Natural Gradient matrix, while keeping all other parameters fixed, and dedicating sufficient time to tuning the new setup. This would help determine whether the Hessian or the SGD pre-training is responsible for the improved performance.
The paper proposes a novel approach to optimizing high-dimensional objectives by explicitly handling saddle points and negative curvature, and the experiments are intriguing. However, the current experiments do not adequately support the paper's main claim, and further investigation is needed to establish the relative merits of |H| and the Fisher matrix.