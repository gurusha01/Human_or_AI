This paper proposes a novel proximal Newton framework for optimizing "superposition-structured" or "dirty" statistical estimators, which are commonly used in high-dimensional problems. The authors develop a quadratic approximation framework with active subspace selection, allowing for efficient optimization of these estimators. The paper provides a thorough introduction to the background and applications of superposition-structured models, including Gaussian graphical models with latent variables and multi-task learning.
The proposed algorithm, QUIC & DIRTY, uses a block coordinate descent method to update each parameter component, and an active subspace selection technique to reduce the problem size. The authors provide convergence guarantees for their algorithm, including global convergence and asymptotic quadratic convergence rate. The paper also presents empirical results on two real-world applications, demonstrating that the proposed algorithm is more than 10 times faster than state-of-the-art methods.
The strengths of this paper include:
* The proposal of a novel proximal Newton framework for optimizing superposition-structured estimators, which fills a gap in the existing literature.
* The development of an active subspace selection technique, which allows for efficient optimization of these estimators.
* The provision of convergence guarantees for the proposed algorithm, including global convergence and asymptotic quadratic convergence rate.
* The presentation of empirical results on two real-world applications, demonstrating the effectiveness of the proposed algorithm.
The weaknesses of this paper include:
* The assumption that the Hessian matrix is positive definite, which may not always be the case in practice.
* The requirement for the loss function to be strongly convex, which may not always be satisfied.
* The lack of comparison with other optimization methods, such as stochastic gradient descent or quasi-Newton methods.
Overall, this paper presents a significant contribution to the field of optimization and statistical estimation, and the proposed algorithm has the potential to be widely used in practice. The authors have provided a thorough and well-written paper, with clear explanations and concise notation.
Arguments pro acceptance:
* The paper proposes a novel and efficient algorithm for optimizing superposition-structured estimators.
* The authors provide convergence guarantees for the proposed algorithm, including global convergence and asymptotic quadratic convergence rate.
* The empirical results demonstrate the effectiveness of the proposed algorithm on two real-world applications.
Arguments con acceptance:
* The assumption that the Hessian matrix is positive definite may not always be satisfied in practice.
* The requirement for the loss function to be strongly convex may not always be satisfied.
* The lack of comparison with other optimization methods may limit the scope of the paper.