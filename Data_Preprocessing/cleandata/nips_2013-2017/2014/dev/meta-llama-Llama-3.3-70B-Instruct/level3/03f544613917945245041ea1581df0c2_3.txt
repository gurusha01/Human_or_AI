This paper proposes a novel computational framework for optimizing "superposition-structured" or "dirty" statistical estimators, which are commonly used in high-dimensional problems. The authors develop a proximal Newton framework that leverages the structure of the regularizers to efficiently solve the optimization problem. The framework consists of two key components: a quadratic approximation of the loss function and an active subspace selection technique to reduce the problem size.
The paper provides a thorough theoretical analysis of the proposed framework, including convergence guarantees and asymptotic quadratic convergence rates. The authors also demonstrate the effectiveness of their approach through extensive experiments on two real-world applications: Gaussian Markov Random Fields with latent variables and multi-task learning with superposition-structured regularizers.
The strengths of the paper include its novel approach to optimizing superposition-structured estimators, its thorough theoretical analysis, and its impressive experimental results. The authors also provide a clear and well-organized presentation of their work, making it easy to follow and understand.
However, there are some potential weaknesses and areas for improvement. One concern is the assumption of strongly convex loss functions, which may not always hold in practice. The authors acknowledge this limitation and suggest that considering statistical error may reduce the need for highly accurate solutions. Additionally, the paper could benefit from more comparisons with other state-of-the-art methods and a more detailed analysis of the computational complexity of the proposed framework.
Overall, I believe that this paper is a strong contribution to the field of machine learning and optimization, and it has the potential to make a significant impact. The proposed framework is novel, well-motivated, and thoroughly analyzed, and the experimental results are impressive. With some minor revisions to address the potential weaknesses, I would recommend this paper for publication at NIPS.
Arguments pro acceptance:
* Novel and well-motivated approach to optimizing superposition-structured estimators
* Thorough theoretical analysis, including convergence guarantees and asymptotic quadratic convergence rates
* Impressive experimental results on two real-world applications
* Clear and well-organized presentation
Arguments con acceptance:
* Assumption of strongly convex loss functions may not always hold in practice
* Limited comparisons with other state-of-the-art methods
* Computational complexity of the proposed framework could be more thoroughly analyzed
Recommendation: Accept with minor revisions.