This paper presents a significant contribution to the field of non-convex optimization, particularly in the context of deep learning. The authors argue that the proliferation of saddle points, rather than local minima, is a major obstacle to rapid optimization in high-dimensional spaces. They provide a comprehensive review of the literature, drawing from statistical physics, random matrix theory, and neural network theory to support their claim.
The paper's main contributions are:
1. Theoretical justification: The authors provide a thorough theoretical analysis of the prevalence of saddle points in high-dimensional non-convex optimization problems, using results from statistical physics and random matrix theory.
2. Experimental validation: They experimentally validate their theory by measuring the statistical properties of critical points in neural network error surfaces, confirming that saddle points are indeed prevalent and that local minima with high error are exponentially rare.
3. Saddle-free Newton method: The authors propose a new optimization algorithm, the saddle-free Newton method, which can rapidly escape saddle points by rescaling gradients by the absolute value of the inverse Hessian.
The paper is well-written, and the authors provide a clear and concise explanation of their theory and methodology. The experimental results are convincing, and the proposed algorithm shows promising performance on several neural network training problems.
Strengths:
* The paper provides a comprehensive review of the literature, making it a valuable resource for researchers in the field.
* The theoretical analysis is thorough and well-supported by experimental results.
* The proposed algorithm is simple and easy to implement, making it a promising candidate for practical applications.
Weaknesses:
* The paper assumes a certain level of familiarity with statistical physics and random matrix theory, which may make it challenging for some readers to follow.
* The experimental results are limited to a few specific problems, and it is unclear how well the algorithm will perform on more complex tasks.
* The paper could benefit from a more detailed discussion of the computational complexity of the proposed algorithm and its scalability to large-scale problems.
Suggestions for improvement:
* Provide a more detailed explanation of the theoretical background, including a brief introduction to statistical physics and random matrix theory, to make the paper more accessible to a broader audience.
* Include more experimental results on a wider range of problems to demonstrate the robustness and scalability of the proposed algorithm.
* Discuss the potential applications of the saddle-free Newton method beyond deep learning, such as in other areas of machine learning or optimization.
Conclusion:
This paper presents a significant contribution to the field of non-convex optimization, providing a comprehensive theoretical analysis and experimental validation of the prevalence of saddle points in high-dimensional spaces. The proposed saddle-free Newton method shows promising performance on several neural network training problems, making it a valuable addition to the toolkit of optimization algorithms. With some minor improvements, this paper has the potential to become a seminal work in the field. 
Rating: 9/10
Recommendation: Accept, with minor revisions.