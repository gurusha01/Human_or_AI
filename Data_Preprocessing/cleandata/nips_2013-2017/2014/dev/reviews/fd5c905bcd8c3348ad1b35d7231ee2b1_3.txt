SUMMARY
This paper reports a bespoke variational scheme for inverting state-space models of spectral density implicit in LFP time-series. In particular, its contribution is to provide a bespoke variational inversion using a hidden Markov model of discrete brain states that generate activity, where the form of spectral responses provides a Gaussian process model for the time-series. I thought that this was an interesting if colloquial application of variational Bayes that may need to be contextualized within the broader church of dynamical causal modelling – and its application to neurobiological time-series.
COMMENTS TO AUTHORS
I enjoyed reading this interesting and detailed description of a variational state-space model inversion for time-series. Technically, this was an impressive piece of work. My main suggestions would be to contextualize this within the broader church of dynamic causal modelling and highlight the potential usefulness of your scheme. Perhaps you could consider the following:
1)	In the neurosciences, the variational inversion of state-space models of electrophysiological time-series is usually described in terms of dynamic causal modelling. In fact, there is a literature on the dynamic causal modelling of cross spectral density that has been applied to multi region LFP time-series (and MEG). I think it would be scholarly to look at this work. You can find an overview of dynamic causal modelling at:
http://www.scholarpedia.org/article/Dynamiccausalmodeling
Your special contribution is a state-space model that is formulated in terms of a hidden Markov model. This contrasts with usual DCMs that are based upon differential equations. You might want to highlight this because it is particularly useful for things like steep staging or endogenous transitions among different brain states. 
2)	Your description of the generative model is framed for a machine learning audience (with things like spectral mixture kernel and Gaussian processes). However, your rhetoric may confuse people in engineering and signal processing (and neurobiology). It would be useful to link your terminology to more standard concepts (perhaps in a glossary). For example, the Fourier transform of your spectral mixture is simply the auto-covariance function. Furthermore, your use of the word kernel is colloquial. In other fields, the kernel will be taken to mean the impulse response function or first order Volterra kernel whose Fourier transform is the transfer function (that corresponds to the spectral mixture). It might be useful to clarify terminology here?
3)	When you introduce the bound on model evidence or marginal likelihood, could you describe this as the variational free energy? This will enable people to see the connections between the use of variational free energy in dynamic causal modelling and in your application.
4)	To illustrate the potential usefulness of your approach perhaps you could make more of the clusters implicit in the sleep data. Perhaps with something like:
"To illustrate the potential importance of our (Bayes-optimal) states-space model inversion, we can now relate the clusters identified during sleep to classical sleep staging schemes (four distinct states). By examining the similarity between the clusters (spectral mixtures) we identified and the classic spectral profiles, we can see how stage four can be decomposed into three sub-stages……"
I am not sure how you would do this but it would be very nice if you could provide a proof of principle that your approach can take us beyond what we already know.
MINOR POINTS
1)	In the abstract, I would say: "The model is able to estimate the number of brain states…."
2)	On page 2, it is not clear which of "the above two methods" you are referring to. Can I suggest you say:
"More recently new methods for tensor factorisation have been developed: in reference 7, tensor factorisation was applied to short-term FFT…………."
This will make it clear that the tensor factorisation does not refer to the current paper. 
3)	Below Equation 6, I would say the parameters describe the auto-correlation content associated with each y". I know what you mean but you are actually characterising data in the time domain not the spectral domain. In other words, you are using spectral mixtures to provide constraints on the Gaussian process.
4)	At the top of page 8, it was not clear to me exactly what was being predicted by the results of Table 1. When you talk about a held out log predictive probability for different priors. What was this probability distribution over?
5)	Finally, I think you need to address a crucial issue in your generative model. Usually, state-space models of spectral density (or auto covariance functions) accommodate cross spectra or cross covariance functions. In other words, it is not just the spectral density at each node or region but the coupling between regions that is predicted on the basis of connectivity among regions. I think you need to make it clear that your generative model does not consider complex cross spectra (cross covariance functions) and that – in principle – you could extend the generative model in this direction.
6)	In the supplementary material, when talking about the updates for global probability vectors, you might want to mention that the use of point estimates means you do not have to consider the entropy of the posterior distribution implicit in the variational free energy (and that you can use the log posterior directly). 
I hope these comments help should any revision be required.
 This was an interesting variational scheme for state-spaces models based on a HMM and a spectral mixture model of electrophysiological time-series. It Is not very biologically plausible but may have a role in sleep staging and classification of epileptic discharges.