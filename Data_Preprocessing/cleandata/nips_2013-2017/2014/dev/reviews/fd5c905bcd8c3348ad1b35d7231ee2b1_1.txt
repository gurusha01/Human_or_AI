The manuscript describes a very interesting model for the analysis of brain states for multi-region LFP time-series. The time-series are separated in different time-windows. An infinite mixture of Gaussian Processes is considered to model the observations in each window. Brain states are assigned to each observation by means of an underlying HDP and brain regions are assigned to clusters by means of a HDP. The paper is original and overall clearly written, but the interpretation of the results needs some improvement. More specific comments are below:
a) Please clarify the algorithm used to separate the time-windows. That's very important and can affect considerably the results. 
Line 41: you should clearly distinguish between brain connectivity and brain states. 
Line 100: "each window is considered a single observation" Since you are not summarizing the observations in each window in a statistic, the statement has no clear meaning to me
Line 098 and following: There's a bit of confusion in the notation, Are the windows different in each region, as suggested by "For each region, the time-series is split...", or the windows are common across regions as suggested by the model formulation?
Line 107 What is L? How do you choose it? 
Line 113: In (1) \lambda_g^{(a)} should be explicitly written down. Is it a vector across states or animals? That becomes clear only on line 124
Line 183: Once (6) is established, the induced joint distribution over all windows is not block diagonal anymore. Worse, the joint distribution is not even well defined, since the joint covariance matrix is not semi-definite positive anymore. As a matter of fact, your infinite mixtures of Gaussian Processes is not a Process anymore, which is odd. This limitation should at least be acknowledged. 
Section 2.2 Gaussian Processes are probably not the best to describe brain connectivity in each given state. The covariance function depends on only a few parameters and cannot reproduce the pattern of sparsity of the brain (even when coupled across regions). This limitation, again, should be acknowledged. 
Section 2.3 I don't see the necessity of this section. Since you are decomposing a matrix of latent probabilities, way down in the hierarchy, the tensor characterization seems quite silly to me. 
Line 414 The statement is repeated (see line 410). In addition, the explanation is very weak. The results may depend on the choice of the clustering mechanism. It is well known that the DP shouldn't be used for cluster analysis in an absolute way (Antoniak, 1974; Miller and Harrison, 2013). Besides, you fix the parameters of the DPs (see line 333). Hence, your conclusions don't seem well supported. 
Line 418: What's the significance of this "network" with respect to the literature?  The paper is interesting, original, and overall clearly written, but the interpretation of the results needs some improvement.