The paper proposes a new regression method, namely calibrated multivariate regression (CMR), for high dimensional data analysis. Besides proposing the CMR formulation, the paper focuses on (1) using a smoothed proximal gradient method to compute CMR's optimal solutions; (2) analyzing CMR' statical properties. 
One key contribution of the paper lies in the introduction of this CMR formulation; its loss term can be interpreted as calibrating each regression task's loss term with respect to its noise level. I am wondering whether there is any more intuitive interpretation behind the use of the noise level for calibration? The authors are encouraged to explain more on this point. 
The results from Theorem 3.2 shows that CMR achieves the same rates of convergences as its non-calibrated counterpart OMR. Since OMR has a differentiable loss term, OMR seems to have computational advantages compared to CMR. The authors are encouraged to provide some guides on the selection between OMR and CMR. 
 The paper proposes a new regression method, called CMR, for high-dimensional data analysis. The papers propose to employ the smoothed proximal method to compute its solution, and also theoretically analyze its statistical properties.