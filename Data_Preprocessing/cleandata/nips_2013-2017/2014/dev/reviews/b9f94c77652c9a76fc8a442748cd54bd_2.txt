The paper presents a CRF trained by auto-encoding through latent
variables y. (a) The P(y|x) is parameterized as in traditional CRFs.
(b) The regeneration P(\hat{x}|y) is a categorical distribution
independent for each \hat{x}i and yi pair. \hat{x}_i may be a
transformation of x_i, such as Brown cluster ids.
The paper is delightfully sensible and relatively simple (in a good
way). It falls into the category of papers that makes the reader say,
"Why hasn't this been done before; I wish I had thought of it."
I like it. The experimental results are positive, and I'm inclined
towards acceptance.
There is no reason this approach couldn't be used in a semi-supervised
setting. It would be great to see some results on these lines.
Along these lines, loosely related work that combines conditional and
generative training with log-linear models (but not necessarly in a
structured output setting) includes: (1) Tom Minka. Discriminative
models, not discriminative training. MSR-TR-2005-144, 2005. (2)
Andrew McCallum, Chris Pal, Greg Druck and Xuerui Wang.
Multi-Conditional Learning: Generative/Discriminative Training for
Clustering and Classification. AAAI, 2006.
Near the top of page 5 you describe alternative methods that would
require approximations on Z in the reconstruction phase. But I don't
believe you provide empircal comparisons with those methods. Are they
beaten by the other methods you do compare against?
On a related note, reading after Equation (5), and the "Feature HMM":
What do you loose by putting a "multi-feature view" of x only on the
P(y|x) side, but not on the P(\hat{x}|y) side? It would be nice to
have some discussion of this.
Section 3.2: You say the goal is being "coherent and interpretable".
Why is this the goal? You don't evaluate interpretability?
You picked just a subset of the languages in the CoNLL-X shared task.
This seems suspicious. How did you select them? Why not show all
languages?
Especially since scalability is an advertized advantage of your
approach, I would have liked to see a graph of test accuracy as the
amount of training data increases.
The objective function for this model is certainly not convex. It
would be nice to see some discussion of local minima, and the extent
to which you see empirial evidence of problems with respect to local
minima. What initialization do you use?
Minor writing issues:
Page 2: "offers two ways locations to impose" -> "offers two ways for
locations to impose" ?
"condition on on side information" -> "condition on side information"
Page 6: "and parsing Though" -> "and parsing. Though"
 Well-written paper on a clean idea, with positive experimentalresults. Relatively simple---in a good way---such that I expect it tobe used and have impact.