This paper argues about the existence of saddle points, which becomes the main burden that first order methods (such as SGD), and some second order methods (such as methods derived from Newton method) fail to exploit. 
In particular, if a saddle point is encountered, an algorithm should exploit the direction of negative curvature to make progress: SGD methods will simply fail as they don't make much progress on directions of low curvature (as it has been argued in pervious art), while second order methods based on trust region will generally not exploit directions of (large) negative curvature, along which rapid progress could be achieved.
The authors propose a method that follows Vinyals and Povey's work, except that instead of using the Gauss Newton matrix, and regular Newton method on the Krylov subspace, they instead propose a new trust region method that generalizes to non-PSD Hessian matrices. They use the absolute value of the eigenvalues to set the trust region. This copes with saddle points in a much better way than SGD does (since it uses curvature), and much better than Newton based methods (since it can escape saddles naturally).
Some comments to the authors:
-At around line 315, perhaps clarify what you mean by k biggest eigenvectors - are they the biggest in eigenvalue absolute value? If not, how can the Krylov approximation capture negative curvature to be exploited by your algorithm?
-The improvement with SFN seems dramatic in, e.g., PTB. Could the authors report perplexities or some more standard metric on the held out set, for comparison's sake (carefully tuned SGD does very well on the PTB).
-Implementing these methods can be quite tricky. Are the authors planning on releasing code or a precise set of instructions with all the heuristics used?
-From a practical stand-point, how do things like regularization (e.g. dropout) affect saddle points? 
-What was the network architecture, and how would the authors deal with ReLu nets? Can we say something about those in terms of second order behavior? Good paper, tackling an important and general problem in machine learning and deep neural nets - optimization. This paper argues about saddle points being a major cause for first order methods (e.g. SGD) to suffer for slowness and stuckness. Enough evidence and fairly in depth analysis is given on several machine learning tasks that supports the main argument of the paper.