The authors present a recurrent spiking neural network model for sampling from multivariate probability distributions through MCMC. A key advantage of their approach is to separate computation and representation: Samples of the target distribution are obtained from the network response via a linear decoder, i.e. neurons do not need to directly correspond to random variables. This gives rise to a rich distributed spatio-temporal code to represent the underlying distribution. Furthermore, the resulting flexibility in the network architecture allows to run multiple MCMC chains in parallel and, thus, to approximate the target distribution near-instantaneously.
The theory is complemented by computer simulations that (a) demonstrate the general feasibility of the approach, (b) illustrate how the network performs inference (in a simple generative model) and (c) compare response characteristics with findings from experimental neuroscience. Finally, the authors explore how the approach can be used to identify (properties of) the underlying distribution when the linear decoder is not known; an important step for an application to experimental data.
Overall, this is a good manuscript. It contributes several new ideas which are of interest to a broader research community. The manuscript is mostly very well written (particularly the introduction and discussion), the computer simulations and figures illustrate the network properties well. Finally, the work perfectly fits the scope of the conference.
Nonetheless, I have some critical remarks which reduced the otherwise positive impression.
(A) The manuscript suffers from missing or extra words which, in some cases, even affect the content. E.g. in lines 46, 47, 142, 153, 188, 223 and 386.
(B) The spike trains o_i are not defined in line 80. The reset mechanism (described in [6]) is not mentioned around line 140.
(C) The derivation is highly compressed and definitely demands reading of refs [6-8]. Also the SI contains relevant information (e.g. the generative model definition). Furthermore, I didn't find important parameters of the simulations (e.g. number of neurons and decoding matrix in Fig 1, lambda, tau_slow,...). This renders a verification of the results almost impossible. Some details to the simulations should be added to the SI.
(D) While the network supports a high degree of freedom in the recurrent weight matrices, connections are still symmetric (at least for the Gaussian case). Such limitations should be mentioned at some point.
(E) I was wondering if all equations are fully correct. I didn't delve into the details, so the following are just some points the authors might want the check. In line 141, shouldn't the summation run over the first index of the decoder? The sparseness parameter lambda changes the spike response but does not enter any other equation to account for this effect. Is this correct? Similarly, the time constant tauslow that scales the recurrent weights and drift is not further specified. Can it be chosen arbitrarily, or is this tauv?
In summary, I had the impression that a bit too many different research questions were put into this nine pages manuscript. This comes at the cost of reduced clarity of the individual parts. For instance, since the derivation rests upon some approximations, it would be helpful to explore the range of validity. Also the intriguing multi-chain sampler would have deserved a more comprehensive presentation. Still, I consider this manuscript an important and valuable contribution to the conference if the authors address some of the above issues, and I hope that (one or more) follow-up papers can investigate the presented ideas in greater detail.
 This is a good manuscript that introduces some highly interesting ideas. It suffers from some deficits in the presentation; but I am optimistic that they can be fixed in a final version.