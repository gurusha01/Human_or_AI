Summary:
This paper argues that saddle points, and not local minima, are the primary difficulty in gradient- or Netwon's method-based optimization of high dimensional non convex functions such as deep neural networks. The paper reviews related theoretical results, empirically demonstrates the problem in deep neural networks doing visual object recognition, and proposes a solution which is shown to be effective for training deep neural networks.
Main comments:
This is an original paper that could significantly reshape intuitions about learning in deep networks (and about other non convex optimization problems). The review of relevant theoretical work focuses on results that are not widely known in the deep learning community, and a useful contribution of the paper is the empirical verification on MNIST and CIFAR of the relationship between training error and index, showing that the result was not dependent on the simplifying assumptions used in the theory.
Is there a justification for only using the k biggest eigenvectors of the Hessian? It seems like the Krylov method is going to ignore directions with low or negative curvature that could be seen near saddle points. In particular, it seems possible that the Hessian could look fully positive definite in the krylov subspace when in fact there are other descent directions with negative eigenvalues. Consider the classical saddle from Fig 2a. My understanding is, if the Krylov subspace had dimension 1 it would point in the high positive curvature parabola even though the negative curvature direction exists. Hence the specific approximate SFN method proposed here seems like it could also be trapped by saddle points for which there are more positive, large eigenvalues than the Krylov subspace dimension. 
It could be interesting to visualize the weights at the end of the MSGD epoch compared to a few iterations after SFN takes over. Is there a clear pattern to the changes, such as an unused unit with very small norm weights that becomes active? There may not be but if there is, it would be interesting to gain intuition: In practice is this mainly overcoming scaling symmetries across layers? Permutation symmetries among hidden units? Etc.
The paper does not particularly emphasize its result of lower error for the deep auto encoder compared to Hessian free optimization, but it should be noted that the Hessian free result did not use carefully tuned MSGD to initialize its behavior. This could have impacted the comparison.
The paper is clear and well written.
Minor:
It would be helpful to put a vertical line on Fig 4b where the switch to SFN occurs.
 The paper convincingly argues that saddle points, not local minima, represent a key difficulty for current learning algorithms applied to non-convex models like deep neural networks. The paper highlights an algorithm able to more rapidly escape saddle points.