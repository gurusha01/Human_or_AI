The authors propose to use a Gaussian Process (GP) dynamics model learned from data together with a DDP-like trajectory optimization algorithm operating on the mean and covariance of the state distribution. The proposed method is compared to PILCO and an ablated variant (using just the mean as the state), and achieves slightly worse results but at a very large improvement in computation time.
It appears that, besides learning a GP model of the dynamics, the proposed method is simply performing Gaussian belief space planning. This connection is not mentioned by the authors, but seems pretty clear. For example, the paper "Motion planning under uncertainty using iterative local optimization in belief space" describes a method that, except for the GP learning phase, appears extremely similar (there are numerous other methods in this general "family" of belief space planning techniques). The novelty of the approach therefore would seem to be quite low, as the method is simply taking belief space planning and combining it with learned GP models. Since belief space planning algorithms are typically agnostic to the form of the simulator used, a GP-based dynamics model would be straightforward to combine with any previous belief space planning algorithm, yielding an extremely similar approach.
There are several other issues with this work. Among previous methods, the authors only compare to PILCO. The body of belief space planning work is completely ignored, though perhaps this is because such work often does not consider learning the model (since any model learning technique can easily be used in conjunction with any belief space planner). However, there is also other work on model-based RL to compare to. Since the authors are claiming computation time as a major advantage of their method, the choice of PILCO doesn't really make sense, since PILCO has exceedingly extravagant computational requirements that are far outside the "norm" for model-based RL. Without a comparison to any other method, it's not clear whether the improvement in the tradeoff between computation and sample complexity is actually notable.
The title of the paper is also a bit too ambitious. There are plenty of papers that discuss the "probabilistic" aspects of DDP, including a paper entitled "Stochastic DDP," papers that deal with noise and uncertainty (including state and action dependent noise, etc), and of course the previously mentioned field of belief space planning. Something like Gaussian Process DDP would have been more appropriate, but since the primary contribution of the paper appears to be combining belief space planning with learned GP models, something about "GP models for belief space planning" might also make sense.
-----
Regarding the rebuttal: I'm not entirely clear on the distinction that the authors are drawing between their approach and belief-space planning. In belief-space planning, the underlying true dynamics is sometimes assumed to be Gaussian (not the same as deterministic, but it's true that E[f(x)] = f(E[x])). But the resulting mean and covariance propagation equations look similar to the ones presented in this paper: stochastic transitions in terms of the state are turned into deterministic transitions in terms of the state mean and covariance. If there is some distinction between them, it should be demonstrated experimentally, as it is not at all obvious from the description whether one method is better or worse.
I do appreciate the clarification with PI^2, though I'm not sure if this algorithm is the best candidate for a comparison, since requiring 2500 iterations to solve pendulum swingup seems quite excessive (see for ex NP-ALP, Reinforcement Learning In Continuous Time and Space Doya '00, and other papers that compare on the pendulum benchmark). Another model-based method would be a better candidate here I think, since you are comparing model-based methods with a particular kind of inductive bias (smooth dynamics functions). The proposed method appears to be DDP-based belief space planning combined with a learned GP dynamics model. In light of prior work, this doesn't seem particularly novel. The experimental results also fail to convincingly demonstrate the advantage of the proposed method in terms of the computation cost-sample complexity tradeoff, since the only prior method compared to (PILCO) has unusually high computational cost compared to most model-based RL methods.