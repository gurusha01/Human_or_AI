This paper presents an efficient way of doing unsupervised structured prediction by using a CRF as the first layer of an autoencoder. The paper is very clearly written, the idea is of the "I wish I had thought of this" kind, and the experimental results in POS tagging and MT alignment are solid and well-discussed. The relationship to previous work is presented fairly with one notable exception: the paper of Suzuki and Isosaki on semisup CRF training at ACL 2008. If you squint a bit, their proposal is actually rather similar to yours. In my own words (their paper is quite a bit harder to read than yours), what Suzuki and Isosaki do is to jointly train a CRF p(y|x) and a generative model p(x'|y) with a loss function that bounds a combination of the error on labeled data and the disagreement between the two models on unlabeled data. Their general approach is way more complicated than yours, and your presentation is leaner and more effective, but I feel that you need to give a careful account of the connection between the two approaches (and take care to look at more recent related work of theirs as well).
You claim that neural autoencoders cannot learn latent sequential structure without labeled data. That feels like an argument from ignorance. Maybe we/some of us don't know how to do that, but RNNs and LSTMs have at least the potential for doing it. I'd suggest a more measured comparison between these differen types of autoencoders.
 A clearly written, convincing presentation of a simple but effective idea for unsupervised learning of structured predictors. I like this paper a lot, but the connection to some earlier work not cited needs to be sorted out.