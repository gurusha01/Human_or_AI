Spectral methods have recently been proposed as an alternative and efficient framework for inference based on characterizing moments in order to identify model parameters. In this paper the authors extend this framework to the Indian Buffet Process (IBP) as well as two models based on the IBP namely the linear Gaussian Latent Factor Model and the Infinite Sparse Factor Analysis model based on Laplace and Gaussian priors forming respectively a sparse PCA and ICA type of decomposition. Multilinearity of higher order moments are explored and non-identifiability issues in the third order moment is handled by also analyzing the structure of the fourth order moment. An algorithm is proposed for spectral inference and the framework demonstrated to be more efficient than Variational and MCMC inference. Extensive derivations of moments and proofs of bounds are provided in the accompanying appendix. 
The paper builds on the existing ideas of spectral approaches (ref. 5-8) and presently extend these to the Indian Buffet Process. This extension including proofs of bounds is non-trivial and the very condensed paper provides a very technical and detailed mathematical analysis which may warrants its publication. From an application point of view the method appears both to be simple to implement and to work well in practice which may make the framework be adapted by other researchers giving the paper a substantial impact.
While the experiments indicate that the method is more efficient than a variational approach it would improve the paper to analyze how the approach compares in accuracy to a full MCMC procedure. Comparison is provided to MCMC for one synthetic example and it is shown that in less computational time better reconstruction is achieved. However, how would the method compare to the MCMC procedure run for much longer? The results for the real gene expression data also appear to be meaningful but again how do these results compare to Variational and MCMC inference? – and do these existing alternative approaches also identify in the order of 10 (similar) components? Clearly, the spectral approach proposed seems to be accurate and very efficient but it would be interesting to see in practice how accurate the results are compared to more exhaustive MCMC procedures. 
In order to determine the dimensionality K in algorithm 1 a truncation to eigenvalues larger than epsilon is applied in order to determine the rank of S_2. As determination of K is very central to non-parametric approaches it would improve the paper to further discuss the influence of truncation for determining the order K and how epsilon/the truncation was chosen. In general the spectral approaches admit to quantify model order by truncating eigenvalue decompositions as also remarked by the authors. Thus it would improve the paper to elaborate on how easily this truncation level can be defined in practice. It is mentioned that K can be determined by the largest slope of the eigenvalue spectrum – how reliable can this largest slope be identified in general?
The orthogonal tensor decomposition requires a number of random initializations to be reliably estimated. How many random initializations are needed in practice and how severely do local optima impact the results? (It is unclear in the paper how critical this issue is.)
Minor comments:
Synthetic dataset figures to the right: should 0101 not be 0110 and 1001 be 1010? Otherwise, please explain how the binary notation is used to reflect the symbols learned.
The notation is not always clear in terms of what are scalars and vectors. I.e., \pi is a vector but when writing \pi^2 I believe element wise exponentiation is used. Some places C_x appears as vectors but other places they are treated as scalars. Please clarify this notation to make the paper more assessable. Also diag is used both some places to form a diagonal matrix but also to form an order fourth diagonal tensor.
Minor comments to the extensive appendix:
Line 511: x^T A -> v^T A
Equation 32 is top -> \top
Equation 36 please clarify how Ex becomes only Ez in line 36 and then in line 37 Ez, Ey. 
Line 669: In this section, we provides bounds for moments of linear gaussian latent feature model -> In this section, we provide bounds for moments of the linear gaussian latent feature model
Line 968: Before starting the put everything together -> Before starting to put everything together
Line 1126: in order to – do you here mean: in order for / in order to assure?
Please explain the Poly(.) notation.
Line 1126: Lamma 11 -> Lemma 11
 The authors extend the existing spectral approaches to the Indian Buffet Process including the Linear Gaussian Latent Feature Model and Infinite Sparse Factor Analysis providing what appears to be a useful and very efficient inference framework. Considering non-parametric models it is unclear how well the model orders in general can be determined.