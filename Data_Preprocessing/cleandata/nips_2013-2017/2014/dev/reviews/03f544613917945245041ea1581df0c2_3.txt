The authors extended the results in [11] and [12] to more general settings. The proposed computational framework works for
(a) Strongly convex and twice differentiable Loss functions.
(b) Decomposable regularization functions.
The theoretical analysis sounds reasonable, and the numerical results are also convincing. (b) is natural to sparse learning problems, but (a) seems a little restrictive, since many sparse learning problems are formulated as nonstrongly convex problems.
Here are my two concerns:
(1) Eq.(16) seems strong. Could the authors provide some examples?
(2) Block coordinate descent algorithms are also competitive for solving the optimization problem in Section 5.2. Could the authors provide some detailed comparison?
Response to the rebuttal: If we also consider the statistical error, then we usually do not need a "too accurate" solution. To more comprehensively comparing the proposed algorithm with the first-order block coordinate descent algorithms, the authors may need to conduct more experiments, e.g., timing v.s. classification error in multi-task learning. My conjecture is that the local quadratic convergence might not be so competitive in term of the statistical error. But I agreed that it would not hurt to get a "very accurate" solution if the computational resource is affordable.
 Overall, I am positive for the quality of this paper. I would like to see this paper appearing in NIPS.