In this paper, the authors propose an interesting idea for learning invariances, in the unsupervised learning setting for image classification problems. The key idea is to create surrogate classes consisting of groups of transformations of randomly selected patches. The transformations are 2D functions such as rotations, scaling, translations and color shifts. The formal analysis is interesting and sheds further light on why this approach could work.
The paper is well written and easy to follow. The experiments cover a number of aspects of the problem, including number of transformations and size of network. 
One drawback of the approach in the paper seems to be that two randomly selected patches which are very similar in content will be "forced apart" since they will be considered different surrogate classes. The authors implicitly sidestep this by using only a maximum of 32000 patches from large datasets. However, using very few surrogate patches may impede generalization in case of larger datasets. Do the authors have any comments on this? 
Another experiment which would be nice is to have used the same set of surrogate classes for unsupervised learning and then training dataset-specific classifiers. For example, since STL-10 and CIFAR-10 are closely related, why did the authors need to retrain on each dataset? 
Finally, experiments on varying patch size would also have been interesting. 
 In this paper, the authors propose an interesting idea for learning invariances, in the unsupervised learning setting for image classification problems. The key idea is to create surrogate classes consisting of groups of transformations of randomly selected patches. The paper's idea is novel and interesting for unsupervised learning. It is well written and easy to follow.