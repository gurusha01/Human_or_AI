This paper introduces a new optimization method called SAGA, which improves upon the theory behind SAG and SVRG, two recently proposed incremental gradient algorithms with fast linear convergence rates. The authors claim that SAGA has better theoretical convergence rates and supports composite objectives where a proximal operator is used on the regularizer. The paper provides a detailed analysis of the relationship between SAGA and other fast incremental gradient methods, including SAG, SVRG, Finito, MISO, and SDCA.
The strengths of the paper include its clear and well-organized presentation, its thorough analysis of the related work, and its provision of experimental results to validate the effectiveness of SAGA. The authors also provide a novel transformation of SDCA into an equivalent method that only works with primal quantities, which is a significant contribution to the field.
However, there are some weaknesses to the paper. The authors assume that the initial gradients are known for each fi at the starting point x0, which may not always be the case in practice. The paper also does not provide a detailed analysis of the computational complexity of SAGA, which is an important consideration in many applications.
In terms of the criteria for evaluation, the paper scores well on quality, as the claims are well-supported by theoretical analysis and experimental results. The paper is also well-written and easy to follow, which scores well on clarity. The originality of the paper is also high, as it introduces a new optimization method and provides a novel analysis of the related work. The significance of the paper is also high, as it has the potential to improve the state of the art in optimization methods for machine learning.
Here is a list of arguments pro and con acceptance:
Pro:
* The paper introduces a new optimization method with better theoretical convergence rates
* The paper provides a thorough analysis of the related work
* The paper includes experimental results to validate the effectiveness of SAGA
* The paper is well-written and easy to follow
Con:
* The authors assume that the initial gradients are known for each fi at the starting point x0
* The paper does not provide a detailed analysis of the computational complexity of SAGA
* The paper may not be suitable for all applications, as it requires a certain level of strong convexity
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of optimization methods for machine learning. However, the authors should be encouraged to address the weaknesses of the paper, such as providing a more detailed analysis of the computational complexity of SAGA and considering more general cases where the initial gradients are not known.