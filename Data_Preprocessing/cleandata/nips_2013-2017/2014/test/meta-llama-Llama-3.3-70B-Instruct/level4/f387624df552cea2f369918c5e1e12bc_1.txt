The paper examines the problem of best-arm identification in the context of linear bandits, where an unknown parameter $\theta^ \in R^d$ and a finite set of arms $X \subseteq R^d$ are given. Upon pulling an arm, the observed reward is $x^T\theta^ + \epsilon$, with $\epsilon$ being zero-mean i.i.d noise of bounded range. The objective is to identify $\argmax_{x \in X} x^T\theta^*$ using the minimum number of samples.
The authors provide a characterization of the sample complexity for both static and dynamic allocation strategies aimed at identifying the best arm. 
The technical approach presented in the paper appears to be sound. However, to further strengthen the experiments section, it would be beneficial to include a comparison with existing algorithms for linear bandits. This could involve running existing linear bandit algorithms until a certain condition, such as $\hat{S} \subseteq C(x)$, is met and then evaluating the number of steps required to achieve this. Such a comparison is crucial because, as the paper itself notes, strategies for minimizing sample complexity in best-arm identification for linear bandits may differ from those for regret minimization, unlike in the multi-armed bandit (MAB) scenario.
The paper is generally well-written, contributing to its clarity. 
The significance of the paper lies in its proposal and solution of a fairly general problem. Although the paper could improve in highlighting specific applications, it tackles an abstract problem that likely has significant practical implications. Overall, the study of best-arm identification in linear bandits and the characterization of sample complexity for static and dynamic strategies are noteworthy contributions.