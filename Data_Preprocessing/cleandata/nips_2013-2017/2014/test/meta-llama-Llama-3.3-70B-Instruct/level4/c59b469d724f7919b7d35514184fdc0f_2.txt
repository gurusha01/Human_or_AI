COMMENTS AFTER AUTHOR FEEDBACK
This paper is of high quality and presents a significant contribution to the field.
One minor comment that was not addressed in the rebuttal is the unclear definition of neighborhood due to the use of "jointly". Specifically, Line 178 seems to contradict Lemma 8's proof (Line 576), which may be worth clarifying in the paper.
--------------------------------------------------
Disclaimer: As a reviewer with expertise in Collaborative Filtering rather than Learning Theory, I will rely on the expertise of Learning Theorists in assessing the paper's theoretical significance.
MAIN IDEAS OF THE PAPER
The paper provides a theoretical framework for understanding the performance of collaborative filtering algorithms, which are commonly used in practice to recommend items to users based on their past behavior. The authors analyze a simple collaborative filtering algorithm that relies on finding neighboring users with similar like/dislike patterns and using their extra votes to score items. The goal is to maximize the expected number of liked items presented to each user within a fixed time window.
The paper shows that users can be "learned" after approximately log(km) = log(number of user types) + log(number of items) rounds of recommendations, where km represents the number of user types and items. This result relies on two additional steps in the collaborative filtering algorithm: random exploration and structured exploration. Both steps are necessary to interrogate potential future items and to learn about the similarity between users.
The experimental results simulate a real-world scenario using a curated subset of the Netflix prize data and demonstrate that the proposed algorithm outperforms two existing methods, PAF and DM.
RELATIONSHIP TO PREVIOUS WORK
The paper distinguishes itself from previous work by using two exploration steps, one of which learns the similarity between users. This approach sets it apart from standard multi-arm bandit algorithms, where no arm can be pulled more than once.
QUALITY AND CLARITY
The paper is well-written and clearly explained. I have worked through most of the proofs in the supplementary material and found them to be technically correct. However, I would like to bring the following textual issues to the authors' attention:
1. The definition of neighborhood is unclear due to the use of "jointly". Line 178 suggests that only user-item indexes recommended in the joint exploration step are eligible for defining neighbors, but this contradicts Lemma 8's proof (Line 576).
2. The paper emphasizes the importance of not recommending consumed items to users again, which is a well-known principle in the recommendations community.
3. There are a few typos, including Line 165, which should refer to \tilde{p}_{ui} instead of index j, and Line 524, which contains an unnecessary inequality.
ORIGINALITY AND SIGNIFICANCE
This paper is the first attempt to characterize learning rates in a realistic recommendations setting. The main takeaway is that joint exploration is required to achieve optimality in time logarithmic in the number of items.
PRACTICAL WEAKNESSES TO BE AWARE OF
While the paper presents a significant theoretical contribution, there are some practical weaknesses to be aware of. The assumptions underlying the proofs and algorithm, such as the distribution of user types and the tolerance for errors, may not hold in practice. Additionally, the random and joint exploration steps may hurt user retention in real-world systems. These limitations should be considered when applying the results in practice.
Overall, this is a well-written paper that presents a significant contribution to the field of collaborative filtering. The results provide valuable insights into the importance of structured exploration and the potential for achieving optimal performance in time logarithmic in the number of items.