This significant paper proposes an incremental gradient method, termed SAGA, designed for minimizing the sum of a function $f$, represented as the average of $n$ functions $fi$, and a potentially non-smooth regularizer $h$, for which a proximal operator is known. The method exhibits provable linear convergence rates when $f$ (or each $fi$) is strongly convex, and it achieves a $1/k$ rate in the absence of strong convexity, thereby adapting to the level of strong convexity present in the problem and seamlessly interpolating between these rates.
A key contribution of the paper lies in its elucidation of the connections between SAGA and a broad range of existing algorithms aimed at solving similar problems, including SAG, SDCA, MISO, and SVRG. While the relationship between SAGA and SAG is intuitive, given their naming convention, the paper's exploration of the links to MISO, SVRG, and particularly SDCA is noteworthy and provides valuable insights.
One of the paper's most compelling aspects is its ability to provide a clear, verbal explanation for the method's efficacy and its relationship to other algorithms. The use of a biased gradient, contrasting with SAG's approach, results in reduced variance, which is an interesting strategy. Furthermore, the comparison with SVRG, which trades computational efficiency for memory usage by less frequent updates of the $\phi$ vectors, and the less direct but intriguing connection to MISO/Finito, offer additional insights. The reinterpretation of the SDCA algorithm in the primal domain may also be of independent interest, and aligning it with the form of MISO is a significant contribution.
The core contribution of the paper is found in its concise and lucid proofs of convergence for both the strongly convex and non-strongly convex cases, presented in the appendix. These proofs not only contribute to the understanding of SAGA and related algorithms but will also be instrumental in the design of new methods. The algorithm's adaptability to the presence or absence of strong convexity makes it particularly useful in practical scenarios where the strong convexity constant is unknown.
Given the prevalence and importance of problems involving empirical risk minimization (ERM) with regularizers, with or without strong convexity, in the machine learning community, and considering the comprehensive comparison with existing methods, this paper is likely to be of considerable interest to a broad audience. Both optimization specialists, who will appreciate the theoretical contributions, and practitioners, who will value its practical applicability, will find this work significant.
Minor corrections are suggested: the symbol "s" in several contexts, such as the description of MISO and the proof of Lemma 1, appears to be a typo and should be replaced with "mu". Additionally, in the appendix, "manor" should be corrected to "manner". This paper presents a novel algorithm for minimizing finite sums with regularizers, accompanied by straightforward proofs of convergence and a thorough comparison with existing literature, making it an important contribution that will appeal to both theoreticians and practitioners. The authors are commended for their effort in contextualizing SAGA within the existing algorithmic landscape and for the clarity of their proofs. Therefore, this paper is highly recommended for acceptance.