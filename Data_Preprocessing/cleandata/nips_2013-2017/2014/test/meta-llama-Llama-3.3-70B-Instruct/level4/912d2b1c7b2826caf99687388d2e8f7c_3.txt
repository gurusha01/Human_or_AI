I have examined the authors' response, which has adequately addressed my primary concerns, and I will elaborate on some of these points below. Upon reviewing the other evaluations, my assessment of the paper remains unchanged; I believe it is a robust paper that warrants acceptance. However, I do not think that its theoretical or experimental contributions are substantial enough to merit consideration for an oral or spotlight presentation.
This paper tackles the problem of identifying structured low-rank matrices when the structure is defined by a linear map and nuclear norm regularization is applied. The authors propose a revised formulation of the problem, making it suitable for the application of the generalized conditional gradient method. They present an algorithm that incorporates additional techniques to accelerate convergence and demonstrate its convergence. Empirical studies indicate that the algorithm significantly outperforms its competitors and can handle larger problems.
The paper's strengths include:
(1) It addresses a fundamental optimization problem in machine learning and signal processing, and the authors provide a novel approach that can find a suitable solution much faster than existing methods. The empirical studies effectively demonstrate the approach's efficacy in two practical application problems.
The weaknesses of the paper are:
(1) The authors achieve a significant reduction in computational cost by modifying the objective function, which no longer directly penalizes the rank of the structured matrix. The authors present three reasonable justifications for this modification shortly after introducing the objective function in equation (7). However, the paper does not explore alternative options, such as satisfying the linear constraint asymptotically through a homotopy scheme or exactly through a projection, either empirically or theoretically. The theoretical result primarily focuses on the algorithm's convergence in terms of minimizing the modified objective function, without providing insight into the discrepancy between the identified solution and the original formulation. Figures 2(c) and 2(f) exhibit unusual behavior in the rank of the solution as it converges, which is not discussed and raises questions about when the algorithm should be terminated to obtain an appropriate solution.
The authors have argued that any formulation achieving this goal should be sufficient for the application, which I consider a reasonable argument. Nevertheless, the paper currently presents equation (7) as a relaxation or approximation of equation (5), implying that it aims to find the same solution. I think it would be beneficial to add a sentence or two to clarify that equation (7) is an alternative utility function targeting the same overall objective of a low-rank model. Additionally, it would be helpful to include extra discussion in the main paper or supplementary material to expand on the authors' response regarding the experimental results and the algorithm's behavior.
I have some comments on the paper: Theorem 1 cites [28], but it appears that the theorem is not directly taken from [28], despite similarities in the proof approach. What is the significance of the citation [28]?
The authors have acknowledged that the citation should appear in the proof, following a statement that the proof technique is inspired by the method in [28]. The paper addresses a core optimization problem in machine learning and signal processing, providing a novel procedure that can identify a suitable solution much faster than state-of-the-art approaches. This is achieved by modifying the objective function, and the paper would benefit from a more in-depth discussion and analysis of the impact of this modification.