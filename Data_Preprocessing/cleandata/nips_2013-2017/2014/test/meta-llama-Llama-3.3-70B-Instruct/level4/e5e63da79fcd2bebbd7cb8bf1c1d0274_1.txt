This manuscript presents a key discovery that integrating an identity classification task with a metric-learning-style verification task enhances the learning of features for face classification and verification. The verification task aims to minimize the feature-space distance between instances of the same identity while maximizing the distance between those of different identities. This improvement is incorporated into a state-of-the-art face verification system, which utilizes convolutional neural networks trained on numerous (400) views to generate features, subsequently distilled into a compact set of 25 using feature selection. The results obtained are impressive, with experiments conducted using the LFW dataset as a test set.
Overall, the results are outstanding, achieved through a moderately complex pipeline, and provide a thorough investigation into the contribution of each task in the loss function for feature learning. Although the concept of combining tasks has been previously hinted at (e.g., siamese networks after classification pre-training for DeepFace), this work explicitly explores this idea. Some sections of the writing could be more engaging and edited for clarity, but this does not impede the understanding of the paper.
Further comments:
* The selection process for "m" in equation 1 is unclear. If "m" is learned as a parameter, it appears that it would converge to m=0 or the minimum ||fi-fj|| (even with the procedure outlined at line 152), as this would always yield zero verification error for negative pairs.
* A more specific explanation for how the inclusion of the identification task enhances verification performance and/or learning would be beneficial. For instance, the gradient from the verification task separates two points, whereas the softmax gradient in the identification task pushes the correct identity away from the classification-layer templates for all other identities simultaneously.
* It would be informative to see the individual performance of the top few selected network regions/views or an evaluation using 1, 2, 4, ..., 25 views. Although the final combination achieves good performance, understanding the individual contributions would be valuable.
* At line 38, the term "eternal topic" is an unusual choice (implying the topic will be debated indefinitely, when in fact progress is being made); "central topic" might be a more suitable alternative.
This manuscript describes a well-established state-of-the-art system and explicitly examines the effect of using each of the two tasks during feature learning. While the contributions may seem somewhat incremental, combining existing ideas, this is not viewed as a significant drawback.