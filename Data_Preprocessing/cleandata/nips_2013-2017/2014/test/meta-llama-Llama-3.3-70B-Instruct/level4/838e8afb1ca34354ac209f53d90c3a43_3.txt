Summary: The authors introduce "tag structure regularization", a novel regularization technique that penalizes a structured prediction model during training based on losses on sub-structures of the training example, forcing the model to make independent predictions for each sub-structure. They provide an analysis based on stability arguments, demonstrating that the regularization strength decreases bounds on generalization error and increases the convergence rate of SGD. The authors showcase state-of-the-art accuracies on several sequential prediction tasks by varying the regularization strength, while also accelerating training.
Major comments: 
+ The central idea of this paper is intriguing, as it suggests that breaking up training samples into sub-samples during learning can enhance the generalization of structured models. This approach intuitively makes sense, as it regularizes the model to rely more on local information than on incoming messages from the rest of the graph, potentially preventing error propagation and resulting in a simpler model. Alternatively, the "tag structure regularization" can be seen as preferring simpler models that can make accurate predictions without relying on message passing.
- However, the theoretical analysis in this paper appears to be flawed, as it relies on incorrect assumptions or takes an inappropriate approach. Although increasing alpha does reduce the bound, the tightest setting of alpha = n yields a bound that is still significantly looser than other structured prediction generalization bounds. For instance, the original MMMN paper (Taskar et al.) achieved a logarithmic complexity in both the multi-class label size and the number of variables. More recently, London et al. employed PAC Bayes analysis to demonstrate that for templated models, increasing the example size actually decreases generalization error. This contradicts the use of n as a measure of structure complexity, particularly since feature templates are commonly used in applications, including those in this paper. While the analysis may be technically correct, the authors must reconcile their work with previous research to make their paper coherent.
- The term n^4 in the generalization bound seems excessively large, and upon examining the supplemental material, it appears to stem from the linear decomposition of loss in Lemma 6, as well as the multiplication of norms of two examples and the linear decomposition of the regularization term. This suggests that the current approach may not be suitable.
- Previous work has utilized a more subtle measure of graph complexity in their bounds, based on concentration inequalities, which measures the maximum dependence of one variable on others in the graph. A more effective approach to analysis might involve relating the novel regularization to the resulting complexity of the learned model in terms of a complexity measure like this, assuming some form of templating, rather than simply equating complexity to n.
+ The concept of tag structure regularization bears some resemblance to dropout, where new examples are generated by removing edges in the graph, effectively creating disconnected components in the case of sequences. This similarity highlights the potential of this paper to shed light on other ideas in the field.
+ Despite the theoretical limitations, the experimental results are strong, and the idea is simple enough to be easily experimented with and verified. While the analysis may be flawed, the concept is interesting, and the experiments are compelling, making it a valuable contribution to the community.