This manuscript explores an intriguing notion that has been previously proposed and explored by other researchers, as appropriately acknowledged in the paper: can an increase in data lead to a reduction in the running time required to achieve a given risk? This question is undoubtedly significant, and although some progress has been made, it remains largely uninvestigated.
The paper focuses on the problem of sparse regression, specifically in the noiseless setting of the Regularized Linear Inverse Problem. The core idea is to apply heavier regularization with increasing amounts of data. It has been observed and rigorously proven that regularization can enhance the convergence rate and, more notably, may not substantially alter the solution. Relevant references, such as the works by Wotao Yin and Ming-Jun Lai, demonstrate this concept, reaching a similar conclusion to that of the present paper. These studies show that if the sensing matrix A satisfies certain properties (e.g., improved Restricted Isometry Property (RIP)), regularization can be applied without altering the solution significantly, even without diminishing the regularizer to zero. With more data, it can be readily shown that the sensing matrix A, assuming it originates from a suitable random ensemble, will satisfy RIP with a stronger constant. As the regularizer controls the strong convexity parameter, this accelerates convergence, guaranteeing a global linear rate. Another relevant paper by Agarwal, Negahban, and Wainwright presents a similar idea without relying on RIP, demonstrating that gradient methods exhibit global geometric (linear) convergence due to restricted strong convexity and restricted smoothness. The key connection to the present paper lies in the fact that convergence time explicitly depends on the RSC and RSM parameters, which can be shown to improve with increasing data.
The algorithm employed in this paper is a dual smoothing algorithm, leveraging strong convexity to convert the dual solution to the corresponding unique primal solution.
However, the organization and writing of the paper could be improved for better clarity. Simple calculations for a setting where A originates from Gaussian design would enhance the delivery of the results. Computing Î¼(m) in this context should be feasible and would help illustrate the concept. It would be beneficial to have calculations analogous to Fact 2.3, not only for exact recovery but also to determine the maximum regularizer value that still guarantees exact recovery. The current connection between regularization and Fact 2.3 is not entirely clear and should be, as it is a crucial aspect of the paper.
Overall, the paper pursues a promising direction, building upon established groundwork, but the details could be more transparent. While the authors are advancing in this direction, it is unclear whether the details are fully clarified in this manuscript.