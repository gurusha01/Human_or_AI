This paper presents a novel incremental gradient algorithm, drawing inspiration from several recently proposed related methods, with modifications to the gradient update rule. 
From my understanding, the fundamental principle of incremental gradient algorithms is to minimize the variance of the proximal gradient, thereby achieving a better convergence rate. In this context, the proposed work may not be particularly compelling, as the convergence rate does not exhibit significant improvement and the underlying idea is not exceptionally innovative. This assessment is further supported by the experimental results, which demonstrate comparable convergence speeds with existing methods, without consistently outperforming them.
Additionally, there appear to be minor errors in the proof, such as the potential omission of a scalar eta in the equation on line 324. 
Nevertheless, the paper is well-rounded, featuring straightforward theoretical analysis, and the author provides a clear exposition of the relationship between SAGA and other related works. Although the work is comprehensive and theoretically sound, its primary contribution may not be sufficiently impactful, as the simplicity of the theory analysis is not matched by a significant improvement in convergence rate or novelty of the idea.