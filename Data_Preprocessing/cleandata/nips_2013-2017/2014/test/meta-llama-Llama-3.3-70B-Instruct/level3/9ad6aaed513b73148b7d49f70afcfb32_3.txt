This paper introduces a new optimization method called SAGA, which is an incremental gradient algorithm with fast linear convergence rates. The authors provide a theoretically sound algorithm and experiments on various datasets to demonstrate its effectiveness. The paper is well-organized, and the authors do a good job of relating their work to previous research in the field.
The strengths of the paper include its clear and concise writing style, making it easy to follow and understand the authors' contributions. The authors also provide a thorough analysis of the relationship between SAGA and other fast incremental gradient methods, such as SAG, SVRG, and SDCA. The experimental results demonstrate the effectiveness of SAGA in practice, and the authors provide a detailed discussion of the trade-offs between different methods.
However, there are some weaknesses in the paper. The authors claim that SAGA outperforms existing techniques, but the results show that it takes longer than the L-BFGS approach, which needs clarification. Additionally, the energy versus sample index curve in Figure 2 shows inconsistent patterns that require explanation from the authors. The authors should also provide clearer explanations for the tables in their captions for better understanding.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is also well-organized, making it easy to follow and understand the authors' contributions.
In terms of clarity, the paper is well-written, and the authors provide clear explanations of their methods and results. However, some of the notation and terminology may be unfamiliar to non-experts, and the authors could provide more background information or explanations to make the paper more accessible.
In terms of originality, the paper introduces a new optimization method, SAGA, which is a novel combination of familiar techniques. The authors provide a thorough analysis of the relationship between SAGA and other fast incremental gradient methods, and the experimental results demonstrate the effectiveness of SAGA in practice.
In terms of significance, the paper addresses a difficult problem in optimization, and the results have the potential to impact the field of machine learning. The authors demonstrate the effectiveness of SAGA in practice, and the method has the potential to be used in a variety of applications.
Overall, I would recommend accepting this paper, but with some revisions to address the weaknesses mentioned above. The authors should provide clearer explanations for the tables and figures, and clarify the comparison with the L-BFGS approach. With these revisions, the paper has the potential to make a significant contribution to the field of optimization and machine learning.
Arguments for acceptance:
* The paper introduces a new optimization method, SAGA, which is a novel combination of familiar techniques.
* The authors provide a thorough analysis of the relationship between SAGA and other fast incremental gradient methods.
* The experimental results demonstrate the effectiveness of SAGA in practice.
* The paper addresses a difficult problem in optimization, and the results have the potential to impact the field of machine learning.
Arguments against acceptance:
* The authors claim that SAGA outperforms existing techniques, but the results show that it takes longer than the L-BFGS approach, which needs clarification.
* The energy versus sample index curve in Figure 2 shows inconsistent patterns that require explanation from the authors.
* The authors should provide clearer explanations for the tables in their captions for better understanding.