This paper presents a novel optimization method called SAGA, which improves upon existing incremental gradient algorithms such as SAG, SDCA, and SVRG. The authors provide a clear and well-motivated introduction to the problem of minimizing finite sums, and their proposed method is supported by theoretical analysis and experimental results.
The paper's strengths include its ability to handle non-strongly convex problems without modification, its support for proximal operators, and its adaptivity to the level of strong convexity present in the problem. The authors also provide a detailed comparison with other incremental gradient methods, highlighting the advantages and disadvantages of each approach.
One potential weakness of the paper is the assumption that the step size parameter is chosen optimally, which may not always be the case in practice. Additionally, the authors could provide more discussion on the choice of the regularization parameter and its impact on the convergence rate.
The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets, and the authors provide a thorough analysis of the results, highlighting the trade-offs between different methods. However, it would be useful to see more extensive experiments, including comparisons with other optimization methods and a more detailed analysis of the computational costs.
Overall, the paper is well-written, and the authors provide a clear and concise presentation of their method and its theoretical foundations. The paper's contributions are significant, and the results have the potential to impact the field of machine learning and optimization.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of optimization.
* The authors provide a clear and well-motivated introduction to the problem and their proposed method.
* The paper includes a detailed comparison with other incremental gradient methods, highlighting the advantages and disadvantages of each approach.
* The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets.
Arguments against acceptance:
* The assumption that the step size parameter is chosen optimally may not always be the case in practice.
* The authors could provide more discussion on the choice of the regularization parameter and its impact on the convergence rate.
* The experimental results could be more extensive, including comparisons with other optimization methods and a more detailed analysis of the computational costs.
Recommendation: Accept, with minor revisions to address the above comments.