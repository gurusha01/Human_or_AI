This paper proposes a novel optimization method called SAGA, which improves upon existing incremental gradient algorithms such as SAG and SVRG. The key idea behind SAGA is to use a variance reduction approach to reduce the noise in the gradient estimates, allowing for faster convergence rates. The authors provide a detailed analysis of the algorithm, including its convergence rates and relationships to other existing methods.
The paper is well-written and easy to follow, with clear explanations of the algorithm and its underlying theory. The authors also provide a thorough comparison of SAGA with other existing methods, including SAG, SVRG, and SDCA. The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets.
One of the strengths of the paper is its ability to unify several existing methods under a single framework. The authors show that SAGA can be seen as a midpoint between SAG and SVRG, and that it shares similarities with other methods such as Finito and MISOÂµ. This provides a clear understanding of the relationships between these methods and helps to identify their relative strengths and weaknesses.
However, there are some limitations to the paper. The authors assume prior knowledge of the true signal, which may not be practical in real-world scenarios. Additionally, the experiments are limited to a few benchmark datasets, and it would be useful to see more extensive testing on a wider range of problems.
In terms of originality, the paper builds upon existing work in the field, but the authors provide a novel perspective on the relationships between different incremental gradient methods. The SAGA algorithm itself is also a new contribution, and the authors provide a detailed analysis of its convergence rates and properties.
Overall, the paper is well-written and provides a clear and concise explanation of the SAGA algorithm and its underlying theory. The authors demonstrate the effectiveness of SAGA on several benchmark datasets and provide a thorough comparison with other existing methods. While there are some limitations to the paper, it is a valuable contribution to the field of optimization and incremental gradient methods.
Arguments for acceptance:
* The paper provides a novel perspective on the relationships between different incremental gradient methods.
* The SAGA algorithm is a new contribution, and the authors provide a detailed analysis of its convergence rates and properties.
* The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets.
* The paper is well-written and easy to follow, with clear explanations of the algorithm and its underlying theory.
Arguments against acceptance:
* The authors assume prior knowledge of the true signal, which may not be practical in real-world scenarios.
* The experiments are limited to a few benchmark datasets, and it would be useful to see more extensive testing on a wider range of problems.
* The paper builds upon existing work in the field, and some readers may find the contributions to be incremental rather than groundbreaking.