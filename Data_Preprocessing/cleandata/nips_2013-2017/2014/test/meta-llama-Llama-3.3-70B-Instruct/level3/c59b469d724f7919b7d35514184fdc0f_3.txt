This paper revisits the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective, examining the consistency of the optimal Nitzan-Paroush weighted majority rule. The authors provide sharp error estimates for the optimal rule when expert competence levels are known and offer frequentist and Bayesian analyses for the case where competence levels are unknown. 
The paper's strengths include its thorough exploration of the problem, providing both asymptotic and finitary consistency results. The authors' use of non-standard proof techniques, such as the Kearns-Saul inequality, is notable and may be of independent interest. The paper also poses several challenging open problems, including the computation of the probability of error induced by the Bayesian empirical decision rule.
However, the paper's weaknesses include the fact that the frequentist approach does not admit an optimal empirical decision rule, and the Bayesian approach, while admitting an optimal rule, does not provide a computable estimate of the error probability. Additionally, the paper's focus on the statistical learning perspective may limit its appeal to researchers in other areas.
In terms of quality, the paper is technically sound, with well-supported claims and careful evaluation of the strengths and weaknesses of the work. The authors are honest about the limitations of their approaches and provide a clear discussion of the open problems.
The paper's clarity is generally good, with each paragraph being understandable on its own. However, the paper's organization and notation may be confusing to readers not familiar with the area, and some of the proofs are quite technical.
The paper's originality is high, as it tackles a classic problem from a new perspective and provides new insights and results. The authors have done a good job of referencing related work and highlighting the differences between their approach and others.
The significance of the paper is also high, as it addresses a fundamental problem in decision theory and provides new results that can be used in a variety of applications. The paper's focus on the statistical learning perspective makes it relevant to researchers in machine learning and related areas.
Arguments for acceptance:
* The paper provides new and interesting results on a classic problem
* The authors use non-standard proof techniques that may be of independent interest
* The paper poses several challenging open problems that can stimulate future research
* The paper is technically sound and well-written
Arguments against acceptance:
* The frequentist approach does not admit an optimal empirical decision rule
* The Bayesian approach does not provide a computable estimate of the error probability
* The paper's focus on the statistical learning perspective may limit its appeal to researchers in other areas
* Some of the proofs are quite technical and may be difficult to follow for non-experts.