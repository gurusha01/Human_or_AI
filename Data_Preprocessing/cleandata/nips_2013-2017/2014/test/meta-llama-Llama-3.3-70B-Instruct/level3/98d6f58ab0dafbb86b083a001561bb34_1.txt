This paper explores the concept of reducing running time to obtain a given risk with more data, a question that has been proposed and considered by others but still needs more progress. The authors consider the problem of sparse regression, specifically the Regularized Linear Inverse Problem, and propose regularizing more heavily with more data to increase the rate of convergence. The idea of regularizing more heavily with more data has been observed and proved rigorously in other papers, which show that it can lead to faster convergence without changing the solution significantly.
The algorithm used in the paper is a dual smoothing algorithm that exploits strong convexity to convert the dual solution to the corresponding primal solution. The paper provides a detailed analysis of the algorithm's convergence rate and shows that it improves on the theory behind SAG and SVRG, with better theoretical convergence rates. The algorithm also has support for composite objectives where a proximal operator is used on the regulariser.
However, the paper's organization and writing could be improved with simpler calculations and clearer connections between the concepts, such as the relationship between regularizing and Fact 2.3. The paper is pursuing a promising direction with groundwork laid by others, but the details and clarity of the paper could be improved for better understanding and delivery of the results.
The strengths of the paper include its novel approach to sparse regression, its improved convergence rates, and its support for composite objectives. The weaknesses of the paper include its complex calculations and unclear connections between concepts. 
Arguments pro acceptance:
- The paper explores a promising direction with groundwork laid by others.
- The algorithm improves on the theory behind SAG and SVRG, with better theoretical convergence rates.
- The algorithm has support for composite objectives where a proximal operator is used on the regulariser.
Arguments con acceptance:
- The paper's organization and writing could be improved with simpler calculations and clearer connections between the concepts.
- The paper's clarity and details could be improved for better understanding and delivery of the results.
- The paper may benefit from more experimental results to validate the effectiveness of the algorithm.
Overall, the paper is well-motivated and explores a promising direction, but could benefit from improvements in clarity, organization, and experimental results. With revisions to address these issues, the paper has the potential to make a significant contribution to the field. 
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis. However, the paper could benefit from more experimental results to validate the effectiveness of the algorithm. In terms of clarity, the paper is well-written, but some calculations and connections between concepts could be simplified and clarified. In terms of originality, the paper proposes a novel approach to sparse regression, but builds on existing work in the field. In terms of significance, the paper has the potential to make a significant contribution to the field, but could benefit from more experimental results to demonstrate its effectiveness.