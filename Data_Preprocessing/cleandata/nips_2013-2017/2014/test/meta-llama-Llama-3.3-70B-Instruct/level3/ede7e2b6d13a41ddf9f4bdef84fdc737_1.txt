This paper introduces the SAGA algorithm, a novel incremental gradient method that improves upon existing methods like SAG, SDCA, MISO, and SVRG. The SAGA algorithm has provable linear convergence rates for strongly convex functions and a 1/k rate for non-strongly convex functions, adapting to the level of strong convexity in the problem. The paper provides simple and clear proofs of convergence for both strongly convex and non-strongly convex cases, making it a valuable resource for understanding and designing new algorithms.
The paper is well-written, and the authors provide a thorough analysis of the SAGA algorithm, including its relationship to other incremental gradient methods. The experimental results demonstrate the effectiveness of SAGA on various datasets, including MNIST, COVTYPE, IJCNN1, and MILLIONSONG. The results show that SAGA performs similarly to other state-of-the-art methods, such as Finito and SDCA, and outperforms SAG in some cases.
The strengths of the paper include:
* The introduction of a new algorithm with provable convergence rates
* A thorough analysis of the algorithm's relationship to other incremental gradient methods
* Simple and clear proofs of convergence
* Experimental results demonstrating the effectiveness of the algorithm
The weaknesses of the paper include:
* The algorithm's performance is not significantly better than other state-of-the-art methods
* The paper could benefit from more detailed comparisons to other algorithms, including a discussion of the trade-offs between different methods
Overall, the paper is well-written, and the SAGA algorithm is a significant contribution to the field of optimization. The paper provides a valuable resource for understanding and designing new algorithms, and the experimental results demonstrate the effectiveness of the algorithm.
Arguments for acceptance:
* The paper introduces a new algorithm with provable convergence rates
* The paper provides a thorough analysis of the algorithm's relationship to other incremental gradient methods
* The experimental results demonstrate the effectiveness of the algorithm
Arguments against acceptance:
* The algorithm's performance is not significantly better than other state-of-the-art methods
* The paper could benefit from more detailed comparisons to other algorithms
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a significant contribution to the field of optimization. The experimental results demonstrate the effectiveness of the algorithm, and the paper provides a valuable resource for understanding and designing new algorithms. Overall, I recommend accepting the paper.