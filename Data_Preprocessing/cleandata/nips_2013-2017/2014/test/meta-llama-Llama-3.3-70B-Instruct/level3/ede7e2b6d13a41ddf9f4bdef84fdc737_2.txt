This paper proposes a new incremental gradient algorithm called SAGA, which improves upon the theory behind SAG and SVRG with better theoretical convergence rates. The algorithm is applicable to both strongly convex and non-strongly convex problems, and it supports composite objectives with a proximal operator. The authors provide a detailed analysis of the relationship between SAGA and other fast incremental gradient methods, including SAG, SVRG, Finito, MISO, and SDCA.
The strengths of the paper include its clear and well-organized presentation, as well as its thorough analysis of the algorithm's convergence properties. The authors also provide a useful discussion of the relationships between different incremental gradient methods, which helps to clarify the contributions of their work. The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets.
However, there are some weaknesses to the paper. The idea of SAGA is not particularly novel, and the algorithm's convergence rate is not significantly improved compared to existing methods. The experimental results show similar convergence speeds with other methods, and SAGA does not always outperform existing methods. Additionally, there may be small errors in the proof, such as a missing scalar eta in an equation, which affects the work's credibility.
Overall, the paper is well-written and provides a clear presentation of the SAGA algorithm and its properties. However, the lack of significant improvements in convergence rate and the potential errors in the proof are notable weaknesses.
Arguments pro acceptance:
* The paper provides a clear and well-organized presentation of the SAGA algorithm and its properties.
* The authors provide a thorough analysis of the relationships between different incremental gradient methods.
* The experimental results demonstrate the effectiveness of SAGA on several benchmark datasets.
Arguments con acceptance:
* The idea of SAGA is not particularly novel, and the algorithm's convergence rate is not significantly improved compared to existing methods.
* The experimental results show similar convergence speeds with other methods, and SAGA does not always outperform existing methods.
* There may be small errors in the proof, such as a missing scalar eta in an equation, which affects the work's credibility.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and significance. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written and well-organized, making it easy to follow and understand. The significance of the paper is moderate, as it provides a new incremental gradient algorithm with improved theoretical convergence rates, but the improvements are not dramatic. Overall, I would recommend accepting the paper, but with revisions to address the potential errors in the proof and to provide more significant improvements in convergence rate.