This paper introduces a novel optimization method called SAGA, which is an incremental gradient algorithm with fast linear convergence rates. The approach is both elegant and computationally efficient, making it a significant contribution to the field. The paper is technically sound, with impressively diverse experiments and convincing results that outperform the baseline approach ADMM-Poly.
The SAGA algorithm is well-motivated and clearly explained, with a detailed analysis of its convergence properties. The authors provide a thorough discussion of the relationship between SAGA and other fast incremental gradient methods, such as SAG, SVRG, and SDCA. The experimental results demonstrate the effectiveness of SAGA on various datasets, including MNIST, COVTYPE, IJCNN1, and MILLIONSONG.
One potential weakness of the approach is that the semi-definite program for finding a DC decomposition may not have a solution, which could be a barrier to adoption. However, the authors provide a clear and concise explanation of the algorithm and its properties, making it easy to understand and implement.
The paper is well-written, but some aspects, such as the claimed global convergence, could be clarified to avoid misinterpretation. Additionally, the authors could provide more discussion on the potential applications of SAGA in computer vision and imaging, as well as its limitations and potential future directions.
Overall, the paper presents a significant contribution to the field of optimization and machine learning, with a novel and efficient algorithm that has the potential to benefit many researchers and practitioners. The strengths of the paper include its technical soundness, clear explanation, and convincing experimental results. The weaknesses are minor and do not detract from the overall quality of the paper.
Arguments for acceptance:
* The paper introduces a novel and efficient optimization algorithm with fast linear convergence rates.
* The approach is technically sound, with a clear and concise explanation of the algorithm and its properties.
* The experimental results demonstrate the effectiveness of SAGA on various datasets.
* The paper provides a thorough discussion of the relationship between SAGA and other fast incremental gradient methods.
Arguments against acceptance:
* The semi-definite program for finding a DC decomposition may not have a solution, which could be a barrier to adoption.
* Some aspects of the paper, such as the claimed global convergence, could be clarified to avoid misinterpretation.
* The authors could provide more discussion on the potential applications of SAGA in computer vision and imaging, as well as its limitations and potential future directions.