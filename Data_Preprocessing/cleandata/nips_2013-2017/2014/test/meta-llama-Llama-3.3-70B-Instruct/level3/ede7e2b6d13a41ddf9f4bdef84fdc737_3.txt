This paper introduces SAGA, a new incremental gradient technique that improves upon existing methods such as SAG, SVRG, and SDCA. The main advantage of SAGA is its ability to work without modification in the absence of strong convexity, unlike SDCA which requires explicit addition of a regularizer. SAGA also requires knowledge of Lipschitz and strong convexity constants for linear convergence, but only needs to tune one step-size parameter.
The paper provides a valuable summary of popular methods, including SDCA, and shows a clear connection to other primal-based methods like MISOmu. The authors demonstrate that SAGA improves upon existing methods with better analysis, ability to deal with prox operators, fewer parameters to tune, and automatic handling of non-strongly convex composite functions.
The strengths of the paper include its well-written and unified exposition of incremental gradient methods, as well as its clear and concise presentation of the SAGA algorithm and its theoretical convergence rates. The authors also provide a thorough discussion of related work, including a novel transformation of SDCA into an equivalent method that only works with primal quantities.
However, the paper could be strengthened by a clearer differentiation from other algorithms, including the impact of tuning parameters and handling non-strongly convex functions. Additionally, the paper could benefit from more computational examples and clearer comparisons to other algorithms.
In terms of quality, the paper is technically sound, with well-supported claims and a complete piece of work. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is also well-organized and clearly written, making it easy to follow and understand.
In terms of originality, the paper presents a novel combination of familiar techniques, with a new algorithm that improves upon existing methods. The authors provide a clear explanation of how SAGA differs from other methods, and the paper includes a thorough discussion of related work.
In terms of significance, the paper addresses a difficult problem in a better way than previous research, with a new algorithm that has the potential to be widely used in practice. The results are important, with implications for the field of machine learning and optimization.
Overall, I would recommend accepting this paper, with some minor revisions to address the areas mentioned above. The paper is well-written, technically sound, and presents a significant contribution to the field.
Arguments pro acceptance:
* The paper presents a novel and improved algorithm for incremental gradient methods
* The authors provide a clear and concise presentation of the SAGA algorithm and its theoretical convergence rates
* The paper includes a thorough discussion of related work and a clear explanation of how SAGA differs from other methods
* The results are important, with implications for the field of machine learning and optimization
Arguments con acceptance:
* The paper could benefit from more computational examples and clearer comparisons to other algorithms
* The authors could provide a clearer differentiation from other algorithms, including the impact of tuning parameters and handling non-strongly convex functions.