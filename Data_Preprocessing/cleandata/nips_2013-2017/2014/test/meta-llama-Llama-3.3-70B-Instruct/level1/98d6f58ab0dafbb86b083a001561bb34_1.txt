This paper proposes a tradeoff between sample complexity and computation time for statistical estimators based on convex optimization. The authors argue that as the amount of data increases, optimization problems can be smoothed more aggressively to achieve accurate estimates more quickly. The paper provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.
The paper relates to previous work by Shalev-Shwartz and Srebro, Agarwal et al., and Lai and Yin, who have identified related tradeoffs in machine learning and statistical computation. The authors' contribution is to propose a continuous sequence of relaxations based on smoothing, which allows for a more efficient solution of the optimization problem.
The paper is well-written and well-organized, making it easy to follow. The authors provide a clear introduction to the problem, a detailed description of the proposed method, and a thorough analysis of the results. The experimental results demonstrate the effectiveness of the proposed approach, showing a significant reduction in computational cost as the sample size increases.
The strengths of the paper include:
* A clear and well-motivated proposal for a tradeoff between sample complexity and computation time
* A thorough analysis of the theoretical foundations of the proposed method
* A detailed description of the experimental results, which demonstrate the effectiveness of the approach
* A clear connection to previous work in the field
The weaknesses of the paper include:
* The paper assumes a specific form of the regularizer, which may not be applicable to all problems
* The experimental results are limited to two specific examples, and it is not clear how well the approach will generalize to other problems
* The paper does not provide a clear comparison to other methods that may achieve similar results
Arguments for acceptance:
* The paper proposes a novel and well-motivated approach to trading off sample complexity and computation time
* The theoretical analysis is thorough and well-founded
* The experimental results demonstrate the effectiveness of the approach
* The paper is well-written and easy to follow
Arguments against acceptance:
* The paper assumes a specific form of the regularizer, which may limit its applicability
* The experimental results are limited to two specific examples
* The paper does not provide a clear comparison to other methods that may achieve similar results
Overall, I believe that the paper is a strong contribution to the field and deserves to be accepted. The proposed approach is novel and well-motivated, and the theoretical analysis is thorough and well-founded. The experimental results demonstrate the effectiveness of the approach, and the paper is well-written and easy to follow. While there are some limitations to the paper, I believe that they do not outweigh the strengths of the contribution. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results.
Clarity: 9/10
The paper is well-written and easy to follow, with a clear introduction, a detailed description of the proposed method, and a thorough analysis of the results.
Originality: 8/10
The paper proposes a novel approach to trading off sample complexity and computation time, but it builds on previous work in the field.
Significance: 8/10
The paper has the potential to make a significant impact in the field of machine learning and statistical computation, as it proposes a new approach to optimizing computational resources.