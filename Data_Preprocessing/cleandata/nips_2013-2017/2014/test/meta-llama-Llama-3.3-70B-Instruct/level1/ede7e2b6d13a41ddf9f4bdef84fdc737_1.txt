This paper introduces a new optimization method called SAGA, which is an incremental gradient algorithm with fast linear convergence rates. The authors relate SAGA to previous work, including SAG, SDCA, MISO, and SVRG, and show that SAGA improves on the theory behind SAG and SVRG with better theoretical convergence rates. SAGA also supports composite objectives with a proximal operator, unlike SDCA, and is adaptive to any inherent strong convexity of the problem.
The paper is well-written and clearly explains the SAGA algorithm, its relationship to other methods, and its theoretical convergence rates. The authors provide a detailed analysis of the algorithm's properties and advantages, including its ability to handle non-strongly convex problems without modification and its support for proximal operators. The experimental results demonstrate the effectiveness of SAGA on several datasets, including MNIST, COVTYPE, IJCNN1, and MILLIONSONG.
The strengths of the paper include:
* The introduction of a new optimization method with improved theoretical convergence rates
* A clear and detailed analysis of the algorithm's properties and advantages
* Experimental results demonstrating the effectiveness of SAGA on several datasets
The weaknesses of the paper include:
* The paper assumes a strong background in optimization and machine learning, which may make it difficult for non-experts to follow
* The experimental results are limited to a few datasets and may not be representative of the algorithm's performance on other problems
* The paper could benefit from a more detailed comparison with other optimization methods, including those not mentioned in the paper
Arguments for acceptance:
* The paper introduces a new optimization method with improved theoretical convergence rates, which is a significant contribution to the field
* The analysis of the algorithm's properties and advantages is clear and detailed, making it easy to understand the strengths and weaknesses of the method
* The experimental results demonstrate the effectiveness of SAGA on several datasets, which suggests that the method has practical applications
Arguments against acceptance:
* The paper assumes a strong background in optimization and machine learning, which may limit its accessibility to non-experts
* The experimental results are limited to a few datasets and may not be representative of the algorithm's performance on other problems
* The paper could benefit from a more detailed comparison with other optimization methods, including those not mentioned in the paper.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of optimization and provides a clear and detailed analysis of the SAGA algorithm. However, I would suggest that the authors consider adding more experimental results and comparisons with other optimization methods to strengthen the paper.