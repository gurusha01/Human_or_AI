Authors propose a method of estimating a graphical model for continuous data that blends the following three, established ideas: 1) assume the data follows a multivariate Gaussian and estimate using the graphical lasso; 2) do not assume the data follows a multivariate Gaussian and instead use a Gaussian copula, the nonparanormal, to allow arbitrary single variable marginals; or 3) assume a specific tree-structured factorization and model arbitrary bivariate marginals along the tree structure.
The proposed method introduces the blossom tree, which is a specific factorization of the model into a collection of densely connected blossom components that are connected by a specific set of tree edges. In particular, each blossom is connected (via a pedicel node) to at most one tree edge. The blossom components are modeled as sparse multivariate Gaussians (or using the non-paranormal copula) and the tree edges are modeled as arbitrary bivariate distributions with single variable marginals that are consistent with the marginal of any blossom pedicel to which they are attached.
The main idea of this paper, the factorization of a high-dimensional graphical model via the blossom tree structure appears to be unique and the ability to flexibly model components of a high-dimensional graphical model as non-Gaussian seems appealing. However, I'm not convinced that the blossom-tree factorization adds any real value. In other words, why should I do all of this work to learn a blossom-tree model, over a non-paranomral model, a sparse multivariate gaussian or a forest-tree model? The authors only considered simulated data, constructed using a process favorable for their proposed factorization.
I found the result in Theorem 3.1 a bit confusing. It tells us about the quality of estimating the negentropy. Why is this useful for the joint estimation problem? What does it tell us about the estimates produced using the blossom-tree factorization?
In addition, the proposed method of learning the blossom-tree structure seems a bit weak. Learning a sequence of blossom tree models, one non-blossom edge at a time, seems unreasonable for any moderately sized problem. In addition, please be specific about the complexity of the proposed method, relative to learning using graphical lasso, non paranormal and forest-trees. 
Finally, I think the experiments section could use a real boost. Running on at least one real-world data set would help convince the reader that there is real value in this factorization.
Detailed comments:
- I think that calling this a blossom tree is a poor choice. The word blossom was introduced by Edmonds for the matching problem and refers to an odd-sized set of vertices. 
- In the experiments, why are the '0' trunks not equal to the graphical lasso? If you don't add any trunks isn't there just one blossom, modeled as a multivariate gaussian?
- Runtimes for the different methods should also be reported. 
- Line 60: The statement "maintain tractable inference without placing limitations on the independence graph" is a bit misleading. They do encourage the graph to be sparse; they are just not structural like, say a graph laplacian.  Interesting article that is well written and appears to be introduce a novel factorization for graphical models of continuous data. However, the lack of experimentation on real-world data failed to convince me of the utility of the blossom tree factorization, while the proposed greed blossom tree construction method seems to intractable to be useful.