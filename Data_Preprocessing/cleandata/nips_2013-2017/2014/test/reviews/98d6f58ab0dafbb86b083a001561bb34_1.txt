This paper pursues an intriguing concept that has been proposed and considered by others as well (as appropriately cited in the present paper): given more data, can we reduce the running time required to obtain a given risk. This is clearly an important question, and while there has been some progress, the surface has really barely been scratched.
This paper considers the problem of sparse regression, in particular in the setting of no noise â€” the Regularized Linear Inverse Problem. The main idea is to regularize more heavily when there are more data. It has been observed and also proved rigorously, that regularizing increases the rate of convergence, but more interestingly, may not change the solution (by much, or at all). Two important references in this respect are papers by Wotao Yin and Ming-Jun Lai, that demonstrate precisely this idea, with the goal of showing a very similar conclusion as that reached in this paper. In those papers, they show that if the sensing matrix A satisfies some additional properties (e.g., better RIP) then one can regularize and still recover (nearly) the same answer, but without sending the regularizer to zero (just like what is done in this paper). If one has more data, then it is again fairly straightforward calculation to show that the sensing matrix A (assuming it comes from a suitable random ensemble) will indeed satisfy RIP with a stronger constant. Then, since the regularizer controls the strong convexity parameter of the problem, this speeds up convergence (and in particular, guarantees a global linear rate). Another paper that is relevant and has a very similar idea but without using RIP, is by Agarwal, Negahban and Wainwright, where the authors show that thanks to restricted strong convexity and restricted smoothness, gradient methods have global geometric (linear) convergence. The key connection with the present paper is that convergence time depends explicitly on the RSC and RSM parameters, and one can show (as the authors there do) that these improve when one has more data.
The algorithm in the present paper is a dual smoothing algorithm. Strong convexity is exploited in order to convert the dual solution to the corresponding (and unique) primal solution.
The organization and writing of this paper is not as clear as it might be. One thing that would improve the delivery of the results, is some simple calculations for a setting where, say, A comes from Gaussian design. Computing \mu(m) here should not be that difficult, yet would help tell the story. That is, it would be nice to have some calculations analogous to Fact 2.3, but not just for when exact recovery holds, but rather, how big the regularizer can be while still guaranteeing exact recovery. The current connection to regularizing and Fact 2.3 is not completely clear to me; it should be, as it is one of the core pieces of the paper.
 In general the paper is pursuing a nice direction, for which there has been much ground work laid, but the details are not all as clear as they could be. I'm not sure that they are so in this paper either, though the authors are certainly pushing in this direction.