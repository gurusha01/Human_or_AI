This paper proposes a new random forest based method for zero-shot learning that better accommodates the uncertainty in attribute classifier predictions. Uncertainty is measured via the performance (true positive rate, false positive rate, etc.,) of attribute classifiers on a held-out validation set. The paper proposes to use this performance information in the information gain computation during the learning of the random forest (for pursuing a given category-attribute signature along multiple paths of a decision tree), thereby accommodating the "uncertainty" of attribute predictions. 
Quality: The proposed ideas are supported well with relevant empirical/quantitative analysis. However it lacks in qualitative analysis (For e.g., I would have liked to see the types of attributes and the categories that are effected/improved by this method.) Also it is unclear how many of the parameters involved were selected (for e.g., why was the 80%-20% split for training-validation chosen, etc.,)
Clarity: The paper is well-written and easy to read. 
Originality: While most previous works have addressed the problem of "attribute strength", this paper claims to focus on the lesser explored problem of "attribute reliability". Although many learning algorithms/techniques implicitly account for classifier unreliability, I believe this work is novel in terms of explicit unreliability handling.
Significance: While the proposed method (random forest) is not new, the idea (of modeling unreliability by measuring classifier performance on validation data to improve zero-shot learning performance) introduced in this paper is interesting/thought-provoking to researchers working on attributes.
 While the idea and the method introduced in this paper are not significantly novel, their application towards improving zero-shot learning is revealing/interesting.