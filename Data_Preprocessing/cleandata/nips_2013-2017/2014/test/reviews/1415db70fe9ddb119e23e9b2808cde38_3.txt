This paper proposes using both the encoding and decoding phrase vectors of a recursive neural network auto-encoder in a sentiment classification task. The authors refer to this as belief propagation, although it is not an instance of this inference algorithm and as such this name serves to confuse the reader. In general this is a well written paper, however the motivation for the proposed model is vague and lacks a clear theoretical justification.
The key weakness of this paper is the evaluation. There is no reason to present an ensemble result in the context of this evaluation, none of the benchmarks are ensembles and this obfuscates the result. The exact composition of the models compared is also not clear. Key questions that should be clarified are: how were the models tuned and the ensemble/best models selected? How were the benchmarks in Table 2 tuned? Were they using the same augmented data as the BP-RNN? Were the RNN and Bidirectional-RNN also using hybrid word vectors?
In the absence of this information I would guess that the second entry in Table 3 is the most comparable to the benchmarks in Tables 1&2 when assessing the BP-RNN architecture.
Minor points:
- I am surprised that an off the shelf parser was used to parse tweets. I would be interested to know how accurate it is on this data.
- the Irsoy and Cardie citation is incomplete (as well as a number of others).
 This paper presents a recursive neural network model for classifying phrasal sentiment in context. The results appear reasonable, but the model lacks a strong motivation and the experimental methodology is not entirely clear.