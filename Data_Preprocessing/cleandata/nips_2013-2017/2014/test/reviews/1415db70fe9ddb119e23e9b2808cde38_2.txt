In NLP, recursive neural networks (RNNs) have been used to produce representations snippets of text by recursively combining pairs of representations of words/shorter snippets. The process starts with the representation of words and proceeds up a pre-specified parse tree until the representation of the entire snippet is obtained at the root node. The unidirectional nature of this process, however, does not allow the information to be propagated down the tree from the larger to the smaller contexts. This paper introduces a bidirectional extension of RNNs in which the upward pass through the tree is followed by a downward pass, augmenting the representations of all the nodes with the information from larger contexts. The resulting system achieves state-of-the-art performance on Task 2 of SemEval 2012.
The extension of the established RNN-based approach proposed in the paper is simple, elegant and effective. Though the high-level idea is quite similar to the one from [18], the computation performed on the downward pass in the paper is different and the resulting performance appears to be superior.
The method is well motivated and clearly presented and the paper is nicely written in general. The experimental results appear to be excellent, but I have some concerns about the experimental setup. Was the result for the alternative bidirectional RNN model from [18] reported in Table 2 obtained using dropout and the additional training data? Were the hybrid vectors also used in that case? Also, while the ensemble result is impressive, it does not really belong in a table comparing the performance of individual models.
What does the "widely used mix of both" refer to in Section 4.4? A well executed paper based on a simple and elegant idea.