This paper investigates the compelling question of whether increasing data can reduce the computational time required to achieve a given statistical risk, focusing on sparse regression in a noise-free setting. The authors propose a novel approach that leverages heavier regularization with more data, demonstrating improved convergence rates in regularized linear inverse problems (RLIPs). This work builds on prior research, such as the restricted isometry property (RIP)-based methods by Lai and Yin, and extends ideas like restricted strong convexity (RSC) and restricted smoothness (RSM) to establish a time–data tradeoff. The proposed dual-smoothing algorithm exploits strong convexity to link dual and primal solutions, offering theoretical and experimental evidence for its efficacy.
Strengths:  
The paper addresses a relatively underexplored but significant problem in statistical computation, contributing to the emerging perspective that data can be treated as a computational resource. The theoretical framework is grounded in well-established concepts like statistical dimension and descent cones, and the authors provide rigorous bounds for the proposed smoothing approach. The experimental results, particularly for sparse vector and low-rank matrix recovery, convincingly demonstrate the time–data tradeoff, with observed speedups of up to 5.4× in some cases. The dual-smoothing method is a meaningful extension of prior work, offering a continuous relaxation framework that is computationally efficient and broadly applicable.
Weaknesses:  
The paper's clarity and organization leave room for improvement. While the theoretical contributions are significant, the presentation of key results, such as Fact 2.3 and its connection to regularization, is not sufficiently clear. The lack of concrete examples, such as Gaussian design cases, makes it harder for readers to intuitively grasp the implications of the results. Additionally, the connection between regularization and Fact 2.3 is underdeveloped, weakening the argument for the proposed method. The experimental section could benefit from more diverse datasets and a discussion of practical limitations, such as the sensitivity of the method to the choice of smoothing parameters.
Pro and Con Arguments for Acceptance:  
Pro:  
1. The paper tackles an important and underexplored problem, advancing the state of the art in statistical computation.  
2. The dual-smoothing algorithm is theoretically sound and demonstrates significant empirical improvements.  
3. The work is broadly applicable to sparse regression and low-rank matrix recovery, with potential extensions to noisy settings and statistical learning.  
Con:  
1. The paper's presentation is suboptimal, with unclear connections between theoretical results and practical implications.  
2. The experiments, while promising, lack diversity and fail to explore potential limitations of the method.  
3. The connection between regularization and the exact recovery condition (Fact 2.3) is inadequately explained.
Recommendation:  
This paper presents a promising direction with substantial theoretical and experimental contributions. However, the issues with clarity and presentation need to be addressed to maximize its impact. I recommend acceptance, contingent on revisions to improve the exposition, clarify theoretical connections, and provide more illustrative examples.