This paper addresses the problem of structured low-rank matrix recovery, a central challenge in machine learning and signal processing, by proposing a novel reformulation that facilitates the use of the Generalized Conditional Gradient (GCG) method. The authors demonstrate that their approach achieves significant computational efficiency, with a per-iteration complexity of \(O(MN)\), and scales well to large problem sizes. The empirical results on stochastic system realization (SSR) and spectral compressed sensing (SCS) validate the method's effectiveness and efficiency, showing substantial improvements over state-of-the-art methods.
Strengths:
1. Novelty and Practical Relevance: The reformulation of the structured rank minimization problem is innovative and directly addresses the computational bottlenecks of prior methods, such as reliance on full singular value decomposition (SVD) or solving linear systems per iteration. This makes the approach highly relevant for large-scale applications.
2. Efficiency: The proposed algorithm achieves a lower computational complexity compared to existing methods, which is theoretically justified and empirically validated. The scalability to larger problems is a significant advantage.
3. Empirical Validation: The experiments on SSR and SCS problems are well-designed and demonstrate the algorithm's ability to outperform state-of-the-art methods in terms of speed and solution quality.
4. Clarity in Algorithm Design: The use of GCG with local search and the maintenance of low-rank factorizations throughout iterations are clearly explained and practically beneficial.
Weaknesses:
1. Discrepancy Between Formulations: While the reformulated objective function achieves computational efficiency, the paper lacks a thorough theoretical or empirical analysis of the trade-offs introduced by this modification. Specifically, the impact of the penalty parameter \(\lambda\) on the solution quality and convergence is not explored in depth.
2. Rank Convergence Behavior: The paper notes strange behavior in rank convergence for some cases but does not provide a detailed analysis or guidance on when to terminate the algorithm. This omission could hinder reproducibility and practical adoption.
3. Limited Discussion of Experimental Results: While the empirical results are promising, the paper could benefit from a more detailed discussion of the experimental findings, particularly the behavior of the algorithm under varying problem sizes, noise levels, and penalty parameters.
Suggestions:
1. Clarify that the modified objective function is not a relaxation of the original problem but an alternative utility function targeting the same low-rank model. This would help readers better understand the trade-offs involved.
2. Expand the discussion of experimental results, either in the main text or supplementary material, to provide deeper insights into the algorithm's behavior and limitations.
3. Address the rank convergence issue by either providing theoretical guarantees or empirical heuristics for termination criteria.
4. Improve the citation of Theorem 1 to clarify that the proof technique is inspired by, but not directly derived from, prior work.
Overall Assessment:
The paper is a strong contribution to the field of structured matrix rank minimization. Its novel formulation and algorithmic approach address key computational challenges, making it a valuable addition to the literature. However, the lack of exploration into the trade-offs introduced by the reformulation and the rank convergence behavior are notable weaknesses. While the paper is not suitable for oral or spotlight presentation due to these limitations, it is well-suited for acceptance as a conference paper. The authors are encouraged to address the identified weaknesses in a future revision to further strengthen their contribution.