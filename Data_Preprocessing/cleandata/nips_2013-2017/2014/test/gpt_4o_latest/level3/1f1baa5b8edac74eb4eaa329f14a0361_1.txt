This paper presents a novel approach to zero-shot learning (ZSL) by addressing the critical challenge of attribute prediction unreliability, a limitation that often hinders the practical application of ZSL methods. The authors propose a random forest-based framework that explicitly incorporates attribute detector reliability into the training process, enabling more robust classification of unseen categories. The method is further extended to the few-shot learning (FSL) setting, demonstrating state-of-the-art performance across three challenging datasets: AwA, aPY, and SUN.
Strengths:
1. Quality: The paper is technically sound and provides a comprehensive theoretical foundation for the proposed method. The authors rigorously evaluate their approach with controlled noise experiments, ablation studies, and comparisons to strong baselines, including DAP and other state-of-the-art methods. The results convincingly demonstrate the robustness of the proposed framework, particularly in scenarios with unreliable attribute predictions.
   
2. Originality: The work is highly original in its application of random forests to ZSL, specifically in modeling attribute unreliability during training. This is a novel extension to existing ZSL methods, which often assume idealized attribute predictions. The integration of receiver operating characteristics (ROC) into the decision tree construction process is a particularly innovative contribution.
3. Significance: The proposed method addresses a critical gap in ZSL by making it more robust to real-world challenges, such as noisy attribute predictions and uncertain class-attribute associations. The ability to extend the framework to FSL further enhances its practical relevance, offering a scalable solution for low-data scenarios. The state-of-the-art performance on multiple datasets underscores its potential impact on the field.
4. Clarity: The paper is generally well-written and organized, with clear explanations of the methodology and experimental setup. However, Section 3.2.1 could benefit from additional clarification regarding the threshold \( t \) in the random forest splits, as this section initially lacks sufficient detail. The issue is later resolved, but early clarification would improve readability.
Weaknesses:
1. Clarity in Section 3.2.1: The explanation of the threshold \( t \) in the random forest training process is initially unclear, which may confuse readers unfamiliar with the technical details of decision tree algorithms.
2. Scalability: While the method performs well on the evaluated datasets, the scalability of the approach to scenarios with a significantly larger number of unseen classes or attributes is not thoroughly discussed. This could be a limitation in real-world applications.
Arguments for Acceptance:
- The paper addresses a critical and underexplored challenge in ZSL, making a significant contribution to the field.
- The proposed method is theoretically sound, well-validated through experiments, and achieves state-of-the-art results.
- The approach is novel and extends the applicability of ZSL to more realistic scenarios, such as noisy attribute predictions and FSL.
Arguments Against Acceptance:
- Minor clarity issues in Section 3.2.1 could hinder comprehension for some readers.
- The scalability of the method to larger datasets or more complex attribute vocabularies is not fully explored.
Recommendation:
This paper represents a significant advancement in zero-shot learning by effectively bridging the gap between theory and practice. Its innovative use of random forests to model attribute unreliability and its strong empirical results make it a valuable contribution to the field. Despite minor clarity issues, the work is of high quality and should be accepted for publication.