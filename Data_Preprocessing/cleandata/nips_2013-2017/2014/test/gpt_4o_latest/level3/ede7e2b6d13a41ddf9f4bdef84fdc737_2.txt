The paper introduces SAGA, a new incremental gradient algorithm aimed at improving the theoretical convergence rates of existing methods like SAG and SVRG. The authors claim that SAGA offers better theoretical guarantees and supports composite objectives with proximal operators. Additionally, SAGA is designed to handle non-strongly convex problems without modification and adapt to inherent strong convexity. The paper also explores the connections between SAGA and related methods, providing a unified theoretical framework. Experimental results are presented to validate the algorithm's performance on standard datasets.
Strengths:
1. Theoretical Clarity: The paper provides a clear theoretical analysis of SAGA, including its convergence rates in both strongly convex and non-strongly convex settings. The relationship between SAGA and other methods (e.g., SAG, SVRG, SDCA) is well-articulated, offering a unified perspective on variance reduction techniques.
2. Support for Composite Objectives: The ability of SAGA to handle composite objectives with proximal operators is a notable contribution, as this extends its applicability to a broader class of optimization problems.
3. Practical Insights: The discussion of implementation details, such as storage requirements and step size selection, is practical and useful for researchers and practitioners.
4. Experimental Validation: The experiments are comprehensive, comparing SAGA to several state-of-the-art methods on well-known datasets. The results confirm that SAGA performs comparably to existing methods.
Weaknesses:
1. Limited Novelty: While the paper provides a unified view of existing methods and introduces SAGA as a midpoint between SAG and SVRG, the core idea of variance reduction is not novel. The algorithm itself appears to be an incremental modification rather than a groundbreaking innovation.
2. Marginal Convergence Improvements: The theoretical convergence rates, while slightly better than SAG and SVRG, do not represent a significant leap forward. Experimental results also show that SAGA does not consistently outperform existing methods.
3. Errors in Proofs: There are minor errors in the theoretical analysis, such as a missing scalar in line 324. These issues, while not critical, suggest a lack of thorough proofreading.
4. Limited Practical Impact: Despite its theoretical contributions, the practical advantages of SAGA over existing methods are not compelling. For instance, Finito and SVRG demonstrate comparable or better performance in some cases.
Recommendation:
While the paper is technically sound and well-written, its contributions are incremental, and the practical significance of SAGA is limited. The lack of consistent experimental superiority and the marginal theoretical improvements reduce its impact. I recommend rejecting the paper in its current form but encourage the authors to refine their work, address the proof errors, and explore more impactful applications or extensions of SAGA.
Arguments for Acceptance:
- Clear theoretical analysis and connections to existing methods.
- Support for composite objectives with proximal operators.
- Comprehensive experimental validation.
Arguments for Rejection:
- Limited novelty in the algorithmic contribution.
- Marginal improvements in convergence rates.
- Inconsistent experimental performance compared to existing methods.
- Minor errors in theoretical proofs.
Overall, while the paper is a solid piece of work, it does not meet the bar for significant scientific contribution required for acceptance at a top-tier conference.