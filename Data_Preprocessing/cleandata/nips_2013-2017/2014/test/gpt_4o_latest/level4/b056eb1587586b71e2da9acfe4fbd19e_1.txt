This paper introduces an algorithm for attribute (feature) selection, specifically applied to activity recognition. The proposed method defines three criteria for selecting attributes: high discrimination between pairwise classes, consistency in discrimination across all class pairs, and comprehensive coverage of all class pairs. The first two criteria are modeled using a random walk framework, while the third employs a set-cover approach. A greedy optimization strategy is then utilized to iteratively select an attribute subset, starting from an empty set.
The research presented in this paper is compelling. The primary contribution lies in the proposed feature selection algorithm, which appears to be novel and represents an intriguing combination of random walk, set-cover, and greedy optimization techniques. The experimental results are promising, demonstrating improvements over existing methods and baselines, including cases with no feature selection and alternative strategies. Although some experimental details are missing, the results appear robust.
My primary concern with the paper is the motivation and intuition behind the approach. The components of the method seem to be selected to facilitate submodular optimization. Specifically, the discussion on lines 133â€“145 outlines the three criteria, but the motivation for the second criterion is unclear. Why is it necessary for attributes to perform similarly across all class pairs? Couldn't some attributes be more effective for certain classes than others, with the final classifier combining them appropriately? This second criterion appears to add complexity to the optimization process, requiring the use of the proposed techniques.
The resulting submodular optimization essentially reduces to a greedy approach. It seems plausible that a similar greedy optimization could be applied to alternative formulations, including those without the second criterion. It would be interesting to empirically evaluate whether this criterion is crucial to the method's performance.
Despite this concern, it is not a fundamental issue. Overall, I find the paper to be of high quality and recommend its acceptance.
Additional comments:
- I did not review the supplementary material in detail, but the intuitive explanations provided in the main paper seem reasonable.  
- Typo: "Combing" should be corrected.  
- The role of sparse coding (Eq. 5) in the paper is unclear. The experiments reference KSVD for the DDA dataset, but sparse coding of attributes does not appear to be a central component of the paper. If it is used, it should be compared to alternatives like k-means, though space limitations in a conference paper are understandable.  
- How were the hyperparameters set in the experiments? Were they tuned using test (cross-validation) accuracies?  
- The experimental results outperform prior work, but there are discrepancies in Table 2. For example, [20] reports a per-activity AP list and an mAP of 71.6, but Fig. 10(b) in [20] shows an mAP of 74.38%, which seems higher than the results in this paper.  
- The related work section is comprehensive, but it could benefit from including the following reference:  
  Learning Multi-Modal Latent Attributes  
  Y. Fu, T. Hospedales, T. Xiang, and S. Gong  
  IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI 2013)  
In summary, the paper proposes a novel algorithm for attribute or feature selection. While the motivation for certain aspects of the approach is not entirely clear, the method itself is innovative and demonstrates strong empirical performance.