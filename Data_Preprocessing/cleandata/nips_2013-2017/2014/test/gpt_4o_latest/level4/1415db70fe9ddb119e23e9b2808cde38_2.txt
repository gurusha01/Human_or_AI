In natural language processing (NLP), recursive neural networks (RNNs) have been employed to generate representations of text snippets by recursively combining representations of words or smaller snippets. This process begins with word representations and progresses upward along a pre-defined parse tree until the representation of the entire snippet is constructed at the root node. However, the unidirectional nature of this approach prevents information from being propagated downward through the tree, from larger contexts to smaller ones. This paper proposes a bidirectional extension to RNNs, where the upward pass through the tree is followed by a downward pass, enriching the representations of all nodes with information from broader contexts. The proposed system achieves state-of-the-art results on Task 2 of SemEval 2012.
The paper's extension to the traditional RNN-based approach is straightforward, elegant, and effective. While the overarching concept is somewhat similar to that of [18], the computations performed during the downward pass differ, and the resulting performance appears to surpass that of the prior method.
The approach is well-motivated and clearly articulated, and the paper is generally well-written. The experimental results are impressive, but I have a few concerns regarding the experimental setup. Specifically, was the result for the alternative bidirectional RNN model from [18] in Table 2 obtained using dropout and the additional training data? Were hybrid vectors also employed in that case? Furthermore, while the ensemble result is noteworthy, it does not seem appropriate for inclusion in a table that compares the performance of individual models.
Lastly, in Section 4.4, what does the phrase "widely used mix of both" refer to? Overall, this is a well-executed paper built on a simple and elegant idea.