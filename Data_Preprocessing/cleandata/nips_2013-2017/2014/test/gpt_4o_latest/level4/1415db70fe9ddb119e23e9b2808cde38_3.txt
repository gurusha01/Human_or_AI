This paper introduces a sentiment classification approach that leverages both the encoding and decoding phrase vectors of a recursive neural network auto-encoder. The authors term this process "belief propagation," though this terminology is misleading, as it does not correspond to the established inference algorithm of the same name, potentially causing confusion for readers. While the paper is generally well-written, the motivation behind the proposed model is somewhat unclear and lacks a robust theoretical foundation.
The primary shortcoming of this work lies in its evaluation methodology. Presenting an ensemble result in this context is unwarranted, as none of the benchmarks are ensembles, which obscures the comparative analysis. Furthermore, the composition of the models being compared is insufficiently detailed. Critical questions that need to be addressed include: What were the tuning procedures for the models, and how were the ensemble and best-performing models selected? How were the benchmarks in Table 2 tuned? Did they utilize the same augmented data as the BP-RNN? Additionally, were the RNN and Bidirectional-RNN models employing hybrid word vectors? Without this information, it seems reasonable to infer that the second entry in Table 3 provides the most direct comparison to the benchmarks in Tables 1 and 2 when evaluating the BP-RNN architecture.
Minor comments:
- It is surprising that an off-the-shelf parser was used for processing tweets. Clarification on its accuracy for this specific dataset would be valuable.
- The citation for Irsoy and Cardie is incomplete, as are several others.
In summary, this paper presents a recursive neural network model for contextual phrasal sentiment classification. While the results appear promising, the model's motivation is not well-articulated, and the experimental methodology lacks sufficient clarity.