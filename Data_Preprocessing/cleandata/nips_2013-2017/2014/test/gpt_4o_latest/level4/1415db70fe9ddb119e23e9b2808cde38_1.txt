This paper presents the concept of "belief propagation recursive neural networks," which enhance "feedforward" (bottom-up) recursive networks by incorporating a backward (top-down) step during inference. This mechanism enables phrase-level predictions and embeddings to provide feedback to word embeddings and labels.
Overall, the paper is well-written, and the proposed model is both well-motivated and clearly described. It builds upon the Bidirectional Recursive Neural Networks introduced by Irsoy and Cardie in 2008 (though the reference to this prior work is incomplete). A notable contribution of this paper is the introduction of hybrid word vectors, which may represent the most significant distinction from earlier work. However, the experimental comparisons raise some concerns: were the RNN and B-RNN models also equipped with hybrid vectors? How were the "best" models tuned and selected? What criteria were used to identify the single best model? Additionally, it is worth noting that other methods do not rely on ensembles. These aspects require further clarification.
The term "belief propagation" could be considered misleading, and the term "forward-backward" might be more appropriate.
Section 3.5 (Training) is somewhat brief and could benefit from additional details. For instance, it appears that the training process employs the back-propagation through structure algorithm (the paper by Goller, 1996, could be cited here). This algorithm suggests that the recursive model is unfolded, resulting in a very deep network to update the word embedding layer. If this interpretation is correct, the authors should elaborate on this critical step.
Section 3.6 is somewhat unclear, and the distinction between this model and the work of Irsoy and Cardie needs to be explicitly clarified.
Section 4.1 should be revised to make it accessible to readers outside the NLP community. For example, in Task 2 of Semeval 2013, there are two subtasks: contextual polarity and message polarity. It seems that the authors address the contextual polarity subtask. A clearer presentation of the task would help readers better understand the parser constraints.
It might also be worth considering merging Sections 4.3 and 4.4 for improved coherence.
In summary, this paper is well-written and introduces an intriguing variant of the Bidirectional Recursive Neural Network model.