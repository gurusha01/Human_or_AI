The paper explores the decomposition of structured output objects into smaller components within the context of Structured Output (SO) prediction. The authors provide a theoretical analysis demonstrating that this decomposition approach achieves better generalization compared to standard SO models. Specifically, for Conditional Random Fields (CRFs), the paper shows that optimization using Stochastic Gradient Descent (SGD) achieves faster convergence rates. Empirical evaluations on sequence models across four distinct applications reveal that the proposed method delivers superior or competitive performance (3/1) relative to state-of-the-art systems in the literature.
I find the theoretical analysis of the decomposition approach to be innovative, as prior work has largely focused on algorithmic proposals without delving into theoretical justifications. However, one concern is that the provided generalization bounds are significantly looser than those in existing analyses for SO prediction models using hinge-loss, such as those by Taskar et al. (Max-Margin Markov Networks, NIPS 2003) and McAllister (Generalization Bounds and Consistency for Structured Labeling, in Predicting Structured Data, 2006). Specifically, while these works achieve a generalization bound of log(l), the bound in this paper scales as l‚Å¥, where l represents the size of the structured object. This raises questions about how the theoretical insights from the decomposition approach might translate into more informative analyses.
On the experimental front, the results are highly compelling. For segmentation tasks such as Named Entity Recognition (NER) and Word Segmentation, maintaining consistent labeling is crucial (e.g., in BIO tagging, an I (Inside) label is meaningless without a preceding B (Begin) label). The figures suggest that for Chinese word segmentation using CRFs, focusing on bigram labels yields the best performance (46.6/20), and a similar trend is observed for NER (26.5/~10). [Could the authors confirm whether this interpretation of the comparisons is accurate?] I am assuming that the same decomposition algorithm is applied during testing. Additionally, I noticed that the model's performance on the (test?) data in Table 1 aligns closely with the optimal hyperparameter values identified on the development data in Figure 2. It would be helpful if the authors could provide further discussion on these two observations.
The decomposition of the model is performed randomly for each instance, as described in Algorithm 1. This random decomposition naturally extends to sequence models, which remain the most representative SO prediction problem. However, it would be beneficial for the authors to include a (possibly informal) discussion on the implications of the random decomposition approach for models with clique sizes larger than 2. Overall, the paper presents a decomposition-based approach to SO prediction, supported by somewhat limited theoretical analysis but strong empirical results.