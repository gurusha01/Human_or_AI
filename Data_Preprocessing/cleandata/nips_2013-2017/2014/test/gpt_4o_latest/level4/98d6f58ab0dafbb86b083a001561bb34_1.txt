This paper explores an intriguing concept that has been previously proposed and examined by others (as appropriately cited in the paper): whether increasing the amount of data can reduce the computational time required to achieve a given risk. This is undoubtedly an important question, and while some progress has been made in this area, much remains unexplored.
The paper focuses on the problem of sparse regression, specifically in the noise-free setting of the Regularized Linear Inverse Problem. The central idea is to apply heavier regularization as more data becomes available. It has been observed and rigorously proven that stronger regularization can accelerate the rate of convergence, while, interestingly, having little to no impact on the solution itself. Two key references in this context are the works of Wotao Yin and Ming-Jun Lai, which demonstrate this concept and arrive at conclusions closely aligned with those in the present paper. These works show that if the sensing matrix \( A \) satisfies certain additional properties (e.g., improved RIP), one can regularize without driving the regularizer to zero and still recover (nearly) the same solution. As more data becomes available, it is relatively straightforward to show that the sensing matrix \( A \) (assuming it is drawn from a suitable random ensemble) will satisfy RIP with a stronger constant. Since the regularizer influences the strong convexity parameter of the problem, this leads to faster convergence, specifically guaranteeing a global linear convergence rate. Another relevant paper with a similar idea, though not relying on RIP, is by Agarwal, Negahban, and Wainwright. In their work, the authors demonstrate that gradient methods achieve global geometric (linear) convergence due to restricted strong convexity (RSC) and restricted smoothness (RSM). The connection to the present paper lies in the fact that convergence time explicitly depends on the RSC and RSM parameters, which improve as the amount of data increases, as shown in their analysis.
The algorithm proposed in this paper is a dual smoothing algorithm, which leverages strong convexity to map the dual solution to the corresponding (and unique) primal solution.
The organization and clarity of the paper could be improved. One way to enhance the presentation of the results would be to include simple calculations for a scenario where \( A \) is drawn from a Gaussian design. Computing \( \mu(m) \) in this setting should not be overly challenging and would help illustrate the main ideas. Specifically, it would be helpful to include calculations analogous to Fact 2.3, not only for cases where exact recovery holds but also to determine how large the regularizer can be while still ensuring exact recovery. The current connection between regularization and Fact 2.3 is somewhat unclear and should be clarified, as it is a central component of the paper.
Overall, the paper explores a promising direction, building on significant prior groundwork. However, the details are not as clearly presented as they could be, and this paper does not fully resolve these issues, though the authors are making meaningful progress in this area.