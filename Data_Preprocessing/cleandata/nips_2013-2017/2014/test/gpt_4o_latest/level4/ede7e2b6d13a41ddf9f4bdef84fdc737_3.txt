SAGA introduces a novel incremental gradient technique. In terms of performance, its key features include provable superiority over SAG and SVRG on strongly convex problems, while achieving speeds comparable to SDCA. A primary advantage of SAGA over SDCA is its ability to operate without modification in scenarios lacking strong convexity. Here, "without modification" refers to the algorithm's capability to function without the explicit addition of a regularizer to enforce strong convexity. However, SAGA does appear to require explicit knowledge of the Lipschitz and strong convexity constants to achieve linear convergence. Another notable benefit of SAGA, as detailed below, is that it only requires tuning a single step-size parameter (\(\eta\) in the paper). It would be insightful to understand how this parameter is determined in practice when the required constants are not explicitly known.
Section 3 not only introduces SAGA but also provides a concise and valuable summary of several other popular methods. This adds significant value to the paper, as many variants of these methods have been in use for some time, and a unified comparison enhances understanding. For instance, the discussion on SDCA is particularly noteworthy. By reformulating the problem, the authors establish a much clearer connection between SDCA and other primal-based methods, such as MISO\(_\mu\).
SAGA stands out as an improvement over these methods due to its superior analysis (e.g., compared to SAG), its ability to handle proximal operators, its reduced need for parameter tuning (e.g., compared to SVRG), and its automatic handling of non-strongly convex problems.
Since the ability to handle non-strongly convex composite functions without additional modifications is a central feature of the algorithm, the paper would be more compelling if the authors could demonstrate—either through explanation or computational evidence—that adding a small regularizer and using an alternative method like SDCA does not yield a superior solution.
The paper is generally well-written. I noticed only minor errors: on line 121, "effect" should be replaced with "affect," and on line 298, a \(\mu\) appears to have been mistakenly replaced with an \(s\). Overall, the unified exposition of various incremental gradient methods is a valuable contribution. However, the paper could be further strengthened by more clearly differentiating SAGA from other algorithms. For example, how challenging is it to tune two parameters instead of one? Additionally, how does SAGA's performance in the absence of strong convexity compare quantitatively to methods that require a regularizer?