In the context of regularized linear inverse problems, the sample complexity required for accurately recovering the true signal has been well-established in recent literature. This paper posits that utilizing more observations than the sample complexity bound necessitates can help reduce computational costs. The proposed approach achieves this by carefully smoothing the original nonsmooth regularizer (e.g., the ℓ₁ norm), thereby reducing the number of computational iterations required, all while preserving the sample complexity bound for recovery. The paper is well-written, and the underlying idea is intriguing, but there are several practical concerns that could have been addressed more thoroughly.
The reviewer finds the concept of leveraging excess observations to enable more aggressive smoothing quite compelling. However, this comes at the potential cost of compromising the recovery probability (as seen in η in Fact 2.3, particularly the success recovery bound). Unfortunately, the authors did not simulate the probability of "exact" recovery in their experiments. Instead, they rely on a predefined notion of "closeness" to the true signal to declare the success of their algorithm. While the authors attempt to mitigate failure by reducing the smoothing parameter by a factor of 4, this approach appears ad hoc and insufficient. It would be both interesting and necessary to investigate whether, and to what extent, aggressive smoothing impacts the probability of exact recovery. Based on the theoretical bound in Fact 2.3, there seems to be another tradeoff at play that warrants further exploration.
The experimental section could also be improved. The reviewer expected to see a comparison against a baseline approach using a constant smoothing parameter, such as μ = 0.1. For this baseline, one could calculate the number of samples required for high-probability recovery (denoted as mμ), which would be smaller than the total number of available samples, m. By randomly discarding m - mμ samples, the computational cost could be reduced by decreasing the problem size. This approach represents a "conventional" constant smoothing algorithm and would serve as a meaningful comparison. In the current setup, the authors allow the competing algorithm to utilize all available samples, which places it at a disadvantage. The two strategies—discarding redundant data versus applying aggressive smoothing—are conceptually similar, as both aim to maintain the recovery bound in a minimal sense. However, the first approach may be more appealing due to its reduced memory requirements. The reviewer strongly recommends conducting a thorough comparison of these strategies and reporting their respective strengths and weaknesses.
Another concern relates to the practicality of the proposed method. The current analysis and experiments assume prior knowledge of the true signal, which is unrealistic in practical scenarios. How can one determine the appropriate level of aggressive smoothing without risking excessive smoothing in the absence of such prior knowledge? On the other hand, tuning the smoothing parameter is already a common practice in real-world applications. From a purely practical perspective, the reviewer questions what is novel or different in the proposed approach. If the key takeaway (as suggested in line 325) is that the smoothing parameter should depend on the sample size, how can this dependency be implemented without requiring significant prior knowledge of the true signal?
Overall, the paper provides a well-written and compelling argument for how excess samples in regularized linear inverse problems can be exploited computationally to accelerate convergence. However, there are critical theoretical and experimental issues that should have been addressed to strengthen the paper's contributions.