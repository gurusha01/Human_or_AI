This paper is among the first to investigate best-arm identification in bandit models with correlated arms, specifically in the fixed-confidence setting. The authors introduce a new complexity notion that accounts for arm correlations and propose algorithms that nearly achieve this complexity. These include both static allocation strategies (independent of observed rewards) and an adaptive strategy.
First, it is worth noting that this paper is not the very first to address best-arm identification in linear bandit problems. For instance:
* Hoffmann, Shahriari, De Freitas, "On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning" (AISTATS 2014)
examines a linear bandit model with Gaussian noise. However, their work focuses on the fixed-budget setting, and the complexity term they derive involves a sum over all arms of squared gaps, which does not fully capture the correlations induced by the linear structure.
The theoretical results hinge on Propositions 1 and 2 in Section 2. The statements of these results are somewhat imprecise, and it should be clarified whether \(n\) is fixed or not. Specifically, the result from [1] presented as Proposition 2 should be:
\[ P(\forall n \in \mathbb{N}, \text{(2) holds}) \geq 1 - \delta, \]
while Proposition 1 should be expressed as:
\[ P(\text{(1) holds}) \geq 1 - \delta \text{ for every } n \text{ and every fixed sequence } x_n. \]
This distinction is important, as an additional union bound over \(n\) may be required when applying Proposition 1. Even with a static sampling rule, the sampling process is adaptive. Furthermore, a precise reference for Proposition 1 should be provided.
The terminology in the paper requires greater precision. In the fixed-confidence setting of best-arm identification, algorithms are typically composed of three components: a sampling rule, a stopping rule, and a recommendation rule (the latter often defaults to selecting the best arm based on the current least-squares estimate). The "static and adaptive strategies" mentioned in the paper pertain only to the sampling strategy, as the stopping strategy for the G-allocation method is adaptive. In the proof of Theorem 1, a union bound over \(n\) seems to be missing (as noted earlier regarding Proposition 1). Specifically, the inequality on line 674 must hold for all \(n\), which would introduce an additional \(\log(CK^2/t^2\delta)\) term (for some constant \(C\)) due to the union bound. Additionally, the term "beta-approximation" in Theorem 1 is not defined, either in the main text or in Lemmas 5 and 6.
Regarding the oracle: Section 3 is somewhat unclear. If the parameter \(\theta^\) were known, the problem would be trivial. It appears that the authors assume algorithms rely on confidence intervals and terminate when one of these intervals is fully contained within \(C(x)\). The focus is then on determining the optimal shape of these intervals to minimize the time required for inclusion in \(C(x^)\). However, the definition of the oracle remains imprecise. Typically, in best-arm identification, the complexity is derived from a lower bound on the number of samples required by any \(\delta\)-PAC algorithm. This paper does not provide such a result, which undermines the claim in the introduction that the complexity of the problem is "characterized."
The proposed algorithms are based on allocation strategies from experimental design theory (many results in Appendix B are adapted from [16]). These strategies are computationally intensive and require approximation. The overall computational complexity of the algorithms and its dependence on various parameters are not clearly discussed.
In the numerical experiments section, it is disappointing that the comparisons are limited to the methods introduced in the paper. This makes the section less informative. A natural baseline would have been to compare against algorithms designed for best-arm identification in classic (unstructured) bandit problems, such as LUCB or UGapE. This is particularly relevant given that the experimental setup involves arms that are orthogonal except for two, a scenario closely resembling the classic \(d\)-armed bandit model. Additional commentary on the rationale behind this specific experimental setup would also be helpful.
Minor Comments
- There is a notation overlap: in the paragraph "the setting," \(\Delta\) is defined in two ways, \(\Delta(x)\) and \(\Delta(y)\) (one as a function of vectors, the other as a function of directions). Distinguishing these explicitly would improve clarity.
- In Footnote 1, "used of all direction y" should likely be "for all vector x."
- Line 232 contains a typo: "inthe" should be corrected to "in the."
- In the boxed environment on page 4 (Figure 2), the reference to Eq. 22 should be updated to Eq. 24 on the second line.
- Figures 2 and 3, which define the algorithms, reference equations that appear only in the supplementary material. The main paper should be made self-contained.
In summary, this paper tackles the challenging problem of best-arm identification in linear bandit models. It is novel and incorporates insights from optimal design theory. However, it does not provide a truly generic notion of sample complexity, and the proposed methods appear computationally expensive. Additionally, the experimental results are limited and lack comparisons with established baselines.