This paper's primary contribution lies in demonstrating that combining an identity classification task with a metric-learning-style verification task leads to improved feature learning for face classification and verification. The "verification task" aims to minimize the feature-space distance between instances of the same identity while maximizing the distance between instances of different identities. This approach is integrated into a state-of-the-art face verification system, which employs convolutional networks trained on a large number (400) of different views to extract features. These features are subsequently distilled into a smaller set of 25 through feature selection. The proposed method achieves strong results, with experiments evaluated on the LFW dataset.
Overall, the results are impressive, achieved through a somewhat intricate pipeline. The paper also provides a thoughtful analysis of the contributions of each "task" in the loss function for feature learning. While the idea of combining tasks has been suggested in prior work (e.g., using a siamese network after classification pre-training in DeepFace), this paper explicitly highlights and investigates this combination. The writing could be slightly more engaging and polished in terms of English, but this does not hinder the clarity or comprehension of the paper.
Further comments:
* The selection process for "m" in Equation 1 is unclear. If "m" is learned as a parameter, wouldn't it collapse to m=0 or, at most, the minimum ||fi-fj|| (even with the procedure described in line 152), since this would always result in zero verification error for negative pairs?
 Could the authors provide a more concrete explanation for why including the identification task enhances verification performance and/or feature learning? The explanation of "richer information" feels somewhat vague. For example, could it be that the gradient from the verification task only adjusts the relationship between two points, whereas the softmax gradient from the identification task pushes the correct identity away from the classification-layer templates for all* other identities simultaneously?
* It would be beneficial to report the individual performance of the top-performing network regions/views or to include an evaluation using varying numbers of views (e.g., 1, 2, 4, ..., 25). While the final combination achieves strong results, it would be insightful to understand the contributions of individual components.
* Line 38: The phrase "eternal topic" seems unusual, as it implies the topic will remain unresolved indefinitely. A more appropriate term might be "central topic," which better reflects the ongoing progress in this area.
This is a well-executed and clearly described state-of-the-art system that explicitly investigates the impact of combining two tasks during feature learning. While the contributions feel somewhat incremental, as they build upon existing ideas, this does not detract significantly from the paper's value.