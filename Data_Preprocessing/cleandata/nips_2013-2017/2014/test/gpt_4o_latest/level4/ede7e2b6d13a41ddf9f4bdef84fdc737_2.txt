This paper introduces a novel incremental gradient algorithm inspired by several recently proposed related methods. The work incorporates some modifications to the gradient update rule.
To the best of my knowledge, the fundamental concept of incremental gradient algorithms is to reduce the variance of the proximal gradient to achieve improved convergence rates. From this perspective, the contribution of this work is less compelling, as the convergence rate does not exhibit significant improvement, and the core idea lacks substantial novelty. This is further reflected in the experimental results, where the proposed method demonstrates a convergence speed comparable to other existing methods and does not consistently outperform them.
Additionally, there appear to be minor errors in the proof, such as a potential omission of a scalar eta in the equation on line 324.
That said, this is a complete piece of work with relatively straightforward theoretical analysis. The authors have clearly articulated the relationship between SAGA and other related methods. Nevertheless, the primary contribution of this paper is not particularly impactful.