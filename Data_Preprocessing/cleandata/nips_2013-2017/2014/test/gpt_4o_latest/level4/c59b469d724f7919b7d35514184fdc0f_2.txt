COMMENTS AFTER AUTHOR FEEDBACK
This is a high-quality paper.
Minor comment: The rebuttal did not address the following point:
"1. The definition of neighbourhood is very unclear due to the use of the term 'jointly.' Line 178 ... contradicts the proof of Lemma 8 (Line 576)."
It would be helpful to clarify this in the paper.
---
Disclaimer: The reviewer is not an expert in Learning Theory but specializes in Collaborative Filtering. In evaluating the theoretical significance of the paper, the reviewer defers to the judgment of expert Learning Theorists.
---
MAIN IDEAS OF THE PAPER:
Collaborative filtering is known to work effectively in practice: users who purchase item A often also purchase item B. This paper takes an ambitious step toward providing theoretical guarantees for its performance, albeit under a restricted and somewhat hypothetical setting. The hypothetical framework simplifies reality to make the problem theoretically tractable.
The paper studies a straightforward collaborative filtering algorithm. Here, collaborative filtering identifies a set of neighboring users with the most similar like/dislike patterns (based on cosine distance) and uses their additional like/dislike votes to score items for recommending the next item to a user. The objective is to maximize the expected number of liked items presented to each user within a fixed time window of 1 to T recommendations.
The authors demonstrate that users become "learnable" after approximately log(km) = log(number of user types) + log(number of items) rounds of recommendations (up to log^{7/3}(km), depending on parameter choices). This means that, in time logarithmic in the number of items, the algorithm can achieve near-optimal performance. The analysis assumes certain conditions, such as having a sufficient number of users.
The results hinge on two additional steps in the collaborative filtering algorithm: (1) random exploration and (2) structured exploration. Both are essential—(1) to probe potential future items for recommendation and (2) to ensure sufficient overlap in like/dislike items for learning user similarity.
The experimental results simulate a realistic scenario by streaming a curated subset of the Netflix Prize data to the dataset's users. In these simulations, the average cumulative reward exceeds those of two existing methods (PAF and DM, neither of which, to the best of my knowledge or that of peers, has been widely adopted in practice).
---
RELATIONSHIP TO PREVIOUS WORK:
This paper distinguishes itself by incorporating two exploration steps, one of which explicitly learns user similarity, setting it apart from standard multi-armed bandit approaches. Additionally, no arm can be pulled more than once.
---
QUALITY AND CLARITY:
The paper is exceptionally well-written and clearly explained.
I reviewed most of the proofs in the supplementary material, and they appear technically correct.
For the authors' benefit, I would like to highlight the following textual issues:
1. The definition of neighborhood is unclear due to the use of the term "jointly." Line 178 suggests that only user-item indices recommended during the joint exploration step are eligible for defining neighbors (and hence cosine similarity). Why are other overlapping items not considered in defining the neighborhood? The confusion arises because Line 178 appears intentional, yet Lemma 8's proof (Line 576) contradicts this interpretation. In the proof, "jointly" refers to items rated in common by two users, not just the subset tagged as "jointly" in the algorithm (see Line 157).
2. Section 2, Line 103, and elsewhere: The paper emphasizes that a consumed item (especially a disliked one) should not be recommended to a user again. While this is a valid distinction from standard multi-armed bandit settings, it is a well-known principle in the recommendations community. Is this emphasis intended to highlight the novelty of your approach?
3. Typos:
   - Line 165: Should this refer to \tilde{p}_{ui} instead of index j?
   - Line 524 (Lemma 5's proof): \bar{Z}s = 1 / s^\alpha – Zs would yield a zero mean.
   - Line 610: There is an unnecessary inequality in the proof.
   - Line 634: "user u has is..." appears to have a grammatical error.
---
ORIGINALITY AND SIGNIFICANCE:
To the best of my knowledge, this is the first attempt to characterize learning rates in a (pseudo-)real recommendation setting.
The practical takeaway is that joint exploration is necessary for the algorithm (and potentially more broadly) to achieve optimality in time logarithmic in the number of items.
---
PRACTICAL WEAKNESSES TO CONSIDER:
To derive the main results—specifically, how long and how much one should explore and exploit before achieving optimal performance—the proofs and algorithm rely on several assumptions. While these assumptions provide valuable insights (e.g., the required number of users relative to the item catalog size and tolerance), they may not hold in real-world scenarios. From a practitioner's perspective, the following limitations should be noted:
1. The assumption that users belong to each type with equal odds (1/k) is unrealistic. In practice, user distributions often follow a power law.
2. While random (and joint) exploration is theoretically appealing (and necessary), it poses risks in real-world systems, such as negatively impacting user retention. The algorithm does not account for the possibility of users leaving the system due to poor recommendations during exploration.
3. Based on calibration plots from real systems, condition A1 does not hold in practice. There are pairs of users for whom the odds are essentially a coin flip.
These are not criticisms but rather practical considerations to keep in mind.
---
This is a beautifully written paper that demonstrates users are "learnable" after approximately log(number of user types) + log(number of items) rounds of recommendations, using a simple algorithm. The results rely on several conditions, which simplify real-world complexities, and highlight the importance of structured (joint) exploration for achieving optimal performance.