The paper introduces SAGA, a novel incremental gradient optimization algorithm, and positions it as an improvement over existing methods like SAG, SVRG, and SDCA. The authors claim that SAGA achieves better theoretical convergence rates, supports composite objectives with proximal operators, and is applicable to non-strongly convex problems without modification. The paper also highlights SAGA's adaptability to inherent strong convexity and provides experimental results to validate its effectiveness.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous theoretical foundation for SAGA, demonstrating improved convergence rates over SAG and SVRG and a factor-of-two gap from SDCA. The theoretical results are well-supported with detailed proofs and clear derivations.
2. Novelty: SAGA introduces an unbiased update mechanism that bridges the gap between SAG and SVRG, offering a unified perspective on variance reduction methods. This is a significant contribution to the field of optimization.
3. Practicality: The algorithm is designed to handle composite objectives, a feature lacking in many existing methods. Additionally, SAGA avoids the need for additional regularization in non-strongly convex problems, simplifying its application.
4. Experimental Validation: The experiments on diverse datasets (e.g., MNIST, COVTYPE, MILLIONSONG) demonstrate SAGA's competitive performance compared to other state-of-the-art methods, confirming its practical utility.
Weaknesses:
1. Limited Novelty in Experimental Design: While the experiments validate SAGA's effectiveness, they largely focus on standard benchmarks and do not explore more challenging or diverse problem settings. This limits the scope of the empirical evaluation.
2. Storage Requirements: Although the authors discuss storage optimizations, SAGA's need to maintain a table of gradients may still pose challenges for high-dimensional or memory-constrained applications.
3. Clarity: While the theoretical sections are thorough, the paper could benefit from clearer explanations of key concepts, particularly for readers less familiar with variance reduction techniques. For instance, the connection between SAGA and other methods like Finito/MISO could be more intuitively explained.
4. Acknowledgment of Limitations: The paper does not explicitly discuss the limitations of SAGA, such as potential inefficiencies in sparse settings or the computational overhead of maintaining gradient tables.
Pro and Con Arguments for Acceptance:
Pro:
- Strong theoretical contributions with rigorous proofs.
- Novel algorithm that advances the state of the art in incremental gradient methods.
- Practical applicability to a wide range of optimization problems.
- Competitive empirical performance.
Con:
- Limited exploration of diverse experimental settings.
- Potential storage and computational challenges in certain scenarios.
- Somewhat dense presentation of theoretical material.
Recommendation:
Overall, the paper makes a significant contribution to the field of optimization by introducing a novel and theoretically sound algorithm. While there are areas for improvement, particularly in experimental diversity and clarity, the strengths outweigh the weaknesses. I recommend acceptance with minor revisions to address the clarity and experimental scope.