This paper addresses the critical issue of model identifiability in Bayesian observer models, a cornerstone of perceptual decision-making research. The authors propose a novel framework for systematically testing the identifiability of Bayesian observer models, with a focus on improving experimental design. They demonstrate the utility of their approach through two case studies: time interval estimation and speed perception tasks. The paper makes a significant contribution by offering a theoretical and practical tool to assess and mitigate the degeneracy of Bayesian models, which has long been a challenge in this domain.
Strengths:
1. Novelty and Relevance: The proposed framework is a fresh and much-needed addition to the field. By addressing the degeneracy in Bayesian models, the authors tackle a fundamental limitation that affects the reliability of inferred mental representations.
2. Methodological Rigor: The framework is well-constructed, leveraging a combination of theoretical analysis and efficient computational techniques. The use of KL-divergence to quantify identifiability is both elegant and practical.
3. Practical Applications: The case studies are well-chosen and demonstrate the framework's utility in real-world experimental scenarios. The ability to rank experimental designs by their reliability is particularly impactful for researchers designing psychophysical studies.
4. Clarity of Results: The results are presented with sufficient detail, including visualizations that effectively communicate the findings. The discussion of how experimental constraints (e.g., motor noise, loss functions) affect identifiability is insightful and actionable.
5. Broader Implications: The framework has the potential to influence not only psychophysics but also other fields where Bayesian models are applied, such as neuroscience and machine learning.
Weaknesses:
1. Complexity and Accessibility: While the framework is robust, its complexity may limit its accessibility to researchers without a strong computational background. Simplifying some of the mathematical exposition or providing a more intuitive overview could broaden its appeal.
2. Limited Scope of Case Studies: Although the two case studies are compelling, they focus on relatively specific tasks. It would strengthen the paper to include a broader range of tasks or paradigms, such as multi-cue integration or discrete-choice models.
3. Assumptions in Observer Models: The framework relies on several assumptions about the observer models (e.g., Gaussian noise, specific loss functions). While these are justified, the authors could discuss the potential limitations or biases introduced by these assumptions in greater depth.
4. Experimental Validation: The framework is primarily theoretical, and while the simulations are convincing, empirical validation with real experimental data would significantly bolster the claims.
Recommendation:
This paper is a strong candidate for acceptance. Its contributions to the field are both theoretical and practical, addressing a longstanding challenge in Bayesian modeling. While the complexity of the framework and the limited scope of case studies leave room for improvement, these do not detract from the paper's overall quality and significance. I recommend acceptance with minor revisions to improve accessibility and expand the discussion of limitations.
Pro and Con Arguments for Acceptance:
Pro:
- Addresses a fundamental problem in Bayesian modeling with a novel and rigorous approach.
- Demonstrates practical utility through well-executed case studies.
- Offers actionable insights for experimental design in psychophysics.
Con:
- Framework complexity may limit accessibility to a broader audience.
- Lacks empirical validation with real-world experimental data.
Overall, this paper represents a meaningful advancement in the field and should be of interest to the NIPS community.