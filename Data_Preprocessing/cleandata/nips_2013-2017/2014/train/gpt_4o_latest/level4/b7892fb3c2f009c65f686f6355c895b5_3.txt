This paper primarily extends the Bayesian linear SVM framework proposed in [5] to a nonlinear variant and further integrates the Bayesian nonlinear SVM with factor models. 
The transition from linear to nonlinear is achieved through the straightforward application of standard kernel tricks. However, the resulting nonlinear formulation introduces a more complex inference problem, as it also involves learning the parameters of the kernel function. The integration with factor models is accomplished by combining the two objectives, with kernels being defined over the factor representations.
In terms of model extension and combination strategies, the paper lacks significant novelty. The overall learning framework results in a highly intricate non-convex optimization problem. While some inference procedures are proposed within the probabilistic Bayesian framework, there is no discussion or analysis of the computational complexity of the overall inference process.
The experimental evaluation is limited to Gaussian kernels. Could other types of nonlinear kernels be employed? If so, how would this impact the inference algorithm? Additionally, the datasets used in the experiments are quite small (refer to Table 1), and larger-scale experiments are necessary to validate the approach. Furthermore, the authors only compare their method to SVM and GPC approaches. Given that the tasks addressed in the paper are simple binary classification problems, why not benchmark the method against more advanced state-of-the-art techniques?
The authors frame their work within the broader context of discriminative feature-learning models, which is a highly general topic. However, the related work section does not adequately discuss prior research in this area. While the paper extends prior work from linear to nonlinear models, the experimental results are insufficient to convincingly demonstrate the effectiveness of the proposed approach.