The paper introduces a novel technique for managing "options" (abstract, high-level actions) in reinforcement learning. It asserts that the proposed method is more efficient and offers the benefit of generating models that are independent of the reward function. Furthermore, the paper demonstrates that the technique can be extended to scenarios involving linear function approximation. Experimental evaluations are conducted in two distinct domains.
The findings represent a notable advancement in the utilization of options, which appear to be a natural and potentially powerful framework for addressing complex environments. The paper is exceptionally well-written, and the research is robust and of high quality.
I recommend that the authors include a complexity analysis of their method, focusing on its time and space requirements. Additionally, the paper would be strengthened by a more detailed discussion of any limitations of the approach or contexts in which it might be less effective.  
Important results, exceptionally well-presented.