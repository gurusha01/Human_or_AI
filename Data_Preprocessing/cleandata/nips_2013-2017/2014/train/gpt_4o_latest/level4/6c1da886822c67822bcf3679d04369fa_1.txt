This paper introduces a privacy-preserving mechanism for matrix-factorization-based recommender systems. The proposed approach involves dividing users into two distinct groups: public users, who share all their ratings with the system, and private users, who do not share any information. The authors demonstrate that, under specific technical conditions, it is possible to bound the estimation accuracy of item features based on the number of observed ratings. This estimation, in turn, allows for bounding the reconstruction error for private users. The empirical performance of the proposed method is evaluated using the Movielens 10M dataset.
Privacy concerns in recommender systems are a critical and emerging area of research. To the best of my knowledge, most prior formal approaches rely on differential privacy. This paper, however, considers a scenario where even the recommender engine itself is untrusted, which is a practical and reasonable assumption. Within this context, the authors develop an intriguing framework. The theoretical formalism presented in the paper is clear, and the derivations appear to be sound. The results, specifically the bounds on item factor estimation errors and the subsequent bounds on reconstruction errors, are compelling. However, providing additional intuition for Theorem 3.5 would enhance the clarity of the theoretical contributions.
The paper is well-written, with a clear structure that makes it easy to follow. 
The experimental evaluation is appropriate, demonstrating the proposed method's effectiveness. The comparisons with other methods are reasonably convincing. 
In the first set of experiments, it would be helpful to clarify the meaning of the label "Percentage of Users." Based on my understanding, it refers to the proportion of all users designated as public users. 
In the second set of experiments, the authors use 100 public users and up to 400 private users. It is unclear why results with a larger number of private users (given the dataset contains 10K users) are not reported. What happens when more private users are included? Do any of the differential privacy (DP) methods, such as LAP with Îµ=5, achieve similar performance to PMC and PMD in such scenarios? Was there a specific reason for limiting the evaluation to 400 private users? Additionally, it would be valuable to demonstrate that PMC and PMD perform well in less synthetic settings. Another interesting scenario to explore would be when private users are not necessarily those who have consumed the most items.
Overall, this is a strong paper that introduces an innovative approach to preserving privacy in matrix-factorization-based recommender systems. The theoretical analysis and empirical results are robust and well-supported.