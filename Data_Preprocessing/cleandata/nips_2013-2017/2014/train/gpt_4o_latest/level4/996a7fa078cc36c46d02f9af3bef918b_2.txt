This paper introduces and examines "Universal Option Models," an extension of discounted occupancy functions (Ng & Russell, 2000) to "options" (Sutton et al., 1999)â€”high-level actions characterized by a policy and a state-dependent termination probability function. The authors establish a relationship between the discounted state occupancy function of an option and its rewards (Theorem 1) before exploring the use of options for learning and planning in large state spaces via linear function approximation. The paper provides consistency and convergence results (Theorems 2, 3, and 4) and demonstrates the method's effectiveness compared to a prior option-based approach across two domains.
Overall, the paper is well-motivated, and the main concepts are presented in a clear and logical manner. The experimental methodology is well-documented, and the results demonstrate a significant and meaningful improvement over the method proposed by Sorg & Singh (2010). While I am not deeply familiar with the reinforcement learning literature, this aspect of the paper appears to be a notable strength. However, I find it surprising that the paper does not reference related reinforcement learning work from 2000 to 2010, instead citing only PageRank (2002) and citation network studies (2008) during this period.
My primary concerns pertain to the theoretical aspects of the paper. The term "reward-less MDP" is introduced in Theorems 1 and 2 but is not formally defined anywhere in the text. Relying on a loosely defined concept in quotation marks for a theorem is concerning, as it leaves ambiguity about the scope and applicability of the result. A formal definition and a brief discussion of the concept's significance would strengthen the paper.
The use of the inner product between the vectors \( u^o(s) \) and \( r^\pi \) on line 151 is also unclear. Before Definition 1, \( u^o(s) \) is defined as a vector indexed by states over the second argument of \( u^o \), while \( r^\pi \) is a vector indexed by states. Based on this, my interpretation of the inner product \( (u^o(s))^\top r^\pi \) would be \( \sum_{s'} u^o(s, s') r^\pi(s') \), which does not align with the summation in equation (4) for \( R^o(s) \). The proof of Theorem 1 in the appendix avoids using the inner product notation and instead directly demonstrates that the return of option \( o \) satisfies equation (4). Since the vector form of \( R^o \) does not appear to be used elsewhere in the paper, I do not believe this issue significantly affects the rest of the work.
Theorem 4 references the "Robbins-Monro conditions," but these are neither defined nor cited in the paper. Even if these conditions are widely known in reinforcement learning, it is poor scholarly practice to omit such details when stating a theorem. Additionally, the "proof" of Theorem 4 is more of a sketch than a rigorous argument. Given that the appendix is not subject to a page limit, the authors should provide a complete proof. While the paper's main ideas, analysis, and experimental results are novel, compelling, and impactful, the theoretical contributions are not articulated with sufficient clarity, and the proof of Theorem 4 lacks the necessary level of rigour.