This paper introduces the concept of 'deep' recursive networks by extending depth across spatial dimensions, inspired by the success of similar approaches with recurrent networks. Each layer is parameterized independently, with parameters shared across different heights within the same layer. The authors also incorporate dropout and ReLUs into their model while keeping word representations fixed to pre-trained embeddings. Experiments are conducted on the sentiment treebank dataset by Socher et al.
The proposed approach is innovative, though it can be viewed as a generalization of the methodology applied to recurrent networks. The authors report impressive performance on the sentiment treebank dataset, and the observed performance gains from using dropout and ReLUs are noteworthy. However, certain modeling and experimental design decisions are either insufficiently justified or raise questions, and I believe additional experiments are necessary before this work can be recommended for acceptance.
The paper is generally well-written, with sufficient detail provided to allow readers to replicate the results.
Detailed Comments and Questions:
- Line 138: The definitions of \( W^\eta \) and \( W^{xh} \) are unclear. Are these shorthand for \( W^{\eta}L \) and \( W^{\eta}R \)? It would be clearer to present two separate equations: one for the inputs and another for the hidden layers. Additionally, summarizing the parameter space (e.g., \( \{WL, WR, U, b, c\} \)) and specifying the dimensionalities of the matrices upon definition would improve clarity.
- Lines 148–154: This section appears speculative. Are these claims based on empirical results? If so, this should be explicitly stated. Otherwise, I am not convinced this is a significant issue.
- Baseline Comparison: The authors should include results from "A Convolutional Neural Network for Modeling Sentences" (Kalchbrenner et al., ACL 2014) as a baseline for comparison.
- Word Vector Fine-Tuning: Why were the word vectors not fine-tuned? Fine-tuning on a sentiment task allows word embeddings to better capture sentiment-specific information. It seems counterintuitive to have phrase and sentence vectors that are sentiment-discriminative while individual word vectors are not. While this might be acceptable for a deep RNN (since word vectors are processed through multiple layers), it raises the question of whether the observed improvements with deep RNNs are due to the lack of fine-tuning. A single-layer RNN with fine-tuned embeddings might perform better. This should be controlled for in the experiments.
- Line 251 (Shared Dropout Units): What motivated the choice to use shared dropout units?
- Lines 255–256 (Stability): Instead of the current approach, you could consider constraining the weight norms or truncating gradients, as is commonly done with recurrent networks.
- Binary Classification: Binary classification experiments should be conducted separately. Including neutral class examples during training deviates from the standard experimental protocol, making the results incomparable to prior work.
- Analogy with Deep Recurrent Nets: The analogy to deep recurrent networks is reasonable when predictions are made at each node in the tree, as in the treebank experiments. However, it would be valuable to evaluate whether deep RNNs offer an advantage when only a single, root-level label is used. Would you still expect deep RNNs to outperform a single-layer RNN in this scenario? While the proposed approach is promising, additional experiments are needed to strengthen the claims. Specifically, I recommend redoing the binary classification experiment following the correct protocol, investigating the impact of fine-tuning word embeddings, and evaluating the method on 1–2 additional datasets with only global labels. These additional experiments would significantly enhance the paper and provide stronger evidence for the utility of depth across spatial dimensions.