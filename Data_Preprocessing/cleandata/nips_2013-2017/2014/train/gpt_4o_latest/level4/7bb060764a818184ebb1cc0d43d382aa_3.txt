This paper introduces a multi-class extension to the recently proposed ensemble learning algorithm, DeepBoost, and establishes new data-dependent bounds for convex ensembles within a multi-class classification framework.
The manuscript is well-written, and the technical derivations appear to be correct. Notably, it provides data-dependent bounds that are tighter than existing ones due to their explicit dependency on the mixture weights in the convex combination. A significant contribution is the special case of the bound derived in Theorem 1, which results in a linear dependency on the number of classes. This is a substantial improvement over the existing bounds by Koltchinskii and Panchenko, where the dependency is quadratic. Consequently, the derived bound is particularly advantageous for scenarios involving a large number of classes. Additionally, the authors propose optimization objectives and multi-class DeepBoosting algorithms that demonstrate strong performance on UCI datasets when compared to the multi-class versions of Adaboost and logistic regression.
However, certain aspects of the paper require further elaboration and clarification, particularly regarding its relationship to the existing work on Deep Boosting presented at ICML 2014. The proposed approach appears somewhat incremental, as the proof techniques, objective function design, and developed algorithms bear significant similarity to the earlier work. This overlap limits the novelty of the current contribution to some extent. Another point of concern is the statement in Line 143 (Section 3.1) that the generalization error of \( f \) and \( f / \rho \) is the same. Based on the interpretation of \( \rho \) as the margin from the definitions in Equation (2), it is not immediately evident why these two would yield identical generalization errors. Furthermore, it seems that the authors may be using test set labels to tune the hyperparameters of the algorithm. In contrast, the ICML paper on binary classification employs a separate validation set for parameter selection, rather than relying on the test set. The authors should clarify this aspect of their experimental setup.
+ The paper provides data-dependent bounds that are tighter than existing ones due to their explicit dependency on the mixture weights in the convex combination.  
+ The special case of the bound derived in Theorem 1 results in a linear dependency on the number of classes, which is a notable improvement over existing quadratic dependencies.  
+ The authors propose optimization objectives and multi-class DeepBoosting algorithms that perform well on UCI datasets.  
- The work is somewhat incremental compared to the Deep Boosting paper presented at ICML 2014.  
- Certain points require clarification, including the experimental setup and the clear novelty relative to the ICML 2014 paper.