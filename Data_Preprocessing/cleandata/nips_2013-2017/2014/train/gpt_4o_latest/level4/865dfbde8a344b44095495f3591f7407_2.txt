The paper introduces stochastic variational inference (SVI) tailored for hidden Markov models (HMMs). Unlike latent Dirichlet allocation (LDA), the chain-based dependency structure in HMMs creates challenges for SVI. Specifically, subsampling during each iteration cannot assume independence among sequential data, and omitting data outside the chain risks introducing errors. (In contrast, network models face different challenges, where pairwise dependencies replace chain-based ones. Subsampling in such settings has been addressed in prior work (e.g., Gopalan et al. (2012)).)
In this study, the authors propose a method to scale noisy subsample gradients for unbiased estimates and to bound the error caused by updating only variables associated with subsampled subchains. To achieve this, they augment subsampled subchains with additional observations. They argue that this buffering mechanism, combined with incremental forward-backward computations, ensures computational efficiency. The proposed algorithm is applied to a large genomics dataset, where it achieves comparable performance to variational Bayes (VB).
Pros:  
- The paper is well-written, and the contributions—developing an SVI algorithm for HMMs—are both clear and significant.
Cons:  
1. The primary limitation of this work is the lack of comparison with Johnson et al. (2014), which introduces an SVI algorithm for large collections of independent time-series. While the assumptions in the current paper are more appropriate for chain-based dependencies, it remains unclear whether the added complexity is practically justified when tested on real-world data. A comparison with an expectation-maximization (EM) algorithm alone is insufficient, as the observed gains may stem from using SVI rather than the specific innovations in this work.  
2. While the authors claim "significant gains" in computational efficiency for the genomic dataset compared to the EM algorithm, no runtime results are provided to substantiate this claim.  
3. The paper does not explicitly discuss the computational complexity of the batch algorithm versus the SVI algorithm, even though it is evaluated. Presumably, the SVI algorithm still incurs quadratic per-iteration complexity with respect to the number of states.  
4. The worst-case computational cost when the subchain length (L) is poorly chosen is not addressed. The algorithm depends on selecting appropriate subchain lengths (L) and the number of chains per mini-batch, which is a common challenge in SVI algorithms (e.g., Hoffman et al. (2010)). However, the choice of L is particularly critical here, as an unsuitable value could lead the buffering mechanism to include excessive observations, undermining the approximation quality. While the authors claim this is not an issue, their evidence is limited to simulated data, which is not entirely convincing.  
In summary, the paper provides a comprehensive development of SVI for HMMs, respecting the chain-based dependencies inherent in long time-series data. Previous work has either addressed different dependency challenges, such as those in network data, or focused on collections of independent time-series. The authors demonstrate improvements over the EM algorithm on a large genomics dataset and analyze algorithmic decisions using simulated data. However, a key limitation is the absence of a comparison with prior SVI methods, such as Johnson et al. (2014), for independent time-series data.