The authors introduce a regression framework grounded in locally weighted regression (LWR). By establishing connections between LWR and Gaussian process (GP) regression, they derive a probabilistic model. While the model initially exhibits scalability challenges akin to a GP (cubic in \(N\)), a variational approach mitigates these issues. The variational updates closely resemble the LWR procedure, thereby restoring the algorithm's scalability.
Clarity  
The paper is exceptionally well-written, with clear exposition, consistent and simple notation, and sufficient detail that avoids overwhelming the reader. The connection drawn between LWR and GP regression is particularly elegant. While it is well-understood that each individual model in the LWR framework can be viewed as a GP (as any linear Gaussian model), the interpretation of the entire methodology in this light appears novel.
However, one section of the paper—Section 4—lacks the clarity of the rest. While the authors may argue that space constraints are a factor, this section feels somewhat anecdotal compared to the otherwise rigorous presentation. The inclusion of the algorithm is somewhat helpful in this regard.
Quality and Significance  
The paper is technically robust, addresses a topic of interest to a broad segment of the NIPS community, and includes solid experimental results. The experiments are well-chosen, tackling interesting challenges, and the proposed method demonstrates notable improvements in speed while maintaining accuracy.
That said, I would strongly encourage the authors to mention the availability of the implementation, as this would further enhance the paper's impact.
I do have one significant concern regarding the quality of the results: the probabilistic nature of the proposed algorithm is not adequately explored in the experiments. For instance, I would have liked to see the average log-density of held-out data reported alongside the mean squared error (MSE). While the LWR method may not provide probabilistic estimates, both the proposed method and SSGPR do. In robotics or other decision-making environments where uncertainty is critical, log \(p(y^*)\) is arguably a more informative metric than MSE. It is well-documented that different GP approximations can vary significantly in terms of predictive density (e.g., FITC often yields conservative predictive densities). Could the authors include a supplementary table reporting log-density scores?
Queries  
1. To make the variational approximation tractable, uncertainties are introduced via the parameters \(\beta\). In practice, what values did these parameters converge to? Does this slight modification to the model significantly impact its performance?  
2. While the variational updates for the local models are independent, the \(\beta\) parameters are global. Does this introduce computational overhead? Do you interleave fewer global updates with the local updates to manage this?  
3. In Table 2, the SSGPR method is pre-trained with 200 features, whereas the LWGPR method utilizes approximately 500 local models. Would it be fair to claim that the two methods are of comparable complexity? Could the SSGPR method not perform better with more than 200 features? My understanding is that SSGPR scales cubically with the number of features, so it seems feasible to include more. The discrepancy in Table 3 is even more pronounced, likely due to the offline-online differences in the procedures.
Summary  
This is a well-written and engaging paper. While I have a few technical questions, my overall score may change based on the authors' rebuttals. The paper excels in presentation, tackles a relevant topic, and includes strong experiments, but it is somewhat weakened by the absence of probabilistic metrics in the results.