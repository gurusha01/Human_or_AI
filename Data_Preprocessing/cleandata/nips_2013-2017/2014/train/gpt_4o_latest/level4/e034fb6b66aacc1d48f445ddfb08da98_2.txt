The authors enhance log-bilinear language models by substituting the conventional two-way interactions (matrix) with three-way interactions (tensor), which are subsequently factorized. In essence, the authors replace the energy function with a factored energy function, a technique commonly employed when converting RBMs into factored RBMs.
The concept of using factored models has been extensively studied across various tasks and applications. However, the authors provide several compelling examples of applying factored log-bilinear models to language-related tasks, including context-sensitive language modeling, sentiment classification, cross-lingual document classification, blog authorship attribution, and conditional word similarity.
What stood out to me the most in these applications was the selection of attributes. On the other hand, I was underwhelmed by the models' performance on quantitative tasks. The authors suggest that a more thorough hyper-parameter search could reduce this performance gap, though I find this claim speculative. It is also plausible that the relatively small datasets used in these experiments necessitate either stronger regularization techniques or significantly larger datasets for models with more parameters.
The experiment on blog authorship attribution makes a strong case for broader adoption of factored log-bilinear models in language tasks traditionally handled by log-bilinear models. Additionally, the experiment on conditional word similarity is particularly well-executed and noteworthy.
Overall, the paper explores the application of factored log-bilinear models across a diverse set of language tasks and partially demonstrates the utility of context variables. The paper is well-written, engaging, and clear.