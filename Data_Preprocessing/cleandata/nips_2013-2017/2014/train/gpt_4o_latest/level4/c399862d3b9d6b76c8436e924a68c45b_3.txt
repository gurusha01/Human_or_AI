In this paper, the authors introduce a novel algorithm for Bandit Convex Optimization (BCO) with strongly-convex and smooth loss functions, employing an exploration scheme that diminishes over time. They demonstrate that the proposed algorithm achieves a regret of \tilde{O}(\sqrt{T}) (see Theorem 10), which aligns with the best-known lower bound up to logarithmic factors. The technical analysis presented in the paper appears to be correct.
The contribution of this work is noteworthy as the authors not only establish a tighter regret bound for BCO with strongly-convex and smooth loss functions (improving from \tilde{O}(T^{2/3}) to \tilde{O}(T^{1/2})), but also achieve a regret bound that matches the lower bound established in (Shamir 2013). Therefore, I recommend acceptance of this paper. However, I suggest marginal acceptance rather than strong acceptance due to the following issues with the paper's presentation:
1) The paper contains numerous lemmas, but it is unclear (without referring to the appendix) which lemmas are novel contributions and which are pre-existing results. This lack of clarity makes it somewhat challenging to assess the technical significance of the work.
2) The authors dedicate excessive space to reviewing prior results, with the main contribution of the paper only beginning near the end of page 6.
3) Some symbols in the paper are not properly defined, such as \delta in Eqn(4). Additionally, certain results are not clearly stated; for example, the result of Lemma 9 should hold for any \omega, but this is not explicitly mentioned by the authors.
4) Algorithm 2 requires revision: the algorithm should not include a loop, and \Nabla ht (xt)'s should be treated as inputs to the algorithm.
While the results of the paper are significant, the quality of writing and organization needs improvement, and the paper would benefit from a thorough rewrite.