The authors present a CNN architecture that incorporates top-down processing through attention-based feedback mechanisms. Their results on the CIFAR datasets demonstrate state-of-the-art performance.
This paper is engaging and introduces novelty in both model design and its application to vision tasks. It is forward-thinking in its approach to integrating both bottom-up and top-down signals for training a CNN. I particularly appreciate the evaluation of the model over time and the use of gated signals to constrain the learned representations. However, since the quality of the learned representations depends heavily on the attention policy, it would be more compelling if the authors could provide examples or discuss scenarios where the algorithm might fail.
The experiments are well-executed, with the authors employing various techniques, such as visualizations, to enhance the interpretability of the model. That said, given the increased complexity of the proposed method compared to traditional feedforward CNNs, the paper would benefit from results on ILSVRC to demonstrate its scalability to larger datasets with higher-resolution images. This is my primary concern and suggestion for improvement. The combination of attention-based feedback with CNNs is an intriguing direction, and further exploration, particularly on larger datasets, would provide more practical insights and strengthen the work.