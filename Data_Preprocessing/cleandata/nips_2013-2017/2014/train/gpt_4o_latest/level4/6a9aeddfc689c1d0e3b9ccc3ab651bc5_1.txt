This paper explores alternatives to least squares (LS) for addressing linear regression problems. Traditional statistical methods often employ a score function to weight observations, typically capturing only an additive error that deviates from Gaussian assumptions. In contrast, this paper incorporates the concept of point influence. While statisticians have long excluded influential outliers, as far as I can determine, this is the first automated procedure that leverages influence to systematically down-weight observations in regression.
The probabilistic model introduced in this work is not the central focus of the paper. Specifically, identifying contaminated versus clean points is relatively straightforward, particularly in high-dimensional settings (large p), where the two distributions are clearly separable. Consequently, if the proposed estimator were strictly tied to this model, the results would be less compelling. However, the estimator itself is quite intuitive and natural, with the model serving primarily as an illustrative example.
1) Include a benchmark line indicating the accuracy of the maximum likelihood estimator (MLE) for your probabilistic model. While your method does not assume the model, this line would still provide a more meaningful comparison than the LS baseline. If solving the deconvolution problem is too complex, you could assume that the MLE has access to the random variables U, which would provide a lower bound on its performance. As p → ∞, this lower bound should become increasingly tight.
2) Connect your work to the existing literature on scoring functions, which dates back to the early 1970s. For instance, the aRWS approach can be interpreted as down-weighting points by a factor of 1/ε². If the weighting were instead 1/ε, it would correspond to L1 regression, analogous to assuming a double exponential error distribution. Your weighting function is even more stringent, so it would be insightful to determine which error distribution it corresponds to in the context of scoring functions. (It might approximate a Cauchy distribution, but only loosely.) Note that your approach involves sampling, whereas traditional methods focus on weights. If your 1/ε² sampling is equivalent to a 1/ε weight, this would approximate L1 regression—a connection worth highlighting if true.
3) Are you claiming to be the first to propose weighting by 1/influence? While this may be accurate, it is a bold assertion and should be supported carefully.
4) It seems your method uses distinct regression functions for "large observations" versus "small observations." To test this and provide a more direct comparison with LS, you could define tilde-l as you currently do but interact this variable with all the X's in your regression. This would allow LS to incorporate tilde-l, enabling it to estimate different slopes for pure X observations versus X+W observations, potentially improving its performance.
5) There is a substantial body of literature on errors-in-variables and robust regression, with over 1,000 papers in these areas. Incorporating connections to at least some of this work would strengthen your claims and provide context for the novelty of your methods.
6) Please review the following 1980s NBER paper:  
   [http://www.nber.org/chapters/c11698.pdf](http://www.nber.org/chapters/c11698.pdf)  
   Pay particular attention to equations (15) and (16), which closely resemble your estimator. It would be beneficial to explore modern references stemming from this work to identify any relevant connections to your approach.
This paper introduces a novel alternative to robust regression by down-weighting observations based on their influence. The authors present a useful theorem and evaluate promising alternatives in the empirical section.