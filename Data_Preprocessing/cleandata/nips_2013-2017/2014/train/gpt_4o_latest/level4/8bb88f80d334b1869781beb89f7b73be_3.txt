Review - Summary
This paper explores multiple variations on the theme of training a deep network using data generated by a Monte-Carlo Tree Search (MCTS) agent. The study is conducted on the Atari 2600 platform and is motivated by the observation that, while MCTS achieves exceptional performance on Atari 2600 games, it is computationally prohibitive for practical use. The authors present empirical results across several Atari 2600 games.
Overall
The primary contribution of this paper, in my view, is the proposal to "compile" the UCT value function (or policy, depending on the algorithm) into a deep network. The paper is well-written, and the results are of good quality, but the work lacks sufficient significance. While the results are promising—showing performance improvements on all but one game—the exploration of the topic feels incomplete. There is limited insight into how these findings might generalize to other domains or lead to meaningful algorithmic advancements.
Major Concerns
- The UCC-I algorithm described in the paper bears a strong resemblance to the DAgger algorithm proposed by Ross et al. (2011). A discussion of this relationship would be beneficial. Does this approach offer better sampling guarantees compared to DAgger?  
- Are UCR/UCC truly competitive with prior learning approaches? These methods still require full access to the simulator, which limits their applicability.  
- The reliance on game-specific tuning for the empirical results diminishes their overall impact. Would the results differ significantly if a uniform parameterization across environments were used?  
Suggestions for Improvement
There are several research directions that could enhance the paper:  
- Investigate further how effectively a policy or value function can be distilled into a deep network. Your results suggest that summarizing the policy is more effective—does this reveal something fundamental about Atari 2600 environments?  
- Consider performing regression on the empirical return rather than the UCT value. Is there a theoretical or practical advantage to one approach over the other?  
- The UCT agent does not utilize "features," but such features may emerge during the "policy compilation" process. A related concept was explored by Cobo et al. (2013) in learning features from expert data. Could this be examined in your context?  
- How is partial observability addressed in your approach? While the optimal stimuli plots provide some hints, this remains an underexplored aspect of the work.  
Minor Points
- Line 120: The phrase "two broad classes of approaches" implies that no other approaches are valid. Consider rephrasing for clarity.  
- Lines 136-143: The related work section would benefit from a discussion of how non-deep network features differ from the proposed approach. For instance, is the primary distinction the use of learned visual features or the adoption of a deep architecture?  
- Line 136: Citation [4] appears to be incorrect in the bibliography. Other citations also seem misnumbered.  
Conclusion
Overall, this paper presents an interesting idea, but it lacks a central thesis or cohesive theory to guide the investigation. While the results are promising, they require deeper exploration to justify acceptance.