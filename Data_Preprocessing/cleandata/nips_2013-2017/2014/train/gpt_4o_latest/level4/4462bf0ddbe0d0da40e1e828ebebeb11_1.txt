This paper focuses on the task of learning kernel matrices for use in determinantal point processes (DPPs). The primary contribution lies in a reformulation of the optimization objective, which surpasses the conventional approach of directly differentiating the log-likelihood with respect to kernel parameters and applying projected gradient ascent (referred to as K-Ascent (KA) in the paper). The proposed method reimagines the problem within an Expectation-Maximization (EM) framework. The key insight is that any DPP can be expressed as a mixture of "elementary DPPs," where each subset of elements corresponds to an elementary DPP. A variational distribution is introduced over these subsets, and the algorithm alternates between updating this distribution—represented as a k-DPP—in the E-step and performing single gradient updates in the M-step. Experimental results demonstrate that the EM approach significantly outperforms the KA method when KA is initialized naively and achieves modest improvements in certain scenarios when KA is initialized more effectively.
Overall, I find the proposed approach to be interesting, and the EM method does appear to provide some performance gains. However, the M-step is quite intricate, and the approximations and caveats required to make it functional are somewhat unsatisfying. That said, it is commendable that the authors managed to get the method to work effectively.
From an experimental perspective, I would have appreciated a deeper analysis of the impact of initialization on the EM algorithm. For instance, if the EM method were initialized using the output of KA, would it lead to further improvements over the baseline?
Additionally, I found the use of relative log-likelihood as a reporting metric somewhat confusing. Log-likelihood differences inherently represent probability ratios, so it is unclear why ratios of log-likelihoods are considered instead of straightforward differences. This choice makes it challenging to interpret the reported values.
On a minor note (apologies for the nitpicking!), I found certain aspects of the writing style slightly off-putting. For example, I would recommend avoiding self-descriptions of the algorithm as "elegant," particularly when the elegance is debatable given the approximations and caveats in the M-step. Similarly, bold text in the introduction feels unnecessary. 
In summary, this is an interesting and novel approach to learning kernels for DPPs. While the experimental evaluation could be strengthened, and the improvements over the baseline KA method are not particularly dramatic, the paper is overall a solid contribution.