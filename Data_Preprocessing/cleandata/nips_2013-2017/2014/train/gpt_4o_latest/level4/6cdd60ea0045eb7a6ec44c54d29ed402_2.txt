The manuscript introduces a novel approach for robust logistic regression, with a primary emphasis on handling high-leverage outliers. The authors assume that outliers originate from an arbitrary and unknown distribution, but they also assume that the number of outliers is known in advance. While this assumption is theoretically convenient, it may pose practical challenges.
The proposed method involves maximizing the sum of y*x'beta, but only over the observations that have the smallest absolute contributions to the objective function, thereby excluding high-leverage outliers. This formulation is elegantly recast as a linear programming problem.
The authors derive risk bounds for their method. As is typical with such bounds, they are likely too loose for practical quantitative application but remain valuable for qualitative insights.
The manuscript includes a small simulation study, demonstrating that the proposed method performs significantly better than classical logistic regression.
Detailed comments:
- Why is the preprocessing step necessary? Observations with large ||x|| should naturally correspond to those with large y*x'beta and would thus be excluded by the algorithm. Additionally, T decreases with n (approaching 0 asymptotically), meaning that more observations are discarded as the dataset grows. For instance, in the simulation, what proportion of observations is removed during preprocessing? Furthermore, it is surprising that T is not selected based on n or lambda.
- Relatedly, it would be interesting to examine how classical logistic regression performs when subjected to the same preprocessing step.
- Given the preprocessing and the method's design, scenarios where outliers exhibit much larger variance in the covariates (as in the presented simulation) are particularly advantageous for the proposed method. Such outliers are likely to satisfy the preprocessing criterion or contribute significantly to the objective function, leading to their exclusion. However, what happens when sigma_o is reduced, making outliers less distinguishable and more likely to fall within the n smallest contributions? Does the method's performance improve or deteriorate in such cases?
- A critical question is how the method compares to logistic regression in the absence of outliers. The far-left portion of Figure 2 suggests minimal differences in beta estimation but a more pronounced gap in misclassification rates. What is the trade-off for achieving robustness in such scenarios?
- In practice, how should n be chosen? Additionally, how sensitive is the method to variations in n?
The manuscript is well-written and clearly organized. It presents an innovative method for robust logistic regression, primarily targeting high-leverage observations. The aspect I found most compelling is the transformation of the problem of identifying the least-contributing observations into a linear programming framework.