This paper introduces a framework for learning with prior constraints on parameters and applies it to sparse learning, incorporating prior knowledge about the cardinality of the parameter support.
* Although the paper provides a novel perspective by enforcing parameter constraints through projection in KL-divergence, it does not yield significant theoretical advancements or novel algorithms.
* The presentation of the proposed algorithm lacks clarity. Including an outline, such as pseudocode, would improve comprehensibility.
* The experimental results indicate that the proposed method (Sparse-G) outperforms other approaches; however, the underlying reasons for this improvement are not adequately explained. Additionally, the observation that Spike-and-slab performs significantly worse than Lasso contradicts existing literature. This discrepancy might stem from the choice of hyper-parameter settings.
* The writing is generally clear, but there are areas for improvement:
  - The concept of "structure" requires a more detailed explanation.
  - Line 310, which states "... spike and slab does not return sparse estimates ...," is misleading, as spike-and-slab is a well-established prior for sparse learning.
In summary, while the paper introduces an interesting framework for learning with prior constraints via KL-divergence projection, it falls short of providing substantial theoretical or algorithmic contributions.