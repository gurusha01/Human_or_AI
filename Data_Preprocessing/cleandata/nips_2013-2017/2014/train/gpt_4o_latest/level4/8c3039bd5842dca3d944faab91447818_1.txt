This paper presents the concept of deep Gaussian mixture models (Deep GMMs). A Gaussian mixture model (GMM) can be interpreted as comprising a single isotropic unit norm Gaussian, with each mixture component resulting from applying a distinct linear transformation to that Gaussian. The authors extend this idea to a multilayer network, where each node corresponds to a linear transformation, and each path through the network represents a sequence of such transformations. Consequently, the number of mixture components corresponds to the total number of paths through the network.
The paper is well-written, and the proposed idea is both straightforward and compelling. Overall, I found the paper enjoyable. However, I was disappointed by the low-dimensional nature of the experimental results, especially given the significant effort dedicated in the paper to discussing the computational tractability of the algorithm.
Additionally, the proposed algorithm appears to be closely related to the work by Tang, Yichuan, Ruslan Salakhutdinov, and Geoffrey Hinton titled "Deep mixtures of factor analysers" (International Conference on Machine Learning, 2012). This prior work warrants citation and discussion in the paper.
I strongly recommend that the authors release the source code for their experimental results as supplemental material. Doing so would enhance the paper's credibility, increase its citation potential, and promote a culture of reproducible science, which benefits the broader research community.
Detailed comments are as follows:
- Line 39: Replace "in function" with "as a function."
- Line 73: Replace "A3,1" with "A1,3."
- Line 89: While the arguments in lines 312–318 effectively justify why this is true for the proposed algorithm, I am skeptical that EM is generally more parallelizable than stochastic gradient descent. Consider rephrasing this claim more specifically.
- Line 139: The network has an exponential number of paths, and assigning probabilities to each without factorization would quickly become computationally infeasible, even for relatively small networks. I am surprised that factorization is not required almost universally.
- Line 199: This is commonly referred to as the MAP approximation. Why not instead sample a single nonzero gamma with probability proportional to pi? This approach corresponds to EM, with the E-step represented as a sample from the posterior. Unlike the MAP approximation, this would yield an unbiased estimate of the learning gradient and likely result in a higher log-likelihood model.
- Line 230: Replace "in function of" with "as a function of."
- Line 235: The plot indicates that the method never converges to the optimum.
- Line 251: Replace "and scalable." with "and in a scalable way."
- Lines 307, 341: If you are interested in quasi-Newton optimization with minibatches and without hyperparameters, consider exploring [https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer](https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer).
- Lines 312–318: This section is well-done and provides strong evidence for the advantages of EM.
- Line 331: Replace "have to construction" with "have to construct."
- Line 337: Replace "with using" with "using."
- Line 344: I strongly suspect that this behavior is a consequence of the MAP approximation to the posterior, rather than incomplete optimization during the M-step. (As a side note, EM can be viewed as maximizing a lower bound on the log-likelihood. Notably, this lower bound increases even if the M-step is not fully optimized.)
- Figure 4: The X-axis for Deep GMMs does not seem meaningful, as there is nothing inherently special about the number of components in the top layer. Using the number of parameters as the X-axis might be more appropriate.
- Line 436: The author name is listed as "anonymous."
In summary, the idea presented in the paper is simple, well-motivated, and clearly articulated. Overall, I liked the paper, but I found the experimental results to be unexpectedly weak.