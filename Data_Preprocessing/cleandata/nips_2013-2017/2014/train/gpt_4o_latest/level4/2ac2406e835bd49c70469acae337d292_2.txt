This paper introduces a spectral algorithm for learning a mixture of hidden Markov models (MHMM). It demonstrates that an existing spectral algorithm for learning HMMs (Anandkumar et al., 2012) can be directly applied by reformulating an MHMM as a specific instance of an HMM. The authors address the parameter permutation issue by proposing a stabilization-based approach, which relies on the assumption that each HMM's transition matrix has a single eigenvalue equal to 1.
However, there are some concerns regarding the notation used in the paper. The authors should improve their discussion of related work by including references (e.g., initializing EM with spectral estimates has already been explored in Chaganty and Liang (2013)). The experimental evaluation is fairly limited, and I strongly suggest presenting results in a tabular format instead of the graphical representation in Figure 4.
Despite these issues, the proposed stabilization technique to handle parameter permutation is both interesting and intuitive. Additionally, the authors provide an analysis for scenarios involving estimation noise, which introduces a compelling use of eigenvalues (Equation (7)) in the algorithm.
Comments:
- The terms in Corollary 1 were initially unclear. It would be helpful to explicitly define \( 1J^T \) (as a row vector of length \( J \) filled with ones) and elaborate on the structure of \( \lim{e \to \infty} \bar{A}^e \) (noting that it is a \( JK \times JK \) block diagonal matrix where the columns within each \( J \times J \) block are identical).
- In general, please define each vector/matrix along with its dimensions when it is first introduced. This will significantly improve the clarity of the presentation.
- Figure 4 is difficult to interpret; please replace it with tables. Additionally, the poor performance of EM in Table 1 is puzzling. Typically, even with random initialization, EM surpasses spectral methods after sufficient iterations (despite the local optima issue). It is standard practice to report the best (not average) accuracy of EMâ€”please include this result.
- Minor suggestion: Adopting the established notation from Hsu et al. (2009) could improve readability. Specifically, use \( T \) for transition matrices (instead of \( A \)) and \( n, m \) for the number of observation and hidden states (instead of \( L, J \)).
In summary, the paper proposes an algorithm for learning a mixture of hidden Markov models. This approach combines an existing spectral algorithm for learning HMMs with a novel stabilization step to resolve parameter permutation issues.