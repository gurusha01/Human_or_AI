Paraphrased Review
Summary:  
This paper introduces an influence reweighted sampling method (IWS-LS), along with a residual weighted sampling method (RWS-LS), for solving large-scale least squares problems. The proposed methods are designed to be robust against certain types of data corruption, such as specific sub-Gaussian additive noise. The authors utilize existing approximation techniques to compute the ordinary least squares (OLS) estimate and leverage scores. They provide a theoretical analysis of the estimation error for IWS-LS and present empirical evaluations on both synthetic and real-world datasets.
Comments:  
Overall, the paper is well-written and clear. The influence reweighted subsampling method proposed by the authors is novel, and both the theoretical analysis and empirical results appear to be valid and reliable.
Regarding the experimental setup, the dataset with 100,000 samples in a p=500 dimensional space cannot be considered particularly large-scale, especially since the real-world Airline Delay dataset used is even smaller. Given the computational complexity of O(n pÂ²) for calculating the OLS estimate and leverage scores, these computations should be feasible on a standard desktop, even if they require several minutes or hours. Therefore, it would be beneficial to include the results of exact methods as a baseline for comparison. Additionally, details about the experimental environment should be provided. Since time efficiency is a critical factor in large-scale learning, the authors should also report the running time of their methods.
The paper focuses on developing estimators that are robust to outliers caused by data corruption. However, it would be valuable for the authors to discuss related work that leverages data corruption (or "nosing") to enhance classification or regression performance. Relevant recent studies include dropout training for deep neural networks [1,2] and learning with marginalized corrupted features [3,4], among others.
References:  
[1] G. Hinton et al., Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint.  
[2] S. Wager, et al., Dropout training as adaptive regularization. NIPS, 2014.  
[3] L. van der Maaten, et al., Learning with marginalized corrupted features. ICML, 2013.  
[4] N. Chen, et al. Dropout Training for Support Vector Machines, AAAI, 2014.  
Lastly, a few typographical errors should be addressed. For instance, on line 42, the phrase "more more realistic" should be corrected, and on line 254, the reference "ideas from $4 and $4" requires revision.  
In conclusion, the paper is well-executed, and the proposed influence reweighted subsampling method is a meaningful contribution. Both the theoretical and empirical findings are compelling.