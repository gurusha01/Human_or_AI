The paper introduces a modification to the SVM learning algorithm designed to handle weakly supervised data. Specifically, it assigns a per-example weight (ranging between 0 and 1) to each training instance and adjusts the SVM loss function to incorporate this weight. The approach is further extended to scenarios where the per-example weight is unobserved and treated as a latent variable.
The method is clearly presented, and the experimental results suggest that, for tasks such as semantic segmentation and object detection, this approach may offer advantages over treating all examples with equal weight. However, I find it difficult to distinguish the proposed method from existing techniques involving "example-dependent costs," which are well-established in the machine learning community. For instance, SVMlight already includes a mechanism for handling such costs.
The extension to cases where the per-example weight is unobserved is intriguing but relatively straightforward. In this scenario, the problem becomes non-convex, and no theoretical analysis is provided. The use of a strong regularization term implies that the per-example weights remain close to their initial values. Could the authors provide further clarification on this point?
Minor comment: The statement on L050 that "it is also not a standard regression problem since the positiveness belongs to a bounded interval [0, 1]" is misleading. Logistic regression, which is a standard approach, is also bounded within [0, 1].
Minor typo in the abstract: "continues" should be corrected to "continuous."  
In summary, the paper proposes a modified SVM learning algorithm that incorporates per-example weights into the loss function. However, the concept of example-dependent costs is already well-known and widely applied in machine learning.