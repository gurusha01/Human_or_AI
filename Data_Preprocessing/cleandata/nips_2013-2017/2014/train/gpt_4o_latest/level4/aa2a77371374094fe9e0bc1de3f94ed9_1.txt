The authors address the problem of interpolation (cokriging) and forecasting on spatio-temporal tensor data. They demonstrate that both tasks can be formulated as a (non-convex) low-rank tensor optimization problem and propose a learning method that integrates forward greedy selection with an optional orthogonal projection step. Their experiments include simulated data, two climatological datasets, and a dataset of FourSquare check-ins.
While I do not consider myself an expert in this specific domain and am therefore unable to assess the novelty or rigor of the presented proofs, I find the paper to be clearly written. The proposed greedy approach to low-rank learning appears reasonable, and the experimental results demonstrate clear improvements over competing methods. Overall, this seems to be a solid paper with no apparent flaws.
Minor Points
- Although the discussions in Sections 2.1 and 2.2 are straightforward, it is not immediately clear how equation (4) in Section 2.3 corresponds to equation (1), how equation (5) corresponds to equation (3), and how both (4) and (5) relate to equation (6). Adding a reference or providing a more detailed explanation in the supplementary material would enhance clarity.
- In the discussion of Fig. 1c, the authors state that the "run time of ADMM increase[s] rapidly with data size while the greedy algorithm stays steady." This characterization seems inaccurate, as the runtime increases for all algorithms. If the longest runtime is indeed capped at 1000 seconds, I recommend that the authors conduct additional, longer runs to better evaluate the scalability of their algorithm.
- Is the RMSE used to evaluate cokriging and forecasting performance normalized in any way? If so, it would be helpful for the authors to provide insights into why the CCDS dataset shows comparatively large performance gains, while all algorithms perform similarly on the Foursquare dataset.
- Line 34: "From machine learning perspective" -> "From [a] machine learning perspective"
- Line 305: "five folds cross-validation" -> "five-fold cross-validation"
The greedy approach to low-rank learning proposed in this paper appears reasonable, and the experimental results show clear advantages over other methods. Overall, this is a well-written, solid paper with no evident shortcomings.