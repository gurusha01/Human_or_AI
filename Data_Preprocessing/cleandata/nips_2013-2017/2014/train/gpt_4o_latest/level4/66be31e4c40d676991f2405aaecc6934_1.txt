The authors introduce the concept of learning with pseudo-ensembles, a framework that unifies and explains several existing learning methods, such as dropout in deep neural networks. They propose Pseudo-Ensemble Variance (PEV) regularization, which aims to enhance the robustness of learned models (e.g., hidden unit activations across layers in deep neural networks) under perturbations in the model space. Since PEV regularization does not rely on label information, it naturally extends to semi-supervised learning scenarios. The authors evaluate their method against multiple baselines across supervised, semi-supervised, and transfer learning settings, demonstrating performance improvements.
The pseudo-ensemble framework builds on prior work on robust learning under input perturbations (e.g., Burge & Scholkopf 1997, Chapelle et al. 2000, Maaten et al. 2013, Wager et al. 2013) and represents a straightforward extension. The proposed PEV regularization is particularly noteworthy, achieving comparable performance to dropout in supervised learning while significantly outperforming it in semi-supervised and transfer learning contexts.
In Section 4.1, the authors attempt to establish a connection between PEV regularization and dropout. The pseudo-ensemble objective defined in Equation (1) provides an explanation for dropout in a limiting case. However, the text does not clearly demonstrate how PEV regularization approximates dropout in practice. The claim that discouraging co-adaptation is the key to the success of both PEV regularization and dropout is not fully substantiated, as it is based solely on the similarity in performance metrics (e.g., accuracy).
Starting from Equation (1), a more intuitive formulation might involve penalizing the variance of the output layer's distribution, rather than summing penalties over all hidden layers from 2 to d, as shown in Equation (3). The authors should clarify the rationale behind the current formulation and provide insights into the potential performance trade-offs if penalties on hidden layers were omitted.
Minor comments:
1. Figure 1 is not discussed in the text and requires explanation.
2. Table 2 suggests that even with PEV regularization, DAE pre-training (PEV+) significantly enhances model performance. This observation implies that PEV regularization may be less effective in leveraging unlabeled data.
Overall, the work introduces an intriguing regularization technique for multi-layer neural networks, inspired by dropout and pseudo-ensemble learning. The regularization could potentially be extended to other model classes. While the authors attempt to directly relate PEV regularization to dropout in Section 4.1, the connection remains unclear in its current presentation. Despite its simplicity, the PEV regularization delivers strong performance and may interest a subset of the research community.