This paper addresses the problem of active learning in the context of linear regression.  
In contrast to classification, the goal in active regression is to enhance the constant factors in the distribution-dependent rates of convergence relative to passive learning. This is because it is well-established that the asymptotic dependence on the number of labeled samples cannot typically be improved over passive learning, nor can the worst-case values of the constant factors. The authors argue that for passive learning, the rate of convergence depends on a distribution-dependent constant factor, which active learning can sometimes improve.
The authors specifically propose a rejection sampling scheme that modifies the sampling distribution to a more advantageous one without altering the optimal solution. However, this rejection sampler requires a scaling function \(\phi\) as a parameter, and achieving strong performance guarantees hinges on carefully selecting this function with some dependence on the joint distribution of \((X, Y)\). Since this joint distribution is unknown, the algorithm optimizes the choice of \(\phi\) among piecewise constant functions, using an estimated linear function derived from an initial sample. The authors provide a risk bound for this method, showing that it can approach the "oracle" rate (where the optimal \(\phi\) is known) and can demonstrably outperform passive learning methods in certain cases.
Overall, this is a strong contribution that I believe will appeal to a broad audience.
My primary concern is the lack of discussion regarding the dependence on \(K\) versus \(\rho^A\) in Theorem 5.1. Some terms increase with \(K\), while achieving \(\rho^A \to \rho^\) likely requires \(K\) to grow. This trade-off between \(K\) and \(\rho^_A\) could influence the rates. Including examples to illustrate how this trade-off might behave for reasonable distributions and sensible partitions would be valuable.
Minor Comments:
1. A relevant citation that is missing is Efromovich (2005): Sequential Design and Estimation in Heteroscedastic Nonparametric Regression. This work also investigates active regression (albeit for a nonparametric setting) and demonstrates improvements in constant factors based on the degree of heteroscedasticity.
2. I would like to echo another reviewer's comment regarding the notation for \(P\phi\), which seems somewhat unclear. My understanding is that, given \(Q\phi\) as the measure with density \(\phi\) relative to \(D\), \(P\phi\) represents the distribution of \((X/\sqrt{\phi(X)}, Y/\sqrt{\phi(X)})\) for \((X, Y) \sim Q\phi\). If this is the intended definition, then \(P\phi\) should indeed be well-defined, and the equality \(L(w, D) = L(w, P\phi)\) would follow from the law of the unconscious statistician. Is this interpretation of \(P_\phi\) correct?
This paper provides a robust approach to improving distribution-dependent constant factors in linear regression through active learning.