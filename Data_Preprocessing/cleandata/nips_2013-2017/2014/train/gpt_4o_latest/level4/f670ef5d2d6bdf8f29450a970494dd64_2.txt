This paper offers a novel perspective on sparsity-inducing regularizations. The authors introduced the concept of group-majorization and demonstrated that several well-known regularizations can be derived by appropriately defining a group \( G \) and a seed vector \( v \). Additionally, the paper presents gradient-based optimization methods and a heuristic for constructing regularization paths.
Overall, the paper is well-structured, and the central idea is conveyed clearly. Characterizing sparsity-inducing regularizations is a significant contribution, as sparsity plays a critical role in modern machine learning. This work has the potential to deepen our understanding of the topic.
The incorporation of the orbitope is a distinctive aspect of this research. However, the discussion surrounding its practical utility appears insufficient. As highlighted in Corollary 4, there is a connection between the orbitope and the atomic norm, and the premutahedra and sorted \( \ell_1 \)-norms discussed in the paper are both examples of atomic norms. While the orbitope and atomic norms may differ in general, how does this distinction lead to new insights? Specifically, can the orbitope enable the discovery of practically useful new regularizations that existing studies cannot achieve? Expanding on this point would strengthen the paper. For instance, the regularization path heuristic in Section 6 seems to be a potential advantage of using the orbitope. This heuristic facilitates adaptive tuning of the regularization, which may not be feasible with atomic norms. 
This research introduces a novel framework for interpreting sparsity-inducing regularizations through the lens of the orbitope. While the content is engaging, the practical benefits of employing the orbitope remain somewhat unclear and warrant further elaboration.