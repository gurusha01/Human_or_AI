Review - Summary:  
The authors present a multi-class extension of the Deep Boosting framework introduced by Cortes and Mohri. Initially, they establish a generalization bound for a linear combination of base hypotheses, where each hypothesis belongs to distinct sets with varying Rademacher complexities. This bound improves upon the standard multi-class generalization bound proposed by Koltchinskii and Panchenko. Building on this theoretical result, the authors design an optimization problem aimed at minimizing an upper bound on the generalization error. They propose a coordinate-descent-style algorithm to solve this problem. Experimental results demonstrate that the proposed algorithm, using decision trees as base hypotheses, outperforms traditional multi-class boosting methods such as AdaBoost.MR and its variants.
Comments:  
The optimization problem is well-justified by the authors' improved generalization bound. Moreover, the formulation naturally leads to a coordinate-descent algorithm for solving it. Notably, the derived criterion for selecting a weak hypothesis combines the weighted error and the complexity of the hypothesis class, which is an interesting and insightful approach.
The experimental results appear to validate the effectiveness of the proposed method. However, as suggested in the earlier ICML'14 paper on Deep Boosting, it would be beneficial to use validation sets for parameter tuning to further strengthen the empirical evaluation.
The formulation presented by the authors is "primal" in nature, focusing on minimizing losses (e.g., exponential or logistic loss), similar to frameworks like AdaBoost or LogitBoost. An alternative perspective is the "dual" view of boosting. For instance, AdaBoost can be interpreted as minimizing the relative entropy of the current distribution over the sample under certain linear constraints (see Kivinen and Warmuth, COLT99). Further developments in this dual perspective are explored in TotalBoost (Warmuth et al., ICML06), SoftBoost (Warmuth et al., NIPS07), and ERLPBoost (Warmuth et al., ALT08). Investigating the dual view for the proposed algorithm could provide additional insights and deepen the understanding of its mechanics.
After considering the authors' comments:  
Given that the authors have provided updated experimental results based on the reviewers' suggestions, I am inclined to raise my evaluation. I believe the theoretical contributions are significant and valuable for the NIPS community.