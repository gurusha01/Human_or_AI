This paper introduces a novel neural network architecture designed for classifying videos of human actions. The proposed model integrates the predictions of two convolutional neural networks: one trained on individual video frames and the other trained on short sequences of dense optical flow images. The approach achieves substantial improvements over other ConvNet-based methods for action recognition and matches the current state-of-the-art performance on two widely-used video classification datasets.
Although the paper primarily adapts the standard ConvNet image classification framework to a new data modality (dense optical flow frames) and combines it with a single-frame ConvNet, the experiments are comprehensive, and the results are compelling. The inclusion of multiple training strategies for ConvNets on optical flow, along with a clear demonstration of the advantages of using optical flow as input, is particularly commendable.
I have only a few minor suggestions:
- The reason why the spatial network significantly outperforms the models from [13], despite the similarity in architectures, is not entirely clear.
- A comparison of different frame sampling strategies for the spatial stream would add valuable insights. Overall, this is a strong empirical paper that will be of interest to researchers working on video classification using ConvNets.