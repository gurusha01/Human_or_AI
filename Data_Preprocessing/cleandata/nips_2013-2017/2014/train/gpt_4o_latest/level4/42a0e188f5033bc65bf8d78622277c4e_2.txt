This paper introduces a parallel direction method of multipliers (PDMM) to minimize block-separable convex functions under linear constraints. Unlike ADMM, which updates primal variables in a Gauss-Seidel fashion but is restricted to two blocks, this work proposes updating the primal blocks in a Jacobian fashion with multiple blocks. The primary contribution of the paper is the introduction of the dual backward step, which compensates for the limited information propagation inherent in Jacobian updates by effectively reducing the step size for the dual updates. Similar to ADMM, the paper establishes O(1/T) convergence results for PDMM.
The supplementary materials demonstrate that two prior methods, sADMM and PJADMM, are special cases of the proposed approach, thereby providing a deeper understanding of the relationships among various methods within the framework of decomposition methods based on the augmented Lagrangian. Experimental results validate the effectiveness of the PDMM method compared to ADMM and its variants. However, the results are somewhat counterintuitive, as Jacobian-type methods generally exhibit slower convergence than Gauss-Seidel-type methods. It would be helpful to provide further elaboration on this aspect. The dual backward step in PDMM appears to be novel and critical for ensuring convergence under general conditions, similar to ADMM. Overall, the proposed method offers additional alternatives and possibilities for distributed optimization.