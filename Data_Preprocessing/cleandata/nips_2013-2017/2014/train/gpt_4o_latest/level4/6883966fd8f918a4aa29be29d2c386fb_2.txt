This paper introduces a novel approach to actively learning a linear regressor under the standard least-squares loss function. The proposed method can be summarized as follows:
1. Consider the data residing in R^d. The process begins by selecting a partition of the space. Each data point and its corresponding response value are then reweighted in a straightforward manner, ensuring uniformity within each cell of the partition. This reweighting is governed by a single real-valued parameter per cell. The authors demonstrate that this reweighting does not alter the squared loss of any linear function.
2. Despite preserving the squared loss, different reweightings result in varying rates of convergence for the linear regressor. The authors make a key contribution here by leveraging a recent generalization bound for linear regression in an insightful way.
3. Determining the optimal weighting—one that achieves the best convergence rate—requires label information. The authors propose a parsimonious, active approach to obtain these labels, which simultaneously facilitates the estimation of the linear regressor.
4. The above steps (1)-(3) are designed for a fixed partition of the space. The authors further suggest employing progressively finer partitions as the dataset size increases.
The paper provides label complexity bounds for the steps outlined above.
Comments:
This work introduces several novel ideas and insights, particularly the reweighting technique for the data distribution and the innovative use of the recent generalization bound for regression. Additionally, the paper presents an algorithm that has the potential to be implemented in a reasonably practical manner. Overall, this represents a significant advancement in the field of active learning for linear regression.
There are, however, a few points that could be critiqued:
1. The analysis assumes that the "label noise" for a data point x is bounded by O(||x||^2). While it would be preferable to relax this assumption, it is worth noting that prior work on active regression has often relied on much stronger noise assumptions.
2. The paper does not provide a specific method for refining the partition or analyze the asymptotic rate achievable with such refinements. Nevertheless, this omission is not particularly concerning, as in practice, a reasonable partition could be constructed using techniques like hierarchical clustering.
In summary, this is a novel and insightful contribution that significantly advances the state of the art in active learning for least-squares linear regression.