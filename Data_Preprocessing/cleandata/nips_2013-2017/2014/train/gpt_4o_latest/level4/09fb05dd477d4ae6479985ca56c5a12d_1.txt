This paper tackles the problem of object detection, specifically addressing the challenge of obtaining bounding boxes at a scale comparable to the availability of category labels for object categorization. The authors circumvent this challenge by proposing an approach to adapt object classifiers for the detection task. The proposed algorithm is relatively simple and straightforward, which is not inherently a drawback. The experimental setup involves training on 100 categories (with both category labels and bounding boxes) and testing on 100 held-out categories, for which only category labels are available. The results are promising, bridging approximately 35% of the gap between an oracle and a baseline on the 100 held-out categories.
Quality and Clarity: The paper is of good quality and is clearly written.
Originality: The paper integrates two existing ideas related to adaptation: adapting object classifiers for detection and adapting across different sets of categories. To the best of this reviewer's knowledge, this is a novel and intriguing idea, though not revolutionary.
Significance: The primary contribution of this paper lies in its novel (albeit not groundbreaking) approach. While the long-term significance of the results is difficult to assess, they are likely to be surpassed in the future by more advanced methods, which is to be expected. A key limitation of the results is the inherent trade-off in performance (mAP) due to the reduced reliance on bounding boxes, and there is no objective metric to evaluate whether this trade-off is justified.
(As an aside, it is unclear whether the problem being addressed represents a critical long-term challenge. While bounding boxes are indeed more "expensive" to obtain than category labels, it is conceivable that resource allocation could mitigate this issue. Not long ago, the idea of having millions of images labeled with 20,000 object categories seemed equally implausible...)
Detailed issues that should be addressed:
- In Figure 3a, the authors should explicitly clarify which methods, aside from their own and R-CNN, utilize category labels to assist with detection. This would ensure a fair apples-to-apples comparison.
- The evaluation of the proposed approach appears to hinge on the blue bars in Figure 3b, where the DNN method achieves approximately one-third of the gap between the baseline and the oracle. Do the authors agree that this is a key result? If so, it should be emphasized more prominently, rather than the 78% figure mentioned on line 341, which is problematic for two reasons: (1) the evaluation should focus solely on the held-out categories, and (2) performance should not be expressed as a simple percentage of the oracle but should also account for the baseline. A similar issue arises with the claim of a 50% improvement on line 425.
- The authors should include a dedicated table or subsection explicitly outlining the differences between their method and R-CNN, which appears to be the most comparable alternative. Currently, these distinctions are scattered throughout the paper and could be made more prominent. Consider omitting Figure 6, which merely provides anecdotal examples, to make room for this clarification.
Minor issues (no response needed):
- Is the proposed method referred to as "DNN" or "DDA"?
- Typo on line 431 ("classi?ers").  
Overall, this is a solid paper that introduces a reasonably novel approach. I recommend its acceptance and believe it will be of interest to the NIPS community. While the results are likely to be surpassed in the near future due to the simplicity of the method (which is acceptable for a first attempt at this approach), the long-term impact of the paper—and the approach itself—remains uncertain.