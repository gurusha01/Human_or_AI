This paper introduces a novel Gibbs sampler algorithm tailored for FHMMs. The key innovation lies in incorporating an auxiliary variable, U, into the state of the Gibbs sampler. The role of U is to constrain the set of potential values that the hidden state X can assume at the subsequent step of the Gibbs sampler. Since the number of possible values for X_i at each time point i is relatively small, it becomes feasible to update X conditioned on U (and the observed data) using the FFBS algorithm.
I find this to be an original and insightful approach to tackling an important class of problems. The paper is exceptionally well-written, and the empirical evaluation of the proposed algorithm is conducted comprehensively. Most notably, the authors provide clear intuition regarding why (and under what circumstances) this method is effective. The potential for extending this general idea is evident, which underscores the paper's potential for significant impact.
For instance, one possible extension could involve defining the set of values Xi can take given U in a manner that incorporates yi (i.e., leveraging likelihood information to perform FFBS while focusing on the most probable states of X_i at each time step).
I did notice a few minor typos:
- p.3: "four bits" → "for four bits"
- p.3: Could you define Hamming distance?
- p.4: "U as auxiliary" → "U as an auxiliary"
Overall, this is a novel contribution to MCMC methods for FHMMs, presented with clarity and offering substantial potential to influence the development of algorithms for this class of models.