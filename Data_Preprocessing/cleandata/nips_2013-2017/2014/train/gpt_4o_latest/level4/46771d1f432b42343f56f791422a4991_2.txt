This paper investigates the relationship between dimensionality and communication cost in distributed learning problems. In this context, samples from an unknown distribution are distributed across \( m \) machines, and the objective is to estimate the mean at the optimal minimax rate while minimizing the communication cost. The authors demonstrate that the communication cost scales linearly with the number of dimensions. By leveraging prior lower bounds for the one-dimensional interactive setting [4] and improved bounds for the simultaneous setting, the paper establishes new lower bounds of \( \Omega(md / \log(m)) \) and \( \Omega(md) \) for the communication cost required to achieve the minimax squared loss in the interactive and simultaneous settings, respectively. Furthermore, the paper presents an interactive protocol that achieves the minimax squared loss with \( O(md) \) bits of communication. Recognizing the strong lower bounds in the general case, the paper initiates the study of distributed parameter estimation problems with structured parameters. Specifically, for \( s \)-sparse parameters, the authors propose a protocol that achieves the minimax squared loss with high probability and a communication cost proportional to \( s \) rather than the ambient dimension \( d \).
Quality: The authors should assume that readers are unfamiliar with the problem discussed in [4].
Clarity: Several definitions are missing in Section 2:
0. What is the definition of \( s \)-sparse?
1. In the definition of \( R(\hat{\theta}, \theta) \), why is the expectation taken over \( \hat{\theta}, X, \theta \)? How is the mean \( \vec{\theta}(Y) \) of \( Y \) obtained? Why is \( Y \in \mathcal{X}^n \)? Can an example of a transcript be provided?
2. The section on Private/Public Randomness is difficult to follow. For example:
   a. What are private and public randomness? What is a protocol? These terms need formal definitions.
   b. Why can public randomness be shared among machines before the protocol begins?
   c. Why does the protocol perform well on average over all public randomness?
   d. How does private randomness allow a machine to hide information from other machines in a protocol?
   e. The definition of "(\( \Pi, \vec{\theta} \)) solves \( T(d, m, n, \sigma^2, \mathcal{D}_\theta^d) \) with \( C \) and \( R \)" is unclear.
Originality: The paper appears to derive new lower and upper bounds for the interactive and simultaneous settings, as well as a notable negative result. However, the authors should clarify the challenges involved in deriving these results from [4].
Significance: The claim that "the communication cost scales linearly with the number of dimensions" requires further justification. While the authors provide a mathematical derivation, this result seems intuitively obvious. The authors should explain why this finding differs from their expectations. Additionally, the paper assumes a level of familiarity with the material that readers may not possess. Introductory explanations would improve accessibility. That said, the underlying idea is novel and valuable.