In this manuscript, the authors propose a rate-based neuronal network designed to store multiple stable activity patterns, or memories. The aim is to address key limitations of previous attractor models, specifically the violation of Dale's law and the saturation of activity levels in memory states. The training process involves applying gradient descent to a cost function that minimizes activity changes in the target attractors, reduces the Frobenius norm of the weight matrix, and enhances the stability of the attractors. The primary outcomes include a weight distribution consistent with experimental observations and a balanced interplay of excitation and inhibition in the attractor states.
The manuscript is well-written, with the model, training procedure, and results clearly presented and easy to follow. The integration of biologically inspired constraints into the model, along with the emergent features after training, makes this work a valuable contribution to the study of attractor neural networks. By training a rate-based neuronal network to encode graded, non-saturated attractors in the synaptic weights and inhibitory activities, the authors demonstrate that biologically plausible constraints on synapses and neurons result in a network-level balance of excitation and inhibition.