The paper presents a method for estimating sparse connectivity graphs of firing neurons. It employs an L_2 norm to derive a penalized inverse covariance matrix, which enhances the cost function. Additionally, a previously proposed hard thresholding approach is substituted with a soft thresholding method. The cost function is formulated as the cross-entropy and optimized using the BFGS algorithm.
The paper is self-contained, with an extensive set of references. The text is highly readable, though the structure of the paper occasionally emerges organically from the narrative rather than being explicitly outlined for the reader. As a result, the paper comes across as chronological and incremental in its presentation rather than being driven by theoretical and scientific motivations.
The proposed method compares favorably with other published approaches, particularly those evaluated in the Kaggle Connectomics competition. However, the frequent references to the competition give the impression that the paper is a retrospective submission, benefiting from hindsight to achieve superior results. This framing further contributes to the incremental tone of the work.
Comments following author rebuttal:  
I have decided to raise my score. What persuaded me was the demonstrated improvement in computational efficiency ("parametrized in a differentiable way with a very simple, easy-to-implement formula") and the clearer justification provided for key design choices, such as the filter order and chi-squared value. The paper is well-written and theoretically robust. However, it still feels like a delayed contribution to the Kaggle competition, showcasing a notable but primarily incremental improvement. This improvement is achieved largely through the use of logistic regression for preprocessing and a cross-entropy-based cost formulation that enables the use of efficient, off-the-shelf solvers.