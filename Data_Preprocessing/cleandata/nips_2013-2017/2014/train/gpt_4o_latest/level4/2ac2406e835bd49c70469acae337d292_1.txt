Paraphrased Review
Summary:  
The authors address the challenge of learning a mixture of Hidden Markov Models (HMMs). They propose a spectral learning algorithm to estimate the parameters of a single HMM and subsequently introduce a method to resolve the permutation ambiguity in the transition matrix, enabling the recovery of its block-diagonal structure.
Major Comments:  
1. Overall, I found the paper to be well-written. However, the experimental results section could benefit from additional detail and elaboration. Specifically, the...  
2. The authors build on the premise that a mixture of HMMs can be represented as a single HMM. A key motivation for learning a mixture of HMMs, as opposed to directly learning a single HMM, is the sparsity of the block-diagonal structure in the mixture. This sparsity is particularly advantageous for problems that are too large to model comprehensively, especially when the dataset is limited. However, the proposed algorithm assumes that the full HMM parameters are first accurately estimated, after which the permutation is reversed to identify the block-diagonal structure. This assumption seems impractical for most real-world scenarios. Could the authors provide guidance on the types of problems where their algorithm would be suitable and where it might fail?  
3. Furthermore, in practical scenarios, the recovered transition matrix is unlikely to exhibit sparsity. Is there a way to ensure that the transition matrix learned using the proposed spectral methods will approximate sparsity? It seems plausible that there could be many full transition matrices that are close to the sparse block-diagonal structure the authors aim to recover. However, it is unclear whether the matrix learned using the methods in [1,2] will be sparse, especially in the presence of even small amounts of noise.  
4. I am particularly concerned that the experimental results do not adequately capture the impact of noise on the transition matrix. For instance, in Section 4.1, the authors test their algorithm on a transition matrix perturbed by noise sampled from a Dirichlet distribution. However, the focus of the paper is on learning parameters of mixtures of HMMs, not merely depermuting matrices. Would it not be more appropriate to generate observation data from a mixture of HMMs and then attempt to recover all parameters from the observed data? This would provide a more realistic assessment of how noise in the observations and limited data availability affect the learned transition matrix and the difficulty of resolving the permutation ambiguity.  
5. The experiment in Section 4.2 begins to address the concerns I raised about the previous section, but it lacks sufficient detail to evaluate the experiment's quality. For instance, the specifics of the mixture of HMMs used in the experiment are unclear. The exact parameters of the transition and observation matrices are critical for assessing the difficulty of learning the model from data. I recommend that the authors provide more details about these parameters, perhaps in an appendix if space constraints are an issue.  
6. In Table 1, why does the EM algorithm initialized with spectral learning sometimes perform worse than spectral learning alone? Intuitively, EM should only enhance the results.  
Minor Comments:  
1. In Section 3.2.1, the subscript of \(\Lambda\) in the bottom-right corner of the first matrix in the proof appears to be incorrect. It should likely be \(K\) instead of \(2\).  
2. The text and numerical labels in the figures are excessively small and difficult to read.  
Conclusion:  
The paper presents some intriguing ideas and strong theoretical results. However, I am concerned about the practical applicability of the proposed method to real-world datasets.