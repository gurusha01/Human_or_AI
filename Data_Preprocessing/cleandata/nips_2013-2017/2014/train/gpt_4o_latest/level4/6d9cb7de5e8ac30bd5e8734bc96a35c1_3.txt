After reviewing the authors' feedback, I realize that I misunderstood the "mode" in the experiment, which aligns with what was intended. I have adjusted my score accordingly.
===
In this paper, the authors investigate an approach to multi-task learning by representing the (linear) models of tasks as a tensor and imposing a low-rank structure on the tensor to capture task relatedness and facilitate knowledge transfer among tasks. Existing methods, such as the latent trace norm, fail to address scenarios where task dimensions are heterogeneous. This paper introduces an improved version of the latent trace norm, which normalizes along each dimension. The authors derive error bounds for the two existing tensor norms and the proposed norm, relating them to the expected dual norm and providing a unified comparison of the three norms. This unified perspective is a valuable contribution. Finally, the authors present experimental results on synthetic data and two real-world datasets. Overall, this is a strong paper with interesting theoretical insights, and I outline my comments below.
My primary concern with this paper is the practical utility of tensor-based multi-task learning. While it is possible to model relatedness using a "features × aspects × customers" tensor, the question arises: is it necessary to go beyond a two-dimensional representation? For any tensor model, one could flatten the tensor into a "features × tasks" matrix and enforce a low rank to capture task relatedness. Although tensor formulations may capture additional information, it is unclear how significantly they improve performance. Even if there is a marginal performance gain, would practitioners accept the increased computational cost? The lack of experimental comparisons with simpler matrix trace norm approaches amplifies this concern.
The main novelty of this paper lies in the scaled version of the traditional latent trace norm, achieved by introducing a scaling factor for the unfolding along each mode. This implies that tasks (and their unfoldings) should be treated differently for specific reasons. The idea appears related to the work "Multivariate Regression with Calibration" by Liu et al. The authors may wish to explore potential connections between their work and this paper. Establishing such a link would be interesting.
What algorithm is used in this paper to solve the formulation with the new norm? Is it the same as the one employed for the original latent tensor norm? What is the computational complexity of the algorithm? In cases where the scaled latent trace norm outperforms the original version, how much time does it take to converge?
As previously mentioned, for multi-task learning, any tensor-based method can also be addressed using a flat trace norm. The authors should compare their approach to these "flat" multi-task learning methods, such as the trace norm, in terms of both performance and efficiency. Additionally, it is unusual that the authors use a different evaluation metric (explained variance) for the school dataset. Many multi-task learning papers report MSE/NMSE for the school dataset. Using MSE for this dataset would make the paper more consistent.
Some minor comments:
1. The notation "W_{(k)}^{(k)}" in Equation (2) and subsequent sections is confusing and has not been defined prior to its use.
2. Typo: "Turing" should be corrected to "Turning" on page 5, 10 lines from the bottom.
In summary, the authors propose a scaled latent trace norm for multi-task learning, providing both theoretical analysis and empirical evaluation. While the paper is well-executed, I have some concerns about its practical impact.