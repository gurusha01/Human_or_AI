The authors address the challenging problem of proposing convex relaxations for learning deep architectures. The primary difficulty in learning deep architectures arises from the iterative composition of layers that defines the model. For each layer, the critical components are the weight matrix (which parameterizes the inputs) and the output matrix (which encodes the output labels). The output matrix of one layer serves as the input to the subsequent layer, which encapsulates the core challenge in formulating the learning problem for deep architectures.
In this work, the authors introduce a novel convex relaxation approach inspired by a prior relaxation technique that gained prominence in the context of maximum margin clustering. For single-layer learning problems like maximum margin clustering, the standard convex relaxation involves lifting the problem into a higher-dimensional space by parameterizing it with the equivalence matrix (Theta Theta') instead of the output matrix (Theta). In contrast, the authors propose a relaxation based on the normalized quantity M = Theta' (Theta Theta')Theta, where denotes the pseudo-inverse and ' represents the transpose. This new convex relaxation yields several notable advantages. Specifically, with appropriate modeling, the proposed approach results in constraints on M that are exclusively spectral, whereas the unnormalized relaxation would involve both spectral and non-spectral constraints. This elegant formulation enables efficient implementation using a conditional gradient algorithm. Preliminary experimental results are provided on both synthetic and real-world datasets.
Detailed Comments
The discussion (lines 349-352) regarding convergence to a stationary point is currently too concise and should be significantly expanded in the final version of the paper. The clarity of the exposition could be enhanced by leveraging the supplemental material for detailed proofs and mathematical arguments, while reserving the main body of the paper for clear explanations and illustrative examples.
I commend the authors for their scholarly effort in meticulously defining the problem and designing a meaningful convex relaxation. By systematically reviewing multiple potential approaches and highlighting their associated challenges, the authors provide a valuable resource for addressing the current difficulties in developing theoretically-grounded deep learning methods. Even aside from the paper's primary contributions, these sections alone constitute an exceptional review of the topic. It is rare to encounter such a thorough and thoughtful effort, particularly in a conference paper. I am genuinely impressed by the depth and rigor of this work. The paper is innovative and refreshing. Designing effective convex relaxations for deep learning problems is arguably one of the most formidable challenges in the machine learning community today. This paper offers a novel and compelling approach. A clear accept.