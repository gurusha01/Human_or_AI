Paraphrased Review
Summary of the Paper:  
This paper presents a novel class of multi-class boosting algorithms that incorporate base learners regularized based on their Rademacher complexities. Experimental results demonstrate significant improvements compared to other multi-class methods in the literature as well as the non-regularized variant of the proposed algorithm.
Detailed Remarks:  
Regarding the Title:  
The term "deep" in deep learning typically refers to hierarchical representations with compositional structures. Decision trees, as used in this work, do not satisfy this definition, as they represent a shallow architecture with a single "hidden layer" of trees combined linearly. Using large trees in boosting, while common in practice, does not make the approach "deep." Therefore, I find the title misleading and strongly object to its use.
Lines 042-043:  
It is well-established that boosted trees generally outperform boosted stumps \cite{CaNi06, Keg14}. When tree sizes are appropriately tuned through hyperparameter optimization, they can grow quite large, as observed in your experiments. Additionally, prior work such as \cite{Keg14} has shown that boosting multi-class Hamming trees in AB.MH can avoid overfitting on most datasets, even with trees containing several tens of inner nodes and thousands of iterations (e.g., pendigits and letters datasets). The claim that "boosting has been observed to overfit in practice," based on outdated references, should be revised. Overfitting is context-dependent, and we lack a comprehensive understanding of when and why it occurs.
Lines 046-047:  
The concept of adaptive regularization for base classifiers was first introduced in \cite{KeWa04}, where the intuitive idea and final algorithm bear similarities to your approach. Specifically, coefficients were adjusted based on the empirical complexity of the weak classifiers. This prior work should be acknowledged.
While it is unreasonable to expect a conference paper to survey all multi-class boosting algorithms, the paper should at least reference key state-of-the-art methods, such as AOSO \cite{SuReZh12}, ABC \cite{Li09,Li09a}, Gao-Koller's iterative weak learner in hinge-boost \cite{GaKo11}, and AB.MH with Hamming trees \cite{Keg14}. Notably, the trees in the latter appear similar to those used in this submission.
Experimental Setup:  
I was quite surprised by the following statement:  
"We recorded the parameter tuple that had the lowest average error across all 10 runs, and this average error and the standard deviation of the error is reported in Table 1 and Table 2, along with the average number of trees and the average size of the trees in the ensembles."  
This approach involves validating on the test set, which is a fundamental mistake we caution students against. The implications are twofold. First, the reported errors cannot be compared to those in the literature. For instance, some errors (e.g., on pendigit) appear suspiciously low—3-4 times lower than previously reported—prompting me to scrutinize your experimental setup. Second, the empirical comparison of algorithms is compromised. Adding hyperparameters (e.g., AB.MR → L1 AB.MR → MDeepBoostSum) inherently reduces the minimum test error due to increased parameter tuning, even if the underlying distributions are identical. The observed "improvements" align with this phenomenon, as the minimum of a larger sample is statistically smaller than that of a smaller one.
For practitioners, the claims in your paper are significant. While earlier works (e.g., \cite{KeWa04}) explored regularizing weak learners, results were inconclusive, particularly on small datasets. If you assert that regularization now provides clear benefits, the experimental validation must be rigorous. I recommend redoing the experiments with proper double cross-validation during the rebuttal period and presenting the revised results. If the results remain inconclusive (i.e., the regularized version does not outperform the standard algorithm), the paper could still be accepted, but the narrative should shift to: "Here is an algorithm with strong theoretical justifications, but its practical benefits remain unclear."
Providing an open-source implementation of the algorithm would enhance reproducibility and facilitate adoption by the community.
Pseudocode:  
For readers attempting to implement the method based on the pseudocode, it would be helpful to clarify the definitions of quantities such as \(\Lambda\) and \(S_t\) for \(t = 1\), which are currently undefined in the caption.
Follow-Up on Revised Results:  
After reviewing the updated experimental results, they appear more reasonable. As anticipated, errors increased significantly (e.g., by 6-7 standard deviations on letters and pendigits). This directly contradicts your earlier assertion:  
"While it is of course possible to overestimate the performance of a learning algorithm by optimizing hyperparameters on the test set, this concern is less valid when the size of the test set is large relative to the 'complexity' of hyperparameter space (as any generalization bound will attest). Note that our experiments varied only three hyperparameters over a large dataset."  
Your revised results clearly demonstrate that my concerns were valid. The updated results show no significant improvement from the added regularization, contradicting the paper's central claim. The observed differences (1-2 standard deviations) are well within the range of statistical fluctuations. However, your results with AB.MR and AB.MR-L1 are strong, suggesting that your tree-building procedure incorporates effective techniques not detailed in the paper.
I sympathize with your position, as describing a state-of-the-art algorithm without theoretical contributions is unlikely to be accepted at a venue like NIPS. However, your theoretical results are irrelevant to practitioners (as they do not explain the algorithm's empirical success), and your practical results are irrelevant to theoreticians (as they do not validate the theoretical contributions). This creates a disjointed narrative with a misleading conclusion. I would be inclined to support acceptance if the paper were more transparent about these limitations. While I appreciate the algorithm and its potential, the current experimental setup and framing of the results warrant a clear rejection.