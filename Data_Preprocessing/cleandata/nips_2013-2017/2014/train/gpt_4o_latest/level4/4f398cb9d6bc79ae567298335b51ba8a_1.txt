The submission presents a convex deep learning framework that integrates several key concepts. First, the authors propose a training objective that explicitly incorporates the outputs of hidden layers as variables to be optimized. These outputs are connected to linear responses through a loss function, and the overall objective is defined as the sum of these loss functions across layers, along with regularization terms. Subsequently, the authors perform a series of variable transformations to reparameterize the objective into a convex form, leveraging the representer theorem and the concept of value regularization. The resulting convex objective is expressed in terms of three matrices per layer, one of which is a nonparametric "normalized output kernel" matrix that substitutes for directly optimizing over hidden layer outputs. However, this approach necessitates a transductive method, requiring simultaneous optimization for both training and test inputs. Finally, a relaxation is introduced to ensure convexity, as the set of valid kernel matrices is generally non-convex (and potentially discrete, depending on assumptions about hidden layer values). Experiments are provided, comparing the method to a prior two-layer convex model on small datasets.
Quality:
The foundational ideas in the paper appear technically sound, though I was unable to rigorously verify several aspects of the derivation. Achieving a convex deep learning formulation is a significant technical accomplishment, and the authors employ a range of sophisticated techniques, assumptions, and observations to realize this goal. Much of the derivation hinges on reparameterizations of the objective, heavily relying on the representer theorem.
Despite the technical achievement, my overall impression is that the proposed method is more of an academic exploration than a practical tool. This is due to several reasons. First, the derivation involves critical assumptions and relaxations to achieve convexity, raising questions about whether the resulting model truly captures the essence of traditional deep networks. A notable example is the relaxation of the output kernel set to a convex set, which diverges from the original motivation. The initial objective appears inspired by a Boltzmann-machine-like model, where hidden units are binary; however, the submission proposes inferring hidden states deterministically. The convex relaxation of the output kernel effectively removes the binary constraint on hidden units, which distances the method from its original conceptual basis and raises interpretability concerns. Similarly, the use of a hinge loss in place of a saturating loss is insufficiently justified. Additionally, the transductive nature of the algorithm limits its scalability, which is disappointing.
I also encountered difficulties understanding several critical steps in the derivation. One technical concern is whether the constraints induced by the reparameterizations are convex. For instance, in Eq. 8, the constraint \( S \in MRK \), where \( M \) and \( K \) are convex sets and \( R \) is an appropriate matrix, does not obviously define a convex set. Clarification or a reference would be helpful here. Additionally, I believe that in Eq. 5, \( \Phi U \) should be \( U \Phi \).
The experimental results are weak and reinforce doubts about the method's practicality. The datasets used are small subsets of larger datasets, suggesting scalability issues. The only comparison provided is to a two-layer version of the method, with no comparisons to traditional deep networks or even shallow methods as baselines. A comparison to nonlinear optimization of the original non-convex objective would also be insightful.
Clarity:
The submission's lack of clarity is its most significant weakness. The authors appear to have a specific mental framework and rationale for their approach, but this is not effectively communicated. For example, passing remarks are made about the loss function's properties (e.g., convexity in certain dimensions), but these assumptions are not clearly stated upfront. The connection to and inspiration from neural-net-like Bayesian networks or Boltzmann machines should be better articulated to motivate the objective. For instance, the authors could concretely describe the loss function for a Bayesian network and compare it to the proposed loss, discussing the implications for the method's expressive capacity. These issues need to be addressed more explicitly.
Several critical points in the derivation are inadequately explained, particularly around Eqs. 5-7. This section forms the core of the method, yet the explanation is limited to references to value regularization and recent clustering papers. This lack of detail is disappointing, especially since Section 2 is relatively clear but ultimately does not lead to actionable insights. A more concise Section 2 and a more detailed Section 3 would improve the paper significantly.
Originality:
While the work builds on earlier research on two-layer convex networks and shares the same foundational model, the extension to arbitrary nesting appears novel. The proposed optimization algorithm also seems original.
Significance:
As acknowledged in the paper, achieving a convex training formulation is not a universally prioritized goal among deep learning researchers, and local minima are not typically a major concern in practical deep networks. The proposed training procedure is complex and appears to scale poorly, as evidenced by the experiments. The formulation is also inflexible, being limited to a specific set of loss functions, and the transductive nature of the algorithm further hampers performance and scalability. Consequently, the practical impact of this work is likely to be minimal.
That said, the work has the potential to inspire further research, as the idea of representing a deep architecture through a convex objective is intriguing. Future studies could explore the theoretical limitations of this approach compared to traditional deep networks. While the main result—that a deep architecture can be expressed as a convex objective—is interesting, the paper provides little insight into the method's expressivity limitations relative to standard non-convex formulations. At this stage, the method does not appear to be practical.