This paper is commendable for its refreshing scope and ambition. Many NIPS papers tend to focus on a single, narrowly defined idea explored in exhaustive detail, but this work takes a broader approach. However, while the paper demonstrates significant breadth and includes substantial evaluation efforts, I feel it may lean too far toward being broad and shallow rather than narrow and deep. Below, I outline specific concerns that limit the overall impact of the presented results.
The field of algorithm selection and metareasoning is crucial for both AI and cognitive science. It is surprising that more research has not been conducted in this important area. While I am not deeply familiar with the literature, I rely on the authors' review, which primarily cites work from over a decade ago. However, the authors fail to reference research on cognitive architectures such as SOAR and ACT-R, which, while perhaps not directly relevant to metastrategy learning, could still provide useful insights.
I find the authors' choice to focus on the domain of sorting algorithms somewhat puzzling. This domain does not appear to align well with the rational metareasoning (RM) framework, which aims to optimize a combined measure of algorithm accuracy and time/opportunity cost. For sorting algorithms, however, all methods achieve perfect accuracy. Consequently, the modeling of scoring (Equations 3, 8, 9, and 10) seems irrelevant in this context.
Additionally, I question whether the Bayesian linear regression approach offers significant advantages over simpler methods like ridge regression, given that only parameter means are utilized (Equation 12). That said, the chosen representation—incorporating two features, their logarithms, and second-order polynomial terms involving these features and their logarithms—appears reasonable and well-suited to the problem.
The paper claims to contribute to both state-of-the-art AI approaches for algorithm selection and the psychological theory of metareasoning. I have concerns about its contributions to each area, which I address separately below.
Regarding AI, the authors compare their results to two existing models. However, the Lagoudakis method does not appear to account for presortedness, making it seem more like a straw man than a serious competitor. For the comparison with Guo, I was initially concerned (based on the text on line 160) that a different measure of presortedness might have been used, but the authors clarified in their rebuttal that they employed the same representation as Guo.
Regarding the contribution to cognitive science, human strategy selection involves a trade-off between accuracy and opportunity cost. Importantly, the opportunity cost includes not only the runtime of an algorithm but also the cost of selecting an algorithm, as highlighted by Simon's notion of satisficing. Thus, I question whether sorting algorithms are the most appropriate domain for studying human strategy selection. While the experimental setup in Section 5 is impressive and elaborate, the coarse performance metrics (e.g., proportion of merge sort selections and overall quality of selections) do not provide compelling evidence that the RM model is an accurate cognitive model. The experiment merely shows that both the RM model and humans perform reasonably well at strategy selection, whereas other models do not. This result offers limited insight into whether the RM model is a valid cognitive model.
I also found limited details about how the models in Section 5 were trained. If these models are intended to serve as cognitive models, they should be trained on the same data available to human participants during practice and under the same total number of trials. The authors clarified in their rebuttal that the training data is indeed identical.
Minor Comments:
- [093]: While a Gaussian distribution may be an acceptable approximation for sorting algorithm runtimes, it is likely not ideal for modeling human response times. Across a range of tasks, from simple button presses to complex problem-solving, human reaction times typically exhibit a long-tailed, asymmetric distribution.
  
- [091]: The standard deviation does not appear to be modeled as a polynomial in the extended features (Equation 7).
- [168]: The table caption should clarify the performance measure. I believe it refers to the percentage of runs in which the optimal sorting algorithm was selected.
- [295]: The text states that Equations 13 and 14 describe the conditions under which merge sort is chosen over cocktail sort. However, the coefficients on `ncomparisons` and `nmoves` in Equation 13 are smaller than those in Equation 14, suggesting that cocktail sort should be chosen except for very short lists.
In summary, this paper addresses an important challenge in both AI and cognitive science. The authors propose a straightforward approach to learning strategy selection and achieve reasonable results in a constrained domain (sorting algorithm selection). However, the limited scope of the domain and the lack of deeper insights into cognitive modeling diminish the broader impact of the work.