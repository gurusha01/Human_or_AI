The paper proposes a methodology for learning a semantic space tailored for object categorization, where semantic entities such as supercategories and attributes are employed to constrain the resulting discriminative embedding. The core idea is to represent a category in a sparse manner using its supercategory combined with a set of attributes (e.g., tiger = striped + feline). A notable advantage of the approach is its ability to produce compact semantic descriptions for the learned categories. The paper is well-structured, the motivation is compelling, and the results effectively highlight the superiority of the proposed techniques. While some aspects of designing cost functions for discriminative embeddings with strong inter-class separation have been explored in prior works (e.g., [7], [14]), the authors present novel implementations of these ideas to establish meaningful connections between categories, supercategories, and attributes.
- Other Comments
The introduction could be more coherent, as it introduces too many concepts without a clear progression. For instance, the generative and discriminative objectives mentioned in lines 077–079 are not clearly defined until later in the text. The mix of precise terminology (e.g., generative/discriminative) with insufficient contextual grounding makes this section harder to follow.
Line 145: The notation should be S(z_i,…) instead of S(z,…).
The models are non-convex. Could the authors clarify how the initialization is handled?  
Overall, the paper presents a compact semantic space model that learns a discriminative embedding for object categorization by incorporating constraints from supercategories and attributes. The approach demonstrates improved performance on the AWA dataset and offers the additional benefit of generating human-interpretable decompositions of categories.