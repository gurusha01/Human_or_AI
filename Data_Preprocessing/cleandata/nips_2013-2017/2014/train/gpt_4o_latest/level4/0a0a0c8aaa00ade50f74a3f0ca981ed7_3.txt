The paper revisits the classic problem of associative memory in neural networks, offering a fresh perspective and providing significant new insights into this longstanding challenge. The authors highlight that much of the prior work in this area has overlooked biological constraints such as Dale's law, relied on binary memory representations in rate-based models, and employed non-sparse memory representations. While these issues have been addressed individually in previous studies, the authors present a unified framework that tackles all these challenges simultaneously. Specifically, they leverage a recent control-theoretic approach, utilizing spectral bounding techniques, to design networks that address these limitations. 
In their approach, the authors propose a cost function comprising three terms, optimized via gradient descent, to achieve their objectives. The first term ensures the fixed-point nature of the desired memories, the second term promotes the stability of these fixed points, and the third term encourages small weights to prevent saturation. By directly incorporating Dale's principle into the weight parameterization, the authors successfully address the aforementioned shortcomings of prior work, resulting in a network where excitation and inhibition are naturally balanced. Notably, although they do not explicitly enforce saturation in single-neuron firing, this property emerges naturally as a result of solving the optimization problem. Furthermore, the authors restrict memory implementation to excitatory neurons—consistent with the biological observation that excitatory neurons typically send outputs to other regions—while treating inhibitory neurons as variables to be optimized. 
The authors validate their framework through numerical simulations, demonstrating its robustness to noise. Beyond showcasing the associative memory properties of their network, they also provide an explanation for experimental findings that show reduced trial-to-trial variability following stimulus onset. This phenomenon, which appears to be a general feature of associative memories with spontaneous baseline firing, is attributed to the system's convergence to the attractor closest to the initial condition.
In summary, this is a well-written and clear paper that addresses a fundamental problem in neural computation using innovative control-theoretic methods. It provides a compelling demonstration of how biologically plausible associative memory systems might operate. While the authors do not propose a biologically plausible learning mechanism in this work, they reference recent developments that could potentially lead to such rules. I found the treatment of inhibitory neurons as optimization resources to be particularly interesting and impactful. Overall, the paper makes a concise, clear, and important contribution to the field of neural computation.
A few minor points for consideration:
- In equation (3), the authors use \(\gamma = 0.04\). It would be helpful to discuss the sensitivity of the results to this choice, though I suspect the impact is minimal.
- On line 115, the authors propose a log-normal distribution for memory states. Please provide a motivation for this choice.
- The cost function in equation (4) might yield solutions that deviate from the exact fixed-point nature of the memories. How significant are these deviations? This should be quantified and discussed.
- Lines 254–256 were unclear to me. Please clarify these statements.
- The authors do not address capacity issues. Even a preliminary discussion of this critical computational aspect would be valuable.
- It would be interesting if the authors could discuss the computational implications of Dale's law. Specifically, can this principle be shown to enhance computational power under certain constraints? Alternatively, is it primarily a consequence of evolutionary or biophysical limitations? A brief discussion on this point would be appreciated.
In conclusion, this is a clear and well-written paper that tackles a classic problem in neural computation using novel control-theoretic tools, while providing insights into how biologically plausible associative memory systems might function.