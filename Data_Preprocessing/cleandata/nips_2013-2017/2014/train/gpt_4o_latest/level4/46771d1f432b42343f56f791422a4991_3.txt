This paper addresses the problem of estimating the mean of a \(d\)-dimensional normal distribution with unit covariance, given \(nm\) samples distributed across \(m\) machines. The authors explore the trade-off between communication complexity and dimensionality.
Several key results are established:
- Communication complexity cannot be reduced by jointly processing the various dimensions.
- Communication complexity can be reduced for sparse means.
- An improved upper bound is provided for the scalar case.
- An improved lower bound is derived under simultaneous protocols.
I have a few minor suggestions for improvement:
1) At the beginning, \(\theta\) is treated as a fixed parameter, but it becomes a random variable after Definition 1. While this is a standard approach in minimax theory, it would be helpful to explicitly highlight this transition. Additionally, before Definition 1, the authors introduce the conditional mutual information given \(\theta\). Strictly speaking, this is only well-defined when \(\theta\) is a random variable (even if it takes a constant value with probability 1).
2) In Corollary 3.1, the minimax rate for the stated problem is not \(d\sigma^2/(nm)\). Consider the case where \(\sigma^2 = 1000000\), \(n = m = d = 1\). In such a scenario, the estimator \(\hat{\theta} = 0\) can achieve a better rate. The statement should be appropriately qualified.
3) In Protocol 2, the last two lines contain a minor issue. I believe that \(Yi\) in the argument of \(\hat{\theta}i\) should not have a subscript.
Overall, this is a very well-written paper with several interesting and valuable contributions.