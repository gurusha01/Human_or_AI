Review - Summary:  
This paper introduces a regularization method for neural networks that encourages the model to minimize the variance of hidden layer representations caused by dropout noise in the preceding layers. The approach is extended to "pseudo-ensemble" models, where other types of perturbations can also be applied.
The primary contribution of this work is the proposed variance regularizer. The authors conduct experiments on MNIST (both supervised and semi-supervised settings) and the NIPS'11 transfer learning dataset (CIFAR-100, TinyImages) using standard neural networks with dropout perturbations. Additionally, experiments are performed on the Stanford Sentiment Treebank dataset with Recursive Neural Tensor Networks using alternative perturbations. The results demonstrate that the variance regularizer performs as well as or better than using perturbations alone.
Strengths:  
- The proposed model achieves promising results in challenging scenarios with very limited labeled data.  
- The experiments are thoughtfully selected to demonstrate the versatility of the method across different models and datasets.  
Weaknesses:  
- Certain sections of the paper appear somewhat extraneous. For instance, the discussion on Baggy/Boosty PE does not clearly contribute to the central focus on the variance regularizer.  
- Some critical experimental details are missing, as outlined below.  
The authors should address the following points:  
1. How many noise samples were used to compute the variances?  
2. Was backpropagation performed through each perturbed (dropped-out) model, or only through the clean model?  
3. Dropout is known to slow down training, and this approach likely exacerbates the issue by requiring multiple forward and backward passes per gradient update (with/without noise or with different noise samples for variance computation). It would be helpful to analyze the extent of this slowdown by plotting training/test error against time (rather than epochs).  
4. What was the stopping criterion for the semi-supervised MNIST experiments? The paper mentions training all networks for 1000 epochs without early stopping. Does this also apply to the semi-supervised experiments? If so, was the 1000-epoch limit maintained even for cases with only 100 labeled examples? It seems likely that such a setup would lead to overfitting on the labeled data, even with dropout, given the small dataset size. Clarifying whether early stopping with a validation set was used is crucial, as large validation sets are often impractical in small data regimes.  
5. If multiple forward and backward passes (e.g., $n$ passes) are performed per gradient update in PEV, is it fair to compare models based on a fixed number of epochs? Wouldn't each PEV update be equivalent to approximately $n$ standard stochastic gradient descent updates?  
6. For the semi-supervised experiments, did each mini-batch contain a mix of labeled and unlabeled examples? If so, what proportion of the mini-batch was labeled?  
7. Consider including a comparison with SDE+ in Table 1.  
8. Was the same architecture and dropout rate used for both SDE and PEV in Table 1? If so, is this comparison fair? Since PEV is a "stronger" regularizer, it might be worth exploring whether SDE could perform better with a smaller network or a higher dropout rate. Allowing SDE to be optimized in its own way could provide a more balanced comparison.  
Quality:  
The experiments are well-executed, but the inclusion of additional explanations and comparisons, as suggested above, would enhance the overall quality.  
Clarity:  
The paper is clearly written, with only minor typographical errors.  
Originality:  
The variance regularizer is a novel and original contribution.  
Significance:  
This work has the potential to significantly impact researchers working with small datasets. The proposed regularization approach is innovative, and the experimental results are compelling. However, the paper would benefit from addressing the suggested clarifications and providing additional details.