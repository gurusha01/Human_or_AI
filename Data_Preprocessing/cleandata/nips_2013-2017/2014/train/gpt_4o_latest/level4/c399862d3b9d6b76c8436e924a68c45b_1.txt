This paper presents an approach to online bandit optimization for smooth strongly convex losses. Unlike the online "full information" setting, where the gradient is provided to the learner, this work addresses a scenario where feedback is limited to point-wise evaluations of the function. The authors propose a meta-algorithm that leverages careful sampling strategies to construct an unbiased gradient approximation, which is then fed into an optimization routine designed for the full information setting. The paper includes a thorough theoretical analysis of the approach and compares the resulting regret bound of O(T^{1/2}) to those achieved by existing methods.
The authors tackle a significant problem in online optimization. The paper is well-written, with each argument presented clearly and precisely, and the results demonstrate a tangible improvement over prior techniques. The proofs are straightforward, easy to follow, and appear free of any evident errors.
The paper also points to intriguing directions for future work, such as achieving either an improved O(T^{1/2}) upper bound for smooth or strictly convex losses, or establishing an \Omega(T^{2/3}) lower bound.
The authors provide a clear and concise algorithm for online bandit optimization with smooth strongly convex losses, achieving an improved regret bound. The paper is of high quality and merits a strong accept.