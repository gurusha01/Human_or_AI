Paraphrased Review:
Summary of the paper:  
This paper introduces a trajectory optimization method inspired by iLQG, where the linearized dynamics are learned from sampled data rather than derived from the linearization of a known model. To ensure proper convergence in iLQG, a backtracking line search is typically employed during the forward pass to prevent excessive deviation between successive trajectories. However, in this work, since the dynamics are unknown, the authors replace the line search with a constraint on the KL divergence between the distributions of the current and previous trajectories, thereby limiting the extent of trajectory changes during optimization. A GMM model is utilized as a prior for the sampled unknown dynamics. Additionally, a parametrized policy is trained using guided policy search within the trajectory optimization framework, with a neural network employed to approximate the policy. The paper includes experiments on three simplified robotics problems and provides comparisons with alternative methods to substantiate its claims.
Comments:  
The paper is well-written, engaging, and addresses a significant topic in robotics research. Regarding the novelty of the approach, the primary contribution lies in demonstrating how iterative LQG can be adapted to a model-free setting while ensuring convergence by substituting the backtracking line search with a KL divergence constraint. While this is an intriguing idea, it could be argued that this modification is relatively incremental, given existing knowledge of iLQG techniques and linear Gaussian approximations of dynamics. The guided policy search framework is also compelling but is closely related to prior work, such as that of Mordatch et al. (RSS 2014). A strong point of the experiments is that they effectively illustrate the faster convergence of the proposed method compared to other approaches, even when the GMM prior is not employed.
One observation regarding the use of a GMM prior is that it effectively serves as a model, which makes it unsurprising that the proposed method performs comparably to model-based trajectory optimization techniques. As noted in Section 3.2, larger mixtures that capture the dynamics with greater fidelity yield the best results, which is expected since they essentially provide a high-quality dynamic model. A key question that arises is how the GMM is constructedâ€”specifically, what methodology is used to sample the potentially high-dimensional space while ensuring safety in real robotic systems? Additional discussion on the feasibility of scaling this approach to more complex robot models, where constructing such a GMM would be significantly more challenging, would be valuable.
Another potential limitation of the approach is its reliance on an example demonstration for the walking task, which is limited to a 2D walking scenario. It is well-established that 3D walking is substantially more challenging than 2D walking, where simpler feedback control solutions are often sufficient (e.g., the work of M. Spong, R. Gregg, or J. Grizzle in this domain). The need for an example demonstration implies that the problem is already partially solved. It would be beneficial for the paper to address this issue and discuss how the method could be extended to more complex tasks, such as 3D walking or swimming with a higher number of degrees of freedom. Given the assumption of a precise GMM prior, it is surprising that the optimizer does not independently discover a walking solution, especially since iLQG has been shown to solve 3D walking tasks with many degrees of freedom.
Another point of concern relates to the comparison with iLQG for tasks involving contacts. What contact model is used in these evaluations, and how does it compare to the contact model employed by the iLQG algorithm? Previous work by Tassa et al. has demonstrated excellent performance on tasks involving contacts and high degrees of freedom using iLQG, partly due to the choice of a contact model that is well-suited to iLQG optimization techniques. For a fair comparison, the paper should ensure that the same contact model is used for iLQG in these tasks. 
Overall, the paper is well-executed and provides useful comparisons with other learning approaches. However, my primary concerns are: (1) the use of KL divergence for line search, which represents a relatively incremental modification of iLQG, and (2) the scalability of the proposed method to more realistic and complex scenarios.