This paper presents an incremental contribution to kernel approximation through the use of Random Features. The proposed method, termed Sparse Random Features, incorporates l1-regularization into the Random Feature framework, preventing the model's size from growing linearly with the number of features. The authors further demonstrate that the proposed algorithm can be interpreted as a Randomized Coordinate Descent approach in Hilbert Space.
The paper is well-written, clear, and easy to follow. The contribution appears to be significant, offering a practical solution for efficiently addressing large-scale problems compared to existing kernel methods.
That said, I recommend improving the presentation and analysis of the experiments and results. Specifically, there is a noticeable drop in accuracy for the proposed algorithm when applied to the Covtype dataset with Laplacian and Perceptron, which warrants further discussion. Additionally, it is unclear whether the reported results in the tables are based on cross-validation (which is preferable) or a simple data split.
While the narrative is generally clear and accessible, some sentences could benefit from proofreading for improved clarity and polish. Overall, this is an interesting and impactful piece of work with direct relevance to machine learning applications. The paper is well-executed, but the experiments section could be strengthened, and the inclusion of a conclusions section with future work directions would further enhance the manuscript.