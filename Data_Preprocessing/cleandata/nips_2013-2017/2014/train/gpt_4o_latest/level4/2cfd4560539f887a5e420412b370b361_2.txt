This compelling paper presents a novel deep recursive neural architecture that achieves state-of-the-art performance on a challenging sentiment classification dataset.
The results produced by the proposed architecture are remarkable, and the presentation is exceptionally clear.
The modifications introduced to prior RNN models are innovative, well-justified, and empirically shown to be effective:
- Decoupling leaf nodes from nonterminal nodes.
- Employing rectified linear units (ReLUs) in the recursive framework.
- Utilizing large unsupervised word vectors while keeping the hidden nonterminal nodes smaller.
- Implementing a deep architecture that goes beyond merely replacing a single RNN layer with a deep layer by introducing connections between layers and tree nodes. -- It would have been interesting to compare a setup where the outputs of the last hidden layer at each node were the sole input to the next parent node, i.e., omitting the connection from matrix V.
However, the results in Table 1 (a) should have been reported on the development set rather than the final test set, as tuning on the final test set is problematic.
Typos:
- "comprise a class of architecture"