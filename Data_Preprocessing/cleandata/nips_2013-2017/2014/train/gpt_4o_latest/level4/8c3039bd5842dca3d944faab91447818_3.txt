The manuscript introduces a novel model named DeepGMM, which extends the Mixture of Gaussians (MoG) framework by incorporating an exponential number of components with tied parameters. Each mixture component corresponds to a unique path through a fully connected multilayer network of affine transformations, where the transformation matrix is the Cholesky decomposition of a positive-definite matrix applied to an identity-covariance multivariate Gaussian. While the component weights remain untied, the paper suggests a factorization approach for very deep networks.
The authors hypothesize that tying the parameters across numerous components enables the use of a larger number of components while mitigating the risk of overfitting.
Additionally, the paper proposes a hard-EM-based training algorithm. For the expectation step, the authors recommend coordinate descent and several heuristics to reduce computational overhead. For the maximization step, three optimization strategies are presented:
1. A batch gradient descent (GD) method tailored for DeepGMMs with a limited number of paths. Unfortunately, no specific performance metrics are provided.
2. A batch GD method designed for DeepGMMs handling data with relatively low dimensionality.
3. A stochastic gradient descent (SGD) method for larger-scale DeepGMMs.
The experimental evaluation is conducted on two well-known datasets, with all experiments employing the second optimization technique mentioned above.
Quality:
Pros:  
The paper is technically robust, and its main hypothesis is supported by experimental findings. For instance, Figure 4 demonstrates that tying parameters enables training a DeepMoG with 2,500 effective components, achieving superior performance compared to an untied MoG with 300 components (despite the latter having more parameters). Furthermore, increasing the number of components in the untied mixture does not yield additional improvements.
Cons:  
The experiments are limited to natural image data, which inherently aligns with the inductive bias of the DeepGMM model. Broader evaluations on other types of data would strengthen the findings.
Clarity:
The manuscript is well-organized and generally clear, though it contains minor typographical errors (e.g., "parrallizable" should be "parallelizable," and "netwerk" should be "network").  
- In Figure 4, the maximum value achieved is approximately 154.5, whereas Table 1 reports a value of 156.2. If these results stem from different experiments (e.g., using more data than the 500,000 patches mentioned), this should be clarified.  
- Reporting training times would be valuable. For example, does training a model on 8x8 patches require hours, days, or weeks?  
- In Figure 4, to better illustrate the ability to train a MoG with numerous components without overfitting, it would be more informative to display the effective number of components rather than the number of components in the top layer.  
- If space permits, including samples generated by the model would enhance the paper.  
- Regarding the tinyimages dataset, was it resized to 8x8 pixels? If not, comparing its likelihoods to those obtained on BSDS300 (as done on line 397) may not be valid.
Originality:
The proposed parameter-tying technique for GMMs is novel, as are the training algorithms and heuristics introduced in the paper. These contributions represent meaningful advancements in the field.
Significance:
The results are impactful. While training and evaluating densities at test time may incur high computational costs, sampling from the model is expected to be computationally efficient. The proposed approach is innovative and has the potential to inspire further research.  
The paper introduces a new parameter-tying mechanism for MoGs, enabling state-of-the-art performance on natural image patches. Overall, the work is both interesting and accessible.