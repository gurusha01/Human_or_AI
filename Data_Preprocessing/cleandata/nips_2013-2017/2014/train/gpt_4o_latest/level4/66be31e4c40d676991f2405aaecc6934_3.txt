This paper introduces the concept of a pseudo-ensemble, which involves dependent child models being ensembled together. This approach provides a unified perspective on several related techniques, most notably dropout. The authors then propose the pseudo-ensemble variance regularizer (PEV) and validate it with compelling empirical results. Compared to the method in [23], the proposed technique appears more straightforward to apply in multi-layer, semi-supervised settings.
However, there are some concerns. The idea of pseudo-ensembles as an expectation is already present in prior works such as [5] and [23], so the contribution of naming it and extending it to multi-layer settings does not represent a significant conceptual advancement. While the authors attempt to gain deeper insights by introducing the "boosty" and "baggy" forms, these concepts are not fully developed, and the connection to boosting remains unclear. Additionally, while introducing a regularizer and demonstrating its empirical success is acceptable, it would be more compelling if the regularizer corresponded to or approximated a theoretically grounded concept. The structure of the paper suggests that sections 1-3 establish a general framework, with section 4 intended to test it. However, section 4 instead introduces a new regularizer that is only loosely connected to the preceding sections. While the simplicity and empirical strength of the proposed regularizer are commendable, the overly elaborate presentation detracts from clarity and does not add significant insights. If the authors believe I have misunderstood this point, I am open to clarification and willing to reconsider my evaluation.
Detailed Comments:
1. The formalism of \( f(x; \xi) \) is overly general for the specific case of subsampling child models through \(\xi\). Without explicitly linking \(\xi\) to something like dropout, this formalism lacks meaningful context. While addressing this issue might complicate the notation, it may be acceptable as is.
2. Line 106: The statement "Note how the left objective moves the loss \( L \) inside the expectation over noise" contradicts the corresponding equation, where \( L \) is outside the expectation over noise.
3. Equation 2: The "baggy PE" formulation is unclear. Are you summing over \( i \)? Or should one of the expectations also be taken over \( i \)? This needs clarification.
4. The connection between "boosty PE" and boosting is vague. While there is some intuitive resemblance (e.g., the final prediction involves a sum of weak learners, albeit unweighted), key elements of boosting, such as sequentially adding classifiers and assigning weights to examples, are absent. Can you make this connection more concrete? If not, it might be better to explicitly state that "boosty" is your own definition and that readers should not expect rigorous connections to traditional boosting.
5. Lines 129-140: The comparison with existing approaches seems incomplete or inaccurate. For example:
   - [5]: Learning with MCFs involves a non-convex (sometimes convex) lower bound by moving the expectation inside the log but not inside the exp.
   - [23]: Adaptive regularization uses a delta method (second-order central limit expansion) and provides an explicit, interpretable regularizer. It is non-convex.
   - Wang and Manning: Fast Dropout relies on the central limit theorem and the convergence of noise sums to Gaussian distributions. It addresses both model and input noise.
   - Baldi and Sadowski: The "Understanding Dropout" paper analyzes the geometric mean and also considers model noise.
6. Equation 3: The subscript on the variance notation is confusing. Since the operation is the same regardless of \( i \), the subscript seems unnecessary. However, in line 210, the subscript appears relevant. Is it worth keeping the subscript for consistency?
7. To avoid confusion, you might consider renaming \(\mathcal{V}\) as a "scale-invariant variance penalty" rather than referring to it as "variance." While the font helps distinguish it, calling it "variance" may mislead readers.
8. The motivation for the proposed regularization method is unclear. Can you show that it converges to [23] for linear models or provide additional theoretical justification? Deriving or approximating the regularizer instead of simply defining it would enhance its credibility.
In summary, while the paper presents the pseudo-ensemble variance regularizer and demonstrates its empirical effectiveness, the theoretical motivation for the regularizer is weak. Furthermore, the conceptual discussions of boosting and bagging are underdeveloped, and the presentation style, while elaborate, does not significantly enhance understanding.