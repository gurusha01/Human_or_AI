The objective of this study is to develop generative models for sensory signals within attended regions. The authors introduce a model capable of successfully learning a generative representation of frontal faces from large-scale images containing faces amidst distractors.
Overall, the manuscript is well-written. It integrates and builds upon a diverse range of research areas, spanning neuroscience, computer vision, and machine learning. I believe the manuscript would be both informative and valuable to a broad audience.
The proposed model is presented in a general framework, suggesting its potential applicability to various types of data. However, I find that the model relies on a couple of critical assumptions that considerably constrain its scope. Specifically, it assumes the existence of a "canonical image" (variable "v") and operates under the assumption of similarity transformations. As a result, while the approach is effective in constructing a generative model for "attended image regions," it cannot generalize to underlying three-dimensional "objects" whose sensor observations involve transformations beyond similarity. This limitation likely explains why the experiments in Section 6 focus exclusively on frontal faces, without exploring other viewpoints or object categories. The manuscript would benefit from a more explicit discussion of these constraints in relation to the broader goal. 
In summary, this is an intriguing approach to the challenging problem of building a "generative model of attended image regions," effectively synthesizing insights from multiple fields. While thought-provoking, I find the proposed model to be fairly constrained in its current form.