The paper introduces a method for simultaneously learning multiple tasks, where in each round, a sample is provided for every task, but only one task's sample can be annotated. The authors frame their approach as a balance between exploitation and exploration. Theorem 1 establishes an upper bound on the expected cumulative number of mistakes. The proposed algorithm is evaluated against two alternative strategies for selecting the single sample/task to annotate.
I find the paper to be well-written, with strong theoretical and experimental contributions.
However, I question whether the strict synchronous assumption regarding annotation selection is truly necessary. A more natural assumption might involve a total annotation budget shared across all tasks in the learning system. It would be helpful if the authors could provide additional real-world motivation for this specific setup.
Additionally, I believe the experimental results could be enhanced by including two further comparisons:
a. A scenario where annotation is inexpensive, allowing all samples to be annotated. While the results would likely improve, this would illustrate the trade-off between minimizing annotation costs and treating annotation as inexpensive.
b. An active learning approach applied independently to each task, while ensuring the total number of annotations across all tasks matches the number of learning rounds. This would serve as a comparison to a method that incurs the same total annotation cost but operates under a less restrictive constraint, allowing multiple tasks (or none) to be annotated in each round.
Overall, I enjoyed the paper. It addresses a novel and interesting problem, offering solid theoretical guarantees and experimental validation for the proposed solution. However, I would like to see stronger justification for the specific choice of parallel task learning, supported by additional experimental evidence as outlined in my suggestions above.