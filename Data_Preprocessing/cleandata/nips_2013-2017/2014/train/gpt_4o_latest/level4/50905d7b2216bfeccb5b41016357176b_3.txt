Review - Summary:
This paper explores the application of Difference of Convex Functions (DCA) algorithms to minimize the norm of the Optimal Bellman Residual (OBR). To support this approach, the author(s) demonstrate that the norm of the OBR can be expressed as a difference of convex functions, highlighting the theoretical advantages of this loss function in terms of error bounds.
The decomposition showing that the OBR can be formulated as a difference of convex functions is presented in a clear and accessible manner, requiring no additional proof. This establishes an intriguing connection that, while appearing intuitive in hindsight, represents a novel contribution to the field. 
The paper includes a small-scale empirical study comparing the use of DCA for minimizing the norm of the OBR against other reinforcement learning algorithms (LSPI and Fitted-Q). The experiments, conducted on a set of randomly generated finite-state MDPs, indicate comparable performance with reduced variance.
Discussion:
This work introduces an engaging application of non-convex optimization techniques to reinforcement learning. The authors effectively justify the use of the norm of the OBR and its decomposition into a difference of convex functions, presenting a promising new direction for research. The initial theoretical insights and empirical findings suggest that this approach merits further investigation.
However, the paper leaves several questions unanswered, particularly regarding the empirical study's details. Despite these gaps, the overall contribution is valuable and relevant to the field.
Recommendations:
The empirical study is notably limited. At a minimum, the authors should include an additional application domain and provide a detailed discussion of the methodology underlying the experiments. Specifically, the tunable parameters for DCA, LSPI, and Fitted-Q are not discussed, and the paper lacks information on how these parameters were selected. 
Additionally, there are several typographical errors in both the mathematical expressions and the writing. While these do not significantly detract from the paper, the authors should address them in a revised version.
Overall, this paper offers a novel perspective on value-function-based reinforcement learning. While there is room for improvement, particularly in the empirical evaluation, the work makes an interesting and meaningful contribution to the field.