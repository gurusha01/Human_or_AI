This paper addresses the challenge of performing Bayesian inference under structural priors that impose support constraints on the distribution.
The paper begins by motivating the incorporation of support constraints through the maximum entropy principle, which seeks to impose constraints while deviating minimally from a base distribution, as measured by relative entropy. The resulting distribution, known as the information projection of the base distribution onto the constraint set, is shown to yield intuitive results in the context of support restrictions. Specifically, the information projection of \( p \) onto a support set \( A \) is given by \( 1\{x \in A\} p(x) / \int_{x \in A} p(x) \). This principle is then extended to Bayesian inference, where the posterior distribution is interpreted as the projection of the joint distribution onto the support set defined by the observations.
The paper further demonstrates that when projecting onto the intersection of multiple constraint sets, the order of sequential projections does not affect the result. This property motivates an inference procedure tailored to sparsity constraints, which forms the basis of the paper's main algorithm. The algorithm structures inference as a two-level optimization problem: an inner optimization computes the posterior distribution for a fixed set of \( k \) nonzero entries, while an outer optimization determines the optimal set of nonzero entries. The authors show that this outer optimization problem can be framed as a submodular optimization problem, enabling the use of a greedy forward selection strategy to identify the nonzero dimensions. Experimental results against various baselines demonstrate the effectiveness of the proposed method.
The paper has several strengths:
- The writing is exceptionally clear, and the arguments are well-justified and appear technically sound.
- The use of information projections provides a clean and insightful framework for understanding how constraints can be incorporated into models.
- The proposed algorithm performs well in empirical evaluations.
However, the paper has a notable limitation. While technically solid, the results are not particularly surprising. From a practical standpoint, Section 2 seems somewhat unnecessary for arriving at the algorithm in Section 3. One could directly observe that sparsity constraints correspond to a union of simple constraint sets, naturally leading to the objective in Section 3 and the subsequent algorithm. The submodularity of \( J(s) \) is also relatively straightforward to recognize. As a result, while the paper is interesting and well-executed, it does not provide significant new tools for incorporating structural constraints into models. I found the paper enjoyable to read, but I was left wishing for more novel insights or methods for imposing prior knowledge in modeling tasks.