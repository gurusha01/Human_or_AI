Review of submission 1706:  
Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning  
Summary:  
The authors train a fast deep neural network (NN) to play Atari games in the ALE framework, using a high-quality but computationally expensive traditional game planner as a teacher. This approach outperforms a recent method [19] (referred to here as "DQN"), which employs temporal difference-based reinforcement learning with a deep NN function approximator of the same architecture.  
Comments:  
This is an interesting piece of work, and I appreciate the simplicity of the core approach. It is commendable that the authors implemented this method.  
However, the abstract and main text are overly verbose. I recommend significantly condensing the text, potentially adopting the concise style of the summary provided above.  
Regarding the discussion of prior work:  
"Over the last decade, deep learning (e.g., [13, 12, 18, 8]; see [7] for a survey) has emerged as a powerful technique for learning feature representations from data (again, this is in a stark contrast to the conventional way of hand-crafting features by domain experts)."  
This statement is somewhat misleading, and the references are heavily skewed toward recent papers from a limited set of research groups. It neglects to acknowledge that the success of deep learning predates the cited works. For instance, the referenced "survey [7]" primarily focuses on results post-2006, but deep learning of feature representations in neural networks (and related systems) has a much longer history. A more comprehensive survey, available at http://arxiv.org/abs/1404.7828, covers deep learning developments dating back to 1965.  
General Recommendation:  
The submission is engaging, though it largely confirms an intuitive result: slow but high-performing traditional game planners can train faster deep networks in a supervised manner to surpass similar networks trained using more general reinforcement learning techniques. The paper is publishable, provided the discussion of deep learning history is revised to present a more balanced and comprehensive perspective.