Review - Summary:  
This paper introduces a sample-efficient policy search algorithm designed for large-scale, continuous reinforcement learning tasks. Unlike traditional model-based policy search methods, the proposed approach focuses on learning local models represented as linear Gaussian controllers. Using the rollouts generated by these local models, the method then trains a global, nonlinear policy through an arbitrary parameterization scheme. The Guided Policy Search (GPS) framework alternates iteratively between local trajectory optimization and global policy learning. The experimental results demonstrate that the proposed approach outperforms several state-of-the-art policy search methods, such as REPS and PILCO. The experiments were conducted primarily in two-dimensional dynamic simulations involving continuous control tasks with multi-linked agents.
Quality:  
The paper is well-written and tackles a topic of high relevance to the NIPS community. Policy search algorithms have seen increasing interest in recent years, and this work contributes valuable and intriguing insights to the field. The idea of training a global policy using locally linear models in a supervised manner is excellent. The experimental results convincingly show that the proposed method can surpass well-established policy search algorithms. However, it is somewhat challenging to fully assess the method's quality due to the novelty of the experimental setups, which, to the best of my knowledge, have not been widely adopted by other researchers. For instance, Deisenroth et al. demonstrated that PILCO could solve the "Cart-Pole Swing-up" task within a few trialsâ€”can GPS achieve similar results? Additionally, I am curious about whether the local models might interfere with one another during dynamics fitting. To prevent such interference, it is crucial to ensure that rollouts are generated only within the timeframes where a specific local model is the expert. After multiple readings, I am still uncertain how this issue is addressed in the paper. This concern is particularly relevant when training neural networks, as conflicts between local datasets can severely degrade performance. Despite these questions, I appreciate the flexibility of the proposed method, which allows for the use of various policy representations. While neural networks have been less favored in control tasks, this paper demonstrates that, when trained appropriately, they can be powerful tools for solving continuous control problems.
Clarity:  
The paper is clear and well-structured, with all sections being easy to follow. The authors have done an excellent job of explaining the rationale behind their design choices. I particularly appreciate the inclusion of practical and empirical details, such as parameter settings and techniques to accelerate learning, which provide valuable insights for the reader.
Originality:  
While the paper shares some conceptual similarities with existing policy search algorithms, particularly REPS, it introduces significant distinctions. For example, the use of KL divergence for policy learning is a hallmark of REPS, and the supervised learning aspect has parallels to model-based reinforcement learning. However, these resemblances are largely superficial. At its core, the GPS approach diverges substantially from existing methods. Unlike model-based RL, this method focuses on learning policies through supervised learning rather than modeling the environment. The use of local models offers several advantages, such as simplified training, system stability, and improved convergence.
Significance:  
This paper makes a meaningful and valuable contribution to the fields of policy search and reinforcement learning. While it builds on existing methods, it introduces several innovative ideas. I strongly recommend this paper for publication. It is a well-executed piece of work that proposes a novel method for sample-efficient policy learning in unknown environments. Framing policy search as a supervised learning problem is a fascinating research direction that has the potential to yield further insights into this challenging area.