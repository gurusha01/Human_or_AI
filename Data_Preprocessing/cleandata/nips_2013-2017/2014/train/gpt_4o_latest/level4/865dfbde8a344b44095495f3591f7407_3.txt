This paper introduces a novel approach to solving Hidden Markov Models (HMMs) using Stochastic Variational Inference (SVI). The proposed algorithm samples subchains from sequential data while addressing the challenge of dependency disruptions at the edges of subchains. To tackle this issue, the authors expand the buffer region around the target subchain incrementally until the subchain posteriors stabilize across different buffer lengths. Experimental results demonstrate that the proposed method is significantly more efficient than existing approaches and is applicable to real-world datasets. Additionally, this work offers valuable insights into the application of online learning algorithms for time-dependent models.
Minor comments:
- Equation (1): The parameter φ is missing in p(y₁|x₁).
  
- Line 355: Replace "Table 4" with "Table 1."
- Table 1: Clarify the predictive log-likelihood for this experiment. What percentage of the data was held out? Provide details on the hyperparameter settings for this case. Additionally, why was ||A - A₀||_F not included in the comparison for this experiment? The error is difficult to interpret solely based on the log-predictive results.
- Across all experiments: The authors only specify the hyperparameter setting for k. What about the other hyperparameters?
- Page 8: In the Human chromatin segmentation experiment, what was the runtime for the DBN? This is a critical consideration for advancing large-scale HMM inference.