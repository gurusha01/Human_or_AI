The paper introduces a Deep Gaussian Mixture model (Deep GMM), which extends traditional Gaussian mixtures to a multi-layer architecture. The central concept involves stacking multiple GMM layers hierarchically. Deep GMMs can be interpreted as a generative model where a standard normal random variable undergoes successive transformations along a k-layer network, with each layer applying a transformation consisting of matrix multiplication and bias addition.
An equivalent shallow GMM can be constructed, but it would require an exponential number of mixture components.
Overall, the idea is quite compelling, and the authors propose several heuristics to accelerate the EM learning algorithm, including (1) employing hard EM and (2) utilizing a "folding" technique that collapses all layers above a given layer into a single "shallow" GMM model (though this approach becomes computationally expensive for lower layers).
However, my primary concern is the close relationship between this work and the following prior research on deep mixtures of factor analyzers:
Deep Mixtures of Factor Analyzers (ICML 2012)  
Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton  
Given the strong connections between GMMs and mixtures of factor analyzers, this prior work shares significant conceptual overlap. Similar to the proposed approach, deep MFA models can also be "folded" into shallow MFA models, and learning can be performed using EM, with the option of layer-wise pretraining.
It is crucial for the authors to explicitly discuss the similarities and differences between their work and the deep MFA framework.  
In general, the paper is well-written, but due to its resemblance to previously published work, the authors must clearly delineate their novel contributions.