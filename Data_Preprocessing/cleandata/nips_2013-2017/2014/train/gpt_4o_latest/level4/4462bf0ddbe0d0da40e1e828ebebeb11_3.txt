Determinantal Point Processes (DPPs) are distributions over a fixed ground set that assign higher probabilities to diverse subsets. DPPs can be parameterized using a positive semidefinite matrix (L). Since learning L is NP-hard, prior work has focused on partial learning of L. Learning a scalar vector for L, however, compromises the diversity aspect of DPPs. Previous approaches have often restricted the parametric form of L. In contrast, this paper introduces a learning method that does not impose such restrictions on L's parameterization. Unlike gradient ascent methods, which require a projection step that can compromise the similarity and diversity properties, the proposed method avoids this issue by leveraging an EM framework on eigenvalues and eigenvectors. The authors explore optimization algorithms that eliminate the need for projection. While projected gradient ascent necessitates projecting both eigenvalues and eigenvectors, the proposed method exploits the full-rank property of V to bypass eigenvector projection. The use of Jensen's inequality to lower bound the objective function and construct an EM procedure is a key innovation.
The proposed learning method is significant because it preserves the diversity property of DPPs, unlike prior approaches that primarily focus on the quality property. The step-by-step derivation of the EM procedure is particularly insightful. Mapping the constraint to an optimization over the Stiefel manifold to eliminate eigenvector projection and demonstrating that the inverse distribution is also a DPP are notable contributions. The authors evaluate their learning algorithm on both synthetic datasets and a product recommendation task. The experimental results support the paper's claimsâ€”specifically, that the learning method retains both the quality and diversity properties of DPPs. For instance, in the product recommendation task, only two of the top 10 products are replaced by KA with less likely alternatives, highlighting the paper's contribution and validating its initial claims. However, the conclusion feels somewhat rushed and does not fully emphasize all the contributions of the paper. Nonetheless, the paper is cohesively written, with well-structured proofs and an engaging step-by-step reduction of the optimization problem to an EM framework. 
While the paper is coherent and the proofs are well-articulated, additional experiments on real-world datasets could further underscore the importance of the diversity achieved by this algorithm in comparison to other methods in the field.