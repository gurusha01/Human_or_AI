This paper presents a framework for learning from options in reinforcement learning. An option is defined as a policy with a certain probability of terminating at a specific state. The authors introduce the concept of an "option policy," which functions as a high-level policy enabling multi-step transitions between states. They demonstrate how to make the option model universal with respect to rewards and propose a TD-style algorithm for learning with these models.
The universal option model proposed in this work is highly intriguing and potentially very useful. I appreciate the elegant theory provided, as the option model offers a sophisticated way to address the "memoryless" property of MDPs while preserving much of their simplicity. While I am not an expert in reinforcement learning and therefore cannot assess the novelty of this contribution, I find the model compelling and, as such, recommend acceptance.