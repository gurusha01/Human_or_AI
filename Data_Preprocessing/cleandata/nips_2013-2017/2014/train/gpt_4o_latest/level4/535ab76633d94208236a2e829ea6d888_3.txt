This paper addresses the challenge of determining the true labels of objects within crowd-sourcing systems and explores broader categories of adversarial voting strategies, including highly capable adversaries, beyond what has been previously studied. 
The authors introduce innovative reputation-based algorithms that leverage user disagreements and semi-matchings to identify adversarial users. These algorithms are demonstrated to enhance the quality of vote aggregation across three commonly used vote aggregation methods. Furthermore, the authors establish that their reputation definition aligns with the concept of user reliability and that the algorithm can effectively detect adversarial users, even when these adversaries employ sophisticated strategies, under the assumption that non-adversarial users exhibit high reliability.
Additionally, the authors derive bounds on the minimum level of disruption that can be caused by intelligent adversaries.
Overall, I found the paper to be both novel and engaging, particularly in its consideration of adversarial classes that are less commonly explored. However, the authors rely on several significant assumptions that may not hold in practical scenarios, such as (a) the extent to which 'honest' users correctly label objects (see Assumption 1 in Section 3.2), (b) adversarial users having complete knowledge of the voting patterns of honest users, and (c) adversaries being aware of the decision rule being used. These assumptions raise questions about the practical applicability of the results presented in Theorems 3 and 4.
Another aspect I found intriguing was the authors' rationale for not assigning any credit for agreement (Section 2), as they aim to avoid incentivizing adversaries to align with honest users. While this approach may be reasonable in scenarios where adversaries have full knowledge of honest user votes (e.g., adversaries could align with a subset of items they wish to leave unaffected and that already have substantial honest votes), it is unclear whether this strategy is justified in real-world settings where adversaries lack such knowledge of voting patterns.
Detailed comments:
- It would be valuable to explore how the results in Section 4 vary based on the 20% threshold applied to the set of users being removed.  
This paper offers a fresh perspective on user filtering in crowd-sourcing contexts. The authors present promising practical results supported by interesting theoretical insights, though the theoretical contributions rely on strong assumptions.