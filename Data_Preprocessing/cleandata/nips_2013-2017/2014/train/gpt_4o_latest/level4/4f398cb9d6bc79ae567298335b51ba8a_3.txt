This paper investigates a multi-layer conditional model for deep learning, where the input undergoes sequential multiplication by weight matrices followed by nonlinear transfer functions. The objective is to determine the weight matrices such that the multi-label target vectors of the training data are matched while keeping certain norms of the weight matrices small (regularization). The outputs of non-terminal layers are treated as latent variables. For two-layer models, reference [26] demonstrates that the optimal weight matrices can be approximately determined by solving a tractable convex optimization problem. This convex formulation is achieved using the following techniques:
(1) Nonlinear transfer constraints between the inputs and outputs of each layer are softly enforced via a nonlinear penalty function, which is linear in the layer's weight matrix.
(2) The bilinear coupling between the weight matrices and latent variables is removed through a coordinate transformation, reformulating the problem in terms of the linear response and input kernel matrix of each layer.
For three-layer models, an approximate reformulation is required to ensure joint convexity in the inputs, latent variables, and outputs of intermediate layers. When latent variables are Boolean-valued, [26] proposes an exact reformulation under the assumptions that the penalty functions are convex in the propensity matrix and output kernel, and that the domain of the output kernel admits a tight convex relaxation.
Building on [26], this paper proposes an approximate convex formulation for general models with intermediate layers. The central innovation is the introduction of normalized kernels, which are defined similarly to ordinary kernels but are normalized such that all eigenvalues are either 0 or 1. Additionally, classical regularization is replaced by value regularization (penalizing the norm of the weight matrices multiplied by the layer outputs), and the penalty functions encoding the nonlinear relationship between weighted inputs and outputs are assumed to take a specific form (with implicit linear transfer functions as step functions). A tight convex relaxation of the normalized output kernel domain is also required, with simple spectral constraints suggested.
The paper further introduces a nested optimization algorithm for training. The outer optimization, based on a conditional gradient algorithm, optimizes the training objective as a function of the normalized kernels, effectively setting all propensities to their optimal values given the kernels. This involves a totally corrective update step, efficiently implemented via block coordinate descent.
The idea of using normalized kernels is innovative and merits publication. Numerical experiments demonstrate that three-layer models can outperform two-layer architectures in both synthetic and real-world scenarios.
The paper claims that the final loss function is jointly convex in all optimization variables (normalized kernels and propensities). However, the problem includes non-standard constraints requiring propensities to be representable as matrix products involving the input and output kernels of each layer. These constraints are inherently nonconvex. If my understanding is correct, the paper does not provide a fully convex model for training multi-layer architectures, and the authors may overstate their contribution. It would be helpful to explicitly enumerate all approximations made when transitioning from the desired model to the one actually solved.
The exposition could be improved by presenting the full two-layer model from [26] and the complete three- (or multi-) layer model proposed here, including all constraints. To better understand the new formulation with normalized kernels, it would also be useful to compare this approach with that of [26] when applied to a two-layer model. What are the specific differences between the two methods in this case?
Additionally, it is unclear whether the methods from [26] extend to multi-layer architectures when all latent variables are Boolean-valued. This point requires clarification.
On line 352, it is stated that R is not jointly convex in its variables, and thus an alternating optimization scheme can only find a stationary point of R. However, I disagree that the limit is necessarily a stationary point or local minimum. For example, consider a convex maximization problem with linear constraints and a concave objective function: max f(x,y) s.t. 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, where f(x,y) = min{-x + 2y, 2x - y}, which is jointly concave in x and y. Starting at (1,0) and optimizing with respect to x (fixing y = 0) leads to (0,0), and optimizing over y does not move away from this point. This demonstrates that alternating optimization can get stuck at a non-stationary point. It is unclear why the proposed block coordinate descent algorithm (lines 348/349) would not encounter similar issues. If we cannot guarantee a stationary point of R, the relevance of Theorem 1, a central contribution of the paper, becomes questionable.
Specific Comments:
- Line 212: Explicitly state that the squared Frobenius norms of the weight matrices are used as regularization terms.
- Postulate 1: Clarify the meaning of the superscript "u" in L^u. Does it denote "unnormalized"?
- Postulate 2: The statement that the domain of N = Θ'Θ can be "reasonably approximated" is too vague and should be made more precise.
- Line 194: If the assumption that Θ generally has full row rank is necessary, it should be explicitly stated.
- Equation (7): Verify whether the leftmost term should involve Φ' (transposed) instead of Φ.
- Section 3.2: This section is difficult to follow. It would help to more clearly explain why the proposed penalty function corresponds to a step-transfer function.
- Line 396: The claim that "Parity is easily computable" should be supported by a reference.
The idea of using normalized kernels to reformulate deep learning models as optimization problems with favorable convexity properties is both innovative and nontrivial. While the authors have made a commendable effort to present their material, the contributions of the paper appear somewhat overstated due to the reliance on several approximation steps. Furthermore, the claim that block coordinate descent necessarily converges to a stationary point seems incorrect in general.