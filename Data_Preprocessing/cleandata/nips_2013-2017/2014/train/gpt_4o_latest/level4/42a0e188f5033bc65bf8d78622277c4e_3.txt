This paper introduces a randomized parallel variant of ADMM, designed to handle problems with multiple blocks. In each iteration, the proposed PADMM algorithm selects K random blocks, updates the block primal vector, and subsequently updates the dual vector using a backward step. The authors emphasize the significance of the backward step, as it renders the dual update more conservative, thereby ensuring global convergence. Additionally, the algorithm incorporates the option to include a proximal term in the primal update, simplifying certain optimization problems. The authors provide a rigorous theoretical analysis of the algorithm, proving its global convergence and establishing its iteration complexity. Specifically, for a total of J blocks, when PADMM selects K blocks randomly per iteration, the convergence rate is shown to be O(J/(TK)) after T iterations.
The PADMM algorithm offers two primary advantages: (1) it enables full parallelization of the primal step, and (2) it supports relatively larger update step sizes compared to other methods. Notably, PADMM can outperform sADMM due to its allowance for larger step sizes. It is also more flexible than PJADMM and offers greater parallelizability than GSADMM. The authors validate the algorithm on robust principal component analysis and overlapping group lasso problems, with experimental results demonstrating promising outcomes. PADMM achieves the desired accuracy with reduced computation time. However, the authors have not presented results for PADMM's performance in a parallel implementation. Additionally, compared to traditional ADMM, tuning the three parameters of PADMM is less straightforward.
In summary, this work represents a valuable contribution to addressing ADMM-type problems with multiple constraints. While maintaining the same convergence rate as ADMM, the proposed algorithm supports parallelization and demonstrates strong practical performance. The paper is well-written, with both the theoretical and empirical components being robust.