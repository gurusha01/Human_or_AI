The paper introduces a novel approach for solving Markov Decision Processes (MDPs). This method, proposed as an alternative to approximate policy or value iteration, focuses on directly minimizing the Optimal Bellman Residual (OBR). The authors begin by justifying their approach, demonstrating that the loss bound associated with OBR is often tighter than that of policy or value iteration, a result previously established in the literature [9,15]. They then prove that an empirical estimate of OBR is consistent in the Vapnik sense, meaning that minimizing the empirical OBR corresponds to minimizing an upper bound on the true OBR, even when the MDP model is unknown. Furthermore, the authors show that OBR can be expressed as the difference of two convex functions, enabling the use of standard Difference of Convex Functions (DC) optimization techniques to find a local optimum. They argue that this local optimum is typically close to the global optimum. Finally, the authors demonstrate that their method performs comparably to standard techniques like LSPI and Fitted-Q learning, while also yielding value estimates with lower variance.
Pros:  
The paper is exceptionally clear, well-written, and well-structured. From a technical standpoint, it is robust and sound. The proofs are both non-trivial and original. Theorem 4, which establishes the decomposition of OBR into a difference of convex functions, stands out as the most practically significant result. The motivation provided through the loss bound analysis in Section 2.2 is compelling. Additionally, the authors establish the consistency of empirical risk minimization when the value function is approximated using basis functions (features), which is a valuable theoretical contribution.
Cons:  
The empirical evaluation is notably weak. The authors test their method on a single, simple artificial MDP, and the results do not show any clear advantage over LSPI or Fitted-Q learning. If OBR minimization consistently fails to outperform LSPI, the practical relevance of this contribution becomes questionable. Furthermore, the authors do not report the computational cost associated with solving multiple convex optimization problems. Intuitively, if the DC decomposition is not well-constructed, DC programming could be slower than dynamic programming. That said, the primary contribution of this paper lies in introducing this novel approach to solving MDPs, which is intriguing. It would be interesting to explore how the DC iterations relate to policy or value iterations.
Questions/Comments:  
1. What is \(\mu\) in line 120, and how does it differ from the state-action distribution \(v\)? This should be clarified before Equation (1).  
2. Equation (3) largely repeats Equation (2) and could be omitted.  
3. There are typos in line 158 ("better that," "manly") and line 313 ("is currently is").  
4. The proof of Theorem 3 is straightforward and could be omitted. Removing Equation (3) would allow space to bring the more critical proof of Theorem 4 into the main paper.  
5. In your experiments, is the computational cost of DC programming comparable to that of dynamic programming?  
Overall:  
This is a strong and well-written paper. To the best of my knowledge, the decomposition of the Optimal Bellman Residual into a difference of convex functions is an original contribution. While the empirical evaluation is weak and inconclusive, I do not believe this should detract from the paper's value.