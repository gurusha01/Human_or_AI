The authors propose an intriguing novel method for supervised and semi-supervised learning based on Fredholm kernels. The paper provides a detailed discussion of various aspects of the proposed approach. Overall, I find the work to be highly engaging and thought-provoking.
However, I have a few suggestions and questions that the authors may wish to address:
- Section 2 (from Eq. 1 to the equation on line 99):  
The Nystrom method also involves an approximation to an integral equation (in that case, kernel PCA). It would be beneficial for the authors to mention this and clarify how their approach is similar to or distinct from the Nystrom method.
- Section 2, Eq. 2:  
In the domain of support vector machines and kernel methods, there are existing approaches that utilize operator equations. It would be helpful if the authors could reference this prior work and explain whether Eq. 2 represents a special case of these approaches or how it differs from them.
- Section 2, line 128 (Representer Theorem):  
Eq. 2 is relatively uncommon in learning theory. The authors assume the existence of a representer theorem. They should either provide a proof of the representer theorem in this context or cite a precise reference and elaborate on how Eq. 2 fits as a special case.
- Section 4, Eq. 8:  
The interpretation of the method as soft-thresholding PCA is both appealing and insightful. However, it would be useful to discuss how this approach compares to other methods, such as Fisher kernels and probability product kernels.
- Typos:  
  - Line 165: "Not that" should be corrected.  
  - Line 221: "achived" should be "achieved."  
  - Line 341: "chossing" should be "choosing."  
  - Line 672: "Table ??" should be clarified or corrected.
The proposed method introduces a new approach to supervised and unsupervised learning using Fredholm kernels, which are data-dependent.