This paper addresses a significant challenge in unsupervised machine learning and optimization: clustering with outlier detection. Clustering has been extensively studied in both theoretical and practical domains. Researchers have explored various clustering problems, including k-means, k-medians, and k-centers in geometric contexts, as well as correlation clustering and spectral clustering in graph theory. However, a key issue in clustering is the presence of outliers, which can substantially affect the final results.
The authors propose a gradient descent algorithm to tackle clustering with outlier detection. They adapt the integer programming model from [8] by incorporating an outlier component into the constraints. The modified model is then relaxed into a series of Lagrange relaxations, which are solved using a gradient descent approach.
The experimental evaluation includes both synthetic and real-world datasets, demonstrating the proposed method's advantages over two existing approaches.
Positive aspects:
1. The use of Lagrange relaxation for outlier detection is an innovative approach.
2. The algorithm is straightforward and easy to implement, making it practical for real-world applications.
Negative aspects:
1. The theoretical analysis is insufficient. For instance, in Section 4.2, the authors should provide more detailed insights into the convergence of their method.
2. The paper lacks sufficient references. In computational statistics, there are several recent techniques for outlier trimming in regression and clustering, such as those discussed in David Mount et al.'s works, "A practical approximation algorithm for the LMS line estimator" and "On the least trimmed squares estimator."
In summary, this paper introduces a novel Lagrange relaxation approach to address a challenging problem in clustering. While the technique is innovative, it requires more comprehensive theoretical analysis and a broader contextualization within the existing literature.