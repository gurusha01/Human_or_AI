This paper presents an extension to the earlier Bayesian formulation of traditional Support Vector Machines (SVMs). The prior Bayesian approach introduced a likelihood modeled as a Gaussian mixture, and the authors build upon this by incorporating priors on the scale-mean parameter. This extension leads to a skewed Laplace likelihood, which deviates from the earlier hinge loss formulation. Furthermore, the authors propose a nonlinear SVM formulation by extending the Gaussian prior on the weight vectors to a Gaussian process. Two distinct optimization methods are introduced to solve the resulting optimization problem.
The paper provides a compelling example of extending the conventional SVM framework into a Bayesian paradigm through a well-structured formulation involving prior distributions. As stated by the authors, prior work [4] established a link between an infinite Gaussian mixture model and a hinge loss likelihood, which yielded the SVM solution, but the formulation suffered from an improper flat prior. The authors demonstrate how this formulation can be refined and how inference can be conducted using a more rigorous approach. The results effectively illustrate the transition from a hinge-type likelihood to a skewed Laplace likelihood. The explanations are clear, and I found the manuscript enjoyable to read. However, I struggled to follow the derivation of the predictive distribution in Eq. (11). Is the derivation included in the supplementary material? If not, I would like to see a detailed derivation of Eq. (11) either in the main text or in the supplementary material if the paper is accepted.
Overall, the paper is well-written, and the proposed formulation appears to be sound. It would serve as a valuable resource for most NIPS readers, offering an insightful example of a Bayesian formulation of SVMs.