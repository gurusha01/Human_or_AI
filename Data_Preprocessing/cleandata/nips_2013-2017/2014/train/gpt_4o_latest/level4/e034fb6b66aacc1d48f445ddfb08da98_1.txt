This paper introduces a novel approach to enhancing vector-space word embeddings by incorporating side information through an "attribute vector" that modulates the word-projection matrices. Conceptually, this can be thought of as word-projection tensors (though these tensors are factorized in practice), with the attribute vector determining the loadings for the tensor slices. While the study is conducted within the framework of log-bilinear language models, the core idea is likely applicable to other word embedding methodologies.
The theoretical framework presented in the paper is clear and well-articulated. However, the experimental section lacks the same level of clarity and depth. A more focused set of experiments with deeper analysis would have been preferable to the current broad set of experiments, which provide limited insight. For instance, the experiments at the start of section 3 (Tables 2 & 3) are somewhat unclear in their purpose. Many models are capable of generating text, and it is not evident what these examples specifically demonstrate about the proposed model. Even a simple n-gram language model can interpolate between texts like the Bible and Caesar. Additionally, the authors should specify the values used for the initial learning rate, decay factor, momentum, and momentum increase factor.
In section 3.1, the nature of the attributes is unclear. Are they simply sentence vectors averaged over sub-phrases, or something else entirely? The paper would benefit from a clearer explanation and motivation for the choice of attributes.
In section 3.2, the notation could be improved for clarity. For example, it should be made explicit that S is derived from l, while S' and Ck are derived from l'. It might also be clearer to denote v as vl. Furthermore, the attributes (x) in this context are not well-defined. In figure 1 (right), the term "1-hot attribute vector" is misleading, as it actually refers to the language-id vector rather than the attribute vector. Additionally, the discrepancy between Germany and Deutschland being far apart, while translations of other countries are close together, warrants further investigation. Is this an error, noise that the model does not account for, or does it reveal an interesting aspect of the model's learning process (e.g., a potential "us vs. them" distinction where Germany in English and Deutschland in German are treated as different concepts)?
In section 3.3, the choice of a 100-dimensional attribute vector for 234 unique attributes is not adequately justified. A clearer rationale for this decision would strengthen the paper. 
Overall, the paper introduces the concept of an attribute vector to modulate word embeddings, with a strong theoretical foundation. However, the experimental section requires significant improvement to better support the proposed approach.