The paper addresses a significant problem in the multi-armed bandit literature: whether sub-linear regret bounds can be achieved in scenarios with non-stationary mean rewards of finite variation. It provides a conclusive answer by establishing matching lower and upper bounds of order O(T^(2/3)) under the assumption that the total variation of mean rewards is bounded by some VT ≥ 0 over T rounds. The upper bound is derived for a phased version of the EXP3 algorithm, which resets itself after every O((T/VT)^(2/3)) rounds.
General:
The paper is well-written and easy to comprehend. I carefully reviewed the main results and technical proofs (those included in the main paper) and did not identify any errors. Furthermore, the critical steps of the technical proofs are clearly explained. Overall, I believe the paper makes a sufficient contribution to merit acceptance at the conference.
Technical Comments:
1. A closely related framework to the non-stationary reward setting is that of state-dependent ergodic bandits (e.g., restless bandits), as acknowledged in the paper. While some earlier works in ergodic bandits are discussed, the paper does not fully cover the state-of-the-art in this area and overlooks some recent works. For instance, the regret bounds for restless bandits with unknown dynamics (Ortner, 2012) and the bandit model for correlated feedback (Azar, 2014) could provide additional context and insights.
2. The REXP3 algorithm requires VT to be provided as an input, which may pose a limitation in scenarios where the variation in rewards is unknown. It would be valuable to explore whether this requirement can be relaxed while still achieving similar regret bounds. Additionally, some discussion on the implications of underestimating VT would strengthen the paper.
3. The proposed algorithm adapts a variant of EXP3, originally designed for adversarial bandits, to address a problem that is stochastic (albeit time-varying) in nature. While this approach suffices to achieve minimax regret rates in the worst-case scenario, it does not leverage the stochastic structure of the problem. Consequently, it may not be the most effective strategy for problem-dependent settings. It would be interesting to investigate whether problem-dependent bounds could be achieved by extending stochastic bandit algorithms to this setting—perhaps through a variant of UCB that incorporates a mechanism to "forget" outdated information.
Summary:
The results are impressive, technically sound, and rigorous. However, the paper would benefit from a discussion on handling cases with unknown variation bounds.