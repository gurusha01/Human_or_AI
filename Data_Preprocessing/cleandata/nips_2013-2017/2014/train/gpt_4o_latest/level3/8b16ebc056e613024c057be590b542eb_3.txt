This paper addresses the problem of reconstructing neural network topology from calcium imaging data by enhancing inverse covariance estimation with a learned convolution filter. The authors propose a supervised optimization approach to learn the convolution filter and regularization parameters, achieving competitive Area Under the Curve (AUC) scores while significantly reducing computational time compared to prior methods. The work builds on the Kaggle Connectomics competition results, where inverse covariance estimation was a key component of top-performing solutions. By introducing a convolution filter, the authors improve both the accuracy and generalizability of the method, avoiding the need for computationally expensive grid searches or coordinate ascent.
Strengths:
The paper tackles an important neuroscience problem—inferring neural connectivity from noisy calcium imaging data—a task with implications for understanding brain function and diagnosing diseases. The proposed method is original in its application of a learned convolution filter to preprocess signals prior to inverse covariance estimation. This innovation reduces training time to under two hours while maintaining competitive AUC scores, a significant improvement over previous approaches requiring days of computation. The methodology is well-motivated, and the results are clearly presented, with strong empirical evidence supporting the claims. The use of supervised optimization to learn signal processing parameters is a notable contribution, as it generalizes better to datasets with varying sparsity levels.
Weaknesses:
The paper has several limitations that warrant further discussion. First, the convolution filter length is restricted to 10 time steps, but the rationale for this choice is not thoroughly justified. It is unclear whether this restriction allows the filter to fully converge or if longer filters could yield better results. Second, the inherent limitation of inverse covariance estimation—its restriction to undirected graph recovery—is not addressed. This is a significant drawback, as directionality is often critical in neural connectivity studies. Additionally, the parameter sensitivity of $\chi$ in Eq. 7 is not explored in depth, leaving questions about its generalizability to networks with different sparsity levels. Lastly, while the convolution filter improves performance, the underlying reasons for its generalization capabilities remain unexplained, limiting the interpretability of the method.
Clarity:
The paper is generally well-written and organized, but it could benefit from additional clarity in certain areas. For instance, the dataset, timescales, and evaluation metrics (e.g., AUC) are not explained in sufficient detail for non-expert readers. Including a more accessible explanation of these aspects would enhance the paper's readability and impact.
Originality and Significance:
The work is original and provides an incremental improvement to a known algorithm. However, it does not address the fundamental limitations of inverse covariance estimation, such as its inability to infer directed connections. While the proposed method is faster and more generalizable, its contribution is incremental rather than transformative. Nevertheless, the study is likely to be of interest to researchers in neuroscience and machine learning due to its practical implications and computational efficiency.
Recommendation:
Pros for acceptance: The paper addresses a relevant problem, offers a novel and computationally efficient solution, and demonstrates strong empirical results.  
Cons for acceptance: The work is incremental, does not address key limitations of the underlying method, and lacks clarity in some areas.  
Overall, this paper is a solid contribution to the field but would benefit from addressing the noted weaknesses. I recommend acceptance with minor revisions.