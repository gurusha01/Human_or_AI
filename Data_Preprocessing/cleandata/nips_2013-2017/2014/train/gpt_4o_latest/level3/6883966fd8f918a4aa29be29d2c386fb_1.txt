The paper presents a novel active learning algorithm for parametric linear regression with random design, focusing on improving distribution-dependent constant factors in convergence rates compared to passive learning. This is a significant contribution, as it is the first active learner for this setting that provably improves over passive learning. The authors employ a rejection sampling scheme to adjust the sampling distribution without altering the optimal solution, which is innovative and grounded in stratification techniques from Monte Carlo integration. The algorithm optimizes a scaling function \(\phi\) among piecewise constant functions, leveraging an initial sample to estimate a linear function. Theoretical guarantees are provided, including a risk bound that approaches the "oracle" rate, demonstrating the potential for active learning to outperform passive learning in certain scenarios.
Strengths:
1. Theoretical Contribution: The paper provides rigorous finite-sample convergence guarantees, addressing a gap in active learning for linear regression. The risk bound and its connection to the oracle rate are well-motivated and theoretically sound.
2. Novelty: The use of rejection sampling to simulate distributions and the stratification approach to approximate the oracle risk are innovative. The algorithm's ability to adapt to the underlying distribution is a notable advancement.
3. Clarity of Results: The paper clearly demonstrates the potential improvement of active learning over passive learning, particularly in cases where the distribution is asymmetric or heteroscedastic.
4. Broader Impact: The proposed method has implications for domains where labeled data is expensive but unlabeled data is abundant, making it relevant to a wide audience.
Weaknesses:
1. Trade-off Analysis: While Theorem 5.1 is central to the paper, the trade-off between \(K\) (number of partitions) and \(\rho^*_A\) (oracle risk for a given partition) is underexplored. Including concrete examples or simulations to illustrate this trade-off would enhance understanding.
2. Notation Ambiguity: The notation for \(P_\phi\) is unclear and could benefit from additional clarification. This would improve the paper's accessibility to readers unfamiliar with the specific rejection sampling framework.
3. Missing Citation: The paper omits a relevant reference to Efromovich (2005), which also studies active regression in a nonparametric context. Including this citation would situate the work more effectively within the broader literature.
4. Practical Considerations: The algorithm assumes access to certain properties of the marginal distribution \(D_X\), such as the covariance matrix, which may not always be feasible in practice. A discussion on how to address this limitation would strengthen the paper.
Recommendation:
This paper is a solid contribution to the field of active learning and regression. It is technically sound, addresses an important problem, and introduces novel ideas with theoretical guarantees. However, the authors should address the trade-off between \(K\) and \(\rho^*_A\), clarify ambiguous notation, and include the missing citation. With these improvements, the paper would be even more impactful. I recommend acceptance, contingent on minor revisions. 
Pros:
- Rigorous theoretical guarantees.
- Novel use of rejection sampling and stratification.
- Demonstrates clear improvement over passive learning.
Cons:
- Insufficient discussion of trade-offs in Theorem 5.1.
- Ambiguity in notation and missing citation.
- Limited discussion of practical challenges.