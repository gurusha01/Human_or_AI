The paper presents a novel approach to solving large Markov Decision Processes (MDPs) by leveraging Difference of Convex functions (DC) Programming to minimize the norm of the Optimal Bellman Residual (OBR). The authors establish that the OBR norm can be expressed as a difference of convex functions, enabling the application of DC algorithms (DCA) to reinforcement learning (RL). This decomposition is both novel and straightforward, offering a compelling theoretical contribution. The authors also demonstrate that minimizing the OBR norm provides theoretical benefits, such as tighter error bounds compared to traditional Approximate Dynamic Programming (ADP) methods like Approximate Value Iteration (AVI) and Approximate Policy Iteration (API). Furthermore, they prove the Vapnik-consistency of the empirical norm of the OBR, ensuring that the proposed approach is theoretically sound.
The paper's strengths lie in its originality and theoretical rigor. The decomposition of the OBR into a difference of convex functions is a novel contribution, and the connection to DC programming opens up new possibilities for addressing non-convex optimization problems in RL. The authors also provide a detailed theoretical analysis, including consistency proofs and error bounds, which are well-grounded in the literature. The small empirical study comparing DCA with LSPI and Fitted-Q on finite-state MDPs shows that the proposed approach is competitive, achieving comparable performance with lower variance, which is a promising result.
However, the paper has notable weaknesses. The empirical evaluation is limited to synthetic Garnet problems and lacks experiments on real-world domains, which restricts the practical significance of the results. Additionally, the methodology for parameter tuning in DCA is not detailed, leaving questions about the robustness and reproducibility of the approach. The paper also contains several typos in both mathematical expressions and writing, which detracts from its clarity and readability. While the theoretical contributions are strong, the empirical validation does not fully demonstrate the scalability or applicability of the method to more complex or realistic settings.
Arguments for Acceptance:
1. Novel and theoretically sound decomposition of the OBR as a difference of convex functions.
2. Promising new direction for reinforcement learning with potential for further exploration.
3. Competitive empirical results with lower variance compared to state-of-the-art methods.
Arguments Against Acceptance:
1. Limited empirical evaluation, with no experiments on real-world domains.
2. Lack of detailed methodology for parameter tuning and implementation specifics.
3. Typos and clarity issues in the manuscript.
Recommendation:
The paper provides a novel and theoretically significant contribution to reinforcement learning, but the limited empirical evaluation and presentation issues need to be addressed. I recommend acceptance with major revisions, contingent on improving the empirical study and addressing clarity and presentation concerns.