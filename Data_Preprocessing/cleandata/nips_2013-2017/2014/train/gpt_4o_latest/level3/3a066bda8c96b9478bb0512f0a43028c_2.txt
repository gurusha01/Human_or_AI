The paper introduces a novel algorithm, expectation loss SVM (e-SVM), designed to address scenarios where training labels are continuous values (positiveness) rather than binary. This method incorporates per-example weights into the loss function, extending traditional SVMs to handle weakly supervised learning tasks. The authors further propose a latent variable extension of e-SVM for cases where positiveness values are unobserved. Experimental results demonstrate its effectiveness in semantic segmentation and object detection tasks, achieving state-of-the-art performance on the PASCAL VOC 2007 dataset. However, the paper lacks a theoretical analysis of the non-convex optimization problem introduced by the latent variable extension, which could strengthen its contributions.
Strengths:
1. Novelty and Applicability: The e-SVM algorithm addresses a relevant and challenging problem in weakly supervised learning, particularly for large-scale datasets where detailed annotations are impractical. Its application to semantic segmentation and object detection is timely and impactful.
2. Experimental Validation: The method demonstrates superior performance compared to SVC and SVR baselines, with measurable improvements in object detection (e.g., a 1.5% increase in AP on PASCAL VOC 2007). The introduction of new evaluation metrics (TAPC and NDCG) for segment classification is a valuable contribution.
3. Flexibility: The algorithm is generalizable, supporting various segment proposal detectors, image features, and classifiers. This adaptability enhances its potential for broader adoption.
Weaknesses:
1. Theoretical Gaps: The latent variable extension introduces non-convexity, but the paper does not provide a theoretical analysis of convergence or guarantees. This omission weakens the rigor of the proposed approach.
2. Limited Differentiation: The method's novelty is somewhat undermined by its similarity to existing instance-weighted SVMs (e.g., SVMlight). The authors do not sufficiently clarify how e-SVM fundamentally differs from or improves upon these approaches.
3. Strong Regularization: The use of strong regularization to constrain latent variables raises questions about the method's robustness and generalizability. The paper does not explore the impact of varying regularization strength.
4. Minor Issues: The claim in L050 about logistic regression being unbounded is incorrect, as logistic regression outputs probabilities in [0, 1]. Additionally, there is a typo in the abstract ("continues" should be "continuous").
Recommendation:
While the paper presents a promising approach with strong empirical results, the lack of theoretical analysis and insufficient differentiation from existing methods are significant drawbacks. If these issues can be addressed, the paper would make a valuable contribution to the field. For now, I recommend acceptance with major revisions.
Arguments for Acceptance:
- The proposed method addresses a relevant and challenging problem in weakly supervised learning.
- Experimental results demonstrate clear improvements over baseline methods.
- The algorithm's flexibility and generalizability make it a useful tool for the community.
Arguments Against Acceptance:
- Theoretical gaps in the latent variable extension need to be addressed.
- The novelty of the method is not convincingly established relative to existing approaches.
- Strong regularization raises concerns about the robustness of the results.