The paper introduces the Parallel Direction Method of Multipliers (PDMM), a novel randomized block coordinate method for minimizing block-separable convex functions under linear constraints. PDMM generalizes existing Alternating Direction Method of Multipliers (ADMM) variants by allowing parallel updates of multiple blocks of the primal variable and incorporating a backward step for the dual variable. The authors establish global convergence and a sublinear iteration complexity for PDMM, demonstrating its theoretical soundness. Experimental results on robust principal component analysis (RPCA) and overlapping group lasso problems indicate that PDMM outperforms state-of-the-art methods in terms of runtime and convergence.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous theoretical foundation for PDMM, including global convergence and sublinear iteration complexity. The connection between PDMM and existing methods (e.g., sADMM and PJADMM) is insightful and enhances understanding of the broader family of ADMM variants.
2. Practical Relevance: The proposed method addresses a critical gap in the literature by enabling parallel updates for multi-block problems, which is particularly relevant for large-scale optimization tasks in machine learning.
3. Experimental Validation: The experiments convincingly demonstrate the practical advantages of PDMM over competing methods, particularly in terms of runtime efficiency and scalability.
4. Novelty: PDMM introduces a backward step for the dual variable, a unique feature that mitigates the aggressive progress of dual updates in traditional ADMM approaches. This innovation is well-motivated and supported by empirical results.
Weaknesses:
1. Clarity of Algorithm Description: The equations (5) and (6) describing the algorithm lack clarity regarding whether all or a subset of dual variable blocks are updated and how the blocks are selected. This ambiguity could hinder reproducibility.
2. Notation Issues: In Theorem 2, the notation appears incomplete, as the summation "\(\sum_{i=1}^I\)" is missing. This oversight could confuse readers and detracts from the paper's overall polish.
3. Discussion of Theorem 2: The paper does not sufficiently discuss the consistency of Theorem 2 with the convergence rates of other ADMM variants. Additionally, the optimal strategy for splitting the transformation matrix \(A\) is not explored in depth, leaving a gap in the practical guidance for implementing PDMM.
4. Comparative Analysis: While the experiments are compelling, the paper could benefit from comparisons with tensor completion problems, which also involve multi-block structures. This would broaden the scope of the evaluation and further validate PDMM's applicability.
Minor Issues:
- There is a missing space in equation (9).
- An extra space appears before "Section" on line 217.
Recommendation:
I am positive about accepting this paper, as it makes a significant theoretical and practical contribution to the field of optimization and machine learning. However, I strongly encourage the authors to address the clarity issues in the algorithm description and provide additional discussion on Theorem 2, particularly regarding its consistency with existing ADMM variants and practical implementation considerations. These improvements would enhance the paper's accessibility and impact.