The paper presents a novel randomized parallel algorithm, Parallel Direction Method of Multipliers (PDMM), as a generalization of ADMM for solving optimization problems with multi-block linear constraints. PDMM introduces several key innovations, including a backward step in the dual update to ensure global convergence, the ability to add a proximal term to the primal update for inexact solutions, and support for parallel randomized block coordinate updates. The authors establish theoretical guarantees for global convergence and iteration complexity, achieving a convergence rate of \(O(J/(TK))\) after \(T\) iterations when \(K\) blocks are updated out of \(J\). Experimental results on robust principal component analysis (RPCA) and overlapping group lasso demonstrate the algorithm's computational efficiency and competitive performance compared to existing methods.
Strengths:
1. Theoretical Contributions: The paper provides rigorous theoretical analysis, including global convergence and iteration complexity guarantees. The convergence rate matches that of traditional ADMM while enabling parallelization, which is a significant advancement.
2. Algorithmic Innovation: The backward step in the dual update is a novel contribution that addresses the instability of direct Jacobi updates. Additionally, the flexibility to add proximal terms enhances the algorithm's applicability to a broader range of problems.
3. Practical Efficiency: PDMM supports full parallelization of the primal step and allows larger step sizes compared to methods like sADMM, PJADMM, and GSADMM, making it computationally efficient.
4. Empirical Validation: The experimental results on RPCA and overlapping group lasso demonstrate the algorithm's effectiveness, with reduced computation time compared to state-of-the-art methods.
5. Clarity and Organization: The paper is well-written, with clear explanations of the algorithm, theoretical results, and experimental setup. The connections to existing methods (e.g., sADMM and PJADMM) are insightful and help contextualize the contributions.
Weaknesses:
1. Parallel Implementation: While the algorithm is designed for parallelization, the paper does not report results for a parallel implementation. This omission limits the practical evaluation of PDMM's scalability and real-world efficiency.
2. Parameter Tuning: PDMM introduces three parameters (\(\taui\), \(\nui\), and \(\eta_j\)), which require careful tuning. This makes the algorithm less convenient to use compared to traditional ADMM, where fewer parameters need adjustment.
3. Experimental Scope: The experiments focus on two applications (RPCA and overlapping group lasso). While these are relevant benchmarks, additional experiments on other problem domains could strengthen the empirical validation.
4. Comparison to GSADMM: Although PDMM is shown to outperform GSADMM in runtime, GSADMM achieves faster convergence in terms of iterations. A deeper discussion of this trade-off would be helpful.
Arguments for Acceptance:
- The paper makes a strong theoretical and practical contribution by extending ADMM to multi-block problems with randomized parallel updates.
- The convergence guarantees and empirical results demonstrate the algorithm's robustness and efficiency.
- The work is well-situated in the literature, addressing limitations of existing methods and providing new insights into ADMM variants.
Arguments Against Acceptance:
- The lack of parallel implementation results weakens the practical evaluation of the proposed method.
- The need for extensive parameter tuning may limit the algorithm's usability in practice.
Recommendation:
Overall, the paper is a valuable contribution to the field of optimization and machine learning. While the absence of parallel implementation results is a notable limitation, the theoretical and empirical advancements justify acceptance. I recommend accepting the paper, with a suggestion to include parallel implementation results in future work.