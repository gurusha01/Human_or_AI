The paper introduces the concept of pseudo-ensembles, a framework that formalizes the idea of perturbing a parent model to spawn a collection of child models, as exemplified by dropout in deep neural networks. The authors propose Pseudo-Ensemble Agreement (PEA) regularization, a novel method that enhances model robustness to perturbations and extends naturally to semi-supervised learning. The paper demonstrates that PEA regularization matches dropout's performance in supervised learning and outperforms it in semi-supervised and transfer learning scenarios. Additionally, the authors apply pseudo-ensembles to improve the Recursive Neural Tensor Network (RNTN) for sentiment analysis, achieving competitive results.
Strengths:
1. Conceptual Contribution: The formalization of pseudo-ensembles provides a unifying framework for understanding methods like dropout and input-space perturbations, offering a fresh perspective on robust learning.
2. Novel Regularization Technique: PEA regularization is a meaningful contribution that shows strong empirical performance across supervised, semi-supervised, and transfer learning tasks. Its ability to outperform dropout in semi-supervised and transfer learning is particularly noteworthy.
3. Broad Applicability: The pseudo-ensemble framework is general and can be applied to various model classes and noise processes, as demonstrated in the sentiment analysis case study.
4. Empirical Validation: The paper provides comprehensive experimental results, including comparisons with state-of-the-art methods, which support the efficacy of PEA regularization.
Weaknesses:
1. Unclear Connection to Dropout: While the authors claim that PEA regularization is conceptually related to dropout, the connection is not well-articulated. The explanation of how discouraging co-adaptation contributes to success is unconvincing and requires further theoretical or empirical support.
2. Design Choices: The decision to penalize variance across all layers rather than focusing on the output layer is not adequately justified. A more natural formulation might involve penalizing only the output layer variance.
3. Limited Explanation of Figures: Figure 1 is not explained in the text, which detracts from the clarity of the presentation.
4. Semi-Supervised Learning Limitations: Table 2 suggests that PEA regularization is less effective at leveraging unlabeled data compared to DAE pre-training (PEV+), which raises questions about its generalizability in semi-supervised settings.
Pro and Con Arguments for Acceptance:
- Pro: The paper introduces a novel and generalizable regularization technique with strong empirical results, making it a valuable contribution to the field.
- Con: The unclear theoretical connection to dropout and the lack of justification for certain design choices weaken the paper's conceptual rigor.
Recommendation: The paper is of high quality and significance, particularly for researchers interested in robust learning and semi-supervised methods. However, the authors should address the unclear dropout connection, justify their design choices, and improve the clarity of the presentation. With these revisions, the paper would make a strong contribution to the conference.