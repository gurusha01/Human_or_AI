This paper presents a novel approach to deep learning by proposing a fully convex formulation for multi-layer architectures, leveraging normalized kernels and innovative regularization techniques. The primary objective is to address the challenges of training deep models to global optimality while maintaining computational efficiency. The authors extend prior work on two-layer convex models ([25]) to three-layer architectures, introducing normalized kernels with eigenvalues constrained to 0 or 1, and proposing a nested optimization algorithm combining conditional gradient methods and block coordinate descent.
Strengths:
1. Innovation: The use of normalized kernels to achieve convexity in multi-layer architectures is a significant contribution. This approach effectively addresses the bilinear coupling and joint input-output optimization challenges that arise in deep models.
2. Empirical Results: The experiments convincingly demonstrate that the proposed three-layer model (CVX3) outperforms the two-layer model (CVX2) on both synthetic and real-world datasets. The results highlight the representational power of deeper architectures even under convex formulations.
3. Algorithmic Design: The nested optimization algorithm is well-constructed, with a focus on computational efficiency. The use of block coordinate descent to address the complexity of totally corrective updates is particularly noteworthy.
4. Theoretical Insights: The paper provides a rigorous theoretical foundation for the proposed methods, including proofs of convexity under specific relaxations and guarantees for stationary points.
Weaknesses:
1. Overstated Contributions: While the paper introduces several approximations (e.g., convex relaxation of kernel domains, assumptions about Boolean-valued latent variables), these are not explicitly listed or discussed in detail. This could mislead readers about the generality of the proposed approach.
2. Clarity Issues: Certain sections, such as the explanation of the penalty function in Section 3.2 and the reasoning behind normalized kernels, are not sufficiently clear. Additionally, vague assumptions (e.g., domain of \(N\), full row rank of \(\Theta\)) need to be explicitly stated.
3. Convexity Limitations: The final formulation is not fully convex due to non-standard constraints, which could limit its applicability in practice. The authors should clarify the implications of these nonconvex constraints.
4. Comparison with Prior Work: The paper does not adequately compare the proposed three-layer model with the two-layer model from [25]. A direct comparison of the new approach with prior methods would strengthen the paper's claims.
5. Boolean Latent Variables: It remains unclear whether the methods from [25] extend to multi-layer architectures with Boolean-valued latent variables. This ambiguity weakens the theoretical contribution.
Suggestions for Improvement:
1. Include a detailed list of all approximations and assumptions made in the paper.
2. Provide a clearer exposition of the penalty function and the rationale for normalized kernels.
3. Compare the proposed approach more explicitly with [25], particularly for two-layer models.
4. Address the claim that "Parity is easily computable" with a proper reference.
5. Correct potential errors, such as the transposition issue in Equation (7).
Recommendation:
While the paper introduces innovative ideas and demonstrates promising results, the overstated contributions, lack of clarity in certain sections, and insufficient comparison with prior work are significant concerns. If these issues are addressed, the paper has the potential to make a strong contribution to the field. Conditional acceptance is recommended, contingent on revisions that clarify assumptions, improve exposition, and provide a more thorough comparison with prior work.