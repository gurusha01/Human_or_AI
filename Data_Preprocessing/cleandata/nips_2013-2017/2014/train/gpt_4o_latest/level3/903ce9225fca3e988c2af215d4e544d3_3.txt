The paper addresses the problem of multi-armed bandits (MAB) with non-stationary rewards by introducing a novel model that bounds the total variation in expected rewards over time. The authors establish a lower bound on regret for this setting and propose a new algorithm, Rexp3, which achieves a matching upper bound, demonstrating theoretical minimax optimality. This work bridges the gap between stochastic and adversarial MAB frameworks, offering insights into the exploration-exploitation tradeoff under non-stationary conditions. The paper is well-structured, clearly written, and appropriately cites prior work, situating its contributions within the broader MAB literature.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous characterization of regret in non-stationary MAB problems, linking the variation budget \( V_T \) to achievable performance. The lower and upper bounds are well-aligned, showcasing the tightness of the analysis.
2. Algorithm Design: The proposed Rexp3 algorithm, an extension of EXP3, is theoretically sound and achieves minimax regret up to logarithmic factors in the number of arms.
3. Clarity: The proof sketch for Theorem 1 is concise and well-presented, making the technical results accessible. The discussion of the "remembering vs. forgetting" tradeoff is particularly insightful.
4. Novelty: The formulation of the problem as an intermediate setting between stochastic and adversarial bandits is innovative and broadens the scope of MAB research.
Weaknesses:
1. Practical Applicability: The bounded variation assumption, while mathematically elegant, lacks clear motivation for real-world scenarios. Testing or estimating \( V_T \) in practice is not straightforward, and the authors do not sufficiently address this challenge.
2. Algorithm Limitations: The Rexp3 algorithm, while theoretically optimal, is impractical due to its reliance on forgetting past information at each epoch. This design choice may hinder its usability in real-world applications where historical data is valuable.
3. Assumption Critique: The assumption that \( V_T \) is known or learnable is questionable. The paper briefly discusses potential estimation methods but does not provide a concrete or robust solution.
4. Stationary Case Oversight: Theorem 2 does not explicitly address the scenario where \( V_T \to 0 \), which corresponds to the stationary case. This omission leaves a gap in understanding how the proposed framework transitions to classical stationary MAB settings.
Recommendation:
While the paper makes significant theoretical contributions, the unresolved issues regarding the practical applicability of the algorithm and the bounded variation assumption limit its impact. The lack of a detailed discussion on how \( V_T \) can be estimated or learned in practice is a critical weakness. Additionally, the impracticality of Rexp3 in retaining past information raises concerns about its real-world utility. For these reasons, I recommend rejection at this stage. However, with further work addressing these limitations, the paper has the potential to make a strong contribution to the field.
Pro/Con Summary:
Pros:
- Rigorous theoretical analysis with tight bounds.
- Novel problem formulation bridging stochastic and adversarial settings.
- Clear and well-organized presentation.
Cons:
- Limited practical applicability of the bounded variation assumption.
- Impractical algorithm design.
- Insufficient discussion on estimating \( V_T \) and stationary cases.