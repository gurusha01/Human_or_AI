This paper presents an approach to train a fast deep neural network (NN) for Atari games by leveraging a high-quality but computationally expensive planning-based agent as a teacher. The proposed method outperforms the state-of-the-art reinforcement learning (RL) method, Deep Q-Network (DQN), on several Atari games. The authors introduce three variations of their method, with UCTtoClassification-Interleaved emerging as the most effective. The approach is grounded in supervised learning, where the planning agent generates training data for a convolutional neural network (CNN). This work bridges the performance gap between slow but strong planning agents and real-time RL agents, demonstrating that supervised training with high-quality planners can yield superior results.
Strengths:
1. Simplicity and Effectiveness: The proposed method is straightforward and builds on well-established techniques, making it easy to understand and implement. The authors successfully demonstrate that supervised learning with planning-based agents can outperform RL methods like DQN in real-time settings.
2. Empirical Results: The experimental results are compelling, showing significant performance improvements over DQN in most games. The use of interleaved training to address distribution mismatch between training and testing is particularly noteworthy.
3. Implementation: The authors provide detailed descriptions of their methods, including preprocessing steps, CNN architecture, and training procedures, which enhances reproducibility.
4. Insightful Visualizations: The visualizations of learned features and policies offer valuable insights into the behavior of the trained agents and the strengths and limitations of the approach.
Weaknesses:
1. Clarity: The paper is overly verbose, particularly in the abstract and introduction. The text could be significantly condensed to improve clarity and readability. For example, the background sections on RL and DL could be streamlined without losing critical information.
2. Discussion of Prior Work: The discussion of related work is unbalanced, focusing heavily on recent results like DQN while neglecting foundational contributions in deep learning and reinforcement learning. A more comprehensive review of prior work, including earlier methods for combining RL and DL, would strengthen the paper.
3. Expected Results: While the results are impressive, they align with the intuitive expectation that supervised learning with strong planners can outperform RL for similar architectures. The novelty lies more in the empirical validation than in the conceptual contribution.
4. Limited Scope: The evaluation is restricted to seven Atari games, and the generality of the method across other RL benchmarks or more complex environments remains unexplored.
Arguments for Acceptance:
- The method is simple, effective, and well-implemented, with strong empirical results that advance the state of the art in real-time Atari game playing.
- The interleaved training approach addresses a critical issue of distribution mismatch, providing a novel and practical solution.
Arguments Against Acceptance:
- The paper lacks a balanced discussion of prior work, which undermines its positioning within the broader context of RL and DL research.
- The verbosity and lack of focus in the text detract from the clarity of the presentation.
Recommendation:
This paper makes a meaningful contribution to the field by demonstrating the effectiveness of supervised learning with planning-based agents for real-time game playing. However, revisions are needed to improve clarity and provide a more balanced discussion of related work. If these issues are addressed, the paper would be a strong candidate for acceptance.