This paper introduces two novel subsampling methods, Influence Weighted Subsampling (IWS-LS) and Residual Weighted Subsampling (RWS-LS), aimed at improving robustness in large-scale least squares learning under corrupted observation models. The authors leverage the concept of influence, a regression diagnostic tool, to detect and mitigate the impact of corrupted data points. Theoretical contributions include an estimation error bound for IWS-LS, demonstrating its ability to reduce both bias and variance relative to ordinary least squares (OLS) and existing randomized approximations. Empirical results on synthetic and real-world datasets, including the airline delay dataset, further validate the proposed methods, showing superior performance in handling corrupted data compared to state-of-the-art methods like ULURU and SRHT-LS.
Strengths:
1. Novelty and Originality: The paper addresses a critical gap in robust large-scale regression by proposing influence-based subsampling, which is novel and well-motivated. The extension of subsampling methods to corrupted observation models advances the state of the art.
2. Theoretical Rigor: The authors provide a detailed theoretical analysis of IWS-LS, including bounds on estimation error, which strengthens the validity of their claims.
3. Empirical Validation: The experiments, though limited in dataset size, convincingly demonstrate the advantages of IWS-LS and RWS-LS over existing methods, particularly in scenarios with significant data corruption.
4. Clarity and Writing: The paper is generally well-written and organized, with clear explanations of the proposed methods and their theoretical underpinnings.
Weaknesses:
1. Dataset Size and Baselines: The experiments are conducted on relatively small datasets, which may not fully showcase the scalability of the proposed methods. Additionally, exact methods should be included as baselines for a more comprehensive comparison.
2. Experiment Details: The paper lacks sufficient details about the experimental environment, such as hardware specifications and running times. Given the focus on large-scale learning, time efficiency is a critical metric that should be reported.
3. Limited Discussion on Related Work: While the paper focuses on robustness to data corruption, it could benefit from discussing related techniques like dropout training and marginalized corrupted features, which also aim to handle noisy data.
4. Minor Typos: There are minor typographical errors (e.g., "more more realistic" and "ideas from $4 and $4") that should be corrected for clarity.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound approach to a challenging problem in large-scale regression.
- The proposed methods are empirically validated and show clear improvements over existing techniques.
- The work is relevant to the conference's focus on advancing machine learning methods.
Arguments Against Acceptance:
- The experiments are limited in scale and lack critical details on computational efficiency.
- The paper could better situate its contributions within the broader context of related work on handling noisy data.
Recommendation:
I recommend acceptance with minor revisions. While the paper makes significant theoretical and empirical contributions, addressing the experimental limitations and providing additional context on related work would strengthen its impact.