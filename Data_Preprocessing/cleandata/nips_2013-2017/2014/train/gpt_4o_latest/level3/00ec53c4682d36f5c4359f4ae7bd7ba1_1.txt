The paper presents a novel two-stream ConvNet architecture for action recognition in videos, combining spatial and temporal information through separate networks for appearance (still frames) and motion (optical flow). The authors demonstrate the utility of optical flow as input for temporal ConvNets, achieving competitive results on UCF-101 and HMDB-51 datasets. The work also explores multi-task learning to leverage additional training data and mitigate overfitting on small datasets. The proposed method advances the state of the art in video classification and provides a compelling alternative to hand-crafted shallow representations.
Strengths:
The paper makes a significant contribution by demonstrating the effectiveness of optical flow fields as input for action recognition, a novel approach in deep learning. The two-stream architecture effectively decouples spatial and temporal information, allowing each network to specialize and achieve complementary results. The experiments are well-designed, with extensive evaluations on two standard benchmarks. The use of multi-task learning is particularly innovative, enabling the model to generalize better by leveraging multiple datasets. The results are competitive with state-of-the-art methods, and the approach has the potential to influence future research in video-based vision tasks.
Weaknesses:
A notable limitation of the model is its reliance on fixed-length adjacent frames, which restricts its ability to capture the overall sequence of actions in a video. This could be problematic for actions requiring long-term temporal dependencies. Additionally, while the experiments are thorough, the paper lacks error mode analysis and insights into misclassified videos, which would provide a deeper understanding of the model's limitations. The computational cost of pre-computing optical flow and the reliance on pre-trained spatial ConvNets may also limit the model's scalability to larger datasets or real-time applications.
Quality:
The paper is technically sound, with claims well-supported by experimental results. However, the absence of error analysis and discussion of failure cases is a missed opportunity to provide actionable insights for improvement.
Clarity:
The paper is clearly written and well-organized, with sufficient detail to allow reproduction of results. The explanation of the two-stream architecture and the role of optical flow is particularly clear.
Originality:
The use of optical flow as input for temporal ConvNets is a novel and impactful idea. The combination of spatial and temporal streams in a deep learning framework is a creative extension of prior work, and the multi-task learning approach adds further originality.
Significance:
The proposed method addresses a challenging problem in video action recognition and provides a promising approach that could inspire further research. The competitive results on standard benchmarks highlight its practical relevance.
Recommendation:
Accept with minor revisions. The paper makes a strong contribution to the field of video-based action recognition, but it would benefit from additional analysis of failure cases and a discussion of potential improvements, such as handling long-term dependencies and reducing computational overhead.