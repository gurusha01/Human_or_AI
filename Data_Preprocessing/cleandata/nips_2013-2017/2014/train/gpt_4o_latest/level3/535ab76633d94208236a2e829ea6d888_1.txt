This paper tackles the critical problem of identifying and filtering adversarial workers in crowdsourced classification tasks, a challenge that undermines the reliability of crowdsourced datasets. The authors propose a novel reputation-based algorithm that assigns penalties to workers based on disagreements rather than rewarding agreements, a departure from traditional approaches. Two penalty algorithms—soft and hard—are introduced, with theoretical guarantees provided under various adversarial strategies. The experimental evaluation spans both synthetic and real-world datasets, demonstrating the algorithm's potential to enhance the performance of existing label aggregation methods.
Strengths:
1. Novelty: The paper addresses a more realistic setting compared to prior work, moving beyond the assumption that workers' votes are i.i.d. samples. This broadens its applicability to real-world scenarios where worker behavior is often non-random.
2. Theoretical Rigor: The authors provide strong theoretical guarantees for their algorithms, including bounds on adversarial impact (Theorems 3 and 4). This is a significant contribution to understanding the limits of adversarial influence in crowdsourced systems.
3. Practical Relevance: The empirical results on real-world datasets demonstrate the algorithm's ability to identify diverse adversarial strategies, including malicious, random, and uniform labeling behaviors. The integration of the reputation algorithm with existing methods like EM and KOS shows measurable performance improvements.
4. Algorithmic Design: The use of optimal semi-matchings for load-balanced penalty assignment is a clever and computationally efficient approach to mitigating adversarial influence.
Weaknesses:
1. Assumptions: The assumption that honest workers are perfect (i.e., always label correctly) is overly strong and limits the generalizability of the theoretical results. While empirical evaluations partially address this, a more relaxed assumption would strengthen the paper.
2. Clarity of Theoretical Results: Theorems 3 and 4, while valuable, are dense and could benefit from illustrative examples to clarify the bounds and their implications.
3. Experimental Gaps: The experimental results are incomplete in some areas. For instance, the number of filtered users varies across settings, but the paper does not explore the full range of possible outcomes or provide a systematic analysis of this variability.
4. Dataset Limitations: The algorithm's performance on datasets where workers complete varying numbers of tasks is not fully explored, which is a common scenario in real-world crowdsourcing.
Arguments for Acceptance:
- The paper addresses a critical and underexplored problem in crowdsourcing.
- It introduces a novel and theoretically grounded approach with practical relevance.
- Empirical results demonstrate the algorithm's effectiveness in real-world settings.
Arguments Against Acceptance:
- The strong assumption of perfect honest workers limits the theoretical contributions.
- The clarity of some theoretical results and completeness of experiments need improvement.
- The paper does not fully address scenarios with varying worker-task distributions.
Recommendation:
This is a borderline paper. While it makes interesting contributions and advances the state of the art, the limitations in assumptions, clarity, and experimental completeness prevent it from being a clear accept. If the authors can address these issues in a revision, the paper would be a strong candidate for acceptance.