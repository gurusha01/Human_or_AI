This paper explores the relationship between dimensionality and communication cost in distributed learning, focusing on the problem of estimating the mean of a high-dimensional Gaussian distribution. The authors establish that communication cost scales linearly with the number of dimensions and derive novel lower bounds for both interactive and simultaneous communication settings. Specifically, they prove lower bounds of \(\Omega(md / \log(m))\) and \(\Omega(md)\) for the communication cost in these settings, respectively. Additionally, they present an interactive protocol achieving the minimax squared loss with \(O(md)\) bits of communication, improving upon prior results. The paper also initiates the study of distributed parameter estimation for structured (s-sparse) parameters, showing that communication cost scales with sparsity \(s\) rather than dimensionality \(d\).
Strengths
1. Key Contributions: The derivation of new lower bounds and the direct-sum theorem are significant theoretical contributions. The direct-sum theorem is a generic tool that could be applied to other statistical problems beyond Gaussian mean estimation.
2. Novelty: The paper introduces the idea of leveraging sparsity in distributed parameter estimation, which is a promising direction for reducing communication costs in high-dimensional settings.
3. Improved Upper Bound: The interactive protocol achieving \(O(md)\) communication cost demonstrates a meaningful improvement over existing methods.
4. Potential Impact: The results are relevant to distributed learning, a critical area in modern machine learning, where communication bottlenecks are a major challenge.
Weaknesses
1. Clarity: The paper is difficult to follow due to missing definitions of key concepts such as s-sparsity, private/public randomness, and protocol. This lack of clarity may hinder accessibility for readers unfamiliar with the prior work.
2. Dependence on Prior Work: The paper assumes familiarity with [4], which may not be accessible to all readers. A more detailed explanation of the derivations from [4] would improve self-containment.
3. Significance of Results: While the linear scaling of communication cost with dimensions is rigorously proven, it appears intuitive. The authors need to better articulate why this result is surprising or impactful.
4. Sparse Parameter Estimation: The conjecture about the optimal tradeoff between communication cost and squared loss for s-sparse parameters is intriguing but remains unproven, leaving a gap in the completeness of the work.
Suggestions for Improvement
1. Provide clear definitions and introductory explanations for key concepts to make the paper accessible to a broader audience.
2. Include a more detailed discussion of why the linear scaling result is significant and how it advances the state of the art.
3. Expand on the derivations from [4] to ensure the paper is self-contained.
4. Strengthen the experimental or theoretical validation of the conjecture on sparse parameter estimation.
Recommendation
While the paper makes several important contributions, the clarity and accessibility issues, as well as the lack of a proven conjecture, limit its impact. I recommend acceptance with minor revisions, contingent on addressing the clarity concerns and better contextualizing the significance of the results. The theoretical contributions and potential applications in distributed learning make this work a valuable addition to the field.