This paper introduces SHAMPO, an online multitask learning algorithm that addresses the challenge of sharing a single annotator across multiple tasks. The authors propose a framework where tasks compete for limited annotation resources, and SHAMPO uses a multi-armed bandit-inspired exploration-exploitation strategy to allocate annotations effectively. The algorithm is analyzed in the mistake-bound model, and its performance is evaluated on several datasets, including OCR, vowel prediction, and document classification. The authors also demonstrate how SHAMPO can be adapted to contextual bandit problems, such as one-vs-rest and one-vs-one multiclass classification. Empirical results show that SHAMPO outperforms baseline methods, achieving higher accuracy for the same annotation effort.
Strengths:
The paper tackles an important and practical problem in multitask learning, particularly in scenarios with limited annotation resources. The use of multi-armed bandit techniques to balance exploration and exploitation is novel in this context and aligns well with the interests of the NeurIPS community. The theoretical analysis, including mistake bounds, provides a solid foundation for the proposed approach. Empirical results on diverse datasets demonstrate the effectiveness of SHAMPO, and the extension to contextual bandits adds versatility to the framework. The paper also highlights the practical benefits of focusing annotation efforts on harder tasks, which is a valuable insight for real-world applications.
Weaknesses:
1. Task Independence Assumption: The paper assumes no dependency between tasks, which deviates from standard multitask learning literature. This assumption is not well-justified and limits the applicability of the method to scenarios where task relationships are important.
2. Single Feature Annotation Restriction: The restriction of annotating only one feature at a time lacks practical motivation, particularly in real-world crowdsourcing platforms like Mechanical Turk, where annotators can often handle multiple tasks simultaneously.
3. Underutilization of Task Independence: While the paper assumes task independence, it does not exploit this property to simplify the algorithm or analysis, unlike prior work such as Romera-Paredes et al. (AISTATS 2012).
4. Comparison with Fully Labeled Tasks: The paper does not evaluate SHAMPO's performance against fully labeled tasks, which would have been useful for validating Theorem 1 and understanding the trade-offs in annotation efficiency.
5. Minor Issues: Typographical errors in references [25], [209], and [347] should be corrected.
Evaluation:
- Quality: The paper is technically sound, with rigorous theoretical analysis and comprehensive empirical evaluation. However, the lack of justification for key assumptions weakens its impact.
- Clarity: The paper is generally well-written and organized, but some sections, such as the justification for task independence, could be elaborated further.
- Originality: The approach is novel, particularly the use of bandit techniques in multitask learning with a shared annotator. However, the lack of comparison to closely related work on task independence is a missed opportunity.
- Significance: The results are promising and demonstrate the potential for SHAMPO to improve annotation efficiency. The framework could inspire further research in multitask learning and contextual bandits.
Recommendation:
While the paper has notable strengths, the weaknesses, particularly the unjustified assumptions and lack of comparison to fully labeled tasks, limit its overall impact. I recommend acceptance with minor revisions, provided the authors address the task independence assumption, clarify the practical motivation for single-feature annotation, and correct typographical errors.