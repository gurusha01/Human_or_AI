The paper proposes a novel method for estimating sparse connectivity graphs of firing neurons using an \( L_2 \)-norm penalized inverse covariance matrix estimation, with significant improvements over existing approaches. The authors replace the hard thresholding step with a soft thresholding method, which enhances performance, and formulate the cost function as cross-entropy, optimized using the BFGS algorithm. This methodological shift is well-motivated and addresses key challenges in neural connectivity inference, such as noise, low resolution, and the computational inefficiency of brute-force parameter optimization.
Strengths:  
The paper is technically sound and demonstrates a clear improvement over existing methods, particularly those used in the Kaggle Connectomics competition. The proposed approach achieves competitive AUC scores while significantly reducing computation time, which is a notable contribution to the field. The use of a convolution filter for preprocessing and the introduction of a supervised optimization framework are innovative and well-executed. The authors provide extensive references, making the paper self-contained and accessible to readers. The rebuttal further strengthens the paper by addressing computational speed and providing better justification for design choices, which enhances its overall quality.
Weaknesses:  
While the paper is clear and easy to follow, its structure feels chronological and incremental rather than being driven by a strong theoretical framework. The frequent references to the Kaggle competition give the work a retrospective feel, making it seem more like a competition solution than a forward-looking scientific contribution. Additionally, while the authors acknowledge the non-Gaussian nature of the data, the assumption of Gaussianity in the inverse covariance estimation is a limitation that could impact the generalizability of the method. The paper also lacks a detailed discussion of potential applications beyond the Kaggle dataset, which would have strengthened its significance.
Pro and Con Arguments for Acceptance:  
Pro:  
- Demonstrates a significant improvement in computational efficiency and accuracy over existing methods.  
- Introduces a novel combination of techniques (soft thresholding, convolution filters, and supervised optimization) that could inspire future research.  
- Provides extensive experimental validation and clear results.  
Con:  
- Feels like a belated Kaggle competition contribution rather than a foundational advance in the field.  
- Limited theoretical motivation and generalizability to other datasets or domains.  
- Structure could be improved to better highlight the novelty and broader impact of the work.  
Conclusion:  
This paper makes a meaningful contribution to the field of neural connectivity inference by addressing computational inefficiencies and improving accuracy. While it could benefit from a more theoretically motivated structure and broader applicability, its methodological innovations and experimental results justify its acceptance. I recommend acceptance with minor revisions to improve clarity and emphasize the broader significance of the work.