Review of the Paper
Summary
This paper proposes a framework for designing structured priors in Bayesian models using KL-divergence-based information projection. The authors focus on sparse structures, demonstrating that approximate inference over convex subsets is computationally tractable and equivalent to maximizing a submodular function. A greedy forward selection algorithm is proposed, achieving a provable approximation guarantee. The framework is evaluated on simulated data and high-dimensional neuroimaging data, showing improved support recovery and predictive accuracy over baseline models. The paper claims that its Sparse-G method outperforms existing approaches such as Lasso, Ridge, and Spike-and-Slab in both synthetic and real-world tasks.
Strengths
1. Relevance and Motivation: The paper addresses a critical challenge in high-dimensional Bayesian modelingâ€”incorporating structural constraints into prior design. This is particularly relevant for neuroimaging applications, where data is high-dimensional and sparse.
2. Experimental Results: The experiments demonstrate the potential of the Sparse-G method to outperform existing baselines in terms of support recovery and predictive accuracy, especially in challenging scenarios like high noise or limited samples.
3. Submodularity and Greedy Optimization: The theoretical guarantee of (1-1/e) optimality for the greedy algorithm is a notable strength, providing a solid foundation for the proposed approach.
4. Application to Neuroimaging: The application to fMRI data is compelling, as it demonstrates the practical utility of the method in a real-world, high-dimensional setting.
Weaknesses
1. Lack of Theoretical Contributions: While the framework is well-motivated, the paper lacks significant novel theoretical contributions. The use of KL-divergence projection and submodular optimization is not new, and the results largely extend existing ideas rather than introducing fundamentally new concepts.
2. Algorithm Presentation: The proposed algorithm is not clearly outlined, and the absence of pseudocode makes it difficult for readers to fully understand and reproduce the method. This lack of clarity detracts from the paper's overall quality.
3. Experimental Concerns: Although Sparse-G outperforms baselines, the reasoning behind its superior performance is not sufficiently explained. Additionally, inconsistencies with prior literature on hyperparameter settings (e.g., Spike-and-Slab) raise concerns about the fairness of comparisons.
4. Clarity and Writing: While the writing is generally clear, the concept of "structure" in the context of priors is not adequately explained. Furthermore, the discussion of spike-and-slab priors is inconsistent with established findings, which could confuse readers.
5. Scalability: The scalability of the method is not thoroughly addressed. While Sparse-G performs well on the neuroimaging dataset, the computational feasibility of the approach for even larger datasets remains unclear.
Arguments for Acceptance
- The paper addresses a relevant and challenging problem in Bayesian modeling.
- The experimental results, particularly on neuroimaging data, demonstrate the practical utility of the proposed method.
- The theoretical guarantee for the greedy algorithm adds rigor to the approach.
Arguments Against Acceptance
- The paper lacks significant theoretical or algorithmic novelty.
- The unclear presentation of the algorithm and experimental inconsistencies reduce reproducibility and trust in the results.
- The writing could be improved, particularly in explaining key concepts and aligning claims with prior literature.
Recommendation
While the paper makes a meaningful contribution to the application of structured priors in Bayesian modeling, the lack of theoretical novelty and clarity in presentation are significant drawbacks. I recommend rejection in its current form but encourage the authors to address these issues and resubmit. Specifically, the authors should:
1. Clearly outline the proposed algorithm with pseudocode.
2. Provide a more thorough explanation of why Sparse-G outperforms baselines.
3. Clarify the concept of "structure" and ensure consistency with prior literature.
4. Address scalability concerns and provide a more detailed discussion of computational complexity. 
With these improvements, the paper could make a stronger case for acceptance in future submissions.