This paper introduces Deep Attention Selective Networks (dasNet), a novel architecture that integrates feedback connections into convolutional neural networks (CNNs) to dynamically modulate filter activations during classification. By leveraging reinforcement learning (RL) through Separable Natural Evolution Strategies (SNES), dasNet learns an attention policy that iteratively adjusts filter sensitivities to improve classification performance. The model achieves state-of-the-art results on CIFAR-10 and CIFAR-100 datasets, demonstrating the potential of feedback mechanisms in vision tasks.
Strengths
The paper addresses an important challenge in computer vision: improving classification accuracy for ambiguous cases by mimicking the iterative, feedback-driven processing observed in biological vision systems. While the integration of feedback into CNNs is not entirely novel, the execution is robust and well-grounded. The use of SNES for policy optimization is a thoughtful choice, given the high-dimensional parameter space involved. The experimental results are compelling, with dasNet achieving significant performance gains over baseline CNNs, establishing a new state-of-the-art for unaugmented CIFAR datasets. The paper is well-written, clearly structured, and provides sufficient technical details for reproducibility. The authors also situate their work within the broader context of neuroscience-inspired machine learning, citing relevant literature.
Weaknesses
Despite its strengths, the paper has some limitations. First, while the results on CIFAR datasets are impressive, the scope of the experiments is limited to relatively small datasets. Extending the approach to larger, more challenging datasets like ImageNet would provide stronger evidence of the model's scalability and generalizability. Second, the novelty of the approach is somewhat incremental, as it builds on existing ideas of feedback and attention in neural networks. The use of SNES, while effective, is not a groundbreaking contribution. Lastly, the interpretability of the learned attention policies remains an open question. While the authors provide qualitative insights into filter activations, a more rigorous analysis of the learned dynamics would strengthen the paper.
Arguments for Acceptance
1. The paper demonstrates a well-executed application of feedback mechanisms and RL to improve CNN performance.
2. The results are significant, achieving state-of-the-art accuracy on CIFAR datasets.
3. The work is clearly presented and provides a solid foundation for future research in feedback-driven architectures.
Arguments Against Acceptance
1. The novelty of the approach is limited, as it builds on existing concepts without introducing fundamentally new techniques.
2. The experiments are restricted to small datasets, leaving questions about scalability and generalizability unanswered.
3. The interpretability of the model's attention mechanisms could be improved.
Recommendation
Overall, this paper represents a meaningful contribution to the field of computer vision and neural network design. While the novelty is incremental, the strong execution and promising results justify its acceptance. I recommend accepting the paper, with the suggestion that future work should explore larger datasets and provide deeper insights into the learned attention mechanisms.